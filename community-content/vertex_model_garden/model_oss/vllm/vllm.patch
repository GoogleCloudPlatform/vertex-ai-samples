diff --git a/pyproject.toml b/pyproject.toml
deleted file mode 100644
index b197256..0000000
--- a/pyproject.toml
+++ /dev/null
@@ -1,34 +0,0 @@
-[build-system]
-# Should be mirrored in requirements-build.txt
-requires = [
-    "ninja",
-    "packaging",
-    "setuptools >= 49.4.0",
-    "torch == 2.1.2",
-    "wheel",
-]
-build-backend = "setuptools.build_meta"
-
-[tool.ruff.lint]
-select = [
-    # pycodestyle
-    "E",
-    # Pyflakes
-    "F",
-    # pyupgrade
-    # "UP",
-    # flake8-bugbear
-    "B",
-    # flake8-simplify
-    "SIM",
-    # isort
-    # "I",
-]
-ignore = [
-    # star imports
-    "F405", "F403",
-    # lambda expression assignment
-    "E731",
-    # line too long, handled by black formatting
-    "E501",
-]
diff --git a/requirements.txt b/requirements.txt
index 92ba0a7..3506a73 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -5,9 +5,7 @@ pandas  # Required for Ray data.
 pyarrow  # Required for Ray data.
 sentencepiece  # Required for LLaMA tokenizer.
 numpy
-torch == 2.1.2
 transformers >= 4.36.0  # Required for Mixtral.
-xformers == 0.0.23.post1  # Required for CUDA 12.1.
 fastapi
 uvicorn[standard]
 pydantic == 1.10.13  # Required for OpenAI server.
diff --git a/setup.py b/setup.py
index 811d494..6e0ac70 100644
--- a/setup.py
+++ b/setup.py
@@ -12,7 +12,7 @@ from torch.utils.cpp_extension import BuildExtension, CUDAExtension, CUDA_HOME,
 
 ROOT_DIR = os.path.dirname(__file__)
 
-MAIN_CUDA_VERSION = "12.1"
+MAIN_CUDA_VERSION = "11.8"
 
 # Supported NVIDIA GPU architectures.
 NVIDIA_SUPPORTED_ARCHS = {"7.0", "7.5", "8.0", "8.6", "8.9", "9.0"}
@@ -123,10 +123,11 @@ def get_torch_arch_list() -> Set[str]:
     arch_list = torch_arch_list.intersection(valid_archs)
     # If none of the specified architectures are valid, raise an error.
     if not arch_list:
-        raise RuntimeError(
+        print(
             "None of the CUDA/ROCM architectures in `TORCH_CUDA_ARCH_LIST` env "
             f"variable ({env_arch_list}) is supported. "
             f"Supported CUDA/ROCM architectures are: {valid_archs}.")
+        return None
     invalid_arch_list = torch_arch_list - valid_archs
     if invalid_arch_list:
         warnings.warn(
diff --git a/vllm/engine/arg_utils.py b/vllm/engine/arg_utils.py
index 7e58069..6791248 100644
--- a/vllm/engine/arg_utils.py
+++ b/vllm/engine/arg_utils.py
@@ -5,6 +5,78 @@ from typing import Optional, Tuple
 
 from vllm.config import (CacheConfig, ModelConfig, ParallelConfig,
                          SchedulerConfig)
+from vllm.logger import init_logger
+import os
+from google.cloud import storage
+import boto3
+
+logger = init_logger(__name__)
+GCS_PREFIX = "gs://"
+S3_PREFIX = "s3://"
+
+
+def is_s3_path(input_path: str) -> bool:
+    return input_path.startswith(S3_PREFIX)
+
+
+def download_s3_dir_to_local(s3_dir: str, local_dir: str):
+    if os.path.isdir(local_dir):
+        return
+    # s3://bucket_name/dir
+    bucket_name = s3_dir.split('/')[2]
+    prefix = s3_dir[len(S3_PREFIX + bucket_name) :].strip('/')
+
+    access_key_id = os.environ['AWS_ACCESS_KEY_ID']
+    secret_key = os.environ['AWS_SECRET_ACCESS_KEY']
+    client = boto3.client(
+        's3',
+        aws_access_key_id=access_key_id,
+        aws_secret_access_key=secret_key,
+    )
+    blobs = client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)
+    if not blobs:
+        raise ValueError(f"No blobs found in {s3_dir}")
+    for blob in blobs['Contents']:
+        name = blob['Key']
+        if name[-1] == '/':
+            continue
+        file_path = name[len(prefix) :].strip('/')
+        local_file_path = os.path.join(local_dir, file_path)
+        os.makedirs(os.path.dirname(local_file_path), exist_ok=True)
+        if file_path.endswith(".bin") or file_path.endswith(".safetensors"):
+            with open(local_file_path, 'w') as f:
+                f.write(f'{S3_PREFIX}{bucket_name}/{prefix}/{file_path}')
+        else:
+            print(f"==> Download {s3_dir}/{file_path} to {local_file_path}")
+            client.download_file(bucket_name, name, local_file_path)
+
+
+def is_gcs_path(input_path: str) -> bool:
+    return input_path.startswith(GCS_PREFIX)
+
+
+def download_gcs_dir_to_local(gcs_dir: str, local_dir: str):
+    if os.path.isdir(local_dir):
+        return
+    # gs://bucket_name/dir
+    bucket_name = gcs_dir.split('/')[2]
+    prefix = gcs_dir[len(GCS_PREFIX + bucket_name) :].strip('/')
+    client = storage.Client()
+    blobs = client.list_blobs(bucket_name, prefix=prefix)
+    if not blobs:
+        raise ValueError(f"No blobs found in {gcs_dir}")
+    for blob in blobs:
+        if blob.name[-1] == '/':
+            continue
+        file_path = blob.name[len(prefix) :].strip('/')
+        local_file_path = os.path.join(local_dir, file_path)
+        os.makedirs(os.path.dirname(local_file_path), exist_ok=True)
+        if file_path.endswith(".bin") or file_path.endswith(".safetensors"):
+            with open(local_file_path, 'w') as f:
+                f.write(f'{GCS_PREFIX}{bucket_name}/{prefix}/{file_path}')
+        else:
+            print(f"==> Download {gcs_dir}/{file_path} to {local_file_path}")
+            blob.download_to_filename(local_file_path)
 
 
 @dataclass
@@ -37,6 +109,14 @@ class EngineArgs:
     max_context_len_to_capture: int = 8192
 
     def __post_init__(self):
+        if not self.model:
+            self.model = os.environ.get("AIP_STORAGE_URI", "")
+            logger.info(
+                f"Load self.model from AIP_STORAGE_URI: {self.model}."
+            )
+        if not self.model:
+            raise ValueError("self.model is must be set.")
+
         if self.tokenizer is None:
             self.tokenizer = self.model
 
@@ -52,7 +132,7 @@ class EngineArgs:
         parser.add_argument(
             '--model',
             type=str,
-            default='facebook/opt-125m',
+            default=None,
             help='name or path of the huggingface model to use')
         parser.add_argument(
             '--tokenizer',
@@ -212,9 +292,39 @@ class EngineArgs:
         engine_args = cls(**{attr: getattr(args, attr) for attr in attrs})
         return engine_args
 
+    def process_gcs(self):
+        # Download GCS tokenizer.
+        if is_gcs_path(self.tokenizer) and self.tokenizer != self.model:
+            local_dir = "/tmp/gcs_tokenizer"
+            download_gcs_dir_to_local(self.tokenizer, local_dir)
+            self.tokenizer = local_dir
+        # Download GCS model without bin files.
+        if is_gcs_path(self.model):
+            local_dir = "/tmp/gcs_model"
+            download_gcs_dir_to_local(self.model, local_dir)
+            if self.tokenizer == self.model:
+                self.tokenizer = local_dir
+            self.model = local_dir
+
+    def process_s3(self):
+        # Download S3 tokenizer.
+        if is_s3_path(self.tokenizer) and self.tokenizer != self.model:
+            local_dir = "/tmp/s3_tokenizer"
+            download_s3_dir_to_local(self.tokenizer, local_dir)
+            self.tokenizer = local_dir
+        # Download S3 model without bin files.
+        if is_s3_path(self.model):
+            local_dir = "/tmp/s3_model"
+            download_s3_dir_to_local(self.model, local_dir)
+            if self.tokenizer == self.model:
+                self.tokenizer = local_dir
+            self.model = local_dir
+
     def create_engine_configs(
         self,
     ) -> Tuple[ModelConfig, CacheConfig, ParallelConfig, SchedulerConfig]:
+        self.process_gcs()
+        self.process_s3()
         model_config = ModelConfig(self.model, self.tokenizer,
                                    self.tokenizer_mode, self.trust_remote_code,
                                    self.download_dir, self.load_format,
diff --git a/vllm/entrypoints/api_server.py b/vllm/entrypoints/api_server.py
index 6910b32..7aee07a 100644
--- a/vllm/entrypoints/api_server.py
+++ b/vllm/entrypoints/api_server.py
@@ -1,4 +1,5 @@
 import argparse
+import copy
 import json
 from typing import AsyncGenerator
 
@@ -17,6 +18,16 @@ app = FastAPI()
 engine = None
 
 
+# Required by Vertex deployment.
+@app.get("/ping")
+async def ping() -> Response:
+    return Response(status_code=200)
+
+def format_output(prompt: str, output: str):
+    output = output.strip("\n")
+    return f"Prompt:\n{prompt.strip()}\nOutput:\n{output}"
+
+
 @app.get("/health")
 async def health() -> Response:
     """Health check."""
@@ -33,8 +44,12 @@ async def generate(request: Request) -> Response:
     - other fields: the sampling parameters (See `SamplingParams` for details).
     """
     request_dict = await request.json()
+    is_on_vertex = "instances" in request_dict
+    if is_on_vertex:
+        request_dict = request_dict["instances"][0]
     prompt = request_dict.pop("prompt")
     stream = request_dict.pop("stream", False)
+    raw_response = request_dict.pop("raw_response", False)
     sampling_params = SamplingParams(**request_dict)
     request_id = random_uuid()
 
@@ -42,12 +57,33 @@ async def generate(request: Request) -> Response:
 
     # Streaming case
     async def stream_results() -> AsyncGenerator[bytes, None]:
+        prior_request_output = None
         async for request_output in results_generator:
             prompt = request_output.prompt
-            text_outputs = [
-                prompt + output.text for output in request_output.outputs
-            ]
-            ret = {"text": text_outputs}
+            text_outputs = []
+            for i, output in enumerate(request_output.outputs):
+                if prior_request_output is not None:
+                    prior_output = prior_request_output.outputs[i]
+                    text_output = output.text[len(prior_output.text):]
+                else:
+                    text_output = output.text
+                text_outputs.append(text_output)
+            ret = {"predictions": text_outputs}
+            if raw_response:
+                output_token_counts = []
+                for i, output in enumerate(request_output.outputs):
+                    if prior_request_output is not None:
+                        prior_output = prior_request_output.outputs[i]
+                        output_token_count = len(output.token_ids) - len(prior_output.token_ids)
+                    else:
+                        output_token_count = len(output.token_ids)
+                    output_token_counts.append(output_token_count)
+                cumulative_logprobs = [output.cumulative_logprob for output in request_output.outputs]
+                ret.update({
+                    "output_token_counts": output_token_counts,
+                    "cumulative_logprobs": cumulative_logprobs
+                })
+            prior_request_output = copy.deepcopy(request_output)
             yield (json.dumps(ret) + "\0").encode("utf-8")
 
     if stream:
@@ -63,9 +99,19 @@ async def generate(request: Request) -> Response:
         final_output = request_output
 
     assert final_output is not None
-    prompt = final_output.prompt
-    text_outputs = [prompt + output.text for output in final_output.outputs]
-    ret = {"text": text_outputs}
+    if raw_response:
+        text_outputs = [output.text for output in final_output.outputs]
+        output_token_counts = [len(output.token_ids) for output in final_output.outputs]
+        cumulative_logprobs = [output.cumulative_logprob for output in final_output.outputs]
+        ret = {
+            "predictions": text_outputs,
+            "output_token_counts": output_token_counts,
+            "cumulative_logprobs": cumulative_logprobs
+        }
+    else:
+        prompt = final_output.prompt
+        text_outputs = [format_output(prompt, output.text) for output in final_output.outputs]
+        ret = {"predictions": text_outputs}
     return JSONResponse(ret)
 
 
diff --git a/vllm/model_executor/weight_utils.py b/vllm/model_executor/weight_utils.py
index 365c847..eeb9c75 100644
--- a/vllm/model_executor/weight_utils.py
+++ b/vllm/model_executor/weight_utils.py
@@ -286,3 +286,181 @@ def initialize_dummy_weights(
     for param in model.state_dict().values():
         if torch.is_floating_point(param):
             param.data.uniform_(low, high)
+
+
+import time
+import boto3
+from google.cloud import storage
+from huggingface_hub import hf_hub_download
+
+HF_PREFIX = "hf://"
+MODEL_DIR = "/tmp/vllm_model"
+
+
+def prepare_hf_model_weights_on_the_fly(
+    model_name_or_path: str,
+    cache_dir: Optional[str] = None,
+    use_safetensors: bool = False,
+    fall_back_to_pt: bool = True,
+    revision: Optional[str] = None,
+) -> Tuple[List[str], bool]:
+    logger.info("Loading weights on the fly.")
+    lock = get_lock(model_name_or_path, cache_dir)
+
+    hf_weights_files = []
+    if use_safetensors:
+        logger.info("Looking for .safetensors files")
+        index_filename = "model.safetensors.index.json"
+        allow_patterns = "*.safetensors"
+    else:
+        logger.info("Looking for .bin files")
+        index_filename = "pytorch_model.bin.index.json"
+        allow_patterns = "*.bin"
+    if not os.path.isdir(model_name_or_path):
+        try:
+            with lock:
+                index_file = hf_hub_download(repo_id=model_name_or_path,
+                                             filename=index_filename,
+                                             cache_dir=cache_dir)
+        except:
+            logger.info("The model is in HF hub with 1 file, download it directly.")
+            with lock:
+                hf_folder = snapshot_download(repo_id=model_name_or_path,
+                                              allow_patterns=allow_patterns,
+                                              cache_dir=cache_dir,
+                                              tqdm_class=Disabledtqdm)
+            hf_weights_files = [x for x in glob.glob(os.path.join(hf_folder, allow_patterns))]
+        else:
+            logger.info("The model is in HF hub with multiple files, do not download it now.")
+            with open(index_file, "r") as f:
+                index = json.loads(f.read())
+            weight_filenames = set(index["weight_map"].values())
+            hf_weights_files = [f"{HF_PREFIX}{model_name_or_path}/{weight_filename}" for weight_filename in weight_filenames]
+    else:
+        logger.info("The model is possibly in local disk.")
+        hf_weights_files = [x for x in glob.glob(os.path.join(model_name_or_path, allow_patterns))]
+
+    if not use_safetensors:
+        # Exclude files that are not needed for inference.
+        # https://github.com/huggingface/transformers/blob/v4.34.0/src/transformers/trainer.py#L227-L233
+        blacklist = [
+            "training_args.bin",
+            "optimizer.bin",
+            "optimizer.pt",
+            "scheduler.pt",
+            "scaler.pt",
+        ]
+        hf_weights_files = [
+            f for f in hf_weights_files
+            if not any(f.endswith(x) for x in blacklist)
+        ]
+    hf_weights_files.sort()
+
+    if not hf_weights_files and use_safetensors:
+        return prepare_hf_model_weights_on_the_fly(model_name_or_path,
+                                        cache_dir=cache_dir,
+                                        use_safetensors=False,
+                                        fall_back_to_pt=False,
+                                        revision=revision)
+    if not hf_weights_files:
+        raise RuntimeError(f"No weight files found in {model_name_or_path}")
+    logger.info(f"Fetched weight files: {hf_weights_files}")
+    return hf_weights_files, use_safetensors
+
+
+def hf_model_weights_iterator_download_on_the_fly(
+    model_name_or_path: str,
+    cache_dir: Optional[str] = None,
+    load_format: str = "auto",
+    revision: Optional[str] = None,
+    fall_back_to_pt: Optional[bool] = True,
+) -> Iterator[Tuple[str, torch.Tensor]]:
+    lock = get_lock(model_name_or_path, cache_dir)
+    hf_weights_files, use_safetensors = prepare_hf_model_weights_on_the_fly(
+            model_name_or_path=model_name_or_path,
+            cache_dir=cache_dir,
+            use_safetensors=True,
+            fall_back_to_pt=fall_back_to_pt,
+            revision=revision)
+    os.makedirs(MODEL_DIR, exist_ok=True)
+    for hf_weight_file in hf_weights_files:
+        delete_download = False
+
+        if os.path.exists(hf_weight_file):
+            prefix = open(hf_weight_file, "rb").read(2)
+            # Download from GCS.
+            if prefix == b"gs":
+                gcs_path = open(hf_weight_file).read()
+                hf_weight_filename = gcs_path.split("/")[-1]
+                local_file = os.path.join(MODEL_DIR, hf_weight_filename)
+                with lock:
+                    if not os.path.exists(local_file):
+                        client = storage.Client()
+                        with open(local_file, 'wb') as f:
+                            logger.info(f"Download {gcs_path} to {hf_weight_file}")
+                            client.download_blob_to_file(gcs_path, f)
+                hf_weight_file = local_file
+                delete_download = True
+            # Download from S3.
+            elif prefix == b"s3":
+                s3_path = open(hf_weight_file).read()
+                hf_weight_filename = s3_path.split("/")[-1]
+                local_file = os.path.join(MODEL_DIR, hf_weight_filename)
+
+                bucket_name = s3_path.split('/')[2]
+                obj_key = s3_path.split(bucket_name)[1][1:]
+                with lock:
+                    if not os.path.exists(local_file):
+                        access_key_id = os.environ['AWS_ACCESS_KEY_ID']
+                        secret_key = os.environ['AWS_SECRET_ACCESS_KEY']
+                        client = boto3.client(
+                            's3',
+                            aws_access_key_id=access_key_id,
+                            aws_secret_access_key=secret_key,
+)
+                        with open(local_file, 'wb') as f:
+                            logger.info(f"Download {s3_path} to {hf_weight_file}")
+                            client.download_fileobj(bucket_name, obj_key, f)
+                hf_weight_file = local_file
+                delete_download = True
+
+        else:
+            # Download from HF.
+            assert hf_weight_file.startswith(HF_PREFIX)
+            hf_weight_filename = os.path.basename(hf_weight_file)
+            local_file = os.path.join(MODEL_DIR, hf_weight_filename)
+            with lock:
+                if not os.path.exists(local_file):
+                    logger.info(f"Download {model_name_or_path}/{hf_weight_filename} to {local_file}")
+                    hf_hub_download(repo_id=model_name_or_path,
+                                    filename=hf_weight_filename,
+                                    local_dir=MODEL_DIR,
+                                    local_dir_use_symlinks=False,
+                                    force_download=True)
+            hf_weight_file = local_file
+            delete_download = True
+
+        if use_safetensors:
+            with safe_open(hf_weight_file, framework="pt") as f:
+                for name in f.keys():
+                    param = f.get_tensor(name)
+                    yield name, param
+                torch.distributed.barrier()
+        else:
+            torch.distributed.barrier()
+            logger.info(f"Load {hf_weight_file} to memory.")
+            state = torch.load(hf_weight_file, map_location="cpu")
+            for name, param in state.items():
+                yield name, param
+            del state
+            torch.cuda.empty_cache()
+            torch.distributed.barrier()
+
+        if delete_download:
+            with lock:
+                if os.path.exists(hf_weight_file):
+                    logger.info(f"Delete {hf_weight_file}")
+                    os.remove(hf_weight_file)
+
+
+hf_model_weights_iterator = hf_model_weights_iterator_download_on_the_fly

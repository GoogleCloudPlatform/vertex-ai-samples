name: Deploy model to endpoint for Google Cloud Vertex AI Model
description: Deploys Google Cloud Vertex AI Model to a Google Cloud Vertex AI Endpoint.
metadata:
  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/KFPv2_hell/components/google-cloud/Vertex_AI/Models/Deploy_to_endpoint/workaround_for_buggy_KFPv2_compiler/component.yaml'}
inputs:
- {name: model_name, type: String, description: Full resource name of a Google Cloud
    Vertex AI Model}
- name: endpoint_name
  type: String
  description: |-
    Optional. Full name of Google Cloud Vertex Endpoint. A new
    endpoint is created if the name is not passed.
  optional: true
- name: machine_type
  type: String
  description: |-
    The type of the machine. See the [list of machine types
    supported for prediction
    ](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types).
    Defaults to "n1-standard-2"
  default: n1-standard-2
  optional: true
- name: min_replica_count
  type: Integer
  description: |-
    Optional. The minimum number of machine replicas this deployed
    model will be always deployed on. If traffic against it increases,
    it may dynamically be deployed onto more replicas, and as traffic
    decreases, some of these extra replicas may be freed.
  default: '1'
  optional: true
- name: max_replica_count
  type: Integer
  description: |-
    Optional. The maximum number of replicas this deployed model may
    be deployed on when the traffic against it increases. If requested
    value is too large, the deployment will error, but if deployment
    succeeds then the ability to scale the model to that many replicas
    is guaranteed (barring service outages). If traffic against the
    deployed model increases beyond what its replicas at maximum may
    handle, a portion of the traffic will be dropped. If this value
    is not provided, the smaller value of min_replica_count or 1 will
    be used.
  default: '1'
  optional: true
- name: accelerator_type
  type: String
  description: |-
    Optional. Hardware accelerator type. Must also set accelerator_count if used.
    One of ACCELERATOR_TYPE_UNSPECIFIED, NVIDIA_TESLA_K80, NVIDIA_TESLA_P100,
    NVIDIA_TESLA_V100, NVIDIA_TESLA_P4, NVIDIA_TESLA_T4
  optional: true
- {name: accelerator_count, type: Integer, description: Optional. The number of accelerators
    to attach to a worker replica., optional: true}
outputs:
- {name: endpoint_name, type: String}
- {name: endpoint_dict, type: JsonObject}
implementation:
  container:
    image: python:3.9
    command:
    - sh
    - -c
    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
      'google-cloud-aiplatform==1.7.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
      -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform==1.7.0'
      --user) && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp)
      printf "%s" "$0" > "$program_path"
      python3 -u "$program_path" "$@"
    - |
      def deploy_model_to_endpoint_for_Google_Cloud_Vertex_AI_Model(
          model_name,
          endpoint_name = None,
          machine_type = "n1-standard-2",
          min_replica_count = 1,
          max_replica_count = 1,
          accelerator_type = None,
          accelerator_count = None,
          #
          # Uncomment when anyone requests these:
          # deployed_model_display_name: str = None,
          # traffic_percentage: int = 0,
          # traffic_split: dict = None,
          # service_account: str = None,
          # explanation_metadata: "google.cloud.aiplatform_v1.types.explanation_metadata.ExplanationMetadata" = None,
          # explanation_parameters: "google.cloud.aiplatform_v1.types.explanation.ExplanationParameters" = None,
          #
          # encryption_spec_key_name: str = None,
      ):
          """Deploys Google Cloud Vertex AI Model to a Google Cloud Vertex AI Endpoint.

          Args:
              model_name: Full resource name of a Google Cloud Vertex AI Model
              endpoint_name: Optional. Full name of Google Cloud Vertex Endpoint. A new
                  endpoint is created if the name is not passed.
              machine_type: The type of the machine. See the [list of machine types
                  supported for prediction
                  ](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types).
                  Defaults to "n1-standard-2"
              min_replica_count (int):
                  Optional. The minimum number of machine replicas this deployed
                  model will be always deployed on. If traffic against it increases,
                  it may dynamically be deployed onto more replicas, and as traffic
                  decreases, some of these extra replicas may be freed.
              max_replica_count (int):
                  Optional. The maximum number of replicas this deployed model may
                  be deployed on when the traffic against it increases. If requested
                  value is too large, the deployment will error, but if deployment
                  succeeds then the ability to scale the model to that many replicas
                  is guaranteed (barring service outages). If traffic against the
                  deployed model increases beyond what its replicas at maximum may
                  handle, a portion of the traffic will be dropped. If this value
                  is not provided, the smaller value of min_replica_count or 1 will
                  be used.
              accelerator_type (str):
                  Optional. Hardware accelerator type. Must also set accelerator_count if used.
                  One of ACCELERATOR_TYPE_UNSPECIFIED, NVIDIA_TESLA_K80, NVIDIA_TESLA_P100,
                  NVIDIA_TESLA_V100, NVIDIA_TESLA_P4, NVIDIA_TESLA_T4
              accelerator_count (int):
                  Optional. The number of accelerators to attach to a worker replica.
          """
          import json
          from google.cloud import aiplatform

          model = aiplatform.Model(model_name=model_name)

          if endpoint_name:
              endpoint = aiplatform.Endpoint(endpoint_name=endpoint_name)
          else:
              endpoint_display_name = model.display_name[:118] + "_endpoint"
              endpoint = aiplatform.Endpoint.create(
                  display_name=endpoint_display_name,
                  project=model.project,
                  location=model.location,
                  # encryption_spec_key_name=encryption_spec_key_name,
                  labels={"component-source": "github-com-ark-kun-pipeline-components"},
              )

          endpoint = model.deploy(
              endpoint=endpoint,
              # deployed_model_display_name=deployed_model_display_name,
              machine_type=machine_type,
              min_replica_count=min_replica_count,
              max_replica_count=max_replica_count,
              accelerator_type=accelerator_type,
              accelerator_count=accelerator_count,
              # service_account=service_account,
              # explanation_metadata=explanation_metadata,
              # explanation_parameters=explanation_parameters,
              # encryption_spec_key_name=encryption_spec_key_name,
          )

          endpoint_json = json.dumps(endpoint.to_dict(), indent=2)
          print(endpoint_json)
          return (endpoint.resource_name, endpoint_json)

      def _serialize_json(obj) -> str:
          if isinstance(obj, str):
              return obj
          import json
          def default_serializer(obj):
              if hasattr(obj, 'to_struct'):
                  return obj.to_struct()
              else:
                  raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
          return json.dumps(obj, default=default_serializer, sort_keys=True)

      def _serialize_str(str_value: str) -> str:
          if not isinstance(str_value, str):
              raise TypeError('Value "{}" has type "{}" instead of str.'.format(str(str_value), str(type(str_value))))
          return str_value

      import argparse
      _parser = argparse.ArgumentParser(prog='Deploy model to endpoint for Google Cloud Vertex AI Model', description='Deploys Google Cloud Vertex AI Model to a Google Cloud Vertex AI Endpoint.')
      _parser.add_argument("--model-name", dest="model_name", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--endpoint-name", dest="endpoint_name", type=str, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("--machine-type", dest="machine_type", type=str, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("--min-replica-count", dest="min_replica_count", type=int, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("--max-replica-count", dest="max_replica_count", type=int, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("--accelerator-type", dest="accelerator_type", type=str, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("--accelerator-count", dest="accelerator_count", type=int, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
      _parsed_args = vars(_parser.parse_args())
      _output_files = _parsed_args.pop("_output_paths", [])

      _outputs = deploy_model_to_endpoint_for_Google_Cloud_Vertex_AI_Model(**_parsed_args)

      _output_serializers = [
          _serialize_str,
          _serialize_json,

      ]

      import os
      for idx, output_file in enumerate(_output_files):
          try:
              os.makedirs(os.path.dirname(output_file))
          except OSError:
              pass
          with open(output_file, 'w') as f:
              f.write(_output_serializers[idx](_outputs[idx]))
    args:
    - --model-name
    - {inputValue: model_name}
    - if:
        cond: {isPresent: endpoint_name}
        then:
        - --endpoint-name
        - {inputValue: endpoint_name}
    - if:
        cond: {isPresent: machine_type}
        then:
        - --machine-type
        - {inputValue: machine_type}
    - if:
        cond: {isPresent: min_replica_count}
        then:
        - --min-replica-count
        - {inputValue: min_replica_count}
    - if:
        cond: {isPresent: max_replica_count}
        then:
        - --max-replica-count
        - {inputValue: max_replica_count}
    - if:
        cond: {isPresent: accelerator_type}
        then:
        - --accelerator-type
        - {inputValue: accelerator_type}
    - if:
        cond: {isPresent: accelerator_count}
        then:
        - --accelerator-count
        - {inputValue: accelerator_count}
    - '----output-paths'
    - {outputPath: endpoint_name}
    - {outputPath: endpoint_dict}

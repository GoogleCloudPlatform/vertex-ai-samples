{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "<!-- <table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/ai-platform-samples/blob/master/ai-platform-unified/notebooks/notebook_template.ipynb\"\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/ai-platform-samples/blob/master/ai-platform-unified/notebooks/notebook_template.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table> -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yTsQctS8QLd"
      },
      "source": [
        "# Orchestrating ML workflow to Train and Deploy a PyTorch Text Classification Model on [Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook is an extension to the [previous notebook](./pytorch-text-classification-vertex-ai-train-tune-deploy.ipynb) to fine-tune and deploy a [pre-trained BERT model from HuggingFace Hub](https://huggingface.co/bert-base-cased) for sentiment classification task. This notebook shows how to automate and monitor a PyTorch based ML workflow by orchestrating the pipeline in a serverless manner using [Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction).\n",
        " \n",
        "The notebook defines a pipeline using [Kubeflow Pipelines v2 (`kfp.v2`) SDK](https://www.kubeflow.org/docs/components/pipelines/sdk-v2/) and submits the pipeline to Vertex AI Pipelines services.\n",
        "\n",
        "### Dataset\n",
        "\n",
        "The notebook uses [IMDB movie review dataset](https://huggingface.co/datasets/imdb) from [Hugging Face Datasets](https://huggingface.co/datasets).\n",
        "\n",
        "### Objective\n",
        "\n",
        "How to **orchestrate PyTorch ML workflows on [Vertex AI](https://cloud.google.com/vertex-ai)** and emphasize first class support for training, deploying and orchestrating PyTorch workflows on Vertex AI. \n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "This notebook covers following sections:\n",
        "\n",
        "---\n",
        "- [High Level Flow of Building a Pipeline](#High-Level-Flow-of-Building-a-Pipeline): Understand pipeline concepts and pipeline schematic\n",
        "- [Define the Pipeline Components](#Define-the-Pipeline-Components-for-PyTorch-based-ML-Workflow): Authoring custom pipeline components  for PyTorch based ML Workflow\n",
        "- [Define Pipeline Specification](#Define-Pipeline-Specification): Author pipeline specification using KFP v2 SDK for PyTorch based ML workflow\n",
        "- [Submit Pipeline](#Submit-Pipeline): Compile and execute pipeline on Vertex AI Pipelines\n",
        "- [Monitoring the Pipeline](#Monitoring-the-Pipeline): Monitor progress of pipeline and view logs, lineage, artifacts and pipeline runs\n",
        "---\n",
        "\n",
        "### Costs \n",
        "\n",
        "This tutorial uses billable components of Google Cloud Platform (GCP):\n",
        "\n",
        "* [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench)\n",
        "* [Vertex AI Training](https://cloud.google.com/vertex-ai/docs/training/custom-training)\n",
        "* [Vertex AI Predictions](https://cloud.google.com/vertex-ai/docs/predictions/getting-predictions)\n",
        "* [Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction)\n",
        "* [Cloud Storage](https://cloud.google.com/storage)\n",
        "* [Container Registry](https://cloud.google.com/container-registry)\n",
        "* [Cloud Build](https://cloud.google.com/build) *[Optional]*\n",
        "\n",
        "Learn about [Vertex AI Pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage Pricing](https://cloud.google.com/storage/pricing) and [Cloud Build Pricing](https://cloud.google.com/build/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage.\n",
        "\n",
        "***\n",
        "**NOTE:** This notebook does not require a GPU runtime. However, you must have GPU quota for running the jobs with GPUs launched by pipelines. Check the [quotas](https://console.cloud.google.com/iam-admin/quotas) page to ensure that you have enough GPUs available in your project. If GPUs are not listed on the quotas page or you require additional GPU quota, [request a quota increase](https://cloud.google.com/compute/quotas#requesting_additional_quota). Free Trial accounts do not receive GPU quota by default.\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze4-nDLfK4pw"
      },
      "source": [
        "### Set up your local development environment\n",
        "\n",
        "**If you are using Colab or Google Cloud Notebooks**, your environment already meets\n",
        "all the requirements to run this notebook. You can skip this step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCuSR8GkAgzl"
      },
      "source": [
        "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
        "You need the following:\n",
        "\n",
        "* The Google Cloud SDK\n",
        "* Git\n",
        "* Python 3\n",
        "* virtualenv\n",
        "* Jupyter notebook running in a virtual environment with Python 3\n",
        "\n",
        "The Google Cloud guide to [Setting up a Python development\n",
        "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
        "installation guide](https://jupyter.org/install) provide detailed instructions\n",
        "for meeting these requirements. The following steps provide a condensed set of\n",
        "instructions:\n",
        "\n",
        "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
        "2. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
        "3. [Install virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv) and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
        "4. To install Jupyter, run `pip3 install jupyter` on the command-line in a terminal shell.\n",
        "5. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
        "6. Open this notebook in the Jupyter Notebook Dashboard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7EUnXsZhAGF"
      },
      "source": [
        "### Install additional packages\n",
        "\n",
        "Following are the Python dependencies required for this notebook and will be installed in the Notebooks instance itself.\n",
        "\n",
        "- [Kubeflow Pipelines v2 SDK](https://pypi.org/project/kfp/)\n",
        "- [Google Cloud Pipeline Components](https://pypi.org/project/google-cloud-pipeline-components/) \n",
        "- [Vertex AI SDK for Python](https://pypi.org/project/google-cloud-aiplatform/) \n",
        "\n",
        "---\n",
        "The notebook has been tested with the following versions of Kubeflow Pipelines SDK and Google Cloud Pipeline Components\n",
        "\n",
        "```\n",
        "kfp version: 1.8.10\n",
        "google_cloud_pipeline_components version: 0.2.2\n",
        "```\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaYsrh0Tc17L"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34243b3c5bae"
      },
      "outputs": [],
      "source": [
        "!pip -q install {USER_FLAG} --upgrade kfp\n",
        "!pip -q install {USER_FLAG} --upgrade google-cloud-pipeline-components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76ed78b69ae0"
      },
      "source": [
        "#### Install Vertex AI SDK for Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a96cc9f4156"
      },
      "source": [
        "The notebook uises [Vertex AI SDK for Python](https://cloud.google.com/vertex-ai/docs/start/client-libraries#python) to interact with Vertex AI services. The high-level `google-cloud-aiplatform` library is designed to simplify common data science workflows by using wrapper classes and opinionated defaults."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aR7LNYMUCVKc"
      },
      "outputs": [],
      "source": [
        "!pip -q install {USER_FLAG} --upgrade google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhq5zEbGg0XX"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzrelQZ22IZj"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GPgNN7eeX1l"
      },
      "source": [
        "Check the versions of the packages you installed.  The KFP SDK version should be >=1.6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NN0mULkEeb84"
      },
      "outputs": [],
      "source": [
        "!python3 -c \"import kfp; print('kfp version: {}'.format(kfp.__version__))\"\n",
        "!python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWEdiXsJg0XY"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "This notebook does not require a GPU runtime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF1j6f9HApxa"
      },
      "source": [
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "1. Enable following APIs in your project required for running the tutorial\n",
        "    - [Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)\n",
        "    - [Cloud Storage API](https://console.cloud.google.com/flows/enableapi?apiid=storage.googleapis.com)\n",
        "    - [Container Registry API](https://console.cloud.google.com/flows/enableapi?apiid=containerregistry.googleapis.com)\n",
        "    - [Cloud Build API](https://console.cloud.google.com/flows/enableapi?apiid=cloudbuild.googleapis.com)\n",
        "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
        "1. Enter your project ID in the cell below. Then run the cell to make sure the Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WReHDGG5g0XY"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab5f23615e4a"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # <---CHANGE THIS TO YOUR PROJECT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM1iC_MfAts1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Get your Google Cloud project ID using google.auth\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    import google.auth\n",
        "\n",
        "    _, PROJECT_ID = google.auth.default()\n",
        "    print(\"Project ID: \", PROJECT_ID)\n",
        "\n",
        "# validate PROJECT_ID\n",
        "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
        "    print(\n",
        "        f\"Please set your project id before proceeding to next step. Currently it's set as {PROJECT_ID}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06571eb4063b"
      },
      "source": [
        "#### Timestamp\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append it onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "697568e92bd6"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "\n",
        "def get_timestamp():\n",
        "    return datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "\n",
        "\n",
        "TIMESTAMP = get_timestamp()\n",
        "print(f\"TIMESTAMP = {TIMESTAMP}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr--iN2kAylZ"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "---\n",
        "\n",
        "**If you are using Google Cloud Notebooks**, your environment is already authenticated. Skip this step.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBCra4QMA2wR"
      },
      "source": [
        "**If you are using Colab**, run the cell below and follow the instructions\n",
        "when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "1. In the Cloud Console, go to the [**Create service account key** page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
        "2. Click **Create service account**.\n",
        "3. In the **Service account name** field, enter a name, and click **Create**.\n",
        "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\" into the filter box, and select **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "5. Click *Create*. A JSON file that contains your key downloads to your local environment.\n",
        "6. Enter the path to your service account key as the `GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# If on Google Cloud Notebooks, then don't execute this code\n",
        "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxhCPW6e46EF"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "When you submit a training job using the Cloud SDK, you upload a Python package containing your training code to a Cloud Storage bucket. Vertex AI runs the code from this package. In this tutorial, Vertex AI also saves the trained model that results from your job in the same bucket. Using this model artifact, you can then create Vertex AI model and endpoint resources in order to serve online predictions.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. It must be unique across all Cloud Storage buckets.\n",
        "\n",
        "You may also change the `REGION` variable, which is used for operations throughout the rest of this notebook. Make sure to [choose a region where Vertex AI services are available](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions). You may not use a Multi-Regional Storage bucket for training with Vertex AI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZPew6MljTcP"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}\n",
        "REGION = \"us-central1\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf221059d072"
      },
      "outputs": [],
      "source": [
        "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
        "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EcIXiGsCePi"
      },
      "source": [
        "---\n",
        "\n",
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIq7R4HZCfIc"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucvCsknMCims"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhOb7YnwClBb"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3EQyqZiEMmf"
      },
      "source": [
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNV3Jd8BEMmj"
      },
      "source": [
        "Import python libraries required to run the pipeline and define constants.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99c88db5877c"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnz2aQ_EEMmk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import NamedTuple\n",
        "\n",
        "import google_cloud_pipeline_components\n",
        "import kfp\n",
        "from google.cloud import aiplatform\n",
        "from google.cloud.aiplatform import gapic as aip\n",
        "from google.cloud.aiplatform import pipeline_jobs\n",
        "from google.protobuf.json_format import MessageToDict\n",
        "from google_cloud_pipeline_components import aiplatform as aip_components\n",
        "from google_cloud_pipeline_components.experimental import custom_job\n",
        "from kfp.v2 import compiler, dsl\n",
        "from kfp.v2.dsl import Input, Metrics, Model, Output, component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4366d8966fc7"
      },
      "outputs": [],
      "source": [
        "APP_NAME = \"finetuned-bert-classifier\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwT_YZM6W5Pj"
      },
      "outputs": [],
      "source": [
        "PATH = %env PATH\n",
        "%env PATH={PATH}:/home/jupyter/.local/bin\n",
        "\n",
        "# Pipeline root is the GCS path to store the artifacts from the pipeline runs\n",
        "PIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline_root/{APP_NAME}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7da19337f842"
      },
      "outputs": [],
      "source": [
        "print(f\"Kubeflow Pipelines SDK version = {kfp.__version__}\")\n",
        "print(\n",
        "    f\"Google Cloud Pipeline Components version = {google_cloud_pipeline_components.__version__}\"\n",
        ")\n",
        "print(f\"Pipeline Root = {PIPELINE_ROOT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba9ab90f9590"
      },
      "source": [
        "## High Level Flow of Building a Pipeline\n",
        "\n",
        "Following is the high level flow to define and submit a pipeline on Vertex AI Pipelines:\n",
        "\n",
        "1. Define pipeline components involved in training and deploying a PyTorch model\n",
        "2. Define a pipeline by stitching the components in the workflow including pre-built [Google Cloud pipeline components](https://cloud.google.com/vertex-ai/docs/pipelines/components-introduction) and custom components\n",
        "3. Compile and submit the pipeline to Vertex AI Pipelines service to run the workflow\n",
        "4. Monitor the pipeline and analyze the metrics and artifacts generated\n",
        "\n",
        "![High level flow of pipeline](./images/pipelines-high-level-flow.png)\n",
        "\n",
        "This notebook builds on the training and serving code developed in previously this [notebook](../pytorch-text-classification-vertex-ai-train-tune-deploy.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9e6c2c62ac9"
      },
      "source": [
        "---\n",
        "\n",
        "### Concepts of a Pipeline\n",
        "\n",
        "Let's look at the terminology and concepts used in [Kubeflow Pipelines SDK v2](https://www.kubeflow.org/docs/components/pipelines/sdk-v2/).\n",
        "\n",
        "![concepts of a pipeline](./images/concepts-of-a-pipeline.png)\n",
        "\n",
        "- **Component:** A component is a self-contained set of code performing a single task in a ML workflow, for example, training a model. A component interface is composed of inputs, outputs and a container image that the component’s code runs in - including an executable code and environment definition.\n",
        "- **Pipeline:** A pipeline is composed of modular tasks defined as components that are chained together via inputs and outputs. Pipeline definition includes configuration such as parameters required to run the pipeline. Each component in a pipeline executes independently and the data (inputs and outputs) is passed between the components in a serialized format.\n",
        "- **Inputs & Outputs:** Component’s inputs and outputs must be annotated with data type, which makes input or output a parameter or an artifact. \n",
        "    - **Parameters:** Parameters are inputs or outputs to support simple data types such as `str`, `int`, `float`, `bool`, `dict`, `list`. Input parameters are always passed by value between the components and are stored in the [Vertex ML Metadata](https://cloud.google.com/vertex-ai/docs/ml-metadata/introduction) service.\n",
        "    - **Artifacts:** Artifacts are references to the objects or files produced by pipeline runs that are passed as inputs or outputs. Artifacts support rich or larger data types such as datasets, models, metrics, visualizations that are written as files or objects. Artifacts are defined by name, uri and metadata which is stored automatically in the Vertex ML Metadata service and the actual content of artifacts is referred to a path in Cloud Storage bucket. Input artifacts are always passed by reference.\n",
        "\n",
        "Learn more about KFP SDK v2 concepts [here](https://www.kubeflow.org/docs/components/pipelines/sdk-v2/).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99dbf92cbad3"
      },
      "source": [
        "### Pipeline schematic\n",
        "\n",
        "Following is the high level pipeline schematic with tasks involved in the pipeline for the PyTorch based text classification model including input and outputs:\n",
        "\n",
        "![Pipeline schematic for PyTorch text classification model](./images/pipeline-schematic-pytorch-text-classification.png)\n",
        "\n",
        "- **Build custom training image:** This step builds a custom training container image from the training application code and associated Dockerfile with the dependencies. The output from this step is the Container or Artifact registry URI of the custom training container.\n",
        "- **Run the custom training job to train and evaluate the model:** This step downloads and preprocesses training data from IMDB sentiment classification dataset on  HuggingFace, then trains and evaluates a model on the custom training container from the previous step. The step outputs Cloud Storage path to the trained model artifacts and the model performance metrics.\n",
        "- **Package model artifacts:** This step packages trained model artifacts including custom prediction handler to create a model archive (.mar) file using Torch Model Archiver tool. The output from this step is the location of model archive (.mar) file on GCS.\n",
        "- **Build custom serving image:** The step builds a custom serving container running TorchServe HTTP server to serve prediction requests for the models mounted. The output from this step is the Container or Artifact registry URI to the custom serving container.\n",
        "- **Upload model with custom serving container:** This step creates a model resource using the custom serving image and MAR file from the previous steps.\n",
        "- **Create an endpoint:** This step creates a Vertex AI Endpoint to provide a service URL where the prediction requests are sent.\n",
        "- **Deploy model to endpoint for serving:** This step deploys the model to the endpoint created that creates necessary compute resources (based on the machine spec configured) to serve online prediction requests.\n",
        "- **Validate deployment:** This step sends test requests to the endpoint and validates the deployment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzmFXOmeXLiT"
      },
      "source": [
        "## Define the Pipeline Components for PyTorch based ML Workflow\n",
        "\n",
        "The pipeline uses a mix of pre-built components from [Google Cloud Pipeline Components SDK](https://cloud.google.com/vertex-ai/docs/pipelines/components-introduction) to interact with Google Cloud services such as Vertex AI and define custom components for some steps in the pipeline. This section of the notebook defines custom components to perform the tasks in the pipeline using [KFP SDK v2 component spec](https://www.kubeflow.org/docs/components/pipelines/sdk-v2/component-development/). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0be5d976b66"
      },
      "source": [
        "**Create pipeline directory locally to save the component and pipeline specifications**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e456d56c58dc"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ./pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7b335530b41"
      },
      "source": [
        "### 1. Component: Build Custom Training Container Image\n",
        "\n",
        "This step builds a custom training container image using Cloud Build. The build job pulls the training application code and associated `Dockerfile` with the dependencies from GCS location and build/push the custom training container image to Container Registry. \n",
        "\n",
        "- **Inputs**: The inputs to this component are GCS path to the training application code and Dockerfile.\n",
        "- **Outputs**: The output from this step is the Container or Artifact registry URI of the custom training container. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "897960d83aba"
      },
      "source": [
        "**Create `Dockerfile` from PyTorch GPU image as base, install required dependencies and copy training application code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7d97b0bfcac"
      },
      "outputs": [],
      "source": [
        "%%writefile ./custom_container/Dockerfile\n",
        "\n",
        "# Use pytorch GPU base image\n",
        "# FROM gcr.io/cloud-aiplatform/training/pytorch-gpu.1-7\n",
        "FROM us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-10:latest\n",
        "\n",
        "# set working directory\n",
        "WORKDIR /app\n",
        "\n",
        "# Install required packages\n",
        "RUN pip install google-cloud-storage transformers datasets tqdm cloudml-hypertune\n",
        "\n",
        "# Copies the trainer code to the docker image.\n",
        "COPY ./trainer/__init__.py /app/trainer/__init__.py\n",
        "COPY ./trainer/experiment.py /app/trainer/experiment.py\n",
        "COPY ./trainer/utils.py /app/trainer/utils.py\n",
        "COPY ./trainer/metadata.py /app/trainer/metadata.py\n",
        "COPY ./trainer/model.py /app/trainer/model.py\n",
        "COPY ./trainer/task.py /app/trainer/task.py\n",
        "\n",
        "# Set up the entry point to invoke the trainer.\n",
        "ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c0f5c0ecf56"
      },
      "source": [
        "**Copy training application code and `Dockerfile` from local path to GCS location**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4f72a2298ba8"
      },
      "outputs": [],
      "source": [
        "# copy training Dockerfile\n",
        "!gsutil cp ./custom_container/Dockerfile {BUCKET_NAME}/{APP_NAME}/train/\n",
        "\n",
        "# copy training application code\n",
        "!gsutil cp -r ./python_package/trainer/ {BUCKET_NAME}/{APP_NAME}/train/\n",
        "\n",
        "# list copied files from GCS location\n",
        "!gsutil ls -Rl {BUCKET_NAME}/{APP_NAME}/train/\n",
        "\n",
        "print(\n",
        "    f\"Copied training application code and Dockerfile to {BUCKET_NAME}/{APP_NAME}/train/\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d95036b54b6"
      },
      "source": [
        "**Define custom pipeline component to build custom training container**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f08f04d404b"
      },
      "outputs": [],
      "source": [
        "@component(\n",
        "    base_image=\"gcr.io/google.com/cloudsdktool/cloud-sdk:latest\",\n",
        "    packages_to_install=[\"google-cloud-build\"],\n",
        "    output_component_file=\"./pipelines/build_custom_train_image.yaml\",\n",
        ")\n",
        "def build_custom_train_image(\n",
        "    project: str, gs_train_src_path: str, training_image_uri: str\n",
        ") -> NamedTuple(\"Outputs\", [(\"training_image_uri\", str)]):\n",
        "    \"\"\"custom pipeline component to build custom training image using\n",
        "    Cloud Build and the training application code and dependencies\n",
        "    defined in the Dockerfile\n",
        "    \"\"\"\n",
        "\n",
        "    import logging\n",
        "    import os\n",
        "\n",
        "    from google.cloud.devtools import cloudbuild_v1 as cloudbuild\n",
        "    from google.protobuf.duration_pb2 import Duration\n",
        "\n",
        "    # initialize client for cloud build\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "    build_client = cloudbuild.services.cloud_build.CloudBuildClient()\n",
        "\n",
        "    # parse step inputs to get path to Dockerfile and training application code\n",
        "    gs_dockerfile_path = os.path.join(gs_train_src_path, \"Dockerfile\")\n",
        "    gs_train_src_path = os.path.join(gs_train_src_path, \"trainer/\")\n",
        "\n",
        "    logging.info(f\"training_image_uri: {training_image_uri}\")\n",
        "\n",
        "    # define build steps to pull the training code and Dockerfile\n",
        "    # and build/push the custom training container image\n",
        "    build = cloudbuild.Build()\n",
        "    build.steps = [\n",
        "        {\n",
        "            \"name\": \"gcr.io/cloud-builders/gsutil\",\n",
        "            \"args\": [\"cp\", \"-r\", gs_train_src_path, \".\"],\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"gcr.io/cloud-builders/gsutil\",\n",
        "            \"args\": [\"cp\", gs_dockerfile_path, \"Dockerfile\"],\n",
        "        },\n",
        "        # enabling Kaniko cache in a Docker build that caches intermediate\n",
        "        # layers and pushes image automatically to Container Registry\n",
        "        # https://cloud.google.com/build/docs/kaniko-cache\n",
        "        {\n",
        "            \"name\": \"gcr.io/kaniko-project/executor:latest\",\n",
        "            \"args\": [f\"--destination={training_image_uri}\", \"--cache=true\"],\n",
        "        },\n",
        "    ]\n",
        "    # override default timeout of 10min\n",
        "    timeout = Duration()\n",
        "    timeout.seconds = 7200\n",
        "    build.timeout = timeout\n",
        "\n",
        "    # create build\n",
        "    operation = build_client.create_build(project_id=project, build=build)\n",
        "    logging.info(\"IN PROGRESS:\")\n",
        "    logging.info(operation.metadata)\n",
        "\n",
        "    # get build status\n",
        "    result = operation.result()\n",
        "    logging.info(\"RESULT:\", result.status)\n",
        "\n",
        "    # return step outputs\n",
        "    return (training_image_uri,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f33cfadfbb0"
      },
      "source": [
        "There are a few things to notice about the component specification:\n",
        "- The standalone function defined is converted as a pipeline component using the [`@kfp.v2.dsl.component`](https://github.com/kubeflow/pipelines/blob/master/sdk/python/kfp/v2/components/component_decorator.py) decorator.\n",
        "- All the arguments in the standalone function must have data type annotations because KFP uses the function’s inputs and outputs to define the component’s interface.\n",
        "- By default Python 3.7 is used as the base image to run the code defined. You can [configure the `@component` decorator](https://www.kubeflow.org/docs/components/pipelines/sdk-v2/python-function-components/#building-python-function-based-components) to override the default image by specifying `base_image`, install additional python packages using `packages_to_install` parameter and write the compiled component file as a YAML file using `output_component_file` to share or reuse the component.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39b9add646fe"
      },
      "source": [
        "### 2. Component: Get Custom Training Job Details from Vertex AI\n",
        "\n",
        "This step gets details from a custom training job from Vertex AI including training elapsed time, model performance metrics that will be used in the next step before the model deployment. The step additionally creates [Model](https://github.com/kubeflow/pipelines/blob/master/sdk/python/kfp/v2/components/types/artifact_types.py#L77) artifact with trained model artifacts. \n",
        "\n",
        "**NOTE:** The pre-built [custom job component](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-0.2.1/google_cloud_pipeline_components.experimental.custom_job.html) used in the pipeline outputs CustomJob resource but not the model artifacts.\n",
        "\n",
        "- **Inputs**:\n",
        "    - **`job_resource`:** Custom job resource returned by pre-built CustomJob component\n",
        "    - **`project`:** Project ID where the job ran\n",
        "    - **`region`:** Region where the job ran\n",
        "    - **`eval_metric_key`:** Evaluation metric key name such as eval_accuracy\n",
        "    - **`model_display_name`:** Model display name for saving model artifacts\n",
        "\n",
        "- **Outputs**: \n",
        "    - **`model`**: Trained model artifacts created by the training job with added model metadata\n",
        "    - **`metrics`**: Model performance metrics captured from the training job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b25a7b6a558c"
      },
      "outputs": [],
      "source": [
        "@component(\n",
        "    base_image=\"python:3.9\",\n",
        "    packages_to_install=[\n",
        "        \"google-cloud-pipeline-components\",\n",
        "        \"google-cloud-aiplatform\",\n",
        "        \"pandas\",\n",
        "        \"fsspec\",\n",
        "    ],\n",
        "    output_component_file=\"./pipelines/get_training_job_details.yaml\",\n",
        ")\n",
        "def get_training_job_details(\n",
        "    project: str,\n",
        "    location: str,\n",
        "    job_resource: str,\n",
        "    eval_metric_key: str,\n",
        "    model_display_name: str,\n",
        "    metrics: Output[Metrics],\n",
        "    model: Output[Model],\n",
        ") -> NamedTuple(\n",
        "    \"Outputs\", [(\"eval_metric\", float), (\"eval_loss\", float), (\"model_artifacts\", str)]\n",
        "):\n",
        "    \"\"\"custom pipeline component to get model artifacts and performance\n",
        "    metrics from custom training job\n",
        "    \"\"\"\n",
        "    import logging\n",
        "    import shutil\n",
        "    from collections import namedtuple\n",
        "\n",
        "    import pandas as pd\n",
        "    from google.cloud.aiplatform import gapic as aip\n",
        "    from google.protobuf.json_format import Parse\n",
        "    from google_cloud_pipeline_components.proto.gcp_resources_pb2 import \\\n",
        "        GcpResources\n",
        "\n",
        "    # parse training job resource\n",
        "    logging.info(f\"Custom job resource = {job_resource}\")\n",
        "    training_gcp_resources = Parse(job_resource, GcpResources())\n",
        "    custom_job_id = training_gcp_resources.resources[0].resource_uri\n",
        "    custom_job_name = \"/\".join(custom_job_id.split(\"/\")[-6:])\n",
        "    logging.info(f\"Custom job name parsed = {custom_job_name}\")\n",
        "\n",
        "    # get custom job information\n",
        "    API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(location)\n",
        "    client_options = {\"api_endpoint\": API_ENDPOINT}\n",
        "    job_client = aip.JobServiceClient(client_options=client_options)\n",
        "    job_resource = job_client.get_custom_job(name=custom_job_name)\n",
        "    job_base_dir = job_resource.job_spec.base_output_directory.output_uri_prefix\n",
        "    logging.info(f\"Custom job base output directory = {job_base_dir}\")\n",
        "\n",
        "    # copy model artifacts\n",
        "    logging.info(f\"Copying model artifacts to {model.path}\")\n",
        "    destination = shutil.copytree(job_base_dir.replace(\"gs://\", \"/gcs/\"), model.path)\n",
        "    logging.info(destination)\n",
        "    logging.info(f\"Model artifacts located at {model.uri}/model/{model_display_name}\")\n",
        "    logging.info(f\"Model artifacts located at model.uri = {model.uri}\")\n",
        "\n",
        "    # set model metadata\n",
        "    start, end = job_resource.start_time, job_resource.end_time\n",
        "    model.metadata[\"model_name\"] = model_display_name\n",
        "    model.metadata[\"framework\"] = \"pytorch\"\n",
        "    model.metadata[\"job_name\"] = custom_job_name\n",
        "    model.metadata[\"time_to_train_in_seconds\"] = (end - start).total_seconds()\n",
        "\n",
        "    # fetch metrics from the training job run\n",
        "    metrics_uri = f\"{model.path}/model/{model_display_name}/all_results.json\"\n",
        "    logging.info(f\"Reading and logging metrics from {metrics_uri}\")\n",
        "    metrics_df = pd.read_json(metrics_uri, typ=\"series\")\n",
        "    for k, v in metrics_df.items():\n",
        "        logging.info(f\"     {k} -> {v}\")\n",
        "        metrics.log_metric(k, v)\n",
        "\n",
        "    # capture eval metric and log to model metadata\n",
        "    eval_metric = (\n",
        "        metrics_df[eval_metric_key] if eval_metric_key in metrics_df.keys() else None\n",
        "    )\n",
        "    eval_loss = metrics_df[\"eval_loss\"] if \"eval_loss\" in metrics_df.keys() else None\n",
        "    logging.info(f\"     {eval_metric_key} -> {eval_metric}\")\n",
        "    logging.info(f'     \"eval_loss\" -> {eval_loss}')\n",
        "\n",
        "    model.metadata[eval_metric_key] = eval_metric\n",
        "    model.metadata[\"eval_loss\"] = eval_loss\n",
        "\n",
        "    # return output parameters\n",
        "    outputs = namedtuple(\"Outputs\", [\"eval_metric\", \"eval_loss\", \"model_artifacts\"])\n",
        "\n",
        "    return outputs(eval_metric, eval_loss, job_base_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "578b7d62c023"
      },
      "source": [
        "### 3. Component: Create Model Archive (MAR) file using Torch Model Archiver\n",
        "\n",
        "This step packages trained model artifacts and custom prediction handler (define in the earlier notebook) as a model archive (.mar) file usign [Torch Model Archiver](https://github.com/pytorch/serve/tree/master/model-archiver) tool.\n",
        "\n",
        "- **Inputs**:\n",
        "    - **`model_display_name`:** Model display name for saving model archive file\n",
        "    - **`model_version`:** Model version  for saving model archive file\n",
        "    - **`handler`:** Location of custom prediction handler\n",
        "    - **`model`:** Trained model artifacts from the previous step\n",
        "\n",
        "- **Outputs**: \n",
        "    - **`model_mar`**: Packaged model archive file (artifact) on GCS\n",
        "    - **`mar_env`**: A list of environment variables required for creating model resource\n",
        "    - **`mar_export_uri`**: GCS path to the model archive file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13d4215ddabd"
      },
      "source": [
        "**Copy custom prediction handler code from local path to GCS location**\n",
        "\n",
        "**NOTE**: Custom prediction handler is defined in the [previous notebook](./pytorch-text-classification-vertex-ai-train-tune-deploy.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9b6c854aa4dd"
      },
      "outputs": [],
      "source": [
        "# copy custom prediction handler\n",
        "!gsutil cp ./predictor/custom_handler.py ./predictor/index_to_name.json {BUCKET_NAME}/{APP_NAME}/serve/predictor/\n",
        "\n",
        "# list copied files from GCS location\n",
        "!gsutil ls -lR {BUCKET_NAME}/{APP_NAME}/serve/\n",
        "\n",
        "print(f\"Copied custom prediction handler code to {BUCKET_NAME}/{APP_NAME}/serve/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5a77290959a"
      },
      "source": [
        "**Define custom pipeline component to create model archive file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbc6e4a0f9a3"
      },
      "outputs": [],
      "source": [
        "@component(\n",
        "    base_image=\"python:3.9\",\n",
        "    packages_to_install=[\"torch-model-archiver\"],\n",
        "    output_component_file=\"./pipelines/generate_mar_file.yaml\",\n",
        ")\n",
        "def generate_mar_file(\n",
        "    model_display_name: str,\n",
        "    model_version: str,\n",
        "    handler: str,\n",
        "    model: Input[Model],\n",
        "    model_mar: Output[Model],\n",
        ") -> NamedTuple(\"Outputs\", [(\"mar_env_var\", list), (\"mar_export_uri\", str)]):\n",
        "    \"\"\"custom pipeline component to package model artifacts and custom\n",
        "    handler to a model archive file using Torch Model Archiver tool\n",
        "    \"\"\"\n",
        "    import logging\n",
        "    import os\n",
        "    import subprocess\n",
        "    import time\n",
        "    from collections import namedtuple\n",
        "    from pathlib import Path\n",
        "\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "    # create directory to save model archive file\n",
        "    model_output_root = model.path\n",
        "    mar_output_root = model_mar.path\n",
        "    export_path = f\"{mar_output_root}/model-store\"\n",
        "    try:\n",
        "        Path(export_path).mkdir(parents=True, exist_ok=True)\n",
        "    except Exception as e:\n",
        "        logging.warning(e)\n",
        "        # retry after pause\n",
        "        time.sleep(2)\n",
        "        Path(export_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # parse and configure paths for model archive config\n",
        "    handler_path = (\n",
        "        handler.replace(\"gs://\", \"/gcs/\") + \"predictor/custom_handler.py\"\n",
        "        if handler.startswith(\"gs://\")\n",
        "        else handler\n",
        "    )\n",
        "    model_artifacts_dir = f\"{model_output_root}/model/{model_display_name}\"\n",
        "    extra_files = [\n",
        "        os.path.join(model_artifacts_dir, f)\n",
        "        for f in os.listdir(model_artifacts_dir)\n",
        "        if f != \"pytorch_model.bin\"\n",
        "    ]\n",
        "\n",
        "    # define model archive config\n",
        "    mar_config = {\n",
        "        \"MODEL_NAME\": model_display_name,\n",
        "        \"HANDLER\": handler_path,\n",
        "        \"SERIALIZED_FILE\": f\"{model_artifacts_dir}/pytorch_model.bin\",\n",
        "        \"VERSION\": model_version,\n",
        "        \"EXTRA_FILES\": \",\".join(extra_files),\n",
        "        \"EXPORT_PATH\": f\"{model_mar.path}/model-store\",\n",
        "    }\n",
        "\n",
        "    # generate model archive command\n",
        "    archiver_cmd = (\n",
        "        \"torch-model-archiver --force \"\n",
        "        f\"--model-name {mar_config['MODEL_NAME']} \"\n",
        "        f\"--serialized-file {mar_config['SERIALIZED_FILE']} \"\n",
        "        f\"--handler {mar_config['HANDLER']} \"\n",
        "        f\"--version {mar_config['VERSION']}\"\n",
        "    )\n",
        "    if \"EXPORT_PATH\" in mar_config:\n",
        "        archiver_cmd += f\" --export-path {mar_config['EXPORT_PATH']}\"\n",
        "    if \"EXTRA_FILES\" in mar_config:\n",
        "        archiver_cmd += f\" --extra-files {mar_config['EXTRA_FILES']}\"\n",
        "    if \"REQUIREMENTS_FILE\" in mar_config:\n",
        "        archiver_cmd += f\" --requirements-file {mar_config['REQUIREMENTS_FILE']}\"\n",
        "\n",
        "    # run archiver command\n",
        "    logging.warning(\"Running archiver command: %s\", archiver_cmd)\n",
        "    with subprocess.Popen(\n",
        "        archiver_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n",
        "    ) as p:\n",
        "        _, err = p.communicate()\n",
        "        if err:\n",
        "            raise ValueError(err)\n",
        "\n",
        "    # set output variables\n",
        "    mar_env_var = [{\"name\": \"MODEL_NAME\", \"value\": model_display_name}]\n",
        "    mar_export_uri = f\"{model_mar.uri}/model-store/\"\n",
        "\n",
        "    outputs = namedtuple(\"Outputs\", [\"mar_env_var\", \"mar_export_uri\"])\n",
        "    return outputs(mar_env_var, mar_export_uri)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "895e91cb6487"
      },
      "source": [
        "### 4. Component: Create custom serving container running TorchServe\n",
        "\n",
        "The step builds a [custom serving container](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements) running [TorchServe](https://pytorch.org/serve/) HTTP server to serve prediction requests for the models mounted. The output from this step is the Container registry URI to the custom serving container.\n",
        "\n",
        "- **Inputs**:\n",
        "    - **`project`:**  Project ID to run \n",
        "    - **`serving_image_uri`:** Custom serving container URI from Container registry\n",
        "    - **`gs_serving_dependencies_path`:** Location of serving dependencies - Dockerfile\n",
        "- **Outputs**: \n",
        "    - **`serving_image_uri`**: Custom serving container URI from Container registry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4366fa221967"
      },
      "source": [
        "**Create `Dockerfile` from TorchServe CPU image as base, install required dependencies and run TorchServe serve command**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "be7320901415"
      },
      "outputs": [],
      "source": [
        "%%bash -s $APP_NAME\n",
        "\n",
        "APP_NAME=$1\n",
        "\n",
        "cat << EOF > ./predictor/Dockerfile.serve\n",
        "\n",
        "FROM pytorch/torchserve:latest-cpu\n",
        "\n",
        "USER root\n",
        "# run and update some basic packages software packages, including security libs\n",
        "RUN apt-get update && \\\n",
        "    apt-get install -y software-properties-common && \\\n",
        "    add-apt-repository -y ppa:ubuntu-toolchain-r/test && \\\n",
        "    apt-get update && \\\n",
        "    apt-get install -y gcc-9 g++-9 apt-transport-https ca-certificates gnupg curl\n",
        "\n",
        "# Install gcloud tools for gsutil as well as debugging\n",
        "RUN echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main\" | \\\n",
        "    tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && \\\n",
        "    curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | \\\n",
        "    apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - && \\\n",
        "    apt-get update -y && \\\n",
        "    apt-get install google-cloud-sdk -y\n",
        "\n",
        "USER model-server\n",
        "\n",
        "# install dependencies\n",
        "RUN python3 -m pip install --upgrade pip\n",
        "RUN pip3 install transformers\n",
        "\n",
        "ARG MODEL_NAME=$APP_NAME\n",
        "ENV MODEL_NAME=\"\\${MODEL_NAME}\"\n",
        "\n",
        "# health and prediction listener ports\n",
        "ARG AIP_HTTP_PORT=7080\n",
        "ENV AIP_HTTP_PORT=\"\\${AIP_HTTP_PORT}\"\n",
        "\n",
        "ARG MODEL_MGMT_PORT=7081\n",
        "\n",
        "# expose health and prediction listener ports from the image\n",
        "EXPOSE \"\\${AIP_HTTP_PORT}\"\n",
        "EXPOSE \"\\${MODEL_MGMT_PORT}\"\n",
        "EXPOSE 8080 8081 8082 7070 7071\n",
        "\n",
        "# create torchserve configuration file\n",
        "USER root\n",
        "RUN echo \"service_envelope=json\\n\" \\\n",
        "    \"inference_address=http://0.0.0.0:\\${AIP_HTTP_PORT}\\n\" \\\n",
        "    \"management_address=http://0.0.0.0:\\${MODEL_MGMT_PORT}\" >> \\\n",
        "    /home/model-server/config.properties\n",
        "USER model-server\n",
        "\n",
        "# run Torchserve HTTP serve to respond to prediction requests\n",
        "CMD [\"echo\", \"AIP_STORAGE_URI=\\${AIP_STORAGE_URI}\", \";\", \\\n",
        "    \"gsutil\", \"cp\", \"-r\", \"\\${AIP_STORAGE_URI}/\\${MODEL_NAME}.mar\", \"/home/model-server/model-store/\", \";\", \\\n",
        "    \"ls\", \"-ltr\", \"/home/model-server/model-store/\", \";\", \\\n",
        "    \"torchserve\", \"--start\", \"--ts-config=/home/model-server/config.properties\", \\\n",
        "    \"--models\", \"\\${MODEL_NAME}=\\${MODEL_NAME}.mar\", \\\n",
        "    \"--model-store\", \"/home/model-server/model-store\"]\n",
        "EOF\n",
        "\n",
        "echo \"Writing ./predictor/Dockerfile\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16c3bd57152c"
      },
      "source": [
        "**Copy serving `Dockerfile` from local path to GCS location**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bf8aced15cea"
      },
      "outputs": [],
      "source": [
        "# copy serving Dockerfile\n",
        "!gsutil cp ./predictor/Dockerfile.serve {BUCKET_NAME}/{APP_NAME}/serve/\n",
        "\n",
        "# list copied files from GCS location\n",
        "!gsutil ls -lR {BUCKET_NAME}/{APP_NAME}/serve/\n",
        "\n",
        "print(f\"Copied serving Dockerfile to {BUCKET_NAME}/{APP_NAME}/serve/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccfc5380998e"
      },
      "source": [
        "**Define custom pipeline component to build custom serving container**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c65003f4d9c"
      },
      "outputs": [],
      "source": [
        "@component(\n",
        "    base_image=\"python:3.9\",\n",
        "    packages_to_install=[\"google-cloud-build\"],\n",
        "    output_component_file=\"./pipelines/build_custom_serving_image.yaml\",\n",
        ")\n",
        "def build_custom_serving_image(\n",
        "    project: str, gs_serving_dependencies_path: str, serving_image_uri: str\n",
        ") -> NamedTuple(\"Outputs\", [(\"serving_image_uri\", str)],):\n",
        "    \"\"\"custom pipeline component to build custom serving image using\n",
        "    Cloud Build and dependencies defined in the Dockerfile\n",
        "    \"\"\"\n",
        "    import logging\n",
        "    import os\n",
        "\n",
        "    from google.cloud.devtools import cloudbuild_v1 as cloudbuild\n",
        "    from google.protobuf.duration_pb2 import Duration\n",
        "\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "    build_client = cloudbuild.services.cloud_build.CloudBuildClient()\n",
        "\n",
        "    logging.info(f\"gs_serving_dependencies_path: {gs_serving_dependencies_path}\")\n",
        "    gs_dockerfile_path = os.path.join(gs_serving_dependencies_path, \"Dockerfile.serve\")\n",
        "\n",
        "    logging.info(f\"serving_image_uri: {serving_image_uri}\")\n",
        "    build = cloudbuild.Build()\n",
        "    build.steps = [\n",
        "        {\n",
        "            \"name\": \"gcr.io/cloud-builders/gsutil\",\n",
        "            \"args\": [\"cp\", gs_dockerfile_path, \"Dockerfile\"],\n",
        "        },\n",
        "        # enabling Kaniko cache in a Docker build that caches intermediate\n",
        "        # layers and pushes image automatically to Container Registry\n",
        "        # https://cloud.google.com/build/docs/kaniko-cache\n",
        "        {\n",
        "            \"name\": \"gcr.io/kaniko-project/executor:latest\",\n",
        "            \"args\": [f\"--destination={serving_image_uri}\", \"--cache=true\"],\n",
        "        },\n",
        "    ]\n",
        "    # override default timeout of 10min\n",
        "    timeout = Duration()\n",
        "    timeout.seconds = 7200\n",
        "    build.timeout = timeout\n",
        "\n",
        "    # create build\n",
        "    operation = build_client.create_build(project_id=project, build=build)\n",
        "    logging.info(\"IN PROGRESS:\")\n",
        "    logging.info(operation.metadata)\n",
        "\n",
        "    # get build status\n",
        "    result = operation.result()\n",
        "    logging.info(\"RESULT:\", result.status)\n",
        "\n",
        "    # return step outputs\n",
        "    return (serving_image_uri,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d26c6e423061"
      },
      "source": [
        "### 5. Component: Test model deployment making online prediction requests\n",
        "\n",
        "This step sends test requests to the Vertex AI Endpoint and validates the deployment by sending test prediction requests. Deployment is considered successful when the response from model server returns text sentiment.\n",
        "\n",
        "- **Inputs**:\n",
        "    - **`project`:**  Project ID to run \n",
        "    - **`bucket`:** Staging GCS bucket path\n",
        "    - **`endpoint`:** Location of Vertex AI Endpoint from the Endpoint creation task\n",
        "    - **`instances`:** List of test prediction requests\n",
        "- **Outputs**: \n",
        "    - None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebc57f1a6567"
      },
      "outputs": [],
      "source": [
        "@component(\n",
        "    base_image=\"python:3.9\",\n",
        "    packages_to_install=[\"google-cloud-aiplatform\", \"google-cloud-pipeline-components\"],\n",
        "    output_component_file=\"./pipelines/make_prediction_request.yaml\",\n",
        ")\n",
        "def make_prediction_request(project: str, bucket: str, endpoint: str, instances: list):\n",
        "    \"\"\"custom pipeline component to pass prediction requests to Vertex AI\n",
        "    endpoint and get responses\n",
        "    \"\"\"\n",
        "    import base64\n",
        "    import logging\n",
        "\n",
        "    from google.cloud import aiplatform\n",
        "    from google.protobuf.json_format import Parse\n",
        "    from google_cloud_pipeline_components.proto.gcp_resources_pb2 import \\\n",
        "        GcpResources\n",
        "\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "    aiplatform.init(project=project, staging_bucket=bucket)\n",
        "\n",
        "    # parse endpoint resource\n",
        "    logging.info(f\"Endpoint = {endpoint}\")\n",
        "    gcp_resources = Parse(endpoint, GcpResources())\n",
        "    endpoint_uri = gcp_resources.resources[0].resource_uri\n",
        "    endpoint_id = \"/\".join(endpoint_uri.split(\"/\")[-8:-2])\n",
        "    logging.info(f\"Endpoint ID = {endpoint_id}\")\n",
        "\n",
        "    # define endpoint client\n",
        "    _endpoint = aiplatform.Endpoint(endpoint_id)\n",
        "\n",
        "    # call prediction endpoint for each instance\n",
        "    for instance in instances:\n",
        "        if not isinstance(instance, (bytes, bytearray)):\n",
        "            instance = instance.encode()\n",
        "        logging.info(f\"Input text: {instance.decode('utf-8')}\")\n",
        "        b64_encoded = base64.b64encode(instance)\n",
        "        test_instance = [{\"data\": {\"b64\": f\"{str(b64_encoded.decode('utf-8'))}\"}}]\n",
        "        response = _endpoint.predict(instances=test_instance)\n",
        "        logging.info(f\"Prediction response: {response.predictions}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1df6e7be96e"
      },
      "source": [
        "## Define Pipeline Specification\n",
        "\n",
        "The pipeline definition describes how input and output parameters and artifacts are passed between the steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "811b8073ea11"
      },
      "source": [
        "**Set environment variables**\n",
        "\n",
        "These environment variables will be used to define resource specifications such as training jobs, model resource etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1294bfa799cf"
      },
      "outputs": [],
      "source": [
        "os.environ[\"PROJECT_ID\"] = PROJECT_ID\n",
        "os.environ[\"BUCKET\"] = BUCKET_NAME\n",
        "os.environ[\"REGION\"] = REGION\n",
        "os.environ[\"APP_NAME\"] = APP_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdf5b473744f"
      },
      "source": [
        "**Create pipeline configuration file**\n",
        "\n",
        "Pipeline configuration files helps in templatizing a pipeline enabling to run the same pipeline with different parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cf0b34d5edb"
      },
      "outputs": [],
      "source": [
        "%%writefile ./pipelines/pipeline_config.py\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "PROJECT_ID = os.getenv(\"PROJECT_ID\", \"\")\n",
        "BUCKET = os.getenv(\"BUCKET\", \"\")\n",
        "REGION = os.getenv(\"REGION\", \"us-central1\")\n",
        "\n",
        "APP_NAME = os.getenv(\"APP_NAME\", \"finetuned-bert-classifier\")\n",
        "VERSION = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "MODEL_NAME = APP_NAME\n",
        "MODEL_DISPLAY_NAME = f\"{MODEL_NAME}-{VERSION}\"\n",
        "\n",
        "PIPELINE_NAME = f\"pytorch-{APP_NAME}\"\n",
        "PIPELINE_ROOT = f\"{BUCKET}/pipeline_root/{MODEL_NAME}\"\n",
        "GCS_STAGING = f\"{BUCKET}/pipeline_root/{MODEL_NAME}\"\n",
        "\n",
        "TRAIN_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/pytorch_gpu_train_{MODEL_NAME}\"\n",
        "SERVE_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/pytorch_cpu_predict_{MODEL_NAME}\"\n",
        "\n",
        "MACHINE_TYPE = \"n1-standard-8\"\n",
        "REPLICA_COUNT = \"1\"\n",
        "ACCELERATOR_TYPE = \"NVIDIA_TESLA_T4\"\n",
        "ACCELERATOR_COUNT = \"1\"\n",
        "NUM_WORKERS = 1\n",
        "\n",
        "SERVING_HEALTH_ROUTE = \"/ping\"\n",
        "SERVING_PREDICT_ROUTE = f\"/predictions/{MODEL_NAME}\"\n",
        "SERVING_CONTAINER_PORT= [{\"containerPort\": 7080}]\n",
        "SERVING_MACHINE_TYPE = \"n1-standard-4\"\n",
        "SERVING_MIN_REPLICA_COUNT = 1\n",
        "SERVING_MAX_REPLICA_COUNT=1\n",
        "SERVING_TRAFFIC_SPLIT='{\"0\": 100}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12e5eeeb25c4"
      },
      "source": [
        "**Define pipeline specification**\n",
        "\n",
        "The pipeline is defined as a standalone Python function annotated with the [`@kfp.dsl.pipeline`](https://github.com/kubeflow/pipelines/blob/master/sdk/python/kfp/v2/components/pipeline_context.py) decorator, specifying the pipeline's name and the root path where the pipeline's artifacts are stored.\n",
        "\n",
        "The pipeline definition consists of both pre-built and custom defined components:\n",
        "- Pre-built components from [Google Cloud Pipeline Components SDK](https://cloud.google.com/vertex-ai/docs/pipelines/components-introduction) are defined for tasks calling Vertex AI services such as submitting custom training job (`custom_job.CustomTrainingJobOp`), uploading a model (`ModelUploadOp`), creating an endpoint (`EndpointCreateOp`) and deploying a model to the endpoint (`ModelDeployOp`)\n",
        "- Custom components are defined for tasks to build custom containers for training (`build_custom_train_image`), get training job details (`get_training_job_details`), create mar file (`generate_mar_file`) and serving (`build_custom_serving_image`) and validating the model deployment task (`ake_prediction_request`). Refer to the notebook for custom component specification for these tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f7acf482906"
      },
      "outputs": [],
      "source": [
        "from pipelines import pipeline_config as cfg\n",
        "\n",
        "\n",
        "@dsl.pipeline(\n",
        "    name=cfg.PIPELINE_NAME,\n",
        "    pipeline_root=cfg.PIPELINE_ROOT,\n",
        ")\n",
        "def pytorch_text_classifier_pipeline(\n",
        "    pipeline_job_id: str,\n",
        "    gs_train_script_path: str,\n",
        "    gs_serving_dependencies_path: str,\n",
        "    eval_acc_threshold: float,\n",
        "    is_hp_tuning_enabled: str = \"n\",\n",
        "):\n",
        "    # ========================================================================\n",
        "    # build custom training container image\n",
        "    # ========================================================================\n",
        "    # build custom container for training job passing the\n",
        "    # GCS location of the training application code\n",
        "    build_custom_train_image_task = (\n",
        "        build_custom_train_image(\n",
        "            project=cfg.PROJECT_ID,\n",
        "            gs_train_src_path=gs_train_script_path,\n",
        "            training_image_uri=cfg.TRAIN_IMAGE_URI,\n",
        "        )\n",
        "        .set_caching_options(True)\n",
        "        .set_display_name(\"Build custom training image\")\n",
        "    )\n",
        "\n",
        "    # ========================================================================\n",
        "    # model training\n",
        "    # ========================================================================\n",
        "    # train the model on Vertex AI by submitting a CustomJob\n",
        "    # using the custom container (no hyper-parameter tuning)\n",
        "    # define training code arguments\n",
        "    training_args = [\"--num-epochs\", \"2\", \"--model-name\", cfg.MODEL_NAME]\n",
        "    # define job name\n",
        "    JOB_NAME = f\"{cfg.MODEL_NAME}-train-pytorch-cstm-cntr-{TIMESTAMP}\"\n",
        "    GCS_BASE_OUTPUT_DIR = f\"{cfg.GCS_STAGING}/{TIMESTAMP}\"\n",
        "    # define worker pool specs\n",
        "    worker_pool_specs = [\n",
        "        {\n",
        "            \"machine_spec\": {\n",
        "                \"machine_type\": cfg.MACHINE_TYPE,\n",
        "                \"accelerator_type\": cfg.ACCELERATOR_TYPE,\n",
        "                \"accelerator_count\": cfg.ACCELERATOR_COUNT,\n",
        "            },\n",
        "            \"replica_count\": cfg.REPLICA_COUNT,\n",
        "            \"container_spec\": {\"image_uri\": cfg.TRAIN_IMAGE_URI, \"args\": training_args},\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    run_train_task = (\n",
        "        custom_job.CustomTrainingJobOp(\n",
        "            project=cfg.PROJECT_ID,\n",
        "            location=cfg.REGION,\n",
        "            display_name=JOB_NAME,\n",
        "            base_output_directory=GCS_BASE_OUTPUT_DIR,\n",
        "            worker_pool_specs=worker_pool_specs,\n",
        "        )\n",
        "        .set_display_name(\"Run custom training job\")\n",
        "        .after(build_custom_train_image_task)\n",
        "    )\n",
        "\n",
        "    # ========================================================================\n",
        "    # get training job details\n",
        "    # ========================================================================\n",
        "    training_job_details_task = get_training_job_details(\n",
        "        project=cfg.PROJECT_ID,\n",
        "        location=cfg.REGION,\n",
        "        job_resource=run_train_task.output,\n",
        "        eval_metric_key=\"eval_accuracy\",\n",
        "        model_display_name=cfg.MODEL_NAME,\n",
        "    ).set_display_name(\"Get custom training job details\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # model deployment when condition is met\n",
        "    # ========================================================================\n",
        "    with dsl.Condition(\n",
        "        training_job_details_task.outputs[\"eval_metric\"] > eval_acc_threshold,\n",
        "        name=\"model-deploy-decision\",\n",
        "    ):\n",
        "        # ===================================================================\n",
        "        # create model archive file\n",
        "        # ===================================================================\n",
        "        create_mar_task = generate_mar_file(\n",
        "            model_display_name=cfg.MODEL_NAME,\n",
        "            model_version=cfg.VERSION,\n",
        "            handler=gs_serving_dependencies_path,\n",
        "            model=training_job_details_task.outputs[\"model\"],\n",
        "        ).set_display_name(\"Create MAR file\")\n",
        "\n",
        "        # ===================================================================\n",
        "        # build custom serving container running TorchServe\n",
        "        # ===================================================================\n",
        "        # build custom container for serving predictions using\n",
        "        # the trained model artifacts served by TorchServe\n",
        "        build_custom_serving_image_task = build_custom_serving_image(\n",
        "            project=cfg.PROJECT_ID,\n",
        "            gs_serving_dependencies_path=gs_serving_dependencies_path,\n",
        "            serving_image_uri=cfg.SERVE_IMAGE_URI,\n",
        "        ).set_display_name(\"Build custom serving image\")\n",
        "\n",
        "        # ===================================================================\n",
        "        # create model resource\n",
        "        # ===================================================================\n",
        "        # upload model to vertex ai\n",
        "        model_upload_task = (\n",
        "            aip_components.ModelUploadOp(\n",
        "                project=cfg.PROJECT_ID,\n",
        "                display_name=cfg.MODEL_DISPLAY_NAME,\n",
        "                serving_container_image_uri=cfg.SERVE_IMAGE_URI,\n",
        "                serving_container_predict_route=cfg.SERVING_PREDICT_ROUTE,\n",
        "                serving_container_health_route=cfg.SERVING_HEALTH_ROUTE,\n",
        "                serving_container_ports=cfg.SERVING_CONTAINER_PORT,\n",
        "                serving_container_environment_variables=create_mar_task.outputs[\n",
        "                    \"mar_env_var\"\n",
        "                ],\n",
        "                artifact_uri=create_mar_task.outputs[\"mar_export_uri\"],\n",
        "            )\n",
        "            .set_display_name(\"Upload model\")\n",
        "            .after(build_custom_serving_image_task)\n",
        "        )\n",
        "\n",
        "        # ===================================================================\n",
        "        # create Vertex AI Endpoint\n",
        "        # ===================================================================\n",
        "        # create endpoint to deploy one or more models\n",
        "        # An endpoint provides a service URL where the prediction requests are sent\n",
        "        endpoint_create_task = (\n",
        "            aip_components.EndpointCreateOp(\n",
        "                project=cfg.PROJECT_ID,\n",
        "                display_name=cfg.MODEL_NAME + \"-endpoint\",\n",
        "            )\n",
        "            .set_display_name(\"Create endpoint\")\n",
        "            .after(create_mar_task)\n",
        "        )\n",
        "\n",
        "        # ===================================================================\n",
        "        # deploy model to Vertex AI Endpoint\n",
        "        # ===================================================================\n",
        "        # deploy models to endpoint to associates physical resources with the model\n",
        "        # so it can serve online predictions\n",
        "        model_deploy_task = aip_components.ModelDeployOp(\n",
        "            endpoint=endpoint_create_task.outputs[\"endpoint\"],\n",
        "            model=model_upload_task.outputs[\"model\"],\n",
        "            deployed_model_display_name=cfg.MODEL_NAME,\n",
        "            dedicated_resources_machine_type=cfg.SERVING_MACHINE_TYPE,\n",
        "            dedicated_resources_min_replica_count=cfg.SERVING_MIN_REPLICA_COUNT,\n",
        "            dedicated_resources_max_replica_count=cfg.SERVING_MAX_REPLICA_COUNT,\n",
        "            traffic_split=cfg.SERVING_TRAFFIC_SPLIT,\n",
        "        ).set_display_name(\"Deploy model to endpoint\")\n",
        "\n",
        "        # ===================================================================\n",
        "        # test model deployment\n",
        "        # ===================================================================\n",
        "        # test model deployment by making online prediction requests\n",
        "        test_instances = [\n",
        "            \"Jaw dropping visual affects and action! One of the best I have seen to date.\",\n",
        "            \"Take away the CGI and the A-list cast and you end up with film with less punch.\",\n",
        "        ]\n",
        "        predict_test_instances_task = make_prediction_request(\n",
        "            project=cfg.PROJECT_ID,\n",
        "            bucket=cfg.BUCKET,\n",
        "            endpoint=model_deploy_task.outputs[\"gcp_resources\"],\n",
        "            instances=test_instances,\n",
        "        ).set_display_name(\"Test model deployment making online predictions\")\n",
        "        predict_test_instances_task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c59a095145de"
      },
      "source": [
        "Let’s unpack this code and understand a few things:\n",
        "\n",
        "- A component’s inputs can be set from the pipeline's inputs (passed as arguments) or they can depend on the output of other components within this pipeline. For example, `ModelUploadOp` depends on custom serving container image URI from `build_custom_serving_image` task along with the pipeline’s inputs such as project id.\n",
        "- `kfp.dsl.Condition` is a control structure with a group of tasks which runs only when the condition is met. In this pipeline, model deployment steps run only when the trained model performance exceeds the set threshold. If not, those steps are skipped.\n",
        "- Each component in the pipeline runs within its own container image. You can specify machine type for each pipeline step such as CPU, GPU and memory limits. By default, each component runs as a Vertex AI CustomJob using an e2-standard-4 machine.\n",
        "- By default, pipeline execution caching is enabled. Vertex AI Pipelines service checks to see whether an execution of each pipeline step exists in Vertex ML metadata. It uses a combination of pipeline name, step’s inputs, output and component specification. When a matching execution already exists, the step is skipped and thereby reducing costs. Execution caching can be turned off at task level or at pipeline level."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1ea4c27bdfd"
      },
      "source": [
        "Following is the runtime graph generated for this pipeline\n",
        "\n",
        "![pytorch-pipeline-runtime-graph](./images/pytorch-pipeline-runtime-graph.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "623c008ba04a"
      },
      "source": [
        "To learn more about building pipelines, refer to the [building Kubeflow pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline#build-pipeline) section, and follow the [pipelines samples and tutorials](https://cloud.google.com/vertex-ai/docs/pipelines/notebooks#general-tutorials)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d1696c659da"
      },
      "source": [
        "## Submit Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95909f224f23"
      },
      "source": [
        "#### Compile Pipeline Specification as JSON\n",
        "\n",
        "After defining the pipeline, it must be compiled for [executing on Vertex AI Pipeline services](https://cloud.google.com/vertex-ai/docs/pipelines/run-pipeline). When the pipeline is compiled, the KFP SDK analyzes the data dependencies between the components to create a directed acyclic graph. The compiled pipeline is in JSON format with all information required to run the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xREwNd4b2Oif"
      },
      "outputs": [],
      "source": [
        "PIPELINE_JSON_SPEC_PATH = \"./pipelines/pytorch_text_classifier_pipeline_spec.json\"\n",
        "compiler.Compiler().compile(\n",
        "    pipeline_func=pytorch_text_classifier_pipeline, package_path=PIPELINE_JSON_SPEC_PATH\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32197916df4a"
      },
      "source": [
        "#### Submit Pipeline for Execution on Vertex AI Pipelines\n",
        "\n",
        "Pipeline is submitted to Vertex AI Pipelines by defining a PipelineJob using Vertex AI SDK for Python client, passing necessary pipeline inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9jd9MW_2gyx"
      },
      "outputs": [],
      "source": [
        "# initialize Vertex AI SDK\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cac1bf7e4b1"
      },
      "outputs": [],
      "source": [
        "# define pipeline parameters\n",
        "# NOTE: These parameters can be included in the pipeline config file as needed\n",
        "\n",
        "PIPELINE_JOB_ID = f\"pipeline-{APP_NAME}-{get_timestamp()}\"\n",
        "TRAIN_APP_CODE_PATH = f\"{BUCKET_NAME}/{APP_NAME}/train/\"\n",
        "SERVE_DEPENDENCIES_PATH = f\"{BUCKET_NAME}/{APP_NAME}/serve/\"\n",
        "\n",
        "pipeline_params = {\n",
        "    \"pipeline_job_id\": PIPELINE_JOB_ID,\n",
        "    \"gs_train_script_path\": TRAIN_APP_CODE_PATH,\n",
        "    \"gs_serving_dependencies_path\": SERVE_DEPENDENCIES_PATH,\n",
        "    \"eval_acc_threshold\": 0.87,\n",
        "    \"is_hp_tuning_enabled\": \"n\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdd8d61222c7"
      },
      "outputs": [],
      "source": [
        "# define pipeline job\n",
        "pipeline_job = pipeline_jobs.PipelineJob(\n",
        "    display_name=cfg.PIPELINE_NAME,\n",
        "    job_id=PIPELINE_JOB_ID,\n",
        "    template_path=PIPELINE_JSON_SPEC_PATH,\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        "    parameter_values=pipeline_params,\n",
        "    enable_caching=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "628cc5ede02b"
      },
      "source": [
        "**When the pipeline is submitted, the logs show a link to view the pipeline run on Google Cloud Console or access the run by opening [Pipelines dashboard on Vertex AI](https://console.cloud.google.com/vertex-ai/pipelines)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8a240ea6d0f"
      },
      "outputs": [],
      "source": [
        "# submit pipeline job for execution\n",
        "response = pipeline_job.run(sync=True)\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbb0672d8148"
      },
      "source": [
        "## Monitoring the Pipeline\n",
        "\n",
        "You can monitor the progress of a pipeline execution by navigating to the [Vertex AI Pipelines dashboard](https://console.cloud.google.com/vertex-ai/pipelines).\n",
        "\n",
        "\n",
        "```\n",
        "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
        "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/<project-id>/locations/<region>/pipelineJobs/pipeline-finetuned-bert-classifier-20220119061941\n",
        "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
        "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/<project-id>/locations/<region>/pipelineJobs/pipeline-finetuned-bert-classifier-20220119061941')\n",
        "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
        "https://console.cloud.google.com/vertex-ai/locations/region/pipelines/runs/pipeline-finetuned-bert-classifier-20220119061941?project=<project-id>\n",
        "```\n",
        "\n",
        "#### Component Execution Logs\n",
        "\n",
        "Since every step in the pipeline runs in its own container or as a remote job (such as Dataflow, Dataproc job), you can view the step logs by clicking on \"View Logs\" button on a step.\n",
        "\n",
        "![pipeline-step-logs](./images/pipeline-step-logs.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9056bd9baad3"
      },
      "source": [
        "#### Artifacts and Lineage\n",
        "\n",
        "In the pipeline graph, you can notice the small boxes after each step. Those are artifacts generated from the step. For example, \"Create MAR file\" step generates MAR file as an artifact. Click on the artifact to know more details about it.\n",
        "\n",
        "![pipeline-artifact-and-lineage](./images/pipeline-artifact-and-lineage.png)\n",
        "\n",
        "You can track lineage of an artifact describing its relationship with the steps in the pipeline. Vertex AI Pipelines automatically tracks the metadata and lineage. This lineage aids in establishing model governance and reproducibility. Click on \"View Lineage\" button on an artifact and it shows you the lineage graph as below.\n",
        "\n",
        "![artifact-lineage](./images/artifact-lineage.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08940bfabf7a"
      },
      "source": [
        "#### Comparing Pipeline runs with the Vertex AI SDK\n",
        "\n",
        "When running pipeline executions for different experiments, you may want to compare the metrics across the pipeline runs. You can [compare pipeline runs](https://cloud.google.com/vertex-ai/docs/pipelines/visualize-pipeline#compare_pipeline_runs_using) from the Vertex AI Pipelines dashboard.\n",
        "\n",
        "Alternatively, you can use `aiplatform.get_pipeline_df()` method from Vertex AI SDK for Python that fetches pipeline execution metadata for a pipeline and returns a Pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f0f1a10000d"
      },
      "outputs": [],
      "source": [
        "# underscores are not supported in the pipeline name, so\n",
        "# replace underscores with hyphen\n",
        "df_pipeline = aiplatform.get_pipeline_df(pipeline=cfg.PIPELINE_NAME.replace(\"_\", \"-\"))\n",
        "df_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c826ea88075d"
      },
      "source": [
        "## Cleaning up \n",
        "\n",
        "### Cleaning up training and deployment resources\n",
        "\n",
        "To clean up all Google Cloud resources used in this notebook, you can [delete the Google Cloud project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:\n",
        "\n",
        "- Training Jobs\n",
        "- Model\n",
        "- Endpoint\n",
        "- Cloud Storage Bucket\n",
        "- Container Images\n",
        "- Pipeline runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "337013b4d0fc"
      },
      "source": [
        "Set flags for the resource type to be deleted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bf72f5a24f3"
      },
      "outputs": [],
      "source": [
        "delete_custom_job = False\n",
        "delete_hp_tuning_job = False\n",
        "delete_endpoint = False\n",
        "delete_model = False\n",
        "delete_bucket = False\n",
        "delete_image = False\n",
        "delete_pipeline_job = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72e3ba307c50"
      },
      "source": [
        "Define clients for jobs, models and endpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a893ef2722c3"
      },
      "outputs": [],
      "source": [
        "# API Endpoint\n",
        "API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
        "\n",
        "# Vertex AI location root path for your dataset, model and endpoint resources\n",
        "PARENT = f\"projects/{PROJECT_ID}/locations/{REGION}\"\n",
        "\n",
        "client_options = {\"api_endpoint\": API_ENDPOINT}\n",
        "\n",
        "# Initialize Vertex SDK\n",
        "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "978a2ee4ca2c"
      },
      "outputs": [],
      "source": [
        "# functions to create client\n",
        "def create_job_client():\n",
        "    client = aip.JobServiceClient(client_options=client_options)\n",
        "    return client\n",
        "\n",
        "\n",
        "def create_model_client():\n",
        "    client = aip.ModelServiceClient(client_options=client_options)\n",
        "    return client\n",
        "\n",
        "\n",
        "def create_endpoint_client():\n",
        "    client = aip.EndpointServiceClient(client_options=client_options)\n",
        "    return client\n",
        "\n",
        "\n",
        "def create_pipeline_client():\n",
        "    client = aip.PipelineServiceClient(client_options=client_options)\n",
        "    return client\n",
        "\n",
        "\n",
        "clients = {}\n",
        "clients[\"job\"] = create_job_client()\n",
        "clients[\"model\"] = create_model_client()\n",
        "clients[\"endpoint\"] = create_endpoint_client()\n",
        "clients[\"pipeline\"] = create_pipeline_client()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "594be7bb9d13"
      },
      "source": [
        "Define functions to list the jobs, models and endpoints starting with APP_NAME defined earlier in the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c446439412d1"
      },
      "outputs": [],
      "source": [
        "def list_custom_jobs():\n",
        "    client = clients[\"job\"]\n",
        "    jobs = []\n",
        "    response = client.list_custom_jobs(parent=PARENT)\n",
        "    for row in response:\n",
        "        _row = MessageToDict(row._pb)\n",
        "        if _row[\"displayName\"].startswith(APP_NAME):\n",
        "            jobs.append((_row[\"name\"], _row[\"displayName\"]))\n",
        "    return jobs\n",
        "\n",
        "\n",
        "def list_hp_tuning_jobs():\n",
        "    client = clients[\"job\"]\n",
        "    jobs = []\n",
        "    response = client.list_hyperparameter_tuning_jobs(parent=PARENT)\n",
        "    for row in response:\n",
        "        _row = MessageToDict(row._pb)\n",
        "        if _row[\"displayName\"].startswith(APP_NAME):\n",
        "            jobs.append((_row[\"name\"], _row[\"displayName\"]))\n",
        "    return jobs\n",
        "\n",
        "\n",
        "def list_models():\n",
        "    client = clients[\"model\"]\n",
        "    models = []\n",
        "    response = client.list_models(parent=PARENT)\n",
        "    for row in response:\n",
        "        _row = MessageToDict(row._pb)\n",
        "        if _row[\"displayName\"].startswith(APP_NAME):\n",
        "            models.append((_row[\"name\"], _row[\"displayName\"]))\n",
        "    return models\n",
        "\n",
        "\n",
        "def list_endpoints():\n",
        "    client = clients[\"endpoint\"]\n",
        "    endpoints = []\n",
        "    response = client.list_endpoints(parent=PARENT)\n",
        "    for row in response:\n",
        "        _row = MessageToDict(row._pb)\n",
        "        if _row[\"displayName\"].startswith(APP_NAME):\n",
        "            endpoints.append((_row[\"name\"], _row[\"displayName\"]))\n",
        "    return endpoints\n",
        "\n",
        "\n",
        "def list_pipelines():\n",
        "    client = clients[\"pipeline\"]\n",
        "    pipelines = []\n",
        "    request = aip.ListPipelineJobsRequest(\n",
        "        parent=PARENT, filter=f'display_name=\"{cfg.PIPELINE_NAME}\"', order_by=\"end_time\"\n",
        "    )\n",
        "    response = client.list_pipeline_jobs(request=request)\n",
        "\n",
        "    for row in response:\n",
        "        _row = MessageToDict(row._pb)\n",
        "        pipelines.append(_row[\"name\"])\n",
        "    return pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f972114ef358"
      },
      "source": [
        "### Deleting custom training jobs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba6f1cbb8b31"
      },
      "outputs": [],
      "source": [
        "# Delete the custom training using the Vertex AI fully qualified identifier for the custom training\n",
        "try:\n",
        "    if delete_custom_job:\n",
        "        custom_jobs = list_custom_jobs()\n",
        "        for job_id, job_name in custom_jobs:\n",
        "            print(f\"Deleting job {job_id} [{job_name}]\")\n",
        "            clients[\"job\"].delete_custom_job(name=job_id)\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36cf0021c2ef"
      },
      "source": [
        "### Deleting hyperparameter tuning jobs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15dc2c9ffa58"
      },
      "outputs": [],
      "source": [
        "# Delete the hyperparameter tuning jobs using the Vertex AI fully qualified identifier for the hyperparameter tuning job\n",
        "try:\n",
        "    if delete_hp_tuning_job:\n",
        "        hp_tuning_jobs = list_hp_tuning_jobs()\n",
        "        for job_id, job_name in hp_tuning_jobs:\n",
        "            print(f\"Deleting job {job_id} [{job_name}]\")\n",
        "            clients[\"job\"].delete_hyperparameter_tuning_job(name=job_id)\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b278a6c1046"
      },
      "source": [
        "### Undeploy models and Delete endpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec2a6315ee9a"
      },
      "outputs": [],
      "source": [
        "# Delete the endpoint using the Vertex AI fully qualified identifier for the endpoint\n",
        "try:\n",
        "    if delete_endpoint:\n",
        "        endpoints = list_endpoints()\n",
        "        for endpoint_id, endpoint_name in endpoints:\n",
        "            endpoint = aiplatform.Endpoint(endpoint_id)\n",
        "            # undeploy models from the endpoint\n",
        "            print(f\"Undeploying all deployed models from the endpoint {endpoint_name}\")\n",
        "            endpoint.undeploy_all(sync=True)\n",
        "            # deleting endpoint\n",
        "            print(f\"Deleting endpoint {endpoint_id} [{endpoint_name}]\")\n",
        "            clients[\"endpoint\"].delete_endpoint(name=endpoint_id)\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "969ec07baf38"
      },
      "source": [
        "### Deleting models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bd8988fda0bc"
      },
      "outputs": [],
      "source": [
        "# Delete the model using the Vertex AI fully qualified identifier for the model\n",
        "try:\n",
        "    if delete_model:\n",
        "        models = list_models()\n",
        "        for model_id, model_name in models:\n",
        "            print(f\"Deleting model {model_id} [{model_name}]\")\n",
        "            clients[\"model\"].delete_model(name=model_id)\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a349bd90f31"
      },
      "source": [
        "### Deleting pipeline runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1800e905194b"
      },
      "outputs": [],
      "source": [
        "# Delete the pipeline execution using the Vertex AI fully qualified identifier for the pipeline job\n",
        "try:\n",
        "    if delete_pipeline_job:\n",
        "        pipelines = list_pipelines()\n",
        "        for pipeline_name in pipelines[:1]:\n",
        "            print(f\"Deleting pipeline run {pipeline_name}\")\n",
        "            if delete_custom_job:\n",
        "                print(\"\\t Deleting underlying custom jobs\")\n",
        "                pipeline_job = clients[\"pipeline\"].get_pipeline_job(name=pipeline_name)\n",
        "                pipeline_job = MessageToDict(pipeline_job._pb)\n",
        "                task_details = pipeline_job[\"jobDetail\"][\"taskDetails\"]\n",
        "                for task in tasks:\n",
        "                    if \"containerDetail\" in task[\"executorDetail\"]:\n",
        "                        custom_job_id = task[\"executorDetail\"][\"containerDetail\"][\n",
        "                            \"mainJob\"\n",
        "                        ]\n",
        "                        print(\n",
        "                            f\"\\t Deleting custom job {custom_job_id} for task {task['taskName']}\"\n",
        "                        )\n",
        "                        clients[\"job\"].delete_custom_job(name=custom_job_id)\n",
        "            clients[\"pipeline\"].delete_pipeline_job(name=pipeline_name)\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8e1b65a7d96"
      },
      "source": [
        "### Delete contents from the staging bucket\n",
        "\n",
        "---\n",
        "\n",
        "***NOTE: Everything in this Cloud Storage bucket will be DELETED. Please run it with caution.***\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "226f3979c4a6"
      },
      "outputs": [],
      "source": [
        "if delete_bucket and \"BUCKET_NAME\" in globals():\n",
        "    print(f\"Deleting all contents from the bucket {BUCKET_NAME}\")\n",
        "\n",
        "    shell_output = ! gsutil du -as $BUCKET_NAME\n",
        "    print(\n",
        "        f\"Size of the bucket {BUCKET_NAME} before deleting = {shell_output[0].split()[0]} bytes\"\n",
        "    )\n",
        "\n",
        "    # uncomment below line to delete contents of the bucket\n",
        "    # ! gsutil rm -r $BUCKET_NAME\n",
        "\n",
        "    shell_output = ! gsutil du -as $BUCKET_NAME\n",
        "    if float(shell_output[0].split()[0]) > 0:\n",
        "        print(\n",
        "            \"PLEASE UNCOMMENT LINE TO DELETE BUCKET. CONTENT FROM THE BUCKET NOT DELETED\"\n",
        "        )\n",
        "\n",
        "    print(\n",
        "        f\"Size of the bucket {BUCKET_NAME} after deleting = {shell_output[0].split()[0]} bytes\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e272daf3fd2"
      },
      "source": [
        "### Delete images from Container Registry\n",
        "\n",
        "Deletes all the container images created in this tutorial with prefix defined by variable APP_NAME from the registry. All associated tags are also deleted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c4c32373348"
      },
      "outputs": [],
      "source": [
        "gcr_images = !gcloud container images list --repository=gcr.io/$PROJECT_ID --filter=\"name~\"$APP_NAME\n",
        "\n",
        "if delete_image:\n",
        "    for image in gcr_images:\n",
        "        if image != \"NAME\":  # skip header line\n",
        "            print(f\"Deleting image {image} including all tags\")\n",
        "            !gcloud container images delete $image --force-delete-tags --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "876123f5d0e6"
      },
      "source": [
        "### Cleaning up Notebook Environment\n",
        "\n",
        "After you are done experimenting, you can either STOP or DELETE the AI Notebook instance to prevent any charges. If you want to save your work, you can choose to stop the instance instead.\n",
        "\n",
        "```\n",
        "# Stopping Notebook instance\n",
        "gcloud notebooks instances stop example-instance --location=us-central1-a\n",
        "\n",
        "\n",
        "# Deleting Notebook instance\n",
        "gcloud notebooks instances delete example-instance --location=us-central1-a\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "pytorch-text-classification-vertex-ai-pipelines.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

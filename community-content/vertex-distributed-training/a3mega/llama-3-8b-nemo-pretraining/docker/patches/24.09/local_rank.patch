diff -ruN old-datasets/blended_megatron_dataset_builder.py datasets/blended_megatron_dataset_builder.py
--- old-datasets/blended_megatron_dataset_builder.py	2025-05-02 04:08:45.369199665 +0000
+++ datasets/blended_megatron_dataset_builder.py	2025-05-02 04:10:47.369119891 +0000
@@ -2,6 +2,7 @@

 import logging
 import math
+import os
 from concurrent.futures import ThreadPoolExecutor
 from typing import Any, Callable, Iterable, List, Optional, Type, Union

@@ -353,7 +354,7 @@
         num_dataset_builder_threads = self.config.num_dataset_builder_threads

         if torch.distributed.is_initialized():
-            rank = torch.distributed.get_rank()
+            rank = int(os.getenv("LOCAL_RANK", "0"))
             # First, build on rank 0
             if rank == 0:
                 num_workers = num_dataset_builder_threads
@@ -475,7 +476,7 @@
             Optional[Union[DistributedDataset, Iterable]]: The DistributedDataset instantion, the Iterable instantiation, or None
         """
         if torch.distributed.is_initialized():
-            rank = torch.distributed.get_rank()
+            rank = int(os.getenv("LOCAL_RANK", "0"))

             dataset = None

diff -ruN old-datasets/gpt_dataset.py datasets/gpt_dataset.py
--- old-datasets/gpt_dataset.py	2025-05-02 04:08:45.369199665 +0000
+++ datasets/gpt_dataset.py	2025-05-02 04:09:30.309170278 +0000
@@ -351,7 +351,7 @@

         if not path_to_cache or (
             not cache_hit
-            and (not torch.distributed.is_initialized() or torch.distributed.get_rank() == 0)
+            and (not torch.distributed.is_initialized() or int(os.getenv("LOCAL_RANK", "0")) == 0)
         ):

             log_single_rank(

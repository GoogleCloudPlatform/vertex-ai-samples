{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "copyright"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# Vertex AI SDK for Python: AutoML training hierarchical forecasting for batch prediction\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/automl/sdk_automl_forecasting_hierarchical_batch.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/automl/sdk_automl_forecasting_hierarchical_batch.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "<a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/automl/sdk_automl_forecasting_hierarchical_batch.ipynb\" target='_blank'>\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>                                                                                               \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview:automl"
      },
      "source": [
        "## Overview\n",
        "\n",
        "\n",
        "This tutorial demonstrates how to use the Vertex AI SDK for Python to create hierarchical forecasting models using a Google Cloud [AutoML](https://cloud.google.com/vertex-ai/docs/start/automl-users)and do batch prediction. Specifically, you predict a fictional store's sales based on historical sales data.\n",
        "\n",
        "Learn more about [Hierarchical forecasting for tabular data](https://cloud.google.com/vertex-ai/docs/tabular-data/forecasting/hierarchical)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "objective:automl,training,online_prediction"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you create an AutoML hierarchical forecasting model and deploy it for batch prediction using the Vertex AI SDK for Python. You can alternatively create and deploy models using the `gcloud` command-line tool or batch using the Cloud Console.\n",
        "The rationale for a hierarchical forecasting model is to minimize the error for a given group of sales data. In this tutorial, you will be minimizing the error for sale predictions at the \"product\" level.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services:\n",
        "\n",
        "- `AutoML Training`\n",
        "- `Vertex AI Datasets`\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Create a Vertex AI `TimeSeriesDataset` resource.\n",
        "- Train the model.\n",
        "- View the model evaluation.\n",
        "- Make a batch prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset:gsod,lrg"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset used for this tutorial is a synthetically generated dataset of sales data for a fictional outdoor gear store. In this dataset, you predict a fictional store's sales based on historical sales data.\n",
        "\n",
        "This dataset is synthesized to mimic sales pattern for a fictional outdoor gear store. There is a hierarchy between product_category, product_type and product as shown below:\n",
        "\n",
        "- product_category: snow\n",
        "    - product_type: skis\n",
        "        - product: \n",
        "        \n",
        "Additionally, this company has 3 store locations, each with their respective level of foot traffic.\n",
        "- store: suburbs\n",
        "- store: flagship\n",
        "- store: downtown\n",
        "\n",
        "Additional season effects are present in the data.\n",
        "\n",
        "Link to data: gs://cloud-samples-data/vertex-ai/structured_data/forecasting/synthetic_sales_data.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "costs"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_aip"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Install the latest version of Cloud Storage, Bigquery and Vertex AI SDKs for Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fd00fa70a2a"
      },
      "outputs": [],
      "source": [
        "# Install the packages\n",
        "! pip3 install --upgrade google-cloud-aiplatform \\\n",
        "                        google-cloud-storage \\\n",
        "                        google-cloud-bigquery[pandas] \\\n",
        "                        seaborn \\\n",
        "                        scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blGlVGFYW9Pt"
      },
      "source": [
        "### Colab only: Uncomment the following cell to restart the kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JrvuK6LUYnQ"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "# import IPython\n",
        "\n",
        "# app = IPython.Application.instance()\n",
        "# app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a47846030fef"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "project_id"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, try the following:\n",
        "* Run `gcloud config list`.\n",
        "* Run `gcloud projects list`.\n",
        "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c8049930470"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a54f9d7c1876"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aaadaaf9b30"
      },
      "outputs": [],
      "source": [
        "REGION = \"us-central1\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c0404984792"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaFKzJ_xXpvm"
      },
      "source": [
        "**1. Vertex AI Workbench**\n",
        "* Do nothing as you are already authenticated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fV-KyGAX4Xl"
      },
      "source": [
        "**2. Local JupyterLab instance, uncomment and run:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uXB1HAPX6L_"
      },
      "outputs": [],
      "source": [
        "# ! gcloud auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ab_TRMQIYCCX"
      },
      "source": [
        "**3. Colab, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vx25htmYYExI"
      },
      "outputs": [],
      "source": [
        "# from google.colab import auth\n",
        "# auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZdA0-jBYGqt"
      },
      "source": [
        "**4. Service account or other**\n",
        "* See how to grant Cloud Storage permissions to your service account at https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bucket:custom"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "Create a storage bucket to store intermediate artifacts such as datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bucket"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = \"gs://your-bucket-name-unique\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_bucket"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oz8J0vmSlugt"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "750d53e37094"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9d3ac73dfbc"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "# Initialize the Vertex AI SDK\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_vars"
      },
      "source": [
        "### Set up variables\n",
        "\n",
        "Next, set up some variables used throughout the tutorial.\n",
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "import google.cloud.aiplatform as aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk"
      },
      "source": [
        "## Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project and the corresponding bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "aiplatform.init(project=PROJECT_ID, location=REGION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tutorial_start:automl"
      },
      "source": [
        "# Tutorial\n",
        "\n",
        "Now you are ready to start creating your own AutoML time-series forecasting model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_dataset:tabular,bq,lrg"
      },
      "source": [
        "### Create the Dataset\n",
        "\n",
        "Use `TimeSeriesDataset.create()` to create a `TimeSeriesDataset` resource, which takes the following parameters:\n",
        "\n",
        "- `display_name`: The human readable name for the `Dataset` resource.\n",
        "- `gcs_source`: A list of one or more dataset index files to import the data items into the `Dataset` resource.\n",
        "- `bq_source`: Alternatively, import data items from a BigQuery table into the `Dataset` resource.\n",
        "\n",
        "This operation may take several minutes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0234d188ed3"
      },
      "source": [
        "### Download data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e310c5f598f"
      },
      "outputs": [],
      "source": [
        "DATASET_URI = \"gs://cloud-samples-data/vertex-ai/structured_data/forecasting/synthetic_sales_data.csv\"\n",
        "\n",
        "# Download the dataset\n",
        "! gsutil cp {DATASET_URI} dataset.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee795ca660f9"
      },
      "source": [
        "#### Split\n",
        "\n",
        "In this use case, you are predicting the sales volume per product per store. \n",
        "Hence, you will need to create a new column called 'product_at_store', which is a concatenation of the 'product' and 'store' columns. This will be passed as the 'target_column' during training.\n",
        "\n",
        "Lastly, split the dataset into a train and test dataset.\n",
        "The train dataset is saved to CSV but the test dataset needs further treatment such as removing the target column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "219ff473f02c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "DATASET_TRAIN_FILENAME = \"sales_forecasting_train.csv\"\n",
        "DATASET_TEST_FILENAME = \"sales_forecasting_test.csv\"\n",
        "DATASET_TRAIN_URI = f\"{BUCKET_URI}/{DATASET_TRAIN_FILENAME}\"\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "\n",
        "df[\"date\"] = df[\"date\"].astype(\"datetime64[ns]\")\n",
        "\n",
        "# Add a target column\n",
        "df[\"product_at_store\"] = df[\"product\"] + \" (\" + df[\"store\"] + \")\"\n",
        "\n",
        "# Split dataset into train and test by taking the first 90% of data for training.\n",
        "dates_unique = df[\"date\"].unique()\n",
        "date_cutoff = sorted(dates_unique)[round(len(dates_unique) * 9 / 10)]\n",
        "\n",
        "# Save train dataset\n",
        "df[df[\"date\"] < date_cutoff].to_csv(DATASET_TRAIN_FILENAME, index=False)\n",
        "\n",
        "# Create test dataset\n",
        "df_test = df[df[\"date\"] >= date_cutoff]\n",
        "\n",
        "# Upload to GCS bucket\n",
        "! gsutil cp {DATASET_TRAIN_FILENAME} {DATASET_TRAIN_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f031cc0274a"
      },
      "source": [
        "#### Plot the dataset\n",
        "\n",
        "Plot the 'sales' vs 'product_at_store' to get a sense of the dataset.\n",
        "\n",
        "Note the peak in 'Snow' products during winter months and a peak in 'Water' products in the summer months."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "343a931623aa"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "sns.relplot(\n",
        "    data=df,\n",
        "    x=\"date\",\n",
        "    y=\"sales\",\n",
        "    hue=\"product_at_store\",\n",
        "    row=\"product_category\",\n",
        "    aspect=4,\n",
        "    kind=\"line\",\n",
        "    style=\"store\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_dataset:tabular,bq,lrg"
      },
      "outputs": [],
      "source": [
        "dataset_time_series = aiplatform.TimeSeriesDataset.create(gcs_source=DATASET_TRAIN_URI)\n",
        "\n",
        "print(dataset_time_series.resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_automl_pipeline:tabular,lrg,transformations"
      },
      "source": [
        "### Create and run training pipeline\n",
        "\n",
        "To train an AutoML model, create and run a training pipeline.\n",
        "\n",
        "#### Create training job\n",
        "\n",
        "Create an AutoML training pipeline using the `AutoMLForecastingTrainingJob` class, with the following parameters:\n",
        "\n",
        "- `display_name`: The human readable name for the `TrainingJob` resource.\n",
        "- `column_transformations`: (Optional): Transformations to apply to the input columns\n",
        "- `optimization_objective`: The optimization objective (minimize or maximize).\n",
        "  - regression:\n",
        "    - `minimize-rmse`\n",
        "    - `minimize-mae`\n",
        "    - `minimize-rmsle`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_automl_pipeline:tabular,lrg,transformations"
      },
      "outputs": [],
      "source": [
        "training_job = aiplatform.AutoMLForecastingTrainingJob(\n",
        "    display_name=\"hierachical_sales_forecasting\",\n",
        "    optimization_objective=\"minimize-rmse\",\n",
        "    column_specs={\n",
        "        \"date\": \"timestamp\",\n",
        "        \"sales\": \"numeric\",\n",
        "        \"product_type\": \"categorical\",\n",
        "        \"product_category\": \"categorical\",\n",
        "        \"product\": \"categorical\",\n",
        "        \"store\": \"categorical\",\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42ce6f1f9633"
      },
      "source": [
        "### Set context and horizon\n",
        "\n",
        "You need to the context window and forecast horizon when you train a forecasting model.\n",
        "- The context window sets how far back the model looks during training (and for forecasts). In other words, for each training datapoint, the context window determines how far back the model looks for predictive patterns.\n",
        "- The forecast horizon determines how far into the future the model forecasts the target value for each row of prediction data.\n",
        "\n",
        "See more here: [Considerations for setting the context window and forecast horizon](https://cloud.google.com/vertex-ai/docs/datasets/bp-tabular?hl=en#context-window)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8315876232af"
      },
      "outputs": [],
      "source": [
        "# Each row represents a day, so we set context and time horizon to 30 to represent 30 days.\n",
        "\n",
        "CONTEXT_WINDOW = 30\n",
        "TIME_HORIZON = 30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_automl_pipeline:tabular"
      },
      "source": [
        "#### Run the training pipeline\n",
        "\n",
        "Run the training job by invoking the `run` method with the following parameters:\n",
        "\n",
        "- `dataset`: The `Dataset` resource to train the model.\n",
        "- `model_display_name`: The human readable name for the trained model.\n",
        "- `training_fraction_split`: The percentage of the dataset to use for training.\n",
        "- `test_fraction_split`: The percentage of the dataset to use for test (holdout data).\n",
        "- `validation_fraction_split`: The percentage of the dataset to use for validation.\n",
        "- `target_column`: The name of the column to train as the label.\n",
        "- `budget_milli_node_hours`: (optional) Maximum training time specified in unit of millihours (1000 = hour).\n",
        "\n",
        "The `run` method when completed returns the `Model` resource.\n",
        "\n",
        "#### Setting the hierarchical parameters\n",
        "We want to group by 'product' to minimize the error at this level.\n",
        "Hence, you should set the group parameter to \"product\".\n",
        "\n",
        "Setting the `group_total_weight` to a non-zero weight means that you want to weigh the group aggregated loss relative to the individual loss. Set that to 10 for demonstration purposes.\n",
        "\n",
        "See more info at https://cloud.google.com/vertex-ai/docs/tabular-data/forecasting/hierarchical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_automl_pipeline:tabular"
      },
      "outputs": [],
      "source": [
        "time_column = \"date\"\n",
        "time_series_identifier_column = \"product_at_store\"\n",
        "target_column = \"sales\"\n",
        "\n",
        "model = training_job.run(\n",
        "    dataset=dataset_time_series,\n",
        "    target_column=target_column,\n",
        "    time_column=time_column,\n",
        "    time_series_identifier_column=time_series_identifier_column,\n",
        "    available_at_forecast_columns=[time_column],\n",
        "    unavailable_at_forecast_columns=[target_column],\n",
        "    time_series_attribute_columns=[\n",
        "        \"product_type\",\n",
        "        \"product_category\",\n",
        "        \"store\",\n",
        "        \"product\",\n",
        "    ],\n",
        "    forecast_horizon=TIME_HORIZON,\n",
        "    data_granularity_unit=\"day\",\n",
        "    data_granularity_count=1,\n",
        "    model_display_name=\"hierarchical_sales_forecasting_model\",\n",
        "    weight_column=None,\n",
        "    hierarchy_group_columns=[\"product\"],\n",
        "    hierarchy_group_total_weight=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluate_the_model:mbsdk"
      },
      "source": [
        "## Review model evaluation scores\n",
        "After your model has finished training, you can review its evaluation scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluate_the_model:mbsdk"
      },
      "outputs": [],
      "source": [
        "# Get evaluations\n",
        "model_evaluations = model.list_model_evaluations()\n",
        "\n",
        "model_evaluation = list(model_evaluations)[0]\n",
        "print(model_evaluation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "337625a4c33c"
      },
      "source": [
        "## Send a batch prediction request\n",
        "\n",
        "Now you can make a batch prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bff79eb475fa"
      },
      "source": [
        "### Prepare the test dataset\n",
        "\n",
        "For forecasting, the test dataset needs to have context window rows which have information on the target column and subsequent time horizon rows where the target column is unknown. Construct these two sections and combine them into a single CSV file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cec4a813a6c4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Store start and end dates for context and horizon\n",
        "date_context_window_start = date_cutoff\n",
        "date_context_window_end = date_cutoff + np.timedelta64(CONTEXT_WINDOW, \"D\")\n",
        "time_horizon_end = date_context_window_end + np.timedelta64(TIME_HORIZON, \"D\")\n",
        "\n",
        "# Extract dataframes for context and horizon\n",
        "df_test_context = df_test[\n",
        "    (df_test[\"date\"] >= date_context_window_start)\n",
        "    & (df_test[\"date\"] < date_context_window_end)\n",
        "]\n",
        "df_test_horizon = df_test[\n",
        "    (df_test[\"date\"] >= date_context_window_end) & (df_test[\"date\"] < time_horizon_end)\n",
        "].copy()\n",
        "\n",
        "# Save a copy for validation of predictions\n",
        "df_test_horizon_actual = df_test_horizon.copy()\n",
        "\n",
        "# Remove sales for horizon (i.e. future dates)\n",
        "df_test_horizon[\"sales\"] = \"\"\n",
        "\n",
        "# Write test data to CSV\n",
        "df_test = pd.concat([df_test_context, df_test_horizon])\n",
        "df_test.to_csv(DATASET_TEST_FILENAME, index=False)\n",
        "\n",
        "# Save test dataset\n",
        "DATASET_TEST_URI = f\"{BUCKET_URI}/{DATASET_TEST_FILENAME}\"\n",
        "\n",
        "# Upload to GCS bucket\n",
        "! gsutil cp {DATASET_TEST_FILENAME} {DATASET_TEST_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69651b1d06da"
      },
      "source": [
        "### Examine the context dataframe\n",
        "\n",
        "Note that the sales column is filled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f002e35f78cb"
      },
      "outputs": [],
      "source": [
        "df_test_context.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd04bb099daf"
      },
      "source": [
        "### Examine the time horizon dataframe\n",
        "\n",
        "Note that the sales column is empty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da09515ce3dd"
      },
      "outputs": [],
      "source": [
        "df_test_horizon.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93bc53cc960d"
      },
      "source": [
        "### Create a results dataset\n",
        "\n",
        "Create a BigQuery dataset to store the prediction results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6a0730cafb2"
      },
      "outputs": [],
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "# Create client in default region\n",
        "bigquery_client = bigquery.Client(\n",
        "    project=PROJECT_ID,\n",
        "    credentials=aiplatform.initializer.global_config.credentials,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9439d436c826"
      },
      "outputs": [],
      "source": [
        "def create_bigquery_dataset(name: str, region: str):\n",
        "    batch_predict_bq_output_uri_prefix = \"bq://{}.{}\".format(PROJECT_ID, name)\n",
        "\n",
        "    bq_dataset = bigquery.Dataset(\"{}.{}\".format(PROJECT_ID, name))\n",
        "\n",
        "    dataset_region = region\n",
        "    bq_dataset.location = dataset_region\n",
        "    bq_dataset = bigquery_client.create_dataset(bq_dataset)\n",
        "    print(\n",
        "        \"Created bigquery dataset {} in {}\".format(\n",
        "            batch_predict_bq_output_uri_prefix, dataset_region\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return batch_predict_bq_output_uri_prefix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb1fb03d6eae"
      },
      "outputs": [],
      "source": [
        "batch_predict_bq_output_uri_prefix = create_bigquery_dataset(\n",
        "    name=\"hierarchical_forecasting_unique\", region=REGION\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36c070503d2f"
      },
      "source": [
        "### Make the batch prediction request\n",
        "\n",
        "You can make a batch prediction by invoking the batch_predict() method, with the following parameters:\n",
        "\n",
        "- `job_display_name`: The human readable name for the batch prediction job.\n",
        "- `gcs_source`: A list of one or more batch request input files.\n",
        "- `bigquery_destination_prefix`: The BigQuery destiantion location for storing the batch prediction results.\n",
        "- `instances_format`: The format for the input instances, either 'bigquery', 'csv' or 'jsonl'. Defaults to 'jsonl'.\n",
        "- `predictions_format`: The format for the output predictions, either 'csv', 'jsonl' or 'bigquery'. Defaults to 'jsonl'.\n",
        "- `machine_type`: The type of machine to use for training.\n",
        "- `accelerator_type`: The hardware accelerator type.\n",
        "- `accelerator_count`: The number of accelerators to attach to a worker replica.\n",
        "- `sync`: If set to True, the call will block while waiting for the asynchronous batch job to complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbd40d78ea46"
      },
      "outputs": [],
      "source": [
        "batch_prediction_job = model.batch_predict(\n",
        "    job_display_name=\"hierarchical_forecasting_unique\",\n",
        "    gcs_source=DATASET_TEST_URI,\n",
        "    instances_format=\"csv\",\n",
        "    bigquery_destination_prefix=batch_predict_bq_output_uri_prefix,\n",
        "    predictions_format=\"bigquery\",\n",
        "    generate_explanation=True,\n",
        "    sync=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fc95aae00b0"
      },
      "source": [
        "###  View the batch prediction results\n",
        "\n",
        "Use the BigQuery Python client to query the destination table and return results as a Pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fafe1b0f654b"
      },
      "outputs": [],
      "source": [
        "# View the results as a dataframe\n",
        "df_output = batch_prediction_job.iter_outputs(bq_max_results=1000).to_dataframe()\n",
        "\n",
        "# Convert the dates to the datetime64 datatype\n",
        "df_output[\"date\"] = df_output[\"date\"].astype(\"datetime64[ns]\")\n",
        "\n",
        "# Extract the predicted sales and convert to floats\n",
        "df_output[\"predicted_sales\"] = (\n",
        "    df_output[\"predicted_sales\"].apply(lambda x: x[\"value\"]).astype(float)\n",
        ")\n",
        "\n",
        "df_output.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "305edda2bff8"
      },
      "source": [
        "### Compare predictions vs ground truth\n",
        "\n",
        "Plot the predicted sales vs the ground truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98679eff6973"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a shared dataframe to plot predictions vs ground truth\n",
        "df_output[\"sales_comparison\"] = df_output[\"predicted_sales\"]\n",
        "df_output[\"is_ground_truth\"] = False\n",
        "df_test_horizon_actual[\"sales_comparison\"] = df_test_horizon_actual[\"sales\"]\n",
        "df_test_horizon_actual[\"is_ground_truth\"] = True\n",
        "df_prediction_comparison = pd.concat([df_output, df_test_horizon_actual])\n",
        "\n",
        "# Plot sales\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(24, 12)\n",
        "\n",
        "sns.relplot(\n",
        "    data=df_prediction_comparison,\n",
        "    x=\"date\",\n",
        "    y=\"sales_comparison\",\n",
        "    hue=\"product_at_store\",\n",
        "    style=\"store\",\n",
        "    row=\"is_ground_truth\",\n",
        "    height=5,\n",
        "    aspect=4,\n",
        "    kind=\"line\",\n",
        "    ci=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cleanup:mbsdk"
      },
      "source": [
        "# Clean up\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:\n",
        "\n",
        "- Model\n",
        "- AutoML Training Job\n",
        "- Batch Job\n",
        "- Cloud Storage Bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec0171e4b4e8"
      },
      "outputs": [],
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "# Create client in default region\n",
        "bq_client = bigquery.Client(\n",
        "    project=PROJECT_ID,\n",
        "    credentials=aiplatform.initializer.global_config.credentials,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3be2c2bf9146"
      },
      "outputs": [],
      "source": [
        "# Delete BigQuery datasets\n",
        "bq_client.delete_dataset(\n",
        "    f\"{PROJECT_ID}.hierarchical_forecasting_unique\",\n",
        "    delete_contents=True,\n",
        "    not_found_ok=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup:mbsdk"
      },
      "outputs": [],
      "source": [
        "# Delete Vertex AI resources\n",
        "dataset_time_series.delete()\n",
        "model.delete()\n",
        "training_job.delete()\n",
        "batch_prediction_job.delete()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "sdk_automl_forecasting_hierarchical_batch.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "copyright"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# Vertex AI SDK for Python: Vertex AI AutoML training hierarchical forecasting for batch prediction\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/automl/sdk_automl_forecasting_hierarchical_batch.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Fautoml%2Fsdk_automl_forecasting_hierarchical_batch.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/automl/sdk_automl_forecasting_hierarchical_batch.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br>\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "<a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/automl/sdk_automl_forecasting_hierarchical_batch.ipynb\" target='_blank'>\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br>\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>                                                                                               \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview:automl"
      },
      "source": [
        "## Overview\n",
        "\n",
        "\n",
        "This tutorial demonstrates how to use the Vertex AI SDK for Python to create hierarchical forecasting models using  Google Cloud Vertex AI and do batch prediction. Specifically, you predict a fictional store's sales based on historical sales data.\n",
        "\n",
        "Learn more about [Hierarchical forecasting for tabular data](https://cloud.google.com/vertex-ai/docs/tabular-data/forecasting/hierarchical)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "objective:automl,training,online_prediction"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you create an AutoML hierarchical forecasting model and deploy it for batch prediction using the Vertex AI SDK for Python. You can alternatively create and deploy models using the `gcloud` command-line tool or batch using the Cloud Console.\n",
        "The rationale for a hierarchical forecasting model is to minimize the error for a given group of sales data. The objective of this tutorial is to minimize the error for sale predictions at the \"product\" level.\n",
        "\n",
        "This tutorial uses the following Google Cloud Vertex AI services:\n",
        "\n",
        "- AutoML training\n",
        "- Vertex AI datasets\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Create a Vertex AI TimeSeriesDataset resource.\n",
        "- Train the model.\n",
        "- View the model evaluation.\n",
        "- Make a batch prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset:gsod,lrg"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset used for this tutorial is a synthetically generated dataset of sales data for a fictional outdoor gear store. In this dataset, you predict a fictional store's sales based on historical sales data.\n",
        "\n",
        "This dataset is synthesized to mimic sales pattern for a fictional outdoor gear store. There is a hierarchy between product_category, product_type and product as shown below:\n",
        "\n",
        "- product_category: snow\n",
        "    - product_type: skis\n",
        "        - product: \n",
        "        \n",
        "Additionally, this company has 3 store locations, each with their respective level of foot traffic.\n",
        "- store: suburbs\n",
        "- store: flagship\n",
        "- store: downtown\n",
        "\n",
        "Additional season effects are present in the data.\n",
        "\n",
        "Link to data: gs://cloud-samples-data/vertex-ai/structured_data/forecasting/synthetic_sales_data.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "costs"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0316df526f8"
      },
      "source": [
        "## Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2c2cb2109a0"
      },
      "source": [
        "### Install Vertex AI SDK for Python and other required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6319584dc783"
      },
      "outputs": [],
      "source": [
        "! pip3 install --upgrade google-cloud-aiplatform google-cloud-storage google-cloud-bigquery[pandas] seaborn scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff555b32bab8"
      },
      "source": [
        "### Restart runtime (Colab only)\n",
        "\n",
        "To use the newly installed packages, you must restart the runtime on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f09b4dff629a"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee775571c2b5"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92e68cfc3a90"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "Authenticate your environment on Google Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46604f70e831"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f872cd812d0"
      },
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI SDK for Python\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "294fe4e5a671"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bucket:custom"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "Create a storage bucket to store intermediate artifacts such as datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bucket"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_bucket"
      },
      "source": [
        "**If your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oz8J0vmSlugt"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $LOCATION -p $PROJECT_ID $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "750d53e37094"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9d3ac73dfbc"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "# Initialize the Vertex AI SDK\n",
        "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tutorial_start:automl"
      },
      "source": [
        "# Tutorial\n",
        "\n",
        "Now you are ready to start creating your own AutoML time-series forecasting model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_dataset:tabular,bq,lrg"
      },
      "source": [
        "### Create the Dataset\n",
        "\n",
        "Use `TimeSeriesDataset.create()` to create a TimeSeriesDataset resource, which takes the following parameters:\n",
        "\n",
        "- `display_name`: The human readable name for the dataset resource.\n",
        "- `gcs_source`: A list of one or more dataset index files to import the data items into the dataset resource.\n",
        "- `bq_source`: Alternatively, import data items from a BigQuery table into the dataset resource.\n",
        "\n",
        "This operation may take several minutes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0234d188ed3"
      },
      "source": [
        "### Download data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e310c5f598f"
      },
      "outputs": [],
      "source": [
        "DATASET_URI = \"gs://cloud-samples-data/vertex-ai/structured_data/forecasting/synthetic_sales_data.csv\"\n",
        "\n",
        "# Download the dataset\n",
        "! gsutil cp {DATASET_URI} dataset.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee795ca660f9"
      },
      "source": [
        "#### Split\n",
        "\n",
        "In this use case, you are predicting the sales volume per product per store. \n",
        "Hence, you will need to create a new column called 'product_at_store', which is a concatenation of the 'product' and 'store' columns. This will be passed as the 'target_column' during training.\n",
        "\n",
        "Lastly, split the dataset into a train and test dataset.\n",
        "The train dataset is saved to CSV but the test dataset needs further treatment such as removing the target column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "219ff473f02c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "DATASET_TRAIN_FILENAME = \"sales_forecasting_train.csv\"\n",
        "DATASET_TEST_FILENAME = \"sales_forecasting_test.csv\"\n",
        "DATASET_TRAIN_URI = f\"{BUCKET_URI}/{DATASET_TRAIN_FILENAME}\"\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "\n",
        "df[\"date\"] = df[\"date\"].astype(\"datetime64[ns]\")\n",
        "\n",
        "# Add a target column\n",
        "df[\"product_at_store\"] = df[\"product\"] + \" (\" + df[\"store\"] + \")\"\n",
        "\n",
        "# Split dataset into train and test by taking the first 90% of data for training.\n",
        "dates_unique = df[\"date\"].unique()\n",
        "date_cutoff = sorted(dates_unique)[round(len(dates_unique) * 9 / 10)]\n",
        "\n",
        "# Save train dataset\n",
        "df[df[\"date\"] < date_cutoff].to_csv(DATASET_TRAIN_FILENAME, index=False)\n",
        "\n",
        "# Create test dataset\n",
        "df_test = df[df[\"date\"] >= date_cutoff]\n",
        "\n",
        "# Upload to GCS bucket\n",
        "! gsutil cp {DATASET_TRAIN_FILENAME} {DATASET_TRAIN_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f031cc0274a"
      },
      "source": [
        "#### Plot the dataset\n",
        "\n",
        "Plot the 'sales' vs 'product_at_store' to get a sense of the dataset.\n",
        "\n",
        "Note the peak in 'Snow' products during winter months and a peak in 'Water' products in the summer months."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "343a931623aa"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "sns.relplot(\n",
        "    data=df,\n",
        "    x=\"date\",\n",
        "    y=\"sales\",\n",
        "    hue=\"product_at_store\",\n",
        "    row=\"product_category\",\n",
        "    aspect=4,\n",
        "    kind=\"line\",\n",
        "    style=\"store\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_dataset:tabular,bq,lrg"
      },
      "outputs": [],
      "source": [
        "dataset_time_series = aiplatform.TimeSeriesDataset.create(gcs_source=DATASET_TRAIN_URI)\n",
        "\n",
        "print(dataset_time_series.resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_automl_pipeline:tabular,lrg,transformations"
      },
      "source": [
        "### Create and run training pipeline\n",
        "\n",
        "To train an AutoML model, create and run a training pipeline.\n",
        "\n",
        "#### Create training job\n",
        "\n",
        "Create an AutoML training pipeline using the `AutoMLForecastingTrainingJob` class, with the following parameters:\n",
        "\n",
        "- `display_name`: The human readable name for the TrainingJob resource.\n",
        "- `column_transformations`: (Optional): Transformations to apply to the input columns\n",
        "- `optimization_objective`: The optimization objective (minimize or maximize).\n",
        "  - regression:\n",
        "    - `minimize-rmse`\n",
        "    - `minimize-mae`\n",
        "    - `minimize-rmsle`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_automl_pipeline:tabular,lrg,transformations"
      },
      "outputs": [],
      "source": [
        "training_job = aiplatform.AutoMLForecastingTrainingJob(\n",
        "    display_name=\"hierachical_sales_forecasting\",\n",
        "    optimization_objective=\"minimize-rmse\",\n",
        "    column_specs={\n",
        "        \"date\": \"timestamp\",\n",
        "        \"sales\": \"numeric\",\n",
        "        \"product_type\": \"categorical\",\n",
        "        \"product_category\": \"categorical\",\n",
        "        \"product\": \"categorical\",\n",
        "        \"store\": \"categorical\",\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42ce6f1f9633"
      },
      "source": [
        "### Set context and horizon\n",
        "\n",
        "You need to the context window and forecast horizon when you train a forecasting model.\n",
        "- The context window sets how far back the model looks during training (and for forecasts). In other words, for each training datapoint, the context window determines how far back the model looks for predictive patterns.\n",
        "- The forecast horizon determines how far into the future the model forecasts the target value for each row of prediction data.\n",
        "\n",
        "See more here: [Considerations for setting the context window and forecast horizon](https://cloud.google.com/vertex-ai/docs/datasets/bp-tabular?hl=en#context-window)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8315876232af"
      },
      "outputs": [],
      "source": [
        "# Each row represents a day, so we set context and time horizon to 30 to represent 30 days.\n",
        "\n",
        "CONTEXT_WINDOW = 30\n",
        "TIME_HORIZON = 30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_automl_pipeline:tabular"
      },
      "source": [
        "#### Run the training pipeline\n",
        "\n",
        "Run the training job by invoking the `run` method with the following parameters:\n",
        "\n",
        "- `dataset`: The dataset resource to train the model.\n",
        "- `model_display_name`: The human readable name for the trained model.\n",
        "- `training_fraction_split`: The percentage of the dataset to use for training.\n",
        "- `test_fraction_split`: The percentage of the dataset to use for test (holdout data).\n",
        "- `validation_fraction_split`: The percentage of the dataset to use for validation.\n",
        "- `target_column`: The name of the column to train as the label.\n",
        "- `budget_milli_node_hours`: (optional) Maximum training time specified in unit of millihours (1000 = hour).\n",
        "\n",
        "The `run` method when completed returns the model resource.\n",
        "\n",
        "#### Setting the hierarchical parameters\n",
        "We want to group by 'product' to minimize the error at this level.\n",
        "Hence, you should set the group parameter to \"product\".\n",
        "\n",
        "Setting the `group_total_weight` to a non-zero weight means that you want to weigh the group aggregated loss relative to the individual loss. Set that to 10 for demonstration purposes.\n",
        "\n",
        "See more info at https://cloud.google.com/vertex-ai/docs/tabular-data/forecasting/hierarchical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_automl_pipeline:tabular"
      },
      "outputs": [],
      "source": [
        "time_column = \"date\"\n",
        "time_series_identifier_column = \"product_at_store\"\n",
        "target_column = \"sales\"\n",
        "\n",
        "model = training_job.run(\n",
        "    dataset=dataset_time_series,\n",
        "    target_column=target_column,\n",
        "    time_column=time_column,\n",
        "    time_series_identifier_column=time_series_identifier_column,\n",
        "    available_at_forecast_columns=[time_column],\n",
        "    unavailable_at_forecast_columns=[target_column],\n",
        "    time_series_attribute_columns=[\n",
        "        \"product_type\",\n",
        "        \"product_category\",\n",
        "        \"store\",\n",
        "        \"product\",\n",
        "    ],\n",
        "    forecast_horizon=TIME_HORIZON,\n",
        "    data_granularity_unit=\"day\",\n",
        "    data_granularity_count=1,\n",
        "    model_display_name=\"hierarchical_sales_forecasting_model\",\n",
        "    weight_column=None,\n",
        "    hierarchy_group_columns=[\"product\"],\n",
        "    hierarchy_group_total_weight=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluate_the_model:mbsdk"
      },
      "source": [
        "## Review model evaluation scores\n",
        "After your model has finished training, you can review its evaluation scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluate_the_model:mbsdk"
      },
      "outputs": [],
      "source": [
        "# Get evaluations\n",
        "model_evaluations = model.list_model_evaluations()\n",
        "\n",
        "model_evaluation = list(model_evaluations)[0]\n",
        "print(model_evaluation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "337625a4c33c"
      },
      "source": [
        "## Send a batch prediction request\n",
        "\n",
        "Now you can make a batch prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bff79eb475fa"
      },
      "source": [
        "### Prepare the test dataset\n",
        "\n",
        "For forecasting, the test dataset needs to have context window rows which have information on the target column and subsequent time horizon rows where the target column is unknown. Construct these two sections and combine them into a single CSV file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cec4a813a6c4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Store start and end dates for context and horizon\n",
        "date_context_window_start = date_cutoff\n",
        "date_context_window_end = date_cutoff + np.timedelta64(CONTEXT_WINDOW, \"D\")\n",
        "time_horizon_end = date_context_window_end + np.timedelta64(TIME_HORIZON, \"D\")\n",
        "\n",
        "# Extract dataframes for context and horizon\n",
        "df_test_context = df_test[\n",
        "    (df_test[\"date\"] >= date_context_window_start)\n",
        "    & (df_test[\"date\"] < date_context_window_end)\n",
        "]\n",
        "df_test_horizon = df_test[\n",
        "    (df_test[\"date\"] >= date_context_window_end) & (df_test[\"date\"] < time_horizon_end)\n",
        "].copy()\n",
        "\n",
        "# Save a copy for validation of predictions\n",
        "df_test_horizon_actual = df_test_horizon.copy()\n",
        "\n",
        "# Remove sales for horizon (i.e. future dates)\n",
        "df_test_horizon[\"sales\"] = \"\"\n",
        "\n",
        "# Write test data to CSV\n",
        "df_test = pd.concat([df_test_context, df_test_horizon])\n",
        "df_test.to_csv(DATASET_TEST_FILENAME, index=False)\n",
        "\n",
        "# Save test dataset\n",
        "DATASET_TEST_URI = f\"{BUCKET_URI}/{DATASET_TEST_FILENAME}\"\n",
        "\n",
        "# Upload to GCS bucket\n",
        "! gsutil cp {DATASET_TEST_FILENAME} {DATASET_TEST_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69651b1d06da"
      },
      "source": [
        "### Examine the context dataframe\n",
        "\n",
        "Note that the sales column is filled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f002e35f78cb"
      },
      "outputs": [],
      "source": [
        "df_test_context.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd04bb099daf"
      },
      "source": [
        "### Examine the time horizon dataframe\n",
        "\n",
        "Note that the sales column is empty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da09515ce3dd"
      },
      "outputs": [],
      "source": [
        "df_test_horizon.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93bc53cc960d"
      },
      "source": [
        "### Create a results dataset\n",
        "\n",
        "Create a BigQuery dataset to store the prediction results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6a0730cafb2"
      },
      "outputs": [],
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "# Create client in default region\n",
        "bigquery_client = bigquery.Client(\n",
        "    project=PROJECT_ID,\n",
        "    credentials=aiplatform.initializer.global_config.credentials,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9439d436c826"
      },
      "outputs": [],
      "source": [
        "def create_bigquery_dataset(name: str, region: str):\n",
        "    batch_predict_bq_output_uri_prefix = \"bq://{}.{}\".format(PROJECT_ID, name)\n",
        "\n",
        "    bq_dataset = bigquery.Dataset(\"{}.{}\".format(PROJECT_ID, name))\n",
        "\n",
        "    dataset_region = region\n",
        "    bq_dataset.location = dataset_region\n",
        "    bq_dataset = bigquery_client.create_dataset(bq_dataset)\n",
        "    print(\n",
        "        \"Created bigquery dataset {} in {}\".format(\n",
        "            batch_predict_bq_output_uri_prefix, dataset_region\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return batch_predict_bq_output_uri_prefix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb1fb03d6eae"
      },
      "outputs": [],
      "source": [
        "batch_predict_bq_output_uri_prefix = create_bigquery_dataset(\n",
        "    name=\"hierarchical_forecasting_unique\", region=LOCATION\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36c070503d2f"
      },
      "source": [
        "### Make the batch prediction request\n",
        "\n",
        "You can make a batch prediction by invoking the `batch_predict()` method, with the following parameters:\n",
        "\n",
        "- `job_display_name`: The human readable name for the batch prediction job.\n",
        "- `gcs_source`: A list of one or more batch request input files.\n",
        "- `bigquery_destination_prefix`: The BigQuery destiantion location for storing the batch prediction results.\n",
        "- `instances_format`: The format for the input instances, either 'bigquery', 'csv' or 'jsonl'. Defaults to 'jsonl'.\n",
        "- `predictions_format`: The format for the output predictions, either 'csv', 'jsonl' or 'bigquery'. Defaults to 'jsonl'.\n",
        "- `machine_type`: The type of machine to use for training.\n",
        "- `accelerator_type`: The hardware accelerator type.\n",
        "- `accelerator_count`: The number of accelerators to attach to a worker replica.\n",
        "- `sync`: If set to True, the call will block while waiting for the asynchronous batch job to complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbd40d78ea46"
      },
      "outputs": [],
      "source": [
        "batch_prediction_job = model.batch_predict(\n",
        "    job_display_name=\"hierarchical_forecasting_unique\",\n",
        "    gcs_source=DATASET_TEST_URI,\n",
        "    instances_format=\"csv\",\n",
        "    bigquery_destination_prefix=batch_predict_bq_output_uri_prefix,\n",
        "    predictions_format=\"bigquery\",\n",
        "    generate_explanation=True,\n",
        "    sync=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fc95aae00b0"
      },
      "source": [
        "###  View the batch prediction results\n",
        "\n",
        "Use the BigQuery Python client to query the destination table and return results as a Pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fafe1b0f654b"
      },
      "outputs": [],
      "source": [
        "# View the results as a dataframe\n",
        "df_output = batch_prediction_job.iter_outputs(bq_max_results=1000).to_dataframe()\n",
        "\n",
        "# Convert the dates to the datetime64 datatype\n",
        "df_output[\"date\"] = df_output[\"date\"].astype(\"datetime64[ns]\")\n",
        "\n",
        "# Extract the predicted sales and convert to floats\n",
        "df_output[\"predicted_sales\"] = (\n",
        "    df_output[\"predicted_sales\"].apply(lambda x: x[\"value\"]).astype(float)\n",
        ")\n",
        "\n",
        "df_output.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "305edda2bff8"
      },
      "source": [
        "### Compare predictions vs ground truth\n",
        "\n",
        "Plot the predicted sales vs the ground truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98679eff6973"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a shared dataframe to plot predictions vs ground truth\n",
        "df_output[\"sales_comparison\"] = df_output[\"predicted_sales\"]\n",
        "df_output[\"is_ground_truth\"] = False\n",
        "df_test_horizon_actual[\"sales_comparison\"] = df_test_horizon_actual[\"sales\"]\n",
        "df_test_horizon_actual[\"is_ground_truth\"] = True\n",
        "df_prediction_comparison = pd.concat([df_output, df_test_horizon_actual])\n",
        "\n",
        "# Plot sales\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(24, 12)\n",
        "\n",
        "sns.relplot(\n",
        "    data=df_prediction_comparison,\n",
        "    x=\"date\",\n",
        "    y=\"sales_comparison\",\n",
        "    hue=\"product_at_store\",\n",
        "    style=\"store\",\n",
        "    row=\"is_ground_truth\",\n",
        "    height=5,\n",
        "    aspect=4,\n",
        "    kind=\"line\",\n",
        "    ci=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cleanup:mbsdk"
      },
      "source": [
        "# Clean up\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:\n",
        "\n",
        "- Model\n",
        "- AutoML Training Job\n",
        "- Batch Job\n",
        "- Cloud Storage Bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec0171e4b4e8"
      },
      "outputs": [],
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "# Create client in default region\n",
        "bq_client = bigquery.Client(\n",
        "    project=PROJECT_ID,\n",
        "    credentials=aiplatform.initializer.global_config.credentials,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3be2c2bf9146"
      },
      "outputs": [],
      "source": [
        "# Delete BigQuery datasets\n",
        "bq_client.delete_dataset(\n",
        "    f\"{PROJECT_ID}.hierarchical_forecasting_unique\",\n",
        "    delete_contents=True,\n",
        "    not_found_ok=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup:mbsdk"
      },
      "outputs": [],
      "source": [
        "# Delete Vertex AI resources\n",
        "dataset_time_series.delete()\n",
        "model.delete()\n",
        "training_job.delete()\n",
        "batch_prediction_job.delete()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "sdk_automl_forecasting_hierarchical_batch.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

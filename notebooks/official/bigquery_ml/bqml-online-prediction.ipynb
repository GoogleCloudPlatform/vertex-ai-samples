{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Online prediction with BigQuery ML\n",
        "\n",
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/bigquery_ml/bqml-online-prediction.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/bigquery_ml/bqml-online-prediction.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/bigquery_ml/bqml-online-prediction.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>                                                                                               \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "962e636b5cee"
      },
      "source": [
        "**_NOTE_**: This notebook has been tested in the following environment:\n",
        "\n",
        "* Python version = 3.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfc112ddad4c"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates how to train a churn prediciton model using BigQuery ML on Google Cloud Platform. The churn model training is performed on Google Analytics 4 app event data and is supposed to estimate the likelihood of churning users for the app. \n",
        "\n",
        "Learn more about the usecase and the data from the blog: [Churn prediction for game developers using Google Analytics 4 (GA4) and BigQuery ML](https://cloud.google.com/blog/topics/developers-practitioners/churn-prediction-game-developers-using-google-analytics-4-ga4-and-bigquery-ml).\n",
        "\n",
        "After training the BigQuery ML model, this notebook also explores registering the trained model in Vertex AI and deploying to an endpoint for online predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you fetch the required data from a public BigQuery dataset and prepare it for training. Then you train a churn prediction model on the data using BigQuery ML. Finally, you deploy the model in Vertex AI and get predictions from the endpoint.\n",
        "\n",
        "This tutorial uses the following Google Cloud data analytics and ML services:\n",
        "\n",
        "- BigQuery\n",
        "- BigQuery ML\n",
        "- Vertex AI Model Registry\n",
        "- Vertex AI Endpoints\n",
        "\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Query and fetch the data from the public BigQuery dataset.\n",
        "- Prepare the data for training.\n",
        "- Train a churn classification model using BigQuery ML.\n",
        "- Save the trained model to Vertex AI Model Registry.\n",
        "- Deploy the model to a Vertex AI Endpoint.\n",
        "- Make online prediction requests to the endpoint.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c640527e06e6"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "This notebook uses the public BigQuery dataset [`firebase-public-project.analytics_153293282`](https://console.cloud.google.com/bigquery?p=firebase-public-project&d=analytics_153293282&t=events_20181003&page=table), which contains raw event data from a real mobile gaming app called Flood It! (Android app, iOS app). The data schema originates from Google Analytics for Firebase, but has the same schema as Google Analytics 4. Therefore, the steps in this notebook can be applied to either Google Analytics for Firebase or Google Analytics 4 data.\n",
        "\n",
        "Google Analytics 4 (GA4) uses an event-based measurement model. Events provide insight on what is happening in an app or on a website, such as user actions, system events, or errors. Every row in the data is an event, with various characteristics relevant to that event stored in a nested format within the row.\n",
        "\n",
        "Learn more about the [dataset](https://cloud.google.com/blog/topics/developers-practitioners/churn-prediction-game-developers-using-google-analytics-4-ga4-and-bigquery-ml)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff6017ac879f"
      },
      "source": [
        "### Costs \n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* BigQuery\n",
        "* BigQuery ML\n",
        "* Vertex AI\n",
        "\n",
        "\n",
        "Learn about <a href=\"https://cloud.google.com/bigquery/pricing\" target=\"_blank\">BigQuery Pricing</a>, <a href=\"https://cloud.google.com/bigquery-ml/pricing\" target=\"_blank\">BigQuery ML pricing</a>, <a href=\"https://cloud.google.com/vertex-ai/pricing\" target=\"_blank\">Vertex AI\n",
        "pricing</a>, and use the <a href=\"https://cloud.google.com/products/calculator/\" target=\"_blank\">Pricing\n",
        "Calculator</a>\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7EUnXsZhAGF"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Install the following packages required to execute this notebook. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b4ef9b72d43"
      },
      "outputs": [],
      "source": [
        "! pip3 install --upgrade google-cloud-aiplatform \\\n",
        "                                 google-cloud-bigquery \\\n",
        "                                 db-dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "restart"
      },
      "source": [
        "### Colab only: Uncomment the following cell to restart the kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-ZBOjErv5mM"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "# import IPython\n",
        "\n",
        "# app = IPython.Application.instance()\n",
        "# app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c587f3b9e1e9"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "3. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "4. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfEglUHQk9S3"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, try the following:\n",
        "* Run `gcloud config list`.\n",
        "* Run `gcloud projects list`.\n",
        "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_project_id"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "region"
      },
      "outputs": [],
      "source": [
        "REGION = \"us-central1\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcp_authenticate"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below.\n",
        "\n",
        "**1. Vertex AI Workbench**\n",
        "* Do nothing as you are already authenticated.\n",
        "\n",
        "**2. Local JupyterLab instance, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce6043da7b33"
      },
      "outputs": [],
      "source": [
        "# ! gcloud auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0367eac06a10"
      },
      "source": [
        "**3. Colab, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21ad4dbb4a61"
      },
      "outputs": [],
      "source": [
        "# from google.colab import auth\n",
        "# auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c13224697bfb"
      },
      "source": [
        "**4. Service account or other**\n",
        "* See how to grant Cloud Storage permissions to your service account at https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bucket:mbsdk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "Create a storage bucket to serve as a staging bucket for Vertex AI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bucket"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_bucket"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRUOFELefqf1"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from typing import Union\n",
        "\n",
        "import google.cloud.aiplatform as vertex_ai\n",
        "import pandas as pd\n",
        "from google.cloud import bigquery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "source": [
        "### Initialize Vertex AI and BigQuery SDKs for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for your project and create a BigQuery client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "outputs": [],
      "source": [
        "# initialize the vertex ai sdk\n",
        "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)\n",
        "\n",
        "# create the bigquery client object\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f94734ac9312"
      },
      "source": [
        "## Create a dataset in BigQuery\n",
        "\n",
        "Before you create a dataset, define a helper function for running queries in BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e364dab1d353"
      },
      "outputs": [],
      "source": [
        "# Wrapper to use BigQuery client to run query/job, return job ID or result as DF\n",
        "\n",
        "\n",
        "def run_bq_query(sql: str) -> Union[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Input: SQL query, as a string, to execute in BigQuery\n",
        "    Returns the query results as a pandas DataFrame, or error, if any\n",
        "    \"\"\"\n",
        "    # Try dry run before executing query to catch any errors\n",
        "    job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)\n",
        "    bq_client.query(sql, job_config=job_config)\n",
        "\n",
        "    # If dry run succeeds without errors, proceed to run query\n",
        "    job_config = bigquery.QueryJobConfig()\n",
        "    client_result = bq_client.query(sql, job_config=job_config)\n",
        "\n",
        "    job_id = client_result.job_id\n",
        "\n",
        "    # Wait for query/job to finish running. then get & return data frame\n",
        "    df = client_result.result().to_arrow().to_pandas()\n",
        "    print(f\"Finished job_id: {job_id}\")\n",
        "    # return the dataframe (if any tabular data is returned)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7df9da480041"
      },
      "source": [
        "Create a BigQuery dataset using the `Create` statement in SQL.\n",
        "\n",
        "Learn more about [SQL dialects in BigQuery](https://cloud.google.com/bigquery/docs/introduction-sql#bigquery-sql-dialects)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "088e3b9577b3"
      },
      "outputs": [],
      "source": [
        "# provide the dataset name\n",
        "BQ_DATASET_NAME = \"churnprediction_unique\"  # @param {type:\"string\"}\n",
        "\n",
        "# create a SQL query for creating the dataset\n",
        "create_dataset_query = (\n",
        "    f\"\"\"CREATE SCHEMA IF NOT EXISTS `{PROJECT_ID}.{BQ_DATASET_NAME}`\"\"\"\n",
        ")\n",
        "\n",
        "# run the query\n",
        "run_bq_query(create_dataset_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c6f3e53d02c"
      },
      "source": [
        "## Prepare the training data\n",
        "\n",
        "Before you can train a machine learning model, the data needs to be processed and proper features need to be extracted from it. \n",
        "\n",
        "The event data also includes some behavioral data of the users which can be considered for generating features for training. The timestamp information from the data is processed to determine whether a user churned or not which is used as the ground truth for training.\n",
        "\n",
        "In this section, you perform the following steps:\n",
        "\n",
        "1. Identify whether a user has churned or not based on the event timestamp information.\n",
        "2. Extract behavioral data for each user.\n",
        "3. Join the behavioral data with the churn label data.\n",
        "\n",
        "\n",
        "Note: The original [blog](https://cloud.google.com/blog/topics/developers-practitioners/churn-prediction-game-developers-using-google-analytics-4-ga4-and-bigquery-ml) about training on this dataset also considers user demographic data which is omitted in this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19bab7c21a72"
      },
      "source": [
        "### Identify the label for each user\n",
        "\n",
        "The event dataset doesn't have a feature that tells you whether a user has \"churned\" or \"returned\". So, in this section, you create this label based on the following criteria:\n",
        "\n",
        "```\n",
        "If a user shows no event data after 24 hrs from their first engagement with the app, the user is considered churned.\n",
        "```\n",
        "\n",
        "There are many ways user churn can be defined. For this notebook, you define a 1-day churn. \n",
        "\n",
        "Additionally, you extract calendar information from the timestamps to use as features and remove the \"bouncing\" cases from the data. Bouncing cases refer to the cases where a user just spends a few minutes (say 10 min.) with the app.\n",
        "\n",
        "Run the following cell to create a view named **`returningusers`** with the below columns:\n",
        "\n",
        "- `user_pseudo_id`: An id (false) for the user.\n",
        "- `user_first_engagement`: Earliest event timestamp of the user.\n",
        "- `user_last_engagement`: Latest event timestamp of the user.\n",
        "- `month`: Month of the year for the first engagement of the user.\n",
        "- `julianday`: Day of the year for the first engagement of the user.\n",
        "- `dayofweek`: Day of the week for the first engagement of the user.\n",
        "- `ts_24hr_after_first_engagement`: Timestamp after 24hrs from the first user engagement.\n",
        "- `churned`: Boolean field with **1** representing churned and **0** representing not churned.\n",
        "- `bounced`: Boolean field with **1** representing bounced and **0** representing not bounced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0782ceb77217"
      },
      "outputs": [],
      "source": [
        "# define the sql query\n",
        "create_label_data_query = f\"\"\"\n",
        "    CREATE OR REPLACE VIEW `{PROJECT_ID}.{BQ_DATASET_NAME}.returningusers` AS (\n",
        "      WITH firstlasttouch AS (\n",
        "        SELECT\n",
        "          user_pseudo_id,\n",
        "          MIN(event_timestamp) AS user_first_engagement,\n",
        "          MAX(event_timestamp) AS user_last_engagement\n",
        "        FROM\n",
        "          `firebase-public-project.analytics_153293282.events_*`\n",
        "        WHERE event_name=\"user_engagement\"\n",
        "        GROUP BY\n",
        "          user_pseudo_id\n",
        "\n",
        "      )\n",
        "      SELECT\n",
        "        user_pseudo_id,\n",
        "        user_first_engagement,\n",
        "        user_last_engagement,\n",
        "        EXTRACT(MONTH from TIMESTAMP_MICROS(user_first_engagement)) as month,\n",
        "        EXTRACT(DAYOFYEAR from TIMESTAMP_MICROS(user_first_engagement)) as julianday,\n",
        "        EXTRACT(DAYOFWEEK from TIMESTAMP_MICROS(user_first_engagement)) as dayofweek,\n",
        "\n",
        "        #add 24 hr to user's first touch\n",
        "        (user_first_engagement + 86400000000) AS ts_24hr_after_first_engagement,\n",
        "\n",
        "    #churned = 1 if last_touch within 24 hr of app installation, else 0\n",
        "    IF (user_last_engagement < (user_first_engagement + 86400000000),\n",
        "        1,\n",
        "        0 ) AS churned,\n",
        "\n",
        "    #bounced = 1 if last_touch within 10 min, else 0\n",
        "    IF (user_last_engagement <= (user_first_engagement + 600000000),\n",
        "        1,\n",
        "        0 ) AS bounced,\n",
        "      FROM\n",
        "        firstlasttouch\n",
        "      GROUP BY\n",
        "        1,2,3\n",
        "        );\n",
        "\n",
        "    SELECT \n",
        "      * \n",
        "    FROM \n",
        "      `{PROJECT_ID}.{BQ_DATASET_NAME}.returningusers`\n",
        "    LIMIT 10;\n",
        "\"\"\"\n",
        "\n",
        "# run the query\n",
        "run_bq_query(create_label_data_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "364730f55b45"
      },
      "source": [
        "### Extract behavioral data for each user\n",
        "\n",
        "Behavioral data in the raw event data spans across multiple events. Hence, you aggregate and extract the behavioral data for each user. \n",
        "\n",
        "Since the model needs to predict based on user activity within the first 24 hrs, you extract aggregates from data less than 24 hrs of the app usage.\n",
        "\n",
        "For aggregation, you count the total number of the following event types in the data per user:\n",
        "\n",
        "- user_engagement\n",
        "- level_start_quickplay\n",
        "- level_end_quickplay\n",
        "- level_complete_quickplay\n",
        "- level_reset_quickplay\n",
        "- post_score\n",
        "- spend_virtual_currency\n",
        "- ad_reward\n",
        "- challenge_a_friend\n",
        "- completed_5_levels\n",
        "- use_extra_steps\n",
        "\n",
        "Run the below cell to create a view named `user_aggregate_behavior` querying the aggregate behavioral data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e67b3fa3560e"
      },
      "outputs": [],
      "source": [
        "create_behavioral_data_query = f\"\"\"\n",
        "    CREATE OR REPLACE VIEW `{PROJECT_ID}.{BQ_DATASET_NAME}.user_aggregate_behavior` AS (\n",
        "    WITH\n",
        "      events_first24hr AS (\n",
        "        SELECT\n",
        "          e.*\n",
        "        FROM\n",
        "          `firebase-public-project.analytics_153293282.events_*` e\n",
        "        JOIN\n",
        "          `{PROJECT_ID}.{BQ_DATASET_NAME}.returningusers` r\n",
        "        ON\n",
        "          e.user_pseudo_id = r.user_pseudo_id\n",
        "        WHERE\n",
        "          e.event_timestamp <= r.ts_24hr_after_first_engagement\n",
        "        )\n",
        "    SELECT\n",
        "      user_pseudo_id,\n",
        "      SUM(IF(event_name = 'user_engagement', 1, 0)) AS cnt_user_engagement,\n",
        "      SUM(IF(event_name = 'level_start_quickplay', 1, 0)) AS cnt_level_start_quickplay,\n",
        "      SUM(IF(event_name = 'level_end_quickplay', 1, 0)) AS cnt_level_end_quickplay,\n",
        "      SUM(IF(event_name = 'level_complete_quickplay', 1, 0)) AS cnt_level_complete_quickplay,\n",
        "      SUM(IF(event_name = 'level_reset_quickplay', 1, 0)) AS cnt_level_reset_quickplay,\n",
        "      SUM(IF(event_name = 'post_score', 1, 0)) AS cnt_post_score,\n",
        "      SUM(IF(event_name = 'spend_virtual_currency', 1, 0)) AS cnt_spend_virtual_currency,\n",
        "      SUM(IF(event_name = 'ad_reward', 1, 0)) AS cnt_ad_reward,\n",
        "      SUM(IF(event_name = 'challenge_a_friend', 1, 0)) AS cnt_challenge_a_friend,\n",
        "      SUM(IF(event_name = 'completed_5_levels', 1, 0)) AS cnt_completed_5_levels,\n",
        "      SUM(IF(event_name = 'use_extra_steps', 1, 0)) AS cnt_use_extra_steps,\n",
        "    FROM\n",
        "      events_first24hr\n",
        "    GROUP BY\n",
        "      1\n",
        "      );\n",
        "\n",
        "    SELECT\n",
        "      *\n",
        "    FROM\n",
        "      `{PROJECT_ID}.{BQ_DATASET_NAME}.user_aggregate_behavior`\n",
        "    LIMIT 10\n",
        "\"\"\"\n",
        "# run the query\n",
        "run_bq_query(create_behavioral_data_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b796b180476"
      },
      "source": [
        "### Combine the label data and behavioral data\n",
        "\n",
        "Now, join both the churn label data and the behavioral data on the id field i.e., `user_pseudo_id`. \n",
        "\n",
        "This operation creates a view named `train` which is further used for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "418d17066053"
      },
      "outputs": [],
      "source": [
        "combine_data_query = f\"\"\"\n",
        "    CREATE OR REPLACE VIEW `{PROJECT_ID}.{BQ_DATASET_NAME}.train` AS (\n",
        "\n",
        "      SELECT\n",
        "        ret.user_pseudo_id,\n",
        "        IFNULL(beh.cnt_user_engagement, 0) AS cnt_user_engagement,\n",
        "        IFNULL(beh.cnt_level_start_quickplay, 0) AS cnt_level_start_quickplay,\n",
        "        IFNULL(beh.cnt_level_end_quickplay, 0) AS cnt_level_end_quickplay,\n",
        "        IFNULL(beh.cnt_level_complete_quickplay, 0) AS cnt_level_complete_quickplay,\n",
        "        IFNULL(beh.cnt_level_reset_quickplay, 0) AS cnt_level_reset_quickplay,\n",
        "        IFNULL(beh.cnt_post_score, 0) AS cnt_post_score,\n",
        "        IFNULL(beh.cnt_spend_virtual_currency, 0) AS cnt_spend_virtual_currency,\n",
        "        IFNULL(beh.cnt_ad_reward, 0) AS cnt_ad_reward,\n",
        "        IFNULL(beh.cnt_challenge_a_friend, 0) AS cnt_challenge_a_friend,\n",
        "        IFNULL(beh.cnt_completed_5_levels, 0) AS cnt_completed_5_levels,\n",
        "        IFNULL(beh.cnt_use_extra_steps, 0) AS cnt_use_extra_steps,\n",
        "        ret.user_first_engagement,\n",
        "        ret.month,\n",
        "        ret.julianday,\n",
        "        ret.dayofweek,\n",
        "        ret.churned\n",
        "      FROM\n",
        "        `{PROJECT_ID}.{BQ_DATASET_NAME}.returningusers` ret\n",
        "      LEFT OUTER JOIN \n",
        "        `{PROJECT_ID}.{BQ_DATASET_NAME}.user_aggregate_behavior` beh\n",
        "      ON\n",
        "        ret.user_pseudo_id = beh.user_pseudo_id\n",
        "      WHERE ret.bounced = 0\n",
        "      );\n",
        "\n",
        "    SELECT\n",
        "      *\n",
        "    FROM\n",
        "      `{PROJECT_ID}.{BQ_DATASET_NAME}.train`\n",
        "    LIMIT 10\n",
        "\"\"\"\n",
        "# run the query\n",
        "run_bq_query(combine_data_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5936942dee2"
      },
      "source": [
        "## Train a logistic regression model with BQML\n",
        "\n",
        "\n",
        "BQML provides the capability to train machine learning models on tabular data such as classification, regression, forecasting, and matrix factorization in BigQuery using SQL syntax. BQML uses the scalable infrastructure of BigQuery so you don't need to set up additional infrastructure for training or batch serving.\n",
        "\n",
        "\n",
        "Now, train a logistic regression model on the created training data using BQML. For this, you use the `CREATE OR REPLACE MODEL` statement from BigQuery's SQL dialect with the following options:\n",
        "\n",
        "* `MODEL_TYPE`: Specify the machine learning algorithm to train the model. Learn about other [available model types](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create#model_type).\n",
        "\n",
        "* `INPUT_LABEL_COLS`: The label column names in the training data.\n",
        "\n",
        "* `MODEL_REGISTRY`: Specifies the model registry destination. For now, 'VERTEX_AI' is the only supported model registry destination. To learn more, see [MLOps with BigQuery ML and Vertex AI](https://cloud.google.com/bigquery/docs/create_vertex#register_a_model_to_the).\n",
        "\n",
        "\n",
        "* `VERTEX_AI_MODEL_VERSION_ALIASES`: The Vertex AI model alias to register the model with. It can only be set when `MODEL_REGISTRY` is set to 'VERTEX_AI'. To learn more, see [adding a Vertex AI model alias](https://cloud.google.com/bigquery/docs/create_vertex#add_a_model_alias).\n",
        "\n",
        "Learn more about training models in [BQML](https://cloud.google.com/bigquery/docs/bqml-introduction).\n",
        "\n",
        "Note that the model names also follow the [same rules](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create#model_name) as tables in BigQuery. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b982bd807671"
      },
      "outputs": [],
      "source": [
        "# set the model name\n",
        "BQ_MODEL_NAME = \"churn_model_unique\"  # @param {type:\"string\"}\n",
        "\n",
        "# define the query for creating and training the BQML model\n",
        "# remove the id columns user_first_engagement, user_pseudo_id\n",
        "# for training\n",
        "train_model_query = f\"\"\"\n",
        "    CREATE OR REPLACE MODEL `{PROJECT_ID}.{BQ_DATASET_NAME}.{BQ_MODEL_NAME}`\n",
        "\n",
        "    OPTIONS(\n",
        "      MODEL_TYPE=\"LOGISTIC_REG\",\n",
        "      INPUT_LABEL_COLS=[\"churned\"],\n",
        "      MODEL_REGISTRY=\"VERTEX_AI\",\n",
        "      VERTEX_AI_MODEL_VERSION_ALIASES=['logistic_reg', 'churn_prediction']\n",
        "    ) AS\n",
        "  \n",
        "    SELECT\n",
        "      * EXCEPT(user_first_engagement, user_pseudo_id)\n",
        "    FROM\n",
        "      `{PROJECT_ID}.{BQ_DATASET_NAME}.train`\n",
        "\"\"\"\n",
        "# run the query\n",
        "run_bq_query(train_model_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3f7ed776cac"
      },
      "source": [
        "### Fetch the evaluation scores\n",
        "\n",
        "Once the training is done, fetch the evaluation metric scores for the trained model.\n",
        "\n",
        "The following cell should give results with the below fields:\n",
        "\n",
        "| precision | recall | accuracy | f1_score | log_loss | roc_auc |\n",
        "| -------- | ------- | ------- | ------- | ------- | ------- |\n",
        "\n",
        "\n",
        "Learn more about the [metrics returned by the **ML.EVALUATE** function](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-evaluate#mlevaluate_output)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba9bbe192cfa"
      },
      "outputs": [],
      "source": [
        "# define the query\n",
        "evaluate_model_query = f\"\"\"\n",
        "    SELECT\n",
        "      *\n",
        "    FROM\n",
        "      ML.EVALUATE(MODEL `{PROJECT_ID}.{BQ_DATASET_NAME}.{BQ_MODEL_NAME}`)\n",
        "\"\"\"\n",
        "\n",
        "# run the query\n",
        "run_bq_query(evaluate_model_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b863ecc9bad"
      },
      "source": [
        "## Generate batch predictions with explanations\n",
        "\n",
        "\n",
        "Make a batch prediction in BQML on some samples from the training data. The probability of churn for each of the users is seen in the `probability` column, with the predicted label under the `predicted_churn` column.\n",
        "\n",
        "Additionally, you can also generate explanations in the form of feature attributions using the `ML.EXPLAIN_PREDICT()` function. This allows you to interpret the top contributing features for each prediction.\n",
        "\n",
        "The output returned consists of the following additional columns:\n",
        "\n",
        "- `predicted_churned`: Column with the predicted value of the label (churned).\n",
        "\n",
        "- `probability`: The probability of the predicted label class.\n",
        "\n",
        "- `top_feature_attributions`:  An ARRAY of STRUCTs containing the attributions of the top k features to the final prediction.\n",
        "\n",
        "    - `top_feature_attributions.feature`: The feature name.\n",
        "    \n",
        "    - `top_feature_attributions.attribution`: Attribution of the feature to the final prediction.\n",
        "    \n",
        "- `baseline_prediction_value`: For linear models, the baseline_prediction_value is the intercept of the model.\n",
        "\n",
        "- `prediction_value`: The [logit](https://en.wikipedia.org/wiki/Logit) value (i.e., log-odds) for the predicted class.\n",
        "\n",
        "- `approximation_error`: The approximation error for the algorithm used for explanations. In case of the current logistic regression model, there is no approximation error and this field is always 0. Learn more about the [approximation error](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-explain-predict#mlexplain_predict_output).\n",
        "\n",
        "Learn more about [**ML.EXPLAIN_PREDICT**](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-xai-overview). \n",
        "\n",
        "Note: You can simply use the [**ML.PREDICT**](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-predict#mlpredict_function) to get predictions without explanations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5898da3d9be3"
      },
      "outputs": [],
      "source": [
        "# write sql query for predictions with explanations\n",
        "explain_predict_query = f\"\"\"\n",
        "    SELECT\n",
        "      *\n",
        "    FROM\n",
        "      ML.EXPLAIN_PREDICT(\n",
        "        MODEL `{PROJECT_ID}.{BQ_DATASET_NAME}.{BQ_MODEL_NAME}`,\n",
        "        (\n",
        "            SELECT * FROM `{PROJECT_ID}.{BQ_DATASET_NAME}.train` LIMIT 10\n",
        "        )\n",
        "    )\n",
        "\"\"\"\n",
        "# run the sql query\n",
        "run_bq_query(explain_predict_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcd66c397994"
      },
      "source": [
        "## Get model from Model Registry\n",
        "\n",
        "The `CREATE MODEL` statement with `MODEL_REGISTRY` set to `VERTEX_AI` from earlier creates a model in the Vertex AI Model Registry. \n",
        "\n",
        "Fetch the model resource using the model name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33bb93b73fc6"
      },
      "outputs": [],
      "source": [
        "model = vertex_ai.Model(model_name=BQ_MODEL_NAME)\n",
        "\n",
        "print(model.gca_resource)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "533c5f53b007"
      },
      "source": [
        "## Create an endpoint for deployment\n",
        "\n",
        "While BQML supports batch prediction, it is not suitable for real-time predictions where you need low latency predictions with potentially high frequency of requests. Hence, deploying the BQML model to an endpoint enables you to do online predictions.\n",
        "\n",
        "Create a Vertex AI Endpoint to deploy the BQML model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0580ed33a96"
      },
      "outputs": [],
      "source": [
        "ENDPOINT_NAME = \"churn-model-endpoint-unique\"  # @param {type:\"string\"}\n",
        "\n",
        "endpoint = vertex_ai.Endpoint.create(\n",
        "    display_name=ENDPOINT_NAME,\n",
        "    project=PROJECT_ID,\n",
        "    location=REGION,\n",
        ")\n",
        "\n",
        "print(\"Endpoint display name:\", endpoint.display_name)\n",
        "print(\"Endpoint resource name:\", endpoint.resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad3e6e498a46"
      },
      "source": [
        "## Deploy the model to the endpoint\n",
        "\n",
        "Deploy the model resource to the created endpoint resource using the [**deploy**](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model#google_cloud_aiplatform_Model_deploy) method.\n",
        "\n",
        "When your BQML model is registered in Vertex AI Model Registry, and if it is an Explainable AI supported model type, you can enable Explainable AI on the model when deploying to an endpoint. For this tutorial, you deploy the model without Explainable AI.\n",
        "\n",
        "Learn more about [deploying BQML model to Vertex AI Endpoint with Explainable AI enabled](https://cloud.google.com/bigquery/docs/vertex-xai#enable_explainable_ai_in)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "229cf70af4f1"
      },
      "outputs": [],
      "source": [
        "# deploying the model to the endpoint may take 10-15 minutes\n",
        "model.deploy(endpoint=endpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "527137e45503"
      },
      "source": [
        "## Create payload for online prediction\n",
        "\n",
        "Query the training data for some samples (say 2 records) to send as an online request to the endpoint. The instances payload need to be formatted as a list of key value pairs where a key represents a feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88c06376b10a"
      },
      "outputs": [],
      "source": [
        "# query to select two instances for testing\n",
        "test_df = run_bq_query(\n",
        "    f\"\"\"\n",
        "    SELECT \n",
        "    * except(user_first_engagement, user_pseudo_id, churned) \n",
        "    FROM `{PROJECT_ID}.{BQ_DATASET_NAME}.train` \n",
        "    LIMIT 2\n",
        "    \"\"\"\n",
        ")\n",
        "# convert the dataframe to json instances to send\n",
        "df_sample_requests_list = json.loads(test_df.to_json(orient=\"records\"))\n",
        "\n",
        "# display the data\n",
        "test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc3541d4c4d7"
      },
      "source": [
        "## Send online request to the endpoint\n",
        "\n",
        "Use endpoint's **predict** method and send the list of instances for prediction.\n",
        "\n",
        "Predictions from the returned response consist of the following:\n",
        "\n",
        "- `churned_probs`: probabilities for each of the classes.\n",
        "- `predicted_churned`: class predicted.\n",
        "- `churned_values`: possible values for the classes (1 for churned and 0 for not churned)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85dc6608e4b2"
      },
      "outputs": [],
      "source": [
        "# send prediction request to the endpoint\n",
        "prediction = endpoint.predict(df_sample_requests_list)\n",
        "# print the predictions\n",
        "print(prediction.predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:\n",
        "\n",
        "- Vertex AI Endpoint\n",
        "- Vertex AI Model (BQML model can not be deleted from Vertex AI and needs to be deleted from BigQuery)\n",
        "- BigQuery Dataset\n",
        "- Cloud Storage bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx_vKniMq9ZX"
      },
      "outputs": [],
      "source": [
        "# set True to delete the dataset\n",
        "delete_dataset = False\n",
        "# set True to delete the bucket\n",
        "delete_bucket = False\n",
        "\n",
        "# undeploy model from the endpoint\n",
        "endpoint.undeploy_all()\n",
        "# # delete the endpoint\n",
        "endpoint.delete()\n",
        "\n",
        "# delete BigQuery dataset\n",
        "if delete_dataset or os.getenv(\"IS_TESTING\"):\n",
        "    ! bq rm -r -f $PROJECT_ID:$BQ_DATASET_NAME\n",
        "# delete the Cloud Storage bucket\n",
        "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
        "    ! gsutil -m rm -r $BUCKET_URI"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "bqml-online-prediction.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

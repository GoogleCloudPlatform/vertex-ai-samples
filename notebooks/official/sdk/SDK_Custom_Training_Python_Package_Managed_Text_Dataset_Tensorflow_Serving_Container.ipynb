{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41aMjXA5rKoL"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_uIhhWEZdcBb"
   },
   "source": [
    "# Vertex SDK for Python: Custom Training using Python Package, Managed Text Dataset, and TF-Serving Container Example\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/sdk/SDK_Custom_Training_Python_Package_Managed_Text_Dataset_Tensorflow_Serving_Container.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/sdk/SDK_Custom_Training_Python_Package_Managed_Text_Dataset_Tensorflow_Serving_Container.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/sdk/SDK_Custom_Training_Python_Package_Managed_Text_Dataset_Tensorflow_Serving_Container.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>                                                                                               \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ij53c3EzrKoS"
   },
   "source": [
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to create a Custom Model using Custom Python Package Training, with a Vertex AI Dataset, and how to serve the model using Tensorflow-Serving Container for online prediction, and batch prediction. It will require you provide a bucket where the dataset will be stored.\n",
    "\n",
    "Note: You may incur charges for training, prediction, storage or usage of other GCP products in connection with testing this SDK.\n",
    "\n",
    "## Dataset\n",
    "#### Stack Overflow Data\n",
    "We download the stack overflow data from from  https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz and will create a Vertex AI managed text dataset. \n",
    "\n",
    "The Stack Overflow Data is licensed under the Creative Commons Attribution-ShareAlike 3.0 Unported License. To view a copy of this license, visit http://creativecommons.org/licenses/by-sa/3.0/ \n",
    "\n",
    "For more information about this dataset please visit: https://console.cloud.google.com/marketplace/details/stack-exchange/stack-overflow\n",
    "\n",
    "\n",
    "## Objective\n",
    "\n",
    "- Create utility functions to download data and prepare csv files for creating Vertex AI Managed    Dataset\n",
    "- Download Data\n",
    "- Prepare CSV Files for Creating Managed Dataset\n",
    "- Create Custom Training Python Package\n",
    "- Create TensorFlow Serving Container\n",
    "- Run Custom Python Package Training with Managed Text Dataset\n",
    "- Deploy a Model and Create an Endpoint on Vertex AI\n",
    "- Predict on the Endpoint\n",
    "- Create a Batch Prediction Job on the Model\n",
    "## Costs \n",
    "\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b82b8a653933"
   },
   "source": [
    "### Set up your local development environment\n",
    "\n",
    "**If you are using Colab or Vertex AI Workbench**, your environment already meets\n",
    "all the requirements to run this notebook. You can skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "306eb9fb5ac7"
   },
   "source": [
    "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
    "You need the following:\n",
    "\n",
    "* The Google Cloud SDK\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* Jupyter notebook running in a virtual environment with Python 3\n",
    "\n",
    "The Google Cloud guide to [Setting up a Python development\n",
    "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
    "installation guide](https://jupyter.org/install) provide detailed instructions\n",
    "for meeting these requirements. The following steps provide a condensed set of\n",
    "instructions:\n",
    "\n",
    "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "1. [Install\n",
    "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
    "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
    "\n",
    "1. To install Jupyter, run `pip3 install jupyter` on the\n",
    "command-line in a terminal shell.\n",
    "\n",
    "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
    "\n",
    "1. Open this notebook in the Jupyter Notebook Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOMNWzTbftDr"
   },
   "source": [
    "### Install additional packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9eb4e79842bf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "Be020jY-ftDv",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5226f8eb-fa09-40dc-889d-0da20992e347",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping google-cloud-aiplatform as it is not installed.\u001b[0m\n",
      "Collecting google-cloud-aiplatform\n",
      "  Downloading google_cloud_aiplatform-1.12.1-py2.py3-none-any.whl (1.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8 MB 5.1 MB/s \n",
      "\u001b[?25hCollecting google-cloud-storage<3.0.0dev,>=1.32.0\n",
      "  Downloading google_cloud_storage-2.3.0-py2.py3-none-any.whl (107 kB)\n",
      "\u001b[K     |████████████████████████████████| 107 kB 59.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.7/dist-packages (from google-cloud-aiplatform) (1.31.5)\n",
      "Collecting proto-plus>=1.15.0\n",
      "  Downloading proto_plus-1.20.3-py3-none-any.whl (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 3.3 MB/s \n",
      "\u001b[?25hCollecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
      "  Downloading google_cloud_resource_manager-1.4.1-py2.py3-none-any.whl (402 kB)\n",
      "\u001b[K     |████████████████████████████████| 402 kB 42.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-aiplatform) (21.3)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-aiplatform) (1.21.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (57.4.0)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.25.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (1.35.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (2.23.0)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (1.15.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (1.56.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (3.17.3)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (2022.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (1.44.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (4.2.4)\n",
      "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (0.4.1)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.0.3)\n",
      "Collecting grpc-google-iam-v1<0.13dev,>=0.12.3\n",
      "  Downloading grpc_google_iam_v1-0.12.4-py2.py3-none-any.whl (26 kB)\n",
      "Collecting google-cloud-storage<3.0.0dev,>=1.32.0\n",
      "  Downloading google_cloud_storage-2.2.1-py2.py3-none-any.whl (107 kB)\n",
      "\u001b[K     |████████████████████████████████| 107 kB 65.8 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-2.2.0-py2.py3-none-any.whl (107 kB)\n",
      "\u001b[K     |████████████████████████████████| 107 kB 66.0 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-2.1.0-py2.py3-none-any.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 67.6 MB/s \n",
      "\u001b[?25hCollecting google-cloud-core<2.0dev,>=1.0.3\n",
      "  Downloading google_cloud_core-1.7.2-py2.py3-none-any.whl (28 kB)\n",
      "Collecting google-cloud-storage<3.0.0dev,>=1.32.0\n",
      "  Downloading google_cloud_storage-2.0.0-py2.py3-none-any.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 68.4 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.44.0-py2.py3-none-any.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 68.1 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.43.0-py2.py3-none-any.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 66.7 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.42.3-py2.py3-none-any.whl (105 kB)\n",
      "\u001b[K     |████████████████████████████████| 105 kB 72.5 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.42.2-py2.py3-none-any.whl (105 kB)\n",
      "\u001b[K     |████████████████████████████████| 105 kB 71.2 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.42.1-py2.py3-none-any.whl (105 kB)\n",
      "\u001b[K     |████████████████████████████████| 105 kB 54.7 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.42.0-py2.py3-none-any.whl (105 kB)\n",
      "\u001b[K     |████████████████████████████████| 105 kB 61.1 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.41.1-py2.py3-none-any.whl (105 kB)\n",
      "\u001b[K     |████████████████████████████████| 105 kB 57.7 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.41.0-py2.py3-none-any.whl (104 kB)\n",
      "\u001b[K     |████████████████████████████████| 104 kB 55.7 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.40.0-py2.py3-none-any.whl (104 kB)\n",
      "\u001b[K     |████████████████████████████████| 104 kB 51.3 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.39.0-py2.py3-none-any.whl (103 kB)\n",
      "\u001b[K     |████████████████████████████████| 103 kB 50.7 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.38.0-py2.py3-none-any.whl (103 kB)\n",
      "\u001b[K     |████████████████████████████████| 103 kB 53.7 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.37.1-py2.py3-none-any.whl (103 kB)\n",
      "\u001b[K     |████████████████████████████████| 103 kB 59.2 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.37.0-py2.py3-none-any.whl (103 kB)\n",
      "\u001b[K     |████████████████████████████████| 103 kB 47.8 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.36.2-py2.py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 5.8 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.36.1-py2.py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 5.8 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.36.0-py2.py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 6.0 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.35.1-py2.py3-none-any.whl (96 kB)\n",
      "\u001b[K     |████████████████████████████████| 96 kB 4.7 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.35.0-py2.py3-none-any.whl (96 kB)\n",
      "\u001b[K     |████████████████████████████████| 96 kB 5.2 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.34.0-py2.py3-none-any.whl (96 kB)\n",
      "\u001b[K     |████████████████████████████████| 96 kB 5.3 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.33.0-py2.py3-none-any.whl (92 kB)\n",
      "\u001b[K     |████████████████████████████████| 92 kB 8.8 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.32.0-py2.py3-none-any.whl (92 kB)\n",
      "\u001b[K     |████████████████████████████████| 92 kB 10.2 MB/s \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of google-cloud-resource-manager to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
      "  Downloading google_cloud_resource_manager-1.4.0-py2.py3-none-any.whl (402 kB)\n",
      "\u001b[K     |████████████████████████████████| 402 kB 57.4 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_resource_manager-1.3.3-py2.py3-none-any.whl (286 kB)\n",
      "\u001b[K     |████████████████████████████████| 286 kB 46.1 MB/s \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of google-cloud-core to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting google-cloud-core<2.0dev,>=1.0.3\n",
      "  Downloading google_cloud_core-1.7.1-py2.py3-none-any.whl (28 kB)\n",
      "INFO: pip is looking at multiple versions of google-cloud-resource-manager to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading google_cloud_core-1.7.0-py2.py3-none-any.whl (28 kB)\n",
      "  Downloading google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
      "  Downloading google_cloud_core-1.5.0-py2.py3-none-any.whl (27 kB)\n",
      "  Downloading google_cloud_core-1.4.4-py2.py3-none-any.whl (27 kB)\n",
      "  Downloading google_cloud_core-1.4.3-py2.py3-none-any.whl (27 kB)\n",
      "INFO: pip is looking at multiple versions of google-cloud-core to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading google_cloud_core-1.4.2-py2.py3-none-any.whl (26 kB)\n",
      "  Downloading google_cloud_core-1.4.1-py2.py3-none-any.whl (26 kB)\n",
      "  Downloading google_cloud_core-1.4.0-py2.py3-none-any.whl (26 kB)\n",
      "  Downloading google_cloud_core-1.3.0-py2.py3-none-any.whl (26 kB)\n",
      "  Downloading google_cloud_core-1.2.0-py2.py3-none-any.whl (26 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
      "  Downloading google_cloud_core-1.1.0-py2.py3-none-any.whl (26 kB)\n",
      "  Downloading google_cloud_core-1.0.3-py2.py3-none-any.whl (26 kB)\n",
      "INFO: pip is looking at multiple versions of google-cloud-bigquery to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting google-cloud-bigquery<3.0.0dev,>=1.15.0\n",
      "  Downloading google_cloud_bigquery-2.34.3-py2.py3-none-any.whl (206 kB)\n",
      "\u001b[K     |████████████████████████████████| 206 kB 44.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
      "Collecting google-resumable-media<3.0dev,>=0.6.0\n",
      "  Downloading google_resumable_media-2.3.2-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 4.1 MB/s \n",
      "\u001b[?25hCollecting google-cloud-core<3.0.0dev,>=1.4.1\n",
      "  Downloading google_cloud_core-2.3.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.3.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-cloud-aiplatform) (3.0.8)\n",
      "Collecting protobuf>=3.12.0\n",
      "  Downloading protobuf-3.20.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 48.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (0.4.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (2021.10.8)\n",
      "Installing collected packages: protobuf, google-crc32c, proto-plus, grpc-google-iam-v1, google-resumable-media, google-cloud-core, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, google-cloud-aiplatform\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.17.3\n",
      "    Uninstalling protobuf-3.17.3:\n",
      "      Successfully uninstalled protobuf-3.17.3\n",
      "  Attempting uninstall: google-resumable-media\n",
      "    Found existing installation: google-resumable-media 0.4.1\n",
      "    Uninstalling google-resumable-media-0.4.1:\n",
      "      Successfully uninstalled google-resumable-media-0.4.1\n",
      "  Attempting uninstall: google-cloud-core\n",
      "    Found existing installation: google-cloud-core 1.0.3\n",
      "    Uninstalling google-cloud-core-1.0.3:\n",
      "      Successfully uninstalled google-cloud-core-1.0.3\n",
      "  Attempting uninstall: google-cloud-storage\n",
      "    Found existing installation: google-cloud-storage 1.18.1\n",
      "    Uninstalling google-cloud-storage-1.18.1:\n",
      "      Successfully uninstalled google-cloud-storage-1.18.1\n",
      "  Attempting uninstall: google-cloud-bigquery\n",
      "    Found existing installation: google-cloud-bigquery 1.21.0\n",
      "    Uninstalling google-cloud-bigquery-1.21.0:\n",
      "      Successfully uninstalled google-cloud-bigquery-1.21.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
      "pandas-gbq 0.13.3 requires google-cloud-bigquery[bqstorage,pandas]<2.0.0dev,>=1.11.1, but you have google-cloud-bigquery 2.34.3 which is incompatible.\n",
      "google-cloud-translate 1.5.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.3.0 which is incompatible.\n",
      "google-cloud-firestore 1.7.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.3.0 which is incompatible.\n",
      "google-cloud-datastore 1.8.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.3.0 which is incompatible.\u001b[0m\n",
      "Successfully installed google-cloud-aiplatform-1.12.1 google-cloud-bigquery-2.34.3 google-cloud-core-2.3.0 google-cloud-resource-manager-1.4.1 google-cloud-storage-2.3.0 google-crc32c-1.3.0 google-resumable-media-2.3.2 grpc-google-iam-v1-0.12.4 proto-plus-1.20.3 protobuf-3.20.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "google"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.24.0)\n",
      "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.44.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (13.0.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 4.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
      "Installing collected packages: tf-estimator-nightly\n",
      "Successfully installed tf-estimator-nightly-2.8.0.dev2021122109\n"
     ]
    }
   ],
   "source": [
    "! pip3 uninstall -y google-cloud-aiplatform\n",
    "! pip3 install google-cloud-aiplatform\n",
    "! pip3 install --upgrade tensorflow \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ed89a5527afb"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9a2bb523c478"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4f3be5c2830"
   },
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af017a0645ea"
   },
   "source": [
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). {TODO: Update the APIs needed for your tutorial. Edit the API names, and update the link to append the API IDs, separating each one with a comma. For example, container.googleapis.com,cloudbuild.googleapis.com}\n",
    "\n",
    "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    "\n",
    "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56d591439df1"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2016f37329c0",
    "outputId": "ff82292c-3ede-41cc-ef33-099bdebe7d11"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0426 11:07:22.608679914   26897 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  vertex-ai-dev\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "244af163f3e3"
   },
   "source": [
    "Otherwise, set your project ID here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "547d93dfcc7d"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09021c90b34c",
    "outputId": "93830517-ee25-4532-b9de-691edd2a03bb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0426 11:07:26.408876199   26897 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e663bd062c6f"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append it onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "953fa6e5ddda"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "870777863e09"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "**If you are using Vertex AI Workbench**, your environment is already\n",
    "authenticated. Skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "378e70541ba9"
   },
   "source": [
    "**If you are using Colab**, run the cell below and follow the instructions\n",
    "when prompted to authenticate your account via oAuth.\n",
    "\n",
    "**Otherwise**, follow these steps:\n",
    "\n",
    "1. In the Cloud Console, go to the [**Create service account key**\n",
    "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
    "\n",
    "2. Click **Create service account**.\n",
    "\n",
    "3. In the **Service account name** field, enter a name, and\n",
    "   click **Create**.\n",
    "\n",
    "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
    "into the filter box, and select\n",
    "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
    "\n",
    "5. Click *Create*. A JSON file that contains your key downloads to your\n",
    "local environment.\n",
    "\n",
    "6. Enter the path to your service account key as the\n",
    "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "f81b13f6ed38"
   },
   "outputs": [],
   "source": [
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If on Google Cloud Notebook, then don't execute this code\n",
    "IS_COLAB = False\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        IS_COLAB = True\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bba822af0b32"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
    "Cloud Storage buckets.\n",
    "\n",
    "You may also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook. We suggest that you [choose a region where Vertex AI services are\n",
    "available](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "52b311a64f71"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "REGION = \"[your-region]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "ff28800dc2f8"
   },
   "outputs": [],
   "source": [
    "if BUCKET_URI == \"\" or BUCKET_URI is None or BUCKET_URI == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_URI = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58cb4f5895f0"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5e1288505682",
    "outputId": "1c5fc822-fc72-4da6-8be1-032ae6357863"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0426 11:07:40.987975042   26897 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://vertex-ai-devaip-20220426110730/...\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c664a5abc11a"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "f004d00a7000"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0426 11:07:51.385774748   26897 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2d86f10ef734"
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "d630a1be6a5f"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "from google.cloud import aiplatform, storage\n",
    "from tensorflow.keras import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHNj4P3vrKoV"
   },
   "source": [
    "### Set Your Application Name, Task Name, and Directories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RnNL5St8rKoV",
    "outputId": "ab6a06c6-cd9c-460c-e596-f5683d7a4ccd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Name:      mbsdk_custom-py-pkg-training_keras-text-class-stack-overflow-tag\n",
      "Task Directory: ./mbsdk_custom-py-pkg-training_keras-text-class-stack-overflow-tag\n",
      "Data Directory: ./mbsdk_custom-py-pkg-training_keras-text-class-stack-overflow-tag/data\n"
     ]
    }
   ],
   "source": [
    "APP_NAME = \"keras-text-class-stack-overflow-tag\"\n",
    "TASK_TYPE = \"mbsdk_custom-py-pkg-training\"\n",
    "\n",
    "TASK_NAME = f\"{TASK_TYPE}_{APP_NAME}\"\n",
    "\n",
    "TASK_DIR = f\"./{TASK_NAME}\"\n",
    "DATA_DIR = f\"{TASK_DIR}/data\"\n",
    "\n",
    "print(f\"Task Name:      {TASK_NAME}\")\n",
    "print(f\"Task Directory: {TASK_DIR}\")\n",
    "print(f\"Data Directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hZmDcfLS13W"
   },
   "source": [
    "### Set a GCS Prefix\n",
    "\n",
    "If you want to centeralize all input and output files under the gcs location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W_4gzaP0SyQd",
    "outputId": "ddc16ea2-7746-4f6d-eff9-d7dde2f4c669"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket Name:    vertex-ai-devaip-20220426110730\n",
      "GCS Prefix:     mbsdk_custom-py-pkg-training/keras-text-class-stack-overflow-tag\n"
     ]
    }
   ],
   "source": [
    "BUCKET_NAME = BUCKET_URI.split(\"gs://\")[1]\n",
    "GCS_PREFIX = f\"{TASK_TYPE}/{APP_NAME}\"\n",
    "\n",
    "print(f\"Bucket Name:    {BUCKET_NAME}\")\n",
    "print(f\"GCS Prefix:     {GCS_PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJF047yNftDw"
   },
   "source": [
    "### Utility Functions to Download Data and Prepare CSV Files for Creating Vertex AI Managed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "9yOl-l_oftDx"
   },
   "outputs": [],
   "source": [
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    destination_file_name = os.path.join(\"gs://\", bucket_name, destination_blob_name)\n",
    "\n",
    "    return destination_file_name\n",
    "\n",
    "\n",
    "def download_data(data_dir):\n",
    "    \"\"\"Download data.\"\"\"\n",
    "\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    url = \"https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz\"\n",
    "    dataset = utils.get_file(\n",
    "        \"stack_overflow_16k.tar.gz\",\n",
    "        url,\n",
    "        untar=True,\n",
    "        cache_dir=data_dir,\n",
    "        cache_subdir=\"\",\n",
    "    )\n",
    "    data_dir = os.path.join(os.path.dirname(dataset))\n",
    "\n",
    "    return data_dir\n",
    "\n",
    "\n",
    "def upload_train_data_to_gcs(train_data_dir, bucket_name, destination_blob_prefix):\n",
    "    \"\"\"Create CSV file using train data content.\"\"\"\n",
    "\n",
    "    train_data_dir = os.path.join(data_dir, \"train\")\n",
    "    train_data_fn = os.path.join(data_dir, \"train.csv\")\n",
    "\n",
    "    fp = open(train_data_fn, \"w\", encoding=\"utf8\")\n",
    "    writer = csv.writer(\n",
    "        fp, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_ALL, lineterminator=\"\\n\"\n",
    "    )\n",
    "\n",
    "    for root, _, files in os.walk(train_data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                class_name = root.split(\"/\")[-1]\n",
    "                file_fn = os.path.join(root, file)\n",
    "                with open(file_fn, \"r\") as f:\n",
    "                    content = f.readlines()\n",
    "                    lines = [x.strip().strip('\"') for x in content]\n",
    "                    writer.writerow((lines[0], class_name))\n",
    "\n",
    "    fp.close()\n",
    "\n",
    "    train_gcs_url = upload_blob(\n",
    "        bucket_name, train_data_fn, os.path.join(destination_blob_prefix, \"train.csv\")\n",
    "    )\n",
    "\n",
    "    return train_gcs_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3UDGt8MrKoY"
   },
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ks2ahWfUrKoa",
    "outputId": "a21d3545-d35a-4f64-d08e-77ba59b6130c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz\n",
      "6053888/6053168 [==============================] - 0s 0us/step\n",
      "Data is downloaded to: ./mbsdk_custom-py-pkg-training_keras-text-class-stack-overflow-tag/data\n"
     ]
    }
   ],
   "source": [
    "data_dir = download_data(DATA_DIR)\n",
    "print(f\"Data is downloaded to: {data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l7AHtjj3rKoa",
    "outputId": "e50d4c15-2cbf-406c-9ef0-604789fcbe53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md  stack_overflow_16k.tar.gz.tar.gz  test  train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0426 11:08:38.168362460   26897 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    }
   ],
   "source": [
    "!ls $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "puroD_Wr-W4Y",
    "outputId": "6ecbefcd-6244-4159-da29-06f6f3f5bea8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csharp\tjava  javascript  python\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0426 11:08:39.937543232   26897 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    }
   ],
   "source": [
    "!ls $data_dir/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQNL5AjtrKoa"
   },
   "source": [
    "### Prepare CSV Files for Creating Managed Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ewrG6VArKob"
   },
   "source": [
    "#### Create CSV Files using Data Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GvbpGr0ArKob",
    "outputId": "966d143e-2510-40e4-bab6-02dfc98143a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data content is loaded to gs://vertex-ai-devaip-20220426110730/mbsdk_custom-py-pkg-training/keras-text-class-stack-overflow-tag/data/train.csv\n"
     ]
    }
   ],
   "source": [
    "gcs_source_train_url = upload_train_data_to_gcs(\n",
    "    train_data_dir=os.path.join(data_dir, \"train\"),\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    destination_blob_prefix=f\"{GCS_PREFIX}/data\",\n",
    ")\n",
    "\n",
    "print(f\"Train data content is loaded to {gcs_source_train_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gg_OiKJUrKoc",
    "outputId": "4f4fe330-2843-45e2-f5a2-ae1209f4b51b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0426 11:08:49.754808839   26897 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://vertex-ai-devaip-20220426110730/mbsdk_custom-py-pkg-training/keras-text-class-stack-overflow-tag/data/train.csv\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://$BUCKET_NAME/$GCS_PREFIX/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzKx7Z2WrKoc"
   },
   "source": [
    "# Create Custom Training Python Package\n",
    "\n",
    "Before you can perform custom training with a pre-built container, you must create a [Python Source Distribution](https://docs.python.org/3/distutils/sourcedist.html) that contains your training application and upload it to a Cloud Storage bucket that your Google Cloud project can access.\n",
    "\n",
    "We will create a directory and write all of our package build artifacts into that folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "eBDjNpkLrKoc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0426 11:08:54.617965420   26897 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0426 11:08:54.750518808   26897 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    }
   ],
   "source": [
    "PYTHON_PACKAGE_APPLICATION_DIR = f\"{TASK_NAME}/trainer\"\n",
    "\n",
    "!mkdir -p $PYTHON_PACKAGE_APPLICATION_DIR\n",
    "!touch $PYTHON_PACKAGE_APPLICATION_DIR/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPSRvSvMrKod"
   },
   "source": [
    "### Write the Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zupq7mwzrKod",
    "outputId": "b10296a4-1e2d-479a-84f2-6f38a5ae15ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mbsdk_custom-py-pkg-training_keras-text-class-stack-overflow-tag/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PYTHON_PACKAGE_APPLICATION_DIR}/task.py\n",
    "\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "VOCAB_SIZE = 10000\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "\n",
    "def str2bool(v):\n",
    "  if isinstance(v, bool):\n",
    "    return v\n",
    "  if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "    return True\n",
    "  elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "    return False\n",
    "  else:\n",
    "    raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "def build_model(num_classes, loss, optimizer, metrics, vectorize_layer):\n",
    "  # vocab_size is VOCAB_SIZE + 1 since 0 is used additionally for padding.\n",
    "  model = tf.keras.Sequential([\n",
    "      vectorize_layer,\n",
    "      layers.Embedding(VOCAB_SIZE + 1, 64, mask_zero=True),\n",
    "      layers.Conv1D(64, 5, padding=\"valid\", activation=\"relu\", strides=2),\n",
    "      layers.GlobalMaxPooling1D(),\n",
    "      layers.Dense(num_classes),\n",
    "      layers.Activation('softmax')\n",
    "  ])\n",
    "  model.compile(\n",
    "      loss=loss,\n",
    "      optimizer=optimizer,\n",
    "      metrics=metrics)\n",
    "\n",
    "  return model\n",
    "\n",
    "def get_string_labels(predicted_scores_batch, class_names):\n",
    "  predicted_labels = tf.argmax(predicted_scores_batch, axis=1)\n",
    "  predicted_labels = tf.gather(class_names, predicted_labels)\n",
    "  return predicted_labels\n",
    "\n",
    "def predict(export_model, class_names, inputs):\n",
    "  predicted_scores = export_model.predict(inputs)\n",
    "  predicted_labels = get_string_labels(predicted_scores, class_names)\n",
    "  return predicted_labels\n",
    "\n",
    "def parse_args():\n",
    "  parser = argparse.ArgumentParser(\n",
    "      description='Keras Text Classification on Stack Overflow Questions')\n",
    "  parser.add_argument(\n",
    "      '--epochs', default=25, type=int, help='number of training epochs')\n",
    "  parser.add_argument(\n",
    "      '--batch-size', default=16, type=int, help='mini-batch size')\n",
    "  parser.add_argument(\n",
    "      '--model-dir', default=os.getenv('AIP_MODEL_DIR'), type=str, help='model directory')\n",
    "  parser.add_argument(\n",
    "      '--data-dir', default='./data', type=str, help='data directory')\n",
    "  parser.add_argument(\n",
    "      '--test-run', default=False, type=str2bool, help='test run the training application, i.e. 1 epoch for training using sample dataset')\n",
    "  parser.add_argument(\n",
    "      '--model-version', default=1, type=int, help='model version')\n",
    "  args = parser.parse_args()\n",
    "  return args\n",
    "\n",
    "def load_aip_dataset(aip_data_uri_pattern, batch_size, class_names, test_run, shuffle=True, seed=42):\n",
    "\n",
    "  data_file_urls = list()\n",
    "  labels = list()\n",
    "\n",
    "  class_indices = dict(zip(class_names, range(len(class_names))))\n",
    "  num_classes = len(class_names)\n",
    "\n",
    "  for aip_data_uri in tqdm.tqdm(tf.io.gfile.glob(pattern=aip_data_uri_pattern)):\n",
    "    with tf.io.gfile.GFile(name=aip_data_uri, mode='r') as gfile:\n",
    "      for line in gfile.readlines():\n",
    "        line = json.loads(line)\n",
    "        data_file_urls.append(line['textContent'])\n",
    "        classification_annotation = line['classificationAnnotations'][0]\n",
    "        label = classification_annotation['displayName']\n",
    "        labels.append(class_indices[label])\n",
    "        if test_run:\n",
    "          break\n",
    "\n",
    "  data = list()\n",
    "  for data_file_url in tqdm.tqdm(data_file_urls):\n",
    "    with tf.io.gfile.GFile(name=data_file_url, mode='r') as gf:\n",
    "      txt = gf.read()\n",
    "      data.append(txt)\n",
    "\n",
    "  print(f' data files count: {len(data_file_urls)}')\n",
    "  print(f' data count: {len(data)}')\n",
    "  print(f' labels count: {len(labels)}')\n",
    "\n",
    "  dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "  label_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "  label_ds = label_ds.map(lambda x: tf.one_hot(x, num_classes))\n",
    "\n",
    "  dataset = tf.data.Dataset.zip((dataset, label_ds))\n",
    "\n",
    "  if shuffle:\n",
    "    # Shuffle locally at each iteration\n",
    "    dataset = dataset.shuffle(buffer_size=batch_size * 8, seed=seed)\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  # Users may need to reference `class_names`.\n",
    "  dataset.class_names = class_names\n",
    "\n",
    "  return dataset\n",
    "\n",
    "def main():\n",
    "\n",
    "  args = parse_args()\n",
    "\n",
    "  class_names = ['csharp', 'java', 'javascript', 'python']\n",
    "  class_indices = dict(zip(class_names, range(len(class_names))))\n",
    "  num_classes = len(class_names)\n",
    "  print(f' class names: {class_names}')\n",
    "  print(f' class indices: {class_indices}')\n",
    "  print(f' num classes: {num_classes}')\n",
    "\n",
    "  epochs = 1 if args.test_run else args.epochs\n",
    "\n",
    "  aip_model_dir = os.environ.get('AIP_MODEL_DIR')\n",
    "  aip_data_format = os.environ.get('AIP_DATA_FORMAT')\n",
    "  aip_training_data_uri = os.environ.get('AIP_TRAINING_DATA_URI')\n",
    "  aip_validation_data_uri = os.environ.get('AIP_VALIDATION_DATA_URI')\n",
    "  aip_test_data_uri = os.environ.get('AIP_TEST_DATA_URI')\n",
    "\n",
    "  print(f\"aip_model_dir: {aip_model_dir}\")\n",
    "  print(f\"aip_data_format: {aip_data_format}\")\n",
    "  print(f\"aip_training_data_uri: {aip_training_data_uri}\")\n",
    "  print(f\"aip_validation_data_uri: {aip_validation_data_uri}\")\n",
    "  print(f\"aip_test_data_uri: {aip_test_data_uri}\")\n",
    "\n",
    "  print('Loading AIP dataset')\n",
    "  train_ds = load_aip_dataset(\n",
    "      aip_training_data_uri, args.batch_size, class_names, args.test_run)\n",
    "  print('AIP training dataset is loaded')\n",
    "  val_ds = load_aip_dataset(\n",
    "      aip_validation_data_uri, 1, class_names, args.test_run)\n",
    "  print('AIP validation dataset is loaded')\n",
    "  test_ds = load_aip_dataset(\n",
    "      aip_test_data_uri, 1, class_names, args.test_run)\n",
    "  print('AIP test dataset is loaded')\n",
    "\n",
    "  vectorize_layer = TextVectorization(\n",
    "      max_tokens=VOCAB_SIZE,\n",
    "      output_mode='int',\n",
    "      output_sequence_length=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "  train_text = train_ds.map(lambda text, labels: text)\n",
    "  vectorize_layer.adapt(train_text)\n",
    "  print('The vectorize_layer is adapted')\n",
    "\n",
    "\n",
    "  print('Build model')\n",
    "  optimizer = 'adam'\n",
    "  metrics = ['accuracy']\n",
    "\n",
    "  model = build_model(\n",
    "      num_classes, losses.CategoricalCrossentropy(from_logits=True), optimizer, metrics, vectorize_layer)\n",
    "\n",
    "  history = model.fit(train_ds, validation_data=val_ds, epochs=epochs)\n",
    "  history = history.history\n",
    "\n",
    "  print('Training accuracy: {acc}, loss: {loss}'.format(\n",
    "      acc=history['accuracy'][-1], loss=history['loss'][-1]))\n",
    "  print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "      acc=history['val_accuracy'][-1], loss=history['val_loss'][-1]))\n",
    "\n",
    "  loss, accuracy = model.evaluate(test_ds)\n",
    "  print('Test accuracy: {acc}, loss: {loss}'.format(\n",
    "      acc=accuracy, loss=loss))\n",
    "\n",
    "  inputs = [\n",
    "      \"how do I extract keys from a dict into a list?\",  # python\n",
    "      \"debug public static void main(string[] args) {...}\",  # java\n",
    "  ]\n",
    "  predicted_labels = predict(model, class_names, inputs)\n",
    "  for input, label in zip(inputs, predicted_labels):\n",
    "    print(f'Question: {input}')\n",
    "    print(f'Predicted label: {label.numpy()}')\n",
    "\n",
    "  model_export_path = os.path.join(args.model_dir, str(args.model_version))\n",
    "  model.save(model_export_path)\n",
    "  print(f'Model version {args.model_version} is exported to {args.model_dir}')\n",
    "\n",
    "  loaded = tf.saved_model.load(model_export_path)\n",
    "  input_name = list(loaded.signatures['serving_default'].structured_input_signature[1].keys())[0]\n",
    "  print(f'Serving function input: {input_name}')\n",
    "\n",
    "  return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAhynoNBrKoh"
   },
   "source": [
    "### Build Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kXSBQcqdrKof",
    "outputId": "ecebc4da-6737-4cd6-981a-b920b28c82a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./mbsdk_custom-py-pkg-training_keras-text-class-stack-overflow-tag/setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TASK_DIR}/setup.py\n",
    "\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "setup(\n",
    "    name='trainer',\n",
    "    version='0.1',\n",
    "    packages=find_packages(),\n",
    "    install_requires=(),\n",
    "    include_package_data=True,\n",
    "    description='My training application.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MclanW0UrKoh",
    "outputId": "5f61893a-be18-4f57-bbee-88e305a9a4e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data  setup.py\ttrainer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0426 11:09:02.473810698   26897 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    }
   ],
   "source": [
    "!ls $TASK_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j19Nk2vzrKoh",
    "outputId": "c354c7b9-793b-407d-9d2d-7c175b9f77b0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0426 11:09:03.970114205   26897 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "creating trainer.egg-info\n",
      "writing trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to trainer.egg-info/dependency_links.txt\n",
      "writing top-level names to trainer.egg-info/top_level.txt\n",
      "writing manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n",
      "running check\n",
      "warning: check: missing required meta-data: url\n",
      "\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
      "\n",
      "creating trainer-0.1\n",
      "creating trainer-0.1/trainer\n",
      "creating trainer-0.1/trainer.egg-info\n",
      "copying files to trainer-0.1...\n",
      "copying setup.py -> trainer-0.1\n",
      "copying trainer/__init__.py -> trainer-0.1/trainer\n",
      "copying trainer/task.py -> trainer-0.1/trainer\n",
      "copying trainer.egg-info/PKG-INFO -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/SOURCES.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/dependency_links.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/top_level.txt -> trainer-0.1/trainer.egg-info\n",
      "Writing trainer-0.1/setup.cfg\n",
      "creating dist\n",
      "Creating tar archive\n",
      "removing 'trainer-0.1' (and everything under it)\n"
     ]
    }
   ],
   "source": [
    "!cd $TASK_DIR && python3 setup.py sdist --formats=gztar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9c1r_k69rKoi",
    "outputId": "9e9fceac-c69c-4ee8-dad2-28d8ab46c38c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 jupyter jupyter 3084 Apr 26 11:09 ./mbsdk_custom-py-pkg-training_keras-text-class-stack-overflow-tag/dist/trainer-0.1.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0426 11:09:08.031502616   26897 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr $TASK_DIR/dist/trainer-0.1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyQv9AcNrKoi"
   },
   "source": [
    "### Upload the Package to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9WjF8SFrrKoi",
    "outputId": "5276eb27-c0e8-48c3-c057-7375dbc54851"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Training Python Package is uploaded to: gs://vertex-ai-devaip-20220426110730/custom-training-python-package/keras-text-class-stack-overflow-tag/trainer-0.1.tar.gz\n"
     ]
    }
   ],
   "source": [
    "destination_blob_name = f\"custom-training-python-package/{APP_NAME}/trainer-0.1.tar.gz\"\n",
    "source_file_name = f\"{TASK_DIR}/dist/trainer-0.1.tar.gz\"\n",
    "\n",
    "python_package_gcs_uri = upload_blob(\n",
    "    BUCKET_NAME, source_file_name, destination_blob_name\n",
    ")\n",
    "python_module_name = \"trainer.task\"\n",
    "\n",
    "print(f\"Custom Training Python Package is uploaded to: {python_package_gcs_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWJpv4ejrWi0"
   },
   "source": [
    "# Create TensorFlow Serving Container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IH_NAgsaPSij"
   },
   "source": [
    "Create a tag for registering the image and register the image with Cloud Container Registry (gcr.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "pI50mO1bPMS1"
   },
   "outputs": [],
   "source": [
    "TF_SERVING_CONTAINER_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/tf-serving\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGIs4XJ3QFdE"
   },
   "source": [
    "Executes in Vertex AI Workbench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMOad4Z4aOgv"
   },
   "source": [
    "Download the TensorFlow Serving Docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "KC8e1yrHrVyA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0426 13:25:13.355158522   26897 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest: Pulling from tensorflow/serving\n",
      "Digest: sha256:6651f4839e1124dbde75ee531825112af0a6b8ef082c88ab14ca53eb69a2e4bb\n",
      "Status: Image is up to date for tensorflow/serving:latest\n",
      "docker.io/tensorflow/serving:latest\n"
     ]
    }
   ],
   "source": [
    "if not IS_COLAB:\n",
    "    !docker pull tensorflow/serving:latest\n",
    "else:    \n",
    "    # install docker daemon\n",
    "    ! apt-get -qq install docker.io    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "Bw_A7ynTsjTF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0426 13:25:17.470201415   26897 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0426 13:25:17.642741413   26897 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\n",
      "The push refers to repository [gcr.io/vertex-ai-dev/tf-serving]\n",
      "\n",
      "\u001b[1B23850a27: Preparing \n",
      "\u001b[1Ba33781cd: Preparing \n",
      "\u001b[1B89523b17: Preparing \n",
      "\u001b[1Bf28d5f3c: Preparing \n",
      "\u001b[1Bc6d2db45: Preparing \n",
      "\u001b[1Bbacb0351: Preparing \n",
      "\u001b[1Be1acaabc: Layer already exists \u001b[2A\u001b[2K\u001b[1A\u001b[2Klatest: digest: sha256:6651f4839e1124dbde75ee531825112af0a6b8ef082c88ab14ca53eb69a2e4bb size: 1780\n"
     ]
    }
   ],
   "source": [
    "if not IS_COLAB:\n",
    "    !docker tag tensorflow/serving $TF_SERVING_CONTAINER_IMAGE_URI\n",
    "    !docker push $TF_SERVING_CONTAINER_IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GDiMphcNpnb"
   },
   "source": [
    "Executes in Colab\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dR6xGigWNpnc"
   },
   "source": [
    "Configure Docker with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "Jx3d1pEGNpnd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0426 13:25:23.397042923   26897 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/home/jupyter/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"gcr.io\": \"gcloud\",\n",
      "    \"us.gcr.io\": \"gcloud\",\n",
      "    \"eu.gcr.io\": \"gcloud\",\n",
      "    \"asia.gcr.io\": \"gcloud\",\n",
      "    \"staging-k8s.gcr.io\": \"gcloud\",\n",
      "    \"marketplace.gcr.io\": \"gcloud\",\n",
      "    \"us-central1-docker.pkg.dev\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for all GCR repositories.\n",
      "\u001b[1;33mWARNING:\u001b[0m A long list of credential helpers may cause delays running 'docker build'. We recommend passing the registry name to configure only the registry you are using.\n",
      "gcloud credential helpers already registered correctly.\n"
     ]
    }
   ],
   "source": [
    "! gcloud auth configure-docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86_VILTZu-Ck"
   },
   "outputs": [],
   "source": [
    "%%bash -s $IS_COLAB $TF_SERVING_CONTAINER_IMAGE_URI\n",
    "if [ $1 == \"False\" ]; then\n",
    "  exit 0\n",
    "fi\n",
    "set -x\n",
    "dockerd -b none --iptables=0 -l warn &\n",
    "for i in $(seq 5); do [ ! -S \"/var/run/docker.sock\" ] && sleep 2 || break; done\n",
    "docker pull \"tensorflow/serving:latest\"\n",
    "docker tag tensorflow/serving $2\n",
    "docker push $2\n",
    "kill $(jobs -p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rc_MIJwlrKoj"
   },
   "source": [
    "# Run Custom Python Package Training with Managed Text Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiNL0HSVrKoj"
   },
   "source": [
    "## Initialize Vertex SDK for Python\n",
    "\n",
    "Initialize the *client* for Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "J5MqvGtMrKoj"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGT1uT-HrKoj"
   },
   "source": [
    "## Create a Dataset on Vertex AI\n",
    "We will now create a Vertex AI text dataset using the previously prepared csv files. Choose one of the options below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "lY566K2crKok"
   },
   "outputs": [],
   "source": [
    "dataset_display_name = f\"temp-{APP_NAME}-content\"\n",
    "gcs_source = gcs_source_train_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyLoUsx9rKok"
   },
   "source": [
    "#### Option 1: Create a Dataset with CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h7QwC1ThrKok",
    "outputId": "4ff27233-f4e1-4679-b5fe-15ec7b3c4f02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.datasets.dataset:Creating TextDataset\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:Create TextDataset backing LRO: projects/931647533046/locations/us-central1/datasets/778678532837474304/operations/1090142208159383552\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:TextDataset created. Resource name: projects/931647533046/locations/us-central1/datasets/778678532837474304\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:To use this TextDataset in another session:\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:ds = aiplatform.TextDataset('projects/931647533046/locations/us-central1/datasets/778678532837474304')\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:Importing TextDataset data: projects/931647533046/locations/us-central1/datasets/778678532837474304\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:Import TextDataset data backing LRO: projects/931647533046/locations/us-central1/datasets/778678532837474304/operations/2794754667119116288\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:TextDataset data imported. Resource name: projects/931647533046/locations/us-central1/datasets/778678532837474304\n"
     ]
    }
   ],
   "source": [
    "dataset = aiplatform.TextDataset.create(\n",
    "    display_name=dataset_display_name,\n",
    "    gcs_source=gcs_source,\n",
    "    import_schema_uri=aiplatform.schema.dataset.ioformat.text.single_label_classification\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xksBj3M0rKok"
   },
   "source": [
    "#### Option 2: Create a Dataset, then Import CSV File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93w8LGlx7uWy"
   },
   "source": [
    "```\n",
    "dataset = aiplatform.TextDataset.create(\n",
    "    display_name=dataset_display_name,\n",
    ")\n",
    "dataset.import_data(\n",
    "    gcs_source=gcs_source, \n",
    "    import_schema_uri=aiplatform.schema.dataset.ioformat.text.single_label_classification,\n",
    "    sync=False\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncP9QwvkrKok"
   },
   "source": [
    "#### Option 3: Retrieve a Dataset on Vertex AI\n",
    "If you have previously created a Dataset on Vertex AI, you can retrieve the dataset using the `dataset_name`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmiQDoS_7ncz"
   },
   "source": [
    "```\n",
    "dataset_name = 'YOUR DATASET NAME'\n",
    "\n",
    "dataset = aiplatform.TextDataset(dataset_name)\n",
    "dataset.resource_name\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzLvubIrrKon"
   },
   "source": [
    "## Launch a Training Job and Create a Model on Vertex AI\n",
    "\n",
    "We will now train a model with the python package we just built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cyz86ixRYNtT"
   },
   "source": [
    "### Config a Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "9zk8SRuK1Xvi"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = APP_NAME\n",
    "PRE_BUILT_TRAINING_CONTAINER_IMAGE_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxc_yVxkbbbd"
   },
   "source": [
    "You will need to specify the python package that was built and uploaded to GCS, the module name of the python package, the pre-built training container image uri for training, and in this example, we are using TensorFlow serving container for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "uRGrFdxOftD1"
   },
   "outputs": [],
   "source": [
    "job = aiplatform.CustomPythonPackageTrainingJob(\n",
    "    display_name=f\"temp_{TASK_NAME}_tf-serving\",\n",
    "    python_package_gcs_uri=python_package_gcs_uri,\n",
    "    python_module_name=python_module_name,\n",
    "    container_uri=PRE_BUILT_TRAINING_CONTAINER_IMAGE_URI,\n",
    "    model_serving_container_image_uri=TF_SERVING_CONTAINER_IMAGE_URI,\n",
    "    model_serving_container_command=[\"/usr/bin/tensorflow_model_server\"],\n",
    "    model_serving_container_args=[\n",
    "        f\"--model_name={MODEL_NAME}\",\n",
    "        \"--model_base_path=$(AIP_STORAGE_URI)\",\n",
    "        \"--rest_api_port=8080\",\n",
    "        \"--port=8500\",\n",
    "        \"--file_system_poll_wait_seconds=31540000\",\n",
    "    ],\n",
    "    model_serving_container_predict_route=f\"/v1/models/{MODEL_NAME}:predict\",\n",
    "    model_serving_container_health_route=f\"/v1/models/{MODEL_NAME}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPfZGlZeTN72"
   },
   "source": [
    "### Run the Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V6iY5wcerKon",
    "outputId": "c099522a-a608-4775-c994-a523496932f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.training_jobs:Training Output directory:\n",
      "gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378 \n",
      "INFO:google.cloud.aiplatform.training_jobs:No dataset split provided. The service will use a default split.\n",
      "INFO:google.cloud.aiplatform.training_jobs:View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/745651952318152704?project=931647533046\n"
     ]
    }
   ],
   "source": [
    "model = job.run(\n",
    "    dataset=dataset,\n",
    "    annotation_schema_uri=aiplatform.schema.dataset.annotation.text.classification,\n",
    "    args=[\"--epochs\", \"50\"],\n",
    "    replica_count=1,\n",
    "    model_display_name=f\"temp_{TASK_NAME}_tf-serving\",\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zV1PjANLrKoo",
    "outputId": "0bb62bfe-162b-408b-c4fc-78b4a728f1ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/745651952318152704 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/1460708344325996544?project=931647533046\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/745651952318152704 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/745651952318152704 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/745651952318152704 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/745651952318152704 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/745651952318152704 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/745651952318152704 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/745651952318152704 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/745651952318152704 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/745651952318152704 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob run completed. Resource name: projects/931647533046/locations/us-central1/trainingPipelines/745651952318152704\n",
      "INFO:google.cloud.aiplatform.training_jobs:Model available at projects/931647533046/locations/us-central1/models/3192722282317348864\n"
     ]
    }
   ],
   "source": [
    "model.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XO0-zdXxrKoo"
   },
   "source": [
    "# Deploy a Model and Create an Endpoint on Vertex AI\n",
    "\n",
    "Deploy your model, then wait until the model FINISHES deployment before proceeding to prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "tEg2IDwPftD2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
      "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/931647533046/locations/us-central1/endpoints/3380769556992622592/operations/4535395923097812992\n",
      "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/931647533046/locations/us-central1/endpoints/3380769556992622592\n",
      "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
      "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/931647533046/locations/us-central1/endpoints/3380769556992622592')\n",
      "INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/931647533046/locations/us-central1/endpoints/3380769556992622592\n",
      "INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/931647533046/locations/us-central1/endpoints/3380769556992622592/operations/7237555699520110592\n"
     ]
    }
   ],
   "source": [
    "endpoint = model.deploy(machine_type=\"n1-standard-4\", sync=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9AoIJdCHrKop",
    "outputId": "e35f72a5-ea66-44ad-ea4f-e388d32da854"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Endpoint model deployed. Resource name: projects/931647533046/locations/us-central1/endpoints/3380769556992622592\n"
     ]
    }
   ],
   "source": [
    "endpoint.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z87MXJEDrKoq"
   },
   "source": [
    "## Predict on the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vQXGCg8IrKoq",
    "outputId": "68a6d6d7-13f7-4899-82ff-99bb6a0f7779"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Indices: {'csharp': 0, 'java': 1, 'javascript': 2, 'python': 3}\n",
      "Class Maps:    {0: 'csharp', 1: 'java', 2: 'javascript', 3: 'python'}\n"
     ]
    }
   ],
   "source": [
    "class_names = [\"csharp\", \"java\", \"javascript\", \"python\"]\n",
    "\n",
    "class_ids = range(len(class_names))\n",
    "\n",
    "class_indices = dict(zip(class_names, class_ids))\n",
    "class_maps = dict(zip(class_ids, class_names))\n",
    "print(f\"Class Indices: {class_indices}\")\n",
    "print(f\"Class Maps:    {class_maps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "Di8RtRxipm63"
   },
   "outputs": [],
   "source": [
    "text_inputs = [\n",
    "    \"how do I extract keys from a dict into a list?\",  # python\n",
    "    \"debug public static void main(string[] args) {...}\",  # java\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dPHYGAlvlTAj",
    "outputId": "ae9e1e27-3955-424b-ea03-3bf94cde4136"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: how do I extract keys from a dict into a list?\n",
      "Predicted Tag: python\n",
      "\n",
      "Question: debug public static void main(string[] args) {...}\n",
      "Predicted Tag: java\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = endpoint.predict(instances=text_inputs)\n",
    "for text, predicted_scores in zip(text_inputs, predictions.predictions):\n",
    "    class_id = np.argmax(predicted_scores)\n",
    "    class_name = class_maps[class_id]\n",
    "    print(f\"Question: {text}\")\n",
    "    print(f\"Predicted Tag: {class_name}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRkvzr2-LgA9"
   },
   "source": [
    "# Batch Prediction Job on the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "nlzRioF7UGFd"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def upload_test_data_to_gcs(test_data_dir, test_gcs_url):\n",
    "    \"\"\"Create JSON file using test data content.\"\"\"\n",
    "\n",
    "    input_name = \"text_vectorization_input\"\n",
    "\n",
    "    with tf.io.gfile.GFile(test_gcs_url, \"w\") as gf:\n",
    "\n",
    "        for root, _, files in os.walk(test_data_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(\".txt\"):\n",
    "                    file_fn = os.path.join(root, file)\n",
    "                    with open(file_fn, \"r\") as f:\n",
    "                        content = f.readlines()\n",
    "                        lines = [x.strip().strip('\"') for x in content]\n",
    "\n",
    "                        data = {input_name: lines[0]}\n",
    "                        gf.write(json.dumps(data))\n",
    "                        gf.write(\"\\n\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C_sbOm-5ud5C",
    "outputId": "7fbdd23c-e7d9-42fc-90d6-a8eadc4e421c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data content is loaded to gs://vertex-ai-devaip-20220426110730/mbsdk_custom-py-pkg-training/keras-text-class-stack-overflow-tag/data/test.json\n"
     ]
    }
   ],
   "source": [
    "gcs_source_test_url = f\"gs://{BUCKET_NAME}/{GCS_PREFIX}/data/test.json\"\n",
    "upload_test_data_to_gcs(\n",
    "    test_data_dir=os.path.join(data_dir, \"test\"), test_gcs_url=gcs_source_test_url\n",
    ")\n",
    "\n",
    "print(f\"Test data content is loaded to {gcs_source_test_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QBMBk2WxLqBP",
    "outputId": "037e343d-c675-4d64-a0a0-16a19f28db77"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0426 12:15:19.893240533   26897 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://vertex-ai-devaip-20220426110730/mbsdk_custom-py-pkg-training/keras-text-class-stack-overflow-tag/data/test.json\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $gcs_source_test_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2JhoTiD5LuDW",
    "outputId": "34b86603-0a36-4d6e-acca-38a6b3134c5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:Creating BatchPredictionJob\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob created. Resource name: projects/931647533046/locations/us-central1/batchPredictionJobs/5896081076169736192\n",
      "INFO:google.cloud.aiplatform.jobs:To use this BatchPredictionJob in another session:\n",
      "INFO:google.cloud.aiplatform.jobs:bpj = aiplatform.BatchPredictionJob('projects/931647533046/locations/us-central1/batchPredictionJobs/5896081076169736192')\n",
      "INFO:google.cloud.aiplatform.jobs:View Batch Prediction Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/batch-predictions/5896081076169736192?project=931647533046\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/931647533046/locations/us-central1/batchPredictionJobs/5896081076169736192 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "batch_predict_job = model.batch_predict(\n",
    "    job_display_name=f\"temp_{TASK_NAME}_tf-serving\",\n",
    "    gcs_source=gcs_source_test_url,\n",
    "    gcs_destination_prefix=f\"gs://{BUCKET_NAME}/{GCS_PREFIX}/batch_prediction\",\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "retsI3LLls_W",
    "outputId": "af9d1266-1b2e-473e-fd7b-fe8530347c0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/931647533046/locations/us-central1/batchPredictionJobs/5896081076169736192 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/931647533046/locations/us-central1/batchPredictionJobs/5896081076169736192 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/931647533046/locations/us-central1/batchPredictionJobs/5896081076169736192 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/931647533046/locations/us-central1/batchPredictionJobs/5896081076169736192 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/931647533046/locations/us-central1/batchPredictionJobs/5896081076169736192 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/931647533046/locations/us-central1/batchPredictionJobs/5896081076169736192 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/931647533046/locations/us-central1/batchPredictionJobs/5896081076169736192 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/931647533046/locations/us-central1/batchPredictionJobs/5896081076169736192 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob run completed. Resource name: projects/931647533046/locations/us-central1/batchPredictionJobs/5896081076169736192\n"
     ]
    }
   ],
   "source": [
    "batch_predict_job.wait()\n",
    "bp_iter_outputs = batch_predict_job.iter_outputs()\n",
    "\n",
    "prediction_errors_stats = list()\n",
    "prediction_results = list()\n",
    "for blob in bp_iter_outputs:\n",
    "    if blob.name.split(\"/\")[-1].startswith(\"prediction.errors_stats\"):\n",
    "        prediction_errors_stats.append(blob.name)\n",
    "    if blob.name.split(\"/\")[-1].startswith(\"prediction.results\"):\n",
    "        prediction_results.append(blob.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "KmfK3Tzzlv9C"
   },
   "outputs": [],
   "source": [
    "tags = list()\n",
    "for prediction_result in prediction_results:\n",
    "    gfile_name = f\"gs://{bp_iter_outputs.bucket.name}/{prediction_result}\"\n",
    "    with tf.io.gfile.GFile(name=gfile_name, mode=\"r\") as gfile:\n",
    "        for line in gfile.readlines():\n",
    "            line = json.loads(line)\n",
    "            text = line[\"instance\"][\"text_vectorization_input\"][0]\n",
    "            prediction = line[\"prediction\"]\n",
    "            class_id = np.argmax(prediction)\n",
    "            class_name = class_maps[class_id]\n",
    "            tags.append([text, class_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "UcMQ-daLn_oh",
    "outputId": "d32ead26-2d39-4568-9cee-65a55459d322"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>w</td>\n",
       "      <td>java</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p</td>\n",
       "      <td>csharp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t</td>\n",
       "      <td>csharp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>h</td>\n",
       "      <td>csharp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p</td>\n",
       "      <td>csharp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  question     tag\n",
       "0        w    java\n",
       "1        p  csharp\n",
       "2        t  csharp\n",
       "3        h  csharp\n",
       "4        p  csharp"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tags_df = pd.DataFrame(tags, columns=[\"question\", \"tag\"])\n",
    "tags_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QUV_PHjtoCpQ",
    "outputId": "e6163552-b889-431e-fae0-f7726e2130ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "python        2113\n",
       "javascript    2019\n",
       "java          1962\n",
       "csharp        1906\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_df[\"tag\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1b181a81eb61"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9e7688063ccd",
    "outputId": "7f423f97-db82-4ba6-ea54-de20d763e840"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.base:Deleting TextDataset : projects/931647533046/locations/us-central1/datasets/778678532837474304\n",
      "INFO:google.cloud.aiplatform.base:Delete TextDataset  backing LRO: projects/931647533046/locations/us-central1/operations/8049329532353642496\n",
      "INFO:google.cloud.aiplatform.base:TextDataset deleted. . Resource name: projects/931647533046/locations/us-central1/datasets/778678532837474304\n",
      "INFO:google.cloud.aiplatform.models:Undeploying Endpoint model: projects/931647533046/locations/us-central1/endpoints/3380769556992622592\n",
      "INFO:google.cloud.aiplatform.models:Undeploy Endpoint model backing LRO: projects/931647533046/locations/us-central1/endpoints/3380769556992622592/operations/3942046672191750144\n",
      "INFO:google.cloud.aiplatform.models:Endpoint model undeployed. Resource name: projects/931647533046/locations/us-central1/endpoints/3380769556992622592\n",
      "INFO:google.cloud.aiplatform.base:Deleting Endpoint : projects/931647533046/locations/us-central1/endpoints/3380769556992622592\n",
      "INFO:google.cloud.aiplatform.base:Delete Endpoint  backing LRO: projects/931647533046/locations/us-central1/operations/4986881785741705216\n",
      "INFO:google.cloud.aiplatform.base:Endpoint deleted. . Resource name: projects/931647533046/locations/us-central1/endpoints/3380769556992622592\n",
      "INFO:google.cloud.aiplatform.base:Deleting Model : projects/931647533046/locations/us-central1/models/3192722282317348864\n",
      "INFO:google.cloud.aiplatform.base:Delete Model  backing LRO: projects/931647533046/locations/us-central1/operations/2284722009319407616\n",
      "INFO:google.cloud.aiplatform.base:Model deleted. . Resource name: projects/931647533046/locations/us-central1/models/3192722282317348864\n",
      "INFO:google.cloud.aiplatform.base:Deleting CustomPythonPackageTrainingJob : projects/931647533046/locations/us-central1/trainingPipelines/745651952318152704\n",
      "INFO:google.cloud.aiplatform.base:Delete CustomPythonPackageTrainingJob  backing LRO: projects/931647533046/locations/us-central1/operations/2681038776528011264\n",
      "INFO:google.cloud.aiplatform.base:CustomPythonPackageTrainingJob deleted. . Resource name: projects/931647533046/locations/us-central1/trainingPipelines/745651952318152704\n",
      "INFO:google.cloud.aiplatform.base:Deleting BatchPredictionJob : projects/931647533046/locations/us-central1/batchPredictionJobs/5896081076169736192\n",
      "INFO:google.cloud.aiplatform.base:Delete BatchPredictionJob  backing LRO: projects/931647533046/locations/us-central1/operations/7292724794955399168\n",
      "INFO:google.cloud.aiplatform.base:BatchPredictionJob deleted. . Resource name: projects/931647533046/locations/us-central1/batchPredictionJobs/5896081076169736192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0426 13:21:40.808820535   26897 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/dataset-778678532837474304-text_classification_multi_label-2022-04-26T11:37:38.212669Z/test-00000-of-00003.jsonl#1650973059266544...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/dataset-778678532837474304-text_classification_multi_label-2022-04-26T11:37:38.212669Z/test-00001-of-00003.jsonl#1650973059135365...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/dataset-778678532837474304-text_classification_multi_label-2022-04-26T11:37:38.212669Z/test-00002-of-00003.jsonl#1650973059206805...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/dataset-778678532837474304-text_classification_multi_label-2022-04-26T11:37:38.212669Z/training-00000-of-00022.jsonl#1650973058510310...\n",
      "/ [4 objects]                                                                   \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m rm ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/dataset-778678532837474304-text_classification_multi_label-2022-04-26T11:37:38.212669Z/training-00001-of-00022.jsonl#1650973058468652...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/dataset-778678532837474304-text_classification_multi_label-2022-04-26T11:37:38.212669Z/training-00002-of-00022.jsonl#1650973058499343...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/dataset-778678532837474304-text_classification_multi_label-2022-04-26T11:37:38.212669Z/training-00003-of-00022.jsonl#1650973058489895...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/dataset-778678532837474304-text_classification_multi_label-2022-04-26T11:37:38.212669Z/training-00004-of-00022.jsonl#1650973058495992...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/dataset-778678532837474304-text_classification_multi_label-2022-04-26T11:37:38.212669Z/training-00005-of-00022.jsonl#1650973058483968...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/dataset-778678532837474304-text_classification_multi_label-2022-04-26T11:37:38.212669Z/training-00006-of-00022.jsonl#1650973058465106...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/dataset-778678532837474304-text_classification_multi_label-2022-04-26T11:37:38.212669Z/training-00007-of-00022.jsonl#1650973058492702...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/dataset-778678532837474304-text_classification_multi_label-2022-04-26T11:37:38.212669Z/training-00008-of-00022.jsonl#1650973058518091...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/dataset-778678532837474304-text_classification_multi_label-2022-04-26T11:37:38.212669Z/training-00009-of-00022.jsonl#1650973058499831...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/dataset-778678532837474304-text_classification_multi_label-2022-04-26T11:37:38.212669Z/training-00010-of-00022.jsonl#1650973058829466...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/dataset-778678532837474304-text_classification_multi_label-2022-04-26T11:37:38.212669Z/training-00011-of-00022.jsonl#1650973058784743...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/dataset-778678532837474304-text_classification_multi_label-2022-04-26T11:37:38.212669Z/training-00012-of-00022.jsonl#1650973059213933...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/dataset-778678532837474304-text_classification_multi_label-2022-04-26T11:37:38.212669Z/training-00013-of-00022.jsonl#1650973058877244...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/dataset-778678532837474304-text_classification_multi_label-2022-04-26T11:37:38.212669Z/training-00014-of-00022.jsonl#1650973058812978...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/dataset-778678532837474304-text_classification_multi_label-2022-04-26T11:37:38.212669Z/training-00015-of-00022.jsonl#1650973058842935...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/dataset-778678532837474304-text_classification_multi_label-2022-04-26T11:37:38.212669Z/training-00016-of-00022.jsonl#1650973059187985...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/dataset-778678532837474304-text_classification_multi_label-2022-04-26T11:37:38.212669Z/training-00017-of-00022.jsonl#1650973058894844...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/dataset-778678532837474304-text_classification_multi_label-2022-04-26T11:37:38.212669Z/training-00018-of-00022.jsonl#1650973058865261...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/dataset-778678532837474304-text_classification_multi_label-2022-04-26T11:37:38.212669Z/training-00019-of-00022.jsonl#1650973058878747...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/dataset-778678532837474304-text_classification_multi_label-2022-04-26T11:37:38.212669Z/training-00020-of-00022.jsonl#1650973059048171...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/dataset-778678532837474304-text_classification_multi_label-2022-04-26T11:37:38.212669Z/training-00021-of-00022.jsonl#1650973059109983...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/dataset-778678532837474304-text_classification_multi_label-2022-04-26T11:37:38.212669Z/validation-00000-of-00003.jsonl#1650973059162336...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/dataset-778678532837474304-text_classification_multi_label-2022-04-26T11:37:38.212669Z/validation-00001-of-00003.jsonl#1650973059209270...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/dataset-778678532837474304-text_classification_multi_label-2022-04-26T11:37:38.212669Z/validation-00002-of-00003.jsonl#1650973059234534...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/model/#1650974543651226...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/model/1/#1650974543786139...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/model/1/assets/#1650974545948720...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/model/1/keras_metadata.pb#1650974546516304...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/model/1/saved_model.pb#1650974546305655...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/model/1/variables/#1650974543921772...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/model/1/variables/variables.data-00000-of-00001#1650974545324980...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/aiplatform-custom-training-2022-04-26-11:37:35.378/model/1/variables/variables.index#1650974545524139...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/custom-training-python-package/keras-text-class-stack-overflow-tag/trainer-0.1.tar.gz#1650971350848078...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/mbsdk_custom-py-pkg-training/keras-text-class-stack-overflow-tag/batch_prediction/prediction-temp_mbsdk_custom-py-pkg-training_keras-text-class-stack-overflow-tag_tf-serving-2022_04_26T05_15_24_509Z/prediction.errors_stats-00000-of-00001#1650976397363514...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/mbsdk_custom-py-pkg-training/keras-text-class-stack-overflow-tag/batch_prediction/prediction-temp_mbsdk_custom-py-pkg-training_keras-text-class-stack-overflow-tag_tf-serving-2022_04_26T05_15_24_509Z/prediction.results-00000-of-00006#1650976394310259...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/mbsdk_custom-py-pkg-training/keras-text-class-stack-overflow-tag/batch_prediction/prediction-temp_mbsdk_custom-py-pkg-training_keras-text-class-stack-overflow-tag_tf-serving-2022_04_26T05_15_24_509Z/prediction.results-00001-of-00006#1650976394313623...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/mbsdk_custom-py-pkg-training/keras-text-class-stack-overflow-tag/batch_prediction/prediction-temp_mbsdk_custom-py-pkg-training_keras-text-class-stack-overflow-tag_tf-serving-2022_04_26T05_15_24_509Z/prediction.results-00002-of-00006#1650976394310183...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/mbsdk_custom-py-pkg-training/keras-text-class-stack-overflow-tag/batch_prediction/prediction-temp_mbsdk_custom-py-pkg-training_keras-text-class-stack-overflow-tag_tf-serving-2022_04_26T05_15_24_509Z/prediction.results-00003-of-00006#1650976394307820...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/mbsdk_custom-py-pkg-training/keras-text-class-stack-overflow-tag/batch_prediction/prediction-temp_mbsdk_custom-py-pkg-training_keras-text-class-stack-overflow-tag_tf-serving-2022_04_26T05_15_24_509Z/prediction.results-00004-of-00006#1650976394312622...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/mbsdk_custom-py-pkg-training/keras-text-class-stack-overflow-tag/batch_prediction/prediction-temp_mbsdk_custom-py-pkg-training_keras-text-class-stack-overflow-tag_tf-serving-2022_04_26T05_15_24_509Z/prediction.results-00005-of-00006#1650976394318760...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/mbsdk_custom-py-pkg-training/keras-text-class-stack-overflow-tag/data/test.json#1650975317047541...\n",
      "Removing gs://vertex-ai-devaip-20220426110730/mbsdk_custom-py-pkg-training/keras-text-class-stack-overflow-tag/data/train.csv#1650971326035872...\n",
      "/ [46 objects]                                                                  \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m rm ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "\n",
      "Operation completed over 46 objects.                                             \n",
      "Removing gs://vertex-ai-devaip-20220426110730/...\n"
     ]
    }
   ],
   "source": [
    "delete_bucket = False\n",
    "\n",
    "# Delete the dataset using the Vertex dataset object\n",
    "dataset.delete()\n",
    "\n",
    "# Undeploy model from the endpoint\n",
    "endpoint.undeploy_all()\n",
    "\n",
    "# Delete the endpoint\n",
    "endpoint.delete()\n",
    "\n",
    "# Delete the model using the Vertex model object\n",
    "model.delete()\n",
    "\n",
    "# Delete the AutoML or Pipeline training job\n",
    "job.delete()\n",
    "\n",
    "# Delete the batch prediction job using the Vertex batch prediction object\n",
    "batch_predict_job.delete()\n",
    "\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil rm -r $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "bcKMGrqDrKoS",
    "q3UDGt8MrKoY",
    "bQNL5AjtrKoa",
    "9ewrG6VArKob",
    "eiNL0HSVrKoj",
    "KGT1uT-HrKoj",
    "kyLoUsx9rKok",
    "xksBj3M0rKok",
    "ncP9QwvkrKok",
    "Cyz86ixRYNtT"
   ],
   "name": "SDK_Custom_Training_Python_Package_Managed_Text_Dataset_Tensorflow_Serving_Container_(3).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m89",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m89"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6b56b1c7b76"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c414a395a19b"
      },
      "source": [
        "# PyTorch Image Classification Multi-Node Distributed Data Parallel Training on CPU using Vertex Training with Custom Container"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b98238e32cf7"
      },
      "source": [
        "<table align=\"left\">\n",
        "<td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pytorch_image_classification_distributed_data_parallel_training_with_vertex_sdk/multi_node_ddp_gloo_vertex_training_with_custom_container.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pytorch_image_classification_distributed_data_parallel_training_with_vertex_sdk/multi_node_ddp_gloo_vertex_training_with_custom_container.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>                                                                                               <td>\n",
        "    <a href=\"https://console.cloud.google.com/ai/platform/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pytorch_image_classification_distributed_data_parallel_training_with_vertex_sdk/multi_node_ddp_gloo_vertex_training_with_custom_container.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "196084857666"
      },
      "source": [
        "## Overview\n",
        "\n",
        "\n",
        "This tutorial demonstrates how to create a distributed PyTorch training job using Vertex AI SDK for Python and custom containers. This can help your training job scale to handle a large amount of data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ee27912fc7e"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset used for this tutorial is the <a href=\"http://yann.lecun.com/exdb/mnist/\">MNIST database</a>. The MNIST database of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdc10b671701"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to create a distributed PyTorch training job using Vertex AI SDK for Python and custom containers. You will set up GCP to use a custom container, a Vertex Tensorboard Instance and run a custom training job. \n",
        "\n",
        "This tutorial uses the following Google Cloud ML services:\n",
        "\n",
        "- `Vertex AI SDK`\n",
        "- `Vertex AI TensorBoard`\n",
        "- `CustomContainerTrainingJob`\n",
        "- `Artifact Registry`\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Setting up your GCP project : Setting up the PROJECT_ID, REGION & SERVICE_ACCOUNT\n",
        "- Creating a cloud storage bucket\n",
        "- Building Custom Container using Artifact Registry and Docker\n",
        "- Create a Vertex Tensorboard Instance to store your Vertex AI experiment\n",
        "- Run a Vertex AI SDK CustomContainerTrainingJob\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72ce3c3e56b3"
      },
      "source": [
        "## Costs\n",
        " \n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "\n",
        "* Cloud Storage\n",
        "\n",
        "* Vertex AI TensorBoard (Note that Vertex AI TensorBoard charges a monthly fee of $300 per unique active user. Active users are measured through the Vertex AI TensorBoard UI. You also pay for Google Cloud resources you use with Vertex AI TensorBoard, such as TensorBoard logs stored in Cloud Storage. <a href='https://cloud.google.com/vertex-ai/pricing#tensorboard'>Check the link for latest prices.</a>)\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/),\n",
        "        to generate a cost estimate based on your projected usage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03d216c7f7b1"
      },
      "source": [
        "### Install additional packages\n",
        "\n",
        "Install the latest version of Vertex AI SDK for Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6d015287b38d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4acbeefa0e09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip3 install {USER_FLAG} --upgrade google-cloud-aiplatform -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35942e320683"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "a7e5937088c8"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80f70e8aa911"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`.! pip3 install {USER_FLAG} --upgrade google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3989ed5bea75"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5bf9979b96ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project ID: vertex-ai-dev\n"
          ]
        }
      ],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID:\", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "09021c90b34c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ],
      "source": [
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88dd74c4c84e"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
        "\n",
        "- Americas: `us-central1`\n",
        "- Europe: `europe-west4`\n",
        "- Asia Pacific: `asia-east1`\n",
        "\n",
        "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
        "\n",
        "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5c615e53149f"
      },
      "outputs": [],
      "source": [
        "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
        "\n",
        "if REGION == \"[your-region]\":\n",
        "    REGION = \"us-central1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "870777863e09"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using Google Cloud Notebooks**, your environment is already\n",
        "authenticated. Skip this step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "378e70541ba9"
      },
      "source": [
        "**If you are using Colab**, run the cell below and follow the instructions\n",
        "when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "1. In the Cloud Console, go to the [**Create service account key**\n",
        "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
        "\n",
        "2. Click **Create service account**.\n",
        "\n",
        "3. In the **Service account name** field, enter a name, and\n",
        "   click **Create**.\n",
        "\n",
        "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
        "into the filter box, and select\n",
        "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "\n",
        "5. Click *Create*. A JSON file that contains your key downloads to your\n",
        "local environment.\n",
        "\n",
        "6. Enter the path to your service account key as the\n",
        "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eab0094dbed5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# If on Google Cloud Notebook, then don't execute this code\n",
        "IS_COLAB = False\n",
        "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        IS_COLAB = True\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d98e540b8e3f"
      },
      "source": [
        "#### Service Account\n",
        "If you don't know your service account, try to get your service account using gcloud command by executing the second cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "c5ac73516218"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Service Account: 931647533046-compute@developer.gserviceaccount.com\n"
          ]
        }
      ],
      "source": [
        "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}\n",
        "\n",
        "if (\n",
        "    SERVICE_ACCOUNT == \"\"\n",
        "    or SERVICE_ACCOUNT is None\n",
        "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
        "):\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = !gcloud auth list 2>/dev/null\n",
        "    SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
        "    print(\"Service Account:\", SERVICE_ACCOUNT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "454cb4f7a9e9"
      },
      "source": [
        "#### Timestamp\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "a8c17026c384"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37aa2089e5a5"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "When you initialize the Vertex AI SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ffd7beb19fd0"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6f9da502010b"
      },
      "outputs": [],
      "source": [
        "if BUCKET_URI == \"\" or BUCKET_URI is None or BUCKET_URI == \"gs://[your-bucket-name]\":\n",
        "    BUCKET_URI = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15bb44ff961c"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fde15c57652b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating gs://vertex-ai-devaip-20220802103935/...\n"
          ]
        }
      ],
      "source": [
        "! gsutil mb -l $REGION $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bba6423a764"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0b5ae674177e"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d48860504181"
      },
      "source": [
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "40b9227cb6a1"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "content_name = \"pt-img-cls-multi-node-ddp-cust-cont\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12c03eca5908"
      },
      "source": [
        "# Create Custom Training Python Package\n",
        "\n",
        "Before you can perform local training, you must create source code file, requirements file, docker file.\n",
        "\n",
        "You will create a directory and write all of our files into that folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9f058ed522e8"
      },
      "outputs": [],
      "source": [
        "PYTHON_PACKAGE_APPLICATION_DIR = \"trainer\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5bc3955a567b"
      },
      "outputs": [],
      "source": [
        "!mkdir -p $PYTHON_PACKAGE_APPLICATION_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "436cc7d0197e"
      },
      "source": [
        "### Write the Training Script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "34d5b6e85332"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing trainer/task.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {PYTHON_PACKAGE_APPLICATION_DIR}/task.py\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Main program for PyTorch distributed training.\n",
        "Adapted from: https://github.com/narumiruna/pytorch-distributed-example\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "import torch\n",
        "from torch import distributed\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "from torch.utils import data\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "def parse_args():\n",
        "\n",
        "  parser = argparse.ArgumentParser()\n",
        "\n",
        "  # Using environment variables for Cloud Storage directories\n",
        "  # see more details in https://cloud.google.com/vertex-ai/docs/training/code-requirements\n",
        "  parser.add_argument(\n",
        "      '--model-dir', default=os.getenv('AIP_MODEL_DIR'), type=str,\n",
        "      help='a Cloud Storage URI of a directory intended for saving model artifacts')\n",
        "  parser.add_argument(\n",
        "      '--tensorboard-log-dir', default=os.getenv('AIP_TENSORBOARD_LOG_DIR'), type=str,\n",
        "      help='a Cloud Storage URI of a directory intended for saving TensorBoard')\n",
        "  parser.add_argument(\n",
        "      '--checkpoint-dir', default=os.getenv('AIP_CHECKPOINT_DIR'), type=str,\n",
        "      help='a Cloud Storage URI of a directory intended for saving checkpoints')\n",
        "\n",
        "  parser.add_argument(\n",
        "      '--backend', type=str, default='gloo',\n",
        "      help='Use the `nccl` backend for distributed GPU training.'\n",
        "           'Use the `gloo` backend for distributed CPU training.')\n",
        "  parser.add_argument(\n",
        "      '--init-method', type=str, default='env://',\n",
        "      help='URL specifying how to initialize the package.')\n",
        "  parser.add_argument(\n",
        "      '--world-size', type=int, default=os.environ.get('WORLD_SIZE', 1),\n",
        "      help='The total number of nodes in the cluster. '\n",
        "           'This variable has the same value on every node.')\n",
        "  parser.add_argument(\n",
        "      '--rank', type=int, default=os.environ.get('RANK', 0),\n",
        "      help='A unique identifier for each node. '\n",
        "           'On the master worker, this is set to 0. '\n",
        "           'On each worker, it is set to a different value from 1 to WORLD_SIZE - 1.')\n",
        "  parser.add_argument(\n",
        "      '--epochs', type=int, default=20)\n",
        "  parser.add_argument(\n",
        "      '--no-cuda', action='store_true')\n",
        "  parser.add_argument(\n",
        "      '-lr', '--learning-rate', type=float, default=1e-3)\n",
        "  parser.add_argument(\n",
        "      '--batch-size', type=int, default=128)\n",
        "  parser.add_argument(\n",
        "      '--local-mode', action='store_true', help='use local mode when running on your local machine')\n",
        "\n",
        "  args = parser.parse_args()\n",
        "\n",
        "  return args\n",
        "\n",
        "def makedirs(model_dir):\n",
        "  if os.path.exists(model_dir) and os.path.isdir(model_dir):\n",
        "    shutil.rmtree(model_dir)\n",
        "  os.makedirs(model_dir)\n",
        "  return\n",
        "\n",
        "def distributed_is_initialized():\n",
        "  if distributed.is_available():\n",
        "    if distributed.is_initialized():\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "class Average(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.sum = 0\n",
        "    self.count = 0\n",
        "\n",
        "  def __str__(self):\n",
        "    return '{:.6f}'.format(self.average)\n",
        "\n",
        "  @property\n",
        "  def average(self):\n",
        "    return self.sum / self.count\n",
        "\n",
        "  def update(self, value, number):\n",
        "    self.sum += value * number\n",
        "    self.count += number\n",
        "\n",
        "class Accuracy(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.correct = 0\n",
        "    self.count = 0\n",
        "\n",
        "  def __str__(self):\n",
        "    return '{:.2f}%'.format(self.accuracy * 100)\n",
        "\n",
        "  @property\n",
        "  def accuracy(self):\n",
        "    return self.correct / self.count\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def update(self, output, target):\n",
        "    pred = output.argmax(dim=1)\n",
        "    correct = pred.eq(target).sum().item()\n",
        "\n",
        "    self.correct += correct\n",
        "    self.count += output.size(0)\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, device):\n",
        "    super(Net, self).__init__()\n",
        "    self.fc = torch.nn.Linear(784, 10).to(device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.fc(x.view(x.size(0), -1))\n",
        "\n",
        "class MNISTDataLoader(data.DataLoader):\n",
        "\n",
        "  def __init__(self, root, batch_size, train=True):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,)),\n",
        "    ])\n",
        "\n",
        "    dataset = datasets.MNIST(root, train=train, transform=transform, download=True)\n",
        "    sampler = None\n",
        "    if train and distributed_is_initialized():\n",
        "      sampler = data.DistributedSampler(dataset)\n",
        "\n",
        "    super(MNISTDataLoader, self).__init__(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=(sampler is None),\n",
        "        sampler=sampler,\n",
        "    )\n",
        "\n",
        "class Trainer(object):\n",
        "\n",
        "  def __init__(self,\n",
        "      model,\n",
        "      optimizer,\n",
        "      train_loader,\n",
        "      test_loader,\n",
        "      device,\n",
        "      model_name,\n",
        "      checkpoint_path\n",
        "  ):\n",
        "    self.model = model\n",
        "    self.optimizer = optimizer\n",
        "    self.train_loader = train_loader\n",
        "    self.test_loader = test_loader\n",
        "    self.device = device\n",
        "    self.model_name = model_name\n",
        "    self.checkpoint_path = checkpoint_path\n",
        "\n",
        "  def save(self, model_dir):\n",
        "    model_path = os.path.join(model_dir, self.model_name)\n",
        "    torch.save(self.model.state_dict(), model_path)\n",
        "\n",
        "  def fit(self, epochs, is_chief, writer):\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "      print('Epoch: {}, Training ...'.format(epoch))\n",
        "      train_loss, train_acc = self.train()\n",
        "\n",
        "      if is_chief:\n",
        "        test_loss, test_acc = self.evaluate()\n",
        "        writer.add_scalar('Loss/train', train_loss.average, epoch)\n",
        "        writer.add_scalar('Loss/test', test_loss.average, epoch)\n",
        "        writer.add_scalar('Accuracy/train', train_acc.accuracy, epoch)\n",
        "        writer.add_scalar('Accuracy/test', test_acc.accuracy, epoch)\n",
        "        torch.save(self.model.state_dict(), self.checkpoint_path)\n",
        "\n",
        "        print(\n",
        "            'Epoch: {}/{},'.format(epoch, epochs),\n",
        "            'train loss: {}, train acc: {},'.format(train_loss, train_acc),\n",
        "            'test loss: {}, test acc: {}.'.format(test_loss, test_acc),\n",
        "        )\n",
        "\n",
        "  def train(self):\n",
        "\n",
        "    self.model.train()\n",
        "\n",
        "    train_loss = Average()\n",
        "    train_acc = Accuracy()\n",
        "\n",
        "    for data, target in self.train_loader:\n",
        "      data = data.to(self.device)\n",
        "      target = target.to(self.device)\n",
        "\n",
        "      output = self.model(data)\n",
        "      loss = torch.nn.functional.cross_entropy(output, target)\n",
        "\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "      train_loss.update(loss.item(), data.size(0))\n",
        "      train_acc.update(output, target)\n",
        "\n",
        "    return train_loss, train_acc\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def evaluate(self):\n",
        "    self.model.eval()\n",
        "\n",
        "    test_loss = Average()\n",
        "    test_acc = Accuracy()\n",
        "\n",
        "    for data, target in self.test_loader:\n",
        "      data = data.to(self.device)\n",
        "      target = target.to(self.device)\n",
        "\n",
        "      output = self.model(data)\n",
        "      loss = torch.nn.functional.cross_entropy(output, target)\n",
        "\n",
        "      test_loss.update(loss.item(), data.size(0))\n",
        "      test_acc.update(output, target)\n",
        "\n",
        "    return test_loss, test_acc\n",
        "\n",
        "def main():\n",
        "\n",
        "  args = parse_args()\n",
        "\n",
        "  local_data_dir = './tmp/data'\n",
        "  local_model_dir = './tmp/model'\n",
        "  local_tensorboard_log_dir = './tmp/logs'\n",
        "  local_checkpoint_dir = './tmp/checkpoints'\n",
        "\n",
        "  model_dir = args.model_dir or local_model_dir\n",
        "  tensorboard_log_dir = args.tensorboard_log_dir or local_tensorboard_log_dir\n",
        "  checkpoint_dir = args.checkpoint_dir or local_checkpoint_dir\n",
        "\n",
        "  gs_prefix = 'gs://'\n",
        "  gcsfuse_prefix = '/gcs/'\n",
        "  if model_dir and model_dir.startswith(gs_prefix):\n",
        "    model_dir = model_dir.replace(gs_prefix, gcsfuse_prefix)\n",
        "  if tensorboard_log_dir and tensorboard_log_dir.startswith(gs_prefix):\n",
        "    tensorboard_log_dir = tensorboard_log_dir.replace(gs_prefix, gcsfuse_prefix)\n",
        "  if checkpoint_dir and checkpoint_dir.startswith(gs_prefix):\n",
        "    checkpoint_dir = checkpoint_dir.replace(gs_prefix, gcsfuse_prefix)\n",
        "\n",
        "  writer = SummaryWriter(tensorboard_log_dir)\n",
        "\n",
        "  is_chief = args.rank == 0\n",
        "  if is_chief:\n",
        "    makedirs(checkpoint_dir)\n",
        "    print(f'Checkpoints will be saved to {checkpoint_dir}')\n",
        "\n",
        "  checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint.pt')\n",
        "  print(f'checkpoint_path is {checkpoint_path}')\n",
        "\n",
        "  if args.world_size > 1:\n",
        "    print('Initializing distributed backend with {} nodes'.format(args.world_size))\n",
        "    distributed.init_process_group(\n",
        "          backend=args.backend,\n",
        "          init_method=args.init_method,\n",
        "          world_size=args.world_size,\n",
        "          rank=args.rank,\n",
        "      )\n",
        "    print(f'[{os.getpid()}]: '\n",
        "          f'world_size = {distributed.get_world_size()}, '\n",
        "          f'rank = {distributed.get_rank()}, '\n",
        "          f'backend={distributed.get_backend()} \\n', end='')\n",
        "\n",
        "  if torch.cuda.is_available() and not args.no_cuda:\n",
        "    device = torch.device('cuda:{}'.format(args.rank))\n",
        "  else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "  model = Net(device=device)\n",
        "  if distributed_is_initialized():\n",
        "    model.to(device)\n",
        "    model = DistributedDataParallel(model)\n",
        "\n",
        "  if is_chief:\n",
        "    # All processes should see same parameters as they all start from same\n",
        "    # random parameters and gradients are synchronized in backward passes.\n",
        "    # Therefore, saving it in one process is sufficient.\n",
        "    torch.save(model.state_dict(), checkpoint_path)\n",
        "    print(f'Initial chief checkpoint is saved to {checkpoint_path}')\n",
        "\n",
        "  # Use a barrier() to make sure that process 1 loads the model after process\n",
        "  # 0 saves it.\n",
        "  if distributed_is_initialized():\n",
        "    distributed.barrier()\n",
        "    # configure map_location properly\n",
        "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "    print(f'Initial chief checkpoint is saved to {checkpoint_path} with map_location {device}')\n",
        "  else:\n",
        "    model.load_state_dict(torch.load(checkpoint_path))\n",
        "    print(f'Initial chief checkpoint is loaded from {checkpoint_path}')\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
        "\n",
        "  train_loader = MNISTDataLoader(\n",
        "      local_data_dir, args.batch_size, train=True)\n",
        "  test_loader = MNISTDataLoader(\n",
        "      local_data_dir, args.batch_size, train=False)\n",
        "\n",
        "  trainer = Trainer(\n",
        "      model=model,\n",
        "      optimizer=optimizer,\n",
        "      train_loader=train_loader,\n",
        "      test_loader=test_loader,\n",
        "      device=device,\n",
        "      model_name='mnist.pt',\n",
        "      checkpoint_path=checkpoint_path,\n",
        "  )\n",
        "  trainer.fit(args.epochs, is_chief, writer)\n",
        "\n",
        "  if model_dir == local_model_dir:\n",
        "    makedirs(model_dir)\n",
        "    trainer.save(model_dir)\n",
        "    print(f'Model is saved to {model_dir}')\n",
        "\n",
        "  print(f'Tensorboard logs are saved to: {tensorboard_log_dir}')\n",
        "\n",
        "  writer.close()\n",
        "\n",
        "  if is_chief:\n",
        "    os.remove(checkpoint_path)\n",
        "\n",
        "  if distributed_is_initialized():\n",
        "    distributed.destroy_process_group()\n",
        "\n",
        "  return\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "253359c5153a"
      },
      "source": [
        "### Write requirements file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "46423aa093f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing trainer/requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile {PYTHON_PACKAGE_APPLICATION_DIR}/requirements.txt\n",
        "\n",
        "\n",
        "torch\n",
        "torchvision\n",
        "tensorboard\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34bea792269a"
      },
      "source": [
        "### Write the docker file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "774a0be3a796"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing trainer/Dockerfile\n"
          ]
        }
      ],
      "source": [
        "%%writefile {PYTHON_PACKAGE_APPLICATION_DIR}/Dockerfile\n",
        "\n",
        "\n",
        "FROM pytorch/pytorch:1.8.1-cuda11.1-cudnn8-runtime\n",
        "\n",
        "RUN apt-get update && \\\n",
        "    apt-get install -y curl gnupg && \\\n",
        "    echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main\" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && \\\n",
        "    curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key --keyring /usr/share/keyrings/cloud.google.gpg  add - && \\\n",
        "    apt-get update -y && \\\n",
        "    apt-get install google-cloud-sdk -y\n",
        "\n",
        "COPY . /trainer\n",
        "\n",
        "WORKDIR /trainer\n",
        "\n",
        "RUN pip install -r requirements.txt\n",
        "\n",
        "ENTRYPOINT [\"python\", \"-m\", \"task\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57bf6f8b4361"
      },
      "source": [
        "## Local Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "e5d8a3443da0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dockerfile  requirements.txt  task.py\n",
            "\n",
            "\n",
            "torch\n",
            "torchvision\n",
            "tensorboard\n",
            "Collecting torch\n",
            "  Downloading torch-1.12.0-cp37-cp37m-manylinux1_x86_64.whl (776.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m840.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting torchvision\n",
            "  Downloading torchvision-0.13.0-cp37-cp37m-manylinux1_x86_64.whl (19.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard\n",
            "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->-r trainer/requirements.txt (line 3)) (4.2.0)\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision->-r trainer/requirements.txt (line 4)) (2.28.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->-r trainer/requirements.txt (line 4)) (9.1.1)\n",
            "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->-r trainer/requirements.txt (line 4)) (1.19.5)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard->-r trainer/requirements.txt (line 5)) (59.8.0)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting werkzeug>=1.0.1\n",
            "  Downloading Werkzeug-2.2.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.4/232.4 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard->-r trainer/requirements.txt (line 5)) (3.3.7)\n",
            "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorboard->-r trainer/requirements.txt (line 5)) (0.37.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard->-r trainer/requirements.txt (line 5)) (1.47.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard->-r trainer/requirements.txt (line 5)) (2.9.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorboard->-r trainer/requirements.txt (line 5)) (3.19.4)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.7/site-packages (from tensorboard->-r trainer/requirements.txt (line 5)) (1.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r trainer/requirements.txt (line 5)) (4.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r trainer/requirements.txt (line 5)) (1.16.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r trainer/requirements.txt (line 5)) (5.0.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r trainer/requirements.txt (line 5)) (0.2.7)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r trainer/requirements.txt (line 5)) (1.3.1)\n",
            "Collecting importlib-metadata>=4.4\n",
            "  Downloading importlib_metadata-4.12.0-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->-r trainer/requirements.txt (line 4)) (2022.6.15)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->-r trainer/requirements.txt (line 4)) (2.1.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->-r trainer/requirements.txt (line 4)) (1.26.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->-r trainer/requirements.txt (line 4)) (3.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard->-r trainer/requirements.txt (line 5)) (2.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r trainer/requirements.txt (line 5)) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r trainer/requirements.txt (line 5)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r trainer/requirements.txt (line 5)) (3.2.0)\n",
            "Installing collected packages: tensorboard-plugin-wit, werkzeug, torch, tensorboard-data-server, importlib-metadata, torchvision, google-auth-oauthlib, tensorboard\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.2.0\n",
            "    Uninstalling importlib-metadata-4.2.0:\n",
            "      Successfully uninstalled importlib-metadata-4.2.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 0.5.2\n",
            "    Uninstalling google-auth-oauthlib-0.5.2:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.5.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flake8 4.0.1 requires importlib-metadata<4.3; python_version < \"3.8\", but you have importlib-metadata 4.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-auth-oauthlib-0.4.6 importlib-metadata-4.12.0 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 torch-1.12.0 torchvision-0.13.0 werkzeug-2.2.1\n",
            "\n",
            "\n",
            "\"\"\"\n",
            "Main program for PyTorch distributed training.\n",
            "Adapted from: https://github.com/narumiruna/pytorch-distributed-example\n",
            "\"\"\"\n",
            "\n",
            "import argparse\n",
            "import os\n",
            "import shutil\n",
            "\n",
            "import torch\n",
            "from torch import distributed\n",
            "from torch.nn.parallel import DistributedDataParallel\n",
            "from torch.utils import data\n",
            "from torch.utils.tensorboard import SummaryWriter\n",
            "\n",
            "from torchvision import datasets, transforms\n",
            "\n",
            "def parse_args():\n",
            "\n",
            "  parser = argparse.ArgumentParser()\n",
            "\n",
            "  # Using environment variables for Cloud Storage directories\n",
            "  # see more details in https://cloud.google.com/vertex-ai/docs/training/code-requirements\n",
            "  parser.add_argument(\n",
            "      '--model-dir', default=os.getenv('AIP_MODEL_DIR'), type=str,\n",
            "      help='a Cloud Storage URI of a directory intended for saving model artifacts')\n",
            "  parser.add_argument(\n",
            "      '--tensorboard-log-dir', default=os.getenv('AIP_TENSORBOARD_LOG_DIR'), type=str,\n",
            "      help='a Cloud Storage URI of a directory intended for saving TensorBoard')\n",
            "  parser.add_argument(\n",
            "      '--checkpoint-dir', default=os.getenv('AIP_CHECKPOINT_DIR'), type=str,\n",
            "      help='a Cloud Storage URI of a directory intended for saving checkpoints')\n",
            "\n",
            "  parser.add_argument(\n",
            "      '--backend', type=str, default='gloo',\n",
            "      help='Use the `nccl` backend for distributed GPU training.'\n",
            "           'Use the `gloo` backend for distributed CPU training.')\n",
            "  parser.add_argument(\n",
            "      '--init-method', type=str, default='env://',\n",
            "      help='URL specifying how to initialize the package.')\n",
            "  parser.add_argument(\n",
            "      '--world-size', type=int, default=os.environ.get('WORLD_SIZE', 1),\n",
            "      help='The total number of nodes in the cluster. '\n",
            "           'This variable has the same value on every node.')\n",
            "  parser.add_argument(\n",
            "      '--rank', type=int, default=os.environ.get('RANK', 0),\n",
            "      help='A unique identifier for each node. '\n",
            "           'On the master worker, this is set to 0. '\n",
            "           'On each worker, it is set to a different value from 1 to WORLD_SIZE - 1.')\n",
            "  parser.add_argument(\n",
            "      '--epochs', type=int, default=20)\n",
            "  parser.add_argument(\n",
            "      '--no-cuda', action='store_true')\n",
            "  parser.add_argument(\n",
            "      '-lr', '--learning-rate', type=float, default=1e-3)\n",
            "  parser.add_argument(\n",
            "      '--batch-size', type=int, default=128)\n",
            "  parser.add_argument(\n",
            "      '--local-mode', action='store_true', help='use local mode when running on your local machine')\n",
            "\n",
            "  args = parser.parse_args()\n",
            "\n",
            "  return args\n",
            "\n",
            "def makedirs(model_dir):\n",
            "  if os.path.exists(model_dir) and os.path.isdir(model_dir):\n",
            "    shutil.rmtree(model_dir)\n",
            "  os.makedirs(model_dir)\n",
            "  return\n",
            "\n",
            "def distributed_is_initialized():\n",
            "  if distributed.is_available():\n",
            "    if distributed.is_initialized():\n",
            "      return True\n",
            "  return False\n",
            "\n",
            "class Average(object):\n",
            "\n",
            "  def __init__(self):\n",
            "    self.sum = 0\n",
            "    self.count = 0\n",
            "\n",
            "  def __str__(self):\n",
            "    return '{:.6f}'.format(self.average)\n",
            "\n",
            "  @property\n",
            "  def average(self):\n",
            "    return self.sum / self.count\n",
            "\n",
            "  def update(self, value, number):\n",
            "    self.sum += value * number\n",
            "    self.count += number\n",
            "\n",
            "class Accuracy(object):\n",
            "\n",
            "  def __init__(self):\n",
            "    self.correct = 0\n",
            "    self.count = 0\n",
            "\n",
            "  def __str__(self):\n",
            "    return '{:.2f}%'.format(self.accuracy * 100)\n",
            "\n",
            "  @property\n",
            "  def accuracy(self):\n",
            "    return self.correct / self.count\n",
            "\n",
            "  @torch.no_grad()\n",
            "  def update(self, output, target):\n",
            "    pred = output.argmax(dim=1)\n",
            "    correct = pred.eq(target).sum().item()\n",
            "\n",
            "    self.correct += correct\n",
            "    self.count += output.size(0)\n",
            "\n",
            "class Net(torch.nn.Module):\n",
            "\n",
            "  def __init__(self, device):\n",
            "    super(Net, self).__init__()\n",
            "    self.fc = torch.nn.Linear(784, 10).to(device)\n",
            "\n",
            "  def forward(self, x):\n",
            "    return self.fc(x.view(x.size(0), -1))\n",
            "\n",
            "class MNISTDataLoader(data.DataLoader):\n",
            "\n",
            "  def __init__(self, root, batch_size, train=True):\n",
            "    transform = transforms.Compose([\n",
            "        transforms.ToTensor(),\n",
            "        transforms.Normalize((0.1307,), (0.3081,)),\n",
            "    ])\n",
            "\n",
            "    dataset = datasets.MNIST(root, train=train, transform=transform, download=True)\n",
            "    sampler = None\n",
            "    if train and distributed_is_initialized():\n",
            "      sampler = data.DistributedSampler(dataset)\n",
            "\n",
            "    super(MNISTDataLoader, self).__init__(\n",
            "        dataset,\n",
            "        batch_size=batch_size,\n",
            "        shuffle=(sampler is None),\n",
            "        sampler=sampler,\n",
            "    )\n",
            "\n",
            "class Trainer(object):\n",
            "\n",
            "  def __init__(self,\n",
            "      model,\n",
            "      optimizer,\n",
            "      train_loader,\n",
            "      test_loader,\n",
            "      device,\n",
            "      model_name,\n",
            "      checkpoint_path\n",
            "  ):\n",
            "    self.model = model\n",
            "    self.optimizer = optimizer\n",
            "    self.train_loader = train_loader\n",
            "    self.test_loader = test_loader\n",
            "    self.device = device\n",
            "    self.model_name = model_name\n",
            "    self.checkpoint_path = checkpoint_path\n",
            "\n",
            "  def save(self, model_dir):\n",
            "    model_path = os.path.join(model_dir, self.model_name)\n",
            "    torch.save(self.model.state_dict(), model_path)\n",
            "\n",
            "  def fit(self, epochs, is_chief, writer):\n",
            "\n",
            "    for epoch in range(1, epochs + 1):\n",
            "\n",
            "      print('Epoch: {}, Training ...'.format(epoch))\n",
            "      train_loss, train_acc = self.train()\n",
            "\n",
            "      if is_chief:\n",
            "        test_loss, test_acc = self.evaluate()\n",
            "        writer.add_scalar('Loss/train', train_loss.average, epoch)\n",
            "        writer.add_scalar('Loss/test', test_loss.average, epoch)\n",
            "        writer.add_scalar('Accuracy/train', train_acc.accuracy, epoch)\n",
            "        writer.add_scalar('Accuracy/test', test_acc.accuracy, epoch)\n",
            "        torch.save(self.model.state_dict(), self.checkpoint_path)\n",
            "\n",
            "        print(\n",
            "            'Epoch: {}/{},'.format(epoch, epochs),\n",
            "            'train loss: {}, train acc: {},'.format(train_loss, train_acc),\n",
            "            'test loss: {}, test acc: {}.'.format(test_loss, test_acc),\n",
            "        )\n",
            "\n",
            "  def train(self):\n",
            "\n",
            "    self.model.train()\n",
            "\n",
            "    train_loss = Average()\n",
            "    train_acc = Accuracy()\n",
            "\n",
            "    for data, target in self.train_loader:\n",
            "      data = data.to(self.device)\n",
            "      target = target.to(self.device)\n",
            "\n",
            "      output = self.model(data)\n",
            "      loss = torch.nn.functional.cross_entropy(output, target)\n",
            "\n",
            "      self.optimizer.zero_grad()\n",
            "      loss.backward()\n",
            "      self.optimizer.step()\n",
            "\n",
            "      train_loss.update(loss.item(), data.size(0))\n",
            "      train_acc.update(output, target)\n",
            "\n",
            "    return train_loss, train_acc\n",
            "\n",
            "  @torch.no_grad()\n",
            "  def evaluate(self):\n",
            "    self.model.eval()\n",
            "\n",
            "    test_loss = Average()\n",
            "    test_acc = Accuracy()\n",
            "\n",
            "    for data, target in self.test_loader:\n",
            "      data = data.to(self.device)\n",
            "      target = target.to(self.device)\n",
            "\n",
            "      output = self.model(data)\n",
            "      loss = torch.nn.functional.cross_entropy(output, target)\n",
            "\n",
            "      test_loss.update(loss.item(), data.size(0))\n",
            "      test_acc.update(output, target)\n",
            "\n",
            "    return test_loss, test_acc\n",
            "\n",
            "def main():\n",
            "\n",
            "  args = parse_args()\n",
            "\n",
            "  local_data_dir = './tmp/data'\n",
            "  local_model_dir = './tmp/model'\n",
            "  local_tensorboard_log_dir = './tmp/logs'\n",
            "  local_checkpoint_dir = './tmp/checkpoints'\n",
            "\n",
            "  model_dir = args.model_dir or local_model_dir\n",
            "  tensorboard_log_dir = args.tensorboard_log_dir or local_tensorboard_log_dir\n",
            "  checkpoint_dir = args.checkpoint_dir or local_checkpoint_dir\n",
            "\n",
            "  gs_prefix = 'gs://'\n",
            "  gcsfuse_prefix = '/gcs/'\n",
            "  if model_dir and model_dir.startswith(gs_prefix):\n",
            "    model_dir = model_dir.replace(gs_prefix, gcsfuse_prefix)\n",
            "  if tensorboard_log_dir and tensorboard_log_dir.startswith(gs_prefix):\n",
            "    tensorboard_log_dir = tensorboard_log_dir.replace(gs_prefix, gcsfuse_prefix)\n",
            "  if checkpoint_dir and checkpoint_dir.startswith(gs_prefix):\n",
            "    checkpoint_dir = checkpoint_dir.replace(gs_prefix, gcsfuse_prefix)\n",
            "\n",
            "  writer = SummaryWriter(tensorboard_log_dir)\n",
            "\n",
            "  is_chief = args.rank == 0\n",
            "  if is_chief:\n",
            "    makedirs(checkpoint_dir)\n",
            "    print(f'Checkpoints will be saved to {checkpoint_dir}')\n",
            "\n",
            "  checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint.pt')\n",
            "  print(f'checkpoint_path is {checkpoint_path}')\n",
            "\n",
            "  if args.world_size > 1:\n",
            "    print('Initializing distributed backend with {} nodes'.format(args.world_size))\n",
            "    distributed.init_process_group(\n",
            "          backend=args.backend,\n",
            "          init_method=args.init_method,\n",
            "          world_size=args.world_size,\n",
            "          rank=args.rank,\n",
            "      )\n",
            "    print(f'[{os.getpid()}]: '\n",
            "          f'world_size = {distributed.get_world_size()}, '\n",
            "          f'rank = {distributed.get_rank()}, '\n",
            "          f'backend={distributed.get_backend()} \\n', end='')\n",
            "\n",
            "  if torch.cuda.is_available() and not args.no_cuda:\n",
            "    device = torch.device('cuda:{}'.format(args.rank))\n",
            "  else:\n",
            "    device = torch.device('cpu')\n",
            "\n",
            "  model = Net(device=device)\n",
            "  if distributed_is_initialized():\n",
            "    model.to(device)\n",
            "    model = DistributedDataParallel(model)\n",
            "\n",
            "  if is_chief:\n",
            "    # All processes should see same parameters as they all start from same\n",
            "    # random parameters and gradients are synchronized in backward passes.\n",
            "    # Therefore, saving it in one process is sufficient.\n",
            "    torch.save(model.state_dict(), checkpoint_path)\n",
            "    print(f'Initial chief checkpoint is saved to {checkpoint_path}')\n",
            "\n",
            "  # Use a barrier() to make sure that process 1 loads the model after process\n",
            "  # 0 saves it.\n",
            "  if distributed_is_initialized():\n",
            "    distributed.barrier()\n",
            "    # configure map_location properly\n",
            "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
            "    print(f'Initial chief checkpoint is saved to {checkpoint_path} with map_location {device}')\n",
            "  else:\n",
            "    model.load_state_dict(torch.load(checkpoint_path))\n",
            "    print(f'Initial chief checkpoint is loaded from {checkpoint_path}')\n",
            "\n",
            "  optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
            "\n",
            "  train_loader = MNISTDataLoader(\n",
            "      local_data_dir, args.batch_size, train=True)\n",
            "  test_loader = MNISTDataLoader(\n",
            "      local_data_dir, args.batch_size, train=False)\n",
            "\n",
            "  trainer = Trainer(\n",
            "      model=model,\n",
            "      optimizer=optimizer,\n",
            "      train_loader=train_loader,\n",
            "      test_loader=test_loader,\n",
            "      device=device,\n",
            "      model_name='mnist.pt',\n",
            "      checkpoint_path=checkpoint_path,\n",
            "  )\n",
            "  trainer.fit(args.epochs, is_chief, writer)\n",
            "\n",
            "  if model_dir == local_model_dir:\n",
            "    makedirs(model_dir)\n",
            "    trainer.save(model_dir)\n",
            "    print(f'Model is saved to {model_dir}')\n",
            "\n",
            "  print(f'Tensorboard logs are saved to: {tensorboard_log_dir}')\n",
            "\n",
            "  writer.close()\n",
            "\n",
            "  if is_chief:\n",
            "    os.remove(checkpoint_path)\n",
            "\n",
            "  if distributed_is_initialized():\n",
            "    distributed.destroy_process_group()\n",
            "\n",
            "  return\n",
            "\n",
            "if __name__ == '__main__':\n",
            "  main()\n"
          ]
        }
      ],
      "source": [
        "! ls trainer\n",
        "! cat trainer/requirements.txt\n",
        "! pip install -r trainer/requirements.txt\n",
        "! cat trainer/task.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "c0c6e7dfb3c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoints will be saved to ./tmp/checkpoints\n",
            "checkpoint_path is ./tmp/checkpoints/checkpoint.pt\n",
            "Initial chief checkpoint is saved to ./tmp/checkpoints/checkpoint.pt\n",
            "Initial chief checkpoint is loaded from ./tmp/checkpoints/checkpoint.pt\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./tmp/data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0acecfc9043f42b9b0127e35ac3219c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/9912422 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./tmp/data/MNIST/raw/train-images-idx3-ubyte.gz to ./tmp/data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./tmp/data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1aab657545ee46389fa4e541c543e33a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/28881 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./tmp/data/MNIST/raw/train-labels-idx1-ubyte.gz to ./tmp/data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./tmp/data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3af8e6d4b8c428c9ebb5d7bbcc3d19d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1648877 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./tmp/data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./tmp/data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./tmp/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57c492e8052246e69ab3f440f95082c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4542 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./tmp/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./tmp/data/MNIST/raw\n",
            "\n",
            "Epoch: 1, Training ...\n",
            "Epoch: 1/5, train loss: 0.436803, train acc: 87.75%, test loss: 0.296839, test acc: 91.67%.\n",
            "Epoch: 2, Training ...\n",
            "Epoch: 2/5, train loss: 0.299492, train acc: 91.44%, test loss: 0.286140, test acc: 92.21%.\n",
            "Epoch: 3, Training ...\n",
            "Epoch: 3/5, train loss: 0.283688, train acc: 92.04%, test loss: 0.274255, test acc: 92.44%.\n",
            "Epoch: 4, Training ...\n",
            "Epoch: 4/5, train loss: 0.276154, train acc: 92.26%, test loss: 0.280220, test acc: 92.08%.\n",
            "Epoch: 5, Training ...\n",
            "Epoch: 5/5, train loss: 0.270607, train acc: 92.46%, test loss: 0.269134, test acc: 92.37%.\n",
            "Model is saved to ./tmp/model\n",
            "Tensorboard logs are saved to: ./tmp/logs\n"
          ]
        }
      ],
      "source": [
        "%run trainer/task.py --epochs 5 --no-cuda --local-mode "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "31dfdeede587"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "checkpoints  data  logs  model\n"
          ]
        }
      ],
      "source": [
        "! ls ./tmp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3614fcdfcd62"
      },
      "source": [
        "Clean up temporary files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "48d56ec621cc"
      },
      "outputs": [],
      "source": [
        "! rm -rf ./tmp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f3ea1210749"
      },
      "source": [
        "## Vertex AI Training using a custom container"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93002a20a2a6"
      },
      "source": [
        "### Build Custom Container\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d242773d707"
      },
      "source": [
        "#### Enable Artifact Registry API\n",
        "You must enable the Artifact Registry API service for your project.\n",
        "\n",
        "<a href=\"https://cloud.google.com/artifact-registry/docs/enable-service\">Learn more about Enabling service</a>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "9d72f89cabd5"
      },
      "outputs": [],
      "source": [
        "! gcloud services enable artifactregistry.googleapis.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23f3ba8346f4"
      },
      "source": [
        "### Create a private Docker repository\n",
        "Your first step is to create your own Docker repository in Google Artifact Registry.\n",
        "\n",
        "1 - Run the gcloud artifacts repositories create command to create a new Docker repository with your region with the description \"docker repository\".\n",
        "\n",
        "2 - Run the gcloud artifacts repositories list command to verify that your repository was created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "64fee26c2dff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;31mERROR:\u001b[0m (gcloud.artifacts.repositories.create) ALREADY_EXISTS: the repository already exists\n",
            "Listing items under project vertex-ai-dev, across all locations.\n",
            "\n",
            "                                                                               ARTIFACT_REGISTRY\n",
            "REPOSITORY           FORMAT  DESCRIPTION                                           LOCATION     LABELS  ENCRYPTION          CREATE_TIME          UPDATE_TIME          SIZE (MB)\n",
            "gcr.io               DOCKER                                                        us                   Google-managed key  2022-04-26T08:47:44  2022-04-26T08:47:44  0\n",
            "my-docker-repo       DOCKER  Docker repository                                     us-central1          Google-managed key  2022-04-28T07:36:59  2022-04-28T07:36:59  0\n",
            "my-docker-repo-28    DOCKER  Docker repository                                     us-central1          Google-managed key  2022-08-02T08:54:43  2022-08-02T09:50:32  3948.150\n",
            "my-docker-repo2      DOCKER  Docker repository                                     us-central1          Google-managed key  2022-04-28T08:51:33  2022-04-28T09:00:46  3665.827\n",
            "my-docker-repo3      DOCKER  Docker repository                                     us-central1          Google-managed key  2022-04-28T09:27:47  2022-04-28T09:27:47  0\n",
            "testing-docker-repo  DOCKER  Docker repository for the testing environment images  us-central1          Google-managed key  2022-03-01T09:45:49  2022-03-17T23:14:36  1386.093\n",
            "us.gcr.io            DOCKER                                                        us                   Google-managed key  2022-04-26T08:47:44  2022-04-26T08:47:44  0\n"
          ]
        }
      ],
      "source": [
        "PRIVATE_REPO = \"my-docker-repo\"\n",
        "\n",
        "! gcloud artifacts repositories create {PRIVATE_REPO} --repository-format=docker --location={REGION} --description=\"Docker repository\"\n",
        "\n",
        "! gcloud artifacts repositories list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38e62011c5e1"
      },
      "outputs": [],
      "source": [
        "DEPLOY_IMAGE = (\n",
        "    f\"{REGION}-docker.pkg.dev/\" + PROJECT_ID + f\"/{PRIVATE_REPO}\" + \"/tf_serving\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "775c3eca365c"
      },
      "outputs": [],
      "source": [
        "print(\"Deployment:\", DEPLOY_IMAGE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "535932b8c42d"
      },
      "source": [
        "## Executes in Workbench\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd295337455e"
      },
      "source": [
        "### Configure authentication to your private repo\n",
        "Before you push or pull container images, configure Docker to use the gcloud command-line tool to authenticate requests to Artifact Registry for your region."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "79c3fe4ab7f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;33mWARNING:\u001b[0m Your config file at [/home/jupyter/.docker/config.json] contains these credential helper entries:\n",
            "\n",
            "{\n",
            "  \"credHelpers\": {\n",
            "    \"gcr.io\": \"gcloud\",\n",
            "    \"us.gcr.io\": \"gcloud\",\n",
            "    \"eu.gcr.io\": \"gcloud\",\n",
            "    \"asia.gcr.io\": \"gcloud\",\n",
            "    \"staging-k8s.gcr.io\": \"gcloud\",\n",
            "    \"marketplace.gcr.io\": \"gcloud\"\n",
            "  }\n",
            "}\n",
            "Adding credentials for: us-central1-docker.pkg.dev\n",
            "Docker configuration file updated.\n"
          ]
        }
      ],
      "source": [
        "if not IS_COLAB:\n",
        "    ! gcloud auth configure-docker {REGION}-docker.pkg.dev --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a822a118c210"
      },
      "source": [
        "### Container (Docker) image for serving\n",
        "Set the TensorFlow Serving Docker container image for serving prediction.\n",
        "\n",
        "1. Pull the corresponding CPU or GPU Docker image for TF Serving from Docker Hub.\n",
        "2. Create a tag for registering the image with Artifact Registry\n",
        "3. Register the image with Artifact Registry.\n",
        "\n",
        "<a href=\"https://www.tensorflow.org/tfx/serving/docker\">Learn more about TensorFlow Serving</a>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "dfec228d9de2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sending build context to Docker daemon  14.34kB\n",
            "Step 1/6 : FROM pytorch/pytorch:1.8.1-cuda11.1-cudnn8-runtime\n",
            " ---> 5ffed6c83695\n",
            "Step 2/6 : RUN apt-get update &&     apt-get install -y curl gnupg &&     echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main\" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list &&     curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key --keyring /usr/share/keyrings/cloud.google.gpg  add - &&     apt-get update -y &&     apt-get install google-cloud-sdk -y\n",
            " ---> Using cache\n",
            " ---> 394e0a397ad4\n",
            "Step 3/6 : COPY . /trainer\n",
            " ---> Using cache\n",
            " ---> 27fdce27dfb9\n",
            "Step 4/6 : WORKDIR /trainer\n",
            " ---> Using cache\n",
            " ---> 8834c43a49fe\n",
            "Step 5/6 : RUN pip install -r requirements.txt\n",
            " ---> Using cache\n",
            " ---> bf298e2bec81\n",
            "Step 6/6 : ENTRYPOINT [\"python\", \"-m\", \"task\"]\n",
            " ---> Using cache\n",
            " ---> 116550b5ed83\n",
            "Successfully built 116550b5ed83\n",
            "Successfully tagged us-central1-docker.pkg.dev/vertex-ai-dev/my-docker-repo/tf_serving:latest\n",
            "9913344it [00:00, 47260799.05it/s]                             \n",
            "29696it [00:00, 61386915.52it/s]         \n",
            "1649664it [00:00, 18218516.90it/s]         \n",
            "5120it [00:00, 18528763.14it/s]         ^C\n",
            "Checkpoints will be saved to ./tmp/checkpoints\n",
            "checkpoint_path is ./tmp/checkpoints/checkpoint.pt\n",
            "Initial chief checkpoint is saved to ./tmp/checkpoints/checkpoint.pt\n",
            "Initial chief checkpoint is loaded from ./tmp/checkpoints/checkpoint.pt\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./tmp/data/MNIST/raw/train-images-idx3-ubyte.gz\n",
            "Extracting ./tmp/data/MNIST/raw/train-images-idx3-ubyte.gz to ./tmp/data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./tmp/data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Extracting ./tmp/data/MNIST/raw/train-labels-idx1-ubyte.gz to ./tmp/data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./tmp/data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Extracting ./tmp/data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./tmp/data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./tmp/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Extracting ./tmp/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./tmp/data/MNIST/raw\n",
            "\n",
            "Processing...\n",
            "Done!\n",
            "Epoch: 1, Training ...\n",
            "Epoch: 1/5, train loss: 0.438981, train acc: 87.52%, test loss: 0.299025, test acc: 91.35%.\n",
            "Epoch: 2, Training ...\n",
            "Epoch: 2/5, train loss: 0.300029, train acc: 91.45%, test loss: 0.279441, test acc: 92.08%.\n",
            "Epoch: 3, Training ...\n",
            "\n",
            "/opt/conda/lib/python3.8/site-packages/torchvision/datasets/mnist.py:502: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1616554793803/work/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/trainer/task.py\", line 341, in <module>\n",
            "    main()\n",
            "  File \"/trainer/task.py\", line 321, in main\n",
            "    trainer.fit(args.epochs, is_chief, writer)\n",
            "  File \"/trainer/task.py\", line 174, in fit\n",
            "    train_loss, train_acc = self.train()\n",
            "  File \"/trainer/task.py\", line 197, in train\n",
            "    for data, target in self.train_loader:\n",
            "  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 517, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 557, in _next_data\n",
            "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/opt/conda/lib/python3.8/site-packages/torchvision/datasets/mnist.py\", line 112, in __getitem__\n",
            "    img = self.transform(img)\n",
            "  File \"/opt/conda/lib/python3.8/site-packages/torchvision/transforms/transforms.py\", line 60, in __call__\n",
            "    img = t(img)\n",
            "  File \"/opt/conda/lib/python3.8/site-packages/torchvision/transforms/transforms.py\", line 97, in __call__\n",
            "    return F.to_tensor(pic)\n",
            "  File \"/opt/conda/lib/python3.8/site-packages/torchvision/transforms/functional.py\", line 136, in to_tensor\n",
            "    img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
            "  File \"/opt/conda/lib/python3.8/site-packages/PIL/Image.py\", line 746, in tobytes\n",
            "    if s:\n",
            "KeyboardInterrupt\n",
            "Using default tag: latest\n",
            "The push refers to repository [us-central1-docker.pkg.dev/vertex-ai-dev/my-docker-repo/tf_serving]\n",
            "\n",
            "\u001b[1B4a2a5dc4: Preparing \n",
            "\u001b[1Bf45ba9fd: Preparing \n",
            "\u001b[1Bb721b8f9: Preparing \n",
            "\u001b[1B62e73fa9: Preparing \n",
            "\u001b[1B491659cb: Preparing \n",
            "\u001b[1Bdc413928: Preparing \n",
            "\u001b[1Bad8f2cae: Preparing \n",
            "\u001b[1B581dbc3c: Preparing \n",
            "\u001b[1B6facb613: Layer already exists \u001b[4A\u001b[2Klatest: digest: sha256:6aa8e306e7931c543a9ecf21fb2f087db750c39757536a56980e1ee69758b283 size: 2208\n",
            "Deployment: us-central1-docker.pkg.dev/vertex-ai-dev/my-docker-repo/tf_serving\n"
          ]
        }
      ],
      "source": [
        "if not IS_COLAB:\n",
        "    ! cd trainer && docker build -t $DEPLOY_IMAGE -f Dockerfile .\n",
        "    ! docker run --rm $DEPLOY_IMAGE --epochs 5 --no-cuda --local-mode\n",
        "    ! docker push $DEPLOY_IMAGE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7102219bb621"
      },
      "source": [
        "## Executes in Colab\n",
        "\n",
        "Build and push a Docker image with Cloud Build"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "736e3c0d1fd8"
      },
      "outputs": [],
      "source": [
        "if IS_COLAB:\n",
        "    ! cd trainer && gcloud builds submit --timeout=1800s --region={REGION} --tag $DEPLOY_IMAGE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10c8cc6b3334"
      },
      "source": [
        "### Initialize Vertex AI SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "42e981cefe41"
      },
      "outputs": [],
      "source": [
        "aiplatform.init(\n",
        "    project=PROJECT_ID,\n",
        "    staging_bucket=BUCKET_URI,\n",
        "    location=REGION,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73c92c9298e9"
      },
      "source": [
        "### Create a Vertex AI Tensorboard instance\n",
        "\n",
        "NOTE: <a href=\"https://cloud.google.com/vertex-ai/pricing#tensorboard\">Vertex AI TensorBoard </a> charges a monthly fee of $300 per unique active user. Active users are measured through the Vertex AI TensorBoard UI. You also pay for Google Cloud resources you use with Vertex AI TensorBoard, such as TensorBoard logs stored in Cloud Storage.</a>Please check above link for latest prices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "bde509558cd5"
      },
      "outputs": [],
      "source": [
        "content_name = content_name + \"-cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "6d7908c0083c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating Tensorboard\n",
            "Create Tensorboard backing LRO: projects/931647533046/locations/us-central1/tensorboards/4617790506984800256/operations/5372512399439953920\n",
            "Tensorboard created. Resource name: projects/931647533046/locations/us-central1/tensorboards/4617790506984800256\n",
            "To use this Tensorboard in another session:\n",
            "tb = aiplatform.Tensorboard('projects/931647533046/locations/us-central1/tensorboards/4617790506984800256')\n"
          ]
        }
      ],
      "source": [
        "tensorboard = aiplatform.Tensorboard.create(\n",
        "    display_name=content_name,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1f0a4f54037"
      },
      "source": [
        "#### Option: Use a previously created Vertex AI Tensorboard instance\n",
        "\n",
        "```\n",
        "tensorboard_name = \"Your Tensorboard Resource Name or Tensorboard ID\"\n",
        "tensorboard = aiplatform.Tensorboard(tensorboard_name=tensorboard_name)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4cac84e04ac"
      },
      "source": [
        "### Run a Vertex AI SDK CustomContainerTrainingJob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "f92e8fdd44ee"
      },
      "outputs": [],
      "source": [
        "display_name = content_name\n",
        "gcs_output_uri_prefix = f\"{BUCKET_URI}/{display_name}\"\n",
        "\n",
        "replica_count = 4\n",
        "machine_type = \"n1-standard-4\"\n",
        "\n",
        "args = [\n",
        "    \"--backend\",\n",
        "    \"gloo\",\n",
        "    \"--no-cuda\",\n",
        "    \"--batch-size\",\n",
        "    \"128\",\n",
        "    \"--epochs\",\n",
        "    \"25\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ae4c57df7e07"
      },
      "outputs": [],
      "source": [
        "custom_container_training_job = aiplatform.CustomContainerTrainingJob(\n",
        "    display_name=display_name,\n",
        "    container_uri=DEPLOY_IMAGE,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "35cf3ecdf0df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Output directory:\n",
            "gs://vertex-ai-devaip-20220802103935/pt-img-cls-multi-node-ddp-cust-cont-cpu \n",
            "View Training:\n",
            "https://console.cloud.google.com/ai/platform/locations/us-central1/training/5322270971522973696?project=931647533046\n",
            "View backing custom job:\n",
            "https://console.cloud.google.com/ai/platform/locations/us-central1/training/8705740929073414144?project=931647533046\n",
            "View tensorboard:\n",
            "https://us-central1.tensorboard.googleusercontent.com/experiment/projects+931647533046+locations+us-central1+tensorboards+4617790506984800256+experiments+8705740929073414144\n",
            "CustomContainerTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/5322270971522973696 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "CustomContainerTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/5322270971522973696 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "CustomContainerTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/5322270971522973696 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "CustomContainerTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/5322270971522973696 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "CustomContainerTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/5322270971522973696 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "CustomContainerTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/5322270971522973696 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "CustomContainerTrainingJob run completed. Resource name: projects/931647533046/locations/us-central1/trainingPipelines/5322270971522973696\n",
            "Training did not produce a Managed Model returning None. Training Pipeline projects/931647533046/locations/us-central1/trainingPipelines/5322270971522973696 is not configured to upload a Model. Create the Training Pipeline with model_serving_container_image_uri and model_display_name passed in. Ensure that your training script saves to model to os.environ['AIP_MODEL_DIR'].\n"
          ]
        }
      ],
      "source": [
        "custom_container_training_job.run(\n",
        "    args=args,\n",
        "    base_output_dir=gcs_output_uri_prefix,\n",
        "    replica_count=replica_count,\n",
        "    machine_type=machine_type,\n",
        "    tensorboard=tensorboard.resource_name,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49d10dded73b"
      },
      "outputs": [],
      "source": [
        "print(f\"Custom Training Job Name: {custom_container_training_job.resource_name}\")\n",
        "print(f\"GCS Output URI Prefix: {gcs_output_uri_prefix}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78398f52807b"
      },
      "source": [
        "### View training output artifact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "fc74422de1d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gs://vertex-ai-devaip-20220802103935/pt-img-cls-multi-node-ddp-cust-cont-cpu/\n",
            "gs://vertex-ai-devaip-20220802103935/pt-img-cls-multi-node-ddp-cust-cont-cpu/checkpoints/\n",
            "gs://vertex-ai-devaip-20220802103935/pt-img-cls-multi-node-ddp-cust-cont-cpu/logs/\n"
          ]
        }
      ],
      "source": [
        "! gsutil ls $gcs_output_uri_prefix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e99a6a05b10"
      },
      "source": [
        "# Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:\n",
        "\n",
        "- Artifacts\n",
        "- Vertex AI Tensorboard\n",
        "- Cloud Storage Bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0c1b3f7466b"
      },
      "outputs": [],
      "source": [
        "# Set this to true only if you'd like to delete your bucket\n",
        "delete_bucket = False\n",
        "delete_tensorboard = False\n",
        "\n",
        "! gsutil rm -rf $gcs_output_uri_prefix\n",
        "\n",
        "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
        "    ! gsutil rm -r $BUCKET_URI\n",
        "\n",
        "if delete_tensorboard or os.getenv(\"IS_TESTING\"):\n",
        "    tensorboard.delete()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "multi_node_ddp_gloo_vertex_training_with_custom_container.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

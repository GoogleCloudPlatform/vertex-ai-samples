{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsv4jGuU89rX"
      },
      "source": [
        "# Vertex AI SDK 2.0 Vertex AI Remote Hyperparameter Tuning for OSS ML frameworks\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/training/sdk2_remote_hyperparameter_tuning.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/training/sdk2_remote_hyperparameter_tuning.ipynb\">\n",
        "        <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "    <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/training/sdk2_remote_hyperparameter_tuning.ipynb\">\n",
        "       <img src=\"https://www.gstatic.com/cloud/images/navigation/vertex-ai.svg\" alt=\"Vertex AI logo\">Open in Vertex AI Workbench\n",
        "    </a>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview:automl"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial demonstrates how to use Vertex AI SDK 2.0 for remote model hyperparameter tuning of a local model hyperparameter job for OSS ML frameworks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qwhOI96DjTE"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn to use `Vertex AI SDK 2.0` to remotely hyperparameter tune models of various ML frameworks as a local (on-prem) hyperparameter tuning job.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services:\n",
        "\n",
        "- `Vertex AI Training`\n",
        "- `Vertex AI Remote Hyperparameter Tuning`\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Download and split the dataset\n",
        "- Perform transformations as a Vertex AI remote training.\n",
        "- For scikit-learn, PyTorch, TensorFlow, PyTorch Lightning, Tabnet\n",
        "    - Tune the model remotely.\n",
        "    - Get the best model.\n",
        "\n",
        "**Local tuning**\n",
        "\n",
        "```\n",
        "from google.cloud.aiplatform.private_preview import vertex_ai\n",
        "from google.cloud.aiplatform.private_preview.vertex_ai import VizierHyperparameterTuner\n",
        "\n",
        "from my_module import MyModelClass\n",
        "\n",
        "# Set to False for local training\n",
        "vertex_ai.init(remote=False)\n",
        "\n",
        "X, y = pd.DataFrame(...), pd.DataFrame(...)\n",
        "\n",
        "# Define a function which returns an intialized model. Parameters of this function are tunable.\n",
        "def get_model_func(learning_rate: float, optimizer: str):\n",
        "  # Instantiate the class\n",
        "  return MyModelClass(learning_rate=learning_rate, optimizer=optimizer)\n",
        "\n",
        "# Define the hyperparameter search space\n",
        "hparam_space = [\n",
        "  {\n",
        "    \"parameter_id\": \"learning_rate\",\n",
        "    \"double_value_spec\": {\n",
        "      \"min_value\": 0.01,\n",
        "      \"max_value\": 0.05\n",
        "    }\n",
        "  }, {\n",
        "    \"parameter_id\": \"optimizer\",\n",
        "    \"categorical_value_spec\": {\n",
        "      \"values\"[\"adam\", \"sgd\"]\n",
        "    }\n",
        "  }\n",
        "]\n",
        "\n",
        "tuner = VizierHyperparameterTuner(\n",
        "  get_model_func=get_model_func,\n",
        "  max_trial_count=3,\n",
        "  parallel_trial_count=2,\n",
        "  hparam_space=hparam_space,\n",
        "  metric_id=\"accuracy\",\n",
        "  metric_goal=\"MAXIMIZE\",\n",
        "  max_failed_trial_count=0,\n",
        ")\n",
        "                               \n",
        "# Tune model using Vizier. Tuning and trials run locally.\n",
        "# `epochs` is passed at runtime to model's fit()/train() call\n",
        "# (ex: model.fit(X, y, epochs=5)\n",
        "tuner.fit(X, y, epochs=5...)\n",
        "```\n",
        "\n",
        "*Local tuning supported ML frameworks:*\n",
        "1.  scikit-learn\n",
        "2.  Custom model\n",
        "3.  TensorFlow\n",
        "4.  PyTorch\n",
        "5.  PyTorch Lightning\n",
        "6.  TabNet\n",
        "\n",
        "---\n",
        "\n",
        "**Remote tuning**\n",
        "```\n",
        "from google.cloud.aiplatform.private_preview import vertex_ai\n",
        "from google.cloud.aiplatform.private_preview.vertex_ai import VizierHyperparameterTuner\n",
        "\n",
        "import my_module\n",
        "\n",
        "# Set to True for remote training\n",
        "vertex_ai.init(remote=True, project=\"my-project\", location=\"my-location\", staging_bucket=\"gs://my-bucket\")\n",
        "\n",
        "X, y = pd.DataFrame(...), pd.DataFrame(...)\n",
        "\n",
        "# Define a function which returns an intialized model. Parameters of this function are tunable.\n",
        "def get_model_func(learning_rate: float, optimizer: str):\n",
        "  # Wrap the model class with `vertex_ai.remote`\n",
        "  MyModelClass = vertex_ai.remote(my_module.MyModelClass)\n",
        "\n",
        "  # Instantiate the class\n",
        "  model = MyModelClass(learning_rate=learning_rate, optimizer=optimizer)\n",
        "\n",
        "  # Optionally set remote config\n",
        "  model.fit.vertex.remote_config.display_name = \"MyModelClass-remote-training\"\n",
        "  model.fit.vertex.remote_config.staging_bucket = \"gs://my-bucket\"\n",
        "  return model\n",
        "\n",
        "# Define the hyperparameter search space\n",
        "hparam_space = [\n",
        "  {\n",
        "    \"parameter_id\": \"learning_rate\",\n",
        "    \"double_value_spec\": {\n",
        "      \"min_value\": 0.01,\n",
        "      \"max_value\": 0.05\n",
        "    }\n",
        "  }, {\n",
        "    \"parameter_id\": \"optimizer\",\n",
        "    \"categorical_value_spec\": {\n",
        "      \"values\"[\"adam\", \"sgd\"]\n",
        "    }\n",
        "  }\n",
        "]\n",
        "\n",
        "tuner = VizierHyperparameterTuner(\n",
        "  get_model_func=get_model_func,\n",
        "  max_trial_count=3,\n",
        "  parallel_trial_count=2,\n",
        "  hparam_space=hparam_space,\n",
        "  metric_id=\"accuracy\",\n",
        "  metric_goal=\"MAXIMIZE\",\n",
        "  max_failed_trial_count=0,\n",
        ")\n",
        "                               \n",
        "# Tune model using Vizier. Tuning runs locally and trials run in Vertex CustomJobs.\n",
        "# `epochs` is passed at runtime to model's fit()/train() call\n",
        "# (ex: model.fit(X, y, epochs=5)\n",
        "tuner.fit(X, y, epochs=5...)\n",
        "```\n",
        "\n",
        "*Remote tuning supported OSS ML frameworks:*\n",
        "1.  scikit-learn\n",
        "2.  Custom model\n",
        "3.  TensorFlow\n",
        "4.  PyTorch\n",
        "5.  Pytorch Lightning\n",
        "6.  TabNet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aef4f59195ad"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "This tutorial uses the <a href=\"https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html\">IRIS dataset</a>, which predicts the iris species."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "costs"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_aip:mbsdk"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Install the following packages required to execute this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9dJ5Of-dORl"
      },
      "outputs": [],
      "source": [
        "! pip3 install --upgrade --quiet google-cloud-aiplatform[preview,autologging]\n",
        "! pip3 install --upgrade --quiet lightning\n",
        "! pip3 install --upgrade --quiet tensorflow==2.12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "restart"
      },
      "source": [
        "### Colab only: Uncomment the following cell to restart the kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-ZBOjErv5mM"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "# import IPython\n",
        "\n",
        "# app = IPython.Application.instance()\n",
        "# app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "before_you_begin:nogpu"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, try the following:\n",
        "* Run `gcloud config list`.\n",
        "* Run `gcloud projects list`.\n",
        "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_project_id"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dw8q9fdQEH5"
      },
      "outputs": [],
      "source": [
        "REGION = \"us-central1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcp_authenticate"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below.\n",
        "\n",
        "**1. Vertex AI Workbench**\n",
        "* Do nothing as you are already authenticated.\n",
        "\n",
        "**2. Local JupyterLab instance, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce6043da7b33"
      },
      "outputs": [],
      "source": [
        "# ! gcloud auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0367eac06a10"
      },
      "source": [
        "**3. Colab, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21ad4dbb4a61"
      },
      "outputs": [],
      "source": [
        "# from google.colab import auth\n",
        "# auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c13224697bfb"
      },
      "source": [
        "**4. Service account or other**\n",
        "* See how to grant Cloud Storage permissions to your service account at https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bucket:mbsdk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "Create a storage bucket to store intermediate artifacts such as datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bucket"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "autoset_bucket"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91c46850b49b"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_vars"
      },
      "source": [
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LF2bwT6q-of1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import vertexai\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow import keras\n",
        "from vertexai.preview import VertexModel\n",
        "from vertexai.preview.hyperparameter_tuning import VizierHyperparameterTuner\n",
        "from vertexai.preview.tabular_models import TabNetTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk"
      },
      "source": [
        "## Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-p0YAOY64cA5"
      },
      "outputs": [],
      "source": [
        "vertexai.init(\n",
        "    project=PROJECT_ID,\n",
        "    location=REGION,\n",
        "    staging_bucket=BUCKET_URI,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ud6sRD1WWFIr"
      },
      "source": [
        "## Prepare the dataset\n",
        "\n",
        "Now load the Iris dataset and split the data into train, retrain and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_tbI76DWRWt"
      },
      "outputs": [],
      "source": [
        "dataset = load_iris()\n",
        "\n",
        "X, X_retrain, y, y_retrain = train_test_split(\n",
        "    dataset.data, dataset.target, test_size=0.60, random_state=42\n",
        ")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Data size: \", len(dataset.target))\n",
        "print(\"X_train size: \", len(X_train))\n",
        "print(\"X_retrain size: \", len(X_retrain))\n",
        "print(\"X_test size: \", len(X_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzQfxEwuY22S"
      },
      "source": [
        "## Feature transformation\n",
        "\n",
        "Next, you do feature transformations on the data using the Vertex AI remote training service.\n",
        "\n",
        "First, you re-initialize Vertex AI to enable remote training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae552dab5ed0"
      },
      "outputs": [],
      "source": [
        "# Switch to remote mode for training\n",
        "vertexai.preview.init(remote=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Kf6XbLqw1DY"
      },
      "source": [
        "### Execute remote job for fit_transform() on training data\n",
        "\n",
        "Next, indicate that the `StandardScalar` class is to be executed remotely. Then set up the data transform and call the `fit_transform()` method is executed remotely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGIuXQMBY8LO"
      },
      "outputs": [],
      "source": [
        "REMOTE_JOB_NAME = \"remote-scalar\"\n",
        "REMOTE_JOB_BUCKET = f\"{BUCKET_URI}/{REMOTE_JOB_NAME}\"\n",
        "\n",
        "# Wrap classes to enable Vertex remote execution\n",
        "# Don't need this step after import hook is implemented\n",
        "StandardScaler = vertexai.preview.remote(StandardScaler)\n",
        "\n",
        "\n",
        "# Instantiate transformer\n",
        "transformer = StandardScaler()\n",
        "\n",
        "# Set training config\n",
        "transformer.fit_transform.vertex.remote_config.display_name = (\n",
        "    f\"{REMOTE_JOB_NAME}-fit-transformer\"\n",
        ")\n",
        "transformer.fit_transform.vertex.remote_config.staging_bucket = REMOTE_JOB_BUCKET\n",
        "\n",
        "# Execute transformer on Vertex\n",
        "X_train = transformer.fit_transform(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N15AC1Niw-Rf"
      },
      "source": [
        "### Remote transform on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3tBFU3IaZ-l"
      },
      "outputs": [],
      "source": [
        "# Transform test dataset before calculate test score\n",
        "transformer.transform.vertex.remote_config.display_name = (\n",
        "    REMOTE_JOB_NAME + \"-transformer\"\n",
        ")\n",
        "transformer.transform.vertex.remote_config.staging_bucket = REMOTE_JOB_BUCKET\n",
        "\n",
        "X_test = transformer.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRmP4x0bxCdD"
      },
      "source": [
        "### Local transform on retrain data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4wzmSxJA5py"
      },
      "outputs": [],
      "source": [
        "# Switch to local transformation\n",
        "vertexai.preview.init(remote=False)\n",
        "\n",
        "X_retrain = transformer.transform(X_retrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_tbI76DWRWt"
      },
      "outputs": [],
      "source": [
        "dataset = load_iris()\n",
        "\n",
        "X, X_retrain, y, y_retrain = train_test_split(\n",
        "    dataset.data, dataset.target, test_size=0.60, random_state=42\n",
        ")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Data size: \", len(dataset.target))\n",
        "print(\"X_train size: \", len(X_train))\n",
        "print(\"X_retrain size: \", len(X_retrain))\n",
        "print(\"X_test size: \", len(X_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aNMacb-Ghxs"
      },
      "source": [
        "## scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1uGe_lIviwf"
      },
      "source": [
        "### Remote tuning\n",
        "\n",
        "First, hyperparameter tune the scikit-learn model as a remote tuning job:\n",
        "\n",
        "- Reinitialize Vertex AI for remote tuning.\n",
        "- Set the hyperparameter tuning configuration.\n",
        "- Invoke the hyperparameter tuning job.\n",
        "    - Set LogisticRegression for the remote tuning job.\n",
        "    - Invoke LogisticRegression locally which will launch the remote training job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZzUbiUY5Eko"
      },
      "outputs": [],
      "source": [
        "MAX_TRIAL_COUNT = 4\n",
        "PARALLEL_TRIAL_COUNT = 2\n",
        "\n",
        "HPARAM_SPACE = [\n",
        "    {\"parameter_id\": \"C\", \"discrete_value_spec\": {\"values\": [0.1, 0.5, 1.0]}}\n",
        "]\n",
        "# Use LogisticRegression's score() function\n",
        "METRIC_ID = \"custom\"\n",
        "METRIC_GOAL = \"MAXIMIZE\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNnzgrUKVQDP"
      },
      "outputs": [],
      "source": [
        "REMOTE_JOB_NAME = \"test-sdk2-remote-training\"\n",
        "REMOTE_JOB_BUCKET = \"/\".join([BUCKET_URI, REMOTE_JOB_NAME])\n",
        "\n",
        "# Switch to remote mode for training\n",
        "vertexai.preview.init(remote=True)\n",
        "\n",
        "\n",
        "def get_model_func(C: float):\n",
        "    from sklearn.linear_model import _logistic\n",
        "\n",
        "    # Wrap classes to enable Vertex remote execution\n",
        "    # Don't need this step after import hook is implemented\n",
        "    LogisticRegression = vertexai.preview.remote(_logistic.LogisticRegression)\n",
        "\n",
        "    # Instantiate model. C will be tuned.\n",
        "    model = LogisticRegression(C=C)\n",
        "\n",
        "    # Set training config\n",
        "    model.fit.vertex.remote_config.display_name = REMOTE_JOB_NAME + \"-test-tuning\"\n",
        "    return model\n",
        "\n",
        "\n",
        "tuner = VizierHyperparameterTuner(\n",
        "    get_model_func=get_model_func,\n",
        "    max_trial_count=MAX_TRIAL_COUNT,\n",
        "    parallel_trial_count=PARALLEL_TRIAL_COUNT,\n",
        "    hparam_space=HPARAM_SPACE,\n",
        "    metric_id=METRIC_ID,\n",
        "    metric_goal=METRIC_GOAL,\n",
        ")\n",
        "\n",
        "# Tune model using Vizier. Tuning runs locally while trials run on Vertex.\n",
        "tuner.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7a834e39a0b"
      },
      "source": [
        "#### Get the best model\n",
        "\n",
        "From the hyperparameter tuning, get the best model from the trials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4WynLt95ozt"
      },
      "outputs": [],
      "source": [
        "best_model = tuner.get_best_models()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOy4uXzs5jPF"
      },
      "source": [
        "#### Local evaluation\n",
        "\n",
        "Next, evaluate the best model from the trials locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I39eqL8L5qLN"
      },
      "outputs": [],
      "source": [
        "# Switch to local mode for testing\n",
        "vertexai.preview.init(remote=False)\n",
        "\n",
        "# Evaluate model's accuracy score\n",
        "print(f\"Train accuracy: {best_model.score(X_train, y_train)}\")\n",
        "print(f\"Test accuracy: {best_model.score(X_test, y_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mPjqC2Q_wsH"
      },
      "source": [
        "### Local tuning\n",
        "\n",
        "Now, you repeat the same, but do the tuning locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRHxOjkiHYpd"
      },
      "outputs": [],
      "source": [
        "# Switch to local mode for training\n",
        "vertexai.preview.init(remote=False)\n",
        "\n",
        "\n",
        "def get_model_func(C: float):\n",
        "    # Instantiate model. C will be tuned.\n",
        "    return LogisticRegression(C=C)\n",
        "\n",
        "\n",
        "tuner = VizierHyperparameterTuner(\n",
        "    get_model_func=get_model_func,\n",
        "    max_trial_count=MAX_TRIAL_COUNT,\n",
        "    parallel_trial_count=PARALLEL_TRIAL_COUNT,\n",
        "    hparam_space=HPARAM_SPACE,\n",
        "    metric_id=METRIC_ID,\n",
        "    metric_goal=METRIC_GOAL,\n",
        ")\n",
        "\n",
        "# Tune model using Vizier. Tuning and training run locally.\n",
        "tuner.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myw6W0iF53W-"
      },
      "source": [
        "#### Local evaluation\n",
        "\n",
        "Finally, you do a local evaluation of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iobnepRZ53XK"
      },
      "outputs": [],
      "source": [
        "best_model = tuner.get_best_models()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DoWOMEH53XL"
      },
      "outputs": [],
      "source": [
        "# Switch to local mode for testing\n",
        "vertexai.preview.init(remote=False)\n",
        "\n",
        "best_model = tuner.get_best_models()[0]\n",
        "\n",
        "# Evaluate model's accuracy score\n",
        "print(f\"Train accuracy: {best_model.score(X_train, y_train)}\")\n",
        "print(f\"Test accuracy: {best_model.score(X_test, y_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdg47y-wjoxT"
      },
      "source": [
        "## PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1uGe_lIviwf"
      },
      "source": [
        "### Remote tuning\n",
        "\n",
        "First, hyperparameter tune the PyTorch model as a remote tuning job:\n",
        "\n",
        "- Reinitialize Vertex AI for remote tuning.\n",
        "- Set the hyperparameter tuning configuration.\n",
        "- Invoke the hyperparameter tuning job.\n",
        "    - Set TorchLogisticRegression for the remote training job.\n",
        "    - Invoke TorchLogisticRegression locally which will launch the remote training job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOXFY8vLrmpl"
      },
      "outputs": [],
      "source": [
        "# Switch to remote mode for training\n",
        "vertexai.preview.init(remote=True)\n",
        "\n",
        "HPARAM_SPACE = [\n",
        "    {\n",
        "        \"parameter_id\": \"num_epochs\",\n",
        "        \"integer_value_spec\": {\"min_value\": 100, \"max_value\": 150},\n",
        "    },\n",
        "    {\"parameter_id\": \"lr\", \"double_value_spec\": {\"min_value\": 0.01, \"max_value\": 0.05}},\n",
        "]\n",
        "\n",
        "\n",
        "# Define model\n",
        "class TorchLogisticRegression(VertexModel, torch.nn.Module):\n",
        "    def __init__(self, input_size: int, output_size: int):\n",
        "        torch.nn.Module.__init__(self)\n",
        "        VertexModel.__init__(self)\n",
        "        self.linear = torch.nn.Linear(input_size, output_size)\n",
        "        self.softmax = torch.nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.softmax(self.linear(x))\n",
        "\n",
        "    @vertexai.preview.developer.mark.train()\n",
        "    def train(self, X, y, num_epochs, lr):\n",
        "        X, y = torch.tensor(X).to(torch.float32), torch.tensor(y)\n",
        "        dataloader = torch.utils.data.DataLoader(\n",
        "            torch.utils.data.TensorDataset(X, y),\n",
        "            batch_size=10,\n",
        "            shuffle=True,\n",
        "            generator=torch.Generator(device=X.device),\n",
        "        )\n",
        "\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.SGD(self.parameters(), lr=lr)\n",
        "\n",
        "        for t in range(num_epochs):\n",
        "            for batch, (X, y) in enumerate(dataloader):\n",
        "                optimizer.zero_grad()\n",
        "                pred = self(X)\n",
        "                loss = criterion(pred, y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = torch.tensor(X).to(torch.float32)\n",
        "        with torch.no_grad():\n",
        "            pred = torch.argmax(self(X), dim=1)\n",
        "        return pred\n",
        "\n",
        "\n",
        "def get_model_func():\n",
        "    # Instantiate model\n",
        "    model = TorchLogisticRegression(4, 3)\n",
        "\n",
        "    # Set training config\n",
        "    model.train.vertex.remote_config.display_name = REMOTE_JOB_NAME + \"-test-tuning\"\n",
        "    return model\n",
        "\n",
        "\n",
        "tuner = VizierHyperparameterTuner(\n",
        "    get_model_func=get_model_func,\n",
        "    max_trial_count=MAX_TRIAL_COUNT,\n",
        "    parallel_trial_count=PARALLEL_TRIAL_COUNT,\n",
        "    hparam_space=HPARAM_SPACE,\n",
        ")\n",
        "\n",
        "# Tune model using Vizier. Tuning runs locally while trials run on Vertex.\n",
        "tuner.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae79a205ea1e"
      },
      "source": [
        "#### Get the best model\n",
        "\n",
        "From the hyperparameter tuning, get the best model from the trials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vg_q24ErsKun"
      },
      "outputs": [],
      "source": [
        "best_model = tuner.get_best_models()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOy4uXzs5jPF"
      },
      "source": [
        "#### Local evaluation\n",
        "\n",
        "Next, evaluate the best model from the trials locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFo8usi_kB-F"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Switch to local mode for testing\n",
        "vertexai.preview.init(remote=False)\n",
        "\n",
        "# Evaluate model's accuracy score\n",
        "print(f\"Train accuracy: {accuracy_score(y_train, best_model.predict(X_train))}\")\n",
        "print(f\"Test accuracy: {accuracy_score(y_test, best_model.predict(X_test))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYG9UsK_sTAv"
      },
      "source": [
        "### Local tuning\n",
        "\n",
        "Now, you repeat the same, but do the tuning locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmBenlW5sV8f"
      },
      "outputs": [],
      "source": [
        "# Switch to local mode for training\n",
        "vertexai.preview.init(remote=False)\n",
        "\n",
        "\n",
        "def get_model_func():\n",
        "    # Instantiate model\n",
        "    return TorchLogisticRegression(4, 3)\n",
        "\n",
        "\n",
        "tuner = VizierHyperparameterTuner(\n",
        "    get_model_func=get_model_func,\n",
        "    max_trial_count=MAX_TRIAL_COUNT,\n",
        "    parallel_trial_count=PARALLEL_TRIAL_COUNT,\n",
        "    hparam_space=HPARAM_SPACE,\n",
        ")\n",
        "\n",
        "# Tune model using Vizier. Tuning and training runs locally.\n",
        "tuner.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myw6W0iF53W-"
      },
      "source": [
        "#### Local evaluation\n",
        "\n",
        "Finally, you do a local evaluation of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iobnepRZ53XK"
      },
      "outputs": [],
      "source": [
        "best_model = tuner.get_best_models()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DoWOMEH53XL"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Switch to local mode for testing\n",
        "vertexai.preview.init(remote=False)\n",
        "\n",
        "best_model = tuner.get_best_models()[0]\n",
        "\n",
        "# Evaluate model's accuracy score\n",
        "print(f\"Train accuracy: {accuracy_score(y_train, best_model.predict(X_train))}\")\n",
        "print(f\"Test accuracy: {accuracy_score(y_test, best_model.predict(X_test))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fC-g1B7wGgZ"
      },
      "source": [
        "## TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1uGe_lIviwf"
      },
      "source": [
        "### Remote tuning\n",
        "\n",
        "First, hyperparameter tune the TensorFlow model as a remote tuning job:\n",
        "\n",
        "- Reinitialize Vertex AI for remote tuning.\n",
        "- Set the hyperparameter tuning configuration.\n",
        "- Invoke the hyperparameter tuning job for remote execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgV3626pGoH2"
      },
      "outputs": [],
      "source": [
        "# Switch to remote mode for training\n",
        "vertexai.preview.init(remote=True)\n",
        "\n",
        "HPARAM_SPACE = [\n",
        "    {\n",
        "        \"parameter_id\": \"optimizer\",\n",
        "        \"categorical_value_spec\": {\"values\": [\"adam\", \"sgd\"]},\n",
        "    },\n",
        "    {\"parameter_id\": \"epochs\", \"discrete_value_spec\": {\"values\": [10, 15, 20]}},\n",
        "]\n",
        "\n",
        "\n",
        "def get_model_func(optimizer: str):\n",
        "    # Wrap classes to enable Vertex remote execution\n",
        "    # Don't need this step after import hook is implemented\n",
        "    keras.Sequential = vertexai.preview.remote(keras.Sequential)\n",
        "\n",
        "    # Instantiate model\n",
        "    model = keras.Sequential(\n",
        "        [keras.layers.Dense(5, input_shape=(4,)), keras.layers.Softmax()]\n",
        "    )\n",
        "\n",
        "    # Specify optimizer and loss function\n",
        "    model.compile(optimizer=optimizer, loss=\"mean_squared_error\")\n",
        "\n",
        "    # Set training config\n",
        "    model.fit.vertex.remote_config.display_name = REMOTE_JOB_NAME + \"-test-tuning\"\n",
        "    return model\n",
        "\n",
        "\n",
        "tuner = VizierHyperparameterTuner(\n",
        "    get_model_func=get_model_func,\n",
        "    max_trial_count=MAX_TRIAL_COUNT,\n",
        "    parallel_trial_count=PARALLEL_TRIAL_COUNT,\n",
        "    hparam_space=HPARAM_SPACE,\n",
        ")\n",
        "\n",
        "# Tune model using Vizier. Tuning runs locally while trials run on Vertex.\n",
        "# batch_size is fixed input that is passed to model.fit().\n",
        "tuner.fit(X_train, y_train, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d419933b73b"
      },
      "source": [
        "#### Get the best model\n",
        "\n",
        "From the hyperparameter tuning, get the best model from the trials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vg_q24ErsKun"
      },
      "outputs": [],
      "source": [
        "best_model = tuner.get_best_models()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOy4uXzs5jPF"
      },
      "source": [
        "#### Local evaluation\n",
        "\n",
        "Next, evaluate the best model from the trials locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFo8usi_kB-F"
      },
      "outputs": [],
      "source": [
        "# Switch to local mode for testing\n",
        "vertexai.preview.init(remote=False)\n",
        "\n",
        "# Evaluate model's accuracy score\n",
        "print(f\"Train loss: {best_model.evaluate(X_train, y_train)}\")\n",
        "print(f\"Test loss: {best_model.evaluate(X_test, y_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYG9UsK_sTAv"
      },
      "source": [
        "### Local tuning\n",
        "\n",
        "Now, you repeat the same, but do the tuning locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMyrRUe9a2ci"
      },
      "outputs": [],
      "source": [
        "# Switch to local mode for training\n",
        "vertexai.preview.init(remote=False)\n",
        "\n",
        "\n",
        "def get_model_func(optimizer: str):\n",
        "    # Instantiate model\n",
        "    model = keras.Sequential(\n",
        "        [keras.layers.Dense(5, input_shape=(4,)), keras.layers.Softmax()]\n",
        "    )\n",
        "\n",
        "    # Specify optimizer and loss function\n",
        "    model.compile(optimizer=optimizer, loss=\"mean_squared_error\")\n",
        "    return model\n",
        "\n",
        "\n",
        "tuner = VizierHyperparameterTuner(\n",
        "    get_model_func=get_model_func,\n",
        "    max_trial_count=MAX_TRIAL_COUNT,\n",
        "    parallel_trial_count=PARALLEL_TRIAL_COUNT,\n",
        "    hparam_space=HPARAM_SPACE,\n",
        ")\n",
        "\n",
        "# Tune model using Vizier. Tuning and trials run locally.\n",
        "# batch_size is fixed input that is passed to model.fit().\n",
        "tuner.fit(X_train, y_train, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myw6W0iF53W-"
      },
      "source": [
        "#### Local evaluation\n",
        "\n",
        "Finally, you do a local evaluation of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iobnepRZ53XK"
      },
      "outputs": [],
      "source": [
        "best_model = tuner.get_best_models()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DoWOMEH53XL"
      },
      "outputs": [],
      "source": [
        "# Switch to local mode for testing\n",
        "vertexai.preview.init(remote=False)\n",
        "\n",
        "best_model = tuner.get_best_models()[0]\n",
        "\n",
        "# Evaluate model's accuracy score\n",
        "print(f\"Train loss: {best_model.evaluate(X_train, y_train)}\")\n",
        "print(f\"Test loss: {best_model.evaluate(X_test, y_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6amYysl-h_x"
      },
      "source": [
        "## PyTorch Lightning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1uGe_lIviwf"
      },
      "source": [
        "### Remote tuning\n",
        "\n",
        "First, hyperparameter tune the PyTorch Lightning model as a remote tuning job:\n",
        "\n",
        "- Reinitialize Vertex AI for remote tuning.\n",
        "- Set the hyperparameter tuning configuration.\n",
        "- Invoke the hyperparameter tuning job for remote execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9CwMEdqudsH"
      },
      "outputs": [],
      "source": [
        "import lightning.pytorch as pl\n",
        "\n",
        "# Switch to remote mode for training\n",
        "vertexai.preview.init(remote=True)\n",
        "\n",
        "HPARAM_SPACE = [\n",
        "    {\n",
        "        \"parameter_id\": \"batch_size\",\n",
        "        \"integer_value_spec\": {\"min_value\": 10, \"max_value\": 20},\n",
        "    }\n",
        "]\n",
        "\n",
        "PARALLEL_TRIAL_COUNT = 1\n",
        "\n",
        "# Wrap classes to enable Vertex remote execution\n",
        "# Don't need this step after import hook is implemented\n",
        "pl.Trainer = vertexai.preview.remote(pl.Trainer)\n",
        "\n",
        "\n",
        "class LitLogisticRegression(pl.LightningModule):\n",
        "    def __init__(self, input_size: int, output_size: int):\n",
        "        super().__init__()\n",
        "        self.linear = torch.nn.Linear(input_size, output_size)\n",
        "        self.softmax = torch.nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.softmax(self.linear(x))\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch[0], batch[1]\n",
        "        y_hat = self(x)\n",
        "        loss = torch.nn.functional.cross_entropy(y_hat, y)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=0.05)\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = torch.tensor(X).to(torch.float32)\n",
        "        with torch.no_grad():\n",
        "            pred = torch.argmax(self(X), dim=1)\n",
        "        return pred\n",
        "\n",
        "\n",
        "def get_model_func(X_train, y_train, batch_size):\n",
        "    model = LitLogisticRegression(4, 3)\n",
        "\n",
        "    # Instantiate the training dataloader. batch_size will be tuned.\n",
        "    train_dataloader = torch.utils.data.DataLoader(\n",
        "        torch.utils.data.TensorDataset(\n",
        "            torch.tensor(X_train).to(torch.float32),\n",
        "            torch.tensor(y_train),\n",
        "        ),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "    )\n",
        "\n",
        "    trainer = pl.Trainer(max_epochs=10, accelerator=\"cpu\", log_every_n_steps=10)\n",
        "\n",
        "    # Set training config\n",
        "    trainer.fit.vertex.remote_config.display_name = REMOTE_JOB_NAME + \"-test-tuning\"\n",
        "\n",
        "    return {\"model\": model, \"train_dataloaders\": train_dataloader, \"trainer\": trainer}\n",
        "\n",
        "\n",
        "tuner = VizierHyperparameterTuner(\n",
        "    get_model_func=get_model_func,\n",
        "    max_trial_count=MAX_TRIAL_COUNT,\n",
        "    parallel_trial_count=PARALLEL_TRIAL_COUNT,\n",
        "    hparam_space=HPARAM_SPACE,\n",
        ")\n",
        "\n",
        "# Tune model using Vizier. Tuning runs locally while trials run on Vertex.\n",
        "tuner.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ecbd00fbaa6"
      },
      "source": [
        "#### Get the best model\n",
        "\n",
        "From the hyperparameter tuning, get the best model from the trials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vg_q24ErsKun"
      },
      "outputs": [],
      "source": [
        "best_model = tuner.get_best_models()[0]\n",
        "trainer = best_model[\"trainer\"]\n",
        "model = best_model[\"model\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOy4uXzs5jPF"
      },
      "source": [
        "#### Local evaluation\n",
        "\n",
        "Next, evaluate the best model from the trials locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtVlxUdZvatX"
      },
      "outputs": [],
      "source": [
        "# Switch to local mode for testing\n",
        "vertexai.preview.init(remote=False)\n",
        "\n",
        "# Evaluate model's accuracy score\n",
        "print(f\"Train accuracy: {accuracy_score(y_train, model.predict(X_train))}\")\n",
        "print(f\"Test accuracy: {accuracy_score(y_test, model.predict(X_test))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYG9UsK_sTAv"
      },
      "source": [
        "### Local tuning\n",
        "\n",
        "Now, you repeat the same, but do the tuning locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kD8x0KSb1rsn"
      },
      "outputs": [],
      "source": [
        "# Switch to local mode for training\n",
        "vertexai.preview.init(remote=False)\n",
        "\n",
        "\n",
        "def get_model_func(X_train, y_train, batch_size):\n",
        "    model = LitLogisticRegression(4, 3)\n",
        "\n",
        "    # Instantiate the training dataloader. batch_size will be tuned.\n",
        "    train_dataloader = torch.utils.data.DataLoader(\n",
        "        torch.utils.data.TensorDataset(\n",
        "            torch.tensor(X_train).to(torch.float32),\n",
        "            torch.tensor(y_train),\n",
        "        ),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "    )\n",
        "\n",
        "    trainer = pl.Trainer(max_epochs=10, accelerator=\"cpu\", log_every_n_steps=10)\n",
        "\n",
        "    # Set remote to False\n",
        "    trainer.fit.vertex.remote = False\n",
        "\n",
        "    return {\"model\": model, \"train_dataloaders\": train_dataloader, \"trainer\": trainer}\n",
        "\n",
        "\n",
        "tuner = VizierHyperparameterTuner(\n",
        "    get_model_func=get_model_func,\n",
        "    max_trial_count=MAX_TRIAL_COUNT,\n",
        "    parallel_trial_count=PARALLEL_TRIAL_COUNT,\n",
        "    hparam_space=HPARAM_SPACE,\n",
        ")\n",
        "\n",
        "# Tune model using Vizier. Tuning and training run locally.\n",
        "tuner.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myw6W0iF53W-"
      },
      "source": [
        "#### Local evaluation\n",
        "\n",
        "Finally, you do a local evaluation of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0-vwU1R62Dc"
      },
      "outputs": [],
      "source": [
        "best_model = tuner.get_best_models()[0]\n",
        "trainer = best_model[\"trainer\"]\n",
        "model = best_model[\"model\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiAZ2LeY62Dc"
      },
      "outputs": [],
      "source": [
        "# Switch to local mode for testing\n",
        "vertexai.preview.init(remote=False)\n",
        "\n",
        "# Evaluate model's accuracy score\n",
        "print(f\"Train accuracy: {accuracy_score(y_train, model.predict(X_train))}\")\n",
        "print(f\"Test accuracy: {accuracy_score(y_test, model.predict(X_test))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZjLftoynHkH"
      },
      "source": [
        "## TabNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1uGe_lIviwf"
      },
      "source": [
        "### Remote tuning\n",
        "\n",
        "First, hyperparameter tune the TabNet model as a remote tuning job:\n",
        "\n",
        "- Reinitialize Vertex AI for remote tuning.\n",
        "- Set the hyparameter tuning configuration.\n",
        "- Invoke the hyperparameter tuning job for remote execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Dz2dAQ88PWt"
      },
      "outputs": [],
      "source": [
        "# Switch to remote mode for testing\n",
        "vertexai.preview.init(remote=True)\n",
        "\n",
        "PARALLEL_TRIAL_COUNT = 2\n",
        "HPARAM_SPACE = [\n",
        "    {\n",
        "        \"parameter_id\": \"batch_size\",\n",
        "        \"integer_value_spec\": {\"min_value\": 10, \"max_value\": 100},\n",
        "    },\n",
        "    {\n",
        "        \"parameter_id\": \"learning_rate\",\n",
        "        \"double_value_spec\": {\"min_value\": 0.01, \"max_value\": 0.05},\n",
        "    },\n",
        "]\n",
        "\n",
        "\n",
        "def get_model_func(batch_size, learning_rate):\n",
        "    # Instantiate model\n",
        "    trainer = TabNetTrainer(\n",
        "        model_type=\"classification\",\n",
        "        target_column=\"target\",\n",
        "        learning_rate=learning_rate,\n",
        "        max_steps=100,\n",
        "        batch_size=batch_size,\n",
        "    )\n",
        "\n",
        "    # Set training config\n",
        "    trainer.fit.vertex.remote_config.display_name = REMOTE_JOB_NAME + \"-test-tuning\"\n",
        "    return trainer\n",
        "\n",
        "\n",
        "tuner = VizierHyperparameterTuner(\n",
        "    get_model_func=get_model_func,\n",
        "    max_trial_count=MAX_TRIAL_COUNT,\n",
        "    parallel_trial_count=PARALLEL_TRIAL_COUNT,\n",
        "    hparam_space=HPARAM_SPACE,\n",
        ")\n",
        "\n",
        "\n",
        "# TabNet takes a single dataframe containing features and target column.\n",
        "# y is the target column name.\n",
        "columns = [\"0\", \"1\", \"2\", \"3\"]\n",
        "training_data = pd.DataFrame(X_train, columns=columns)\n",
        "training_data[\"target\"] = y_train\n",
        "training_data[\"target\"] = training_data[\"target\"].astype(\"category\")\n",
        "\n",
        "X_train = pd.DataFrame(X_train, columns=columns)\n",
        "y_train = pd.DataFrame(y_train, columns=[\"target\"])\n",
        "y_train[\"target\"] = y_train[\"target\"].astype(\"category\")\n",
        "\n",
        "X_test = pd.DataFrame(X_test, columns=columns)\n",
        "y_test = pd.DataFrame(y_test, columns=[\"target\"])\n",
        "y_test[\"target\"] = y_test[\"target\"].astype(\"category\")\n",
        "\n",
        "# Tune model using Vizier. Tuning runs locally while trials run on Vertex.\n",
        "tuner.fit(training_data, \"target\", X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d585aba0665"
      },
      "source": [
        "#### Get the best model\n",
        "\n",
        "From the hyperparameter tuning, get the best model from the trials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vg_q24ErsKun"
      },
      "outputs": [],
      "source": [
        "best_model = tuner.get_best_models()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOy4uXzs5jPF"
      },
      "source": [
        "#### Local evaluation\n",
        "\n",
        "Next, evaluate the best model from the trials locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2RfC16y66g2"
      },
      "outputs": [],
      "source": [
        "# Switch to local mode for testing\n",
        "vertexai.preview.init(remote=False)\n",
        "\n",
        "# Evaluate model's accuracy score\n",
        "print(f\"Train accuracy: {accuracy_score(y_train, best_model.predict(X_train))}\")\n",
        "print(f\"Test accuracy: {accuracy_score(y_test, best_model.predict(X_test))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d894dd49b291"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0b28fec16529"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "delete_bucket = False\n",
        "\n",
        "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
        "    ! gsutil rm -rf {BUCKET_URI}"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "remote_hyperparameter_tuning.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

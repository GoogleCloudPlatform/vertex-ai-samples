{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "Get started with TabNet builtin algorithm for training tabular models\n",
        "\n",
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/tabnet/get_started_with_tabnet.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/tabnet/get_started_with_tabnet.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/tabnet/get_started_with_tabnet.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>                                                                                               \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial demonstrates how to use the TabNet builtin algorithm service on the Vertex AI platform to train custom tabular models.\n",
        "\n",
        "TabNet combines the best of two worlds: it is explainable (similar to simpler tree-based models) while benefiting from high performance (similar to deep neural networks). This makes it great for retailers, finance and insurance industry applications such as predicting credit scores, fraud detection and forecasting. \n",
        "\n",
        "TabNet uses a machine learning technique called sequential attention to select which model features to reason from at each step in the model. This mechanism makes it possible to explain how the model arrives at its predictions and helps it learn more accurate models. TabNet not only outperforms other neural networks and decision trees but also provides interpretable feature attributions. \n",
        "\n",
        "Research paper: [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/pdf/1908.07442.pdf)\n",
        "\n",
        "\n",
        "Learn more about [Tabular Workflow for TabNet](https://cloud.google.com/vertex-ai/docs/tabular-data/tabular-workflows/tabnet)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5040751873a"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this notebook, you learn how to run `Vertex AI TabNet` built algorithm for training custom tabular models.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services and resources:\n",
        "\n",
        "- `Vertex AI TabNet`\n",
        "- `Vertex AI Prediction`\n",
        "- `Vertex AI Models`\n",
        "- `Vertex AI Endpoints`\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Get the training data.\n",
        "- Configure training parameters for the `Vertex AI TabNet` container.\n",
        "- Train the model using `Vertex AI Training` using CSV data.\n",
        "- Upload the model as a `Vertex AI Model` resource.\n",
        "- Deploy the `Vertex AI Model` resource to a `Vertex AI Endpoint` resource.\n",
        "- Make a prediction with the deployed model.\n",
        "- Hyperparameter tuning the `Vertex AI TabNet` model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac8c8586ab03"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "This tutorial uses the `petfinder` in the public Cloud Storage bucket `gs://cloud-samples-data/ai-platform-unified/datasets/tabular/`, which was generated from the [PetFinder.my Adoption Prediction](https://www.kaggle.com/c/petfinder-adoption-prediction). This dataset predicts how quickly an animal is adopted.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fc0ad661ebb"
      },
      "source": [
        "### Costs \n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7EUnXsZhAGF"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Install the following packages to execute this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b4ef9b72d43"
      },
      "outputs": [],
      "source": [
        "! pip3 install --upgrade tensorflow --quiet\n",
        "! pip3 install --upgrade google-cloud-aiplatform\n",
        "! gcloud components update --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "restart"
      },
      "source": [
        "### Colab only: Uncomment the following cell to restart the kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-ZBOjErv5mM"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "# import IPython\n",
        "\n",
        "# app = IPython.Application.instance()\n",
        "# app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "before_you_begin:nogpu"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "before_you_begin:nogpu"
      },
      "source": [
        "### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, try the following:\n",
        "* Run `gcloud config list`.\n",
        "* Run `gcloud projects list`.\n",
        "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_project_id"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "region"
      },
      "outputs": [],
      "source": [
        "REGION = \"us-central1\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcp_authenticate"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvQeFm3Gv5mR"
      },
      "source": [
        "**1. Vertex AI Workbench**\n",
        "* Do nothing as you are already authenticated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad1138a125ea"
      },
      "source": [
        "**2. Local JupyterLab instance, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce6043da7b33"
      },
      "outputs": [],
      "source": [
        "# ! gcloud auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0367eac06a10"
      },
      "source": [
        "**3. Colab, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21ad4dbb4a61"
      },
      "outputs": [],
      "source": [
        "# from google.colab import auth\n",
        "# auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c13224697bfb"
      },
      "source": [
        "**4. Service account or other**\n",
        "* See how to grant Cloud Storage permissions to your service account at https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bucket:mbsdk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "Create a storage bucket to store intermediate artifacts such as datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bucket"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EcIXiGsCePi"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIq7R4HZCfIc"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRUOFELefqf1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from google.cloud import aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "accelerators:training,cpu,prediction,cpu,mbsdk"
      },
      "source": [
        "#### Set hardware accelerators\n",
        "\n",
        "You can set hardware accelerators for training and prediction.\n",
        "\n",
        "Set the variables `DEPLOY_GPU/DEPLOY_NGPU` to use a container image supporting a GPU and the number of GPUs allocated to the virtual machine (VM) instance. For example, to use a GPU container image with 4 Nvidia Telsa K80 GPUs allocated to each VM, you would specify:\n",
        "\n",
        "    (aiplatform.gapic.AcceleratorType.NVIDIA_TESLA_K80, 4)\n",
        "\n",
        "\n",
        "Otherwise specify `(None, None)` to use a container image to run on a CPU.\n",
        "\n",
        "Learn more about [hardware accelerator support for your region](https://cloud.google.com/vertex-ai/docs/general/locations#accelerators).\n",
        "\n",
        "*Note*: TF releases before 2.3 for GPU support will fail to load the custom model in this tutorial. It is a known issue and fixed in TF 2.3. This is caused by static graph ops that are generated in the serving function. If you encounter this issue on your own custom models, use a container image for TF 2.3 with GPU support."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "accelerators:training,cpu,prediction,cpu,mbsdk"
      },
      "outputs": [],
      "source": [
        "TRAIN_GPU, TRAIN_NGPU = (aiplatform.gapic.AcceleratorType.NVIDIA_TESLA_K80, 1)\n",
        "\n",
        "DEPLOY_GPU, DEPLOY_NGPU = (aiplatform.gapic.AcceleratorType.NVIDIA_TESLA_K80, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "machine:training"
      },
      "source": [
        "#### Set machine types\n",
        "\n",
        "Next, set the machine types to use for training and prediction.\n",
        "\n",
        "- Set the variables `TRAIN_COMPUTE` and `DEPLOY_COMPUTE` to configure your compute resources for training and prediction.\n",
        " - `machine type`\n",
        "     - `n1-standard`: 3.75GB of memory per vCPU\n",
        "     - `n1-highmem`: 6.5GB of memory per vCPU\n",
        "     - `n1-highcpu`: 0.9 GB of memory per vCPU\n",
        " - `vCPUs`: number of \\[2, 4, 8, 16, 32, 64, 96 \\]\n",
        "\n",
        "*Note: The following is not supported for training:*\n",
        "\n",
        " - `standard`: 2 vCPUs\n",
        " - `highcpu`: 2, 4 and 8 vCPUs\n",
        "\n",
        "*Note: You may also use n2 and e2 machine types for training and deployment, but they do not support GPUs*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "machine:training"
      },
      "outputs": [],
      "source": [
        "TRAIN_COMPUTE = \"n1-standard-4\"\n",
        "print(\"Train machine type\", TRAIN_COMPUTE)\n",
        "\n",
        "DEPLOY_COMPUTE = \"n1-standard-4\"\n",
        "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bffd1dbc8a8c"
      },
      "source": [
        "#### Set the training container\n",
        "\n",
        "Next, you use the prebuilt `Vertex AI TabNet` container for training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f29b4eab8132"
      },
      "outputs": [],
      "source": [
        "TRAIN_IMAGE = \"us-docker.pkg.dev/vertex-ai-restricted/builtin-algorithm/tab_net_v2\"\n",
        "\n",
        "print(\"Training:\", TRAIN_IMAGE, TRAIN_GPU, TRAIN_NGPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "container:training,prediction"
      },
      "source": [
        "#### Set pre-built container for deployment\n",
        "\n",
        "Set the pre-built Docker container image for prediction.\n",
        "\n",
        "\n",
        "For the latest list, see [Pre-built containers for prediction](https://cloud.google.com/ai-platform-unified/docs/predictions/pre-built-containers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "container:training,prediction"
      },
      "outputs": [],
      "source": [
        "TF = \"2.5\".replace(\".\", \"-\")\n",
        "\n",
        "if TF[0] == \"2\":\n",
        "    if DEPLOY_GPU:\n",
        "        DEPLOY_VERSION = \"tf2-gpu.{}\".format(TF)\n",
        "    else:\n",
        "        DEPLOY_VERSION = \"tf2-cpu.{}\".format(TF)\n",
        "\n",
        "DEPLOY_IMAGE = \"{}-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(\n",
        "    REGION.split(\"-\")[0], DEPLOY_VERSION\n",
        ")\n",
        "\n",
        "print(\"Deployment:\", DEPLOY_IMAGE, DEPLOY_GPU, DEPLOY_NGPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dd2b940d35d"
      },
      "source": [
        "## Get the training data\n",
        "\n",
        "First, you get a copy of the training data -- as a CSV file -- from a public Cloud Storage bucket and copy the training data to your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoVqAMOuedPe"
      },
      "outputs": [],
      "source": [
        "# Please note that if you use csv input, the first column is the label column.\n",
        "\n",
        "IMPORT_FILE = \"petfinder-tabular-classification-tabnet-with-header.csv\"\n",
        "TRAINING_DATA_PATH = f\"{BUCKET_URI}/data/petfinder/train.csv\"\n",
        "\n",
        "! gsutil cp gs://cloud-samples-data/ai-platform-unified/datasets/tabular/{IMPORT_FILE} {TRAINING_DATA_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_custom_container_training_job:mbsdk,no_model"
      },
      "source": [
        "### Create and run `Vertex AI TabNet` training job\n",
        "\n",
        "\n",
        "To train a TabNet custom model, you perform two steps: 1) create a custom training job, and 2) run the job.\n",
        "\n",
        "#### Create custom training job\n",
        "\n",
        "A custom training job is created with the `CustomTrainingJob` class, with the following parameters:\n",
        "\n",
        "- `display_name`: The human readable name for the custom training job.\n",
        "- `container_uri`: The training container image.\n",
        "- `model_serving_container_image_uri`: The URI of a container that can serve predictions for your model — either a prebuilt "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3307f420d27"
      },
      "outputs": [],
      "source": [
        "DATASET_NAME = \"petfinder\"  # Change to your dataset name.\n",
        "\n",
        "job = aiplatform.CustomContainerTrainingJob(\n",
        "    display_name=f\"{DATASET_NAME}\",\n",
        "    container_uri=TRAIN_IMAGE,\n",
        "    model_serving_container_image_uri=DEPLOY_IMAGE,\n",
        ")\n",
        "\n",
        "print(job)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc_I_XHz8z6B"
      },
      "source": [
        "## Configure parameter settings for TabNet training\n",
        "\n",
        "The following table shows the parameters for the TabNet training job:\n",
        "\n",
        "| Parameter | Data type | Description | Required |\n",
        "|--|--|--|--|\n",
        "| `preprocess` | boolean argument | Specify this to enable automatic preprocessing. | No|\n",
        "| `job_dir` | string | Cloud Storage directory where the model output files will be stored. | Yes |\n",
        "| `input_metadata_path` | string | The GCS path to the TabNet-specific metadata for the training dataset. Please see above on how to create the metadata. | No . |\n",
        "| `training_data_path` | string | Cloud Storage pattern where training data is stored. | Yes |\n",
        "| `validation_data_path` | string | Cloud Storage pattern where eval data is stored. | No |\n",
        "| `test_data_path` | string | Cloud Storage pattern where test data is stored. | Yes |\n",
        "| `input_type` | string | “bigquery“ or “csv“ - type of the input tabular data. If csv is mentioned then the first column is treated as target. If CSV files have a header, also pass the flag “data_has_header”. If “bigquery” is used, one can either supply training/validation data paths, or supply BigQuery project, dataset, and table names for preprocessing to produce the training and validation datasets.. | Yes |\n",
        "| `model_type` | string | The learning task such as classification or regression. | Yes |\n",
        "| `split_column` | string | The column name used to create the training, validation, and test splits. Values of the columns (a.k.a table['split_column']) should be either “TRAIN”, “VALIDATE”, or “TEST”. “TEST” is optional. Applicable to bigquery input only. | No. |\n",
        "| `train_batch_size` | int | Batch size for training. | No - Default is 1024. |\n",
        "| `eval_split` | float | Split fraction to use for the evaluation dataset, if `validation_data_path` is not provided. | No - Default is 0.2 |\n",
        "| `learning_rate` | float | Learning rate for training. | No - Default is the default learning rate of the specified optimizer. |\n",
        "| `eval_frequency_secs` | int | Frequency at which evaluation and checkpointing will take place.The default is 600. | No . |\n",
        "| `num_parallel_reads` | int | Number of threads used to read input files. We suggest setting it equal or slightly less than the number of CPUs of the machine for maximal performance in most cases. For example, 6 per GPU is a good default choice. | Yes . |\n",
        "| `optimizer` | string | Training optimizer. Lowercase string name of any TF2.3 Keras optimizer is supported ('sgd', 'adam', 'ftrl', etc.). See [TensorFlow documentation](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers). | No - Default is 'adam'. |\n",
        "| `data_cache` | string | Choose to cache data to “memory”, “disk” or “no_cache”. For large datasets, caching the data into memory would throw out-of-memory errors, therefore, we suggest choosing “disk”. You can specify the disk size in the config file (as exemplified below). Make sure to request a sufficiently large (e.g. TB size) disk to write the data, for large (B-scale) datasets. | No, The default one is “memory” . |\n",
        "| `bq_project` | string | The name of the BigQuery project. If input_type=bigquery and using the flag –preprocessing, this is required. This is an alternative to specifying train, validation, and test data paths. | No . |\n",
        "| `dataset_name` | string | The name of the BigQuery dataset. If input_type=bigquery and using the flag –preprocessing, this is required. This is an alternative to specifying train, validation, and test data paths. | No . |\n",
        "| `table_name` | string | The name of the BigQuery table. If input_type=bigquery and using the flag –preprocessing, this is required. This is an alternative to specifying train, validation, and test data paths. | No . |\n",
        "| `loss_function_type` | string | There are several loss function types in TabNet. For regression: mse/mae are included. For classification: cross_entropy/weighted_cross_entropy/focal_loss are included | No . If the value is \"default\", we use mse for regression and cross_entropy for classification.|\n",
        "| `deterministic_data` | boolean argument | Determinism of data reading from tabular data. The default is set to False. When setting to True, the experiment is deterministic. For fast training on large datasets, we suggest deterministic_data=False setting, albeit having randomness in the results (which becomes negligible at large datasets). Note that determinism is still not guaranteed with distributed training, as map-reduce causes randomness due to the ordering of algebraic operations with finite precision. However, this is negligible in practice, especially on large datasets. For the cases where 100% determinism is desired, besides deterministic_data=True setting, we suggest training with a single GPU (e.g. with MACHINE_TYPE=\"n1-highmem-8\").| No, The default is False . |\n",
        "| `stream_inputs` | boolean argument | Stream input data from GCS instead of downloading it locally - this option is suggested for fast runtime. | No . |\n",
        "| `large_category_dim` | int | Dimensionality of the embedding - if the number of distinct categories for a categorical column is greater than large_category_thresh we use a large_category_dim dimensional embedding, instead of 1-D embedding. The default is 1. We suggest increasing it (e.g. to ~5 in most cases and even ~10 if the number of categories is typically very large in the dataset), if pushing the accuracy is the main goal, rather than computational efficiency and explainability.  | No . |\n",
        "| `large_category_thresh` | int | Threshold for categorical column cardinality - if the number of distinct categories for a categorical column is greater than large_category_thresh we use a large_category_dim dimensional embedding, instead of 1-D embedding. The default is 300. We suggest decreasing it (e.g. to ~10), if pushing the accuracy is the main goal, rather than computational efficiency and explainability. | No . |\n",
        "| `yeo_johnson_transform` | boolean argument | Enables trainable Yeo-Johnson Power Transform (the default is disabled). Please see this link: https://www.stat.umn.edu/arc/yjpower.pdf for more information on Yeo-Johnson Power Transform. With our implementation, the transform parameters are learnable along with TabNet, trained in an end-to-end way. | No . |\n",
        "| `apply_log_transform` | boolean argument | If log transform statistics are contained in the metadata and this flag is true then the input features will be log transformed. Use false to not use the transform and true (the default) to use it. Especially for datasets with skewed numerical distributions, log transformations could be very helpful.  | No . |\n",
        "| `apply_quantile_transform` | boolean argument | If quantile statistics are contained in the metadata and this flag is true then the input features will be quantile transformed. Use false to not use the transform and true (the default) to use it. Especially for datasets with skewed numerical distributions, quantile transformations could be very helpful. Currently supported for BigQuery input_type. | No . |\n",
        "| `replace_transformed_features` | boolean argument | If true then if a transformation is applied to a feature that feature will be replaced. If false (the default option) then the transformed feature 'will be added as a new feature to the list of feature columns. Use true to replace the feature with the transform and false to append the transformed feature as a new column. | No . |\n",
        "| `target_column` | string | name of the label column. Note that for classification, the label needs to be of String or integer type. | No . |\n",
        "| `prediction_raw_inputs` | boolean argument | If we set this argument, the model serving allows us to pass the features as a dictionary of tensors instead of CSV row. | No . |\n",
        "| `exclude_key` | boolean argument | If we set this argument, we exclude a key in the input/output. The key is helpful in batch prediction processes input and saves output in an unpredictable order. The key helps match the output with input. | No . |\n",
        "\n",
        "Learn more about [Get started with builtin TabNet algorithm](https://cloud.google.com/ai-platform/training/docs/algorithms/tab-net-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a987b40417de"
      },
      "outputs": [],
      "source": [
        "ALGORITHM = \"tabnet\"\n",
        "MODEL_TYPE = \"classification\"\n",
        "MODEL_NAME = f\"{DATASET_NAME}_{ALGORITHM}_{MODEL_TYPE}\"\n",
        "\n",
        "OUTPUT_DIR = f\"{BUCKET_URI}/{MODEL_NAME}\"\n",
        "print(\"Output dir: \", OUTPUT_DIR)\n",
        "\n",
        "CMDARGS = [\n",
        "    \"--preprocess\",\n",
        "    \"--data_has_header\",\n",
        "    f\"--training_data_path={TRAINING_DATA_PATH}\",\n",
        "    f\"--job-dir={OUTPUT_DIR}\",\n",
        "    f\"--model_type={MODEL_TYPE}\",\n",
        "    \"--max_steps=2000\",\n",
        "    \"--batch_size=4096\",\n",
        "    \"--learning_rate=0.01\",\n",
        "    \"--prediction_raw_inputs\",\n",
        "    \"--exclude_key\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train_custom_job"
      },
      "source": [
        "### Train the TabNet model\n",
        "\n",
        "Use the `run` method to start training, which takes the following parameters:\n",
        "\n",
        "- `args`: The command line arguments to be passed to the TabNet training container.\n",
        "- `replica_count`: The number of worker replicas.\n",
        "- `model_display_name`: The display name of the `Model` if the script produces a managed `Model`.\n",
        "- `machine_type`: The type of machine to use for training.\n",
        "- `accelerator_type`: The hardware accelerator type.\n",
        "- `accelerator_count`: The number of accelerators to attach to a worker replica.\n",
        "\n",
        "The `run` method creates a training pipeline that trains and creates a `Model` object. After the training pipeline completes, the `run` method returns the `Model` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0927403746f"
      },
      "outputs": [],
      "source": [
        "MODEL_DIR = OUTPUT_DIR\n",
        "\n",
        "if TRAIN_GPU:\n",
        "    model = job.run(\n",
        "        model_display_name=f\"{DATASET_NAME}\",\n",
        "        args=CMDARGS,\n",
        "        replica_count=1,\n",
        "        machine_type=TRAIN_COMPUTE,\n",
        "        base_output_dir=MODEL_DIR,\n",
        "        accelerator_type=TRAIN_GPU.name,\n",
        "        accelerator_count=TRAIN_NGPU,\n",
        "        sync=True,\n",
        "    )\n",
        "else:\n",
        "    model = job.run(\n",
        "        model_display_name=f\"{DATASET_NAME}\",\n",
        "        args=CMDARGS,\n",
        "        replica_count=1,\n",
        "        machine_type=TRAIN_COMPUTE,\n",
        "        base_output_dir=MODEL_DIR,\n",
        "        sync=True,\n",
        "    )\n",
        "\n",
        "print(model.gca_resource)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91d661ac0276"
      },
      "source": [
        "#### Delete the training job\n",
        "\n",
        "Use the `delete()` method to delete the training job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e43fc3b7fe8"
      },
      "outputs": [],
      "source": [
        "job.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deploy_model:dedicated"
      },
      "source": [
        "### Deploy the model\n",
        "\n",
        "Before you use your model to make predictions, you need to deploy it to an `Endpoint`. You can do this by calling the `deploy` function on the `Model` resource. This will do two things:\n",
        "\n",
        "1. Create an `Endpoint` resource for deploying the `Model` resource to.\n",
        "2. Deploy the `Model` resource to the `Endpoint` resource.\n",
        "\n",
        "\n",
        "The function takes the following parameters:\n",
        "\n",
        "- `deployed_model_display_name`: A human readable name for the deployed model.\n",
        "- `traffic_split`: Percent of traffic at the endpoint that goes to this model, which is specified as a dictionary of one or more key/value pairs.\n",
        "   - If only one model, then specify as **{ \"0\": 100 }**, where \"0\" refers to this model being uploaded and 100 means 100% of the traffic.\n",
        "   - If there are existing models on the endpoint, for which the traffic will be split, then use `model_id` to specify as **{ \"0\": percent, model_id: percent, ... }**, where `model_id` is the model id of an existing model to the deployed endpoint. The percents must add up to 100.\n",
        "- `machine_type`: The type of machine to use for training.\n",
        "- `accelerator_type`: The hardware accelerator type.\n",
        "- `accelerator_count`: The number of accelerators to attach to a worker replica.\n",
        "- `starting_replica_count`: The number of compute instances to initially provision.\n",
        "- `max_replica_count`: The maximum number of compute instances to scale to. In this tutorial, only one instance is provisioned.\n",
        "\n",
        "### Traffic split\n",
        "\n",
        "The `traffic_split` parameter is specified as a Python dictionary. You can deploy more than one instance of your model to an endpoint, and then set the percentage of traffic that goes to each instance.\n",
        "\n",
        "You can use a traffic split to introduce a new model gradually into production. For example, if you had one existing model in production with 100% of the traffic, you could deploy a new model to the same endpoint, direct 10% of traffic to it, and reduce the original model's traffic to 90%. This allows you to monitor the new model's performance while minimizing the distruption to the majority of users.\n",
        "\n",
        "### Compute instance scaling\n",
        "\n",
        "You can specify a single instance (or node) to serve your online prediction requests. This tutorial uses a single node, so the variables `MIN_NODES` and `MAX_NODES` are both set to `1`.\n",
        "\n",
        "If you want to use multiple nodes to serve your online prediction requests, set `MAX_NODES` to the maximum number of nodes you want to use. Vertex AI autoscales the number of nodes used to serve your predictions, up to the maximum number you set. Refer to the [pricing page](https://cloud.google.com/vertex-ai/pricing#prediction-prices) to understand the costs of autoscaling with multiple nodes.\n",
        "\n",
        "### Endpoint\n",
        "\n",
        "The method will block until the model is deployed and eventually return an `Endpoint` object. If this is the first time a model is deployed to the endpoint, it may take a few additional minutes to complete provisioning of resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMH7GrYMlugy"
      },
      "outputs": [],
      "source": [
        "DEPLOYED_NAME = f\"{DATASET_NAME}\"\n",
        "\n",
        "TRAFFIC_SPLIT = {\"0\": 100}\n",
        "\n",
        "MIN_NODES = 1\n",
        "MAX_NODES = 1\n",
        "\n",
        "if DEPLOY_GPU:\n",
        "    endpoint = model.deploy(\n",
        "        deployed_model_display_name=DEPLOYED_NAME,\n",
        "        traffic_split=TRAFFIC_SPLIT,\n",
        "        machine_type=DEPLOY_COMPUTE,\n",
        "        accelerator_type=DEPLOY_GPU.name,\n",
        "        accelerator_count=DEPLOY_NGPU,\n",
        "        min_replica_count=MIN_NODES,\n",
        "        max_replica_count=MAX_NODES,\n",
        "    )\n",
        "else:\n",
        "    endpoint = model.deploy(\n",
        "        deployed_model_display_name=DEPLOYED_NAME,\n",
        "        traffic_split=TRAFFIC_SPLIT,\n",
        "        machine_type=DEPLOY_COMPUTE,\n",
        "        accelerator_type=DEPLOY_COMPUTE.name,\n",
        "        accelerator_count=0,\n",
        "        min_replica_count=MIN_NODES,\n",
        "        max_replica_count=MAX_NODES,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a7e8daa2350"
      },
      "source": [
        "### Get the serving signature\n",
        "\n",
        "Next, download the model locally and query the model for its serving signature. The serving signature will be of the form:\n",
        "\n",
        "    ( \"feature_name_1\",  \"feature_name_2\", ... )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23374479e310"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "loaded = tf.saved_model.load(MODEL_DIR + \"/model\")\n",
        "loaded.signatures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9321a211f2f7"
      },
      "source": [
        "### Make a prediction\n",
        "\n",
        "Finally, you make a prediction using the `predict()` method. Each instance is specified in the following dictionary format:\n",
        "\n",
        "    { \"feature_name_1\": value, \"feature_name_2\", value, ... }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "625fc7368b65"
      },
      "outputs": [],
      "source": [
        "prediction = endpoint.predict(\n",
        "    [\n",
        "        {\n",
        "            \"Age\": 3,\n",
        "            \"Breed1\": \"Tabby\",\n",
        "            \"Color1\": \"Black\",\n",
        "            \"Color2\": \"White\",\n",
        "            \"Fee\": 100,\n",
        "            \"FurLength\": \"Short\",\n",
        "            \"Gender\": \"Male\",\n",
        "            \"Health\": \"Healthy\",\n",
        "            \"MaturitySize\": \"Small\",\n",
        "            \"PhotoAmt\": 2,\n",
        "            \"Sterilized\": \"No\",\n",
        "            \"Type\": \"Cat\",\n",
        "            \"Vaccinated\": \"No\",\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9EZvfSUWrxS"
      },
      "source": [
        "## Hyperparameter Tuning\n",
        "\n",
        "After successfully training your model, deploying it, and calling it to make predictions, you may want to optimize the hyperparameters used during training to improve your model's accuracy and performance. See the Vertex AI documentation for an overview of hyperparameter tuning and how to use it in your Vertex Training jobs.\n",
        "\n",
        "For this example, the following runs a Vertex AI hyperparameter tuning job with 4 trials that attempts to maximize the validation AUC metric. The hyperparameters it optimizes are the number of max_steps and the learning rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91b2234d636c"
      },
      "source": [
        "### Create trial configuration\n",
        "\n",
        "Next, you construct a YAML file which contains the hyperparameter trial settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6f45751d3300"
      },
      "outputs": [],
      "source": [
        "config = f\"\"\"studySpec:\n",
        "  metrics:\n",
        "  - metricId: auc\n",
        "    goal: MAXIMIZE\n",
        "  parameters:\n",
        "  - parameterId: max_steps\n",
        "    integerValueSpec:\n",
        "      minValue: 2000\n",
        "      maxValue: 3000\n",
        "  - parameterId: learning_rate\n",
        "    doubleValueSpec:\n",
        "      minValue: 0.0000001\n",
        "      maxValue: 0.1\n",
        "trialJobSpec:\n",
        "  workerPoolSpecs:\n",
        "  - machineSpec:\n",
        "      machineType: {TRAIN_COMPUTE}\n",
        "      acceleratorType: NVIDIA_TESLA_V100\n",
        "      acceleratorCount: 1\n",
        "    replicaCount: 1\n",
        "    diskSpec:\n",
        "      bootDiskType: pd-ssd\n",
        "      bootDiskSizeGb: 100\n",
        "    containerSpec:\n",
        "      imageUri: {TRAIN_IMAGE}\n",
        "      args:\n",
        "      - --preprocess \n",
        "      - --data_has_header\n",
        "      - --training_data_path={TRAINING_DATA_PATH}\n",
        "      - --job-dir={OUTPUT_DIR}\n",
        "      - --batch_size=1028\n",
        "      - --model_type={MODEL_TYPE}\n",
        "      - --prediction_raw_inputs\n",
        "\"\"\"\n",
        "\n",
        "!echo $'{config}' > ./config.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dacb3b06fe6b"
      },
      "source": [
        "### Execute the hyperparameter tuning trials\n",
        "\n",
        "Next, you execute the hyperparameter tuning job using the command `gcloud ai hp-tuning-jobs create`.\n",
        "\n",
        "The job will run asynchronouosly. You can poll the status of the job using `gcloud ai hp-tuning-jobs describe`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8865ecaeaaaa"
      },
      "outputs": [],
      "source": [
        "MAX_TRIAL_COUNT=4\n",
        "PARALLEL_TRIAL_COUNT=2\n",
        "\n",
        "output = ! gcloud ai hp-tuning-jobs create \\\n",
        "  --config=config.yaml \\\n",
        "  --max-trial-count={MAX_TRIAL_COUNT} \\\n",
        "  --parallel-trial-count={PARALLEL_TRIAL_COUNT} \\\n",
        "  --region=$REGION \\\n",
        "  --display-name={DATASET_NAME}\n",
        "\n",
        "print(output)\n",
        "\n",
        "try:\n",
        "    DESCRIBE = output[5]\n",
        "    print(\"Describe cmd:\", DESCRIBE)\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e495a6d30fea"
      },
      "source": [
        "#### Cancel the hyperparameter tuning job\n",
        "\n",
        "Next, use the command `gcloud ai hp-tuning-jobs cancel` to cancel the hyperparameter tuning job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f9aa4f68b92"
      },
      "outputs": [],
      "source": [
        "DESCRIBE = output[5]\n",
        "print(\"Describe cmd:\", DESCRIBE)\n",
        "\n",
        "args = DESCRIBE.split(\" \")\n",
        "JOB_ID = args[7]\n",
        "\n",
        "! gcloud ai hp-tuning-jobs cancel {JOB_ID} --region={REGION}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx_vKniMq9ZX"
      },
      "outputs": [],
      "source": [
        "delete_bucket = False\n",
        "\n",
        "# Delete BQ table\n",
        "! bq rm -f {PROJECT_ID}:{DATASET_NAME}.train\n",
        "\n",
        "try:\n",
        "    endpoint.undeploy_all()\n",
        "    endpoint.delete()\n",
        "    model.delete()\n",
        "    model_bq.delete()\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
        "    ! gsutil rm -r $BUCKET_URI"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "get_started_with_tabnet.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Interactive exploratory analysis of BigQuery data in a notebook\n",
        "\n",
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/workbench/exploratory_data_analysis/explore_data_in_bigquery_with_workbench.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/workbench/exploratory_data_analysis/explore_data_in_bigquery_with_workbench.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/workbench/exploratory_data_analysis/explore_data_in_bigquery_with_workbench.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>                                                                                               \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook is written for data analysts and data scientists who have data in BigQuery and want to perform exploratory data analysis to gather insights from that data in an interactive environment.\n",
        "\n",
        "### Objective\n",
        "\n",
        "In this tutorial, you learn about various ways to explore and gain insights from BigQuery data in a Jupyter notebook environment.\n",
        "\n",
        "This tutorial uses the following Google Cloud data analytics and ML services:\n",
        "\n",
        "- BigQuery\n",
        "- Vertex AI\n",
        "- Vertex AI Workbench (optional, this can also be run in other places like [Colab](https://colab.sandbox.google.com/))\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Using Python & SQL to query public data in BigQuery\n",
        "- Exploring the dataset using BigQuery INFORMATION_SCHEMA\n",
        "- Creating interactive elements to help explore interesting parts of the data\n",
        "- Doing some exploratory correlation and time series analysis\n",
        "- Creating static and interactive outputs (data tables and plots) in the notebook\n",
        "- Saving some outputs to Cloud Storage\n",
        "\n",
        "### Dataset\n",
        "\n",
        "The dataset, [available publicly on BigQuery](https://console.cloud.google.com/bigquery?project=bigquery-public-data&d=ga4_obfuscated_sample_ecommerce&p=bigquery-public-data&page=dataset), comes from obfuscated [Google Analytics 4 (GA4) data](https://developers.google.com/analytics/bigquery/web-ecommerce-demo-dataset) from the Google Merchandise Store.\n",
        "\n",
        "### Costs \n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* BigQuery\n",
        "* Vertex AI\n",
        "* Vertex AI Workbench (optional)\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [BigQuery Pricing](https://cloud.google.com/bigquery/pricing), [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Vertex AI Workbench pricing](https://cloud.google.com/vertex-ai/pricing#notebooks), and [Cloud Storage pricing](https://cloud.google.com/storage/pricing)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze4-nDLfK4pw"
      },
      "source": [
        "### Set up your local development environment\n",
        "\n",
        "**If you are using Colab or Vertex AI Workbench notebooks**, your environment already meets\n",
        "all the requirements to run this notebook. You can skip this step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCuSR8GkAgzl"
      },
      "source": [
        "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
        "You need the following:\n",
        "\n",
        "* The Google Cloud SDK\n",
        "* Git\n",
        "* Python 3\n",
        "* virtualenv\n",
        "* Jupyter notebook running in a virtual environment with Python 3\n",
        "\n",
        "The Google Cloud guide to [Setting up a Python development\n",
        "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
        "installation guide](https://jupyter.org/install) provide detailed instructions\n",
        "for meeting these requirements. The following steps provide a condensed set of\n",
        "instructions:\n",
        "\n",
        "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
        "\n",
        "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
        "\n",
        "1. [Install\n",
        "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
        "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
        "\n",
        "1. To install Jupyter, run `pip3 install jupyter` on the\n",
        "command-line in a terminal shell.\n",
        "\n",
        "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
        "\n",
        "1. Open this notebook in the Jupyter Notebook Dashboard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7EUnXsZhAGF"
      },
      "source": [
        "### Install additional packages\n",
        "\n",
        "Install and/or upgrade the following packages required to execute this notebook. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b4ef9b72d43"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Vertex AI Workbench notebook product has specific requirements\n",
        "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
        "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
        "    \"/opt/deeplearning/metadata/env_version\"\n",
        ")\n",
        "\n",
        "# Vertex AI Workbench notebooks require dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_WORKBENCH_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\"\n",
        "\n",
        "# Install/upgrade Vertex AI (\"aiplatform\") library\n",
        "! pip3 install --upgrade google-cloud-aiplatform {USER_FLAG} -q\n",
        "\n",
        "# Install/upgrade BigQuery storage library\n",
        "! pip3 install --upgrade google-cloud-bigquery-storage {USER_FLAG} -q\n",
        "\n",
        "# Install ipywidgets to use interactive widgets in notebooks\n",
        "! pip3 install --upgrade ipywidgets {USER_FLAG} -q\n",
        "\n",
        "# Install itables to use interactive tables in notebooks\n",
        "! pip3 install --upgrade itables {USER_FLAG} -q\n",
        "\n",
        "# Install kaleido to save plotly images as static files\n",
        "! pip3 install --upgrade kaleido {USER_FLAG} -q\n",
        "\n",
        "# Install plotly to create interactive plots\n",
        "! pip3 install --upgrade plotly {USER_FLAG} -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhq5zEbGg0XX"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzrelQZ22IZj"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEQmXjlJ2ozG"
      },
      "source": [
        "After running the cell above, you may see a message pop up that says:\n",
        "\n",
        "\"The kernel for [your_notebook_name].ipynb appears to have died. It will restart automatically.\" (in Vertex AI Workbench)\n",
        "\n",
        "OR\n",
        "\n",
        "\"Your session crashed for an unknown reason.\" (in Colab)\n",
        "\n",
        "This type of message is expected, just wait for the kernel to reset and then proceed with running the cells below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWEdiXsJg0XY"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF1j6f9HApxa"
      },
      "source": [
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "1. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). \n",
        "\n",
        "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
        "\n",
        "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WReHDGG5g0XY"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd85f5c794e5"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM1iC_MfAts1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID:\", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJYoRfYng0XZ"
      },
      "source": [
        "Otherwise, set your project ID here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riG_qUokg0XZ"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
        "\n",
        "- Americas: `us-central1`\n",
        "- Europe: `europe-west4`\n",
        "- Asia Pacific: `asia-east1`\n",
        "\n",
        "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
        "\n",
        "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "of-Pgp7KYjda"
      },
      "outputs": [],
      "source": [
        "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
        "\n",
        "if REGION == \"[your-region]\":\n",
        "    REGION = \"us-central1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06571eb4063b"
      },
      "source": [
        "#### UUID\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a uuid for each instance session, and append it onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "697568e92bd6"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "\n",
        "# Generate a uuid of a specifed length(default=8)\n",
        "def generate_uuid(length: int = 8) -> str:\n",
        "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
        "\n",
        "\n",
        "UUID = generate_uuid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr--iN2kAylZ"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using Vertex AI Workbench notebooks**, your environment is already\n",
        "authenticated. Skip this step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBCra4QMA2wR"
      },
      "source": [
        "**If you are using Colab**, run the cell below and follow the instructions\n",
        "when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "1. In the Cloud Console, go to the [**Create service account key**\n",
        "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
        "\n",
        "2. Click **Create service account**.\n",
        "\n",
        "3. In the **Service account name** field, enter a name, and\n",
        "   click **Create**.\n",
        "\n",
        "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
        "into the filter box, and select\n",
        "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "\n",
        "5. Click *Create*. A JSON file that contains your key downloads to your\n",
        "local environment.\n",
        "\n",
        "6. Enter the path to your service account key as the\n",
        "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "outputs": [],
      "source": [
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# If on Vertex AI Workbench, then don't execute this code\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
        "    \"DL_ANACONDA_HOME\"\n",
        "):\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgPO1eR3CYjk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "\n",
        "In this tutorial, you'll use a Cloud Storage bucket to save some of the outputs from your exploratory data analysis.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. It must be unique across all Cloud Storage buckets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzGDU7TWdts_"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZzVCix08T6w"
      },
      "source": [
        "If you didn't set a Cloud Storage bucket name above, the following cell auto-creates a bucket name based off your project ID and the UUID you previously generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf221059d072"
      },
      "outputs": [],
      "source": [
        "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
        "    BUCKET_NAME = PROJECT_ID + \"aip-\" + UUID\n",
        "    BUCKET_URI = f\"gs://{BUCKET_NAME}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EcIXiGsCePi"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIq7R4HZCfIc"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucvCsknMCims"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhOb7YnwClBb"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "### Import libraries and configure some options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRUOFELefqf1"
      },
      "outputs": [],
      "source": [
        "# Import library to enable some interactive widgets\n",
        "import ipywidgets as widgets\n",
        "# Import options from itables separately\n",
        "import itables.options as itable_opts\n",
        "# Import basic Python data science libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# Import plotly express for quick plots\n",
        "import plotly.express as px\n",
        "# Import AI Platform library (as Vertex AI) and BigQuery library\n",
        "from google.cloud import aiplatform as vertex_ai\n",
        "from google.cloud import bigquery\n",
        "# Import Exceptions library to help with error catching\n",
        "from google.cloud.exceptions import BadRequest\n",
        "# Import IPython display utilities\n",
        "from IPython.display import clear_output\n",
        "# Import particular interactive capabilities\n",
        "from ipywidgets import interact\n",
        "# Import itables for interactive tables\n",
        "from itables import show"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50RUw1EPcjCE"
      },
      "outputs": [],
      "source": [
        "# Configure some options related to interactive tables\n",
        "itable_opts.maxBytes = 1e9\n",
        "itable_opts.maxColumns = 50\n",
        "\n",
        "itable_opts.order = []\n",
        "itable_opts.column_filters = \"header\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "source": [
        "### Prepare Vertex AI and BigQuery for usage\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project and set up BigQuery for use with your project in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ftit2FwIYjdd"
      },
      "outputs": [],
      "source": [
        "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuyuY1pwYjdd"
      },
      "source": [
        "Create the BigQuery client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5qNEtzFYjde"
      },
      "outputs": [],
      "source": [
        "bq_client = bigquery.Client(project=PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuaFz0WfYjde"
      },
      "source": [
        "Create a helper function for sending queries to BigQuery. You use this multiple times in the sections below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9lT5MZYYjde"
      },
      "outputs": [],
      "source": [
        "# Wrapper to use BigQuery client to run query/job, return job ID or result as DF\n",
        "def bq_query(sql, show_job_id=False):\n",
        "    \"\"\"\n",
        "    Input: SQL query, as a string, to execute in BigQuery\n",
        "    Returns the query results as a pandas DataFrame, or error, if any\n",
        "    \"\"\"\n",
        "\n",
        "    # Try dry run before executing query to catch any errors\n",
        "    try:\n",
        "        job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)\n",
        "\n",
        "        bq_client.query(sql, job_config=job_config)\n",
        "\n",
        "    except BadRequest as err:\n",
        "        print(err)\n",
        "        return\n",
        "\n",
        "    job_config = bigquery.QueryJobConfig()\n",
        "    client_result = bq_client.query(sql, job_config=job_config)\n",
        "\n",
        "    job_id = client_result.job_id\n",
        "\n",
        "    # Wait for query/job to finish running, then get & return data frame\n",
        "    df = client_result.result().to_dataframe()\n",
        "\n",
        "    if show_job_id:\n",
        "        print(f\"Finished job_id: {job_id}\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "## Explore schema of Google Analytics 4 (GA4) data in BigQuery\n",
        "In this section, you look at the structure of the sample Google Analytics 4 (GA4) data: the column fields, the number of rows, what dates it covers, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O69TbKGUYjde"
      },
      "source": [
        "### Look at table data structure\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hFjenbkYjde"
      },
      "source": [
        "The sample GA4 data in BigQuery is [partitioned](https://cloud.google.com/bigquery/docs/partitioned-tables) by date, which makes it easier to manage and query your data. There are ways to query that date for a specific date, a set of dates (e.g. a specific month), and across all dates. The latter is what you do for the most part in this notebook.\n",
        "\n",
        "You can use the [`%%bigquery` Python magic commands](https://googleapis.dev/python/bigquery/latest/magics.html) to execute SQL queries directly in a notebook cell, as shown in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4-PYxzIYjde"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project $PROJECT_ID\n",
        "\n",
        "SELECT\n",
        "  *\n",
        "\n",
        "FROM\n",
        "  /* Use '*' at end of table name to allow querying over multiple date \n",
        "     partitions of same overall table */\n",
        "  `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\n",
        "\n",
        "LIMIT 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5gKUegbCAVR"
      },
      "source": [
        "The results above show that the table is somewhat wide, with some [nested columns](https://cloud.google.com/bigquery/docs/nested-repeated) and semi-structured data. Some key columns - event_date, event_timestamp, and event_name - appear at left."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Vf0XoVMYjdf"
      },
      "source": [
        "### Look at number of events by date\n",
        "Next, you get a table of the number of events for each event date in the data.\n",
        "\n",
        "The \"event_dates\" next to the %%bigquery magic enables the query results to be saved into a pandas DataFrame of that name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PR_bZu62Yjdf"
      },
      "outputs": [],
      "source": [
        "%%bigquery event_dates --project $PROJECT_ID \n",
        "SELECT\n",
        "  event_date,\n",
        "  COUNT(*) AS num_events\n",
        "\n",
        "FROM\n",
        "  `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\n",
        "\n",
        "GROUP BY\n",
        "  event_date\n",
        "\n",
        "ORDER BY\n",
        "  event_date"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23zjFFmlDaKh"
      },
      "source": [
        "Print the query results by referencing the results DataFrame in its own cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzJAHctpYjdf"
      },
      "outputs": [],
      "source": [
        "event_dates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAJDP_phDjOQ"
      },
      "source": [
        "You see that there are three months of dates in this data, November 2020-January 2021, and somewhere on the order of tens of thousands of events per day (at least for the days that are visible in the default output)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J4Z-JFdD3M-"
      },
      "source": [
        "See the total number of events in the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jB_-fB8QYjdf"
      },
      "outputs": [],
      "source": [
        "sum(event_dates.num_events)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uO_pMWH9D_T_"
      },
      "source": [
        "There are ~4.3M event rows in this data. Given that and the relatively deep nature of some of the columns, it's ideal to work with it in BigQuery and pull relevant results into Python (rather than pulling the whole table into Python)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-egtPcSmYjdf"
      },
      "source": [
        "### Understand more about data structure using INFORMATION_SCHEMA\n",
        "In this section, you use various features of [BigQuery's INFORMATION_SCHEMA](https://cloud.google.com/bigquery/docs/information-schema-intro) to understand more about the GA4 data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J6wzWWAEw8n"
      },
      "source": [
        "#### Get some information on each table partition\n",
        "Below is a query that uses the \"\\_\\_TABLES__\" functionality to get some information (row count, size, etc.) on each partitioned table in the sample GA4 data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rF--QFUPYjdf"
      },
      "outputs": [],
      "source": [
        "%%bigquery --project $PROJECT_ID \n",
        "SELECT \n",
        "  table_id,\n",
        "\n",
        "  (CASE\n",
        "     WHEN type = 1 THEN 'Table'\n",
        "     WHEN type = 2 THEN 'View'\n",
        "     ELSE NULL\n",
        "     END) AS type,\n",
        "\n",
        "  row_count,\n",
        "  ROUND(size_bytes / POW(2, 20), 1) AS size_MB,\n",
        "  TIMESTAMP_MILLIS(creation_time) AS creation_time,\n",
        "  TIMESTAMP_MILLIS(last_modified_time) AS last_modified_time \n",
        "\n",
        "FROM\n",
        "  /* __TABLES__ functionality to look at all tables in given dataset */\n",
        "  `bigquery-public-data.ga4_obfuscated_sample_ecommerce`.__TABLES__\n",
        "\n",
        "ORDER BY\n",
        "  table_id  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOf8FlmiGmTC"
      },
      "source": [
        "You can see again that there are 92 tables, one corresponding to each date in the span, with a couple dozen MB of data in each."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qInG0akfYjdg"
      },
      "source": [
        "#### Get information on each table column\n",
        "Next, you use the [COLUMNS view](https://cloud.google.com/bigquery/docs/information-schema-columns) from the BigQuery INFORMATION_SCHEMA to get one row for each column in the GA4 sample data table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yyvnv57MYjdg"
      },
      "outputs": [],
      "source": [
        "# Get max event date to use for query below\n",
        "max_event_date = max(event_dates[\"event_date\"])\n",
        "\n",
        "# Get information on columns in GA4 data (using only 1 date table)\n",
        "info_schema_columns_sql = f\"\"\"\n",
        "    SELECT *\n",
        "    \n",
        "    FROM\n",
        "      `bigquery-public-data.ga4_obfuscated_sample_ecommerce`.\n",
        "      INFORMATION_SCHEMA.COLUMNS\n",
        "      \n",
        "    WHERE\n",
        "      table_name = 'events_{max_event_date}'\n",
        "    \"\"\"\n",
        "\n",
        "info_schema_columns = bq_query(info_schema_columns_sql)\n",
        "\n",
        "info_schema_columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvVfJFvhIIBL"
      },
      "source": [
        "You can see the table has 23 columns, including strings, integers, floats, and several nested or repeated fields."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFvLSnGoYjdg"
      },
      "source": [
        "#### Get interactive table of information on each (nested) field\n",
        "To explore the nested fields from the above table in more depth (since they have various useful fields), you can use the [COLUMN_FIELD_PATHS view](https://cloud.google.com/bigquery/docs/information-schema-column-field-paths)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkTniRxJYjdg"
      },
      "outputs": [],
      "source": [
        "info_schema_column_field_paths_sql = f\"\"\"\n",
        "  SELECT\n",
        "    column_name,\n",
        "    field_path,\n",
        "    data_type\n",
        "  \n",
        "  FROM\n",
        "    `bigquery-public-data.ga4_obfuscated_sample_ecommerce`.\n",
        "    INFORMATION_SCHEMA.COLUMN_FIELD_PATHS\n",
        "    \n",
        "  WHERE\n",
        "    table_name = 'events_{max_event_date}'\n",
        "  \"\"\"\n",
        "\n",
        "info_schema_column_field_paths = bq_query(info_schema_column_field_paths_sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0XyuQtbKc5w"
      },
      "source": [
        "In this case, display the query results in an interactive table created using the [itables package](https://mwouts.github.io/itables/quick_start.html). This allows you to explore, filter, or sort your data right in the notebook cell output that has the table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9p68bWAKGzf"
      },
      "outputs": [],
      "source": [
        "show(info_schema_column_field_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azzmrDR6LPyj"
      },
      "source": [
        "There are 108 rows in the table above showing the different levels of hierarchy. Using the column filters, you can see the column fields that contain (e.g.) time fields, user information, and geo (location) data. You can also look at device fields that are categorical, numerical, or other data types."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxJkDNwpYjdg"
      },
      "source": [
        "## Explore event counts by various categories\n",
        "In this section, you use plotting libraries - specifically [matplotlib](https://matplotlib.org/) and [Plotly Express](https://plotly.com/python/plotly-express/) - to generate graphical outputs that help understand the distribution of events across some of the fields in the sample GA4 data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmaK_LHyYjdh"
      },
      "source": [
        "### Explore number of events by type\n",
        "First, you get the number of events by type (i.e. \"event_name\") across the whole data from BigQuery and generate a static bar plot of the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDUelss7Yjdh"
      },
      "outputs": [],
      "source": [
        "event_name_counts_sql = \"\"\"\n",
        "    SELECT\n",
        "      event_name,\n",
        "      COUNT(*) AS num_events\n",
        "\n",
        "    FROM\n",
        "      `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\n",
        "\n",
        "    GROUP BY\n",
        "      event_name\n",
        "\n",
        "    ORDER BY\n",
        "      num_events DESC\n",
        "    \"\"\"\n",
        "\n",
        "event_name_counts = bq_query(event_name_counts_sql)\n",
        "\n",
        "# matplotlib used to create basic bar plot\n",
        "event_name_counts_bar_plot = event_name_counts.plot(\n",
        "    kind=\"bar\", x=\"event_name\", y=\"num_events\"\n",
        ")\n",
        "\n",
        "event_name_counts_bar_plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdCdOru5PO6L"
      },
      "source": [
        "You can see from the bar plot that page views and user engagements are the two most common event types, with a large drop-off to the other types of events."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUsXxR4cPOOx"
      },
      "source": [
        "You can display the same event type distribution data in an interactive bar plot using Plotly Express."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBPksdUtYjdh"
      },
      "outputs": [],
      "source": [
        "event_name_counts_interactive_bar_plot = px.bar(\n",
        "    event_name_counts,\n",
        "    x=\"event_name\",\n",
        "    y=\"num_events\",\n",
        "    title=\"Number of Events by event_name\",\n",
        ")\n",
        "\n",
        "event_name_counts_interactive_bar_plot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBOhgyceZw0k"
      },
      "source": [
        "The bar plot shows the same info as the previous one, with the additional feature of interactivity. You can hover over each bar to see the actual number of events or zoom in to various parts of the chart (e.g. to see how the numbers for the less common events compare on a smaller scale)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVT5nLkaYjdh"
      },
      "source": [
        "### Function to generalize event counts bar plot creation\n",
        "Suppose that you like the previous interactive bar plot of event counts, and want to look at it for other categorical variables in the data like those related to device, user characteristics, or other fields. A nice to way generalize this is to create a function that gets event counts data from BigQuery for any single variable and then creates an interactive bar plot like the one above. Run the following cell to create such a function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDO20g01Yjdh"
      },
      "outputs": [],
      "source": [
        "def get_event_counts_by_categorical_var_bar_plot(\n",
        "    bq_var_name, output_var_name, min_events=500\n",
        "):\n",
        "\n",
        "    # SQL code \"template\" to get event counts for specific variable in data\n",
        "    event_counts_sql = f\"\"\"\n",
        "        SELECT\n",
        "          IFNULL({bq_var_name}, 'NULL') AS {output_var_name},\n",
        "          COUNT(*) AS num_events\n",
        "\n",
        "        FROM\n",
        "          `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\n",
        "\n",
        "        GROUP BY\n",
        "          {bq_var_name}\n",
        "        \n",
        "        /* Filter to only return values of variable that have some min. # of\n",
        "           events - default is 500, can set to 0 to do no filtering */\n",
        "        HAVING\n",
        "          num_events >= {min_events}\n",
        "        \n",
        "        ORDER BY\n",
        "          num_events DESC\n",
        "        \"\"\"\n",
        "\n",
        "    event_counts = bq_query(event_counts_sql)\n",
        "\n",
        "    # Take results from SQL query and turn into bar plot for given variable\n",
        "    event_counts_interactive_bar_plot = px.bar(\n",
        "        event_counts,\n",
        "        x=output_var_name,\n",
        "        y=\"num_events\",\n",
        "        title=(\n",
        "            f\"<b>Number of Events by {output_var_name}<br></b>\"\n",
        "            + f\"Min. {min_events} Events for {output_var_name}\"\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    return event_counts_interactive_bar_plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF9ZVn5AYjdh"
      },
      "source": [
        "### Number of events by country, operating system, and device language\n",
        "Here, you use the function created above to get event counts by three different categorical variables in the data: country, operating system, and device language. These are displayed in three separate tabs of the same output cell below, using the [tab functionality from the ipywidgets package](https://ipywidgets.readthedocs.io/en/stable/examples/Widget%20List.html#Tabs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2HIevweYjdi"
      },
      "outputs": [],
      "source": [
        "# Create table of categorical variables (BQ and output names) to loop over\n",
        "categorical_vars_df = pd.DataFrame.from_records(\n",
        "    data=[\n",
        "        (\"geo.country\", \"country\"),\n",
        "        (\"device.operating_system\", \"operating_system\"),\n",
        "        (\"device.language\", \"device_language\"),\n",
        "    ],\n",
        "    columns=[\"bq_var_name\", \"output_var_name\"],\n",
        ")\n",
        "\n",
        "# Initialize tab outputs list to be appended to\n",
        "tab_outputs = []\n",
        "\n",
        "# Loop over categorical variables data, generating 1 bar plot per tab\n",
        "for index, row in categorical_vars_df.iterrows():\n",
        "    this_tab = widgets.Output()\n",
        "\n",
        "    with this_tab:\n",
        "        get_event_counts_by_categorical_var_bar_plot(\n",
        "            row[\"bq_var_name\"], row[\"output_var_name\"]\n",
        "        ).show()\n",
        "\n",
        "    # Append tab output for this tab to list of outputs\n",
        "    tab_outputs = tab_outputs + [this_tab]\n",
        "\n",
        "# Create output with all tab outputs together\n",
        "output = widgets.Tab(tab_outputs)\n",
        "\n",
        "# Loop over categorical variables again to set output tab titles\n",
        "for index, row in categorical_vars_df.iterrows():\n",
        "    output.set_title(index, row[\"output_var_name\"])\n",
        "\n",
        "display(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JgY8PiPprAu"
      },
      "source": [
        "Clicking through the tabs, you can see that:\n",
        "\n",
        "*   The United States is the country with the most events, followed by India and the United Kingdom.\n",
        "*   \"Web\" is the most common device operating system by far, with iOS and Windows pretty close for second. \n",
        "*   The most common value in the device language field is actually NULL, or no data in the field. This is important to note in any sort of further analysis or modeling using language. You may want to understand why it's missing, what types of users/events have missing language more or less often, and how that may/may not change in future data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7OGUKr7Yjdi"
      },
      "source": [
        "### Interactive: pick category, get corresponding event counts bar plot\n",
        "Building on the previous section, you can go even further with interactivity and allow picking from a set of categories present in the data, then dynamically creating the plot for whichever variable is selected. Below you create an interactive list of device attributes and slider for minimum number of events, then pass those on to the function to create the interactive event counts plot with those parameters. Each time you change one of the parameters, the code will run again to create the new plot accordingly.\n",
        "\n",
        "(NOTE: You must be running this notebook in a \"live\" kernel to use the interactive menus in the cell below. It won't work if you're just looking at previously generated output in the notebook.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKfM7UO2Yjdi"
      },
      "outputs": [],
      "source": [
        "# Filter to categorical fields that are nested under device (to use as options)\n",
        "device_cat_vars = info_schema_column_field_paths[\n",
        "    (info_schema_column_field_paths[\"column_name\"] == \"device\")\n",
        "    & (info_schema_column_field_paths[\"data_type\"] == \"STRING\")\n",
        "].reset_index(drop=True)\n",
        "\n",
        "# Some string manipulation to make device-related variable names\n",
        "device_cat_vars[\"var_name\"] = [\n",
        "    (\"device_\" + field_path.split(\".\")[-1])\n",
        "    for field_path in device_cat_vars[\"field_path\"]\n",
        "]\n",
        "\n",
        "\n",
        "# Interact section that creates a function that responds to user input\n",
        "@interact\n",
        "def show_device_field_event_counts(\n",
        "    # Widget to select single variable to look at event counts for\n",
        "    device_field=widgets.Select(\n",
        "        options=device_cat_vars[\"var_name\"].tolist(),\n",
        "        value=\"device_category\",\n",
        "        description=\"Device Attributes\",\n",
        "        continuous_update=False,\n",
        "    ),\n",
        "    # Slider to pick min. # of events required to return data for given category\n",
        "    min_num_events=widgets.IntSlider(\n",
        "        value=500,\n",
        "        min=0,\n",
        "        max=1e5,\n",
        "        step=100,\n",
        "        description=\"Min Num Events\",\n",
        "        continuous_update=False,\n",
        "    ),\n",
        "):\n",
        "\n",
        "    clear_output()\n",
        "\n",
        "    # Get BQ variable name for selected device field using data frame\n",
        "    bq_var_name = device_cat_vars.loc[\n",
        "        device_cat_vars[\"var_name\"] == device_field, \"field_path\"\n",
        "    ].item()\n",
        "\n",
        "    # Use function to generate event counts bar plot for chosen device field\n",
        "    event_count_plot = get_event_counts_by_categorical_var_bar_plot(\n",
        "        bq_var_name, device_field, min_num_events\n",
        "    )\n",
        "\n",
        "    return event_count_plot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK1VwpAaQnNz"
      },
      "source": [
        "Feel free to pick different combinations for the widgets in the cell above and see the resulting plots. If you leave the defaults (device_category and 500-event minimum), for example, you'll see that desktop is the most common device category, followed by mobile, and then tablet representing a much smaller proportion of all events than the other two.\n",
        "\n",
        "One upside of the interactivity here is that you can look through many plots with relatively quick point-and-click options, without having to write repeated code or pre-determine which things to plot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvWDCaJ0Qnqb"
      },
      "source": [
        "### Generate heatmap of event distribution across operating system by country\n",
        "Building on the analysis from the previous steps, you can also query and plot event distributions by the combination of two categorical variables together. You previously looked at the individual event distributions for country and operating system (OS) separately. In this case, you focus on *the combination* of country and OS together. In addition to the raw number of events in each country-OS combination, you also calculate the number as a percentage of *all events in that country*, to see if certain countries seem to have more/less activity on a given OS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUXFotdSYjdi"
      },
      "outputs": [],
      "source": [
        "country_os_counts_sql = \"\"\"\n",
        "    SELECT\n",
        "      geo.country AS country,\n",
        "      device.operating_system AS os,\n",
        "      \n",
        "      COUNT(*) AS num_events,\n",
        "      \n",
        "      /* Window aggregate function to country total events (across all OS) */\n",
        "      SUM(COUNT(*)) OVER (PARTITION BY geo.country)\n",
        "        AS tot_events_in_country,\n",
        "      \n",
        "      /* Look at # of events in country-os combination as % of all events in\n",
        "        that country,  */\n",
        "      SAFE_DIVIDE(\n",
        "        COUNT(*),\n",
        "        SUM(COUNT(*)) OVER (PARTITION BY geo.country)\n",
        "        ) * 100 AS pct_events_in_country\n",
        "\n",
        "    FROM\n",
        "      `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\n",
        "\n",
        "    GROUP BY\n",
        "      geo.country, device.operating_system\n",
        "    \n",
        "    QUALIFY\n",
        "      tot_events_in_country >= 50000\n",
        "    \n",
        "    ORDER BY\n",
        "      num_events DESC\n",
        "    \"\"\"\n",
        "\n",
        "country_os_counts = bq_query(country_os_counts_sql)\n",
        "\n",
        "country_os_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79-Rm-ImrSeI"
      },
      "source": [
        "One useful visualization for this type of two-category joint distribution data is a heatmap. Below, you create such a visualization of the percentage of events in each country that fall into each device OS, using [Plotly Express' heatmap functionality](https://plotly.com/python/heatmaps/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSosO3N2Yjdi"
      },
      "outputs": [],
      "source": [
        "# Pivot country-OS data to get in form necessary for Plotly Express heat map\n",
        "country_os_counts_matrix = pd.pivot_table(\n",
        "    country_os_counts,\n",
        "    values=\"pct_events_in_country\",\n",
        "    index=[\"os\"],\n",
        "    columns=[\"country\"],\n",
        "    aggfunc=np.sum,\n",
        ")\n",
        "\n",
        "# Create heat map % of events in each country (columns) by device OS (rows)\n",
        "country_os_counts_heatmap = px.imshow(\n",
        "    country_os_counts_matrix,\n",
        "    labels=dict(x=\"Country\", y=\"Device OS\", color=\"% Events in Country on This OS\"),\n",
        "    title=\"Percent of Events in Each Country by Device OS\",\n",
        ")\n",
        "\n",
        "country_os_counts_heatmap.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5HcMZfLsTJA"
      },
      "source": [
        "From looking at the colors and hovering over some of the numbers, it's clear that the distribution of events across device OS doesn't look very different across countries. Web is the dominant OS in every country, accounting for 55-65% of events in each country analyzed here. There are some other systems with slightly higher proportions in some countries (e.g. Windows in France, iOS in Singapore), but you likely come away from the heatmap thinking how relatively consistent things look across countries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IiHhCQDYjdj"
      },
      "source": [
        "## Analyze correlation between user-level aggregate metrics\n",
        "In this section, you look at the event data at the user level. All events that correspond to the same user_pseudo_id get grouped together, and then you perform analysis on the users' activity (instead of individual events, as in the previous section)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JipZu0WLt08N"
      },
      "source": [
        "### Calculate and explore user-level aggregate metrics\n",
        "First, you use BigQuery to generate some interesting aggregates at the user level: page views, user engagements, and total purchase value (sales)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxWnQ_ZKYjdj"
      },
      "outputs": [],
      "source": [
        "%%bigquery user_aggregates --project $PROJECT_ID\n",
        "  SELECT\n",
        "    user_pseudo_id,\n",
        "    \n",
        "    /* Use array functionality to get most recent language by user\n",
        "       (in case of >1 language per user, which is possible) */\n",
        "    IFNULL((ARRAY_AGG(device.language ORDER BY event_timestamp DESC)\n",
        "      [SAFE_ORDINAL(1)]), 'NULL') \n",
        "      AS language,\n",
        "    \n",
        "    SUM(IF(event_name = 'page_view', 1, 0))\n",
        "      AS num_page_views,\n",
        "    \n",
        "    SUM(IF(event_name = 'user_engagement', 1, 0))\n",
        "      AS num_user_engagements,\n",
        "    \n",
        "    SUM(IF(event_name = 'purchase', event_value_in_usd, 0))\n",
        "      AS tot_purchase_value_usd\n",
        "\n",
        "  FROM\n",
        "    `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\n",
        "\n",
        "  GROUP BY\n",
        "    user_pseudo_id\n",
        "    \n",
        "  ORDER BY\n",
        "    tot_purchase_value_usd DESC, num_page_views DESC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-IKCuokYjdj"
      },
      "outputs": [],
      "source": [
        "user_aggregates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oodp-YEtuSf8"
      },
      "source": [
        "Looking at the user aggregates output, you can see there are ~270K distinct users in the sample GA4 data, with the highest spenders purchasing items that total more than $1400 from the online store."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceLuuLvhuoDd"
      },
      "source": [
        "You can now explore this user data further using [Plotly's scatter plot functionality](https://plotly.com/python/line-and-scatter/) to look at engagements vs page views in an interactive plot. The code below does that, while also sizing points by total purchase value and coloring them by user device language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ni04VBRTYjdj"
      },
      "outputs": [],
      "source": [
        "user_page_views_engagements_scatter_plot = px.scatter(\n",
        "    user_aggregates,\n",
        "    x=\"num_page_views\",\n",
        "    y=\"num_user_engagements\",\n",
        "    size=\"tot_purchase_value_usd\",\n",
        "    color=\"language\",\n",
        "    title=(\n",
        "        \"<b>User-Level Engagements vs Page Views</b><br>\"\n",
        "        + \"(Each Point is 1 User, Sized by Total Purchase Value)\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "user_page_views_engagements_scatter_plot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYbk4u4Jwop2"
      },
      "source": [
        "There's a lot you can see by playing around with this interactive plot, including:\n",
        "\n",
        "*   There's an extremely strong positive correlation between user page views and engagements, which makes sense logically.\n",
        "*   Users with higher page views and engagements generally seem to have higher total purchase values, though you can see users with more money spent toward the bottom left and users spending relatively less toward the top right. \n",
        "*   With few exceptions, the most active users (high on both page views and engagements) are those with \"en-us\" as their language or (more commonly) no known device language. You know from before that those are the two most common values in the lanaguage field across all events, so perhaps this is consistent with that and relatively unsurprising. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M088ealAvArM"
      },
      "source": [
        "### Examine correlations between user-level metrics\n",
        "You saw the some of the correlations among user page views, engagements, and purchase value in the above plot. Next, you calculate the actual correlation values among these quantities across users."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5aUnESxYjdj"
      },
      "outputs": [],
      "source": [
        "user_aggregates.corr()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOPUtyhYyGz8"
      },
      "source": [
        "The correlation between user page views and engagements is extremely high (0.98), as expected, while the correlations between each of those and purchase value is positive but much more modest (0.38)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPZV_-y1ycDa"
      },
      "source": [
        "Going beyond the overall correlation, you might want to understand if these relationships are similar across different slices of your data - user demographics, for example. In the query below, you calculate the correlation between each of page views and engagements with purchase value, but this time _by country_, using [BigQuery's built-in correlation functionality](https://cloud.google.com/bigquery/docs/reference/standard-sql/statistical_aggregate_functions#corr)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsvCX2FXYjdj"
      },
      "outputs": [],
      "source": [
        "%%bigquery user_purchase_value_correlations_by_country --project $PROJECT_ID \n",
        "/* Use structure of user_aggregates query from above in WITH clause, as first \n",
        "   step to aggregate data to user level */\n",
        "WITH\n",
        "user_aggregates AS\n",
        "(\n",
        "  SELECT\n",
        "    user_pseudo_id,\n",
        "\n",
        "    /* Use array functionality to get most recent country by user\n",
        "       (in case of >1 country per user, which is possible) */\n",
        "    IFNULL((ARRAY_AGG(geo.country ORDER BY event_timestamp DESC)\n",
        "      [SAFE_ORDINAL(1)]), 'Unknown') \n",
        "      AS country,\n",
        "    \n",
        "    SUM(IF(event_name = 'page_view', 1, 0))\n",
        "      AS num_page_views,\n",
        "    \n",
        "    SUM(IF(event_name = 'user_engagement', 1, 0))\n",
        "      AS num_user_engagements,\n",
        "    \n",
        "    SUM(IF(event_name = 'purchase', event_value_in_usd, 0))\n",
        "      AS tot_purchase_value_usd\n",
        "\n",
        "  FROM\n",
        "    `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\n",
        "\n",
        "  GROUP BY\n",
        "    user_pseudo_id\n",
        ")\n",
        "\n",
        "SELECT\n",
        "  country,\n",
        "  COUNT(*) AS num_users,\n",
        "  CORR(num_page_views, tot_purchase_value_usd)\n",
        "    AS corr_page_views_purchase_value,\n",
        "  \n",
        "  CORR(num_user_engagements, tot_purchase_value_usd)\n",
        "    AS corr_engagements_purchase_value\n",
        "\n",
        "FROM\n",
        "  user_aggregates\n",
        "\n",
        "GROUP BY\n",
        "  country\n",
        "\n",
        "/* Only return countries w/ 500+ users (for higher sample size) */\n",
        "HAVING\n",
        "  num_users >= 500    \n",
        "    \n",
        "ORDER BY\n",
        "  num_users DESC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zu6OKgIjz2hK"
      },
      "source": [
        "With the data read in from BigQuery, you can now display it in an interactive table (with some formatting to avoid having to read really long decimal columns)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCQcZuSjYjdk"
      },
      "outputs": [],
      "source": [
        "with pd.option_context(\"display.float_format\", \"{:,.3f}\".format):\n",
        "    show(user_purchase_value_correlations_by_country)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2UNWftQ0KT1"
      },
      "source": [
        "From sorting the interactive table on the correlation columns in both directions, you can see that the correlations between the user activity metrics and purchase value do differ across countries. Some countries like Greece and Colombia have correlations greater than 0.6, while others like Belgium and Italy have correlations less than 0.25. This says something about how the translation from customer site activity to sales varies in different places around the world - a topic with potential for deeper study and implications for how you might go about modeling purchase behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiQE3stNYjdk"
      },
      "source": [
        "## Time series analysis of purchases by date and time\n",
        "Another common exploratory data analysis pattern is to look at certain metrics across time. There is an entire field - and a [powerful set of Google Cloud tools](https://cloud.google.com/learn/what-is-time-series) - devoted to time series forecasting, but in this case you'll focus on more fundamental exploration of the sample GA4 data aggregated by date and/or time fields."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp2Zux376D0B"
      },
      "source": [
        "### Create template query to aggregate purchase value across date and time\n",
        "\n",
        "You want to analyze purchase data across time - first, aggregated to the date level, and then also across hours of the day. One way to do this without repeating code is to write a SQL query template like the one below, which aggregates to the date level and provides the option to enter in text corresponding to an hour field (which is also added to the grouping for aggregation). \n",
        "\n",
        "In this example, you want to focus on United States data only to simplify the issues with dates across the world meaning different things based on time zones. To enable this, the code below filters to only US events and moves all events to their date/time in US Pacific Time ([the event_timestamp field in Google Analytics data is in UTC](https://support.google.com/analytics/answer/7029846)) to match what is generally day/night in the country."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_xwqV-SYjdk"
      },
      "outputs": [],
      "source": [
        "purchases_by_date_hour_sql_template = \"\"\"\n",
        "    WITH\n",
        "    USDateAggregates AS\n",
        "    (\n",
        "      SELECT\n",
        "        /* Use event_timestamp and move things to date in US (Pacific Time) */\n",
        "        EXTRACT(DATE FROM TIMESTAMP_MICROS(event_timestamp) \n",
        "          AT TIME ZONE \"America/Los_Angeles\") AS event_date,\n",
        "\n",
        "        /* Add template field for hour of day, can use to aggregrate or split */\n",
        "        {hour_of_day_sql} AS hour_of_day,        \n",
        "\n",
        "        COUNT(*) AS num_purchases,\n",
        "        SUM(event_value_in_usd) AS total_purchase_value\n",
        "\n",
        "      FROM \n",
        "        `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\n",
        "\n",
        "      WHERE\n",
        "        /* Filter to US data only */\n",
        "        geo.country = 'United States' AND\n",
        "        /* Look only at purchases with non-NULL values */\n",
        "        event_name = 'purchase' AND\n",
        "        event_value_in_usd IS NOT NULL\n",
        "\n",
        "      GROUP BY\n",
        "        event_date, hour_of_day\n",
        "    )\n",
        "\n",
        "    SELECT\n",
        "      event_date,\n",
        "      hour_of_day,\n",
        "\n",
        "      /* Extract various date-related fields from event date */\n",
        "      EXTRACT(YEAR FROM event_date) AS year,\n",
        "      EXTRACT(MONTH FROM event_date) AS month,\n",
        "      EXTRACT(DAY FROM event_date) AS day,\n",
        "\n",
        "      EXTRACT(WEEK FROM event_date) AS week_of_year_num,\n",
        "      EXTRACT(ISOWEEK FROM event_date) AS iso_week,\n",
        "\n",
        "      EXTRACT(DAYOFWEEK FROM event_date) AS day_of_week_num,\n",
        "      FORMAT_DATE(\"%A\", event_date) AS day_of_week,\n",
        "\n",
        "      EXTRACT(DAYOFYEAR FROM event_date) AS day_of_year,\n",
        "\n",
        "      num_purchases,\n",
        "      total_purchase_value,\n",
        "\n",
        "      SAFE_DIVIDE(\n",
        "        total_purchase_value,\n",
        "        num_purchases\n",
        "        ) AS avg_purchase_value\n",
        "\n",
        "    FROM\n",
        "      USDateAggregates\n",
        "\n",
        "    ORDER BY\n",
        "      event_date, hour_of_day\n",
        "    \"\"\"\n",
        "(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dSrU9eK7DsL"
      },
      "source": [
        "### Look at total purchase value by date\n",
        "\n",
        "First, you look at purchase aggregates by date across the three months in the Google Analytics data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2IXKpRx7CqF"
      },
      "outputs": [],
      "source": [
        "# Run query with 'ALL' (string) for hour of day to aggregate all hours together\n",
        "purchases_by_date = bq_query(\n",
        "    purchases_by_date_hour_sql_template.format(hour_of_day_sql=\"'ALL'\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n42RgsMS-fIO"
      },
      "source": [
        "You can plot the daily purchase sales in an interactive time series plot using [Plotly's line plot functionality](https://plotly.com/python/line-charts/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCk67GQxYjdk"
      },
      "outputs": [],
      "source": [
        "total_purchase_value_by_date_plot = px.line(\n",
        "    purchases_by_date,\n",
        "    x=\"event_date\",\n",
        "    y=\"total_purchase_value\",\n",
        "    # Add day of week to hover data to help read off interactive plot\n",
        "    hover_data=[\"event_date\", \"day_of_week\", \"total_purchase_value\"],\n",
        "    title=\"<b>United States Total Purchase Value (USD) by Date<b>\",\n",
        ")\n",
        "\n",
        "total_purchase_value_by_date_plot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwCMjAfx_KrW"
      },
      "source": [
        "A few things you can see from this plot:\n",
        "\n",
        "*   There's the expected US retail industry pattern of a positive trend in sales throughout November, a relatively high level of sales during the pre-Christmas shopping season (through mid-December), and then a dramatic drop off leading into the end-of-year holidays that mostly persists through January.\n",
        "*   There is substantial variance in total sales from day to day. At the extremes, the day with the highest sales in the period is [\"Cyber Monday\"](https://en.wikipedia.org/wiki/Cyber_Monday) and the day with the lowest sales (among days for which there is full data) is Christmas - both of which are relatively unsurprising. \n",
        "*   There appear to be some day-of-week effects like Mondays being relatively high in sales and weekends being relatively low."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WDuc_kn7J-t"
      },
      "source": [
        "### Look at total purchase value by day of week and time of day\n",
        "Next, you go further by gathering total purchase data on both the date and hour of day level - a finer level of aggregation than above - and using a heatmap to study common purchase patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IvA-mKKYjdk"
      },
      "outputs": [],
      "source": [
        "# Run query with string corresponding to hour of day (in US Pacific Time)\n",
        "purchases_by_date_hour = bq_query(\n",
        "    purchases_by_date_hour_sql_template.format(\n",
        "        hour_of_day_sql=(\n",
        "            \"EXTRACT(HOUR FROM TIMESTAMP_MICROS(event_timestamp)\"\n",
        "            + \"AT TIME ZONE 'America/Los_Angeles')\"\n",
        "        )\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQpb6F8rCDet"
      },
      "source": [
        "To get a heatmap in the desired \"week timeline\" form, you further aggregate to the day-of-week and hour-of-day level while pivoting to the right format. Each cell in the heatmap represents the total US sales for a given hour on the given day of the week across all dates in the three months of data (e.g. the top left represents sales across all Sundays from 12-1am Pacific Time)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d_kpq10su8d"
      },
      "outputs": [],
      "source": [
        "# Add day of week field combining number and name, to help with sorting on plot\n",
        "purchases_by_date_hour[\"day_of_week_combo\"] = (\n",
        "    purchases_by_date_hour[\"day_of_week_num\"].astype(\"str\")\n",
        "    + \" - \"\n",
        "    + purchases_by_date_hour[\"day_of_week\"]\n",
        ")\n",
        "\n",
        "# Pivot to day-of-week/hour-of-day grid, summing purchase value while doing so\n",
        "purchases_by_dayofweek_hourofday_matrix = pd.pivot_table(\n",
        "    purchases_by_date_hour,\n",
        "    values=\"total_purchase_value\",\n",
        "    index=[\"day_of_week_combo\"],\n",
        "    columns=[\"hour_of_day\"],\n",
        "    aggfunc=np.sum,\n",
        ")\n",
        "\n",
        "purchases_by_dayofweek_hourofday_heatmap = px.imshow(\n",
        "    purchases_by_dayofweek_hourofday_matrix,\n",
        "    labels=dict(x=\"Hour of Day\", y=\"Day of Week\", color=\"Total Purchase Value\"),\n",
        "    title=(\"<b>United States Total Purchase Value (USD) by Day & Hour (PDT)<b>\"),\n",
        ")\n",
        "\n",
        "purchases_by_dayofweek_hourofday_heatmap.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzaQYEjA7cM0"
      },
      "source": [
        "A few things you can see from this plot:\n",
        "\n",
        "*   There is substantial variance in total sales across days and hours. At the extremes, there are day/hour combinations with more than $\\$$2,000 in sales and others with less than $\\$$100 in sales over this time period.\n",
        "*   As you saw in the daily sales plot, Saturday and Sunday are relatively low in sales (with the possible exception of late Sunday), while Monday is relatively high (with different peaks throughout the day). Times that correspond to the middle of the night across most of the country (0-5 on the x-axis) are generally slow on most days, with some outliers.\n",
        "\n",
        "Some of the things you see in the plot above don't totally fit with what you'd expect based on time zones, so it may be worth checking for outlier (high) purchase values that could've skewed the data in some of the day/hour combinations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEALdpZlauX6"
      },
      "source": [
        "## Save some outputs to Cloud Storage\n",
        "Sometimes you may want some of your code output - tables, graphs, etc. - to persist outside of your notebook. This could be to preserve some results for when you're done with the notebook or to share with various collaborators via other mediums.\n",
        "\n",
        "In this section, you'll take some of the outputs generated earlier and save them outside of the notebooks in three different ways:\n",
        "*   a table to a CSV file\n",
        "*   an interactive plot to an HTML file\n",
        "*   an interactive plot to a (static) PNG file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A1-0VP627NP"
      },
      "source": [
        "First, you save the table of user-level correlations by country as a CSV. Initially, this will save the CSV file in the local notebook environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylUTealhbiDu"
      },
      "outputs": [],
      "source": [
        "user_country_correlations_output_name = \"user_level_correlations_by_country.csv\"\n",
        "\n",
        "user_purchase_value_correlations_by_country.to_csv(\n",
        "    user_country_correlations_output_name\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjzuSp643f4O"
      },
      "source": [
        "Next, you save the interactive bar plot of event counts by name as an HTML file using [Plotly's HTML export capabilities](https://plotly.com/python/interactive-html-export/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZP4CPJh7Yjdl"
      },
      "outputs": [],
      "source": [
        "event_name_counts_output_name = \"event_name_counts_interactive_bar_plot.html\"\n",
        "\n",
        "event_name_counts_interactive_bar_plot.write_html(event_name_counts_output_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48wqq17_11W7"
      },
      "source": [
        "While interactive plots can be powerful, there are often cases where you need to convert your interactive into a static plot to fit certain constraints or simplify things for readers. In this case, you save the heatmap of purchase by day and hour as a static PNG file using [Plotly's static image export functionality](https://plotly.com/python/static-image-export/). This uses the [kaleido library](https://github.com/plotly/Kaleido) \"under the hood\" to convert the interactive plot to a static one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQ93vjTTYjdl"
      },
      "outputs": [],
      "source": [
        "purchases_day_hour_output_name = \"purchases_by_dayofweek_hourofday_heatmap.png\"\n",
        "\n",
        "purchases_by_dayofweek_hourofday_heatmap.write_image(purchases_day_hour_output_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTyksBB32IQ3"
      },
      "source": [
        "Finally, you take the three different output files in the local notebook environment and copy them to your specified Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFV6Ktw9lNiR"
      },
      "outputs": [],
      "source": [
        "output_file_names = [\n",
        "    user_country_correlations_output_name,\n",
        "    event_name_counts_output_name,\n",
        "    purchases_day_hour_output_name,\n",
        "]\n",
        "\n",
        "for output_file_name in output_file_names:\n",
        "    ! gsutil cp $output_file_name $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS1c8xutYjdl"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the Cloud Storage bucket that you created in this tutorial if you'd like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx_vKniMq9ZX"
      },
      "outputs": [],
      "source": [
        "# Delete Cloud Storage bucket that was created\n",
        "if os.getenv(\"IS_TESTING\"):\n",
        "    ! gsutil -m rm -r $BUCKET_URI"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "explore_data_in_bigquery_with_workbench.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

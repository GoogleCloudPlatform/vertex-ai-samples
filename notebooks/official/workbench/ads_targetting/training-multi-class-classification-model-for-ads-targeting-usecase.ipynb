{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "503077811e70"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e885ac09bc73"
      },
      "source": [
        "# Train a multi-class classification model for ads-targeting\n",
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/workbench/ads_targetting/training-multi-class-classification-model-for-ads-targeting-usecase.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/workbench/ads_targetting/training-multi-class-classification-model-for-ads-targeting-usecase.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "<a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/workbench/ads_targetting/training-multi-class-classification-model-for-ads-targeting-usecase.ipynb\" target='_blank'>\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>                                                                                               \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00f095e91b2d"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial demonstrates how to build a machine learning model for an ads-targeting use case. Ads-targeting is an advertisement technique where chosen or tailor-made ads are shown to the customers based on their past behavior and preferences. Targeted ads are meant to reach specific customers based on demographics, psychographics, behavior, and other second-order activities that are learned usually through data collected from the customers.\n",
        "\n",
        "*Note: If you are using [Vertex AI Workbench managed notebooks](https://cloud.google.com/vertex-ai/docs/workbench/managed/create-instance) instance use the `TensorFlow 2 (Local)` kernel. Some components of this notebook may not work in other notebook environments.*\n",
        "\n",
        "Learn more about [Vertex AI Workbench](https://cloud.google.com/vertex-ai/docs/workbench/introduction) and [Vertex AI Training](https://cloud.google.com/vertex-ai/docs/training/custom-training)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bea2b6e9b25"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to collect data from BigQuery, preprocess it, and train a multi-class classification model on an e-commerce dataset. \n",
        "\n",
        "This tutorial uses the following Google Cloud ML services and resources:\n",
        "\n",
        "- BigQuery\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Fetch the required data from BigQuery\n",
        "- Preprocess the data\n",
        "- Train a TensorFlow (>=2.4) classification model\n",
        "- Evaluate the loss for the trained model\n",
        "- Automate the notebook execution using the executor feature\n",
        "- Save the model to a Cloud Storage path\n",
        "- Clean up the created resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34d623e6dfa3"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "This tutorial uses the `looker-private-demo.ecomm` dataset in BigQuery. The dataset consists of information about various advertisement campaigns including the demographics of users who have clicked and made some purchases after seeing the ads. For this tutorial, the top three campaigns from the USA are selected from this dataset and user information for those who have made purchases shall be used to train a model with the campaigns as the classes. The idea is to see if the advertisement and the user data can be used to identify which campaign is best-suited for the user.\n",
        "\n",
        "The dataset can be accessed by pinning the `looker-private-demo` project in BigQuery. If you are using Vertex AI Workbench managed notebooks instance, instead of going to the BigQuery user interface, this process can be performed from the JupyterLab user interface. Vertex AI Workbench managed notebooks instances support browsing through the datasets and tables from BigQuery through its BigQuery integration. \n",
        "\n",
        "<img src=\"images/Bigquery_UI_new.PNG\"></img>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee02650bb7fd"
      },
      "source": [
        "### Costs \n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* BigQuery\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing), [BigQuery\n",
        "pricing](https://cloud.google.com/bigquery/pricing) and [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y320EIk-kXT7"
      },
      "source": [
        "### Set up your local development environment\n",
        "\n",
        "**If you are using Colab or Vertex AI Workbench Notebooks**, your environment already meets\n",
        "all the requirements to run this notebook. You can skip this step.\n",
        "\n",
        "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
        "You need the following:\n",
        "\n",
        "* The Google Cloud SDK\n",
        "* Git\n",
        "* Python 3\n",
        "* virtualenv\n",
        "* Jupyter notebook running in a virtual environment with Python 3\n",
        "\n",
        "The Google Cloud guide to [Setting up a Python development\n",
        "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
        "installation guide](https://jupyter.org/install) provide detailed instructions\n",
        "for meeting these requirements. The following steps provide a condensed set of\n",
        "instructions:\n",
        "\n",
        "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
        "\n",
        "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
        "\n",
        "1. [Install\n",
        "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
        "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
        "\n",
        "1. To install Jupyter, run `pip3 install jupyter` on the\n",
        "command-line in a terminal shell.\n",
        "\n",
        "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
        "\n",
        "1. Open this notebook in the Jupyter Notebook Dashboard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DouUvNOkXT8"
      },
      "source": [
        "### Install additional packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ayt1jhFXkXT9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Vertex AI Workbench Notebook product has specific requirements\n",
        "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
        "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
        "    \"/opt/deeplearning/metadata/env_version\"\n",
        ")\n",
        "\n",
        "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_WORKBENCH_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\"\n",
        "\n",
        "! pip3 install {USER_FLAG} --upgrade pandas-gbq 'google-cloud-bigquery[bqstorage,pandas]' tensorflow scikit-learn protobuf==3.20.3 -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNeMRbpukXUA"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJ_yvi_9kXUB"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a47846030fef"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71b29797836c"
      },
      "source": [
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. <a href=\"https://console.cloud.google.com/cloud-resource-manager\" target=\"_blank\">Select or create a Google Cloud project</a>. When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "1. <a href=\"https://cloud.google.com/billing/docs/how-to/modify-project\" target=\"_blank\">Make sure that billing is enabled for your project</a>.\n",
        "\n",
        "1. <a href=\"https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com\" target=\"_blank\">Enable the Vertex AI API</a>.\n",
        "\n",
        "1. If you are running this notebook locally, you will need to install the <a href=\"https://cloud.google.com/sdk\" target=\"_blank\">Cloud SDK</a>.\n",
        "\n",
        "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7be962cec10"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c8049930470"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a36c4b991a39"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID:\", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "684595f229b3"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03d8d65b914d"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
        "\n",
        "- Americas: `us-central1`\n",
        "- Europe: `europe-west4`\n",
        "- Asia Pacific: `asia-east1`\n",
        "\n",
        "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
        "\n",
        "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3281bedf6d3c"
      },
      "outputs": [],
      "source": [
        "REGION = \"us-central1\"  # @param {type: \"string\"}\n",
        "\n",
        "if REGION == \"[your-region]\":\n",
        "    REGION = \"us-central1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2b04f364669"
      },
      "source": [
        "#### UUID\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a UUID for each instance session, and append it onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9ee95826661"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "\n",
        "# Generate a uuid of a specifed length(default=8)\n",
        "def generate_uuid(length: int = 8) -> str:\n",
        "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
        "\n",
        "\n",
        "UUID = generate_uuid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoPGk5KOkXUG"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using Vertex AI Workbench Notebooks**, your environment is already\n",
        "authenticated. Skip this step.\n",
        "\n",
        "**If you are using Colab**, run the cell below and follow the instructions\n",
        "when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "1. In the Cloud Console, go to the [**Create service account key**\n",
        "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
        "\n",
        "2. Click **Create service account**.\n",
        "\n",
        "3. In the **Service account name** field, enter a name, and\n",
        "   click **Create**.\n",
        "\n",
        "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
        "into the filter box, and select\n",
        "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "\n",
        "5. Click *Create*. A JSON file that contains your key downloads to your\n",
        "local environment.\n",
        "\n",
        "6. Enter the path to your service account key as the\n",
        "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Teyy6LGqkXUG"
      },
      "outputs": [],
      "source": [
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# If on Vertex AI Workbench, then don't execute this code\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
        "    \"DL_ANACONDA_HOME\"\n",
        "):\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f0d924a47aa"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
        "Cloud Storage buckets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59ebce2a16fa"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0caa7b3c8b8"
      },
      "outputs": [],
      "source": [
        "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
        "    BUCKET_NAME = PROJECT_ID + \"aip-\" + UUID\n",
        "    BUCKET_URI = f\"gs://{BUCKET_NAME}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a7c87b6f171"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76f7e3e043f5"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4353e94da5f"
      },
      "source": [
        "**Finally**, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6f092d6ebc92"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmnMD2MjkXUJ"
      },
      "source": [
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqtZRqDEkXUJ"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "import pandas as pd\n",
        "from google.cloud.bigquery import Client\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9abd63bb1a85"
      },
      "source": [
        "## Tutorial\n",
        "\n",
        "### Fetch the data from BigQuery \n",
        "If you are using ***Vertex AI Workbench managed notebooks instance***, below cell which starts with \"#@bigquery\" will be a SQL Query. If you are using Vertex AI Workbench user managed notebooks instance or Colab it will be a markdown cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a25861cf833"
      },
      "source": [
        "#@bigquery\n",
        "\n",
        "WITH\n",
        "  traindata AS (\n",
        "  SELECT\n",
        "    b.* EXCEPT(ad_event_id,\n",
        "      user_id),\n",
        "    c.* EXCEPT(id),\n",
        "    d.* EXCEPT(keyword_id,\n",
        "      ad_id),\n",
        "    a.amount,\n",
        "    a.device_type,\n",
        "    e.name\n",
        "  FROM\n",
        "    `looker-private-demo.ecomm.ad_events` a\n",
        "  JOIN (\n",
        "    SELECT\n",
        "      ad_event_id,\n",
        "      user_id,\n",
        "      state,\n",
        "      os,\n",
        "      browser\n",
        "    FROM\n",
        "      `looker-private-demo.ecomm.events`\n",
        "    WHERE\n",
        "      event_type=\"Purchase\"\n",
        "      AND country=\"USA\") b\n",
        "  ON\n",
        "    a.id = b.ad_event_id\n",
        "  JOIN (\n",
        "    SELECT\n",
        "      id,\n",
        "      gender,\n",
        "      age\n",
        "    FROM\n",
        "      `looker-private-demo.ecomm.users`) c\n",
        "  ON\n",
        "    b.user_id = c.id\n",
        "  JOIN (\n",
        "    SELECT\n",
        "      keyword_id,\n",
        "      ad_id,\n",
        "      cpc_bid_amount,\n",
        "      bidding_strategy_type,\n",
        "      quality_score,\n",
        "      keyword_match_type\n",
        "    FROM\n",
        "      `looker-private-demo.ecomm.keywords`\n",
        "    WHERE\n",
        "      cpc_bid_amount <= 3000) d\n",
        "  ON\n",
        "    a.keyword_id = d.keyword_id\n",
        "  JOIN (\n",
        "    SELECT\n",
        "      ad_id,\n",
        "      name\n",
        "    FROM\n",
        "      `looker-private-demo.ecomm.ad_groups`) e\n",
        "  ON\n",
        "    d.ad_id = e.ad_id )\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  traindata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "923fdd823683"
      },
      "source": [
        "If you are using Vertex AI Workbench managed notebooks instance, once the results from BigQuery are displayed in the above cell, click the **Query and load as DataFrame** button and execute the generated code stub to fetch the data into the current notebook as a dataframe.\n",
        "\n",
        "*Note: By default the data is loaded into a `df` variable, though this can be changed before executing the cell if required.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d44a10b6884"
      },
      "outputs": [],
      "source": [
        "client = Client(project=PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8b5112f231f"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"WITH traindata AS (\n",
        "SELECT b.* except(ad_event_id, user_id), c.* except(id), d.* except(keyword_id, ad_id), a.amount, a.device_type, e.name\n",
        "FROM `looker-private-demo.ecomm.ad_events` a\n",
        "JOIN\n",
        "(SELECT ad_event_id, user_id, state, os, browser from `looker-private-demo.ecomm.events` WHERE event_type=\"Purchase\" AND country=\"USA\") b\n",
        "ON a.id = b.ad_event_id\n",
        "JOIN\n",
        "(SELECT id, gender, age FROM `looker-private-demo.ecomm.users`) c\n",
        "ON b.user_id = c.id\n",
        "JOIN\n",
        "(SELECT keyword_id, ad_id, cpc_bid_amount, bidding_strategy_type, quality_score, keyword_match_type FROM `looker-private-demo.ecomm.keywords`\n",
        "WHERE cpc_bid_amount <= 3000) d\n",
        "ON a.keyword_id = d.keyword_id\n",
        "JOIN\n",
        "(SELECT ad_id, name FROM `looker-private-demo.ecomm.ad_groups`) e\n",
        "ON d.ad_id = e.ad_id\n",
        ")\n",
        "SELECT * FROM traindata\"\"\"\n",
        "job = client.query(query)\n",
        "df = job.to_dataframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f69bea65019"
      },
      "source": [
        "### Preprocess the data\n",
        "Select the necessary columns from the e-commerce data and divide them based on their type (numerical/categorical)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cea2c44c50b"
      },
      "outputs": [],
      "source": [
        "target = \"name\"\n",
        "categ_cols = [\n",
        "    \"state\",\n",
        "    \"os\",\n",
        "    \"browser\",\n",
        "    \"gender\",\n",
        "    \"bidding_strategy_type\",\n",
        "    \"keyword_match_type\",\n",
        "    \"device_type\",\n",
        "]\n",
        "num_cols = [\"age\", \"cpc_bid_amount\", \"quality_score\", \"amount\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ace612851261"
      },
      "source": [
        "#### Select top three campaigns\n",
        "From the current dataset, only the top three campaigns will be chosen to target the users. All the relevant information about the advertisement and the user who purchased an item after seeing the advertisement is available in the dataframe already. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7282fbab4586"
      },
      "outputs": [],
      "source": [
        "df = df[df[\"name\"].isin([\"Tops & Tees\", \"Active\", \"Accessories\"])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f89106348ffe"
      },
      "source": [
        "#### Encode the target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6cd256f455a"
      },
      "outputs": [],
      "source": [
        "df[\"name\"] = df[\"name\"].map({\"Tops & Tees\": 0, \"Active\": 1, \"Accessories\": 2})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2d5338b1b95"
      },
      "source": [
        "#### One-hot encode the categorical variables\n",
        "After one-hot encoding, the first level-column is dropped to avoid the [dummy-variable trap](https://en.wikipedia.org/wiki/Dummy_variable_(statistics)) scenario. This process is called *dummy-encoding*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d57706df2441"
      },
      "outputs": [],
      "source": [
        "def encode_cols(data, col):\n",
        "    # Creating a dummy variable for the variable 'CategoryID' and dropping the first one.\n",
        "    categ = pd.get_dummies(data[col], prefix=col, drop_first=True)\n",
        "    # Adding the results to the master dataframe\n",
        "    data = pd.concat([data, categ], axis=1)\n",
        "    return data\n",
        "\n",
        "\n",
        "# dummy-encode the categorical fields\n",
        "for i in categ_cols:\n",
        "    df = encode_cols(df, i)\n",
        "    df.drop(columns=[i], inplace=True)\n",
        "\n",
        "# check the data's shape\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3abf027eda2d"
      },
      "source": [
        "#### Split the data into train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0072d44b6163"
      },
      "outputs": [],
      "source": [
        "X = df[[i for i in df.columns if i != target]].copy()\n",
        "y = df[target].copy()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, train_size=0.8, random_state=36\n",
        ")\n",
        "print(X_train.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1a32b9d9640"
      },
      "source": [
        "#### Scale the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9620e04d8db2"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_train.loc[:, num_cols] = scaler.fit_transform(X_train[num_cols])\n",
        "X_test.loc[:, num_cols] = scaler.transform(X_test[num_cols])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b9f3ca04f91"
      },
      "source": [
        "### Train a TensorFlow model\n",
        "Convert the target column to a categorical encoded colum (one-hot encoded)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebc87650ae13"
      },
      "outputs": [],
      "source": [
        "y_train_categ = to_categorical(y_train)\n",
        "y_test_categ = to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dd0014a7e1d"
      },
      "source": [
        "#### Define hyperparameters for model training\n",
        "\n",
        "*Note: Comment or remove the parameters from the following cell if they are provided already as an input parameter through the executor feature.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec020b36af20"
      },
      "outputs": [],
      "source": [
        "optimizer = \"sgd\"\n",
        "num_hidden_layers = 3\n",
        "num_neurons = [64, 128, 256]\n",
        "activ_func = [\"relu\", \"relu\", \"relu\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "406b731f576b"
      },
      "source": [
        "#### Define the architecture and compile the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57839a187cf0"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "# construct the neural network as per the defined parameters\n",
        "for i in range(num_hidden_layers):\n",
        "    if i == 0:\n",
        "        # add the input layer\n",
        "        model.add(\n",
        "            Dense(\n",
        "                num_neurons[i],\n",
        "                activation=activ_func[i],\n",
        "                input_shape=(X_train.shape[1],),\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        # add the hidden layers\n",
        "        model.add(Dense(num_neurons[i], activation=activ_func[i]))\n",
        "\n",
        "# add the output layer\n",
        "model.add(Dense(3, activation=\"softmax\"))\n",
        "# compile the model\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ab12c34f258"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9321005e55ae"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train, y_train_categ, epochs=50, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f08445f2cd02"
      },
      "source": [
        "### Evaluate the model on test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "599df6d2b9a4"
      },
      "outputs": [],
      "source": [
        "test_results = model.evaluate(X_test, y_test_categ, verbose=1)\n",
        "print(f\"Test results - Loss: {test_results}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81ef0e081340"
      },
      "source": [
        "**Note:**  Please note that executor feature is available only in Vertex AI Workbench managed notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9769168778e8"
      },
      "source": [
        "### Automating the execution of the notebook using executor in Vertex AI Workbench managed notebooks instance\n",
        "\n",
        "If you are using Vertex AI Workbench managed notebooks instance, the executor can help you run a notebook file from start to end, with your choice of the environment, machine type, input parameters, and other characteristics. After setting up an execution, the notebook is executed as a job in Vertex AI custom training. Your jobs can be monitored from the <b>Notebook Executor</b> pane in the menu on the left.\n",
        "\n",
        "<img src=\"images/executor.png\"></img>\n",
        "\n",
        "Executor lets you choose the environment and machine type while automating the runs similar to Vertex AI training jobs without switching to the training jobs UI. Apart from the custom container that replicates the existing kernel by default, pre-built environments like TensorFlow Enterprise, PyTorch, and others can also be selected to run the notebook. Furthermore the required compute power can be specified by choosing from the list of machine types available, including GPUs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf486c351581"
      },
      "source": [
        "### Scheduled runs on executor in Vertex AI Workbench managed notebooks instance\n",
        "\n",
        "Vertex AI Workbench managed noteboook runs can also be scheduled recurringly with the executor. To do so, select <b>Schedule-based recurring executions</b> as the run type instead of <b>One-time execution</b>. The frequency of the job and the time when it executes is provided when you create the execution.\n",
        "\n",
        "<img src=\"images/executor_scheduled_runs2.png\"></img>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6561007ac7f2"
      },
      "source": [
        "### Parameterizing the variables\n",
        "\n",
        "If you are using Vertex AI Workbench managed notebooks instance, executor lets you run a notebook with different sets of input parameters. If required, constants in the notebook can be treated as arguments to a function, and when you submit the execution, you can provide those constants as input parameters.\n",
        "\n",
        "<img src=\"images/executor_input_parameters.png\"></img>\n",
        "\n",
        "The hyperparameters defined during the model training step can be passed as arguments while submitting an execution. However, the values defined in the notebook itself should be removed or commented out before submitting the execution. Otherwise, the input parameters would just be overwritten by the values in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c81db97fa3e8"
      },
      "source": [
        "### Save the model to a Cloud Storage path\n",
        "\n",
        "TensorFlow's `model.save()` method supports Cloud Storage paths as well as the local file paths while writing the model object to a file. It needs to be ensured that the service account being used to run this notebook has `write` permissions to the specified Cloud Storage path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dcefeb6a2d8"
      },
      "outputs": [],
      "source": [
        "GCS_PATH = BUCKET_URI + \"/path-to-save/\"\n",
        "model.save(GCS_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29c0ca2a517a"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ab69210d5a8"
      },
      "outputs": [],
      "source": [
        "# Delete the Cloud Storage bucket\n",
        "\n",
        "delete_bucket = False\n",
        "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
        "    ! gsutil -m rm -r $BUCKET_URI"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "training-multi-class-classification-model-for-ads-targeting-usecase.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

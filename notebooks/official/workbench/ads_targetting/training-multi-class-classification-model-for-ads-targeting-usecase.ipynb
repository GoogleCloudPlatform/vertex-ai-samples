{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "503077811e70"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a multi-class classification model for ads-targeting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/workbench/ads_targetting/training-multi-class-classification-model-for-ads-targeting-usecase.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/workbench/ads_targetting/training-multi-class-classification-model-for-ads-targeting-usecase.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/workbench/ads_targetting/training-multi-class-classification-model-for-ads-targeting-usecase.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>                                                                                               \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "* [Overview](#section-1)\n",
    "* [Dataset](#section-2)\n",
    "* [Objective](#section-3)\n",
    "* [Costs](#section-4)\n",
    "* [Tutorial](#section-5)\n",
    "\t- [Fetch the data from BigQuery](#section-5)\n",
    "    - [Preprocess the data](#section-6)\n",
    "    - [Train a TensorFlow model](#section-7)\n",
    "    - [Run the model on test data](#section-8)\n",
    "    - [Automating the execution of the notebook using executor](#section-9)\n",
    "    - [Scheduled runs on executor](#section-10)\n",
    "    - [Parameterizing the variables](#section-11)\n",
    "* [Save the model to a Cloud Storage path](#section-12)\n",
    "* [Clean up](#section-13)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00f095e91b2d"
   },
   "source": [
    "## Overview\n",
    "<a name=\"section-1\"></a>\n",
    "\n",
    "This tutorial demonstrates how to build a machine learning model for an ads-targeting use case. Ads-targeting is an advertisement technique where chosen or tailor-made ads are shown to the customers based on their past behavior and preferences. Targeted ads are meant to reach specific customers based on demographics, psychographics, behavior, and other second-order activities that are learned usually through data collected from the customers.\n",
    "\n",
    "*Note: If you are using [Vertex AI Workbench managed notebooks](https://cloud.google.com/vertex-ai/docs/workbench/managed/create-instance) instance use the `TensorFlow 2 (Local)` kernel. Some components of this notebook may not work in other notebook environments.*\n",
    "\n",
    "## Dataset\n",
    "<a name=\"section-2\"></a>\n",
    "\n",
    "This tutorial uses the `looker-private-demo.ecomm` dataset in BigQuery. The dataset consists of information about various advertisement campaigns including the demographics of users who have clicked and made some purchases after seeing the ads. For this tutorial, the top three campaigns from the USA are selected from this dataset and user information for those who have made purchases shall be used to train a model with the campaigns as the classes. The idea is to see if the advertisement and the user data can be used to identify which campaign is best-suited for the user.\n",
    "\n",
    "The dataset can be accessed by pinning the `looker-private-demo` project in BigQuery. If you are using Vertex AI Workbench managed notebooks instance, instead of going to the BigQuery user interface, this process can be performed from the JupyterLab user interface. Vertex AI Workbench managed notebooks instances support browsing through the datasets and tables from BigQuery through its BigQuery integration. \n",
    "\n",
    "<img src=\"images/Bigquery_UI_new.PNG\"></img>\n",
    "\n",
    "## Objective\n",
    "<a name=\"section-3\"></a>\n",
    "\n",
    "This tutorial demonstrates how to collect data from BigQuery, preprocess it, and train a multi-class classification model on an E-commerce dataset. The steps performed include the following:\n",
    "\n",
    "- Fetch the required data from BigQuery\n",
    "- Preprocess the data\n",
    "- Train a TensorFlow (>=2.4) classification model\n",
    "- Evaluate the loss for the trained model\n",
    "- Automate the notebook execution using the executor feature\n",
    "- Save the model to a Cloud Storage path\n",
    "- Clean up the created resources\n",
    "\n",
    "### Costs \n",
    "<a name=\"section-4\"></a>\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* BigQuery\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing), [BigQuery\n",
    "pricing](https://cloud.google.com/bigquery/pricing) and [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wh1yCaDxkXT6"
   },
   "source": [
    "### Set up your local development environment\n",
    "\n",
    "**If you are using Colab or Google Cloud Notebooks**, your environment already meets\n",
    "all the requirements to run this notebook. You can skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y320EIk-kXT7"
   },
   "source": [
    "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
    "You need the following:\n",
    "\n",
    "* The Google Cloud SDK\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* Jupyter notebook running in a virtual environment with Python 3\n",
    "\n",
    "The Google Cloud guide to [Setting up a Python development\n",
    "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
    "installation guide](https://jupyter.org/install) provide detailed instructions\n",
    "for meeting these requirements. The following steps provide a condensed set of\n",
    "instructions:\n",
    "\n",
    "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "1. [Install\n",
    "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
    "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
    "\n",
    "1. To install Jupyter, run `pip3 install jupyter` on the\n",
    "command-line in a terminal shell.\n",
    "\n",
    "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
    "\n",
    "1. Open this notebook in the Jupyter Notebook Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DouUvNOkXT8"
   },
   "source": [
    "### Install additional packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ayt1jhFXkXT9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95826791kXT_",
    "outputId": "a956472c-e6d7-405d-da1e-59663f9fa011"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 210 kB 11.6 MB/s eta 0:00:01\n",
      "\u001b[K     |████████████████████████████████| 46 kB 3.7 MB/s \n",
      "\u001b[K     |████████████████████████████████| 76 kB 3.8 MB/s \n",
      "\u001b[K     |████████████████████████████████| 180 kB 54.3 MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 46.9 MB/s \n",
      "\u001b[K     |████████████████████████████████| 462 kB 47.7 MB/s \n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-cloud-translate 1.5.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.3.0 which is incompatible.\n",
      "google-cloud-storage 1.18.1 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.3.0 which is incompatible.\n",
      "google-cloud-storage 1.18.1 requires google-resumable-media<0.5.0dev,>=0.3.1, but you have google-resumable-media 2.3.2 which is incompatible.\n",
      "google-cloud-firestore 1.7.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.3.0 which is incompatible.\n",
      "google-cloud-datastore 1.8.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.3.0 which is incompatible.\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "! pip3 install {USER_FLAG} --upgrade pandas-gbq 'google-cloud-bigquery[bqstorage,pandas]' \\\n",
    "                                    tensorflow \\\n",
    "                                    sklearn -q\n",
    "                                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNeMRbpukXUA"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dJ_yvi_9kXUB"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7be962cec10"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "684595f229b3",
    "outputId": "bd257551-1ccb-4be0-f5c0-5ffd78c81189"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0058f55f8cf"
   },
   "source": [
    "Otherwise, set your project ID here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "19579640c063"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "07-xo93jlC6l",
    "outputId": "6bd98a4f-3c43-43bb-dc71-76b0a41c87a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2b04f364669"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append it onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "a9ee95826661"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBV40j3KkXUF"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "**If you are using Google Cloud Notebooks**, your environment is already\n",
    "authenticated. Skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoPGk5KOkXUG"
   },
   "source": [
    "**If you are using Colab**, run the cell below and follow the instructions\n",
    "when prompted to authenticate your account via oAuth.\n",
    "\n",
    "**Otherwise**, follow these steps:\n",
    "\n",
    "1. In the Cloud Console, go to the [**Create service account key**\n",
    "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
    "\n",
    "2. Click **Create service account**.\n",
    "\n",
    "3. In the **Service account name** field, enter a name, and\n",
    "   click **Create**.\n",
    "\n",
    "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
    "into the filter box, and select\n",
    "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
    "\n",
    "5. Click *Create*. A JSON file that contains your key downloads to your\n",
    "local environment.\n",
    "\n",
    "6. Enter the path to your service account key as the\n",
    "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Teyy6LGqkXUG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# If on Google Cloud Notebooks, then don't execute this code\n",
    "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7f0d924a47aa"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
    "Cloud Storage buckets.\n",
    "\n",
    "You may also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook. Make sure to [choose a region where Vertex AI services are\n",
    "available](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions). You may\n",
    "not use a Multi-Regional Storage bucket for training with Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "59ebce2a16fa"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "REGION = \"[your-region]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "d0caa7b3c8b8"
   },
   "outputs": [],
   "source": [
    "if BUCKET_URI == \"\" or BUCKET_URI is None or BUCKET_URI == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_URI = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP\n",
    "    \n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a7c87b6f171"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76f7e3e043f5",
    "outputId": "f8df1ba6-e0a2-4833-e547-415e14eb0e3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://vertex-ai-devaip-20220504053744/...\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4353e94da5f"
   },
   "source": [
    "**Finally**, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6f092d6ebc92"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmnMD2MjkXUJ"
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "oqtZRqDEkXUJ"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9abd63bb1a85"
   },
   "source": [
    "## Tutorial\n",
    "\n",
    "### Fetch the data from BigQuery \n",
    "<a name=\"section-5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using Vertex AI Workbench managed notebooks instance, run below cell else skip it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4a25861cf833"
   },
   "source": [
    "#@bigquery\n",
    "\n",
    "WITH\n",
    "  traindata AS (\n",
    "  SELECT\n",
    "    b.* EXCEPT(ad_event_id,\n",
    "      user_id),\n",
    "    c.* EXCEPT(id),\n",
    "    d.* EXCEPT(keyword_id,\n",
    "      ad_id),\n",
    "    a.amount,\n",
    "    a.device_type,\n",
    "    e.name\n",
    "  FROM\n",
    "    `looker-private-demo.ecomm.ad_events` a\n",
    "  JOIN (\n",
    "    SELECT\n",
    "      ad_event_id,\n",
    "      user_id,\n",
    "      state,\n",
    "      os,\n",
    "      browser\n",
    "    FROM\n",
    "      `looker-private-demo.ecomm.events`\n",
    "    WHERE\n",
    "      event_type=\"Purchase\"\n",
    "      AND country=\"USA\") b\n",
    "  ON\n",
    "    a.id = b.ad_event_id\n",
    "  JOIN (\n",
    "    SELECT\n",
    "      id,\n",
    "      gender,\n",
    "      age\n",
    "    FROM\n",
    "      `looker-private-demo.ecomm.users`) c\n",
    "  ON\n",
    "    b.user_id = c.id\n",
    "  JOIN (\n",
    "    SELECT\n",
    "      keyword_id,\n",
    "      ad_id,\n",
    "      cpc_bid_amount,\n",
    "      bidding_strategy_type,\n",
    "      quality_score,\n",
    "      keyword_match_type\n",
    "    FROM\n",
    "      `looker-private-demo.ecomm.keywords`\n",
    "    WHERE\n",
    "      cpc_bid_amount <= 3000) d\n",
    "  ON\n",
    "    a.keyword_id = d.keyword_id\n",
    "  JOIN (\n",
    "    SELECT\n",
    "      ad_id,\n",
    "      name\n",
    "    FROM\n",
    "      `looker-private-demo.ecomm.ad_groups`) e\n",
    "  ON\n",
    "    d.ad_id = e.ad_id )\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  traindata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "923fdd823683"
   },
   "source": [
    "If you are using Vertex AI Workbench managed notebooks instance, once the results from BigQuery are displayed in the above cell, click the **Query and load as DataFrame** button and execute the generated code stub to fetch the data into the current notebook as a dataframe.\n",
    "\n",
    "*Note: By default the data is loaded into a `df` variable, though this can be changed before executing the cell if required.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "f8b5112f231f"
   },
   "outputs": [],
   "source": [
    "# The following two lines are only necessary to run once.\n",
    "# Comment out otherwise for speed-up.\n",
    "from google.cloud.bigquery import Client\n",
    "\n",
    "client = Client(project=PROJECT_ID)\n",
    "\n",
    "query = \"\"\"WITH traindata AS (\n",
    "SELECT b.* except(ad_event_id, user_id), c.* except(id), d.* except(keyword_id, ad_id), a.amount, a.device_type, e.name\n",
    "FROM `looker-private-demo.ecomm.ad_events` a\n",
    "JOIN\n",
    "(SELECT ad_event_id, user_id, state, os, browser from `looker-private-demo.ecomm.events` WHERE event_type=\"Purchase\" AND country=\"USA\") b\n",
    "ON a.id = b.ad_event_id\n",
    "JOIN\n",
    "(SELECT id, gender, age FROM `looker-private-demo.ecomm.users`) c\n",
    "ON b.user_id = c.id\n",
    "JOIN\n",
    "(SELECT keyword_id, ad_id, cpc_bid_amount, bidding_strategy_type, quality_score, keyword_match_type FROM `looker-private-demo.ecomm.keywords`\n",
    "WHERE cpc_bid_amount <= 3000) d\n",
    "ON a.keyword_id = d.keyword_id\n",
    "JOIN\n",
    "(SELECT ad_id, name FROM `looker-private-demo.ecomm.ad_groups`) e\n",
    "ON d.ad_id = e.ad_id\n",
    ")\n",
    "SELECT * FROM traindata\"\"\"\n",
    "job = client.query(query)\n",
    "df = job.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7f69bea65019"
   },
   "source": [
    "### Preprocess the data\n",
    "<a name=\"section-6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e48d156d8bb6"
   },
   "source": [
    "Select the necessary columns from the E-commerce data and divide them based on their type (numerical/categorical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7cea2c44c50b"
   },
   "outputs": [],
   "source": [
    "target = \"name\"\n",
    "categ_cols = [\n",
    "    \"state\",\n",
    "    \"os\",\n",
    "    \"browser\",\n",
    "    \"gender\",\n",
    "    \"bidding_strategy_type\",\n",
    "    \"keyword_match_type\",\n",
    "    \"device_type\",\n",
    "]\n",
    "num_cols = [\"age\", \"cpc_bid_amount\", \"quality_score\", \"amount\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ace612851261"
   },
   "source": [
    "From the current dataset, only the top three camapigns will be chosen to target the users. All the relevant information about the advertisement and the user who purchased an item after seeing the advertisement is available in the dataframe already. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "7282fbab4586"
   },
   "outputs": [],
   "source": [
    "df = df[df[\"name\"].isin([\"Tops & Tees\", \"Active\", \"Accessories\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f89106348ffe"
   },
   "source": [
    "Encode the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "d6cd256f455a"
   },
   "outputs": [],
   "source": [
    "df[\"name\"] = df[\"name\"].map({\"Tops & Tees\": 0, \"Active\": 1, \"Accessories\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8902f763d1ca"
   },
   "source": [
    "One-hot encode the categorical variables. After one-hot encoding, the first level-column is dropped to avoid the [dummy-variable trap](https://en.wikipedia.org/wiki/Dummy_variable_(statistics)) scenario. This process is called *dummy-encoding*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d57706df2441",
    "outputId": "e9bd72a1-a1cb-442b-887b-33b85ace7b9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6165, 71)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_cols(data, col):\n",
    "    # Creating a dummy variable for the variable 'CategoryID' and dropping the first one.\n",
    "    categ = pd.get_dummies(data[col], prefix=col, drop_first=True)\n",
    "    # Adding the results to the master dataframe\n",
    "    data = pd.concat([data, categ], axis=1)\n",
    "    return data\n",
    "\n",
    "\n",
    "# dummy-encode the categorical fields\n",
    "for i in categ_cols:\n",
    "    df = encode_cols(df, i)\n",
    "    df.drop(columns=[i], inplace=True)\n",
    "\n",
    "# check the data's shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3abf027eda2d"
   },
   "source": [
    "Split the data into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0072d44b6163",
    "outputId": "ecbe5270-8575-4e48-f136-d8e62ab948a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4932, 70) (1233, 70)\n"
     ]
    }
   ],
   "source": [
    "X = df[[i for i in df.columns if i != target]].copy()\n",
    "y = df[target].copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.8, random_state=36\n",
    ")\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1a32b9d9640"
   },
   "source": [
    "Scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "9620e04d8db2"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train.loc[:, num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "X_test.loc[:, num_cols] = scaler.transform(X_test[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5b9f3ca04f91"
   },
   "source": [
    "### Train a TensorFlow model\n",
    "<a name=\"section-7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3e7656556a48"
   },
   "source": [
    "Convert the target column to a categorical encoded colum (one-hot encoded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ebc87650ae13"
   },
   "outputs": [],
   "source": [
    "y_train_categ = to_categorical(y_train)\n",
    "y_test_categ = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dd0014a7e1d"
   },
   "source": [
    "Define hyperparameters for model training. \n",
    "\n",
    "*Note: Comment or remove the parameters from the following cell if they are provided already as an input parameter through the executor feature.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ec020b36af20"
   },
   "outputs": [],
   "source": [
    "optimizer = \"sgd\"\n",
    "num_hidden_layers = 3\n",
    "num_neurons = [64, 128, 256]\n",
    "activ_func = [\"relu\", \"relu\", \"relu\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "406b731f576b"
   },
   "source": [
    "Define the architecture and compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "57839a187cf0",
    "outputId": "186da936-e007-4072-d9e3-e037535d9601"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                4544      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 771       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,659\n",
      "Trainable params: 46,659\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# construct the neural network as per the defined parameters\n",
    "for i in range(num_hidden_layers):\n",
    "    if i == 0:\n",
    "        # add the input layer\n",
    "        model.add(\n",
    "            Dense(\n",
    "                num_neurons[i],\n",
    "                activation=activ_func[i],\n",
    "                input_shape=(X_train.shape[1],),\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        # add the hidden layers\n",
    "        model.add(Dense(num_neurons[i], activation=activ_func[i]))\n",
    "\n",
    "# add the output layer\n",
    "model.add(Dense(3, activation=\"softmax\"))\n",
    "# compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ab12c34f258"
   },
   "source": [
    "Fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9321005e55ae",
    "outputId": "beade145-6920-4164-d0c3-ac5df96b5733"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "155/155 [==============================] - 1s 2ms/step - loss: 1.0938\n",
      "Epoch 2/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0909\n",
      "Epoch 3/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0888\n",
      "Epoch 4/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0869\n",
      "Epoch 5/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0854\n",
      "Epoch 6/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0836\n",
      "Epoch 7/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0820\n",
      "Epoch 8/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0809\n",
      "Epoch 9/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0794\n",
      "Epoch 10/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0780\n",
      "Epoch 11/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0765\n",
      "Epoch 12/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0754\n",
      "Epoch 13/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0739\n",
      "Epoch 14/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0724\n",
      "Epoch 15/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0710\n",
      "Epoch 16/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0696\n",
      "Epoch 17/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0683\n",
      "Epoch 18/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0668\n",
      "Epoch 19/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0650\n",
      "Epoch 20/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0638\n",
      "Epoch 21/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0620\n",
      "Epoch 22/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0608\n",
      "Epoch 23/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0591\n",
      "Epoch 24/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0576\n",
      "Epoch 25/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0561\n",
      "Epoch 26/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0538\n",
      "Epoch 27/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0529\n",
      "Epoch 28/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0512\n",
      "Epoch 29/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0496\n",
      "Epoch 30/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0481\n",
      "Epoch 31/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0466\n",
      "Epoch 32/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0451\n",
      "Epoch 33/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0437\n",
      "Epoch 34/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0422\n",
      "Epoch 35/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0406\n",
      "Epoch 36/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0396\n",
      "Epoch 37/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0374\n",
      "Epoch 38/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0367\n",
      "Epoch 39/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0346\n",
      "Epoch 40/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0337\n",
      "Epoch 41/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0312\n",
      "Epoch 42/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0300\n",
      "Epoch 43/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0293\n",
      "Epoch 44/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0270\n",
      "Epoch 45/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0258\n",
      "Epoch 46/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0248\n",
      "Epoch 47/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0230\n",
      "Epoch 48/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0211\n",
      "Epoch 49/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0195\n",
      "Epoch 50/50\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 1.0179\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train_categ, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51a2d0b52df3"
   },
   "source": [
    "### Run the model on test data\n",
    "<a name=\"section-8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f08445f2cd02"
   },
   "source": [
    "Evaluate the model on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "599df6d2b9a4",
    "outputId": "9df1abe9-4f28-48c7-f77c-9a7b4bc0da9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 0s 1ms/step - loss: 1.0649\n",
      "Test results - Loss: 1.0648765563964844\n"
     ]
    }
   ],
   "source": [
    "test_results = model.evaluate(X_test, y_test_categ, verbose=1)\n",
    "print(f\"Test results - Loss: {test_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please note that executor feature is available only in Vertex AI Workbench managed notebooks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9769168778e8"
   },
   "source": [
    "### Automating the execution of the notebook using executor in Vertex AI Workbench managed notebooks instance\n",
    "<a name=\"section-9\"></a>\n",
    "\n",
    "If you are using Vertex AI Workbench managed notebooks instance, the executor can help you run a notebook file from start to end, with your choice of the environment, machine type, input parameters, and other characteristics. After setting up an execution, the notebook is executed as a job in Vertex AI custom training. Your jobs can be monitored from the <b>Notebook Executor</b> pane in the menu on the left.\n",
    "\n",
    "<img src=\"images/executor.png\"></img>\n",
    "\n",
    "Executor lets you choose the environment and machine type while automating the runs similar to Vertex AI training jobs without switching to the training jobs UI. Apart from the custom container that replicates the existing kernel by default, pre-built environments like TensorFlow Enterprise, PyTorch, and others can also be selected to run the notebook. Furthermore the required compute power can be specified by choosing from the list of machine types available, including GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cf486c351581"
   },
   "source": [
    "### Scheduled runs on executor in Vertex AI Workbench managed notebooks instance\n",
    "<a name=\"section-10\"></a>\n",
    "\n",
    "Vertex AI Workbench managed noteboook runs can also be scheduled recurringly with the executor. To do so, select <b>Schedule-based recurring executions</b> as the run type instead of <b>One-time execution</b>. The frequency of the job and the time when it executes is provided when you create the execution.\n",
    "\n",
    "<img src=\"images/executor_scheduled_runs2.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6561007ac7f2"
   },
   "source": [
    "### Parameterizing the variables\n",
    "<a name=\"section-11\"></a>\n",
    "\n",
    "If you are using Vertex AI Workbench managed notebooks instance, executor lets you run a notebook with different sets of input parameters. If required, constants in the notebook can be treated as arguments to a function, and when you submit the execution, you can provide those constants as input parameters.\n",
    "\n",
    "<img src=\"images/executor_input_parameters.png\"></img>\n",
    "\n",
    "The hyperparameters defined during the model training step can be passed as arguments while submitting an execution. However, the values defined in the notebook itself should be removed or commented out before submitting the execution. Otherwise, the input parameters would just be overwritten by the values in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c81db97fa3e8"
   },
   "source": [
    "### Save the model to a Cloud Storage path\n",
    "<a name=\"section-12\"></a>\n",
    "\n",
    "TensorFlow's `model.save()` method supports Cloud Storage paths as well as the local file paths while writing the model object to a file. It needs to be ensured that the service account being used to run this notebook has `write` permissions to the specified Cloud Storage path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2dcefeb6a2d8",
    "outputId": "d5fc3bca-e3a1-4aad-8c7c-a98727e93ae3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs://vertex-ai-devaip-20220504053744/ui/assets\n"
     ]
    }
   ],
   "source": [
    "GCS_PATH = BUCKET_URI + \"/path-to-save/\"\n",
    "model.save(GCS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29c0ca2a517a"
   },
   "source": [
    "## Clean up\n",
    "<a name=\"section-13\"></a>\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ab69210d5a8",
    "outputId": "743f82f0-2150-4b8a-f888-457f3d6ac06d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing gs://vertex-ai-devaip-20220504053744/ui/#1651643304629237...\n",
      "Removing gs://vertex-ai-devaip-20220504053744/ui/assets/#1651643308972193...\n",
      "Removing gs://vertex-ai-devaip-20220504053744/ui/keras_metadata.pb#1651643309863382...\n",
      "Removing gs://vertex-ai-devaip-20220504053744/ui/saved_model.pb#1651643309517008...\n",
      "Removing gs://vertex-ai-devaip-20220504053744/ui/variables/#1651643305070689...\n",
      "Removing gs://vertex-ai-devaip-20220504053744/ui/variables/variables.data-00000-of-00001#1651643307554278...\n",
      "Removing gs://vertex-ai-devaip-20220504053744/ui/variables/variables.index#1651643307896371...\n",
      "/ [7/7 objects] 100% Done                                                       \n",
      "Operation completed over 7 objects.                                              \n",
      "Removing gs://vertex-ai-devaip-20220504053744/...\n"
     ]
    }
   ],
   "source": [
    "#Delete the Cloud Storage bucket\n",
    "\n",
    "delete_bucket = True\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "new_training-multi-class-classification-model-for-ads-targeting-usecase (1).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m89",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m89"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

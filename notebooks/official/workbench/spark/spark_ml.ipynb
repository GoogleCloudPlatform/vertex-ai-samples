{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-managed-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/workbench/spark/spark_ml.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>                                                                                               \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook tutorial shows you Apache SparkML jobs with Dataproc and BigQuery. Through this notebook, you can learn a common use case in the machine learning pipeline: ingestion, data cleaning, feature engineering, modeling, and evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The two datasets, [NYC TLC (Taxi and Limousine Commission) Trips](https://console.cloud.google.com/marketplace/product/city-of-new-york/nyc-tlc-trips) dataset and [NYC Citi Bike Trips](https://console.cloud.google.com/marketplace/product/city-of-new-york/nyc-citi-bike) dataset, are available in [BigQuery Public Datasets](https://cloud.google.com/bigquery/public-data), and provides free querying of up to 1TB of data each month. It contains trips data for each Taxi and Citi Bike, the public bicycle sharing system serving New York City."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "### Objective\n",
        "\n",
        "This notebook tutorial runs an Apache SparkML job that fetches data from the BigQuery dataset, performs exploratory data analysis, cleans data, executes feature engineering, trains the model, evaluates the model, debriefs for the result and saves the model to a Cloud Storage bucket.\n",
        "\n",
        "This notebook tutorial performs the following steps:\n",
        "\n",
        "- Setting up a Google Cloud project and Dataproc cluster.\n",
        "- Creating a Cloud Storage bucket and a BigQuery dataset.\n",
        "- Configuring the spark-bigquery-connector.\n",
        "- Ingesting data from BigQuery into a Spark DataFrame.\n",
        "- Performing Exploratory Data Analysis (EDA).\n",
        "- Visualizing data with samples.\n",
        "- Cleaning data.\n",
        "- Selecting features.\n",
        "- Training the model.\n",
        "- Debriefing the result\n",
        "- Saving the model to a Cloud Storage bucket.\n",
        "- Deleting the resources created for this notebook tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "### Costs \n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* [Vertex AI](https://cloud.google.com/vertex-ai/pricing)\n",
        "* [Cloud Storage](https://cloud.google.com/storage/pricing)\n",
        "* [Dataproc](https://cloud.google.com/dataproc/pricing)\n",
        "* [BigQuery](https://cloud.google.com/bigquery/pricing)\n",
        "\n",
        "You can use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### Set up your Google Cloud project:\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you create an account, you receive a $300 credit towards to your compute and storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "1. [Enable the BigQuery API, Notebooks API, Vertex AI API, and Dataproc API](https://console.cloud.google.com/flows/enableapi?apiid=bigquery.googleapis.com,notebooks.googleapis.com,aiplatform.googleapis.com,dataproc&_ga=2.209429842.1903825585.1657549521-326108178.1655322249)\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and inserts the value of Python variables prefixed with `$` into the commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "### Installation\n",
        "\n",
        "Install the following packages to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "172533a994ad"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "USER_FLAG = \"\"\n",
        "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
        "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5c0183fba17"
      },
      "outputs": [],
      "source": [
        "if os.getenv(\"IS_TESTING\"):\n",
        "    \"\"\"\n",
        "    The testing suite does not currently support testing on Dataproc clusters,\n",
        "    so the testing environment is setup to replicate Dataproc via the following steps.\n",
        "    \"\"\"\n",
        "    JAVA_VER = \"8u332-b09\"\n",
        "    JAVA_FOLDER = \"/tmp/java\"\n",
        "    FILE_NAME = f\"openlogic-openjdk-{JAVA_VER}-linux-x64\"\n",
        "    TAR_FILE = f\"{JAVA_FOLDER}/{FILE_NAME}.tar.gz\"\n",
        "    DOWNLOAD_LINK = f\"https://builds.openlogic.com/downloadJDK/openlogic-openjdk/{JAVA_VER}/openlogic-openjdk-{JAVA_VER}-linux-x64.tar.gz\"\n",
        "    PYSPARK_VER = \"3.1.3\"\n",
        "\n",
        "    # Download Open JDK 8. Spark requires Java to execute.\n",
        "    ! rm -rf $JAVA_FOLDER\n",
        "    ! mkdir $JAVA_FOLDER\n",
        "    ! wget -P $JAVA_FOLDER $DOWNLOAD_LINK\n",
        "    os.environ[\"JAVA_HOME\"] = f\"{JAVA_FOLDER}/{FILE_NAME}\"\n",
        "    ! tar -zxf $TAR_FILE -C $JAVA_FOLDER\n",
        "    ! echo $JAVA_HOME\n",
        "\n",
        "    # Pin the Spark version to match that the Dataproc 2.0 cluster.\n",
        "    ! pip install {USER_FLAG} pyspark==$PYSPARK_VER -q\n",
        "\n",
        "    # Install GeoPandas.\n",
        "    ! pip install {USER_FLAG} geopandas -q\n",
        "\n",
        "    # Install PyArrow.\n",
        "    ! pip install {USER_FLAG} pyarrow -q\n",
        "\n",
        "    # Install rtree.\n",
        "    ! pip install {USER_FLAG} rtree -q\n",
        "\n",
        "    # Install seaborn.\n",
        "    ! pip install {USER_FLAG} seaborn -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "### Create a Dataproc cluster\n",
        "\n",
        "The Spark job executed in this notebook tutorial is compute intensive. Since the job can take a significant amount time to complete in a standard notebook environment, this notebook tutorial runs on a Dataproc cluster that is created with the Dataproc Component Gateway and Jupyter component installed on the cluster.\n",
        "\n",
        "**Existing Dataproc with Jupyter cluster?**: If you have a running Dataproc cluster that has the [Component Gateway and Jupyter component installed on the cluster](https://cloud.google.com/dataproc/docs/concepts/components/jupyter#gcloud-command)), you can use it in this tutorial. If you plan to use it, skip this step, and go to `Switch your kernel`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4e3cf9e61673"
      },
      "outputs": [],
      "source": [
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    CLUSTER_NAME = \"[your-cluster]\"  # @param {type: \"string\"}\n",
        "    CLUSTER_REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
        "\n",
        "    if CLUSTER_REGION == \"[your-region]\":\n",
        "        CLUSTER_REGION = \"us-central1\"\n",
        "\n",
        "    print(f\"CLUSTER_NAME: {CLUSTER_NAME}\")\n",
        "    print(f\"CLUSTER_REGION: {CLUSTER_REGION}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "Your `CLUSTER_NAME` must be **unique within your Google Cloud project**. It must start with a lowercase letter, followed by up to 51 lowercase letters, numbers, and hyphens, and cannot end with a hyphen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff30d97bf211"
      },
      "outputs": [],
      "source": [
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    !gcloud dataproc clusters create $CLUSTER_NAME \\\n",
        "        --region=$CLUSTER_REGION \\\n",
        "        --enable-component-gateway \\\n",
        "        --image-version=2.0 \\\n",
        "        --optional-components=JUPYTER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "#### Switch your kernel\n",
        "\n",
        "Your notebook kernel is listed at the top of the notebook page. Your notebook should run on the Python 3 kernel running on your Dataproc cluster.\n",
        "\n",
        "Select **Kernel > Change Kernel** from the top menu, then select `Python 3 on CLUSTER_NAME: Dataproc cluster in REGION (Remote)`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "### Set your project ID\n",
        "\n",
        "Run the following cell to get your project ID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c7fe49d5dac"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"\"\n",
        "\n",
        "# Get your Google Cloud project ID from gcloud\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID: \", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "If the previous command has no output, copy your project ID from the project selector in the [Google Cloud console](https://console.cloud.google.com/). Insert the ID in the `[your-project-id]` placeholder, then run the following command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "282bc9bcbffe"
      },
      "outputs": [],
      "source": [
        "if not PROJECT_ID or PROJECT_ID == \"\":\n",
        "    PROJECT_ID = \"[your-project-id]\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eef423a7e5f"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project $PROJECT_ID -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "The machine learning model created in this tutorial is stored in Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "#### Region\n",
        "\n",
        "Before creating a Cloud Storage bucket, re-define the `REGION` variable (when you changed the notebook kernel earlier, previously set variables were deleted)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f1992d08421"
      },
      "outputs": [],
      "source": [
        "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
        "\n",
        "# If you do not specify a region, it will be \"us-central1\".\n",
        "if not REGION or REGION == \"\" or REGION == \"[your-region]\":\n",
        "    REGION = \"us-central1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "#### UUID\n",
        "\n",
        "To avoid name collisions, you can create a UUID for the current notebook session, then append the UUID to the name of resources such as the Cloud Storage bucket and BigQuery dataset that you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6988e7272299"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "\n",
        "# Generate a uuid of a specifed length(default=8)\n",
        "def generate_uuid(length: int = 8) -> str:\n",
        "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
        "\n",
        "\n",
        "UUID = generate_uuid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "Replace the `[your-bucket-name]` placeholder with the name of your Cloud Storage bucket. The name must be unique across all Cloud Storage buckets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78c2bc5c2a35"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}/\"\n",
        "\n",
        "# If you do not specify a bucket name, it will be created based on your project ID and the UUID.\n",
        "if not BUCKET_NAME or BUCKET_NAME == \"\" or BUCKET_NAME == \"[your-bucket-name]\":\n",
        "    BUCKET_NAME = f\"{PROJECT_ID}{UUID}\"\n",
        "    BUCKET_URI = f\"gs://{BUCKET_NAME}/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17be4f420fd2"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "Confirm your access to the Cloud Storage bucket by displaying the bucket's metadata:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad8f9c4923ed"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -L -b $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "### Create a Bigquery Dataset\n",
        "\n",
        "In order to use a query from BigQuery dataset, creating a temporary table is necessary to save the result, of which Spark will read the results from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad248f804f59"
      },
      "outputs": [],
      "source": [
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    DATASET_NAME = \"[your-dataset-name]\"  # @param {type:\"string\"}\n",
        "\n",
        "    if (\n",
        "        DATASET_NAME == \"\"\n",
        "        or DATASET_NAME is None\n",
        "        or DATASET_NAME == \"[your-dataset-name]\"\n",
        "    ):\n",
        "        DATASET_NAME = f\"{PROJECT_ID}{UUID}\"\n",
        "else:\n",
        "    DATASET_NAME = f\"python_docs_samples_tests_{UUID}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0effcc7558a8"
      },
      "outputs": [],
      "source": [
        "! bq mk $DATASET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "## Tutorial\n",
        "\n",
        "### Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRUOFELefqf1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from geopandas import gpd\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import GBTRegressor\n",
        "# A Spark Session is how you interact with Spark SQL to create Dataframes\n",
        "from pyspark.sql import SparkSession\n",
        "# PySpark functions\n",
        "from pyspark.sql.functions import (col, floor, pandas_udf, to_timestamp, udf,\n",
        "                                   unix_timestamp)\n",
        "# These allow us to create a schema for our data\n",
        "from pyspark.sql.types import BooleanType, DoubleType\n",
        "from shapely.geometry import Point"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "source": [
        "### Initialize the SparkSession\n",
        "\n",
        "To use Apache Spark with BigQuery, you must include the [spark-bigquery-connector](https://github.com/GoogleCloudDataproc/spark-bigquery-connector) when you initialize the SparkSession."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "outputs": [],
      "source": [
        "VER = \"0.26.0\"\n",
        "FILE_NAME = f\"spark-bigquery-with-dependencies_2.12-{VER}.jar\"\n",
        "\n",
        "if os.getenv(\"IS_TESTING\"):\n",
        "    connector = f\"https://github.com/GoogleCloudDataproc/spark-bigquery-connector/releases/download/{VER}/{FILE_NAME}\"\n",
        "else:\n",
        "    connector = f\"gs://spark-lib/bigquery/{FILE_NAME}\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Initialize the \"SparkSession\" with the following config. \n",
        "\"viewsEnabled\" and \"materializationDataset\" are required to enable to query data in spark-bigquery-connector.\n",
        "For more information, check https://github.com/GoogleCloudDataproc/spark-bigquery-connector#reading-data-from-a-bigquery-query.\n",
        "\"\"\"\n",
        "spark = (\n",
        "    SparkSession.builder.appName(\"spark-bigquery-polyglot-language-demo\")\n",
        "    .config(\"spark.jars\", connector)\n",
        "    .config(\"viewsEnabled\", \"true\")\n",
        "    .config(\"materializationDataset\", f\"{DATASET_NAME}\")\n",
        "    .getOrCreate()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "source": [
        "### Fetch data from BigQuery\n",
        "\n",
        "In this tutorial, NYC Taxi dataset is in 2016 and NYC Citi Bike dataset is in between 2013 and 2018. Since their time range are different, use SQL query to match both datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "outputs": [],
      "source": [
        "# Load NYC_taxi in Github Activity Public Dataset from BigQuery.\n",
        "taxi_df = (\n",
        "    spark.read.format(\"bigquery\")\n",
        "    .option(\"table\", \"bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2016\")\n",
        "    .load()\n",
        ")\n",
        "\n",
        "bike_sql = \"\"\"\n",
        "    SELECT * \n",
        "    FROM bigquery-public-data.new_york_citibike.citibike_trips \n",
        "    WHERE EXTRACT(YEAR FROM starttime)=2016 and EXTRACT(YEAR FROM stoptime)=2016\n",
        "\"\"\"\n",
        "\n",
        "# Load NYC_Citibike in Github Acitivity Public dataset from BQ.\n",
        "bike_df = spark.read.format(\"bigquery\").load(bike_sql)\n",
        "\n",
        "\"\"\"\n",
        "Although this is not a standard practice in a production environment, only 10% of the data for each dataset will be used in this project since it consumes too much time and computing resources. You can try different values for yourself but the runtime of the notebook linearly scales with the size of data.\n",
        "\"\"\"\n",
        "taxi_df = taxi_df.sample(0.1)\n",
        "bike_df = bike_df.sample(0.1)\n",
        "\n",
        "if os.getenv(\"IS_TESTING\"):\n",
        "    taxi_df = taxi_df.sample(0.0001)\n",
        "    bike_df = bike_df.sample(0.0001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "### Perform Exploratory Data Analysis (EDA)\n",
        "\n",
        "When you get started with a new problem, the first step is to gain an understanding of what the dataset contains. EDA is used to derive insights from the data. Data scientists and analysts try to find different patterns, relations, and anomalies in the data using some statistical graphs and other visualization techniques. It allows analysts to understand the data better before making any assumptions.\n",
        "\n",
        "Check the data types for Taxi dataset first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adeaf84f4054"
      },
      "outputs": [],
      "source": [
        "taxi_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "Filter out unnecessary columns and check null counts of the fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5072e605966"
      },
      "outputs": [],
      "source": [
        "taxi_df = taxi_df.select(\n",
        "    \"pickup_datetime\",\n",
        "    \"dropoff_datetime\",\n",
        "    \"trip_distance\",\n",
        "    \"pickup_longitude\",\n",
        "    \"pickup_latitude\",\n",
        "    \"dropoff_longitude\",\n",
        "    \"dropoff_latitude\",\n",
        "    \"fare_amount\",\n",
        ")\n",
        "\n",
        "taxi_df.describe().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "From this summary, you are able to know a lot of information.\n",
        "  * There are over 13 millions of trip history for Yellow Taxi in 2016. but it has some abnormal values such as null and negative values in it. For instance, non-null value of longitudes and latitudes are only about the half of the total rows.\n",
        "  * `pickup_datetime` and `dropoff_datetime` are string format when it was converted from the BigQuery dataset. To use it effectively, it needs to be re-formatted.\n",
        "  * In 2016, the exact latitude and longitude were used for the pickup and the dropoff locations. It raised a lot of [privacy concerns](https://agkn.wordpress.com/2014/09/15/riding-with-the-stars-passenger-privacy-in-the-nyc-taxicab-dataset/) and the dataset has been providing `pickup_location_id` and `dropoff_location_id` instead. This id is corresponded to the [NYC Taxi Zones](https://data.cityofnewyork.us/Transportation/NYC-Taxi-Zones/d3c5-ddgc), roughly based on NYC Department of City Planningâ€™s Neighborhood Tabulation Areas (NTAs) and are meant to approximate neighborhoods.\n",
        "  \n",
        "Based on numbers shown above, filter out some unrealistic values.\n",
        "  * Every row contains null values must be removed.\n",
        "  * trip_distance must be in between 0 and 20 (miles) since the length of Manhattan is about 13.4 miles long.\n",
        "  * pickup_latitude and dropoff_latitude must be in between 40 and 41\n",
        "  * pickup_longitude and dropoff_longitude must be in between -75 to -73\n",
        "  * fare_amount must be in between 0 and 500 (dollars) since it does not include taxes and tips."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38aa94651f34"
      },
      "outputs": [],
      "source": [
        "taxi_df = taxi_df.where(\n",
        "    (col(\"trip_distance\") > 0)\n",
        "    & (col(\"trip_distance\") < 20)\n",
        "    & (col(\"fare_amount\") > 0)\n",
        "    & (col(\"fare_amount\") < 500)\n",
        "    & (col(\"pickup_latitude\") > 40)\n",
        "    & (col(\"pickup_latitude\") < 41)\n",
        "    & (col(\"dropoff_latitude\") > 40)\n",
        "    & (col(\"dropoff_latitude\") < 41)\n",
        "    & (col(\"pickup_longitude\") > -75)\n",
        "    & (col(\"pickup_longitude\") < -73)\n",
        "    & (col(\"dropoff_longitude\") > -75)\n",
        "    & (col(\"dropoff_longitude\") < -73)\n",
        ").dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "### Perform Feature Engineering\n",
        "\n",
        "To convert the location to the NYC Taxi Zones, Pandas UDF, as known as Vectorized UDF feature is used. For more information, [check this link](https://www.databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html).\n",
        "\n",
        "It converts the given coordinates into the Taxi Zone in NYC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2be027cd9619"
      },
      "outputs": [],
      "source": [
        "# Load the GeoJSON format of NYC Taxi zones.\n",
        "gdf_zone = gpd.read_file(\n",
        "    \"https://data.cityofnewyork.us/api/geospatial/d3c5-ddgc?method=export&format=GeoJSON\"\n",
        ")\n",
        "\n",
        "# Convert the type of \"location_id\" column.\n",
        "gdf_zone[\"location_id\"] = gdf_zone[\"location_id\"].astype(\"long\")\n",
        "\n",
        "# Convert GeoPandas format to a dictionary.\n",
        "hm = gdf_zone.to_dict(\"index\")\n",
        "\n",
        "# Create a set that contains location_id if the location is in Manhattan.\n",
        "manhattan_set = {hm[i][\"location_id\"] for i in hm if hm[i][\"borough\"] == \"Manhattan\"}\n",
        "\n",
        "\n",
        "@pandas_udf(\"long\")\n",
        "def preprocess_zone_id(lat: pd.Series, lon: pd.Series) -> pd.Series:\n",
        "    \"\"\"\n",
        "    The preprocessing function takes latitudes and longitudes and returns the corresponding Taxi Zone id.\n",
        "    Args:\n",
        "        lat: The latitude of the position.\n",
        "        lon: The longitude of the position.\n",
        "    Returns:\n",
        "        The Taxi Zone id.\n",
        "    \"\"\"\n",
        "    point_var = [Point(xy) for xy in zip(lon, lat)]\n",
        "    gdf_points = gpd.GeoDataFrame(\n",
        "        pd.DataFrame({\"lat\": lat, \"lon\": lon}), crs=\"epsg:4326\", geometry=point_var\n",
        "    )\n",
        "    gdf_joined = gpd.sjoin(gdf_points, gdf_zone, how=\"left\")\n",
        "    return gdf_joined[\"location_id\"]\n",
        "\n",
        "\n",
        "@udf(returnType=BooleanType())\n",
        "def is_in_manhattan(location_id):\n",
        "    \"\"\"\n",
        "    The preprocessing function takes location_id and returns whether the given location_id is in the manhattan_set.\n",
        "    Args:\n",
        "        location_id: A number from 0 to 263 that represents NYC Taxi Zone.\n",
        "    Returns:\n",
        "        A Boolean value of whether the given location_id is in the manhattan_set.\n",
        "    \"\"\"\n",
        "    return location_id in manhattan_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10215f2acf7b"
      },
      "outputs": [],
      "source": [
        "# Add \"start_zone_id\" and \"end_zone_id\" by applying the UDF \"preprocess_zone_id\".\n",
        "taxi_df = taxi_df.withColumn(\n",
        "    \"start_zone_id\",\n",
        "    preprocess_zone_id(taxi_df[\"pickup_latitude\"], taxi_df[\"pickup_longitude\"]),\n",
        ")\n",
        "taxi_df = taxi_df.withColumn(\n",
        "    \"end_zone_id\",\n",
        "    preprocess_zone_id(taxi_df[\"dropoff_latitude\"], taxi_df[\"dropoff_longitude\"]),\n",
        ")\n",
        "\n",
        "# Add \"is_start_manhattan\" and \"is_end_manhattan\" by applying the UDF \"is_in_manhattan\".\n",
        "taxi_df = taxi_df.withColumn(\n",
        "    \"is_start_manhattan\", is_in_manhattan(col(\"start_zone_id\"))\n",
        ")\n",
        "taxi_df = taxi_df.withColumn(\"is_end_manhattan\", is_in_manhattan(col(\"end_zone_id\")))\n",
        "\n",
        "# Trips must done within the Manhattan and between different taxi zones.\n",
        "taxi_df = taxi_df.where(\n",
        "    (col(\"start_zone_id\") != col(\"end_zone_id\"))\n",
        "    & (col(\"is_start_manhattan\"))\n",
        "    & (col(\"is_end_manhattan\"))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "Use of `to_timestamp()` function and `unix_timestamp()` function to convert `pickup_datetime` and `dropoff_datetime` as a Unix Timestamp type.\n",
        "\n",
        "Unix time is a way of representing time as the number of seconds since `January 1st, 1970 at 00:00:00 UTC`. Compared to the Timestamp type, Unix time can be represented as an integer, making it easier to parse and use across different systems.\n",
        "\n",
        "After you get `start_time` and `end_time` by converting the original `pickup_datetime` and `dropoff_datetime`, you are able to get more insteresting columns manipulating these two Timestamps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8be30eddd676"
      },
      "outputs": [],
      "source": [
        "# Convert the type of datetime from a string to a Unix timestamp.\n",
        "taxi_df = taxi_df.withColumn(\n",
        "    \"start_time\", unix_timestamp(to_timestamp(col(\"pickup_datetime\")))\n",
        ")\n",
        "taxi_df = taxi_df.withColumn(\n",
        "    \"end_time\", unix_timestamp(to_timestamp(col(\"dropoff_datetime\")))\n",
        ")\n",
        "\n",
        "# Calculate if this is a weekday\n",
        "taxi_df = taxi_df.withColumn(\n",
        "    \"is_weekday\",\n",
        "    ((floor(col(\"start_time\") / 86400) + 4) % 7 > 0)\n",
        "    & ((floor(col(\"start_time\") / 86400) + 4) % 7 < 6),\n",
        ")\n",
        "\n",
        "# Convert start_time to start_time_in_minute.\n",
        "taxi_df = taxi_df.withColumn(\n",
        "    \"start_time_in_minute\", floor((col(\"start_time\") % 86400) / 60)\n",
        ")\n",
        "\n",
        "# Calculate trip_duration.\n",
        "taxi_df = taxi_df.withColumn(\"trip_duration\", col(\"end_time\") - col(\"start_time\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "Prior to visualizing this modified Taxi dataset, do similar feature engineering for the Citi Bike dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f7b3436a287"
      },
      "outputs": [],
      "source": [
        "bike_df.printSchema()\n",
        "bike_df.describe().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "From this summary, there is also interesting information from the dataset's summary.\n",
        "  - There are over 10 million logs of trips for Citi Bike in 2016.\n",
        "  - The current dataset has some abnormal values.\n",
        "  - `starttime` and `stoptime` are string format. To use it effectively, it needs to be re-formatted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77e00e95bf23"
      },
      "outputs": [],
      "source": [
        "# Rename \"tripduration\" to \"trip_duration\".\n",
        "bike_df = bike_df.withColumnRenamed(\"tripduration\", \"trip_duration\")\n",
        "\n",
        "# Convert the type of start/stop time from a string to a Unix timestamp.\n",
        "bike_df = bike_df.withColumn(\n",
        "    \"starttime\", unix_timestamp(to_timestamp(col(\"starttime\")))\n",
        ")\n",
        "bike_df = bike_df.withColumn(\"stoptime\", unix_timestamp(to_timestamp(col(\"stoptime\"))))\n",
        "\n",
        "# Check whether the starttime is a weekday or a weekend.\n",
        "bike_df = bike_df.withColumn(\n",
        "    \"is_weekday\",\n",
        "    ((floor(col(\"starttime\") / 86400) + 4) % 7 > 0)\n",
        "    & ((floor(col(\"starttime\") / 86400) + 4) % 7 < 6),\n",
        ")\n",
        "\n",
        "# Convert starttime to start_time_in_minute\n",
        "bike_df = bike_df.withColumn(\n",
        "    \"start_time_in_minute\", floor((col(\"starttime\") % 86400) / 60)\n",
        ")\n",
        "\n",
        "# Add \"start_zone_id\" and \"end_zone_id\" by applying the UDF \"preprocess_zone_id\".\n",
        "bike_df = bike_df.withColumn(\n",
        "    \"start_zone_id\",\n",
        "    preprocess_zone_id(\n",
        "        bike_df[\"start_station_latitude\"], bike_df[\"start_station_longitude\"]\n",
        "    ),\n",
        ")\n",
        "bike_df = bike_df.withColumn(\n",
        "    \"end_zone_id\",\n",
        "    preprocess_zone_id(\n",
        "        bike_df[\"end_station_latitude\"], bike_df[\"end_station_longitude\"]\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Add \"is_start_manhattan\" and \"is_end_manhattan\" by applying the UDF \"is_in_manhattan\".\n",
        "bike_df = bike_df.withColumn(\n",
        "    \"is_start_manhattan\", is_in_manhattan(col(\"start_zone_id\"))\n",
        ")\n",
        "bike_df = bike_df.withColumn(\"is_end_manhattan\", is_in_manhattan(col(\"end_zone_id\")))\n",
        "\n",
        "# Filter out the bike_df by the following conditions\n",
        "bike_df = bike_df.where(\n",
        "    (\n",
        "        col(\"start_zone_id\") != col(\"end_zone_id\")\n",
        "    )  # Trips must done between different taxi zones.\n",
        "    & (\n",
        "        col(\"starttime\") < col(\"stoptime\")\n",
        "    )  # The start time must be earlier than the end time.\n",
        "    & (col(\"is_start_manhattan\"))\n",
        "    & (col(\"is_end_manhattan\"))  # Trips must start and end within the Manhattan\n",
        ")\n",
        "\n",
        "# Filter out unnecessary columns.\n",
        "bike_df = bike_df.drop(\n",
        "    \"start_station_id\",\n",
        "    \"start_station_name\",\n",
        "    \"end_station_id\",\n",
        "    \"end_station_name\",\n",
        "    \"bikeid\",\n",
        "    \"birth_year\",\n",
        "    \"gender\",\n",
        "    \"customer_plan\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "Assume that the Citi Bike users move through the Manhattan's roads, which are the perfectly perpendicular streets, the trip distance can be calculated by applying the [Manhattan distance](https://en.wikipedia.org/wiki/Taxicab_geometry).\n",
        "\n",
        "To Calulate the Manhattan distance, assume that you are in the **W 15th St and 9th Ave, where Google NYC is located,** and you want to go to **W 33rd St and 5th Ave, where the Empire State building is located**. In a full of city blocks, the fastest way you can get to the destination is to move east until you reached the 5th Avenue, and move north until you reached the W 33rd St. In this case, **W 16th St and 5th Ave** or **W 33rd St and 9th Ave** can be a hinge point. If we set the starting point as **S**, the ending point as **E**, and the hinge point as **H**, the formula of Manhattan distance is `distance(S, H) + distance(H, E)`.\n",
        "\n",
        "However, in a real world, it requires more calculation to get the \"real\" Manhattan distance. First of all, unlike the Cartesian coordinate system. the actual distance in longitude varies with latitude. For example, 1 degree of longitude represents roughly 69 miles at equator, is the same as latitude, while it is about 49 miles at 45 degrees North or South. To calculate the exact distance in a real world using latitudes and longitudes, [Haversine distance](https://en.wikipedia.org/wiki/Haversine_formula) is one of the good choices.\n",
        "\n",
        "Furthermore, the streets in Manhattan are inclined at about 29 degrees to the True north, so you should rotate the point S and E by 29 degrees counter-clockwise.\n",
        "\n",
        "Therefore, the final formula should be `manhattan_distance = haversine_distance(S', H') + haversine_distance(H', E')` where `S'`, `H'`, `E'` are the rotated points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82b6e83a1d26"
      },
      "outputs": [],
      "source": [
        "@udf(returnType=DoubleType())\n",
        "def manhattan_dist(lat_start, lon_start, lat_end, lon_end):\n",
        "    \"\"\"\n",
        "    The preprocessing function takes latitudes and longitudes of start position and end position,\n",
        "    and returns the Manhattan distance between them.\n",
        "    Args:\n",
        "        lat_start: The latitude of the start position.\n",
        "        lon_start: The longitude of the start position.\n",
        "        lat_end: The latitude of the end position.\n",
        "        lon_end: The longitude of the end position.\n",
        "    Returns:\n",
        "        The Manhattan distance between start and end position.\n",
        "    \"\"\"\n",
        "    # Approximate radius of Earth in mile.\n",
        "    EARTH_RADIUS = 3958.76\n",
        "\n",
        "    # Approximate inclined degree of the streets in Manhattan.\n",
        "    THETA = np.radians(-28.904)\n",
        "\n",
        "    def haversine(pos_start, pos_end):\n",
        "        \"\"\"\n",
        "        The helper function takes start and end position and returns the haversine distance.\n",
        "        Args:\n",
        "            pos_start: a latitude and a longitude of start position in np.array form.\n",
        "            pos_end: a latitude and a longitude of end position in np.array form.\n",
        "        Returns:\n",
        "            The Haversine, the spherical distance between pos_start and pos_end on a sphere, in this case, the Earth.\n",
        "        \"\"\"\n",
        "        rad_dist_lat = np.radians(pos_end[0] - pos_start[0])\n",
        "        rad_dist_lon = np.radians(pos_end[1] - pos_start[1])\n",
        "        rad_lat1 = np.radians(pos_start[0])\n",
        "        rad_lat2 = np.radians(pos_end[0])\n",
        "        dist = 2 * np.arcsin(\n",
        "            np.sqrt(\n",
        "                np.sin(rad_dist_lat / 2) ** 2\n",
        "                + np.cos(rad_lat1) * np.cos(rad_lat2) * np.sin(rad_dist_lon / 2) ** 2\n",
        "            )\n",
        "        )\n",
        "        return EARTH_RADIUS * dist\n",
        "\n",
        "    def rotate(pos, theta):\n",
        "        \"\"\"\n",
        "        The helper function takes position and the degree theta, and returns rotated position.\n",
        "        Args:\n",
        "            pos: a latitude and a longitude of position in np.array form.\n",
        "            theta: the degree to rotate.\n",
        "        Returns:\n",
        "            Rotated position.\n",
        "        \"\"\"\n",
        "        rotate = np.array(\n",
        "            [[np.cos(theta), np.sin(theta)], [-np.sin(theta), np.cos(theta)]]\n",
        "        )\n",
        "        return np.matmul(rotate, pos)\n",
        "\n",
        "    if not (lat_start and lon_start and lat_end and lon_end):\n",
        "        return -1\n",
        "\n",
        "    # Convert positions to np.array format.\n",
        "    start, end = np.array([lat_start, lon_start]), np.array([lat_end, lon_end])\n",
        "\n",
        "    # Rotate each positions by 29' using a helper function.\n",
        "    rotated_start, rotated_end = rotate(start, THETA), rotate(end, THETA)\n",
        "\n",
        "    # Get rotated hinge point using rotated start and end point.\n",
        "    rotated_hinge = np.array([rotated_start[0], rotated_end[1]])\n",
        "\n",
        "    # Re-rotate the hinge point.\n",
        "    hinge = rotate(rotated_hinge, -THETA)\n",
        "\n",
        "    # Return the sum of the Haversine distance between start and hinge, hinge and end.\n",
        "    return float(haversine(start, hinge) + haversine(hinge, end))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ece73e72bea"
      },
      "outputs": [],
      "source": [
        "# Calculate the Manhattan distance between \"start_station\" and \"end_station\".\n",
        "bike_df = bike_df.withColumn(\n",
        "    \"trip_distance\",\n",
        "    manhattan_dist(\n",
        "        col(\"start_station_latitude\"),\n",
        "        col(\"start_station_longitude\"),\n",
        "        col(\"end_station_latitude\"),\n",
        "        col(\"end_station_longitude\"),\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc5dd05670e6"
      },
      "outputs": [],
      "source": [
        "# Filter out the bike_df with the same condition as the taxi_df\n",
        "bike_df = bike_df.where(\n",
        "    (col(\"trip_distance\") > 0)\n",
        "    & (col(\"trip_distance\") < 20)\n",
        "    & (col(\"trip_duration\") > 0)\n",
        "    & (col(\"start_station_latitude\") > 40)\n",
        "    & (col(\"start_station_latitude\") < 41)\n",
        "    & (col(\"end_station_latitude\") > 40)\n",
        "    & (col(\"end_station_latitude\") < 41)\n",
        "    & (col(\"start_station_longitude\") > -75)\n",
        "    & (col(\"start_station_longitude\") < -73)\n",
        "    & (col(\"end_station_longitude\") > -75)\n",
        "    & (col(\"end_station_longitude\") < -73)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29d2a801cf6a"
      },
      "outputs": [],
      "source": [
        "bike_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "Check the distributions for the numerical columns. In PySpark, visualizing is expensive because the data is too large. \n",
        "\n",
        "For example, the NYC Taxi dataset has more than 5M rows. Therefore, approximately 3% of total data (approx. 330k rows) are extracted as a sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42a319b0c66c"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "taxi_sample = taxi_df.sample(0.03)\n",
        "taxi_pd = taxi_sample.toPandas()\n",
        "\n",
        "# Convert \"object\" type columns to the float type.\n",
        "taxi_pd[\"trip_distance\"] = taxi_pd[\"trip_distance\"].astype(float)\n",
        "taxi_pd[\"fare_amount\"] = taxi_pd[\"fare_amount\"].astype(float)\n",
        "taxi_pd.info()\n",
        "\n",
        "bike_sample = bike_df.sample(0.03)\n",
        "bike_pd = bike_sample.toPandas()\n",
        "bike_pd.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "Since there are some extreme data points, remove them and bin the data into a box and histogram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "723d2f8fa827"
      },
      "outputs": [],
      "source": [
        "TAXI_COLUMNS_TO_SHOW = {\"trip_distance\", \"trip_duration\", \"fare_amount\"}\n",
        "\n",
        "taxi_pd_filtered = taxi_pd.query(\n",
        "    \"trip_distance > 0 and trip_distance < 20 \\\n",
        "    and trip_duration > 0 and trip_duration < 10000\"\n",
        ")\n",
        "\n",
        "sns.relplot(\n",
        "    data=taxi_pd_filtered,\n",
        "    x=\"trip_distance\",\n",
        "    y=\"trip_duration\",\n",
        "    kind=\"scatter\",\n",
        "    ci=None,\n",
        ")\n",
        "\n",
        "for column in taxi_pd_filtered.columns:\n",
        "    if column in TAXI_COLUMNS_TO_SHOW:\n",
        "        _, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "        taxi_pd_filtered[column].plot(kind=\"box\", ax=ax[0])\n",
        "        taxi_pd_filtered[column].plot(kind=\"hist\", ax=ax[1])\n",
        "        plt.title(column)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "We found that most columns are skewed right and the trip_distance basically increases as trip_duration increases. Furthermore, most trips are done within an hour (3600 sec).\n",
        "\n",
        "Do the similar work for the Citi Bike dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "355716305c5f"
      },
      "outputs": [],
      "source": [
        "BIKE_COLUMNS_TO_SHOW = {\"trip_distance\", \"trip_duration\"}\n",
        "\n",
        "bike_pd_filtered = bike_pd.query(\n",
        "    \"trip_distance > 0 and trip_distance < 20 \\\n",
        "    and trip_duration > 0 and trip_duration < 10000\"\n",
        ")\n",
        "\n",
        "sns.relplot(\n",
        "    data=bike_pd_filtered,\n",
        "    x=\"trip_distance\",\n",
        "    y=\"trip_duration\",\n",
        "    hue=\"usertype\",\n",
        "    kind=\"scatter\",\n",
        "    ci=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "The plot for trip_distance and trip_duration for Citi Bike looks different than the same plot for the Taxi dataset.\n",
        "\n",
        "A lot of trips was longer than expected given the trip distance. The reason for this behavior seems that, unlike taxis, city bike users less likely to have an exact destination of trip, or they did not park their bicycles properly at the station. This behavior is found more on customers, who do not use the Citi Bike regularly.\n",
        "\n",
        "To remove outliers, let's assume the average bike speed is at least 2 miles per hour, and filter them out if a trip_duration multiplied by speed is less trip_distance, and remove `Customer` user type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45112e073997"
      },
      "outputs": [],
      "source": [
        "bike_pd_filtered = bike_pd.query(\n",
        "    \"trip_distance > 0 and trip_distance < 8 \\\n",
        "    and (trip_duration * 0.0005) <= trip_distance \\\n",
        "    and trip_duration > 0 and trip_duration < 3600 \\\n",
        "    and usertype=='Subscriber'\"\n",
        ")\n",
        "\n",
        "sns.relplot(\n",
        "    data=bike_pd_filtered,\n",
        "    x=\"trip_distance\",\n",
        "    y=\"trip_duration\",\n",
        "    hue=\"usertype\",\n",
        "    kind=\"scatter\",\n",
        "    ci=None,\n",
        ")\n",
        "\n",
        "for column in bike_pd_filtered.columns:\n",
        "    if column in BIKE_COLUMNS_TO_SHOW:\n",
        "        _, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "        bike_pd_filtered[column].plot(kind=\"box\", ax=ax[0])\n",
        "        bike_pd_filtered[column].plot(kind=\"hist\", ax=ax[1])\n",
        "        plt.title(column)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "Based on the visualized data, apply filter to remove unrealistic values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09d3908a6012"
      },
      "outputs": [],
      "source": [
        "taxi_df = taxi_df.where(\n",
        "    (col(\"trip_duration\") > 0)\n",
        "    & (col(\"trip_duration\") < 3600)\n",
        "    & ((col(\"trip_duration\") * 0.00055) <= col(\"trip_distance\"))\n",
        ")\n",
        "\n",
        "bike_df = bike_df.where(\n",
        "    (col(\"trip_duration\") > 0)\n",
        "    & (col(\"trip_duration\") < 3600)\n",
        "    & (col(\"usertype\") == \"Subscriber\")\n",
        "    & ((col(\"trip_duration\") * 0.00055) <= col(\"trip_distance\"))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "### Feature Selection\n",
        "\n",
        "Not all features in our dataset will be useful. \n",
        "Since the purpose of this tutorial is comparing two datasets, we need to use same features for training. There are some useful features for training the Taxi dataset, such as `fare_amount`, since Citi Bike dataset does not have it, it will not be used in this tutorial.\n",
        "\n",
        "After choose the following columns as features, they needs to be assembled with `VectorAssembler()`, A feature transformer that merges multiple columns into a vector column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ca926fe3b73"
      },
      "outputs": [],
      "source": [
        "feature_cols = [\n",
        "    \"is_weekday\",\n",
        "    \"start_time_in_minute\",\n",
        "    \"start_zone_id\",\n",
        "    \"end_zone_id\",\n",
        "    \"trip_distance\",\n",
        "]\n",
        "\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "\n",
        "# Transform each column into the vector form.\n",
        "taxi_transformed_data = assembler.transform(taxi_df)\n",
        "bike_transformed_data = assembler.transform(bike_df)\n",
        "\n",
        "# Split randomly with training and test data.\n",
        "(taxi_training_data, taxi_test_data) = taxi_transformed_data.randomSplit([0.95, 0.05])\n",
        "(bike_training_data, bike_test_data) = bike_transformed_data.randomSplit([0.95, 0.05])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "### Training the Model\n",
        "\n",
        "In PySpark, there are several Regression Models provided such as `LinearRegression`, `GeneralizedLinearRegression`, `DecisionTreeRegressor`, `RandomForestRegressor`, and `GBTRegressor`.\n",
        "`GBTRegressor`, as known as `Gradient Boosted Trees` are a popular regression method using ensembles of decision trees. This algorithm iteratively trains decision trees in order to minimize a loss function.\n",
        "Although this model is computationally expensive, in testing it was shown that the `GBT Regressor` provides the best results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d8ef0874cbf"
      },
      "outputs": [],
      "source": [
        "# Define GBTRegressor\n",
        "gbt = GBTRegressor(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"trip_duration\",\n",
        "    predictionCol=\"pred_trip_duration\",\n",
        ")\n",
        "\n",
        "# Define evaluator for r2 score.\n",
        "evaluator_r2 = RegressionEvaluator(\n",
        "    labelCol=gbt.getLabelCol(), predictionCol=gbt.getPredictionCol(), metricName=\"r2\"\n",
        ")\n",
        "\n",
        "# Define evaluator for RMSE error.\n",
        "evaluator_rmse = RegressionEvaluator(\n",
        "    labelCol=gbt.getLabelCol(), predictionCol=gbt.getPredictionCol(), metricName=\"rmse\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0ecfdae3db2"
      },
      "outputs": [],
      "source": [
        "# Train a model and get predictions for the Taxi dataset. It takes several minutes.\n",
        "taxi_gbt_model = gbt.fit(taxi_training_data)\n",
        "taxi_gbt_predictions = taxi_gbt_model.transform(taxi_test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e43b652e6107"
      },
      "outputs": [],
      "source": [
        "# Train a model and get predictions for the Citi Bike dataset. It takes several minutes.\n",
        "bike_gbt_model = gbt.fit(bike_training_data)\n",
        "bike_gbt_predictions = bike_gbt_model.transform(bike_test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4197f357a7b"
      },
      "outputs": [],
      "source": [
        "# Evaluate the r2 score for the Taxi dataset\n",
        "taxi_gbt_accuracy_r2 = evaluator_r2.evaluate(taxi_gbt_predictions)\n",
        "print(f\"Taxi Test GBT R2 Accuracy = {taxi_gbt_accuracy_r2}\")\n",
        "\n",
        "# Evaluate the RMSE for the Taxi dataset\n",
        "taxi_gbt_accuracy_rmse = evaluator_rmse.evaluate(taxi_gbt_predictions)\n",
        "print(f\"Taxi Test GBT RMSE Accuracy = {taxi_gbt_accuracy_rmse}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "410b54ce8541"
      },
      "outputs": [],
      "source": [
        "# Evaluate the r2 score for the Citi Bike dataset\n",
        "bike_gbt_accuracy_r2 = evaluator_r2.evaluate(bike_gbt_predictions)\n",
        "print(f\"Bike Test GBT R2 Accuracy = {bike_gbt_accuracy_r2}\")\n",
        "\n",
        "# Evaluate the RMSE for the Citi Bike dataset\n",
        "bike_gbt_accuracy_rmse = evaluator_rmse.evaluate(bike_gbt_predictions)\n",
        "print(f\"Bike Test GBT RMSE Accuracy = {bike_gbt_accuracy_rmse}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "### Debrief the result\n",
        "\n",
        "The trained model for the Taxi dataset shows about 66% for r2 score and 265 root-mean square error (RMSE), while the model for the Citi Boke dataset shows about 69% for r2 score and 345 RMSE. There are some restrictions to having relatively lower scores.\n",
        "- Do not use CrossValidator\n",
        "  - In real machine learning projects, cross-validation is an essential tool that provides us to utilize our data better. However, it re-run the training algorithm several times so it requres a lot of time and computation resource. For more information, [check this link](https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29).\n",
        "- Do not use every feature in the Taxi dataset.\n",
        "  - The Taxi dataset has a `fare_amount` column, which has a strong correlation between the label, `trip_duration`. If that column was included, the model for the Taxi dataset shows 90+% r2 score.\n",
        "  - The reason that the Taxi dataset has limited features is the purpose of this project is to compare Taxi and the Citi Bike.\n",
        "- Do not use the exact coordinates for both datasets.\n",
        "  - Due to privacy concerns, Taxi dataset is no longer providing the precise coordination of pickup and dropoff locations, but NYC Taxi zones in recent data.\n",
        "  - Taxi Zone is a categorical number that does not represents a geographical zone defined by NYC Taxi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "### Save the model to a Cloud Storage path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2280850da83"
      },
      "outputs": [],
      "source": [
        "BUCKET_FOLDER = \"/tmp/bucket\"\n",
        "\n",
        "# In the testing environment, saving to GCS bucket routes to the local file system.\n",
        "if os.getenv(\"IS_TESTING\"):\n",
        "    ! rm -rf $BUCKET_FOLDER\n",
        "    ! mkdir $BUCKET_FOLDER\n",
        "    bike_gbt_model.write().overwrite().save(f\"{BUCKET_FOLDER}\")\n",
        "    ! gsutil cp -r $BUCKET_FOLDER $BUCKET_URI\n",
        "else:\n",
        "    bike_gbt_model.write().overwrite().save(f\"{BUCKET_URI}/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "See [Clean up](https://cloud.google.com/vertex-ai/docs/workbench/managed/create-managed-notebooks-instance-console-quickstart#clean-up) to delete your project or the managed notebook created in this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "### Delete BigQuery dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7a1a1a5e978"
      },
      "outputs": [],
      "source": [
        "! bq rm -r -f $DATASET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "After you delete the BigQuery dataset, you can check your Datasets in BigQuery using the following command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7a1a1a5e978"
      },
      "outputs": [],
      "source": [
        "! bq ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "### Delete Google Cloud Storage bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7a1a1a5e978"
      },
      "outputs": [],
      "source": [
        "! gsutil rm -r $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "### Delete Dataproc Cluster\n",
        "\n",
        "It is not possible to delete the cluster you are currently using unless you switch the kernel to local. To delete it, you need to switch the kernel to local `Python 3` or `PySpark`, set your `CLUSTER_NAME` and `CLUSTER_REGION` manually in the following cell, and execute the `gcloud` command.\n",
        "\n",
        "See [Deleting a cluster](https://cloud.google.com/dataproc/docs/guides/manage-cluster#console) to delete the Dataproc cluster created in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f6cae539ccc"
      },
      "outputs": [],
      "source": [
        "CLUSTER_NAME = \"[your-cluster-name]\"\n",
        "CLUSTER_REGION = \"[your-cluster-region]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61244eb74c17"
      },
      "outputs": [],
      "source": [
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    ! gcloud dataproc clusters delete $CLUSTER_NAME --region=$CLUSTER_REGION -q"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "spark_ml.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

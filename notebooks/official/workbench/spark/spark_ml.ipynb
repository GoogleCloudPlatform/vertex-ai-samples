{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-managed-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/workbench/spark/spark_ml.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>                                                                                               \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook tutorial shows you Apache SparkML jobs with Dataproc and BigQuery. Through this notebook, you can learn a common use case in the machine learning pipeline: Ingestion, data cleaning, feature engineering, modeling, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The two datasets, [NYC TLC(Taxi and Limousine Commission) Trips](https://console.cloud.google.com/marketplace/product/city-of-new-york/nyc-tlc-trips) dataset and [NYC Citi Bike Trips](https://console.cloud.google.com/marketplace/product/city-of-new-york/nyc-citi-bike) dataset, is available in [BigQuery Public Datasets](https://cloud.google.com/bigquery/public-data), and provides free querying of up to 1TB of data each month. It contains trips data for each Taxi and Citi Bike, the public bicycle sharing system serving the New York City."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Objective\n",
    "\n",
    "This notebook tutorial runs an Apache SparkML job that fetches data from the BigQuery dataset, performs exploratory data analysis, cleans data, executes feature engineering, trains the model, evaluates the model, debriefs for the result and saves the model to a Cloud Storage.\n",
    "\n",
    "This notebook tutorial performs the following steps:\n",
    "\n",
    "- Setting up a Google Cloud project and Dataproc cluster.\n",
    "- Configuring the spark-bigquery-connector.\n",
    "- Ingesting data from BigQuery into a Spark DataFrame.\n",
    "- Performing Exploratory Data Analysis (EDA).\n",
    "- Visualizing Data with samples.\n",
    "- Cleaning Data.\n",
    "- Training the model.\n",
    "- Saving the model to a Cloud Storage path.\n",
    "- Deleting the resources created for this notebook tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Costs \n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* [Vertex AI](https://cloud.google.com/vertex-ai/pricing)\n",
    "* [Cloud Storage](https://cloud.google.com/storage/pricing)\n",
    "* [Dataproc](https://cloud.google.com/dataproc/pricing)\n",
    "\n",
    "You can use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "## Before you begin\n",
    "\n",
    "### Set up your Google Cloud project:\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you create an account, you receive a $300 credit towards to your compute and storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. [Enable the Notebooks API, Vertex AI API, and Dataproc API](https://console.cloud.google.com/flows/enableapi?apiid=notebooks.googleapis.com,aiplatform.googleapis.com,dataproc&_ga=2.209429842.1903825585.1657549521-326108178.1655322249)\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and inserts the value of Python variables prefixed with `$` into the commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Installation\n",
    "\n",
    "Install the following packages to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "USER_FLAG = \"\"\n",
    "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
    "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.getenv(\"IS_TESTING\"):\n",
    "    \"\"\"\n",
    "    The testing suite does not currently support testing on Dataproc clusters,\n",
    "    so the testing environment is setup to replicate Dataproc via the following steps.\n",
    "    \"\"\"\n",
    "    JAVA_VER = \"8u332-b09\"\n",
    "    JAVA_FOLDER = \"/tmp/java\"\n",
    "    FILE_NAME = f\"openlogic-openjdk-{JAVA_VER}-linux-x64\"\n",
    "    TAR_FILE = f\"{JAVA_FOLDER}/{FILE_NAME}.tar.gz\"\n",
    "    DOWNLOAD_LINK = f\"https://builds.openlogic.com/downloadJDK/openlogic-openjdk/{JAVA_VER}/openlogic-openjdk-{JAVA_VER}-linux-x64.tar.gz\"\n",
    "    PYSPARK_VER = \"3.1.3\"\n",
    "\n",
    "    # Download Open JDK 8. Spark requires Java to execute.\n",
    "    ! rm -rf $JAVA_FOLDER\n",
    "    ! mkdir $JAVA_FOLDER\n",
    "    ! wget -P $JAVA_FOLDER $DOWNLOAD_LINK\n",
    "    os.environ[\"JAVA_HOME\"] = f\"{JAVA_FOLDER}/{FILE_NAME}\"\n",
    "    ! tar -zxf $TAR_FILE -C $JAVA_FOLDER\n",
    "    ! echo $JAVA_HOME\n",
    "\n",
    "    # Pin the Spark version to match that the Dataproc 2.0 cluster.\n",
    "    ! pip install {USER_FLAG} pyspark==$PYSPARK_VER -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Create a Dataproc cluster\n",
    "\n",
    "The Spark job executed in this notebook tutorial is compute intensive. Since the job can take a significant amount time to complete in a standard notebook environment, this notebook tutorial runs on a Dataproc cluster that is created with the Dataproc Component Gateway and Jupyter component installed on the cluster.\n",
    "\n",
    "**Existing Dataproc with Jupyter cluster?**: If you have a running Dataproc cluster that has the [Component Gateway and Jupyter component installed on the cluster](https://cloud.google.com/dataproc/docs/concepts/components/jupyter#gcloud-command)), you can use it in this tutorial. If you plan to use it, skip this step, and go to `Switch your kernel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    CLUSTER_NAME = \"[your-cluster]\"  # @param {type: \"string\"}\n",
    "    CLUSTER_REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "    if CLUSTER_REGION == \"[your-region]\":\n",
    "        CLUSTER_REGION = \"us-central1\"\n",
    "\n",
    "    print(f\"CLUSTER_NAME: {CLUSTER_NAME}\")\n",
    "    print(f\"CLUSTER_REGION: {CLUSTER_REGION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    !gcloud dataproc clusters create $CLUSTER_NAME \\\n",
    "        --region=$CLUSTER_REGION \\\n",
    "        --enable-component-gateway \\\n",
    "        --image-version=2.0 \\\n",
    "        --optional-components=JUPYTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "Your `CLUSTER_NAME` must be **unique within your Google Cloud project**. It must start with a lowercase letter, followed by up to 51 lowercase letters, numbers, and hyphens, and cannot end with a hyphen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "#### Switch your kernel\n",
    "\n",
    "Your notebook kernel is listed at the top of the notebook page. Your notebook should run on the Python 3 kernel running on your Dataproc cluster.\n",
    "\n",
    "Select **Kernel > Change Kernel** from the top menu, then select `Python 3 on CLUSTER_NAME: Dataproc cluster in REGION (Remote)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Set your project ID\n",
    "\n",
    "Run the following cell to get you project ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "If the previous command has no output, copy your project ID from the project selector in the [Google Cloud console](https://console.cloud.google.com/). Insert the ID in the `[your-project-id]` placeholder, then run the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"[your-project-id]\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "The Spark DataFrame created in this tutorial is stored in BigQuery, with the data first being written to a Google Cloud Storage bucket before it is written into BigQuery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "#### Region\n",
    "\n",
    "Before creating a Cloud Storage bucket, re-define the `REGION` variable (when you changed the notebook kernel earlier, previously set variables were deleted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "To avoid name collisions, you can create a timestamp for the current notebook session, then append the timestamp to the name of resources that you create in this tutorial, such as the Cloud Storage bucket or BigQuery dataset that you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "Replace the `[your-bucket-name]` placeholder with the name of your Cloud Storage bucket. The name must be unique across all Cloud Storage buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}/\"\n",
    "\n",
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
    "    BUCKET_NAME = f\"{PROJECT_ID}{TIMESTAMP}\"\n",
    "    BUCKET_URI = f\"gs://{BUCKET_NAME}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "Confirm your access to the Cloud Storage bucket by displaying the bucket's metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls -L -b $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "## Tutorial\n",
    "\n",
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "# A Spark Session is how you interact with Spark SQL to create Dataframes\n",
    "from pyspark.sql import SparkSession\n",
    "# PySpark functions\n",
    "from pyspark.sql.functions import col, udf, to_timestamp, unix_timestamp, pandas_udf, PandasUDFType, to_date, floor, abs, lit\n",
    "# These allow us to create a schema for our data\n",
    "from pyspark.sql.types import DoubleType, BooleanType\n",
    "\n",
    "from geopandas import gpd\n",
    "from shapely.geometry import Point\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### Initialize the SparkSession\n",
    "\n",
    "To use Apache Spark with BigQuery, you must include the [spark-bigquery-connector](https://github.com/GoogleCloudDataproc/spark-bigquery-connector) when you initialize the SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "outputs": [],
   "source": [
    "# Initialize the \"SparkSession\" with the following config.\n",
    "VER = \"0.26.0\"\n",
    "FILE_NAME = f\"spark-bigquery-with-dependencies_2.12-{VER}.jar\"\n",
    "\n",
    "if os.getenv(\"IS_TESTING\"):\n",
    "    connector = f\"https://github.com/GoogleCloudDataproc/spark-bigquery-connector/releases/download/{VER}/{FILE_NAME}\"\n",
    "else:\n",
    "    connector = f\"gs://spark-lib/bigquery/{FILE_NAME}\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"spark-bigquery-polyglot-language-demo\")\n",
    "    .config(\"spark.jars\", connector)\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", \"500\")\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### Fetch data from BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "outputs": [],
   "source": [
    "# Load NYC_taxi in Github Activity Public Dataset from BigQuery.\n",
    "taxi_df = (\n",
    "    spark.read.format(\"bigquery\")\n",
    "    .option(\"table\", \"bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2018\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Load NYC_Citibike in Github Acitivity Public dataset from BQ.\n",
    "bike_df = (\n",
    "    spark.read.format(\"bigquery\")\n",
    "    .option(\"table\", \"bigquery-public-data.new_york_citibike.citibike_trips\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Since it consumes too much time and computing resources, we will use 10% of the rows for each dataset.\n",
    "taxi_df = taxi_df.sample(0.1)\n",
    "bike_df = bike_df.sample(0.1)\n",
    "\n",
    "if os.getenv(\"IS_TESTING\"):\n",
    "    taxi_df = taxi_df.sample(0.001)\n",
    "    bike_df = bike_df.sample(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Perform Exploratory Data Analysis(EDA)\n",
    "\n",
    "As we get started with a new problem, the first step is to gain an understanding of what the dataset contains. EDA is used to derive insights from the data. Data scientists and analysts try to find different patterns, relations, and anomalies in the data using some statistical graphs and other visualization techniques. It allows analysts to understand the data better before making any assumptions.\n",
    "\n",
    "Check the data types for Taxi dataset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "Filter out unnecessary columns and check null counts of the fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df = taxi_df.select(\n",
    "    col(\"pickup_datetime\"),\n",
    "    col(\"dropoff_datetime\"),\n",
    "    col(\"trip_distance\"),\n",
    "    col(\"fare_amount\"),\n",
    "    col(\"pickup_location_id\"),\n",
    "    col(\"dropoff_location_id\"),\n",
    ")\n",
    "taxi_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "From this summary, you are able to know a lot of information.\n",
    "  - There are over 11 millions of trip history for Yellow Taxi in 2018.\n",
    "  - The current dataset has some abnormal values such as null and negative values in it.\n",
    "  - `pickup_datetime` and `dropoff_datetime` are string format. To use it effectively, it needs to be re-formatted.\n",
    "  - In previous years, the exact latitude and longitude were used for the pickup and the dropoff locations. It raised a lot of [privacy concerns](https://agkn.wordpress.com/2014/09/15/riding-with-the-stars-passenger-privacy-in-the-nyc-taxicab-dataset/) and the dataset has been providing `pickup_location_id` and `dropoff_location_id` instead. This id is corresponded to the [NYC Taxi Zones](https://data.cityofnewyork.us/Transportation/NYC-Taxi-Zones/d3c5-ddgc), roughly based on NYC Department of City Planningâ€™s Neighborhood Tabulation Areas (NTAs) and are meant to approximate neighborhoods.\n",
    "  - The maximum value of `pickup_location_id` and `dropoff_location_id` shows `99`. However, these might be wrong since the data type of both is string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "First, you can manipulate the time. `pickup_datetime` and `dropoff_datetime` is currently a string format, so using `to_timestamp()` function and `unix_timestamp()` function, you are able to get each pickup and droppoff datetime as a Unix Timestamp type.\n",
    "\n",
    "Unix time is a way of representing time as the number of seconds since `January 1st, 1970 at 00:00:00 UTC`. Compared to the Timestamp type, Unix time can be represented as an integer, making it easier to parse and use across different systems.\n",
    "\n",
    "After we get `start_time` and `end_time` by converting the original `pickup_datetime` and `dropoff_datetime`, we are able to get more insteresting columns using these two Timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the type of datetime from a string to a Unix timestamp.\n",
    "taxi_df = taxi_df.withColumn('start_time', unix_timestamp(to_timestamp(col('pickup_datetime'))))\n",
    "taxi_df = taxi_df.withColumn('end_time', unix_timestamp(to_timestamp(col('dropoff_datetime'))))\n",
    "\n",
    "# Cast the type of location id from string to integer.\n",
    "taxi_df = taxi_df.withColumn(\"start_zone_id\", taxi_df.pickup_location_id.cast('int'))\n",
    "taxi_df = taxi_df.withColumn(\"end_zone_id\", taxi_df.dropoff_location_id.cast('int'))\n",
    "\n",
    "# Convert start_time to days_of_week.\n",
    "taxi_df = taxi_df.withColumn(\n",
    "    'is_weekdays',\n",
    "    ((floor(col('start_time') / 86400) + 4) % 7 > 0) & ((floor(col('start_time') / 86400) + 4) % 7 < 6))\n",
    "\n",
    "# Convert start_time to start_time_in_minute.\n",
    "taxi_df = taxi_df.withColumn(\n",
    "    'start_time_in_minute',\n",
    "    floor((col('start_time') % 86400) / 60) - 300)\n",
    "\n",
    "# Calculate trip_duration.\n",
    "taxi_df = taxi_df.withColumn('trip_duration', col('end_time') - col('start_time'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "Prior to visualize this modified Taxi dataset, do the similar work for the Citibike dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df = bike_df.select(\n",
    "    col(\"tripduration\").alias(\"trip_duration\"),\n",
    "    col(\"starttime\"),\n",
    "    col(\"stoptime\"),\n",
    "    col(\"start_station_latitude\"),\n",
    "    col(\"start_station_longitude\"),\n",
    "    col(\"end_station_latitude\"),\n",
    "    col(\"end_station_longitude\"),\n",
    "    col(\"usertype\"),\n",
    ")\n",
    "bike_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "From this summary, there is also interesting information from the dataset's summary.\n",
    "  - There are over 53 millions of trip history for Citibike from 2013 to 2018.\n",
    "  - The current dataset has some abnormal values.\n",
    "  - `starttime` and `stoptime` are string format. To use it effectively, it needs to be re-formatted.\n",
    "  - Unlike the Taxi dataset, starting and ending location has exact latitude and longitude, but since every bike is parked in their station, these coordinates represent the station.\n",
    "\n",
    "Assume that the Citi Bike users move through the Manhattan's roads, the perfectly perpendicular streets, the trip distance can be calculated by applying the Manhattan distance formula.\n",
    "\n",
    "Calulating the Manhattan distance is easy. Assume that you are in the **W 15th St and 9th Ave, where Google NYC is located,** and you want to go to **W 33rd St and 5th Ave, where the Empire State building is located,**. In a full of city blocks, the fastest way you can get to the destination is to move east until you reached the 5th Avenue, and move north until you reached the W 33rd St. In this case, **W 16th St and 5th Ave** or **W 33rd St and 9th Ave** can be a hinge point. If we set the starting point as **S**, the ending point as **E**, and the hinge point as **H**, the formula of Manhattan distance is `distance(S, H) + distance(H, E)`.\n",
    "\n",
    "However, in a real world, it requires more calculation to get the \"real\" Manhattan distance. First of all, unlike the Cartesian coordinate system. the actual distance in longitude varies with latitude. For example, 1 degree of longitude represents roughly 69 miles at equator, is the same as latitude, while it is about 49 miles at 45 degrees North or South. To calculate the exact distance in a real world using latitudes and longitudes, [Haversine distance](https://en.wikipedia.org/wiki/Haversine_formula) is one of the good choices.\n",
    "\n",
    "Furthermore, the streets in Manhattan are inclined at about 29 degrees to the True north, so you should rotate the point S and E by 29 degrees anti-clockwise.\n",
    "\n",
    "Therefore, the final formula should be `manhattan_distance = haversine_distance(S', H') + haversine_distance(H', E')` where `S'`, `H'`, `E'` are the rotated points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=DoubleType())\n",
    "def manhattan_dist(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    The preprocessing function takes latitudes and longitudes of start position and end position, \n",
    "    and returns the Manhattan distance between them.\n",
    "    Args:\n",
    "        lat1: The latitude of the start position.\n",
    "        lat2: The longitude of the start position.\n",
    "        lon1: The latitude of the end position.\n",
    "        lon2: The longitude of the end position.\n",
    "    Returns:\n",
    "        The Manhattan distance between start and end position.\n",
    "    \"\"\"\n",
    "    EARTH_RADIUS = 3958.76 # Approximate radious of Earth in mile.\n",
    "    THETA = np.radians(-28.904) # Approximate inclined degree of the streets in Manhattan.\n",
    "    \n",
    "    def haversine(pos1, pos2):\n",
    "        \"\"\"\n",
    "        The helper function takes start and end position and returns the haversine distance.\n",
    "        Args:\n",
    "            pos1: a latitude and a longitude of start position in np.array form.\n",
    "            pos2: a latitude and a longitude of end position in np.array form.\n",
    "        Returns:\n",
    "            The Haversine, the spherical distance between pos1 and pos2 on a sphere, in this case, the Earth.\n",
    "        \"\"\"\n",
    "        rad_dist_lat, rad_dist_lon = np.radians(pos2[0] - pos1[0]), np.radians(pos2[1] - pos1[1])\n",
    "        rad_lat1, rad_lat2 = np.radians(pos1[0]), np.radians(pos2[0])\n",
    "        dist = 2 * np.arcsin(np.sqrt(np.sin(rad_dist_lat / 2) ** 2 + np.cos(rad_lat1) * np.cos(rad_lat2) * np.sin(rad_dist_lon / 2) ** 2))\n",
    "        return EARTH_RADIUS * dist\n",
    "    \n",
    "    def rotate(pos, theta):\n",
    "        \"\"\"\n",
    "        The helper function takes position and the degree theta, and returns rotated position.\n",
    "        Args:\n",
    "            pos: a latitude and a longitude of position in np.array form.\n",
    "            theta: the degree to rotate.\n",
    "        Returns:\n",
    "            Rotated position.\n",
    "        \"\"\"\n",
    "        rotate = np.array([[np.cos(theta), np.sin(theta)],\n",
    "                           [-np.sin(theta), np.cos(theta)]])\n",
    "        return np.matmul(rotate, pos)\n",
    "    \n",
    "    if not (lat1 and lon1 and lat2 and lon2):\n",
    "        return -1\n",
    "    \n",
    "    # Convert positions to np.array format.\n",
    "    start, end = np.array([lat1, lon1]), np.array([lat2, lon2])\n",
    "    \n",
    "    # Rotate each positions by 29' using a helper function.\n",
    "    rotated_start, rotated_end  = rotate(start, THETA), rotate(end, THETA)\n",
    "    \n",
    "    # Get rotated hinge point using rotated start and end point.\n",
    "    rotated_hinge = np.array([rotated_start[0], rotated_end[1]])\n",
    "    \n",
    "    # Re-rotate the hinge point.\n",
    "    hinge = rotate(rotated_hinge, -THETA)\n",
    "    \n",
    "    # Return the Haversine distance between start and hinge and hinge to end.\n",
    "    return float(haversine(start, hinge) + haversine(hinge, end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the type of start/stop time from a string to a Unix timestamp.\n",
    "bike_df = bike_df.withColumn('starttime', unix_timestamp(to_timestamp(col('starttime'))))\n",
    "bike_df = bike_df.withColumn('stoptime', unix_timestamp(to_timestamp(col('stoptime'))))\n",
    "\n",
    "# Check whether the starttime is a weekday or a weekend.\n",
    "bike_df = bike_df.withColumn(\n",
    "    'is_weekdays',\n",
    "    ((floor(col('starttime') / 86400) + 4) % 7 > 0) & ((floor(col('starttime') / 86400) + 4) % 7 < 6))\n",
    "\n",
    "# Convert starttime to start_time_in_minute\n",
    "bike_df = bike_df.withColumn(\n",
    "    'start_time_in_minute',\n",
    "    floor((col('starttime') % 86400) / 60) - 300)\n",
    "\n",
    "# Calculate the Manhattan distance between start_station and end_station\n",
    "bike_df = bike_df.withColumn(\"trip_distance\", manhattan_dist(col(\"start_station_latitude\"), col(\"start_station_longitude\"), col(\"end_station_latitude\"), col(\"end_station_longitude\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "#### Visualization\n",
    "Check the distributions for the numerical columns. In PySpark, visualizing is expensive because the data is too large. For example, the NYC Taxi dataset in 2018 has more than 11M rows. Therefore, approximately 3% of total data (approx. 330k rows) are extracted as a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "taxi_sample = taxi_df.sample(0.03)\n",
    "bike_sample = bike_df.sample(0.03)\n",
    "\n",
    "taxi_pd = taxi_sample.toPandas()\n",
    "bike_pd = bike_sample.toPandas()\n",
    "\n",
    "taxi_pd.info()\n",
    "bike_pd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "When a [Decimal Type](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.types.DecimalType.html) column in PySpark are converted to Pandas DataFrame, it is converted into object Type, not float type. To visualize these \"object\" columns, they need to be converted into the float type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "FLOAT_TYPE_COLUMNS = [\"trip_distance\", \"fare_amount\"]\n",
    "\n",
    "taxi_pd = taxi_pd.drop(columns=['pickup_datetime', 'dropoff_datetime'])\n",
    "\n",
    "for COLUMN in FLOAT_TYPE_COLUMNS:\n",
    "    taxi_pd[COLUMN] = taxi_pd[COLUMN].astype(float)\n",
    "\n",
    "taxi_pd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "Since there are some extreme data points, remove them and bin the data into a box and histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAXI_COLUMNS_TO_SHOW = {\"trip_distance\", \"trip_duration\", \"fare_amount\"}\n",
    "\n",
    "taxi_pd_filtered = taxi_pd.query(\"trip_distance > 0 and trip_distance < 8 \\\n",
    "                                and fare_amount > 0 and fare_amount < 40 \\\n",
    "                                and trip_duration > 0 and trip_duration < 5000\")\n",
    "taxi_pd_filtered.plot(x='trip_distance', y='trip_duration', style='o')\n",
    "\n",
    "for column in taxi_pd_filtered.columns:\n",
    "    if column in TAXI_COLUMNS_TO_SHOW:\n",
    "        _, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "        taxi_pd_filtered[column].plot(kind=\"box\", ax=ax[0])\n",
    "        taxi_pd_filtered[column].plot(kind=\"hist\", ax=ax[1])\n",
    "        plt.title(column)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "We found that most columns are skewed right and the trip_distance basically increases as trip_duration increases.\n",
    "\n",
    "Do the similar work for the Citi Bike dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BIKE_COLUMNS_TO_SHOW = {\"trip_distance\", \"trip_duration\"}\n",
    "\n",
    "bike_pd_filtered = bike_pd.query(\"trip_distance > 0 and trip_distance < 8 \\\n",
    "                                and trip_duration > 0 and trip_duration < 5000\")\n",
    "bike_pd_filtered.plot(x='trip_distance', y='trip_duration', style='o')\n",
    "\n",
    "total_count = bike_pd.count()[\"trip_duration\"]\n",
    "outlier_count = bike_pd[(bike_pd.trip_duration > 3600) & (bike_pd.trip_distance < 1)].count()[\"trip_distance\"]\n",
    "print(f\"Approximately {outlier_count/total_count * 100:.2f}% of data records more than 1 hour of trip_duration and less than 1 mile of trip_distance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "The plot for trip_distance and trip_duration for Citi Bike looks odd than the same plot for the Taxi dataset.\n",
    "\n",
    "A lot of trips spent too much time than trip_distance. The reason for this phenomenon seems that, unlike taxis, city bike users did not have a purpose of trip and simply they enjoyed the trip, or they did not park their bicycles properly at the station. \n",
    "\n",
    "To remove outliers, let's assume the average bike speed is at least 6 miles per hour, which is slightly faster than the walking pace, and filter them out if a trip_duration multiplied by speed is less trip_distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_pd_filtered = bike_pd.query(\"trip_distance > 0 and trip_distance < 8 \\\n",
    "                                and (trip_duration * 0.00166666667) <= trip_distance \\\n",
    "                                and trip_duration > 0 and trip_duration < 5000\")\n",
    "bike_pd_filtered.plot(x='trip_distance', y='trip_duration', style='o')\n",
    "\n",
    "for column in bike_pd_filtered.columns:\n",
    "    if column in BIKE_COLUMNS_TO_SHOW:\n",
    "        _, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "        bike_pd_filtered[column].plot(kind=\"box\", ax=ax[0])\n",
    "        bike_pd_filtered[column].plot(kind=\"hist\", ax=ax[1])\n",
    "        plt.title(column)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gdf_zone = gpd.read_file(\"https://data.cityofnewyork.us/api/geospatial/d3c5-ddgc?method=export&format=GeoJSON\")\n",
    "gdf_zone['location_id'] = gdf_zone['location_id'].astype('long')\n",
    "hm = gdf_zone.to_dict('index')\n",
    "location_set = {hm[i]['location_id'] for i in hm if hm[i]['borough'] == \"Manhattan\"}\n",
    "\n",
    "\n",
    "@udf(returnType=BooleanType())\n",
    "def is_in_manhattan(location_id):\n",
    "    return location_id in location_set\n",
    "\n",
    "@pandas_udf('long')\n",
    "def preprocess_zone_id(lat: pd.Series, lon: pd.Series) -> pd.Series:\n",
    "    point_var = [Point(xy) for xy in zip(lon, lat)]\n",
    "    gdf_points = gpd.GeoDataFrame(pd.DataFrame({'lat': lat, 'lon': lon}), crs='epsg:4326', geometry=point_var)\n",
    "    gdf_joined = gpd.sjoin(gdf_points, gdf_zone, how='left')\n",
    "    return gdf_joined['location_id']\n",
    "\n",
    "@udf(returnType=BooleanType())\n",
    "def is_summer(start_year, time):\n",
    "    if not time:\n",
    "        return False\n",
    "    for year in range(start_year, 2019):\n",
    "        summer_start = datetime(year, 6, 1, 4, 0, 0).timestamp()\n",
    "        summer_end = datetime(year, 9, 1, 3, 59, 0).timestamp()\n",
    "        if summer_start <= time <= summer_end:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "taxi_df = taxi_df.withColumn(\"is_start_manhattan\", is_in_manhattan(col(\"start_zone_id\")))\n",
    "taxi_df = taxi_df.withColumn(\"is_end_manhattan\", is_in_manhattan(col(\"end_zone_id\")))\n",
    "\n",
    "taxi_df = taxi_df.where(\n",
    "    (col('start_time') < col('end_time'))\n",
    "    & (col('trip_duration') > 0) & (col('trip_duration') < 3600)\n",
    "    & (col('trip_distance') > 0.2) & (col('trip_distance') < 15)\n",
    "    & (col(\"start_zone_id\") != col(\"end_zone_id\")) \n",
    "    & (col(\"fare_amount\") > 0) & (col(\"fare_amount\") < 500)\n",
    "    & (col(\"is_start_manhattan\") == True)\n",
    "    & (col(\"is_end_manhattan\") == True)\n",
    "    & ((col(\"trip_duration\") * 0.00167) <= col(\"trip_distance\"))\n",
    ").dropna()\n",
    "\n",
    "taxi_df.printSchema()\n",
    "taxi_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bike_df = bike_df.withColumn('start_zone_id', preprocess_zone_id(bike_df['start_station_latitude'], bike_df['start_station_longitude']))\n",
    "bike_df = bike_df.withColumn('end_zone_id', preprocess_zone_id(bike_df['end_station_latitude'], bike_df['end_station_longitude']))\n",
    "bike_df = bike_df.withColumn('is_start_manhattan', is_in_manhattan(col('start_zone_id')))\n",
    "bike_df = bike_df.withColumn('is_end_manhattan', is_in_manhattan(col('end_zone_id')))\n",
    "\n",
    "bike_df = bike_df.where(\n",
    "    (col('tripduration') > 0) & (col('tripduration') < 3600)\n",
    "    & (col(\"start_zone_id\") != col(\"end_zone_id\")) \n",
    "    & (col('usertype') == \"Subscriber\")\n",
    "    & (col('starttime') < col('stoptime'))\n",
    "    & (col(\"is_start_manhattan\") == True) & (col(\"is_end_manhattan\") == True)\n",
    "    & ((col(\"trip_duration\") * 0.00167) <= col(\"trip_distance\"))\n",
    ").dropna()\n",
    "\n",
    "bike_df.printSchema()\n",
    "bike_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Feature Selection\n",
    "\n",
    "Not all features in our dataset will be useful. \n",
    "Since the purpose of this tutorial is comparing two datasets, we need to use similar features. There are some useful features for training the Taxi dataset, such as `fare_amount`, but I removed it since Citi Bike dataset does not have it.\n",
    "\n",
    "After choose the columns as features, they needs to be assembled with `VectorAssembler()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "feature_cols = [\n",
    "    \"is_weekdays\",\n",
    "    \"start_time_in_minute\",\n",
    "    \"start_zone_id\",\n",
    "    \"end_zone_id\",\n",
    "    \"trip_distance\",\n",
    "]\n",
    "assembler = VectorAssembler(inputCols=taxi_feature_cols, outputCol='features')\n",
    "taxi_transformed_data = assembler.transform(taxi_df)\n",
    "bike_transformed_data = assembler.transform(bike_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "\n",
    "taxi_scaled_df = standard_scaler.fit(taxi_transformed_data).transform(taxi_transformed_data)\n",
    "taxi_scaled_df.select(\"features\", \"features_scaled\").show(10, truncate=False)\n",
    "(taxi_training_data, taxi_test_data) = taxi_scaled_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "bike_scaled_df = standard_scaler.fit(bike_transformed_data).transform(bike_transformed_data)\n",
    "bike_scaled_df.select(\"features\", \"features_scaled\").show(10, truncate=False)\n",
    "(bike_training_data, bike_test_data) = bike_scaled_df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Training the Model\n",
    "\n",
    "TODO: description for what is GBT and why GBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gbt = GBTRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"trip_duration\",\n",
    "    predictionCol=\"pred_trip_duration\",\n",
    ")\n",
    "evaluator_r2 = RegressionEvaluator(\n",
    "    labelCol=gbt.getLabelCol(),\n",
    "    predictionCol=gbt.getPredictionCol(),\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "evaluator_rmse = RegressionEvaluator(\n",
    "    labelCol=gbt.getLabelCol(),\n",
    "    predictionCol=gbt.getPredictionCol(),\n",
    "    metricName=\"rmse\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression, GeneralizedLinearRegression, DecisionTreeRegressor, RandomForestRegressor\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"trip_duration\", predictionCol=\"pred_trip_duration\").setMaxIter(10).setRegParam(0.3)\n",
    "glr = GeneralizedLinearRegression(featuresCol=\"features\", labelCol=\"trip_duration\", predictionCol=\"pred_trip_duration\").setFamily(\"gaussian\").setLink(\"identity\").setMaxIter(10).setRegParam(0.3).setLinkPredictionCol(\"linkOut\")\n",
    "dtr = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"trip_duration\", predictionCol=\"pred_trip_duration\")\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"trip_duration\", predictionCol=\"pred_trip_duration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_lr_model = lr.fit(taxi_training_data)\n",
    "taxi_glr_model = glr.fit(taxi_training_data)\n",
    "taxi_dtr_model = dtr.fit(taxi_training_data)\n",
    "taxi_rf_model = rf.fit(taxi_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_lr_model = lr.fit(bike_training_data)\n",
    "bike_glr_model = glr.fit(bike_training_data)\n",
    "bike_dtr_model = dtr.fit(bike_training_data)\n",
    "bike_rf_model = rf.fit(bike_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_lr_predictions = taxi_lr_model.transform(taxi_test_data)\n",
    "taxi_glr_predictions = taxi_glr_model.transform(taxi_test_data)\n",
    "taxi_dtr_predictions = taxi_dtr_model.transform(taxi_test_data)\n",
    "taxi_rf_predictions = taxi_rf_model.transform(taxi_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_lr_predictions = taxi_lr_model.transform(bike_test_data)\n",
    "bike_glr_predictions = taxi_glr_model.transform(bike_test_data)\n",
    "bike_dtr_predictions = taxi_dtr_model.transform(bike_test_data)\n",
    "bike_rf_predictions = taxi_rf_model.transform(bike_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_lr_accuracy_r2 = evaluator_r2.evaluate(taxi_lr_predictions)\n",
    "print(f\"Taxi lr R2 = {taxi_lr_accuracy_r2}\")\n",
    "taxi_glr_accuracy_r2 = evaluator_r2.evaluate(taxi_glr_predictions)\n",
    "print(f\"Taxi glr R2 = {taxi_glr_accuracy_r2}\")\n",
    "taxi_dtr_accuracy_r2 = evaluator_r2.evaluate(taxi_dtr_predictions)\n",
    "print(f\"Taxi dtr R2 = {taxi_dtr_accuracy_r2}\")\n",
    "taxi_rf_accuracy_r2 = evaluator_r2.evaluate(taxi_rf_predictions)\n",
    "print(f\"Taxi rf R2 = {taxi_rf_accuracy_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_lr_accuracy_r2 = evaluator_r2.evaluate(bike_lr_predictions)\n",
    "print(f\"bike lr R2 = {bike_lr_accuracy_r2}\")\n",
    "bike_glr_accuracy_r2 = evaluator_r2.evaluate(bike_glr_predictions)\n",
    "print(f\"bike glr R2 = {bike_glr_accuracy_r2}\")\n",
    "bike_dtr_accuracy_r2 = evaluator_r2.evaluate(bike_dtr_predictions)\n",
    "print(f\"bike dtr R2 = {bike_dtr_accuracy_r2}\")\n",
    "bike_rf_accuracy_r2 = evaluator_r2.evaluate(bike_rf_predictions)\n",
    "print(f\"bike rf R2 = {bike_rf_accuracy_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Taxi glr R2 = {taxi_glr_accuracy_r2}\")\n",
    "print(f\"Taxi dtr R2 = {taxi_dtr_accuracy_r2}\")\n",
    "print(f\"Taxi rf R2 = {taxi_rf_accuracy_r2}\")\n",
    "print(f\"bike glr R2 = {bike_glr_accuracy_r2}\")\n",
    "print(f\"bike dtr R2 = {bike_dtr_accuracy_r2}\")\n",
    "print(f\"bike rf R2 = {bike_rf_accuracy_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "taxi_gbt_model = gbt.fit(taxi_training_data)\n",
    "taxi_gbt_predictions = taxi_gbt_model.transform(taxi_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bike_gbt_model = gbt.fit(bike_training_data)\n",
    "bike_gbt_predictions = bike_gbt_model.transform(bike_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "taxi_gbt_accuracy_r2 = evaluator_r2.evaluate(taxi_gbt_predictions)\n",
    "print(f\"Taxi Test GBT R2 Accuracy = {taxi_gbt_accuracy_r2}\")\n",
    "\n",
    "taxi_gbt_accuracy_rmse = evaluator_rmse.evaluate(taxi_gbt_predictions)\n",
    "print(f\"Taxi Test GBT RMSE Accuracy = {taxi_gbt_accuracy_rmse}\")\n",
    "\n",
    "# print(f\"Taxi Coefficients: {taxi_model.coefficients}\")\n",
    "# print(f\"Taxi Intercept: {taxi_model.intercept}\")\n",
    "# Taxi Test GBT R2 Accuracy = 0.708183954907601 <- \n",
    "# Taxi Test GBT R2 Accuracy = 0.6598682899938532\n",
    "# rmse = 265.5806200025988\n",
    "# Wall time: 2min 13s\n",
    "# Taxi Test GBT R2 Accuracy = 0.7779961832460531\n",
    "# Taxi Test GBT RMSE Accuracy = 197.87517973345072\n",
    "# CPU times: user 34.8 ms, sys: 9.06 ms, total: 43.9 ms\n",
    "# Wall time: 19.6 s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "bike_gbt_accuracy_r2 = evaluator_r2.evaluate(bike_gbt_predictions)\n",
    "print(f\"Bike Test GBT R2 Accuracy = {bike_gbt_accuracy_r2}\")\n",
    "\n",
    "bike_gbt_accuracy_rmse = evaluator_rmse.evaluate(bike_gbt_predictions)\n",
    "print(f\"Bike Test GBT RMSE Accuracy = {bike_gbt_accuracy_rmse}\")\n",
    "\n",
    "# print(f\"bike Coefficients: {bike_gbt_model.coefficients}\")\n",
    "# print(f\"bike Intercept: {bike_gbt_model.intercept}\")\n",
    "\n",
    "# Bike Test GBT R2 Accuracy = 0.9077256551331765\n",
    "# Bike Test GBT RMSE Accuracy = 124.30534645838586\n",
    "# CPU times: user 61.9 ms, sys: 8.79 ms, total: 70.7 ms\n",
    "# Wall time: 1min 58s\n",
    "# Bike Test GBT R2 Accuracy = 0.8554198857801676\n",
    "# Bike Test GBT RMSE Accuracy = 164.1698092435127\n",
    "# CPU times: user 57.1 ms, sys: 4.66 ms, total: 61.8 ms\n",
    "# Wall time: 1min 16s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Save the model to a Cloud Storage path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_FOLDER = \"/tmp/bucket\"\n",
    "\n",
    "# In the testing environment, saving to GCS bucket routes to the local file system.\n",
    "if os.getenv(\"IS_TESTING\"):\n",
    "    ! rm -rf $BUCKET_FOLDER\n",
    "    ! mkdir $BUCKET_FOLDER\n",
    "    bike_gbt_model.write().overwrite().save(f\"{BUCKET_FOLDER}\")\n",
    "    ! gsutil cp $BUCKET_FOLDER gs://$BUCKET_URI\n",
    "else:\n",
    "    bike_gbt_model.write().overwrite().save(f\"{BUCKET_URI}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "See [Clean up](https://cloud.google.com/vertex-ai/docs/workbench/managed/create-managed-notebooks-instance-console-quickstart#clean-up) to delete your project or the managed notebook created in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7a1a1a5e978"
   },
   "outputs": [],
   "source": [
    "# Delete Google Cloud Storage bucket\n",
    "! gsutil rm -r $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "### Delete Dataproc Cluster\n",
    "\n",
    "It is not possible to delete the cluster you are currently using unless you switch the kernel to local. To delete it, you need to switch the kernel to local `Python 3` or `PySpark`, set your `CLUSTER_NAME` and `CLUSTER_REGION` manually in the following cell, and execute the `gcloud` command.\n",
    "\n",
    "See [Deleting a cluster](https://cloud.google.com/dataproc/docs/guides/manage-cluster#console) to delete the Dataproc cluster created in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_NAME = \"[your-cluster-name]\"\n",
    "CLUSTER_REGION = \"[your-cluster-region]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    ! gcloud dataproc clusters delete $CLUSTER_NAME --region=$CLUSTER_REGION -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "# %%time\n",
    "# param_grid = (\n",
    "#     ParamGridBuilder()\n",
    "#     .addGrid(gbt.maxDepth, [2, 5, 10])\n",
    "#     .addGrid(gbt.maxBins, [10, 20, 40])\n",
    "#     .addGrid(gbt.maxIter, [5, 10, 20])\n",
    "#     .build()\n",
    "# )\n",
    "# cv = CrossValidator(\n",
    "#     estimator=gbt,\n",
    "#     evaluator=evaluator_r2,\n",
    "#     estimatorParamMaps=param_grid,\n",
    "#     numFolds=5\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# taxi_cv_model = cv.fit(taxi_training_data)\n",
    "# taxi_predictions = taxi_cv_model.transform(taxi_test_data)\n",
    "# print(f\"R2:{evaluator_r2.evaluate(taxi_predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# bike_cv_model = cv.fit(bike_training_data)\n",
    "# bike_predictions = bike_cv_model.transform(bike_test_data)\n",
    "# print(f\"R2:{evaluator_r2.evaluate(bike_predictions)}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "spark_sample_notebook.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

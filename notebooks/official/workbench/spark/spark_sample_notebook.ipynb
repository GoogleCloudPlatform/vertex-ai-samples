{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-managed-notebook?download_url=https://raw.githubusercontent.com/hyunuk/vertex-ai-samples/experiment/notebooks/official/workbench/spark/spark_sample_notebook.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>                                                                                               \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how you ingest, analyze, and write back data to BigQuery using Apache Spark on Dataproc Serverless. Using the GitHub Activity Data, we will analyze repositories in GitHub and find out what kind of programming languages being used in their repositories.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The dataset we are using is the [GitHub Activity Data](https://console.cloud.google.com/marketplace/product/github/github-repos), available in [BigQuery Public Datasets](https://cloud.google.com/bigquery/public-data). The first 1TB of data queried each month is free.\n",
    "\n",
    "### Objective\n",
    "\n",
    "This notebook demonstrates Apache Spark jobs that fetch data from BigQuery, analyze it, and write the results back to BigQuery. Through this process, we can learn a common use case in data engineering: ingesting data from a database, performing transformations during preprocessing, and writing back to another database. We also learn how to submit Apache Spark jobs in the Dataproc Serverless environment on Google Cloud Platform. \n",
    "\n",
    "In this project, these questions below will be answered.\n",
    "\n",
    "- Which language is the most frequently used among the monoglot repos?\n",
    "- What is the average size of each language among the monoglot repos?\n",
    "- Given a language, which other languages are most frequently found in polyglot repos with it?\n",
    "\n",
    "Note: repositories that encompass polyglot programming are referred to as polyglot repos and those which only contain one programming language are referred to as monoglot repos.\n",
    "\n",
    "\n",
    "The steps performed include the following:\n",
    "\n",
    "- Setting up the serverless environment.\n",
    "- Configuring spark-bigquery-connector.\n",
    "- Ingesting data from BigQuery to Spark DataFrame.\n",
    "- Preprocessing ingested data.\n",
    "- Analyze that the most frequently used programming language among the monoglot repos.\n",
    "- Analyze that the average size of each language among the monoglot repos.\n",
    "- Analyze that the most frequently used languages with a given language in polyglot repos.\n",
    "- Write the result back to BigQuery\n",
    "- Delete Dataproc Serverless Session\n",
    "- Disable APIs being used in the project.\n",
    "\n",
    "### Costs \n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "* Dataproc Serverless\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), [Dataproc Serverless pricing](https://cloud.google.com/dataproc-serverless/pricing)\n",
    "and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY",
    "tags": []
   },
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. [Enable Notebooks API, Vertex AI API, and Cloud Dataproc API](https://console.cloud.google.com/flows/enableapi?apiid=notebooks.googleapis.com,aiplatform.googleapis.com,dataproc&_ga=2.209429842.1903825585.1657549521-326108178.1655322249)\n",
    "\n",
    "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "oM1iC_MfAts1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  dasi22\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJYoRfYng0XZ"
   },
   "source": [
    "Otherwise, set your project ID here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"[your-project-id]\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m You do not appear to have access to project [dasi22] or it does not exist.\n",
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set project $PROJECT_ID -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  We recommend that you choose the region closest to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06571eb4063b"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append it onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "697568e92bd6"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "#### Create a Cloud Storage bucket\n",
    "\n",
    "The Spark DataFrame created during this project will be stored in BigQuery. The data will be written first to the bucket in Google Cloud Storage(GCS) and then it is loaded it to BigQuery.\n",
    "A GCS bucket must be configured to indicate the temporary data location.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all Cloud Storage buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cf221059d072"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
    "    BUCKET_NAME = f\"{PROJECT_ID}{TIMESTAMP}\"\n",
    "    BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucvCsknMCims"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "#### Create a BigQuery resource\n",
    "\n",
    "Using BUCKET_NAME created above, create a BigQuery resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [],
   "source": [
    "! bq mk $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "### Create a Dataproc cluster\n",
    "\n",
    "Note: If you already have a cluster on Dataproc, you can skip this part and go to \"Change your kernel\"\n",
    "\n",
    "To run your Spark jobs on a Dataproc cluster, you need to create a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "oM1iC_MfAts1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "CLUSTER_NAME = \"[your-cluster]\" # @param {type: \"string\"}\n",
    "\n",
    "if CLUSTER_NAME == \"[your-cluster]\":\n",
    "    CLUSTER_NAME = f\"{PROJECT_ID}-{TIMESTAMP}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "oM1iC_MfAts1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.dataproc.clusters.create) PERMISSION_DENIED: Request had insufficient authentication scopes.\n",
      "- '@type': type.googleapis.com/google.rpc.ErrorInfo\n",
      "  domain: googleapis.com\n",
      "  metadata:\n",
      "    method: google.cloud.dataproc.v1.ClusterController.CreateCluster\n",
      "    service: dataproc.googleapis.com\n",
      "  reason: ACCESS_TOKEN_SCOPE_INSUFFICIENT\n",
      "\n",
      "If you are in a compute engine VM, it is likely that the specified scopes during VM creation are not enough to run this command.\n",
      "See https://cloud.google.com/compute/docs/access/service-accounts#accesscopesiam for more information about access scopes.\n",
      "See https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#changeserviceaccountandscopes for how to update access scopes of the VM.\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc clusters create $CLUSTER_NAME \\\n",
    "    --region=$REGION \\\n",
    "    --enable-component-gateway \\\n",
    "    --optional-components=JUPYTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "#### Change your kernel\n",
    "\n",
    "We will execute Apache Spark jobs on Dataproc Clusters, so you need to change your kernel to Python 3 on your cluster name.\n",
    "\n",
    "Click the button on top-right corner and select \"Python 3 on CLUSTER_NAME: Dataproc cluster in REGION (Remote)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "## Tutorial\n",
    "\n",
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import size, col, UserDefinedFunction, explode\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### Initialize the SparkSession and fetch data from BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\"\"\"BigQuery I/O PySpark example.\"\"\"\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('spark-bigquery-polyglot-language-demo') \\\n",
    "    .config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.25.2.jar') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load data from BigQuery.\n",
    "df = spark.read.format('bigquery') \\\n",
    "  .option('table', 'bigquery-public-data.github_repos.languages') \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for further use.\n",
    "LIMIT = 10\n",
    "EXPLODE_PIE_CHART = tuple([.05] * LIMIT)\n",
    "\n",
    "def array_to_mono_language(arr) -> str:\n",
    "    if len(arr) != 1:\n",
    "        return None\n",
    "    return arr[0].name\n",
    "\n",
    "def array_to_mono_size(arr) -> int:\n",
    "    if len(arr) != 1:\n",
    "        return 0\n",
    "    return arr[0].bytes\n",
    "\n",
    "def array_to_poly_language(arr) -> str:\n",
    "    if len(arr) < 2:\n",
    "        return None\n",
    "    arr.sort(key=lambda x : -x.bytes)\n",
    "    sub = arr[:3]\n",
    "    sub.sort(key=lambda x : x.name)\n",
    "    ret = []\n",
    "    for elem in sub:\n",
    "        ret.append(elem.name)\n",
    "    return ', '.join(ret)\n",
    "\n",
    "udf_map = {\n",
    "    \"mono_language\": array_to_mono_language,\n",
    "    \"mono_size\": array_to_mono_size,\n",
    "    \"poly_language\": array_to_poly_language,\n",
    "}\n",
    "\n",
    "preprocessed_df = df.alias(\"preprocessed_df\")\n",
    "for name, udf in udf_map.items():\n",
    "    preprocessed_df = preprocessed_df.withColumn(name, UserDefinedFunction(udf)(col(\"language\")))\n",
    "preprocessed_df = preprocessed_df.drop(\"language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df.createOrReplaceTempView('df_view')\n",
    "preprocessed_df.printSchema()\n",
    "preprocessed_df.show()\n",
    "# See the number of repositories of monoglot(single language used) and polyglot(multiple languages used).\n",
    "mono = preprocessed_df.where(size(col(\"language\")) == 1).count()\n",
    "poly = preprocessed_df.where(size(col(\"language\")) > 1).count()\n",
    "print(f\"The number of repositories that use only one language is {mono}\")\n",
    "print(f\"The number of repositories that use multiple language is {poly}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the monoglot repositories by the popularity of languages and see the top 10 languages.\n",
    "mono_ranking = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        df_view.mono_language,\n",
    "        count(df_view.mono_language) AS `cnt`\n",
    "    FROM\n",
    "        df_view\n",
    "    GROUP BY\n",
    "        df_view.mono_language\n",
    "    ORDER BY\n",
    "        `cnt` DESC\n",
    "\"\"\")\n",
    "\n",
    "mono_ranking.show()\n",
    "mono_panda = mono_ranking.toPandas()[:LIMIT].copy()\n",
    "mono_panda.groupby(['mono_language']) \\\n",
    "        .sum() \\\n",
    "        .plot( \\\n",
    "            kind='pie', \\\n",
    "            y='cnt', \\\n",
    "            autopct='%1.0f%%', \\\n",
    "            figsize=(8, 8), \\\n",
    "            explode=EXPLODE_PIE_CHART \\\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the average size in MegaBytes of each monoglot repositories and sort by it's size\n",
    "mono_ranking_avg_bytes = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        df_view.mono_language, \n",
    "        round(avg(df_view.mono_size/1000)) AS `average(MB)`,\n",
    "        count(df_view.mono_language) AS `cnt`\n",
    "    FROM \n",
    "        df_view\n",
    "    GROUP BY df_view.mono_language\n",
    "    ORDER BY\n",
    "        `average(MB)` DESC\n",
    "\"\"\")\n",
    "mono_ranking_avg_bytes = mono_ranking_avg_bytes.filter(col('cnt') > 100)\n",
    "mono_ranking_avg_bytes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_ranking = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        df_view.poly_language,\n",
    "        count(df_view.poly_language) AS cnt\n",
    "    FROM\n",
    "        df_view\n",
    "    GROUP BY df_view.poly_language\n",
    "    ORDER BY\n",
    "        cnt DESC\n",
    "\"\"\")\n",
    "poly_ranking.show()\n",
    "\n",
    "poly_panda = poly_ranking.toPandas()[:LIMIT].copy()\n",
    "poly_panda.groupby(['poly_language']) \\\n",
    "        .sum() \\\n",
    "        .plot( \\\n",
    "            kind='pie', \\\n",
    "            y='cnt', \\\n",
    "            autopct='%1.0f%%', \\\n",
    "            figsize=(8, 8), \\\n",
    "            explode=EXPLODE_PIE_CHART \\\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_reduce_language(arr):\n",
    "    if len(arr) < 2:\n",
    "        return None\n",
    "    languages = []\n",
    "    for language in arr:\n",
    "        languages.append(language.name)\n",
    "    return languages\n",
    "\n",
    "def preprocess_combination(arr):\n",
    "    if not arr:\n",
    "        return None\n",
    "    arr_combinations = []\n",
    "    for combination in combinations(arr, 2):\n",
    "        arr_combinations.append(combination)\n",
    "        arr_combinations.append(combination[::-1])\n",
    "    return arr_combinations\n",
    "\n",
    "df = df.withColumn(\n",
    "        \"reduced_languages\",\n",
    "        UserDefinedFunction(\n",
    "            preprocess_reduce_language,\n",
    "            ArrayType(StringType())\n",
    "        )(col(\"language\"))\n",
    "    )\n",
    "df = df.withColumn(\n",
    "        \"combinations\",\n",
    "        UserDefinedFunction(\n",
    "            preprocess_combination, \n",
    "            ArrayType(ArrayType(StringType()))\n",
    "        )(col(\"reduced_languages\"))\n",
    "    )\n",
    "\n",
    "frequency_df = df.select(col(\"repo_name\"), col(\"combinations\")).where(size(col(\"language\")) > 1)\n",
    "frequency_df = frequency_df.withColumn(\"languages\", explode(col(\"combinations\")))\n",
    "frequency_df = frequency_df.withColumn(\"lang0\", col('languages')[0])\n",
    "frequency_df = frequency_df.withColumn(\"lang1\", col('languages')[1])\n",
    "frequency_df = frequency_df.crosstab(\"lang0\", \"lang1\")\n",
    "frequency_df = frequency_df.withColumn(\"languages\", col(\"lang0_lang1\"))\n",
    "frequency_df = frequency_df.drop(\"lang0_lang1\")\n",
    "frequency_df.createOrReplaceTempView('frequency_df_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAJOR_LANGUAGES = {\"C\", \"C++\", \"Java\", \"Python\", \"JavaScript\", \"Go\"}\n",
    "df_arr = []\n",
    "\n",
    "# py_df = spark.sql('''\n",
    "#     SELECT languages, `Python` from frequency_df_view ORDER BY `python` DESC LIMIT 20\n",
    "# ''')\n",
    "# py_df.show()\n",
    "\n",
    "# test = frequency_df.select(col(\"languages\"), col(\"Python\")).sort(-col(\"Python\")).limit(20)\n",
    "# test.show()\n",
    "for language in MAJOR_LANGUAGES:\n",
    "    df_arr.append(frequency_df.select(col(\"languages\"), language).sort(-col(language)).limit(20))\n",
    "for i in df_arr:\n",
    "    i.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"[your-bucket-name]\"\n",
    "spark.conf.set('temporaryGcsBucket', bucket)\n",
    "# Saving the data to BigQuery\n",
    "word_count.write.format('bigquery') \\\n",
    "  .option('table', 'wordcount_dataset.wordcount_output') \\\n",
    "  .save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6ppE7imft-y"
   },
   "source": [
    "## General style examples\n",
    "\n",
    "### Notebook heading\n",
    "\n",
    "- Include the collapsed license at the top (this uses Colab's \"Form\" mode to hide the cells).\n",
    "- Only include a single H1 title.\n",
    "- Include the button-bar immediately under the H1.\n",
    "- Check that the Colab and GitHub links at the top are correct.\n",
    "\n",
    "### Notebook sections\n",
    "\n",
    "- Use H2 (##) and H3 (###) titles for notebook section headings.\n",
    "- Use [sentence case to capitalize titles and headings](https://developers.google.com/style/capitalization#capitalization-in-titles-and-headings). (\"Train the model\" instead of \"Train the Model\")\n",
    "- Include a brief text explanation before any code cells.\n",
    "- Use short titles/headings: \"Download the data\", \"Build the model\", \"Train the model\".\n",
    "\n",
    "### Writing style\n",
    "\n",
    "- Use [present tense](https://developers.google.com/style/tense). (\"You receive a response\" instead of \"You will receive a response\")\n",
    "- Use [active voice](https://developers.google.com/style/voice). (\"The service processes the request\" instead of \"The request is processed by the service\")\n",
    "- Use [second person](https://developers.google.com/style/person) and an imperative style. \n",
    "    - Correct examples: \"Update the field\", \"You must update the field\"\n",
    "    - Incorrect examples: \"Let's update the field\", \"We'll update the field\", \"The user should update the field\"\n",
    "- **Googlers**: Please follow our [branding guidelines](http://goto/cloud-branding).\n",
    "\n",
    "### Code\n",
    "\n",
    "- Put all your installs and imports in a setup section.\n",
    "- Save the notebook with the Table of Contents open.\n",
    "- Write Python 3 compatible code.\n",
    "- Follow the [Google Python Style guide](https://github.com/google/styleguide/blob/gh-pages/pyguide.md) and write readable code.\n",
    "- Keep cells small (max ~20 lines)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n",
    "{TODO: Include commands to delete individual resources below}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx_vKniMq9ZX"
   },
   "outputs": [],
   "source": [
    "# Delete endpoint resource\n",
    "! gcloud ai endpoints delete $ENDPOINT_NAME --quiet --region $REGION_NAME\n",
    "\n",
    "# Delete model resource\n",
    "! gcloud ai models delete $MODEL_NAME --quiet\n",
    "\n",
    "# Delete Cloud Storage objects that were created\n",
    "! gsutil -m rm -r $JOB_DIR\n",
    "\n",
    "if os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-managed-notebook?download_url=https://raw.githubusercontent.com/hyunuk/vertex-ai-samples/experiment/notebooks/official/workbench/spark/spark_sample_notebook.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>                                                                                               \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to ingest, analyze, and write data to BigQuery using Apache Spark with [Dataproc](https://cloud.google.com/dataproc). Using GitHub Activity Data, you will analyze repositories in GitHub and find out what kind of programming languages are used in their repositories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset you are using is the [GitHub Activity Data](https://console.cloud.google.com/marketplace/product/github/github-repos), available in [BigQuery Public Datasets](https://cloud.google.com/bigquery/public-data). We will refer to repositories that encompass polyglot programming as polyglot repos and those which only contain one programming language are referred to as monoglot repos. The first 1TB of data queried each month is free."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "### Objective\n",
    "\n",
    "This notebook demonstrates Apache Spark jobs that fetch data from BigQuery, analyze it, and write the results back to BigQuery. Through this process, you will learn a common use case in data engineering: ingesting data from a database, performing transformations during preprocessing, and writing back to another database. You also learn how to submit Apache Spark jobs to Dataproc Serverless on Google Cloud.\n",
    "\n",
    "In this project, you will answer the following questions.\n",
    "\n",
    "- Which language is the most frequently used among the monoglot repos?\n",
    "- What is the average size of each language among the monoglot repos?\n",
    "- Given a language, which other languages are most frequently found in polyglot repos with it?\n",
    "\n",
    "The steps performed include the following:\n",
    "\n",
    "- Setting up the Dataproc Serverless environment.\n",
    "- Configuring the spark-bigquery-connector.\n",
    "- Ingesting data from BigQuery to a Spark DataFrame.\n",
    "- Preprocessing ingested data.\n",
    "- Discover the most frequently used programming language among the monoglot repos.\n",
    "- Discover the average size of each language among the monoglot repos.\n",
    "- Discover the most frequently used languages with a given language in polyglot repos.\n",
    "- Write the result back to BigQuery\n",
    "- Delete Dataproc Serverless Session\n",
    "- Disable APIs used in the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "### Costs \n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "* Dataproc Serverless\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), [Dataproc Serverless pricing](https://cloud.google.com/dataproc-serverless/pricing)\n",
    "and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "## Before you begin\n",
    "\n",
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. [Enable Notebooks API, Vertex AI API, and Cloud Dataproc API](https://console.cloud.google.com/flows/enableapi?apiid=notebooks.googleapis.com,aiplatform.googleapis.com,dataproc&_ga=2.209429842.1903825585.1657549521-326108178.1655322249)\n",
    "\n",
    "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "### Create a Dataproc cluster\n",
    "\n",
    "**Note**: If you already have a cluster on Dataproc, you can skip this part and go to `Switch your kernel`.\n",
    "\n",
    "The Spark job that you are going to execute in this project is compute-intensive and could take a lot of time on a standard Notebook environment, so this tutorial uses a Dataproc cluster. To run your Spark jobs on a Dataproc cluster, you need to create a cluster with component gateway enabled and JupyterLab extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "CLUSTER_NAME = \"[your-cluster]\"  # @param {type: \"string\"}\n",
    "CLUSTER_REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "if CLUSTER_REGION == \"[your-region]\":\n",
    "    CLUSTER_REGION = \"us-central1\"\n",
    "\n",
    "print(f\"CLUSTER_NAME: {CLUSTER_NAME}\")\n",
    "print(f\"CLUSTER_REGION: {CLUSTER_REGION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "!gcloud dataproc clusters create $CLUSTER_NAME \\\n",
    "    --region=$CLUSTER_REGION \\\n",
    "    --enable-component-gateway \\\n",
    "    --optional-components=JUPYTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "**Note**: Your `CLUSTER_NAME` must be unique and must start with a lowercase letter followed by up to 51 lowercase letters, numbers, and hyphens, and cannot end with a hyphen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "#### Switch your kernel\n",
    "\n",
    "**Note**: If you already select your kernel as Python 3 on your Dataproc cluster, you can skip this part.\n",
    "\n",
    "In order to execute Apache Spark jobs on Dataproc Clusters, you need to change your kernel to Python 3 on your cluster.\n",
    "\n",
    "Click the button on the top-right corner and select `Python 3 on CLUSTER_NAME: Dataproc cluster in REGION (Remote)`\n",
    "\n",
    "If you switch your kernel here, you will lose all variables declared earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJYoRfYng0XZ"
   },
   "source": [
    "Otherwise, set your project ID here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"[your-project-id]\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  We recommend that you choose [the region closest to you](https://cloud.google.com/compute/docs/regions-zones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06571eb4063b"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append it onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "697568e92bd6"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "#### Create a Cloud Storage bucket\n",
    "\n",
    "The Spark DataFrame created during this project will be stored in BigQuery. The data will be written first to the Google Cloud Storage(GCS) bucket and then it is loaded it to BigQuery.\n",
    "A GCS bucket must be configured to indicate the temporary data location.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all Cloud Storage buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "\n",
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
    "    BUCKET_NAME = f\"{PROJECT_ID}{TIMESTAMP}\"\n",
    "    BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucvCsknMCims"
   },
   "source": [
    "Validate access to your Cloud Storage bucket by displaying the bucket's metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -L -b $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "#### Create a BigQuery dataset\n",
    "\n",
    "Set a name for your BigQuery Dataset and create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "DATASET_NAME = \"[your-dataset-name]\"  # @param {type:\"string\"}\n",
    "\n",
    "if DATASET_NAME == \"\" or DATASET_NAME is None or DATASET_NAME == \"[your-dataset-name]\":\n",
    "    DATASET_NAME = f\"{PROJECT_ID}{TIMESTAMP}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [],
   "source": [
    "! bq mk $DATASET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "## Tutorial\n",
    "\n",
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "# A Spark Session is how you interact with Spark SQL to create Dataframes\n",
    "from pyspark.sql import SparkSession\n",
    "# PySpark functions\n",
    "from pyspark.sql.functions import avg, col, count, desc, round, size, udf\n",
    "# These allow us to create a schema for our data\n",
    "from pyspark.sql.types import ArrayType, IntegerType, StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### Initialize the SparkSession\n",
    "\n",
    "To use Apache Spark with BigQuery, you must include the [spark-bigquery-connector](https://github.com/GoogleCloudDataproc/spark-bigquery-connector) when you initialize the SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "outputs": [],
   "source": [
    "# Initialize the SparkSession with the following config.\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"spark-bigquery-polyglot-language-demo\")\n",
    "    .config(\n",
    "        \"spark.jars\",\n",
    "        \"gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.25.2.jar\",\n",
    "    )\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", \"500\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### Fetch data from BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "outputs": [],
   "source": [
    "# Load Github Activity Public Dataset from BigQuery.\n",
    "df = (\n",
    "    spark.read.format(\"bigquery\")\n",
    "    .option(\"table\", \"bigquery-public-data.github_repos.languages\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "403176b059ae"
   },
   "source": [
    "### Preprocessing\n",
    "\n",
    "Based on the schema printed above, data of the GitHub Activity is not stored in primitive types, but is instead stored in arrays. \n",
    "\n",
    "To work more effectively with the data, you need to convert it to primitive types and separate data for monoglot repos and polyglot repos.\n",
    "\n",
    "You can see three Python functions with the `@udf` annotation with their return type below. The annotation `@udf` is a short form of [User Defined Function](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.udf.html), which is used to extend the functions of the PySpark framework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2e56d855da76"
   },
   "outputs": [],
   "source": [
    "# Set the LIMIT constant as 10 to get top 10 result for further use.\n",
    "LIMIT = 10\n",
    "\n",
    "# A constant to explode the pie chart to distinguish each part more visible.\n",
    "EXPLODE_PIE_CHART = tuple([0.05] * LIMIT)\n",
    "\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def language_to_mono_language(language) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess function takes language and return it's name if language only has 1 elements.\n",
    "    Args:\n",
    "        language: list of struct that contains name and bytes.\n",
    "                  (e.g., language = [[name: \"C\", bytes: 300]]\n",
    "    Returns:\n",
    "        Monorepo's name\n",
    "    \"\"\"\n",
    "    return language[0].name if len(language) == 1 else None\n",
    "\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def language_to_mono_size(language) -> int:\n",
    "    \"\"\"\n",
    "    Preprocess function takes language and return it's bytes if language only has 1 elements.\n",
    "    Args:\n",
    "        language: list of struct that contains name and bytes.\n",
    "                  (e.g., language = [[name: \"C\", bytes: 300]]\n",
    "    Returns:\n",
    "        Monorepo's bytes\n",
    "    \"\"\"\n",
    "    return language[0].bytes if len(language) == 1 else 0\n",
    "\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def language_to_poly_language(language) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess function takes language and return the top 3 language's name based on their bytes.\n",
    "    Args:\n",
    "        language: list of struct that contains name and bytes.\n",
    "                  (e.g., language = [[name: \"C\", bytes: 300],\n",
    "                                     [name: \"Java\", bytes: 200]]\n",
    "    Returns:\n",
    "        Polyrepo's name in string form separate by commas\n",
    "    \"\"\"\n",
    "    if len(language) < 2:\n",
    "        return None\n",
    "    # Sort language by their bytes in a descending order.\n",
    "    language.sort(key=lambda x: -x.bytes)\n",
    "    top_3 = language[:3]\n",
    "\n",
    "    # Sort top_3 language by their name.\n",
    "    top_3.sort(key=lambda x: x.name)\n",
    "    ret = []\n",
    "    for elem in top_3:\n",
    "        ret.append(elem.name)\n",
    "    return \", \".join(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2e56d855da76"
   },
   "outputs": [],
   "source": [
    "# Create a new DataFrame called preprocessed_df, where the array was splitted into three columns using UDF.\n",
    "preprocessed_df = df.select(\n",
    "    col(\"repo_name\"),\n",
    "    language_to_mono_language(col(\"language\")).alias(\"mono_language\"),\n",
    "    language_to_mono_size(col(\"language\")).alias(\"mono_size\"),\n",
    "    language_to_poly_language(col(\"language\")).alias(\"poly_language\"),\n",
    ")\n",
    "preprocessed_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "668a2549b231"
   },
   "source": [
    "After preprocessing, you can see `preprocessed_df`'s schema, and the language column is separated into three columns, `mono_language`, `mono_size`, and `poly_language`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "efec0bfbfb65"
   },
   "outputs": [],
   "source": [
    "# See the number of repositories of monoglot(single language used) and polyglot(multiple languages used).\n",
    "mono = preprocessed_df.where(col(\"mono_language\").isNotNull()).count()\n",
    "print(f\"The number of repositories that use only one language is {mono}\")\n",
    "\n",
    "poly = preprocessed_df.where(col(\"poly_language\").isNotNull()).count()\n",
    "print(f\"The number of repositories that use multiple language is {poly}\")\n",
    "\n",
    "poly_percent = (poly / (mono + poly)) * 100\n",
    "print(f\"Polyglot repositories account for about {poly_percent:.2f}% of the total repo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0d79c26911f0"
   },
   "source": [
    "### Analyze\n",
    "\n",
    "#### Which language is the most frequently used among the monoglot repos?\n",
    "To answer this question, you can execute a query below with the preprocessed column, `mono_language`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a241cfdd169d"
   },
   "outputs": [],
   "source": [
    "# Get the monoglot repositories and sort them based on the popularity of languages.\n",
    "mono_ranking = (\n",
    "    preprocessed_df.groupBy(\"mono_language\")\n",
    "    .count()\n",
    "    .sort(desc(\"count\"))\n",
    "    .where(col(\"mono_language\").isNotNull())\n",
    ")\n",
    "mono_ranking.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9e5349dca8e6"
   },
   "source": [
    "Using this `mono_ranking`, you can also visualize it using the pie chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7b9eb5a4f47e"
   },
   "outputs": [],
   "source": [
    "# Convert Spark DataFrame to Pandas DataFrame to display the pie chart.\n",
    "mono_panda = mono_ranking.toPandas()[:LIMIT].copy()\n",
    "mono_panda.groupby([\"mono_language\"]).sum().plot(\n",
    "    kind=\"pie\",\n",
    "    y=\"count\",\n",
    "    autopct=\"%1.1f%%\",\n",
    "    label=\"\",\n",
    "    title=\"Monoglot repositories\",\n",
    "    legend=False,\n",
    "    figsize=(7, 7),\n",
    "    explode=EXPLODE_PIE_CHART,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cc68a7c604b4"
   },
   "source": [
    "#### What is the average size of each language among the monoglot repos?\n",
    "\n",
    "You can use preprocessed columns, `mono_size` and `mono_language` to get the average size of each language.\n",
    "\n",
    "the `mono_size` column's bytes are kilobytes. In the following query, `mono_size` is divided by 1000 to convert it to megabytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5d3e2e38df7"
   },
   "outputs": [],
   "source": [
    "mono_ranking_avg_bytes = (\n",
    "    preprocessed_df.groupBy(\"mono_language\")\n",
    "    .agg(\n",
    "        count(\"mono_language\").alias(\"count\"),\n",
    "        round(avg(\"mono_size\") / 1000).alias(\"average_in_MB\"),\n",
    "    )\n",
    "    .sort(desc(\"average_in_MB\"))\n",
    "    .where(col(\"mono_language\").isNotNull() & (col(\"count\") > 500))\n",
    ")\n",
    "\n",
    "mono_ranking_avg_bytes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4551e1b7b28b"
   },
   "source": [
    "#### Given a language, which other languages are most frequently found in polyglot repos with it?\n",
    "\n",
    "You already have a preprocessed column, `poly_language`. Using this column, you can implement a query to show the ranking of polyglot repositories with the top 3 languages, based on their size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8285a340a4a"
   },
   "outputs": [],
   "source": [
    "# Get the polyglot repositories by the popularity of languages.\n",
    "poly_ranking = (\n",
    "    preprocessed_df.groupBy(\"poly_language\")\n",
    "    .count()\n",
    "    .sort(desc(\"count\"))\n",
    "    .where(col(\"poly_language\").isNotNull())\n",
    ")\n",
    "\n",
    "poly_ranking.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "633a0752b053"
   },
   "source": [
    "The majority of results were a mixture of HTML or CSS, so the result had many similar conbinations of HTML, CSS, and Javascript.\n",
    "\n",
    "It may not be as interesting. What about the pie chart?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "30ae2d750ab7"
   },
   "outputs": [],
   "source": [
    "# Convert Spark DataFrame to Pandas DataFrame to display the pie chart.\n",
    "poly_panda = poly_ranking.toPandas()[:LIMIT].copy()\n",
    "poly_panda.groupby([\"poly_language\"]).sum().plot(\n",
    "    kind=\"pie\",\n",
    "    y=\"count\",\n",
    "    autopct=\"%1.1f%%\",\n",
    "    label=\"\",\n",
    "    title=\"Polyglot repositories\",\n",
    "    legend=False,\n",
    "    figsize=(7, 7),\n",
    "    explode=EXPLODE_PIE_CHART,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3905037bcea3"
   },
   "source": [
    "When you visualize to a pie chart with the top 10 result, eight out of ten contain either `HTML` or `CSS`.\n",
    "\n",
    "Using the original data fetched from BigQuery, `df`, you can create combinations of languages in each repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25479dcebd93"
   },
   "outputs": [],
   "source": [
    "# A Python package to get combinations.\n",
    "from itertools import combinations\n",
    "# A Python package to use type hint\n",
    "from typing import List\n",
    "\n",
    "# PySpark functions\n",
    "from pyspark.sql.functions import explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ad6a3e548fc3"
   },
   "outputs": [],
   "source": [
    "def normalize_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Change the name of language since BigQuery has a set of invalid characters that cannot be used in their field.\n",
    "    Args:\n",
    "        name: string\n",
    "    Returns:\n",
    "        Normalized name: string\n",
    "    \"\"\"\n",
    "    normalized_arr = []\n",
    "\n",
    "    # The following set of characters cannot be used in BigQuery's field.\n",
    "    invalid_chars = {\",\", \";\", \"{\", \"}\", \"(\", \")\", \"\\n\", \"\\t\", \"=\"}\n",
    "\n",
    "    # The name must start with a letter or underscore.\n",
    "    if name[0].isnumeric():\n",
    "        normalized_arr.append(\"_\")\n",
    "\n",
    "    for ch in name:\n",
    "        # Skip if a character is in the set of invalid characters.\n",
    "        if ch in invalid_chars:\n",
    "            continue\n",
    "\n",
    "        # Convert space or dot to underscore\n",
    "        elif ch == \" \" or ch == \".\":\n",
    "            normalized_arr.append(\"_\")\n",
    "\n",
    "        # Lower the character to merge the same name (e.g., \"Java\" and \"java\")\n",
    "        else:\n",
    "            normalized_arr.append(ch.lower())\n",
    "\n",
    "    # Convert the array to string\n",
    "    return \"\".join(normalized_arr)\n",
    "\n",
    "\n",
    "def reduce_language(language) -> List[str]:\n",
    "    \"\"\"\n",
    "    Preprocess function takes language and reduce it to remove \"bytes\".\n",
    "    Args:\n",
    "        language: list of struct that contains name and bytes.\n",
    "                  (e.g., language = [[name: \"C\", bytes: 300],\n",
    "                                     [name: \"Java\", bytes: 200]]\n",
    "    Returns:\n",
    "        list of strings that contains name.\n",
    "                  (e.g., reduced_languages = [\"C\", \"Java\"])\n",
    "    \"\"\"\n",
    "    if len(language) < 2:\n",
    "        return None\n",
    "    reduced_languages = []\n",
    "    for elem in language:\n",
    "        # To write back to BigQuery, the name must be normalized. See normalize_name() function below.\n",
    "        normalized_name = normalize_name(elem.name)\n",
    "        reduced_languages.append(normalized_name)\n",
    "    return reduced_languages\n",
    "\n",
    "\n",
    "def preprocess_combination(language) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Preprocess function takes language and return every combination of language.\n",
    "    Args:\n",
    "        language: list of struct that contains name and bytes.\n",
    "                  (e.g., language = [[name: \"C\", bytes: 300],\n",
    "                                     [name: \"Java\", bytes: 200]]\n",
    "    Returns:\n",
    "        List of every possible combinations.\n",
    "                  (e.g., arr_combinations = [[\"C\", \"Java\"], [\"Java\", \"C\"]])\n",
    "    \"\"\"\n",
    "    if not language:\n",
    "        return None\n",
    "    arr_combinations = []\n",
    "    for combination in combinations(language, 2):\n",
    "        arr_combinations.append(combination)\n",
    "        arr_combinations.append(combination[::-1])\n",
    "    return arr_combinations\n",
    "\n",
    "\n",
    "# Preprocess \"reduced_languages\" column using UDF.\n",
    "df = df.withColumn(\n",
    "    \"reduced_languages\",\n",
    "    UserDefinedFunction(reduce_language, ArrayType(StringType()))(col(\"language\")),\n",
    ")\n",
    "\n",
    "# Preprocess \"combinations\" column using UDF.\n",
    "df = df.withColumn(\n",
    "    \"combinations\",\n",
    "    UserDefinedFunction(preprocess_combination, ArrayType(ArrayType(StringType())))(\n",
    "        col(\"reduced_languages\")\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Create another DataFrame from df that has repo_name and combinations as columns.\n",
    "frequency_df = df.select(col(\"repo_name\"), col(\"combinations\")).where(\n",
    "    size(col(\"language\")) > 1\n",
    ")\n",
    "frequency_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5e80986d0095"
   },
   "source": [
    "Now frequency_df has repo name and combinations of languages.\n",
    "\n",
    "Next, you need to use explode function in Spark, which is similar to UNNEST function in SQL.\n",
    "\n",
    "Note that the following table is what your current table looks like.\n",
    "\n",
    "| repo_name   | combinations      |\n",
    "| :----------: | :---------------: |\n",
    "| a           | [['C', 'C++'], ['C++', 'C'], ['C', 'Java'], ['Java', 'C’], ['C++', 'Java'], ['Java', 'C++']]|\n",
    "| b           | [['C', 'C++'], ['C++', 'C'], ['C', 'Python'], ['Python', 'C'], ['C++', 'Python'], ['Python', 'C++']]|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ec92c4d0652"
   },
   "outputs": [],
   "source": [
    "# Using explode(), the elements in combinations converted to rows.\n",
    "frequency_df = frequency_df.withColumn(\"languages\", explode(col(\"combinations\")))\n",
    "\n",
    "# Create columns for combinations of languages.\n",
    "frequency_df = frequency_df.withColumn(\"language0\", col(\"languages\")[0])\n",
    "frequency_df = frequency_df.withColumn(\"language1\", col(\"languages\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5e80986d0095"
   },
   "source": [
    "After exploded and adding two Columns `langauge0` and `language1`, `frequency_df` looks like the following.\n",
    "\n",
    "| repo_name   | languages         | language0    | language1 |\n",
    "| :---------: | :---------------: | :--------:   | :-------: |\n",
    "| a           | ['C', 'C++']      | 'C'          |'C++'      |\n",
    "| a           | ['C++', 'C']      | 'C++'        |'C'        |\n",
    "| a           | ['C', 'Java']     | 'C'          |'Java'     |\n",
    "| a           | ['Java', 'C’]     | 'Java'       |'C'        |\n",
    "| a           | ['C++', 'Java']   | 'C++'        |'Java'     |\n",
    "| a           | ['Java', 'C++']   | 'Java'       |'C++'      |\n",
    "| b           | ['C', 'C++']      | 'C'          |'C++'      |\n",
    "| b           | ['C++', 'C']      | 'C++'        |'C'        |\n",
    "| b           | ['C', 'Python']   | 'C'          |'Python'   |\n",
    "| b           | ['Python', 'C']   | 'Python'     |'C'        |\n",
    "| b           | ['C++', 'Python'] | 'C++'        |'Python'   |\n",
    "| b           | ['Python', 'C++'] | 'Python'     |'C++'      |\n",
    "\n",
    "Now you are going to calculate a pair-wise frequency table of given `language0` and `language1` columns. The first column of each row will be the distinct values of `language0` and the column names will be the distinct values of `language1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ec92c4d0652"
   },
   "outputs": [],
   "source": [
    "# crosstab() reshapes the table into the frequency distribution table by using cross tabulations.\n",
    "frequency_df = frequency_df.crosstab(\"language0\", \"language1\").withColumnRenamed(\n",
    "    \"language0_language1\", \"languages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2c79c4a3326"
   },
   "source": [
    "After applying `crosstab()` to `frequency_df`, the DataFrame looks like the following.\n",
    "\n",
    "| languages  |  C  | C++ | Java | Python |\n",
    "| :--------: | :-: | :-: | :-:  |  :-:   |\n",
    "|     C      |  0  |  2  |  1   |   1    |\n",
    "|     C++    |  2  |  0  |  1   |   1    |\n",
    "|    Java    |  1  |  1  |  0   |   0    |\n",
    "|   Python   |  1  |  1  |  0   |   0    |\n",
    "\n",
    "Note that this table is for example and not the real data.\n",
    "\n",
    "See [frequency distribution](https://en.wikipedia.org/wiki/Frequency_(statistics)) and [cross tabulations](https://en.wikipedia.org/wiki/Contingency_table).\n",
    "\n",
    "Now you have a DataFrame that has frequency of each languages by a given language. Let's visualize it with some famous language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ee1591ca4563"
   },
   "outputs": [],
   "source": [
    "# Set of famous languages. You can modify this set to show your favorite languages.\n",
    "MAJOR_LANGUAGES = {\"C\", \"C++\", \"Java\", \"Python\", \"JavaScript\", \"Go\"}\n",
    "\n",
    "# Declare a dictionary to store key as language name and value as the selected DataFrame\n",
    "df_dict = dict()\n",
    "\n",
    "for language in MAJOR_LANGUAGES:\n",
    "    # Get a top ten languages of each language and store it to the dictionary.\n",
    "    df_dict[language] = (\n",
    "        frequency_df.select(col(\"languages\"), language).sort(-col(language)).limit(10)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5951469b9925"
   },
   "outputs": [],
   "source": [
    "for language in df_dict:\n",
    "    # Convert Spark DataFrame to Pandas DataFrame to display the bar chart.\n",
    "    elem_panda = df_dict[language].toPandas()[:LIMIT].copy()\n",
    "    elem_panda.set_index(\"languages\", inplace=True)\n",
    "    elem_panda.sort_values(language, ascending=True, inplace=True)\n",
    "    elem_panda.plot(\n",
    "        kind=\"barh\",\n",
    "        title=language,\n",
    "        legend=False,\n",
    "        xlabel=\"\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "### Write back to BigQuery\n",
    "\n",
    "After analyzing these queries, we have several DataFrames: the ranking of monoglot repositories, the average bytes of monoglot repositories, and the frequency table of each language being used in a repository. \n",
    "\n",
    "In this project, these three DataFrames will be stored in BigQuery using the [spark-bigquery-connector](https://github.com/GoogleCloudDataproc/spark-bigquery-connector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8ab41bf47dd"
   },
   "outputs": [],
   "source": [
    "dataframes = {\n",
    "    \"mono_ranking\": mono_ranking,\n",
    "    \"mono_ranking_avg_bytes\": mono_ranking_avg_bytes,\n",
    "    \"frequency_table\": frequency_df,\n",
    "}\n",
    "\n",
    "# Iterate through dataframes and save them to the BigQuery.\n",
    "for df in dataframes:\n",
    "    dataframes[df].write.format(\"bigquery\").option(\n",
    "        \"temporaryGcsBucket\", DATASET_NAME\n",
    "    ).option(\"table\", f\"{DATASET_NAME}.{df}\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "If there is no error above, congratulations! Your DataFrame is successfully stored in your BigQuery.\n",
    "\n",
    "You can find the data via [this link](https://pantheon.corp.google.com/bigquery) or execute `bq` command-line tool like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7734cef2fc3"
   },
   "outputs": [],
   "source": [
    "QUERY = f\"SELECT languages, python FROM {PROJECT_ID}.{DATASET_NAME}.frequency_table ORDER BY python DESC LIMIT 10\"\n",
    "\n",
    "! bq query --nouse_legacy_sql $QUERY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "### Delete Vertex AI Workbench - Managed Notebook\n",
    "\n",
    "To delete Vertex Ai Workbench - Managed Notebook used in this project, you can use this [Clean up](https://cloud.google.com/vertex-ai/docs/workbench/managed/create-managed-notebooks-instance-console-quickstart#clean-up) part of `Managed notebooks` page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "### Delete a Dataproc Cluster\n",
    "\n",
    "To delete a Dataproc Cluster, you can use this [Deleting a cluster](https://cloud.google.com/dataproc/docs/guides/manage-cluster#deleting_a_cluster) part of `Manage a cluster` page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7a1a1a5e978"
   },
   "outputs": [],
   "source": [
    "# Delete Google Cloud Storage bucket\n",
    "! gsutil rm -r $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b77295420ab9"
   },
   "outputs": [],
   "source": [
    "# Delete BigQuery dataset\n",
    "! bq rm -r -f $DATASET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "After you delete the BigQuery dataset, you can check your Datasets in BigQuery using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c53e6e169788"
   },
   "outputs": [],
   "source": [
    "! bq ls"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "spark_sample_notebook.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-managed-notebook?download_url=https://raw.githubusercontent.com/hyunuk/vertex-ai-samples/experiment/notebooks/official/workbench/spark/spark_sample_notebook.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>                                                                                               \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO",
    "tags": []
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how you ingest, analyze, and write back data to BigQuery using Apache Spark on Dataproc Serverless. Using the GitHub Activity Data, You will analyze repositories in GitHub and find out what kind of programming languages being used in their repositories.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The dataset you are using is the [GitHub Activity Data](https://console.cloud.google.com/marketplace/product/github/github-repos), available in [BigQuery Public Datasets](https://cloud.google.com/bigquery/public-data). The first 1TB of data queried each month is free.\n",
    "\n",
    "### Objective\n",
    "\n",
    "This notebook demonstrates Apache Spark jobs that fetch data from BigQuery, analyze it, and write the results back to BigQuery. Through this process, you can learn a common use case in data engineering: ingesting data from a database, performing transformations during preprocessing, and writing back to another database. You also learn how to submit Apache Spark jobs in the Dataproc Serverless environment on Google Cloud Platform. \n",
    "\n",
    "In this project, these questions below will be answered.\n",
    "\n",
    "- Which language is the most frequently used among the monoglot repos?\n",
    "- What is the average size of each language among the monoglot repos?\n",
    "- Given a language, which other languages are most frequently found in polyglot repos with it?\n",
    "\n",
    "Note: repositories that encompass polyglot programming are referred to as polyglot repos and those which only contain one programming language are referred to as monoglot repos.\n",
    "\n",
    "\n",
    "The steps performed include the following:\n",
    "\n",
    "- Setting up the serverless environment.\n",
    "- Configuring spark-bigquery-connector.\n",
    "- Ingesting data from BigQuery to Spark DataFrame.\n",
    "- Preprocessing ingested data.\n",
    "- Analyze that the most frequently used programming language among the monoglot repos.\n",
    "- Analyze that the average size of each language among the monoglot repos.\n",
    "- Analyze that the most frequently used languages with a given language in polyglot repos.\n",
    "- Write the result back to BigQuery\n",
    "- Delete Dataproc Serverless Session\n",
    "- Disable APIs being used in the project.\n",
    "\n",
    "### Costs \n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "* Dataproc Serverless\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), [Dataproc Serverless pricing](https://cloud.google.com/dataproc-serverless/pricing)\n",
    "and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY",
    "tags": []
   },
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. [Enable Notebooks API, Vertex AI API, and Cloud Dataproc API](https://console.cloud.google.com/flows/enableapi?apiid=notebooks.googleapis.com,aiplatform.googleapis.com,dataproc&_ga=2.209429842.1903825585.1657549521-326108178.1655322249)\n",
    "\n",
    "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "### Create a Dataproc cluster\n",
    "\n",
    "**Note**: If you already have a cluster on Dataproc, you can skip this part and go to `Switch your kernel`.\n",
    "\n",
    "The Spark job that you are going to execute this project is compute-intensive and could take a lot of time on a standard Notebook environment, so this tutorial uses a Dataproc cluster. To run your Spark jobs on a Dataproc cluster, you need to create a cluster with component gateway enabled and JupyterLab extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "CLUSTER_NAME = \"[your-cluster]\" # @param {type: \"string\"}\n",
    "CLUSTER_REGION = \"[your-region]\" # @param {type: \"string\"}\n",
    "\n",
    "if CLUSTER_REGION == \"[your-region]\":\n",
    "    CLUSTER_REGION = \"us-central1\"\n",
    "\n",
    "print(f\"CLUSTER_NAME: {CLUSTER_NAME}\")\n",
    "print(f\"CLUSTER_REGION: {CLUSTER_REGION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud dataproc clusters create $CLUSTER_NAME \\\n",
    "    --region=$CLUSTER_REGION \\\n",
    "    --enable-component-gateway \\\n",
    "    --optional-components=JUPYTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "**Note**: Your `CLUSTER_NAME` must be unique and must start with a lowercase letter followed by up to 51 lowercase letters, numbers, and hyphens, and cannot end with a hyphen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "#### Switch your kernel\n",
    "\n",
    "**Note**: If you already select your kernel as Python 3 on your Dataproc cluster, you can skip this part.\n",
    "\n",
    "In order to execute Apache Spark jobs on Dataproc Clusters, you need to change your kernel to Python 3 on your cluster name.\n",
    "\n",
    "Click the button on top-right corner and select `Python 3 on CLUSTER_NAME: Dataproc cluster in REGION (Remote)`\n",
    "\n",
    "If you switch your kernel here, you will lost all variables declared above, so you must execute following code after switching your kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJYoRfYng0XZ"
   },
   "source": [
    "Otherwise, set your project ID here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"[your-project-id]\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  We recommend that you choose the region closest to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06571eb4063b"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append it onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "697568e92bd6"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "#### Create a Cloud Storage bucket\n",
    "\n",
    "The Spark DataFrame created during this project will be stored in BigQuery. The data will be written first to the bucket in Google Cloud Storage(GCS) and then it is loaded it to BigQuery.\n",
    "A GCS bucket must be configured to indicate the temporary data location.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all Cloud Storage buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "\n",
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
    "    BUCKET_NAME = f\"{PROJECT_ID}{TIMESTAMP}\"\n",
    "    BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucvCsknMCims"
   },
   "source": [
    "Validate access to your Cloud Storage bucket by displaying the bucket's metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -L -b $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "#### Create a BigQuery resource\n",
    "\n",
    "Using BUCKET_NAME created above, create a BigQuery resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [],
   "source": [
    "! bq mk $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "## Tutorial\n",
    "\n",
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "# A Spark Session is how you interact with Spark SQL to create Dataframes\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# PySpark functions\n",
    "from pyspark.sql.functions import col, size, UserDefinedFunction\n",
    "\n",
    "# These allow us to create a schema for our data\n",
    "from pyspark.sql.types import ArrayType, IntegerType, StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### Initialize the SparkSession and fetch data from BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('spark-bigquery-polyglot-language-demo') \\\n",
    "    .config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.25.2.jar') \\\n",
    "    .config('spark.sql.debug.maxToStringFields', '500') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load Github Activity Public Dataset from BigQuery.\n",
    "df = spark.read.format('bigquery') \\\n",
    "  .option('table', 'bigquery-public-data.github_repos.languages') \\\n",
    "  .load()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Based on the schema printed above, data of the GitHub Activity is not stored in primitive types, but is instead stored in arrays. \n",
    "\n",
    "To work more effectively with the data, you need to preprocess it to primitive types and separate data for monoglot repos and polyglot repos. Once you create preprocessed columns, it makes our future tasks much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for further use.\n",
    "LIMIT = 10\n",
    "EXPLODE_PIE_CHART = tuple([.05] * LIMIT)\n",
    "\n",
    "\n",
    "def language_to_mono_language(language) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess function takes language and return it's name if language only has 1 elements.\n",
    "    Args:\n",
    "        language: list of struct that contains name and bytes. \n",
    "                  (e.g., language = [[name: \"C\", bytes: 300]]\n",
    "    Returns:\n",
    "        Monorepo's name\n",
    "    \"\"\"\n",
    "    if len(language) != 1:\n",
    "        return None\n",
    "    return language[0].name\n",
    "\n",
    "def language_to_mono_size(language) -> int:\n",
    "    \"\"\"\n",
    "    Preprocess function takes language and return it's bytes if language only has 1 elements.\n",
    "    Args:\n",
    "        language: list of struct that contains name and bytes. \n",
    "                  (e.g., language = [[name: \"C\", bytes: 300]]\n",
    "    Returns:\n",
    "        Monorepo's bytes\n",
    "    \"\"\"\n",
    "    if len(language) != 1:\n",
    "        return 0\n",
    "    return language[0].bytes\n",
    "\n",
    "def language_to_poly_language(language) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess function takes language and return the top 3 language's name based on their bytes.\n",
    "    Args:\n",
    "        language: list of struct that contains name and bytes. \n",
    "                  (e.g., language = [[name: \"C\", bytes: 300], \n",
    "                                     [name: \"Java\", bytes: 200]]\n",
    "    Returns:\n",
    "        Polyrepo's name in string form separate by commas\n",
    "    \"\"\"\n",
    "    if len(language) < 2:\n",
    "        return None\n",
    "    # Sort language by their bytes in a descending order.\n",
    "    language.sort(key=lambda x : -x.bytes)\n",
    "    top_3 = language[:3]\n",
    "    \n",
    "    # Sort top_3 language by their name.\n",
    "    top_3.sort(key=lambda x : x.name)\n",
    "    ret = []\n",
    "    for elem in top_3:\n",
    "        ret.append(elem.name)\n",
    "    return ', '.join(ret)\n",
    "\n",
    "# Copy over df to preprocessed_df\n",
    "preprocessed_df = df.alias(\"preprocessed_df\")\n",
    "\n",
    "# Declare dictionary with keys column names and values User Defined Functions (UDF) and return types.\n",
    "udf_map = {\n",
    "    \"mono_language\": (language_to_mono_language, StringType()),\n",
    "    \"mono_size\": (language_to_mono_size, IntegerType()),\n",
    "    \"poly_language\": (language_to_poly_language, StringType()),\n",
    "}\n",
    "\n",
    "# Iterate through udf_map to preprocess columns using UDF.\n",
    "for name, udf in udf_map.items():\n",
    "    preprocessed_df = preprocessed_df.withColumn(\n",
    "        name,\n",
    "        UserDefinedFunction(*udf)(col(\"language\"))\n",
    "    )\n",
    "\n",
    "# Drop the language column.\n",
    "preprocessed_df = preprocessed_df.drop(\"language\")\n",
    "# Create a temporary view for using SparkSQL.\n",
    "preprocessed_df.createOrReplaceTempView('df_view')\n",
    "preprocessed_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preprocessing, the language column is dropped and it is separated into three columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the number of repositories of monoglot(single language used) and polyglot(multiple languages used).\n",
    "mono = preprocessed_df.where(size(col(\"language\")) == 1).count()\n",
    "print(f\"The number of repositories that use only one language is {mono}\")\n",
    "\n",
    "poly = preprocessed_df.where(size(col(\"language\")) > 1).count()\n",
    "print(f\"The number of repositories that use multiple language is {poly}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 60% of repositories uses more than two languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze\n",
    "\n",
    "#### Which language is the most frequently used among the monoglot repos?\n",
    "To answer this question, you can execute simple query below with the preprocessed column, `mono_language`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the monoglot repositories and sort them based on the popularity of languages.\n",
    "mono_ranking = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        df_view.mono_language,\n",
    "        count(df_view.mono_language) AS `count`\n",
    "    FROM\n",
    "        df_view\n",
    "    GROUP BY\n",
    "        df_view.mono_language\n",
    "    ORDER BY\n",
    "        `count` DESC\n",
    "\"\"\")\n",
    "\n",
    "mono_ranking.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this `mono_ranking`, you can also visualize it using the pie chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Spark DataFrame to Pandas DataFrame to display the pie chart.\n",
    "mono_panda = mono_ranking.toPandas()[:LIMIT].copy()\n",
    "mono_panda.groupby(['mono_language']) \\\n",
    "        .sum() \\\n",
    "        .plot( \\\n",
    "            kind='pie', \\\n",
    "            y='count', \\\n",
    "            autopct='%1.1f%%', \\\n",
    "            label='', \\\n",
    "            title='Monoglot repositories', \\\n",
    "            legend=False, \\\n",
    "            figsize=(7, 7), \\\n",
    "            explode=EXPLODE_PIE_CHART \\\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the average size of each language among the monoglot repos?\n",
    "\n",
    "You can use preprocessed columns, `mono_size` and `mono_language` to get the average size of each language.\n",
    "\n",
    "the `mono_size` column's bytes are kilobytes. In the following query, `mono_size` is divided by 1000 to convert it to megabyte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the average size in megabyte of each monoglot repositories and sort by it's size\n",
    "mono_ranking_avg_bytes = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        df_view.mono_language, \n",
    "        round(avg(df_view.mono_size/1000)) AS `average_in_MB`,\n",
    "        count(df_view.mono_language) AS `count`\n",
    "    FROM \n",
    "        df_view\n",
    "    GROUP BY\n",
    "        df_view.mono_language\n",
    "    ORDER BY\n",
    "        `average_in_MB` DESC\n",
    "\"\"\")\n",
    "\n",
    "# Filter the result that the language has at least 500 repositories.\n",
    "mono_ranking_avg_bytes = mono_ranking_avg_bytes.filter(col('count') > 500)\n",
    "mono_ranking_avg_bytes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given a language, which other languages are most frequently found in polyglot repos with it?\n",
    "\n",
    "You already have a preprocessed column, `poly_language`. Using this column, you can implement a simple query to show the ranking of polyglot repositories with the top 3 languages, based on their size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the polyglot repositories by the popularity of languages.\n",
    "poly_ranking = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        df_view.poly_language,\n",
    "        count(df_view.poly_language) AS count\n",
    "    FROM\n",
    "        df_view\n",
    "    GROUP BY\n",
    "        df_view.poly_language\n",
    "    ORDER BY\n",
    "        count DESC\n",
    "\"\"\")\n",
    "\n",
    "poly_ranking.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of results were a mixture of HTML or CSS, so the result had many similar conbinations of HTML, CSS, and Javascript.\n",
    "\n",
    "It may not be as interesting. What about the pie chart?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Spark DataFrame to Pandas DataFrame to display the pie chart.\n",
    "poly_panda = poly_ranking.toPandas()[:LIMIT].copy()\n",
    "poly_panda.groupby(['poly_language']) \\\n",
    "        .sum() \\\n",
    "        .plot( \\\n",
    "            kind='pie', \\\n",
    "            y='count', \\\n",
    "            autopct='%1.1f%%', \\\n",
    "            label='', \\\n",
    "            title='Polyglot repositories', \\\n",
    "            legend=False, \\\n",
    "            figsize=(7, 7), \\\n",
    "            explode=EXPLODE_PIE_CHART \\\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you visualize to a pie chart with the top 10 result, eight out of ten were contains either `HTML` or `CSS`.\n",
    "\n",
    "Using the original data fetched from BigQuery, `df`, you can create combinations of languages in each repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Python package to get combinations.\n",
    "from itertools import combinations\n",
    "# A Python package to use type hint\n",
    "from typing import List\n",
    "\n",
    "# PySpark functions\n",
    "from pyspark.sql.functions import explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_language(language) -> List[str]:\n",
    "    \"\"\"\n",
    "    Preprocess function takes language and reduce it to remove \"bytes\".\n",
    "    Args:\n",
    "        language: list of struct that contains name and bytes. \n",
    "                  (e.g., language = [[name: \"C\", bytes: 300], \n",
    "                                     [name: \"Java\", bytes: 200]]\n",
    "    Returns:\n",
    "        list of strings that contains name.\n",
    "                  (e.g., reduced_languages = [\"C\", \"Java\"])\n",
    "    \"\"\"\n",
    "    if len(language) < 2:\n",
    "        return None\n",
    "    reduced_languages = []\n",
    "    for elem in language:\n",
    "        # To write back to BigQuery, the name must be normalized. See normalize_name() function below.\n",
    "        normalized_name = normalize_name(elem.name)\n",
    "        reduced_languages.append(normalized_name)\n",
    "    return reduced_languages\n",
    "\n",
    "def preprocess_combination(language) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Preprocess function takes language and return every combination of language.\n",
    "    Args:\n",
    "        language: list of struct that contains name and bytes. \n",
    "                  (e.g., language = [[name: \"C\", bytes: 300], \n",
    "                                     [name: \"Java\", bytes: 200]]\n",
    "    Returns:\n",
    "        List of every possible combinations.\n",
    "                  (e.g., arr_combinations = [[\"C\", \"Java\"], [\"Java\", \"C\"]])\n",
    "    \"\"\"\n",
    "    if not language:\n",
    "        return None\n",
    "    arr_combinations = []\n",
    "    for combination in combinations(language, 2):\n",
    "        arr_combinations.append(combination)\n",
    "        arr_combinations.append(combination[::-1])\n",
    "    return arr_combinations\n",
    "\n",
    "def normalize_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Change the name of language since BigQuery has a set of invalid characters that cannot be used in their field.\n",
    "    Args:\n",
    "        name: string\n",
    "    Returns:\n",
    "        Normalized name: string\n",
    "    \"\"\"\n",
    "    normalized_arr = []\n",
    "    \n",
    "    # The following set of characters cannot be used in BigQuery's field.\n",
    "    invalid_chars = {\",\", \";\", \"{\", \"}\", \"(\", \")\", \"\\n\", \"\\t\", \"=\"}\n",
    "\n",
    "    # The name must start with a letter or underscore.\n",
    "    if name[0].isnumeric():\n",
    "        normalized_arr.append(\"_\")\n",
    "        \n",
    "    for ch in name:\n",
    "        # Skip if a character is in the set of invalid characters.\n",
    "        if ch in invalid_chars:\n",
    "            continue\n",
    "        \n",
    "        # Convert space or dot to underscore\n",
    "        elif ch == \" \" or ch == \".\":\n",
    "            normalized_arr.append(\"_\")\n",
    "        \n",
    "        # Lower the character to merge the same name (e.g., \"Java\" and \"java\")\n",
    "        else:\n",
    "            normalized_arr.append(ch.lower())\n",
    "\n",
    "    # Convert the array to string\n",
    "    return ''.join(normalized_arr)\n",
    "\n",
    "# Preprocess \"reduced_languages\" column using UDF.\n",
    "df = df.withColumn(\n",
    "        \"reduced_languages\",\n",
    "        UserDefinedFunction(\n",
    "            reduce_language,\n",
    "            ArrayType(StringType())\n",
    "        )(col(\"language\"))\n",
    "    )\n",
    "\n",
    "# Preprocess \"combinations\" column using UDF.\n",
    "df = df.withColumn(\n",
    "        \"combinations\",\n",
    "        UserDefinedFunction(\n",
    "            preprocess_combination, \n",
    "            ArrayType(ArrayType(StringType()))\n",
    "        )(col(\"reduced_languages\"))\n",
    "    )\n",
    "\n",
    "# Create another DataFrame from df that has repo_name and combinations as columns.\n",
    "frequency_df = df.select(col(\"repo_name\"), col(\"combinations\")).where(size(col(\"language\")) > 1)\n",
    "frequency_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now frequency_df has repo and combinations of languages.\n",
    "\n",
    "Next, you need to use explode function in Spark, which is similar with UNNEST function in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using explode(), the elements in combinations converted to rows.\n",
    "frequency_df = frequency_df.withColumn(\"languages\", explode(col(\"combinations\")))\n",
    "\n",
    "# Create columns for combinations of languages.\n",
    "frequency_df = frequency_df.withColumn(\"language0\", col('languages')[0])\n",
    "frequency_df = frequency_df.withColumn(\"language1\", col('languages')[1])\n",
    "\n",
    "# crosstab() reshapes the current table into the frequency distribution table by using cross tabulations.\n",
    "frequency_df = frequency_df.crosstab(\"language0\", \"language1\")\n",
    "\n",
    "# change the language0_language1 to languages.\n",
    "frequency_df = frequency_df.withColumnRenamed(\"language0_language1\", \"languages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [frequency distribution](https://en.wikipedia.org/wiki/Frequency_(statistics)) and [cross tabulations](https://en.wikipedia.org/wiki/Contingency_table).\n",
    "\n",
    "**Below cells are WIP.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAJOR_LANGUAGES = {\"C\", \"C++\", \"Java\", \"Python\", \"JavaScript\", \"Go\"}\n",
    "df_dict = dict()\n",
    "\n",
    "for language in MAJOR_LANGUAGES:\n",
    "    df_dict[language] = frequency_df.select(col(\"languages\"), language).sort(-col(language)).limit(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in df_dict:\n",
    "    elem_panda = df_dict[language].toPandas()[:LIMIT].copy()\n",
    "    elem_panda.set_index(\"languages\", inplace=True)\n",
    "    elem_panda.plot( \\\n",
    "        kind='bar', \\\n",
    "        title=language, \\\n",
    "        legend=False, \\\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {\n",
    "    \"mono_ranking\": mono_ranking,\n",
    "    \"mono_ranking_avg_bytes\": mono_ranking_avg_bytes,\n",
    "    \"poly_ranking\": poly_ranking,\n",
    "    \"frequency_df\": frequency_df\n",
    "    }\n",
    "# Saving the data to BigQuery\n",
    "for df in dataframes:\n",
    "    dataframes[df].write.format('bigquery') \\\n",
    "          .option(\"temporaryGcsBucket\", BUCKET_NAME) \\\n",
    "          .option('table', f\"{BUCKET_NAME}.{df}\") \\\n",
    "          .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## Cleaning up - TODO\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n",
    "{TODO: Include commands to delete individual resources below}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx_vKniMq9ZX"
   },
   "outputs": [],
   "source": [
    "# Delete endpoint resource\n",
    "! gcloud ai endpoints delete $ENDPOINT_NAME --quiet --region $REGION_NAME\n",
    "\n",
    "# Delete model resource\n",
    "! gcloud ai models delete $MODEL_NAME --quiet\n",
    "\n",
    "# Delete Cloud Storage objects that were created\n",
    "! gsutil -m rm -r $JOB_DIR\n",
    "\n",
    "if os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ur8xi4C7S06n"},"outputs":[],"source":["# Copyright 2022 Google LLC\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."]},{"cell_type":"markdown","metadata":{"id":"XoEqT2Y4DJmf"},"source":["## Tutorial\n","\n","### Import required libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pRUOFELefqf1"},"outputs":[],"source":["# A Spark Session is how you interact with Spark SQL to create Dataframes\n","from pyspark.sql import SparkSession\n","# PySpark functions\n","from pyspark.sql.functions import avg, col, count, desc, round, size, udf, to_timestamp, unix_timestamp, broadcast, pandas_udf, PandasUDFType, to_date\n","# These allow us to create a schema for our data\n","from pyspark.sql.types import ArrayType, IntegerType, StringType\n","\n","from pyspark.ml.linalg import Vectors\n","from geopandas import gpd\n","from shapely import wkt\n","from shapely.geometry import Point\n","import pandas as pd\n","from pyspark.ml.regression import LinearRegression, GBTRegressor, DecisionTreeRegressor, RandomForestRegressor\n","import matplotlib.pyplot as plt\n","from pyspark.ml.stat import Correlation\n","from pyspark.ml.feature import VectorAssembler, StandardScaler\n","from pyspark.ml.evaluation import RegressionEvaluator"]},{"cell_type":"markdown","metadata":{"id":"init_aip:mbsdk,all"},"source":["### Initialize the SparkSession\n","\n","To use Apache Spark with BigQuery, you must include the [spark-bigquery-connector](https://github.com/GoogleCloudDataproc/spark-bigquery-connector) when you initialize the SparkSession."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"init_aip:mbsdk,all"},"outputs":[],"source":["# Initialize the SparkSession with the following config.\n","spark = (\n","    SparkSession.builder.appName(\"spark-bigquery-ml-nyc-trips-demo\")\n","    .config(\n","        \"spark.jars\",\n","        \"gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.25.2.jar\",\n","    )\n","    .config(\"spark.sql.debug.maxToStringFields\", \"500\")\n","    .getOrCreate()\n",")\n","spark.sparkContext.setLogLevel('WARN')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"init_aip:mbsdk,all"},"source":["### Fetch data from BigQuery"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["gdf_zone = gpd.read_file(\"https://data.cityofnewyork.us/api/geospatial/d3c5-ddgc?method=export&format=GeoJSON\")\n","gdf_zone['location_id'] = gdf_zone['location_id'].astype('long')\n","print(gdf_zone.info())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"init_aip:mbsdk,all"},"outputs":[],"source":["MANHATTAN = {153, 128, 127, 243, 120, 244, 116, 152, 42, 166, 41, 74, 24, 151, 238, 239, 143, 142, 43, 75, 236, 263, 262, 237, 141, 140, 50, 48, 163, 230, 161, 162, 229, 233, 170, 164, 100, 68, 246, 186, 90, 234, 107, 137, 224, 4, 79, 113, 114, 249, 158, 125, 211, 144, 148, 232, 45, 231, 209, 87, 13, 261, 12, 88}\n","\n","# Load NYC_taxi in Github Activity Public Dataset from BigQuery.\n","taxi_df = (\n","    spark.read.format(\"bigquery\")\n","    .option(\"table\", \"bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2018\")\n","    .load()\n",")\n","taxi_df.printSchema()\n","\n","# Load NYC_Citibike in Github Acitivity Public dataset from BQ.\n","bike_df = (\n","    spark.read.format(\"bigquery\")\n","    .option(\"table\", \"bigquery-public-data.new_york_citibike.citibike_trips\")\n","    .load()\n",")\n","# bike_df = bike_df.dropna()\n","bike_df.show(5)\n","\n","taxi_zone_geom = (\n","    spark.read.format(\"bigquery\")\n","    .option(\"table\", \"bigquery-public-data.new_york_taxi_trips.taxi_zone_geom\")\n","    .load()\n",")\n","\n","taxi_zone_geom.show()\n","\n","# convert string starttime to unix timestamp"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["@udf(returnType=IntegerType())\n","def preprocessing_days_of_week(timestamp):\n","    return ((timestamp // 86400) + 4) % 7 if timestamp else None\n","\n","@udf(returnType=IntegerType())\n","def preprocessing_time(timestamp):\n","    return timestamp % 1440 if timestamp else None\n","\n","@udf(returnType=StringType())\n","def preprocessing_borough(lat, lon):\n","    point_var = [Point(xy) for xy in zip(lon, lat)]\n","    gdf_points = gpd.GeoDataFrame(pd.DataFrame({'lat': lat, 'lon': lon}), crs='epsg:4326', geometry=point_var)\n","    gdf_joined = gpd.sjoin(gdf_points, gdf_zone, how='left')\n","    return gdf_joined['borough']\n","\n","@udf(returnType=IntegerType())\n","def preprocessing_man(zone):\n","    return zone if zone in MANHATTAN else -1\n","\n","@pandas_udf('string')\n","def preprocess_zone_name(lat: pd.Series, lon: pd.Series) -> pd.Series:\n","    point_var = [Point(xy) for xy in zip(lon, lat)]\n","    gdf_points = gpd.GeoDataFrame(pd.DataFrame({'lat': lat, 'lon': lon}), crs='epsg:4326', geometry=point_var)\n","    gdf_joined = gpd.sjoin(gdf_points, gdf_zone, how='left')\n","    return gdf_joined['borough']\n","\n","@pandas_udf('long')\n","def preprocess_zone_id(lat: pd.Series, lon: pd.Series) -> pd.Series:\n","    point_var = [Point(xy) for xy in zip(lon, lat)]\n","    gdf_points = gpd.GeoDataFrame(pd.DataFrame({'lat': lat, 'lon': lon}), crs='epsg:4326', geometry=point_var)\n","    gdf_joined = gpd.sjoin(gdf_points, gdf_zone, how='left')\n","    return gdf_joined['location_id']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["taxi_df = taxi_df.withColumn('starttime', unix_timestamp(to_timestamp(col('pickup_datetime'))))\n","taxi_df = taxi_df.withColumn('endtime', unix_timestamp(to_timestamp(col('dropoff_datetime'))))\n","taxi_df = taxi_df.withColumn('days_of_week', preprocessing_days_of_week(col('starttime')))\n","taxi_df = taxi_df.withColumn('time', preprocessing_time(col('starttime')))\n","taxi_df = taxi_df.withColumn('tripduration', col('endtime') - col('starttime'))\n","taxi_df.withColumn(\"dropoff_location_id\", taxi_df.dropoff_location_id.cast('int'))\n","taxi_df.withColumn(\"pickup_location_id\", taxi_df.pickup_location_id.cast('int'))\n","taxi_df.withColumn(\"pickup_location_id\", preprocessing_man(col(\"pickup_location_id\")))\n","taxi_df.withColumn(\"dropoff_location_id\", preprocessing_man(col(\"dropoff_location_id\")))\n","\n","taxi_df = taxi_df.select(\n","    col(\"tripduration\"),\n","    col(\"days_of_week\"),\n","    col(\"time\"),\n","    col(\"pickup_location_id\").cast(\"int\").alias(\"pickup_location_id\"),\n","    col(\"dropoff_location_id\").cast(\"int\").alias(\"dropoff_location_id\"),\n","    col(\"trip_distance\"),\n","    col(\"fare_amount\"),\n",").dropna()\n","\n","taxi_df = taxi_df.where(\n","    (col('tripduration') > 360) \n","    & (col(\"pickup_location_id\") != col(\"dropoff_location_id\")) \n","    & (col('tripduration') < 3600)\n","    & (col(\"pickup_location_id\") >= 0)\n","    & (col(\"dropoff_location_id\") >= 0)\n",")\n","\n","taxi_df.printSchema()\n","taxi_df.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# cell for manipulate a timestamp to time and days of week\n","bike_df = bike_df.withColumn('starttime', unix_timestamp(to_timestamp(col('starttime'))))\n","bike_df = bike_df.withColumn('days_of_week', preprocessing_days_of_week(col('starttime')))\n","bike_df = bike_df.withColumn('time', preprocessing_time(col('starttime')))\n","bike_df = bike_df.withColumn('start_zone_name', preprocess_zone_name(bike_df['start_station_latitude'], bike_df['start_station_longitude']))\n","bike_df = bike_df.withColumn('end_zone_name', preprocess_zone_name(bike_df['end_station_latitude'], bike_df['end_station_longitude']))\n","bike_df = bike_df.withColumn('start_zone_id', preprocess_zone_id(bike_df['start_station_latitude'], bike_df['start_station_longitude']))\n","bike_df = bike_df.withColumn('end_zone_id', preprocess_zone_id(bike_df['end_station_latitude'], bike_df['end_station_longitude']))\n","bike_df = bike_df.withColumn(\"start_zone_id\", preprocessing_man(col(\"start_zone_id\")))\n","bike_df = bike_df.withColumn(\"end_zone_id\", preprocessing_man(col(\"end_zone_id\")))\n","\n","bike_df = bike_df.select(\n","    col(\"tripduration\"),\n","    col(\"days_of_week\"),\n","    col(\"time\"),\n","    col('usertype'),\n","    col(\"start_station_longitude\"),\n","    col(\"start_station_latitude\"),\n","    col(\"end_station_longitude\"),\n","    col(\"end_station_latitude\"),\n","    col('start_zone_id'),\n","    col('end_zone_id'),\n",").dropna()\n","\n","bike_df = bike_df.where(\n","    (col('tripduration') > 300) \n","    & (col(\"start_zone_id\") != col(\"end_zone_id\")) \n","    & (col('tripduration') < 3600)\n","    & (col('usertype') == \"Subscriber\")\n","    & (col(\"start_zone_id\") >= 0)\n","    & (col(\"end_zone_id\") >= 0)\n",")\n","\n","# taxi_df = taxi_df.where(col('days_of_week') < 5)\n","bike_df = bike_df.withColumnRenamed(\"start_station_longitude\", \"pickup_longitude\")\n","bike_df = bike_df.withColumnRenamed(\"start_station_latitude\", \"pickup_latitude\")\n","bike_df = bike_df.withColumnRenamed(\"end_station_longitude\", \"dropoff_longitude\")\n","bike_df = bike_df.withColumnRenamed(\"end_station_latitude\", \"dropoff_latitude\")\n","\n","bike_df.printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["bike_df.describe().show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["taxi_df.describe().show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Correlation matrix\n","# # convert to vector column first\n","# # took too much time\n","# vector_col = \"corr_features\"\n","# assembler = VectorAssembler(inputCols=citi_df.columns, outputCol=vector_col)\n","# df_vector = assembler.transform(citi_df).select(vector_col)\n","\n","# matrix = Correlation.corr(df_vector, vector_col)\n","# cor_np = matrix.collect()[0][matrix.columns[0]].toArray()\n","\n","\n","# plt.matshow(cor_np)\n","# print(cor_np)\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["taxi_feature_cols = [\n","    \"days_of_week\",\n","    \"time\",\n","    \"dropoff_location_id\",\n","    \"pickup_location_id\",\n","    \"fare_amount\",\n","]\n","taxi_assembler = VectorAssembler(inputCols=taxi_feature_cols, outputCol='features')\n","taxi_transformed_data = taxi_assembler.transform(taxi_df)\n","standard_scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n","taxi_scaled_df = standard_scaler.fit(taxi_transformed_data).transform(taxi_transformed_data)\n","# taxi_scaled_df.select(\"features\", \"features_scaled\").show(10, truncate=False)\n","(taxi_training_data, taxi_test_data) = taxi_scaled_df.randomSplit([0.7, 0.3])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["bike_feature_cols = [\n","    \"days_of_week\",\n","    \"time\",\n","    \"pickup_longitude\",\n","    \"pickup_latitude\",\n","    \"dropoff_longitude\",\n","    \"dropoff_latitude\",\n","    \"start_zone_id\",\n","    \"end_zone_id\",\n","]\n","bike_assembler = VectorAssembler(inputCols=bike_feature_cols, outputCol='features')\n","bike_transformed_data = bike_assembler.transform(bike_df)\n","standard_scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n","\n","bike_scaled_df = standard_scaler.fit(bike_transformed_data).transform(bike_transformed_data)\n","(bike_training_data, bike_test_data) = bike_scaled_df.randomSplit([0.7, 0.3])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dt = DecisionTreeRegressor(\n","    featuresCol=\"features\",\n","    labelCol=\"tripduration\",\n","    predictionCol=\"pred_tripduration\",\n",")\n","\n","taxi_dt_model = dt.fit(taxi_training_data)\n","# taxi_dt_summary = taxi_dt_model.summary\n","taxi_dt_predictions = taxi_dt_model.transform(taxi_test_data)\n","\n","bike_dt_model = dt.fit(bike_training_data)\n","bike_dt_predictions = bike_dt_model.transform(bike_test_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["gbt = GBTRegressor(\n","    featuresCol=\"features\",\n","    labelCol=\"tripduration\",\n","    predictionCol=\"pred_tripduration\",\n","    maxIter=10\n",")\n","\n","taxi_gbt_model = gbt.fit(taxi_training_data)\n","bike_gbt_model = gbt.fit(bike_training_data)\n","\n","taxi_gbt_predictions = taxi_gbt_model.transform(taxi_test_data)\n","bike_gbt_predictions = bike_gbt_model.transform(bike_test_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rf = RandomForestRegressor(\n","    featuresCol=\"features\",\n","    labelCol=\"tripduration\",\n","    predictionCol=\"pred_tripduration\",\n",")\n","\n","taxi_rf_model = rf.fit(taxi_training_data)\n","bike_rf_model = rf.fit(bike_training_data)\n","# taxi_gbt_summary = taxi_gbt_model.summary\n","\n","# print(taxi_gbt_summary.totalIterations)\n","# print(taxi_gbt_summary.objectiveHistory)\n","# print(taxi_gbt_summary.rootMeanSquaredError)\n","# print(taxi_gbt_summary.r2)\n","# print(f\"Taxi Gradient Boost Tree R^2: {taxi_gbt_summary.r2}\")\n","\n","\n","taxi_rf_predictions = taxi_rf_model.transform(taxi_test_data)\n","bike_rf_predictions = bike_rf_model.transform(bike_test_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# bike_summary = bike_model.summary\n","\n","\n","# print(bike_summary.totalIterations)\n","# print(bike_summary.objectiveHistory)\n","# print(bike_summary.rootMeanSquaredError)\n","# print(bike_summary.r2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# bike_predictions = bike_model.transform(bike_test_data)\n","\n","evaluator = RegressionEvaluator(\n","    labelCol=\"tripduration\",\n","    predictionCol=\"pred_tripduration\",\n","    metricName=\"r2\"\n",")\n","\n","taxi_dt_accuracy = evaluator.evaluate(taxi_dt_predictions)\n","taxi_gbt_accuracy = evaluator.evaluate(taxi_gbt_predictions)\n","taxi_rf_accuracy = evaluator.evaluate(taxi_rf_predictions)\n","\n","bike_dt_accuracy = evaluator.evaluate(bike_dt_predictions)\n","bike_gbt_accuracy = evaluator.evaluate(bike_gbt_predictions)\n","bike_rf_accuracy = evaluator.evaluate(bike_rf_predictions)\n","\n","# print(f\"bike Coefficients: {bike_model.coefficients}\")\n","# print(f\"Taxi Coefficients: {taxi_model.coefficients}\")\n","# print(f\"bike Intercept: {bike_model.intercept}\")\n","# print(f\"Taxi Intercept: {taxi_model.intercept}\")\n","# print(f\"bike R^2: {bike_model.summary.r2}\")\n","\n","# print(f\"bike Test Accuracy = {bike_accuracy}\")\n","print(f\"Taxi Test DT Accuracy = {taxi_dt_accuracy}\")\n","print(f\"Taxi Test GBT Accuracy = {taxi_gbt_accuracy}\")\n","print(f\"Taxi Test RF Accuracy = {taxi_rf_accuracy}\")\n","\n","print(f\"Bike Test DT Accuracy = {bike_dt_accuracy}\")\n","print(f\"Bike Test GBT Accuracy = {bike_gbt_accuracy}\")\n","print(f\"Bike Test RF Accuracy = {bike_rf_accuracy}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # bike_model.summary.residuals.show()\n","# taxi_lr_model.summary.residuals.show()\n","# # print(model.extractParamMap())\n","print(f\"Bike Test GBT Accuracy = {bike_gbt_accuracy}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(taxi_gbt_accuracy)\n","print(taxi_dt_accuracy)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["temp_evaluator = RegressionEvaluator(\n","    labelCol=\"tripduration\",\n","    predictionCol=\"prediction\",\n","    metricName=\"rmse\"\n",")\n","taxi_gbt_rmse = temp_evaluator.evaluate(taxi_gbt_predictions)\n","print(taxi_gbt_rmse)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"403176b059ae"},"source":["### Preprocessing\n","\n","Based on the schema printed above, data of the GitHub Activity is not stored in primitive types, but is instead stored in arrays. \n","\n","To work more effectively with the data, you need to preprocess it to primitive types and separate data for monoglot repos and polyglot repos. Once you create preprocessed columns, it makes our future tasks much faster.\n","\n","You can see three Python functions with the `@udf` annotation with their return type below. The annotation `@udf` is a short form of [User Defined Function](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.udf.html), which is used to extend the functions of the PySpark framework. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2e56d855da76"},"outputs":[],"source":["citi_df.info()\n","# df = df.select(\n","#     col(\"pickup_longitude\"),\n","#     col(\"pickup_latitude\"),\n","#     col(\"dropoff_longitude\"),\n","#     col(\"dropoff_latitude\"),\n","#     unix_timestamp(to_timestamp(col(\"pickup_datetime\"))).alias(\"pickup_datetime\"),\n","#     unix_timestamp(to_timestamp(col(\"dropoff_datetime\"))).alias(\"dropoff_datetime\"),\n","# )\n","\n","# df.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.printSchema()"]},{"cell_type":"markdown","metadata":{"id":"668a2549b231"},"source":["After preprocessing, you can see the preprocessed_df's schema, the language column is separated into three string columns, `mono_language`, `mono_size`, and `poly_language`."]},{"cell_type":"markdown","metadata":{"id":"0d79c26911f0"},"source":["### Analyze\n","\n","#### Which language is the most frequently used among the monoglot repos?\n","To answer this question, you can execute a query below with the preprocessed column, `mono_language`."]},{"cell_type":"markdown","metadata":{"id":"TpV-iwP9qw9c"},"source":["### Write back to the BigQuery\n","\n","After analyzing these queries, we have several DataFrames. The ranking of monoglot repositories, the average bytes of monoglot repositories, and the frequency table of each language being used in a repository. \n","\n","In this project, these three DataFrames will be stored in BigQuery using the [spark-bigquery-connector](https://github.com/GoogleCloudDataproc/spark-bigquery-connector)."]},{"cell_type":"markdown","metadata":{"id":"TpV-iwP9qw9c"},"source":["If there is no error above, congratulations! your DataFrame is successfully stored in your BigQuery.\n","\n","You can find the data via [this link](https://pantheon.corp.google.com/bigquery) or execute `bq` command-line tool like below."]},{"cell_type":"markdown","metadata":{"id":"TpV-iwP9qw9c"},"source":["## Cleaning up\n","\n","To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n","project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n","\n","Otherwise, you can delete the individual resources you created in this tutorial:"]},{"cell_type":"markdown","metadata":{"id":"TpV-iwP9qw9c"},"source":["### Delete Vertex AI Workbench - Managed Notebook\n","\n","To delete Vertex Ai Workbench - Managed Notebook used in this project, you can use this [Clean up](https://cloud.google.com/vertex-ai/docs/workbench/managed/create-managed-notebooks-instance-console-quickstart#clean-up) part of `Managed notebooks` page."]},{"cell_type":"markdown","metadata":{"id":"TpV-iwP9qw9c"},"source":["### Delete a Dataproc Cluster\n","\n","To delete a Dataproc Cluster, you can use this [Deleting a cluster](https://cloud.google.com/dataproc/docs/guides/manage-cluster#deleting_a_cluster) part of `Manage a cluster` page."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d7a1a1a5e978"},"outputs":[],"source":["# Delete Google Cloud Storage bucket\n","! gsutil rm -r $BUCKET_URI"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b77295420ab9"},"outputs":[],"source":["# Delete BigQuery dataset\n","! bq rm -r -f $DATASET_NAME"]},{"cell_type":"markdown","metadata":{"id":"TpV-iwP9qw9c"},"source":["After you delete the BigQuery dataset, you can check your Datasets in BigQuery using the following command."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c53e6e169788"},"outputs":[],"source":["! bq ls"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"spark_sample_notebook.ipynb","toc_visible":true},"kernelspec":{"display_name":"Python 3.8.9 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.9"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":4}

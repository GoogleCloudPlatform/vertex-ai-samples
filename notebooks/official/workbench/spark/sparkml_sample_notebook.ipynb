{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "## Tutorial\n",
    "\n",
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "# A Spark Session is how you interact with Spark SQL to create Dataframes\n",
    "from pyspark.sql import SparkSession\n",
    "# PySpark functions\n",
    "from pyspark.sql.functions import avg, col, count, desc, round, size, udf, to_timestamp, unix_timestamp, broadcast, pandas_udf, PandasUDFType, to_date\n",
    "# These allow us to create a schema for our data\n",
    "from pyspark.sql.types import ArrayType, IntegerType, StringType, DoubleType, BooleanType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "import seaborn as sns\n",
    "from geopandas import gpd\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression, GBTRegressor, DecisionTreeRegressor, RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from shapely.ops import cascaded_union\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### Initialize the SparkSession\n",
    "\n",
    "To use Apache Spark with BigQuery, you must include the [spark-bigquery-connector](https://github.com/GoogleCloudDataproc/spark-bigquery-connector) when you initialize the SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "outputs": [],
   "source": [
    "# Initialize the SparkSession with the following config.\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"spark-bigquery-ml-nyc-trips-demo\")\n",
    "    .config(\n",
    "        \"spark.jars\",\n",
    "        \"gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.25.2.jar\",\n",
    "    )\n",
    "#     .config(\"spark.sql.debug.maxToStringFields\", \"500\")\n",
    "#     .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### Fetch data from BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "outputs": [],
   "source": [
    "# Load NYC_taxi in Github Activity Public Dataset from BigQuery.\n",
    "taxi_df = (\n",
    "    spark.read.format(\"bigquery\")\n",
    "    .option(\"table\", \"bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2018\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Load NYC_Citibike in Github Acitivity Public dataset from BQ.\n",
    "bike_df = (\n",
    "    spark.read.format(\"bigquery\")\n",
    "    .option(\"table\", \"bigquery-public-data.new_york_citibike.citibike_trips\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_zone = gpd.read_file(\"https://data.cityofnewyork.us/api/geospatial/d3c5-ddgc?method=export&format=GeoJSON\")\n",
    "gdf_zone['location_id'] = gdf_zone['location_id'].astype('long')\n",
    "# gdf_zone['borough'] = gdf_zone['borough'].apply(\n",
    "#     lambda s: 1 if s == \"Manhattan\" else 0\n",
    "# ).astype('long')\n",
    "# pd_zone = pd.DataFrame(gdf_zone.filter(items=[\"location_id\", \"borough\"]))\n",
    "\n",
    "# v = gdf_zone.query(f'location_id=={238}')[\"borough\"]\n",
    "location_set = set()\n",
    "hm = gdf_zone.to_dict('index')\n",
    "for i in hm:\n",
    "    if hm[i]['borough'] == \"Manhattan\":\n",
    "        location_set.add(hm[i]['location_id'])\n",
    "# v = pd_zone.loc[pd_zone.location_id==238,'borough'].values[0]\n",
    "# print(v, type(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Perform Exploratory Data Analysis(EDA)\n",
    "\n",
    "As we get started with a new problem, the first step is to gain an understanding of what the dataset contains. EDA is used to derive insights from the data. Data scientists and analysts try to find different patterns, relations, and anomalies in the data using some statistical graphs and other visualization techniques. It allows analysts to understand the data better before making any assumptions.\n",
    "\n",
    "Check the data types for Taxi dataset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "Filter out unnecessary columns and check null counts of the fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df = taxi_df.select(\n",
    "    col(\"pickup_datetime\"),\n",
    "    col(\"dropoff_datetime\"),\n",
    "    col(\"trip_distance\"),\n",
    "    col(\"fare_amount\"),\n",
    "    col(\"pickup_location_id\"),\n",
    "    col(\"dropoff_location_id\"),\n",
    ")\n",
    "taxi_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "From this summary, you are able to know a lot of information.\n",
    "  - There are over 112 millions of trip history for Yellow Taxi in 2018. \n",
    "  - The current dataset has some abnormal values such as null and negative values in it.\n",
    "  - `pickup_datetime` and `dropoff_datetime` are string format. To use it effectively, it needs to be re-formatted.\n",
    "  - In previous years, the exact latitude and longitude were used for the pickup and the dropoff locations. It raised a lot of [privacy concerns](https://agkn.wordpress.com/2014/09/15/riding-with-the-stars-passenger-privacy-in-the-nyc-taxicab-dataset/) and the dataset has been providing `pickup_location_id` and `dropoff_location_id` instead. This id is corresponded to the [NYC Taxi Zones](https://data.cityofnewyork.us/Transportation/NYC-Taxi-Zones/d3c5-ddgc), roughly based on NYC Department of City Planningâ€™s Neighborhood Tabulation Areas (NTAs) and are meant to approximate neighborhoods.\n",
    "  - The maximum value of `pickup_location_id` and `dropoff_location_id` shows `99`. However, these might be wrong since the data type of both is string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "First, you can manipulate the time. `pickup_datetime` and `dropoff_datetime` is currently a string format, so using `to_timestamp()` function and `unix_timestamp()` function, you are able to get each pickup and droppoff datetime as a Unix Timestamp type.\n",
    "\n",
    "Unix time is a way of representing time as the number of seconds since `January 1st, 1970 at 00:00:00 UTC`. Compared to the Timestamp type, Unix time can be represented as an integer, making it easier to parse and use across different systems.\n",
    "\n",
    "After we get `start_time` and `end_time` by converting the original `pickup_datetime` and `dropoff_datetime`, we are able to get more insteresting columns using these two Timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=BooleanType())\n",
    "def is_weekdays(timestamp):\n",
    "    \"\"\"\n",
    "    The preprocessing function takes timestamp and returns whether the timestamp is weekdays or not.\n",
    "    Args:\n",
    "        timestamp: Unix Timestamp format that represent the time.\n",
    "                (e.g., timestamp = 1659268800, represents \"Sun, 31 Jul 2022 12:00:00 GMT\")\n",
    "    Returns:\n",
    "        A boolean value whether the given timestamp is weekdays or not.\n",
    "    \"\"\"\n",
    "    day_of_week = ((timestamp // 86400) + 4) % 7 if timestamp else 7\n",
    "    return 0 < day_of_week < 6\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def timestamp_to_time_in_minutes(timestamp):\n",
    "    \"\"\"\n",
    "    The preprocessing function takes timestamp and returns whether the timestamp is weekdays or not.\n",
    "    Args:\n",
    "        timestamp: Unix Timestamp format that represent the time.\n",
    "                (e.g., if timestamp == 1659268800, represents \"Sun, 31 Jul 2022 12:00:00 GMT\")\n",
    "    Returns:\n",
    "        A number that represents given time in minutes in EST (UTC-05).\n",
    "                (e.g., if timestamp == 1659268800, returns 420 since it is 7:00 in EST)\n",
    "    \"\"\"\n",
    "    return ((timestamp % 86400) // 60) - 300 if timestamp else None\n",
    "\n",
    "@udf(returnType=DoubleType())\n",
    "def preprocess_dist(start_lat, start_lon, end_lat, end_lon):\n",
    "    \"\"\"\n",
    "    The preprocessing function takes two coordinates(latitude and longitude) and returns the Euclidian distance.\n",
    "    Args:\n",
    "        start_lat: The latitude of the start station.\n",
    "        start_lon: The longitude of the start station.\n",
    "        end_lat: The latitude of the end station.\n",
    "        end_lon: The longitude of the end station.\n",
    "    Returns:\n",
    "        The Euclidian distance of given two coordinates.\n",
    "    \"\"\"\n",
    "    return Point(start_lon, start_lat).distance(Point(end_lon, end_lat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the type of pickup_datetime from a string to a Unix timestamp.\n",
    "taxi_df = taxi_df.withColumn('start_time', unix_timestamp(to_timestamp(col('pickup_datetime'))))\n",
    "\n",
    "# Convert the type of dropoff_datetime from a string to a Unix timestamp.\n",
    "taxi_df = taxi_df.withColumn('end_time', unix_timestamp(to_timestamp(col('dropoff_datetime'))))\n",
    "\n",
    "# Convert start_time to days_of_week\n",
    "taxi_df = taxi_df.withColumn('is_weekdays', is_weekdays(col('start_time')))\n",
    "\n",
    "# Convert start_time to start_time_in_minute\n",
    "taxi_df = taxi_df.withColumn('start_time_in_minute', timestamp_to_time_in_minutes(col('start_time')))\n",
    "\n",
    "# Calculate trip_duration\n",
    "taxi_df = taxi_df.withColumn('trip_duration', col('end_time') - col('start_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "Before we go deeper into the Taxi dataset, let's do the similar work for the Citibike dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df = bike_df.select(\n",
    "    col(\"tripduration\").alias(\"trip_duration\"),\n",
    "    col(\"starttime\"),\n",
    "    col(\"stoptime\"),\n",
    "    col(\"start_station_latitude\"),\n",
    "    col(\"start_station_longitude\"),\n",
    "    col(\"end_station_latitude\"),\n",
    "    col(\"end_station_longitude\"),\n",
    "    col(\"usertype\"),\n",
    ")\n",
    "# bike_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "From this summary, there is also interesting information from the dataset's summary.\n",
    "  - There are over 53 millions of trip history for Citibike from 2013 to 2018.\n",
    "  - The current dataset has some abnormal values.\n",
    "  - `starttime` and `stoptime` are string format. To use it effectively, it needs to be re-formatted.\n",
    "  - Unlike the Taxi dataset, starting and ending location has exact latitude and longitude, but since every bike is parked in their station, these coordinates represent the station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# cell for manipulate a timestamp to time and days of week\n",
    "bike_df = bike_df.withColumn('starttime', unix_timestamp(to_timestamp(col('starttime'))))\n",
    "bike_df = bike_df.withColumn('stoptime', unix_timestamp(to_timestamp(col('stoptime'))))\n",
    "bike_df = bike_df.withColumn('is_weekdays', is_weekdays(col('starttime')))\n",
    "bike_df = bike_df.withColumn('start_time_in_minute', timestamp_to_time_in_minutes(col('starttime')))\n",
    "bike_df = bike_df.withColumn('trip_distance', preprocess_dist('start_station_latitude', 'start_station_longitude', 'end_station_latitude', 'end_station_longitude'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "Check the distributions for the numerical columns. In PySpark, visualizing is expensive because the data is too large. For example, the NYC Taxi dataset in 2018 has more than 112M rows. Therefore, approximately 2% of total data (approx. 2.2M rows) are extracted as a sample, which is enough to have 99% confidence interval and less than 0.1% of margin of error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_sample = taxi_df.sample(0.02)\n",
    "\n",
    "df = taxi_sample.toPandas()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "When a [Decimal Type](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.types.DecimalType.html) column in PySpark are converted to Pandas DataFrame, it is converted into object Type, not float type. To visualize these \"object\" columns, they need to be converted into the float type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOAT_TYPE_COLUMNS = [\"trip_distance\", \"fare_amount\", \"pickup_location_id\", \"dropoff_location_id\"]\n",
    "\n",
    "df = df.drop(columns=['pickup_datetime', 'dropoff_datetime'])\n",
    "for COLUMN in FLOAT_TYPE_COLUMNS:\n",
    "    df[COLUMN] = df[COLUMN].astype(float)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "Iterate through columns and plot them to box and histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    if column == 'is_weekdays':\n",
    "        continue\n",
    "    _, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    df[column].plot(kind=\"box\", ax=ax[0])\n",
    "    df[column].plot(kind=\"hist\", ax=ax[1])\n",
    "    plt.title(column)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_2018 = 1514782800 # Jan 1, 2018 00:00:00\n",
    "END_2018 = 1546318800 # Dec 31, 2018 23:59:59\n",
    "\n",
    "@pandas_udf('long')\n",
    "def preprocess_zone_id(lat: pd.Series, lon: pd.Series) -> pd.Series:\n",
    "    point_var = [Point(xy) for xy in zip(lon, lat)]\n",
    "    gdf_points = gpd.GeoDataFrame(pd.DataFrame({'lat': lat, 'lon': lon}), crs='epsg:4326', geometry=point_var)\n",
    "    gdf_joined = gpd.sjoin(gdf_points, gdf_zone, how='left')\n",
    "    return gdf_joined['location_id']\n",
    "\n",
    "@udf(returnType=BooleanType())\n",
    "def is_in_manhattan(location_id):\n",
    "    return location_id in location_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df = taxi_df.dropna()\n",
    "taxi_df = taxi_df.withColumn(\"dropoff_location_id\", taxi_df.dropoff_location_id.cast('int'))\n",
    "taxi_df = taxi_df.withColumn(\"pickup_location_id\", taxi_df.pickup_location_id.cast('int'))\n",
    "taxi_df = taxi_df.withColumn(\"is_start_manhattan\", is_in_manhattan(col(\"pickup_location_id\")))\n",
    "taxi_df = taxi_df.withColumn(\"is_end_manhattan\", is_in_manhattan(col(\"dropoff_location_id\")))\n",
    "\n",
    "taxi_df = taxi_df.where(\n",
    "    (col('start_time') >= START_2018)\n",
    "    & (col('start_time') <= END_2018)\n",
    "    & (col('end_time') >= START_2018)\n",
    "    & (col('end_time') <= END_2018)\n",
    "    & (col('start_time') < col('end_time'))\n",
    "    & (col('trip_duration') > 0) \n",
    "    & (col('trip_duration') < 4000)\n",
    "    & (col('trip_distance') > 0.2)\n",
    "    & (col('trip_distance') < 15)\n",
    "    & (col(\"pickup_location_id\") != col(\"dropoff_location_id\")) \n",
    "    & (col(\"fare_amount\") > 0)\n",
    "    & (col(\"fare_amount\") < 500)\n",
    "    & (col(\"is_start_manhattan\") == True)\n",
    "    & (col(\"is_end_manhattan\") == True)\n",
    ")\n",
    "\n",
    "taxi_df.printSchema()\n",
    "taxi_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bike_df = bike_df.withColumn('start_zone_id', preprocess_zone_id(bike_df['start_station_latitude'], bike_df['start_station_longitude']))\n",
    "bike_df = bike_df.withColumn('end_zone_id', preprocess_zone_id(bike_df['end_station_latitude'], bike_df['end_station_longitude']))\n",
    "bike_df = bike_df.withColumn('is_start_manhattan', is_in_manhattan(col('start_zone_id')))\n",
    "bike_df = bike_df.withColumn('is_end_manhattan', is_in_manhattan(col('end_zone_id')))\n",
    "\n",
    "bike_df = bike_df.where(\n",
    "    (col('tripduration') > 0)\n",
    "    & (col(\"start_zone_id\") != col(\"end_zone_id\")) \n",
    "    & (col('tripduration') < 7200)\n",
    "#     & (col('usertype') == \"Subscriber\")\n",
    "    & (col('starttime') < col('stoptime'))\n",
    "    & (col(\"is_start_manhattan\") == True)\n",
    "    & (col(\"is_end_manhattan\") == True)\n",
    ").dropna()\n",
    "\n",
    "bike_df.printSchema()\n",
    "bike_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_feature_cols = [\n",
    "    \"is_weekdays\",\n",
    "    \"start_time_in_minute\",\n",
    "    \"dropoff_location_id\",\n",
    "    \"pickup_location_id\",\n",
    "    \"trip_distance\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_feature_cols = [\n",
    "    \"is_weekdays\",\n",
    "#     \"start_station_longitude\",\n",
    "#     \"start_station_latitude\",\n",
    "#     \"end_station_longitude\",\n",
    "#     \"end_station_latitude\",\n",
    "    \"start_zone_id\",\n",
    "    \"end_zone_id\",\n",
    "    \"start_time_in_minute\",\n",
    "    \"trip_distance\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_assembler = VectorAssembler(inputCols=taxi_feature_cols, outputCol='features')\n",
    "taxi_transformed_data = taxi_assembler.transform(taxi_df)\n",
    "\n",
    "standard_scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "taxi_scaled_df = standard_scaler.fit(taxi_transformed_data).transform(taxi_transformed_data)\n",
    "\n",
    "taxi_scaled_df.select(\"features\", \"features_scaled\").show(10, truncate=False)\n",
    "(taxi_training_data, taxi_test_data) = taxi_scaled_df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_assembler = VectorAssembler(inputCols=bike_feature_cols, outputCol='features')\n",
    "bike_transformed_data = bike_assembler.transform(bike_df)\n",
    "standard_scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "\n",
    "bike_scaled_df = standard_scaler.fit(bike_transformed_data).transform(bike_transformed_data)\n",
    "(bike_training_data, bike_test_data) = bike_scaled_df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"trip_duration\",\n",
    "    predictionCol=\"pred_trip_duration\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_gbt_model = gbt.fit(taxi_training_data)\n",
    "taxi_gbt_predictions = taxi_gbt_model.transform(taxi_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_gbt_model = gbt.fit(bike_training_data)\n",
    "bike_gbt_predictions = bike_gbt_model.transform(bike_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_r2 = RegressionEvaluator(\n",
    "    labelCol=gbt.getLabelCol(),\n",
    "    predictionCol=gbt.getPredictionCol(),\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "evaluator_rmse = RegressionEvaluator(\n",
    "    labelCol=gbt.getLabelCol(),\n",
    "    predictionCol=gbt.getPredictionCol(),\n",
    "    metricName=\"rmse\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_gbt_accuracy_r2 = evaluator_r2.evaluate(taxi_gbt_predictions)\n",
    "taxi_gbt_accuracy_rmse = evaluator_rmse.evaluate(taxi_gbt_predictions)\n",
    "\n",
    "# RMSE:245.99563606237268\n",
    "\n",
    "# print(f\"Taxi Coefficients: {taxi_model.coefficients}\")\n",
    "# print(f\"Taxi Intercept: {taxi_model.intercept}\")\n",
    "\n",
    "print(f\"Taxi Test GBT R2 Accuracy = {taxi_gbt_accuracy_r2}\")\n",
    "print(f\"Taxi Test GBT RMSE Accuracy = {taxi_gbt_accuracy_rmse}\")\n",
    "\n",
    "# Taxi Test GBT R2 Accuracy = 0.708183954907601 <- \n",
    "# Taxi Test GBT R2 Accuracy = 0.6598682899938532\n",
    "# rmse = 265.5806200025988\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_gbt_accuracy_r2 = evaluator_r2.evaluate(bike_gbt_predictions)\n",
    "bike_gbt_accuracy_rmse = evaluator_rmse.evaluate(bike_gbt_predictions)\n",
    "\n",
    "# print(f\"bike Coefficients: {bike_gbt_model.coefficients}\")\n",
    "# print(f\"bike Intercept: {bike_gbt_model.intercept}\")\n",
    "print(f\"Bike Test GBT R2 Accuracy = {bike_gbt_accuracy_r2}\")\n",
    "print(f\"Bike Test GBT RMSE Accuracy = {bike_gbt_accuracy_rmse}\")\n",
    "\n",
    "# Bike Test GBT R2 Accuracy = 0.540752577358068\n",
    "# Bike Test GBT RMSE Accuracy = 341.8116656204376\n",
    "\n",
    "# Exclude subscriber\n",
    "# Bike Test GBT R2 Accuracy = 0.4413798408518055\n",
    "# Bike Test GBT RMSE Accuracy = 440.703561958477\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Bike Test GBT R2 Accuracy = {bike_gbt_accuracy_r2}\")\n",
    "print(f\"Bike Test GBT RMSE Accuracy = {bike_gbt_accuracy_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_gbt_model = gbt.fit(bike_training_data)\n",
    "\n",
    "# taxi_gbt_predictions = taxi_gbt_model.transform(taxi_test_data)\n",
    "bike_gbt_predictions = bike_gbt_model.transform(bike_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_gbt_accuracy_r2 = evaluator_r2.evaluate(bike_gbt_predictions)\n",
    "bike_gbt_accuracy_rmse = evaluator_rmse.evaluate(bike_gbt_predictions)\n",
    "print(f\"Bike Test GBT r2 Accuracy = {bike_gbt_accuracy_r2}\")\n",
    "print(f\"Bike Test GBT rmse Accuracy = {bike_gbt_accuracy_rmse}\")\n",
    "\n",
    "# Bike Test GBT r2 Accuracy = 0.5394345501867752\n",
    "# Bike Test GBT rmse Accuracy = 342.1333445936911\n",
    "# bike_summary = bike_model.summary\n",
    "\n",
    "\n",
    "# print(bike_summary.totalIterations)\n",
    "# print(bike_summary.objectiveHistory)\n",
    "# print(bike_summary.rootMeanSquaredError)\n",
    "# print(bike_summary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # bike_model.summary.residuals.show()\n",
    "# taxi_lr_model.summary.residuals.show()\n",
    "# # print(model.extractParamMap())\n",
    "print(f\"Bike Test GBT Accuracy = {bike_gbt_accuracy_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(taxi_gbt_accuracy)\n",
    "print(taxi_dt_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"tripduration\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "taxi_gbt_rmse = temp_evaluator.evaluate(taxi_gbt_predictions)\n",
    "print(taxi_gbt_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2e56d855da76"
   },
   "outputs": [],
   "source": [
    "citi_df.info()\n",
    "# df = df.select(\n",
    "#     col(\"pickup_longitude\"),\n",
    "#     col(\"pickup_latitude\"),\n",
    "#     col(\"dropoff_longitude\"),\n",
    "#     col(\"dropoff_latitude\"),\n",
    "#     unix_timestamp(to_timestamp(col(\"pickup_datetime\"))).alias(\"pickup_datetime\"),\n",
    "#     unix_timestamp(to_timestamp(col(\"dropoff_datetime\"))).alias(\"dropoff_datetime\"),\n",
    "# )\n",
    "\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = (ParamGridBuilder()\n",
    "#               .addGrid(gbt.maxDepth, [2, 5, 10])\n",
    "#               .addGrid(gbt.maxBins, [10, 20, 40])\n",
    "#               .addGrid(gbt.maxIter, [5, 10, 20])\n",
    "#               .build()\n",
    "# )\n",
    "\n",
    "# cv = CrossValidator(\n",
    "#     estimator=gbt,\n",
    "#     evaluator=evaluator_rmse,\n",
    "#     estimatorParamMaps=param_grid,\n",
    "#     numFolds=5\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model = cv.fit(taxi_training_data)\n",
    "print(cv_model)\n",
    "gb_predictions = cv_model.transform(taxi_test_data)\n",
    "print(f\"RMSE:{evaluator_rmse.evaluate(gb_predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "668a2549b231"
   },
   "source": [
    "After preprocessing, you can see the preprocessed_df's schema, the language column is separated into three string columns, `mono_language`, `mono_size`, and `poly_language`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0d79c26911f0"
   },
   "source": [
    "### Analyze\n",
    "\n",
    "#### Which language is the most frequently used among the monoglot repos?\n",
    "To answer this question, you can execute a query below with the preprocessed column, `mono_language`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "### Write back to the BigQuery\n",
    "\n",
    "After analyzing these queries, we have several DataFrames. The ranking of monoglot repositories, the average bytes of monoglot repositories, and the frequency table of each language being used in a repository. \n",
    "\n",
    "In this project, these three DataFrames will be stored in BigQuery using the [spark-bigquery-connector](https://github.com/GoogleCloudDataproc/spark-bigquery-connector)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "If there is no error above, congratulations! your DataFrame is successfully stored in your BigQuery.\n",
    "\n",
    "You can find the data via [this link](https://pantheon.corp.google.com/bigquery) or execute `bq` command-line tool like below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "### Delete Vertex AI Workbench - Managed Notebook\n",
    "\n",
    "To delete Vertex Ai Workbench - Managed Notebook used in this project, you can use this [Clean up](https://cloud.google.com/vertex-ai/docs/workbench/managed/create-managed-notebooks-instance-console-quickstart#clean-up) part of `Managed notebooks` page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "### Delete a Dataproc Cluster\n",
    "\n",
    "To delete a Dataproc Cluster, you can use this [Deleting a cluster](https://cloud.google.com/dataproc/docs/guides/manage-cluster#deleting_a_cluster) part of `Manage a cluster` page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7a1a1a5e978"
   },
   "outputs": [],
   "source": [
    "# Delete Google Cloud Storage bucket\n",
    "! gsutil rm -r $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b77295420ab9"
   },
   "outputs": [],
   "source": [
    "# Delete BigQuery dataset\n",
    "! bq rm -r -f $DATASET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "After you delete the BigQuery dataset, you can check your Datasets in BigQuery using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"tripduration\",\n",
    "    predictionCol=\"pred_tripduration\",\n",
    ")\n",
    "\n",
    "taxi_dt_model = dt.fit(taxi_training_data)\n",
    "# taxi_dt_summary = taxi_dt_model.summary\n",
    "taxi_dt_predictions = taxi_dt_model.transform(taxi_test_data)\n",
    "\n",
    "bike_dt_model = dt.fit(bike_training_data)\n",
    "bike_dt_predictions = bike_dt_model.transform(bike_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"tripduration\",\n",
    "    predictionCol=\"pred_tripduration\",\n",
    ")\n",
    "\n",
    "taxi_rf_model = rf.fit(taxi_training_data)\n",
    "bike_rf_model = rf.fit(bike_training_data)\n",
    "# taxi_gbt_summary = taxi_gbt_model.summary\n",
    "\n",
    "# print(taxi_gbt_summary.totalIterations)\n",
    "# print(taxi_gbt_summary.objectiveHistory)\n",
    "# print(taxi_gbt_summary.rootMeanSquaredError)\n",
    "# print(taxi_gbt_summary.r2)\n",
    "# print(f\"Taxi Gradient Boost Tree R^2: {taxi_gbt_summary.r2}\")\n",
    "\n",
    "\n",
    "taxi_rf_predictions = taxi_rf_model.transform(taxi_test_data)\n",
    "bike_rf_predictions = bike_rf_model.transform(bike_test_data)\n",
    "\n",
    "\n",
    "\n",
    "# lon = -73.993915\n",
    "# lat = 40.73532427\n",
    "# point_var = Point(lon, lat)\n",
    "# gdf_point = gpd.GeoDataFrame(crs='epsg:4326', geometry=[point_var])\n",
    "# gdf_joined = gpd.sjoin(gdf_point, gdf_zone, how='left')\n",
    "# print(gdf_point)\n",
    "# print(gdf_joined)\n",
    "# point_var = Point(lon, lat)\n",
    "# zones.contains(point_var, align=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c53e6e169788"
   },
   "outputs": [],
   "source": [
    "! bq ls"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "spark_sample_notebook.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

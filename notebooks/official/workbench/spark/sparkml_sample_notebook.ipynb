{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-managed-notebook?download_url=https://raw.githubusercontent.com/hyunuk/vertex-ai-samples/experiment/notebooks/official/workbench/spark/spark_sample_notebook.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>                                                                                               \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook tutorial shows you Apache SparkML jobs with Dataproc and BigQuery. Through this notebook, you can learn a common use case in the machine learning pipeline: Ingestion, data cleaning, feature engineering, modeling, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The two datasets, [NYC TLC(Taxi and Limousine Commission) Trips](https://console.cloud.google.com/marketplace/product/city-of-new-york/nyc-tlc-trips) dataset and [NYC Citi Bike Trips](https://console.cloud.google.com/marketplace/product/city-of-new-york/nyc-citi-bike) dataset, is available in [BigQuery Public Datasets](https://cloud.google.com/bigquery/public-data), and provides free querying of up to 1TB of data each month. It contains trips data for each Taxi and Citi Bike, the public bicycle sharing system serving the New York City."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Objective\n",
    "\n",
    "This notebook tutorial runs an Apache SparkML job that fetches data from the BigQuery dataset, performs exploratory data analysis, cleans data, executes feature engineering, trains the model, evaluates the model, debriefs for the result and saves the model to a Cloud Storage.\n",
    "\n",
    "This notebook tutorial performs the following steps:\n",
    "\n",
    "- Setting up a Google Cloud project and Dataproc cluster.\n",
    "- Configuring the spark-bigquery-connector.\n",
    "- Ingesting data from BigQuery into a Spark DataFrame.\n",
    "\n",
    "\n",
    "- Preprocessing ingested data.\n",
    "- Querying the most frequently used programming language in monoglot repos.\n",
    "- Querying the average size (MB) of code in each language stored in monoglot repos.\n",
    "- Querying the languages files most frequently found together in polyglot repos.\n",
    "- Writing the query results back into BigQuery.\n",
    "- Deleting the resources created for this notebook tutorial.\n",
    "- Disabling the APIs used in the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Costs \n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* [Vertex AI](https://cloud.google.com/vertex-ai/pricing)\n",
    "* [Cloud Storage](https://cloud.google.com/storage/pricing)\n",
    "* [Dataproc](https://cloud.google.com/dataproc/pricing)\n",
    "\n",
    "You can use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "## Before you begin\n",
    "\n",
    "### Set up your Google Cloud project:\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you create an account, you receive a $300 credit towards to your compute and storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. [Enable the Notebooks API, Vertex AI API, and Dataproc API](https://console.cloud.google.com/flows/enableapi?apiid=notebooks.googleapis.com,aiplatform.googleapis.com,dataproc&_ga=2.209429842.1903825585.1657549521-326108178.1655322249)\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and inserts the value of Python variables prefixed with `$` into the commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Create a Dataproc cluster\n",
    "\n",
    "The Spark job executed in this notebook tutorial is compute intensive. Since the job can take a significant amount time to complete in a standard notebook environment, this notebook tutorial runs on a Dataproc cluster that is created with the Dataproc Component Gateway and Jupyter component installed on the cluster.\n",
    "\n",
    "**Existing Dataproc with Jupyter cluster?**: If you have a running Dataproc cluster that has the [Component Gateway and Jupyter component installed on the cluster](https://cloud.google.com/dataproc/docs/concepts/components/jupyter#gcloud-command)), you can use it in this tutorial. If you plan to use it, skip this step, and go to `Switch your kernel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_NAME = \"[your-cluster]\"  # @param {type: \"string\"}\n",
    "CLUSTER_REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "if CLUSTER_REGION == \"[your-region]\":\n",
    "    CLUSTER_REGION = \"us-central1\"\n",
    "\n",
    "print(f\"CLUSTER_NAME: {CLUSTER_NAME}\")\n",
    "print(f\"CLUSTER_REGION: {CLUSTER_REGION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud dataproc clusters create $CLUSTER_NAME \\\n",
    "    --region=$CLUSTER_REGION \\\n",
    "    --enable-component-gateway \\\n",
    "    --optional-components=JUPYTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "Your `CLUSTER_NAME` must be **unique within your Google Cloud project**. It must start with a lowercase letter, followed by up to 51 lowercase letters, numbers, and hyphens, and cannot end with a hyphen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "#### Switch your kernel\n",
    "\n",
    "Your notebook kernel is listed at the top of the notebook page. Your notebook should run on the Python 3 kernel running on your Dataproc cluster.\n",
    "\n",
    "Select **Kernel > Change Kernel** from the top menu, then select `Python 3 on CLUSTER_NAME: Dataproc cluster in REGION (Remote)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Set your project ID\n",
    "\n",
    "Run the following cell to get you project ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "If the previous command has no output, copy your project ID from the project selector in the [Google Cloud console](https://console.cloud.google.com/). Insert the ID in the `[your-project-id]` placeholder, then run the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"[your-project-id]\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "The Spark DataFrame created in this tutorial is stored in BigQuery, with the data first being written to a Google Cloud Storage bucket before it is written into BigQuery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "#### Region\n",
    "\n",
    "Before creating a Cloud Storage bucket, re-define the `REGION` variable (when you changed the notebook kernel earlier, previously set variables were deleted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "To avoid name collisions, you can create a timestamp for the current notebook session, then append the timestamp to the name of resources that you create in this tutorial, such as the Cloud Storage bucket or BigQuery dataset that you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "Replace the `[your-bucket-name]` placeholder with the name of your Cloud Storage bucket. The name must be unique across all Cloud Storage buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}/\"\n",
    "\n",
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
    "    BUCKET_NAME = f\"{PROJECT_ID}{TIMESTAMP}\"\n",
    "    BUCKET_URI = f\"gs://{BUCKET_NAME}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "Confirm your access to the Cloud Storage bucket by displaying the bucket's metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls -L -b $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "## Tutorial\n",
    "\n",
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "# A Spark Session is how you interact with Spark SQL to create Dataframes\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# PySpark functions\n",
    "from pyspark.sql.functions import avg, col, count, desc, round, size, udf, to_timestamp, unix_timestamp, broadcast, pandas_udf, PandasUDFType, to_date\n",
    "\n",
    "# These allow us to create a schema for our data\n",
    "from pyspark.sql.types import ArrayType, IntegerType, StringType, DoubleType, BooleanType\n",
    "\n",
    "from geopandas import gpd\n",
    "from shapely.geometry import Point\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### Initialize the SparkSession\n",
    "\n",
    "To use Apache Spark with BigQuery, you must include the [spark-bigquery-connector](https://github.com/GoogleCloudDataproc/spark-bigquery-connector) when you initialize the SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "outputs": [],
   "source": [
    "# Initialize the SparkSession with the following config.\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"spark-bigquery-ml-nyc-trips-demo\")\n",
    "    .config(\n",
    "        \"spark.jars\",\n",
    "        \"gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.25.2.jar\",\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### Fetch data from BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "outputs": [],
   "source": [
    "# Load NYC_taxi in Github Activity Public Dataset from BigQuery.\n",
    "taxi_df = (\n",
    "    spark.read.format(\"bigquery\")\n",
    "    .option(\"table\", \"bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2018\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Load NYC_Citibike in Github Acitivity Public dataset from BQ.\n",
    "bike_df = (\n",
    "    spark.read.format(\"bigquery\")\n",
    "    .option(\"table\", \"bigquery-public-data.new_york_citibike.citibike_trips\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Perform Exploratory Data Analysis(EDA)\n",
    "\n",
    "As we get started with a new problem, the first step is to gain an understanding of what the dataset contains. EDA is used to derive insights from the data. Data scientists and analysts try to find different patterns, relations, and anomalies in the data using some statistical graphs and other visualization techniques. It allows analysts to understand the data better before making any assumptions.\n",
    "\n",
    "Check the data types for Taxi dataset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "Filter out unnecessary columns and check null counts of the fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df = taxi_df.select(\n",
    "    col(\"pickup_datetime\"),\n",
    "    col(\"dropoff_datetime\"),\n",
    "    col(\"trip_distance\"),\n",
    "    col(\"fare_amount\"),\n",
    "    col(\"pickup_location_id\"),\n",
    "    col(\"dropoff_location_id\"),\n",
    ")\n",
    "taxi_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "From this summary, you are able to know a lot of information.\n",
    "  - There are over 112 millions of trip history for Yellow Taxi in 2018. \n",
    "  - The current dataset has some abnormal values such as null and negative values in it.\n",
    "  - `pickup_datetime` and `dropoff_datetime` are string format. To use it effectively, it needs to be re-formatted.\n",
    "  - In previous years, the exact latitude and longitude were used for the pickup and the dropoff locations. It raised a lot of [privacy concerns](https://agkn.wordpress.com/2014/09/15/riding-with-the-stars-passenger-privacy-in-the-nyc-taxicab-dataset/) and the dataset has been providing `pickup_location_id` and `dropoff_location_id` instead. This id is corresponded to the [NYC Taxi Zones](https://data.cityofnewyork.us/Transportation/NYC-Taxi-Zones/d3c5-ddgc), roughly based on NYC Department of City Planningâ€™s Neighborhood Tabulation Areas (NTAs) and are meant to approximate neighborhoods.\n",
    "  - The maximum value of `pickup_location_id` and `dropoff_location_id` shows `99`. However, these might be wrong since the data type of both is string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "First, you can manipulate the time. `pickup_datetime` and `dropoff_datetime` is currently a string format, so using `to_timestamp()` function and `unix_timestamp()` function, you are able to get each pickup and droppoff datetime as a Unix Timestamp type.\n",
    "\n",
    "Unix time is a way of representing time as the number of seconds since `January 1st, 1970 at 00:00:00 UTC`. Compared to the Timestamp type, Unix time can be represented as an integer, making it easier to parse and use across different systems.\n",
    "\n",
    "After we get `start_time` and `end_time` by converting the original `pickup_datetime` and `dropoff_datetime`, we are able to get more insteresting columns using these two Timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=BooleanType())\n",
    "def is_weekdays(timestamp):\n",
    "    \"\"\"\n",
    "    The preprocessing function takes timestamp and returns whether the timestamp is weekdays or not.\n",
    "    Args:\n",
    "        timestamp: Unix Timestamp format that represent the time.\n",
    "                (e.g., timestamp = 1659268800, represents \"Sun, 31 Jul 2022 12:00:00 GMT\")\n",
    "    Returns:\n",
    "        A boolean value whether the given timestamp is weekdays or not.\n",
    "    \"\"\"\n",
    "    day_of_week = ((timestamp // 86400) + 4) % 7 if timestamp else 7\n",
    "    return 0 < day_of_week < 6\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def timestamp_to_time_in_minutes(timestamp):\n",
    "    \"\"\"\n",
    "    The preprocessing function takes timestamp and returns whether the timestamp is weekdays or not.\n",
    "    Args:\n",
    "        timestamp: Unix Timestamp format that represent the time.\n",
    "                (e.g., if timestamp == 1659268800, represents \"Sun, 31 Jul 2022 12:00:00 GMT\")\n",
    "    Returns:\n",
    "        A number that represents given time in minutes in EST (UTC-05).\n",
    "                (e.g., if timestamp == 1659268800, returns 420 since it is 7:00 in EST)\n",
    "    \"\"\"\n",
    "    return ((timestamp % 86400) // 60) - 300 if timestamp else None\n",
    "\n",
    "@udf(returnType=DoubleType())\n",
    "def manhattan_dist(start_lat, start_lon, end_lat, end_lon):\n",
    "    \"\"\"\n",
    "    The preprocessing function takes two coordinates(latitude and longitude) and returns the Manhattan distance.\n",
    "    Args:\n",
    "        start_lat: The latitude of the start station.\n",
    "        start_lon: The longitude of the start station.\n",
    "        end_lat: The latitude of the end station.\n",
    "        end_lon: The longitude of the end station.\n",
    "    Returns:\n",
    "        The Manhattan distance of given two coordinates.\n",
    "    \"\"\"\n",
    "    return abs(end_lon - start_lon) + abs(end_lat - start_lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the type of pickup_datetime from a string to a Unix timestamp.\n",
    "taxi_df = taxi_df.withColumn('start_time', unix_timestamp(to_timestamp(col('pickup_datetime'))))\n",
    "\n",
    "# Convert the type of dropoff_datetime from a string to a Unix timestamp.\n",
    "taxi_df = taxi_df.withColumn('end_time', unix_timestamp(to_timestamp(col('dropoff_datetime'))))\n",
    "\n",
    "# Convert start_time to days_of_week\n",
    "taxi_df = taxi_df.withColumn('is_weekdays', is_weekdays(col('start_time')))\n",
    "\n",
    "# Convert start_time to start_time_in_minute\n",
    "taxi_df = taxi_df.withColumn('start_time_in_minute', timestamp_to_time_in_minutes(col('start_time')))\n",
    "\n",
    "# Calculate trip_duration\n",
    "taxi_df = taxi_df.withColumn('trip_duration', col('end_time') - col('start_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "Before we go deeper into the Taxi dataset, let's do the similar work for the Citibike dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df = bike_df.select(\n",
    "    col(\"tripduration\").alias(\"trip_duration\"),\n",
    "    col(\"starttime\"),\n",
    "    col(\"stoptime\"),\n",
    "    col(\"start_station_latitude\"),\n",
    "    col(\"start_station_longitude\"),\n",
    "    col(\"end_station_latitude\"),\n",
    "    col(\"end_station_longitude\"),\n",
    "    col(\"usertype\"),\n",
    ")\n",
    "bike_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "From this summary, there is also interesting information from the dataset's summary.\n",
    "  - There are over 53 millions of trip history for Citibike from 2013 to 2018.\n",
    "  - The current dataset has some abnormal values.\n",
    "  - `starttime` and `stoptime` are string format. To use it effectively, it needs to be re-formatted.\n",
    "  - Unlike the Taxi dataset, starting and ending location has exact latitude and longitude, but since every bike is parked in their station, these coordinates represent the station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the type of starttime from a string to a Unix timestamp.\n",
    "bike_df = bike_df.withColumn('starttime', unix_timestamp(to_timestamp(col('starttime'))))\n",
    "\n",
    "# Convert the type of stoptime from a string to a Unix timestamp.\n",
    "bike_df = bike_df.withColumn('stoptime', unix_timestamp(to_timestamp(col('stoptime'))))\n",
    "\n",
    "# Check whether the starttime is a weekday or a weekend.\n",
    "bike_df = bike_df.withColumn('is_weekdays', is_weekdays(col('starttime')))\n",
    "\n",
    "# Convert starttime to start_time_in_minute\n",
    "bike_df = bike_df.withColumn('start_time_in_minute', timestamp_to_time_in_minutes(col('starttime')))\n",
    "\n",
    "# Calculate the Manhattan distance between start_station and end_station\n",
    "bike_df = bike_df.withColumn('trip_distance', manhattan_dist('start_station_latitude', 'start_station_longitude', 'end_station_latitude', 'end_station_longitude'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df.printSchema()\n",
    "bike_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "#### Visualization\n",
    "Check the distributions for the numerical columns. In PySpark, visualizing is expensive because the data is too large. For example, the NYC Taxi dataset in 2018 has more than 112M rows. Therefore, approximately 2% of total data (approx. 2.2M rows) are extracted as a sample, which is enough to have 99% confidence interval and less than 0.1% of margin of error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_sample = taxi_df.sample(0.02)\n",
    "bike_sample = bike_df.sample(0.02)\n",
    "\n",
    "taxi_pd = taxi_sample.toPandas()\n",
    "bike_pd = bike_sample.toPandas()\n",
    "\n",
    "taxi_pd.info()\n",
    "bike_pd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "When a [Decimal Type](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.types.DecimalType.html) column in PySpark are converted to Pandas DataFrame, it is converted into object Type, not float type. To visualize these \"object\" columns, they need to be converted into the float type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOAT_TYPE_COLUMNS = [\"trip_distance\", \"fare_amount\", \"pickup_location_id\", \"dropoff_location_id\"]\n",
    "\n",
    "taxi_pd = taxi_pd.drop(columns=['pickup_datetime', 'dropoff_datetime'])\n",
    "for COLUMN in FLOAT_TYPE_COLUMNS:\n",
    "    taxi_pd[COLUMN] = taxi_pd[COLUMN].astype(float)\n",
    "taxi_pd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "Iterate through columns and plot them to box and histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    if column == 'is_weekdays':\n",
    "        continue\n",
    "    _, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    taxi_pd[column].plot(kind=\"box\", ax=ax[0])\n",
    "    taxi_pd[column].plot(kind=\"hist\", ax=ax[1])\n",
    "    plt.title(column)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_zone = gpd.read_file(\"https://data.cityofnewyork.us/api/geospatial/d3c5-ddgc?method=export&format=GeoJSON\")\n",
    "gdf_zone['location_id'] = gdf_zone['location_id'].astype('long')\n",
    "location_set = set()\n",
    "hm = gdf_zone.to_dict('index')\n",
    "for i in hm:\n",
    "    if hm[i]['borough'] == \"Manhattan\":\n",
    "        location_set.add(hm[i]['location_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_2018 = 1514782800 # Jan 1, 2018 00:00:00\n",
    "END_2018 = 1546318800 # Dec 31, 2018 23:59:59\n",
    "\n",
    "@pandas_udf('long')\n",
    "def preprocess_zone_id(lat: pd.Series, lon: pd.Series) -> pd.Series:\n",
    "    point_var = [Point(xy) for xy in zip(lon, lat)]\n",
    "    gdf_points = gpd.GeoDataFrame(pd.DataFrame({'lat': lat, 'lon': lon}), crs='epsg:4326', geometry=point_var)\n",
    "    gdf_joined = gpd.sjoin(gdf_points, gdf_zone, how='left')\n",
    "    return gdf_joined['location_id']\n",
    "\n",
    "@udf(returnType=BooleanType())\n",
    "def is_in_manhattan(location_id):\n",
    "    return location_id in location_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df = taxi_df.dropna()\n",
    "taxi_df = taxi_df.withColumn(\"dropoff_location_id\", taxi_df.dropoff_location_id.cast('int'))\n",
    "taxi_df = taxi_df.withColumn(\"pickup_location_id\", taxi_df.pickup_location_id.cast('int'))\n",
    "taxi_df = taxi_df.withColumn(\"is_start_manhattan\", is_in_manhattan(col(\"pickup_location_id\")))\n",
    "taxi_df = taxi_df.withColumn(\"is_end_manhattan\", is_in_manhattan(col(\"dropoff_location_id\")))\n",
    "\n",
    "taxi_df = taxi_df.where(\n",
    "    (col('start_time') >= START_2018)\n",
    "    & (col('start_time') <= END_2018)\n",
    "    & (col('end_time') >= START_2018)\n",
    "    & (col('end_time') <= END_2018)\n",
    "    & (col('start_time') < col('end_time'))\n",
    "    & (col('trip_duration') > 0) \n",
    "    & (col('trip_duration') < 4000)\n",
    "    & (col('trip_distance') > 0.2)\n",
    "    & (col('trip_distance') < 15)\n",
    "    & (col(\"pickup_location_id\") != col(\"dropoff_location_id\")) \n",
    "    & (col(\"fare_amount\") > 0)\n",
    "    & (col(\"fare_amount\") < 500)\n",
    "    & (col(\"is_start_manhattan\") == True)\n",
    "    & (col(\"is_end_manhattan\") == True)\n",
    ")\n",
    "\n",
    "taxi_df.printSchema()\n",
    "# taxi_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df = bike_df.withColumn('start_zone_id', preprocess_zone_id(bike_df['start_station_latitude'], bike_df['start_station_longitude']))\n",
    "bike_df = bike_df.withColumn('end_zone_id', preprocess_zone_id(bike_df['end_station_latitude'], bike_df['end_station_longitude']))\n",
    "bike_df = bike_df.withColumn('is_start_manhattan', is_in_manhattan(col('start_zone_id')))\n",
    "bike_df = bike_df.withColumn('is_end_manhattan', is_in_manhattan(col('end_zone_id')))\n",
    "\n",
    "bike_df = bike_df.where(\n",
    "    (col('tripduration') > 0)\n",
    "    & (col('tripduration') < 7200)\n",
    "    & (col(\"start_zone_id\") != col(\"end_zone_id\")) \n",
    "    & (col('usertype') == \"Subscriber\")\n",
    "    & (col('starttime') < col('stoptime'))\n",
    "    & (col(\"is_start_manhattan\") == True)\n",
    "    & (col(\"is_end_manhattan\") == True)\n",
    ").dropna()\n",
    "\n",
    "bike_df.printSchema()\n",
    "# bike_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_feature_cols = [\n",
    "    \"is_weekdays\",\n",
    "    \"start_time_in_minute\",\n",
    "    \"dropoff_location_id\",\n",
    "    \"pickup_location_id\",\n",
    "    \"trip_distance\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_feature_cols = [\n",
    "    \"is_weekdays\",\n",
    "    \"start_station_longitude\",\n",
    "    \"start_station_latitude\",\n",
    "    \"end_station_longitude\",\n",
    "    \"end_station_latitude\",\n",
    "    \"start_zone_id\",\n",
    "    \"end_zone_id\",\n",
    "    \"start_time_in_minute\",\n",
    "    \"trip_distance\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_assembler = VectorAssembler(inputCols=taxi_feature_cols, outputCol='features')\n",
    "bike_assembler = VectorAssembler(inputCols=bike_feature_cols, outputCol='features')\n",
    "standard_scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "gbt = GBTRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"trip_duration\",\n",
    "    predictionCol=\"pred_trip_duration\",\n",
    ")\n",
    "evaluator_r2 = RegressionEvaluator(\n",
    "    labelCol=gbt.getLabelCol(),\n",
    "    predictionCol=gbt.getPredictionCol(),\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "evaluator_rmse = RegressionEvaluator(\n",
    "    labelCol=gbt.getLabelCol(),\n",
    "    predictionCol=gbt.getPredictionCol(),\n",
    "    metricName=\"rmse\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_transformed_data = taxi_assembler.transform(taxi_df)\n",
    "taxi_scaled_df = standard_scaler.fit(taxi_transformed_data).transform(taxi_transformed_data)\n",
    "taxi_scaled_df.select(\"features\", \"features_scaled\").show(10, truncate=False)\n",
    "(taxi_training_data, taxi_test_data) = taxi_scaled_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Wall time: 1min 3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_transformed_data = bike_assembler.transform(bike_df)\n",
    "bike_scaled_df = standard_scaler.fit(bike_transformed_data).transform(bike_transformed_data)\n",
    "bike_scaled_df.select(\"features\", \"features_scaled\").show(10, truncate=False)\n",
    "(bike_training_data, bike_test_data) = bike_scaled_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Wall time: 2min 19s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_gbt_model = gbt.fit(taxi_training_data)\n",
    "taxi_gbt_predictions = taxi_gbt_model.transform(taxi_test_data)\n",
    "\n",
    "# Wall time: 7min 12s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_gbt_model = gbt.fit(bike_training_data)\n",
    "bike_gbt_predictions = bike_gbt_model.transform(bike_test_data)\n",
    "\n",
    "# Wall time: 16min 8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_gbt_accuracy_r2 = evaluator_r2.evaluate(taxi_gbt_predictions)\n",
    "taxi_gbt_accuracy_rmse = evaluator_rmse.evaluate(taxi_gbt_predictions)\n",
    "\n",
    "print(f\"Taxi Test GBT R2 Accuracy = {taxi_gbt_accuracy_r2}\")\n",
    "print(f\"Taxi Test GBT RMSE Accuracy = {taxi_gbt_accuracy_rmse}\")\n",
    "\n",
    "# RMSE:245.99563606237268\n",
    "\n",
    "# print(f\"Taxi Coefficients: {taxi_model.coefficients}\")\n",
    "# print(f\"Taxi Intercept: {taxi_model.intercept}\")\n",
    "# Taxi Test GBT R2 Accuracy = 0.708183954907601 <- \n",
    "# Taxi Test GBT R2 Accuracy = 0.6598682899938532\n",
    "# rmse = 265.5806200025988\n",
    "# Wall time: 2min 13s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_gbt_accuracy_r2 = evaluator_r2.evaluate(bike_gbt_predictions)\n",
    "bike_gbt_accuracy_rmse = evaluator_rmse.evaluate(bike_gbt_predictions)\n",
    "print(f\"Bike Test GBT R2 Accuracy = {bike_gbt_accuracy_r2}\")\n",
    "print(f\"Bike Test GBT RMSE Accuracy = {bike_gbt_accuracy_rmse}\")\n",
    "\n",
    "# print(f\"bike Coefficients: {bike_gbt_model.coefficients}\")\n",
    "# print(f\"bike Intercept: {bike_gbt_model.intercept}\")\n",
    "\n",
    "# Bike Test GBT R2 Accuracy = 0.540752577358068\n",
    "# Bike Test GBT RMSE Accuracy = 341.8116656204376\n",
    "\n",
    "# Exclude subscriber\n",
    "# Bike Test GBT R2 Accuracy = 0.4413798408518055\n",
    "# Bike Test GBT RMSE Accuracy = 440.703561958477\n",
    "\n",
    "# change to manhattan distance\n",
    "# Bike Test GBT R2 Accuracy = 0.5165664510244884\n",
    "# Bike Test GBT RMSE Accuracy = 350.56915653602664\n",
    "# Wall time: 4min 29s\n",
    "\n",
    "# include lat lon\n",
    "# Bike Test GBT R2 Accuracy = 0.5305022032396006\n",
    "# Bike Test GBT RMSE Accuracy = 345.53336096800973\n",
    "# Wall time: 5min 48s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Save the model to a Cloud Storage path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_gbt_model.write().overwrite().save(f\"{BUCKET_URI}/\")\n",
    "# print(bike_gbt_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "### Delete Vertex AI Workbench - Managed Notebook\n",
    "\n",
    "To delete Vertex Ai Workbench - Managed Notebook used in this project, you can use this [Clean up](https://cloud.google.com/vertex-ai/docs/workbench/managed/create-managed-notebooks-instance-console-quickstart#clean-up) part of `Managed notebooks` page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "### Delete a Dataproc Cluster\n",
    "\n",
    "To delete a Dataproc Cluster, you can use this [Deleting a cluster](https://cloud.google.com/dataproc/docs/guides/manage-cluster#deleting_a_cluster) part of `Manage a cluster` page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7a1a1a5e978"
   },
   "outputs": [],
   "source": [
    "# Delete Google Cloud Storage bucket\n",
    "! gsutil rm -r $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b77295420ab9"
   },
   "outputs": [],
   "source": [
    "# Delete BigQuery dataset\n",
    "! bq rm -r -f $DATASET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "After you delete the BigQuery dataset, you can check your Datasets in BigQuery using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c53e6e169788"
   },
   "outputs": [],
   "source": [
    "! bq ls"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "spark_sample_notebook.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18ebbd838e32"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aef73cfa8725"
      },
      "source": [
        "# Predictive Maintenance using Vertex AI\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/workbench/predictive_maintainance/predictive_maintenance_usecase.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "    <td>\n",
        "        <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/workbench/predictive_maintainance/predictive_maintenance_usecase.ipynb\">\n",
        "        <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\\\" alt=\"Colab logo\"> Run in Colab\n",
        "        </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/workbench/predictive_maintainance/predictive_maintenance_usecase.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "<br/><br/><br/>\n",
        "\n",
        "\n",
        "## Table of contents\n",
        "* [Overview](#section-1)\n",
        "* [Objective](#section-2)\n",
        "* [Dataset](#section-3)\n",
        "* [Costs](#section-4)\n",
        "* [Data analysis](#section-5)\n",
        "* [Fit a regression model](#section-6)\n",
        "* [Evaluate the trained model](#section-7)\n",
        "* [Save the model](#section-8)\n",
        "* [Running a notebook end-to-end using the executor](#section-9)\n",
        "* [Hosting the model on Vertex AI](#section-10)\n",
        "  * [Create an endpoint](#section-11)\n",
        "  * [Deploy the model to the created endpoint](#section-12)\n",
        "  * [Test calling the endpoint](#section-13)\n",
        "* [Clean up](#section-14)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e10c5167a061"
      },
      "source": [
        "## Overview\n",
        "<a name=\"section-1\"></a>\n",
        "\n",
        "In this notebook, you go through a predictive maintenance usecase on industrial data using machine learning techniques, deploy the machine learning model on Vertex AI, and automate the workflow using the executor feature of Vertex AI Workbench.\n",
        "\n",
        "*Note: This notebook file is developed to run in a [Vertex AI Workbench managed notebooks](https://console.cloud.google.com/vertex-ai/workbench/list/managed) instance using the XGBoost (Local) kernel. Some components of this notebook may not work in other notebook environments.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fead9e83ebd7"
      },
      "source": [
        "### Objective\n",
        "<a name=\"section-2\"></a>\n",
        "\n",
        "The objectives of this notebook include:\n",
        "\n",
        "- Loading the required dataset from a Cloud Storage bucket.\n",
        "- Analyzing the fields present in the dataset.\n",
        "- Selecting the required data for the predictive maintenance model.\n",
        "- Training an XGBoost regression model for predicting the remaining useful life.\n",
        "- Evaluating the model.\n",
        "- Running the notebook end-to-end as a training job using Executor.\n",
        "- Deploying the model on Vertex AI.\n",
        "- Clean up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a71f4d96bf80"
      },
      "source": [
        "### Dataset\n",
        "<a name=\"section-3\"></a>\n",
        "\n",
        "The dataset used in this notebook is a part of the [NASA Turbofan Engine Degradation Simulation dataset](https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/), which consists of simulated time-series data for four sets of fleet engines under different combinations of operational conditions and fault modes. A version of this dataset which is saved to a public Cloud Storage bucket is used in this notebook. In this notebook, one of the engine's simulated data (FD001) is used to analyze and train a model that can predict the engine's remaining useful life."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36c53c95b4b9"
      },
      "source": [
        "### Costs\n",
        "<a name=\"section-4\"></a>\n",
        "\n",
        "This tutorial uses the following billable components of Google Cloud:\n",
        "\n",
        "- Vertex AI\n",
        "- Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "629f52f6efe1"
      },
      "source": [
        "### Kernel selection\n",
        "Select <b>XGBoost</b> kernel while running this notebook on Vertex AI Workbench's managed instances. Otherwise, ensure that the following libraries are installed in the environment where this notebook is being run.\n",
        "- XGBoost\n",
        "- Pandas\n",
        "- Seaborn\n",
        "- Sklearn\n",
        "\n",
        "Along with the above libraries, th`e following google-cloud libraries are also used in this notebook.\n",
        "\n",
        "- google.cloud.aiplatform\n",
        "- google.cloud.storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16bee0754628"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Install the following packages to run this notebook outside Vertex AI Workbench's managed instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69520a67e54c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Vertex AI Workbench Notebook product has specific requirements\n",
        "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
        "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
        "    \"/opt/deeplearning/metadata/env_version\"\n",
        ")\n",
        "\n",
        "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_WORKBENCH_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\"\n",
        "    \n",
        "! pip3 install {USER_FLAG} --upgrade google-cloud-aiplatform \\\n",
        "                                    google-cloud-storage \\\n",
        "                                    xgboost \\\n",
        "                                    seaborn \\\n",
        "                                    sklearn \\\n",
        "                                    fsspec \\\n",
        "                                    gcsfs \\\n",
        "                                    pandas -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eda79cca981d"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e200999cabe5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b15a97278df"
      },
      "source": [
        "## Before you begin \n",
        "\n",
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "1. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "1. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
        "\n",
        "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aee4379e8e5"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d229c5c1db1b"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bf9979b96ff"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID:\", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09021c90b34c"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9658ecf524b1"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook.  Below are regions supported for Vertex AI. It is recommended that you choose the region closest to you.\n",
        "\n",
        "- Americas: `us-central1`\n",
        "- Europe: `europe-west4`\n",
        "- Asia Pacific: `asia-east1`\n",
        "\n",
        "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
        "\n",
        "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c615e53149f"
      },
      "outputs": [],
      "source": [
        "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
        "\n",
        "if REGION == \"[your-region]\":\n",
        "    REGION = \"us-central1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f66f96816fd0"
      },
      "source": [
        "#### UUID\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a uuid for each instance session, and append it onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac7ff88c7f84"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "\n",
        "# Generate a uuid of a specifed length(default=8)\n",
        "def generate_uuid(length: int = 8) -> str:\n",
        "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
        "\n",
        "\n",
        "UUID = generate_uuid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df899ce9999c"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using Vertex AI Workbench Notebooks**, your environment is already\n",
        "authenticated.\n",
        "\n",
        "**If you are using Colab**, run the cell below and follow the instructions\n",
        "when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "1. In the Cloud Console, go to the [**Create service account key**\n",
        "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
        "\n",
        "2. Click **Create service account**.\n",
        "\n",
        "3. In the **Service account name** field, enter a name, and\n",
        "   click **Create**.\n",
        "\n",
        "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
        "into the filter box, and select\n",
        "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "\n",
        "5. Click *Create*. A JSON file that contains your key downloads to your\n",
        "local environment.\n",
        "\n",
        "6. Enter the path to your service account key as the\n",
        "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "201e8e760d22"
      },
      "outputs": [],
      "source": [
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# If on Vertex AI Workbench, then don't execute this code\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
        "    \"DL_ANACONDA_HOME\"\n",
        "):\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea53caa30628"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "\n",
        "When you create a model in Vertex AI using the Cloud SDK, you give a Cloud Storage path where the trained model is saved. \n",
        "In this tutorial, Vertex AI saves the trained model to a Cloud Storage bucket. Using this model artifact, you can then\n",
        "create Vertex AI model and endpoint resources in order to serve\n",
        "online predictions.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
        "Cloud Storage buckets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4a1a3af1611d"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31f8a617f42f"
      },
      "outputs": [],
      "source": [
        "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
        "    BUCKET_NAME = PROJECT_ID + \"aip-\" + UUID\n",
        "    BUCKET_URI = f\"gs://{BUCKET_NAME}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e27907c70873"
      },
      "source": [
        "<b>Only if your bucket doesn't already exist</b>: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbd413ba606f"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7481ea67bbe2"
      },
      "source": [
        "Next, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0237e1b73154"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c0f6aac282a"
      },
      "source": [
        "### Import the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33f840806ed4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "%matplotlib inline\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import xgboost as xgb\n",
        "from google.cloud import aiplatform, storage\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08bcba53eb99"
      },
      "source": [
        "Load the data and check the data shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2890f46a1907"
      },
      "outputs": [],
      "source": [
        "# load the data from the source\n",
        "INPUT_PATH = \"gs://cloud-samples-data/ai-platform-unified/datasets/tabular/predictive_maintenance.csv\"  # data source\n",
        "raw_data = pd.read_csv(INPUT_PATH, sep=\" \", header=None)\n",
        "# check the data\n",
        "print(raw_data.shape)\n",
        "raw_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cfc304d35b5"
      },
      "source": [
        "The data itself doesn't contain any feature names and thus needs its columns to be renamed. The data source already provides some data description. Apparently, the <b>ID</b> column represents the unit-number of the fleet-engine and <b>Cycle</b> represents the time in cycles. <b>OpSet1</b>,<b>Opset2</b> & <b>Opset3</b> represent the three operational settings that are described in the original data source and have a substantial effect on engine performance. The rest of the fields show sensor readings collected from 21 different sensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34251837c120"
      },
      "outputs": [],
      "source": [
        "# name the columns (based on the original data source page)\n",
        "raw_data = raw_data[[f for f in range(0, 26)]]\n",
        "raw_data.columns = [\n",
        "    \"ID\",\n",
        "    \"Cycle\",\n",
        "    \"OpSet1\",\n",
        "    \"OpSet2\",\n",
        "    \"OpSet3\",\n",
        "    \"SensorMeasure1\",\n",
        "    \"SensorMeasure2\",\n",
        "    \"SensorMeasure3\",\n",
        "    \"SensorMeasure4\",\n",
        "    \"SensorMeasure5\",\n",
        "    \"SensorMeasure6\",\n",
        "    \"SensorMeasure7\",\n",
        "    \"SensorMeasure8\",\n",
        "    \"SensorMeasure9\",\n",
        "    \"SensorMeasure10\",\n",
        "    \"SensorMeasure11\",\n",
        "    \"SensorMeasure12\",\n",
        "    \"SensorMeasure13\",\n",
        "    \"SensorMeasure14\",\n",
        "    \"SensorMeasure15\",\n",
        "    \"SensorMeasure16\",\n",
        "    \"SensorMeasure17\",\n",
        "    \"SensorMeasure18\",\n",
        "    \"SensorMeasure19\",\n",
        "    \"SensorMeasure20\",\n",
        "    \"SensorMeasure21\",\n",
        "]\n",
        "raw_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da8c475ddc86"
      },
      "source": [
        "## Data Analysis\n",
        "<a name=\"section-5\"></a>\n",
        "The current dataset consists of timeseries data for various unit IDs. The data is represented in terms of cycles. Lets first see the distribution of number of cycles across the units."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e18ef3931be5"
      },
      "outputs": [],
      "source": [
        "# plot the cycle count for each IDs\n",
        "raw_data[[\"ID\", \"Cycle\"]].groupby(by=[\"ID\"]).count().plot(kind=\"bar\", figsize=(12, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43c3f01352ad"
      },
      "source": [
        "On an average, there seem to be around 225 cycles per each ID in the dataset. Next, lets check the data types of the fields and the number of null records in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d83cddab2b28"
      },
      "outputs": [],
      "source": [
        "# check the data-types\n",
        "raw_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da698b792397"
      },
      "source": [
        "The data doesn't have any null records or any categorical fields. Next, lets check the numerical distribution of the fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18d8731e68aa"
      },
      "outputs": [],
      "source": [
        "# check the numerical characteristics of the data\n",
        "raw_data.describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b632592001d5"
      },
      "source": [
        "Features **OpSet3**, **SensorMeasure1**, **SensorMeasure10**, **SensorMeasure18** & **SensorMeasure19** seem to be constant throughout the dataset and thus can be eliminated. Apart from the fields that are constant throughout the data, fields that are correlated highly can also be considered for dropping. Having highly correlated fields in the data often leads to multi-collinearity situation which unnecessarily increases the size of feature-space even if it doesn't affect the accuracy much. Such fields can be identified through correlation-matrices and heatmaps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d41a0e277b2"
      },
      "outputs": [],
      "source": [
        "# plot the correlation matrix\n",
        "plt.figure(figsize=(15, 10))\n",
        "cols = [\n",
        "    i\n",
        "    for i in raw_data.columns\n",
        "    if i\n",
        "    not in [\n",
        "        \"ID\",\n",
        "        \"Cycle\",\n",
        "        \"OpSet3\",\n",
        "        \"SensorMeasure1\",\n",
        "        \"SensorMeasure10\",\n",
        "        \"SensorMeasure18\",\n",
        "        \"SensorMeasure19\",\n",
        "    ]\n",
        "]\n",
        "corr_mat = raw_data[cols].corr()\n",
        "matrix = np.triu(corr_mat)\n",
        "\n",
        "sns.heatmap(corr_mat, annot=True, mask=matrix, fmt=\".1g\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "284debdf4294"
      },
      "source": [
        "Fields **SensorMeasure7**, **SensorMeasure12**, **SensorMeasure20** & **SensorMeasure21** correlate highly with many other fields. These fields can be omitted. Further, **SensorMeasure8**, **SensorMeasure11** and **SensorMeasure4** seem highly correlated with each other and so any one of them, for example, **SensorMeasure4**, can be kept and the rest can be omitted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40477e592ce7"
      },
      "outputs": [],
      "source": [
        "cols = [\n",
        "    i\n",
        "    for i in cols\n",
        "    if i\n",
        "    not in [\n",
        "        \"SensorMeasure7\",\n",
        "        \"SensorMeasure12\",\n",
        "        \"SensorMeasure20\",\n",
        "        \"SensorMeasure21\",\n",
        "        \"SensorMeasure8\",\n",
        "        \"SensorMeasure11\",\n",
        "    ]\n",
        "]\n",
        "corr_mat = raw_data[cols].corr()\n",
        "matrix = np.triu(corr_mat)\n",
        "plt.figure(figsize=(9, 5))\n",
        "sns.heatmap(corr_mat, annot=True, mask=matrix, fmt=\".1g\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8197cdef2cff"
      },
      "source": [
        "As the current objective is to predict the remaining useful life (RUL) of each unit (ID), the target variable needs to be identified. Since you're dealing with a timeseries data that represents the lifetime of a unit, remaining useful life of a unit can be calculated by subtracting the current cycle from the maximum cycle of that unit.\n",
        "\n",
        "\t\t\t\t\tRUL = Max. Cycle - Current Cycle    \n",
        "## RUL calculation and Feature selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9cd2797cdae"
      },
      "outputs": [],
      "source": [
        "# get max-cycle of the ids\n",
        "cols = [\"ID\", \"Cycle\"] + cols\n",
        "max_cycles_df = (\n",
        "    raw_data.groupby([\"ID\"], sort=False)[\"Cycle\"]\n",
        "    .max()\n",
        "    .reset_index()\n",
        "    .rename(columns={\"Cycle\": \"MaxCycleID\"})\n",
        ")\n",
        "# merge back to original dataset\n",
        "FD001_df = pd.merge(raw_data, max_cycles_df, how=\"inner\", on=\"ID\")\n",
        "# calculate rul from max-cycle and current-cycle\n",
        "FD001_df[\"RUL\"] = FD001_df[\"MaxCycleID\"] - FD001_df[\"Cycle\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53a73de9ee27"
      },
      "source": [
        "To ensure that the target field is generated properly, the RUL field can be plotted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecd5aa0a130f"
      },
      "outputs": [],
      "source": [
        "# plot the RUL vs Cycles\n",
        "one_engine = []\n",
        "for i, r in FD001_df.iterrows():\n",
        "    rul = r[\"RUL\"]\n",
        "    one_engine.append(rul)\n",
        "    if rul == 0:\n",
        "        plt.plot(one_engine)\n",
        "        one_engine = []\n",
        "\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc3b82355cdc"
      },
      "source": [
        "The above plot suggests that the RUL, in other words, the remaining cycles, is decreasing as the current cycle increases which is expected. Further, lets see the how the other fields relate to RUL in the current dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30284ff6c8ab"
      },
      "outputs": [],
      "source": [
        "# plot feature vs the RUL\n",
        "def plot_feature(feature):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    for i in FD001_df[\"ID\"].unique():\n",
        "        if i % 10 == 0:  # only plot every 10th ID\n",
        "            plt.plot(\"RUL\", feature, data=FD001_df[FD001_df[\"ID\"] == i])\n",
        "    plt.xlim(250, 0)  # reverse the x-axis so RUL counts down to zero\n",
        "    plt.xticks(np.arange(0, 275, 25))\n",
        "    plt.ylabel(feature)\n",
        "    plt.xlabel(\"RUL\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "for i in cols:\n",
        "    if i not in [\"ID\", \"Cycle\"]:\n",
        "        plot_feature(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "828154ec8d90"
      },
      "source": [
        "The following set of observations can be made from the outcome of the above cell :\n",
        "- Fields **SensorMeasure5** and **SensorMeasure16** don't show much variance with the RUL and seem constant all the time. Hence, they can be removed.\n",
        "- Fields **SensorMeasure2**, **SensorMeasure3**, **SensorMeasure4**, **SensorMeasure13**, **SensorMeasure15** & **SensorMeasure17** show a similar rising trend.\n",
        "- **SensorMeasure9** and **SensorMeasure14** show a similar trend.\n",
        "- **SensorMeasure6** shows a flatline most of the time except in a very few places and therefore can be ignored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28395edac8f7"
      },
      "outputs": [],
      "source": [
        "# remove the unnecessary fields\n",
        "cols = [\n",
        "    i\n",
        "    for i in cols\n",
        "    if i not in [\"ID\", \"SensorMeasure5\", \"SensorMeasure6\", \"SensorMeasure16\"]\n",
        "]\n",
        "cols"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cae198bd96ef"
      },
      "source": [
        "## Split the data into train and test\n",
        "\n",
        "Divide the dataset with the selected features into train and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40a38e17b6b0"
      },
      "outputs": [],
      "source": [
        "# split data into train and test\n",
        "X = FD001_df[cols].copy()\n",
        "y = FD001_df[\"RUL\"].copy()\n",
        "\n",
        "# split the data into 70-30 ratio of train-test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, train_size=0.7, random_state=36\n",
        ")\n",
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43a26d74c687"
      },
      "source": [
        "## Fit a regression model\n",
        "<a name=\"section-6\"></a>\n",
        "\n",
        "Initialize and train a regression model using the XGBoost library with the calculated RUL as the target feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "306a693a00ff"
      },
      "outputs": [],
      "source": [
        "model = xgb.XGBRegressor()\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef2a6b567e33"
      },
      "source": [
        "## Evaluate the trained model\n",
        "<a name=\"section-7\"></a>\n",
        "\n",
        "Check the R2 scores of the model on train and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e30af6e41081"
      },
      "outputs": [],
      "source": [
        "# print test R2 score\n",
        "y_train_pred = model.predict(X_train)\n",
        "train_score = r2_score(y_train, y_train_pred)\n",
        "y_test_pred = model.predict(X_test)\n",
        "test_score = r2_score(y_test, y_test_pred)\n",
        "print(\"Train score:\", train_score)\n",
        "print(\"Test score:\", test_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9e00d61b71c"
      },
      "source": [
        "Check the RMSE errors on train and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e32f5bcd6b0"
      },
      "outputs": [],
      "source": [
        "# print train and test RMSEs\n",
        "train_error = mean_squared_error(y_train, y_train_pred, squared=False)\n",
        "test_error = mean_squared_error(y_test, y_test_pred, squared=False)\n",
        "print(\"Train error:\", train_error)\n",
        "print(\"Test error:\", test_error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18e4cf950007"
      },
      "source": [
        "Plot the predicted values against the target values. The closer the plot to a straight line passing through origin with a unit slope, the better the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bde609ba3f9"
      },
      "outputs": [],
      "source": [
        "# plot the train and test predictions\n",
        "plt.scatter(y_train, y_train_pred)\n",
        "plt.xlabel(\"Target\")\n",
        "plt.ylabel(\"Prediction\")\n",
        "plt.title(\"Train\")\n",
        "plt.show()\n",
        "plt.scatter(y_test, y_test_pred)\n",
        "plt.xlabel(\"Target\")\n",
        "plt.ylabel(\"Prediction\")\n",
        "plt.title(\"Test\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbbcd5de298f"
      },
      "source": [
        "## Save the model\n",
        "<a name=\"section-8\"></a>\n",
        "\n",
        "Save the model to a booster file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fdd760d9a74"
      },
      "outputs": [],
      "source": [
        "# save the trained model to a local file \"model.bst\"\n",
        "FILE_NAME = \"model.bst\"\n",
        "model.save_model(FILE_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a83c6f381722"
      },
      "source": [
        "Copy the model to the cloud-storage bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81fa220c7fd4"
      },
      "outputs": [],
      "source": [
        "# Upload the saved model file to Cloud Storage\n",
        "BLOB_PATH = \"mfg_predictive_maintenance/\"\n",
        "BLOB_NAME = os.path.join(BLOB_PATH, FILE_NAME)\n",
        "bucket = storage.Client().bucket(BUCKET_NAME)\n",
        "blob = bucket.blob(BLOB_NAME)\n",
        "blob.upload_from_filename(FILE_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bd88d7f4bbb"
      },
      "source": [
        "## Running a notebook end-to-end using executor\n",
        "<a name=\"section-9\"></a>\n",
        "\n",
        "**Note:** This section can only be considered when running this notebook on Managed instances from Vertex AI Workbench.\n",
        "### Automating the notebook execution\n",
        "All the steps followed until now can be run as a training job without using any additional code using the Vertex AI Workbench executor. The executor can help you run a notebook file from start to end, with your choice of the environment, machine type, input parameters, and other characteristics. After setting up an execution, the notebook is executed as a job in Vertex AI custom training. Your jobs can be monitored from the Executor pane in the left sidebar.\n",
        "\n",
        "<img src=\"images/executor.PNG\">\n",
        "\n",
        "The executor also lets you choose the environment and machine type while automating the runs similar to Vertex AI training jobs without switching to the training jobs UI. Apart from the custom container that replicates the existing kernel by default, pre-built environments like TensorFlow Enterprise, PyTorch, and others can also be selected to run the notebook. The required compute power can be specified by choosing from the list of machine types available, including GPUs.\n",
        "\n",
        "### Scheduled runs on executor\n",
        "\n",
        "Notebook runs can also be scheduled recurringly with the executor. To do so, select Schedule-based recurring executions as the run type instead of One-time execution. The frequency of the job and the time when it executes is provided when you create the execution.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/7_Vertex_AI_Workbench.max-1100x1100.jpg\">\n",
        "\n",
        "### Parameterizing the variables\n",
        "\n",
        "The executor lets you run a notebook with different sets of input parameters. If you’ve added parameter tags to any of your notebook cells, you can pass in your parameter values to the executor. More about how to use this feature can be found on this [blog](https://cloud.google.com/blog/products/ai-machine-learning/schedule-and-execute-notebooks-with-vertex-ai-workbench).\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/6_Vertex_AI_Workbench.max-700x700.jpg\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57522cb1d5e7"
      },
      "source": [
        "## Hosting the model on Vertex AI\n",
        "<a name=\"section-10\"></a>\n",
        "\n",
        "### Create a model resource\n",
        "\n",
        "The saved model from the Cloud Storage can be deployed easily using the Vertex AI SDK. To do so, first create a model resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "246206c9eedc"
      },
      "outputs": [],
      "source": [
        "ARTIFACT_GCS_PATH = f\"gs://{BUCKET_NAME}/{BLOB_PATH}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aa75b3d4616"
      },
      "source": [
        "Give a display name to the Vertex AI model resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02ca350dba6c"
      },
      "outputs": [],
      "source": [
        "# Set the model-dsiplay-name\n",
        "MODEL_DISPLAY_NAME = \"[your-model-display-name]\"  # @param {type:\"string\"}\n",
        "\n",
        "# Otherwise, use the default name\n",
        "if (\n",
        "    MODEL_DISPLAY_NAME == \"[your-model-display-name]\"\n",
        "    or MODEL_DISPLAY_NAME is None\n",
        "    or MODEL_DISPLAY_NAME == \"\"\n",
        "):\n",
        "    MODEL_DISPLAY_NAME = \"pred_maint_model_\" + UUID\n",
        "\n",
        "print(MODEL_DISPLAY_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49f1acde47e3"
      },
      "outputs": [],
      "source": [
        "# Create a Vertex AI model resource\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "model = aiplatform.Model.upload(\n",
        "    display_name=MODEL_DISPLAY_NAME,\n",
        "    artifact_uri=ARTIFACT_GCS_PATH,\n",
        "    serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-1:latest\",\n",
        ")\n",
        "\n",
        "model.wait()\n",
        "\n",
        "print(model.display_name)\n",
        "print(model.resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7784e605fe20"
      },
      "source": [
        "### Create an Endpoint\n",
        "<a name=\"section-11\"></a>\n",
        "\n",
        "\n",
        "Next, create an endpoint resource for deploying the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1e0cd571992"
      },
      "outputs": [],
      "source": [
        "# Set the endpoint-dsiplay-name\n",
        "ENDPOINT_DISPLAY_NAME = \"[your-endpoint-display-name]\"  # @param {type:\"string\"}\n",
        "\n",
        "# Otherwise, use the default name\n",
        "if (\n",
        "    ENDPOINT_DISPLAY_NAME == \"[your-endpoint-display-name]\"\n",
        "    or ENDPOINT_DISPLAY_NAME is None\n",
        "    or ENDPOINT_DISPLAY_NAME == \"\"\n",
        "):\n",
        "    ENDPOINT_DISPLAY_NAME = \"pred_maint_endpoint_\" + UUID\n",
        "\n",
        "print(ENDPOINT_DISPLAY_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ae2e3b9555e"
      },
      "outputs": [],
      "source": [
        "# Create the Endpoint resource\n",
        "endpoint = aiplatform.Endpoint.create(display_name=ENDPOINT_DISPLAY_NAME)\n",
        "\n",
        "print(endpoint.display_name)\n",
        "print(endpoint.resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08128843a059"
      },
      "source": [
        "### Deploy the model to the created Endpoint\n",
        "<a name=\"section-12\"></a>\n",
        "\n",
        "\n",
        "Configure the following parameters and deploy the model to the created endpoint.\n",
        "\n",
        "- `endpoint`: The `Endpoint` object created using Vertex AI SDK.\n",
        "- `deployed_model_display_name`: A display-name for the deployment.\n",
        "- `machine_type`: Type of the machine required for the deployment environment. See [here](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute) for references."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c3e7597d233"
      },
      "outputs": [],
      "source": [
        "# deploy the model to the endpoint\n",
        "model.deploy(\n",
        "    endpoint=endpoint,\n",
        "    deployed_model_display_name=MODEL_DISPLAY_NAME + \"_deployment\",\n",
        "    machine_type=\"n1-standard-2\",\n",
        ")\n",
        "\n",
        "model.wait()\n",
        "\n",
        "print(model.display_name)\n",
        "print(model.resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d24eb85cf12"
      },
      "source": [
        "## Test calling the endpoint\n",
        "<a name=\"section-13\"></a>\n",
        "\n",
        "Send some sample data to the deployed model on the endpoint to get predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb6b0fc29a16"
      },
      "outputs": [],
      "source": [
        "# get predictions on sample data\n",
        "instances = X_test.iloc[0:2].to_numpy().tolist()\n",
        "print(endpoint.predict(instances=instances).predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cec4549f32c"
      },
      "source": [
        "## Clean up\n",
        "<a name=\"section-14\"></a>\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:\n",
        "* Vertex AI Model\n",
        "* Vertex AI Endpoint\n",
        "* Cloud Storage bucket\n",
        "\n",
        "Set `delete_bucket` to **True** to delete the Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "737a7b8eb6d6"
      },
      "outputs": [],
      "source": [
        "# Undeploy all the models from the endpoint\n",
        "endpoint.undeploy_all()\n",
        "\n",
        "# Delete the endpoint resource\n",
        "endpoint.delete()\n",
        "\n",
        "# Delete the model resource\n",
        "model.delete()\n",
        "\n",
        "# Delete the Cloud Storage bucket\n",
        "delete_bucket = False\n",
        "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
        "    ! gsutil -m rm -r $BUCKET_URI"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "predictive_maintenance_usecase.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Using Vertex AI Multimodal Embeddings and Matching Engine\n",
        "![ ](https://www.google-analytics.com/collect?v=2&tid=G-L6X3ECH596&cid=1&en=page_view&sid=1&dt=sdk_matching_engine_create_multimodal_embeddings.ipynb&dl=notebooks%2Fofficial%2Fmatching_engine%2Fsdk_matching_engine_create_multimodal_embeddings.ipynb)\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/matching_engine/sdk_matching_engine_create_multimodal_embeddings.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/matching_engine/sdk_matching_engine_create_multimodal_embeddings.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "      <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/matching_engine/sdk_matching_engine_create_multimodal_embeddings.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0a74aaf1481"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This example demonstrates how to create text-to-image embeddings using the DiffusionDB dataset and the Vertex AI Multimodal Embeddings model. The embeddings are uploaded to the Vertex AI Matching Engine service, which is a high scale, low latency solution to find similar vectors for a large corpus. Moreover, it is a fully managed offering, further reducing operational overhead. It is built upon [Approximate Nearest Neighbor (ANN) technology](https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html) developed by Google Research.\n",
        "\n",
        "To learn more, see the official documentation for [Vertex AI Multimodal Embeddings](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-multimodal-embeddings#supported_models), and [Vertex AI Matching Engine](https://cloud.google.com/vertex-ai/docs/matching-engine/overview)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34a4b245e795"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this notebook, you learn how to encode custom text embeddings, create an  Approximate Nearest Neighbor (ANN) index, and query against indexes.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services:\n",
        "\n",
        "- `Vertex AI Multimodal Embeddings`\n",
        "- `Vertex AI Matching Engine`\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "* Convert an image dataset to embeddings\n",
        "* Create an index\n",
        "* Upload embeddings to the index\n",
        "* Create an index endpoint\n",
        "* Deploy the index to the index endpoint\n",
        "* Perform an online query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset used for this tutorial is the [DiffusionDB dataset](https://github.com/poloclub/diffusiondb).\n",
        "\n",
        "> DiffusionDB is the first large-scale text-to-image prompt dataset. It contains 14 million images generated by Stable Diffusion using prompts and hyperparameters specified by real users. The unprecedented scale and diversity of this human-actuated dataset provide exciting research opportunities in understanding the interplay between prompts and generative models, detecting deepfakes, and designing human-AI interaction tools to help users more easily use these models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0f1bea346db"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Install the latest version of Cloud Storage and the Vertex AI SDK for Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfbccc635a17"
      },
      "outputs": [],
      "source": [
        "# Install the packages\n",
        "! pip3 install --upgrade google-cloud-aiplatform \\\n",
        "                        google-cloud-storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97473593f37f"
      },
      "source": [
        "Install the latest version of google-cloud-vision for filtering for safe images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5709d1d57927"
      },
      "outputs": [],
      "source": [
        "# Install the packages\n",
        "! pip install google-cloud-vision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b08ba354c6e"
      },
      "source": [
        "### Colab only: Uncomment the following cell to restart the kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bea801acf6b5"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "# import IPython\n",
        "\n",
        "# app = IPython.Application.instance()\n",
        "# app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd28c9e4f067"
      },
      "source": [
        "## Before you begin\n",
        "#### Set your project ID\n",
        "\n",
        "If you don't know your project ID, try the following:\n",
        "* Run `gcloud config list`.\n",
        "* Run `gcloud projects list`.\n",
        "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80c0215f05a0"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f4512bf63b3"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "474be5183c27"
      },
      "outputs": [],
      "source": [
        "REGION = \"us-central1\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "949271bfebe3"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b65b4ce80d9a"
      },
      "source": [
        "**1. Vertex AI Workbench**\n",
        "* Do nothing as you are already authenticated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "985cdbfe7372"
      },
      "source": [
        "**2. Local JupyterLab instance, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fbc9cd30cc4b"
      },
      "outputs": [],
      "source": [
        "# ! gcloud auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79efab26ad02"
      },
      "source": [
        "**3. Colab, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "a336a05c6149"
      },
      "outputs": [],
      "source": [
        "# from google.colab import auth\n",
        "# auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c0a44fa330f"
      },
      "source": [
        "**4. Service account or other**\n",
        "* See how to grant Cloud Storage permissions to your service account at https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3uj8x73nDX_"
      },
      "source": [
        "* Authentication: Rerun the `gcloud auth login` command in the Vertex AI Workbench notebook terminal when you are logged out and need the credential again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgPO1eR3CYjk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "Create a storage bucket to store intermediate artifacts such as datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MzGDU7TWdts_"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = \"gs://your-bucket-name-unique\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EcIXiGsCePi"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIq7R4HZCfIc"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR6Wwv-hCCN-"
      },
      "source": [
        "## Prepare the data\n",
        "\n",
        "You use [DiffusionDB dataset](https://github.com/poloclub/diffusiondb) of image prompt and image pairs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e2a05095082"
      },
      "source": [
        "### Clone the DiffusionDB repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wzS85TeB9dG"
      },
      "outputs": [],
      "source": [
        "! git clone https://github.com/poloclub/diffusiondb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49ee8d2c2df7"
      },
      "source": [
        "### Install the dependencies for downloading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed67efa3c0f3"
      },
      "outputs": [],
      "source": [
        "! pip install -r diffusiondb/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0bc039db70e"
      },
      "source": [
        "### Download image files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7d27d986c61f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading files |████████████████████████████████████████| 4/4 [100%] in 36.3s\u001b[?25h\u001b[J\n"
          ]
        }
      ],
      "source": [
        "# Download image files from 1 to 5. Each file is 1000 images.\n",
        "! python diffusiondb/scripts/download.py -i 1 -r 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a514786370e5"
      },
      "source": [
        "### Extract image archives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b43937b6065d"
      },
      "outputs": [],
      "source": [
        "# Unzip all image files\n",
        "image_directory = \"extracted\"\n",
        "\n",
        "! unzip -n 'images/*.zip' -d '{image_directory}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09a6602bb745"
      },
      "source": [
        "### Load image metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2cacd9869ee5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4000"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "metadatas = {}\n",
        "for file_name in os.listdir(image_directory):\n",
        "    if file_name.endswith(\".json\"):\n",
        "        with open(os.path.join(image_directory, file_name)) as f:\n",
        "            metadata = json.load(f)\n",
        "            metadatas.update(metadata)\n",
        "\n",
        "image_names = list(metadatas.keys())\n",
        "image_paths = [os.path.join(image_directory, image_name) for image_name in image_names]\n",
        "\n",
        "len(metadatas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a593ec69048"
      },
      "source": [
        "### Define function to detect explicit images\n",
        "\n",
        "Define a function to query the Cloud Vision API to detect potential explicit images.\n",
        "\n",
        "Learn more about content detection (SafeSearch) in [Detect explicit content](https://cloud.google.com/vision/docs/detecting-safe-search)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ce31cc893826"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "\n",
        "from google.cloud import vision\n",
        "from google.cloud.vision_v1.types.image_annotator import SafeSearchAnnotation\n",
        "\n",
        "client = vision.ImageAnnotatorClient()\n",
        "\n",
        "\n",
        "def detect_safe_search(path: str) -> Optional[SafeSearchAnnotation]:\n",
        "    \"\"\"Detects unsafe features in the file.\"\"\"\n",
        "\n",
        "    with open(path, \"rb\") as image_file:\n",
        "        content = image_file.read()\n",
        "\n",
        "    image = vision.Image(content=content)\n",
        "\n",
        "    response = client.safe_search_detection(image=image)\n",
        "\n",
        "    if response.error.message:\n",
        "        print(response.error.message)\n",
        "        return None\n",
        "\n",
        "    return response.safe_search_annotation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e330a70627ed"
      },
      "source": [
        "### Define SafeSearch annotation conversion to boolean values\n",
        "Define a function to convert SafeSearch annotation results to a boolean value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fc3c32afb383"
      },
      "outputs": [],
      "source": [
        "from google.cloud.vision_v1.types.image_annotator import Likelihood\n",
        "\n",
        "\n",
        "# Returns true if some annotations have a potential safety issues\n",
        "def convert_annotation_to_safety(safe_search_annotation: SafeSearchAnnotation) -> bool:\n",
        "    return all(\n",
        "        [\n",
        "            (safe_level == Likelihood.VERY_UNLIKELY)\n",
        "            or (safe_level == Likelihood.UNLIKELY)\n",
        "            for safe_level in [\n",
        "                safe_search_annotation.adult,\n",
        "                safe_search_annotation.medical,\n",
        "                safe_search_annotation.violence,\n",
        "                safe_search_annotation.racy,\n",
        "            ]\n",
        "        ]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aded0a519b0c"
      },
      "source": [
        "### Perform rate-limited explicit image detection\n",
        "\n",
        "Cloud Vision has a rate limit for API requests.\n",
        "\n",
        "Use a rate limiter to ensure the requests go under this limit.\n",
        "For better performance, use a ThreadPool to make parallel requests. This is out-of-scope for this notebook.\n",
        "\n",
        "Learn more about [Quotas and Limits](https://cloud.google.com/vision/quotas?hl=en)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "bc753c9264a9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4000/4000 [02:14<00:00, 29.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Safe images = 3292 out of 4000 images\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from typing import Optional\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Create a rate limiter with a limit of 1800 requests per minute\n",
        "seconds_per_job = 1 / (1800 / 60)\n",
        "\n",
        "\n",
        "def process_image(image_path: str) -> Optional[bool]:\n",
        "    try:\n",
        "        annotation = detect_safe_search(image_path)\n",
        "\n",
        "        if annotation:\n",
        "            return convert_annotation_to_safety(safe_search_annotation=annotation)\n",
        "        else:\n",
        "            return None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "# Process images using ThreadPool\n",
        "is_safe_values_cloud_vision = []\n",
        "with ThreadPoolExecutor() as executor:\n",
        "    futures = []\n",
        "    for img_url in tqdm(image_paths, total=len(image_paths), position=0):\n",
        "        futures.append(executor.submit(process_image, img_url))\n",
        "        time.sleep(seconds_per_job)\n",
        "\n",
        "    for future in futures:\n",
        "        is_safe_values_cloud_vision.append(future.result())\n",
        "\n",
        "# Set Nones to False\n",
        "is_safe_values_cloud_vision = [\n",
        "    is_safe or False for is_safe in is_safe_values_cloud_vision\n",
        "]\n",
        "\n",
        "# Print number of safe images found\n",
        "print(\n",
        "    f\"Safe images = {np.array(is_safe_values_cloud_vision).sum()} out of {len(is_safe_values_cloud_vision)} images\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "00c8234f9828"
      },
      "outputs": [],
      "source": [
        "# Filter images by safety\n",
        "metadatas = [\n",
        "    metadata\n",
        "    for metadata, is_safe in zip(metadatas, is_safe_values_cloud_vision)\n",
        "    if is_safe\n",
        "]\n",
        "image_names = [\n",
        "    image_name\n",
        "    for image_name, is_safe in zip(image_names, is_safe_values_cloud_vision)\n",
        "    if is_safe\n",
        "]\n",
        "image_paths = [\n",
        "    image_path\n",
        "    for image_path, is_safe in zip(image_paths, is_safe_values_cloud_vision)\n",
        "    if is_safe\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eab0cbaef92"
      },
      "source": [
        "#### Defining encoding functions\n",
        "\n",
        "Create an EmbeddingPredictionClient which encapsulates the logic to call the embedding API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ec925af6f502"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import time\n",
        "import typing\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "from google.protobuf import struct_pb2\n",
        "\n",
        "\n",
        "class EmbeddingResponse(typing.NamedTuple):\n",
        "    text_embedding: typing.Sequence[float]\n",
        "    image_embedding: typing.Sequence[float]\n",
        "\n",
        "\n",
        "def load_image_bytes(image_uri: str) -> bytes:\n",
        "    \"\"\"Load image bytes from a remote or local URI.\"\"\"\n",
        "    image_bytes = None\n",
        "    if image_uri.startswith(\"http://\") or image_uri.startswith(\"https://\"):\n",
        "        response = requests.get(image_uri, stream=True)\n",
        "        if response.status_code == 200:\n",
        "            image_bytes = response.content\n",
        "    else:\n",
        "        image_bytes = open(image_uri, \"rb\").read()\n",
        "    return image_bytes\n",
        "\n",
        "\n",
        "class EmbeddingPredictionClient:\n",
        "    \"\"\"Wrapper around Prediction Service Client.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        project: str,\n",
        "        location: str = \"us-central1\",\n",
        "        api_regional_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
        "    ):\n",
        "        client_options = {\"api_endpoint\": api_regional_endpoint}\n",
        "        # Initialize client that will be used to create and send requests.\n",
        "        # This client only needs to be created once, and can be reused for multiple requests.\n",
        "        self.client = aiplatform.gapic.PredictionServiceClient(\n",
        "            client_options=client_options\n",
        "        )\n",
        "        self.location = location\n",
        "        self.project = project\n",
        "\n",
        "    def get_embedding(self, text: str = None, image_file: str = None):\n",
        "        if not text and not image_file:\n",
        "            raise ValueError(\"At least one of text or image_file must be specified.\")\n",
        "\n",
        "        # Load image file\n",
        "        image_bytes = None\n",
        "        if image_file:\n",
        "            image_bytes = load_image_bytes(image_file)\n",
        "\n",
        "        instance = struct_pb2.Struct()\n",
        "        if text:\n",
        "            instance.fields[\"text\"].string_value = text\n",
        "\n",
        "        if image_bytes:\n",
        "            encoded_content = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
        "            image_struct = instance.fields[\"image\"].struct_value\n",
        "            image_struct.fields[\"bytesBase64Encoded\"].string_value = encoded_content\n",
        "\n",
        "        instances = [instance]\n",
        "        endpoint = (\n",
        "            f\"projects/{self.project}/locations/{self.location}\"\n",
        "            \"/publishers/google/models/multimodalembedding@001\"\n",
        "        )\n",
        "        response = self.client.predict(endpoint=endpoint, instances=instances)\n",
        "\n",
        "        text_embedding = None\n",
        "        if text:\n",
        "            text_emb_value = response.predictions[0][\"textEmbedding\"]\n",
        "            text_embedding = [v for v in text_emb_value]\n",
        "\n",
        "        image_embedding = None\n",
        "        if image_bytes:\n",
        "            image_emb_value = response.predictions[0][\"imageEmbedding\"]\n",
        "            image_embedding = [v for v in image_emb_value]\n",
        "\n",
        "        return EmbeddingResponse(\n",
        "            text_embedding=text_embedding, image_embedding=image_embedding\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43adc0eefd50"
      },
      "source": [
        "#### Create helper functions to process data in batches\n",
        "\n",
        "Datasets can be large, so it's recommended to load a batch of data at a time into memory using a generator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2b9cda3fe9f"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from typing import Callable, Generator, List\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "def generate_batches(\n",
        "    inputs: List[str], batch_size: int\n",
        ") -> Generator[List[str], None, None]:\n",
        "    \"\"\"\n",
        "    Generator function that takes a list of strings and a batch size, and yields batches of the specified size.\n",
        "    \"\"\"\n",
        "\n",
        "    for i in range(0, len(inputs), batch_size):\n",
        "        yield inputs[i : i + batch_size]\n",
        "\n",
        "\n",
        "API_IMAGES_PER_SECOND = 2\n",
        "\n",
        "\n",
        "def encode_to_embeddings_chunked(\n",
        "    process_function: Callable[[List[str]], List[Optional[List[float]]]],\n",
        "    items: List[str],\n",
        "    batch_size: int = 1,\n",
        ") -> List[Optional[List[float]]]:\n",
        "    \"\"\"\n",
        "    Function that encodes a list of strings into embeddings using a process function.\n",
        "    It takes a list of strings and returns a list of optional lists of floats.\n",
        "    The data is processed in chunks to prevent out-of-memory errors.\n",
        "    \"\"\"\n",
        "\n",
        "    embeddings_list: List[Optional[List[float]]] = []\n",
        "\n",
        "    # Prepare the batches using a generator\n",
        "    batches = generate_batches(items, batch_size)\n",
        "\n",
        "    seconds_per_job = batch_size / API_IMAGES_PER_SECOND\n",
        "\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        futures = []\n",
        "        for batch in tqdm(batches, total=len(items) // batch_size, position=0):\n",
        "            futures.append(executor.submit(process_function, batch))\n",
        "            time.sleep(seconds_per_job)\n",
        "\n",
        "        for future in futures:\n",
        "            embeddings_list.extend(future.result())\n",
        "    return embeddings_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd58b70d87f8"
      },
      "source": [
        "#### Create functions that wrap embedding functions in try-except and retry logic.\n",
        "\n",
        "This particular embedding model can only process 1 image at a time, so inputs are validated to be equal to a length of 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "ae5933cb5a51"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "from typing import List, Optional\n",
        "\n",
        "import numpy as np\n",
        "import requests\n",
        "from tenacity import retry, stop_after_attempt\n",
        "\n",
        "client = EmbeddingPredictionClient(project=PROJECT_ID)\n",
        "\n",
        "\n",
        "# Use a retry handler in case of failure\n",
        "@retry(reraise=True, stop=stop_after_attempt(3))\n",
        "def encode_texts_to_embeddings_with_retry(text: List[str]) -> List[List[float]]:\n",
        "    assert len(text) == 1\n",
        "\n",
        "    try:\n",
        "        return [client.get_embedding(text=text[0], image_file=None).text_embedding]\n",
        "    except Exception:\n",
        "        raise RuntimeError(\"Error getting embedding.\")\n",
        "\n",
        "\n",
        "def encode_texts_to_embeddings(text: List[str]) -> List[Optional[List[float]]]:\n",
        "    try:\n",
        "        return encode_texts_to_embeddings_with_retry(text=text)\n",
        "    except Exception:\n",
        "        return [None for _ in range(len(text))]\n",
        "\n",
        "\n",
        "@retry(reraise=True, stop=stop_after_attempt(3))\n",
        "def encode_images_to_embeddings_with_retry(image_uris: List[str]) -> List[List[float]]:\n",
        "    assert len(image_uris) == 1\n",
        "\n",
        "    try:\n",
        "        return [\n",
        "            client.get_embedding(text=None, image_file=image_uris[0]).image_embedding\n",
        "        ]\n",
        "    except Exception as ex:\n",
        "        print(ex)\n",
        "        raise RuntimeError(\"Error getting embedding.\")\n",
        "\n",
        "\n",
        "def encode_images_to_embeddings(image_uris: List[str]) -> List[Optional[List[float]]]:\n",
        "    try:\n",
        "        return encode_images_to_embeddings_with_retry(image_uris=image_uris)\n",
        "    except Exception as ex:\n",
        "        print(ex)\n",
        "        return [None for _ in range(len(image_uris))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f18ecd0b068"
      },
      "source": [
        "#### Test the encoding function\n",
        "\n",
        "Encode a subset of data and see if the embeddings and distance metrics make sense.\n",
        "\n",
        "Since there is no public paper describing the embedding model, assume that the embeddings are trained using cosine similarity as a loss function since that is quite common."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "42f869fd66f5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a0b8bc02ec814607b62ef9ebcc18d1e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 6.99 s, sys: 1.41 s, total: 8.4 s\n",
            "Wall time: 1min 52s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "# Encode a sample subset of images\n",
        "image_paths_filtered = list(image_paths)[:1000]\n",
        "image_embeddings = encode_to_embeddings_chunked(\n",
        "    process_function=encode_images_to_embeddings, items=image_paths_filtered\n",
        ")\n",
        "\n",
        "# Keep only non-None embeddings\n",
        "indexes_to_keep, image_embeddings = zip(\n",
        "    *[\n",
        "        (index, embedding)\n",
        "        for index, embedding in enumerate(image_embeddings)\n",
        "        if embedding is not None\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(f\"Processed {len(indexes_to_keep)} embeddings successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25f78712545e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def dot_product_distance(\n",
        "    text_embedding: np.ndarray, image_embeddings: np.ndarray\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Compute dot-product distance between text and image embeddings by taking the dot product\"\"\"\n",
        "    return np.dot(text_embedding, image_embeddings.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2595277c886e"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from io import BytesIO\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "text_query = \"Birds in flight\"\n",
        "\n",
        "# Calculate text embedding of query\n",
        "text_embedding = encode_texts_to_embeddings(text=[text_query])[0]\n",
        "\n",
        "print(type(text_embedding))\n",
        "print(type(text_embedding[0]))\n",
        "\n",
        "print(type(image_embeddings))\n",
        "print(type(image_embeddings[0]))\n",
        "\n",
        "# Calculate distance\n",
        "distances = dot_product_distance(\n",
        "    text_embedding=np.array(text_embedding), image_embeddings=np.array(image_embeddings)\n",
        ")\n",
        "\n",
        "# Set the maximum number of images to display\n",
        "MAX_IMAGES = 20\n",
        "\n",
        "# Sort images and scores by descending order of scores and select the top max_images\n",
        "sorted_data = sorted(\n",
        "    zip(image_paths_filtered, distances), key=lambda x: x[1], reverse=True\n",
        ")[:MAX_IMAGES]\n",
        "\n",
        "# Calculate the number of rows and columns needed to display the images\n",
        "num_cols = 4\n",
        "num_rows = math.ceil(len(sorted_data) / num_cols)\n",
        "\n",
        "\n",
        "# Create a grid of subplots to display the images\n",
        "fig, axs = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(10, 12))\n",
        "\n",
        "# Loop through the top max_images images and display them in the subplots\n",
        "for i, (image_path, distance) in enumerate(sorted_data):\n",
        "    # Calculate the row and column index for the current image\n",
        "    row_idx = i // num_cols\n",
        "    col_idx = i % num_cols\n",
        "\n",
        "    # Check if image_path is a remote URL\n",
        "    if image_path.startswith(\"http://\") or image_path.startswith(\"https://\"):\n",
        "        response = requests.get(image_path)\n",
        "        image = Image.open(BytesIO(response.content))\n",
        "    else:\n",
        "        image = Image.open(image_path)\n",
        "\n",
        "    # Display the image in the current subplot\n",
        "    axs[row_idx, col_idx].imshow(image, cmap=\"gray\")\n",
        "\n",
        "    # Set the title of the subplot to the image index and score\n",
        "    axs[row_idx, col_idx].set_title(f\"Rank {i+1}, Distance = {distance:.2f}\")\n",
        "\n",
        "    # Remove ticks from the subplot\n",
        "    axs[row_idx, col_idx].set_xticks([])\n",
        "    axs[row_idx, col_idx].set_yticks([])\n",
        "\n",
        "# Adjust the spacing between subplots and display the plot\n",
        "plt.subplots_adjust(hspace=0.3, wspace=0.1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17f24794c9f0"
      },
      "source": [
        "Save the dimension size for later usage when creating the index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55a128883028"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1408"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DIMENSIONS = len(text_embedding)\n",
        "\n",
        "print(DIMENSIONS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQIQSyF9GtSv"
      },
      "source": [
        "#### Save the embeddings in JSONL format\n",
        "\n",
        "The data must be formatted in JSONL format, which means each embedding dictionary is written as an individual JSON object on its own line.\n",
        "\n",
        "See more information in the docs at [Input data format and structure](https://cloud.google.com/vertex-ai/docs/matching-engine/match-eng-setup/format-structure#data-file-formats)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43aaff23416e"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "\n",
        "# Create temporary file to write embeddings to\n",
        "embeddings_file = tempfile.NamedTemporaryFile(suffix=\".json\", delete=False)\n",
        "\n",
        "embeddings_file.name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "307f468a3ecd"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "BATCH_SIZE = 1000\n",
        "\n",
        "with open(embeddings_file.name, \"a\") as f:\n",
        "    for i in tqdm(range(0, len(image_names), BATCH_SIZE)):\n",
        "        image_names_chunk = image_names[i : i + BATCH_SIZE]\n",
        "        image_paths_chunk = image_paths[i : i + BATCH_SIZE]\n",
        "\n",
        "        embeddings = encode_to_embeddings_chunked(\n",
        "            process_function=encode_images_to_embeddings, items=image_paths_chunk\n",
        "        )\n",
        "\n",
        "        # Append to file\n",
        "        embeddings_formatted = [\n",
        "            json.dumps(\n",
        "                {\n",
        "                    \"id\": str(id),\n",
        "                    \"embedding\": [str(value) for value in embedding],\n",
        "                }\n",
        "            )\n",
        "            + \"\\n\"\n",
        "            for id, embedding in zip(image_names_chunk, embeddings)\n",
        "            if embedding is not None\n",
        "        ]\n",
        "        f.writelines(embeddings_formatted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuVl8DrWG8NS"
      },
      "source": [
        "Upload the training data to Cloud Storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PgsA_vbI8Vg"
      },
      "outputs": [],
      "source": [
        "UNIQUE_FOLDER_NAME = \"embeddings_folder_unique\"\n",
        "EMBEDDINGS_INITIAL_URI = f\"{BUCKET_URI}/{UNIQUE_FOLDER_NAME}/\"\n",
        "! gsutil cp {embeddings_file.name} {EMBEDDINGS_INITIAL_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mglUPwHpJH98"
      },
      "source": [
        "## Create MatchingEngineIndex\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "qiIg9b5zJLi1"
      },
      "outputs": [],
      "source": [
        "DISPLAY_NAME = \"multimodal_diffusiondb\"\n",
        "DESCRIPTION = \"Multimodal DiffusionDB Embeddings\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Y4zooldkGoM4"
      },
      "outputs": [],
      "source": [
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa5eb5cdb2f0"
      },
      "source": [
        "#### Create the index configuration\n",
        "\n",
        "For information on configuration settings, see the [Manage Indexes documentation](https://cloud.google.com/vertex-ai/docs/matching-engine/create-manage-index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzY7TpUSJcTV"
      },
      "outputs": [],
      "source": [
        "tree_ah_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
        "    display_name=DISPLAY_NAME,\n",
        "    contents_delta_uri=EMBEDDINGS_INITIAL_URI,\n",
        "    dimensions=DIMENSIONS,\n",
        "    approximate_neighbors_count=150,\n",
        "    distance_measure_type=\"COSINE_DISTANCE\",\n",
        "    leaf_node_embedding_count=500,\n",
        "    leaf_nodes_to_search_percent=7,\n",
        "    description=DESCRIPTION,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17jrQi501QyX"
      },
      "outputs": [],
      "source": [
        "INDEX_RESOURCE_NAME = tree_ah_index.resource_name\n",
        "\n",
        "print(INDEX_RESOURCE_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f1a9fbecabb"
      },
      "source": [
        "Using the resource name, you can retrieve an existing MatchingEngineIndex."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ddb70647d98"
      },
      "outputs": [],
      "source": [
        "tree_ah_index = aiplatform.MatchingEngineIndex(index_name=INDEX_RESOURCE_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV2xjAnDDObD"
      },
      "source": [
        "## Create an MatchingEngineIndexEndpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuARXzJVGyQX"
      },
      "outputs": [],
      "source": [
        "my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
        "    display_name=DISPLAY_NAME,\n",
        "    description=DISPLAY_NAME,\n",
        "    public_endpoint_enabled=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np2cgVuuIe9k"
      },
      "source": [
        "## Deploy Indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLOYTGygIlMK"
      },
      "outputs": [],
      "source": [
        "DEPLOYED_INDEX_ID = \"deployed_index_id_unique\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uK4WOgqN1NG"
      },
      "outputs": [],
      "source": [
        "my_index_endpoint = my_index_endpoint.deploy_index(\n",
        "    index=tree_ah_index, deployed_index_id=DEPLOYED_INDEX_ID\n",
        ")\n",
        "\n",
        "my_index_endpoint.deployed_indexes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LCGvBNvBd8D"
      },
      "source": [
        "## Create Online Queries\n",
        "\n",
        "After you've built your indexes, you may query against the deployed index to find nearest neighbors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "48e4e417a496"
      },
      "outputs": [],
      "source": [
        "# Encode query\n",
        "text_embeddings = encode_texts_to_embeddings(text=[\"New York skyline\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "A3KYVw5HB-4v"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[MatchNeighbor(id='40d7df76-5413-4948-9a19-6360f9461390.png', distance=0.8957392573356628),\n",
              "  MatchNeighbor(id='f31deabd-b18f-43ae-a336-dfe7a74f521c.png', distance=0.9005841016769409),\n",
              "  MatchNeighbor(id='9d990860-cc36-400c-b414-ea7a6257bff8.png', distance=0.9158724546432495),\n",
              "  MatchNeighbor(id='6ba4dfe8-e163-4264-971a-257d12facb7c.png', distance=0.919234037399292),\n",
              "  MatchNeighbor(id='2ece928d-264b-4283-ae86-78be08b27820.png', distance=0.9204674363136292),\n",
              "  MatchNeighbor(id='b5e3502d-0b1d-4362-b14e-c09c84ff0238.png', distance=0.9226469993591309),\n",
              "  MatchNeighbor(id='9c96ecbe-a749-4c47-bd30-8853606c3c87.png', distance=0.9258934259414673),\n",
              "  MatchNeighbor(id='e7f5f1a7-88d1-4ab3-83b8-f02d50e5d564.png', distance=0.9260851144790649),\n",
              "  MatchNeighbor(id='21c6313b-55bd-4e83-8fab-751d901a8855.png', distance=0.9280754327774048),\n",
              "  MatchNeighbor(id='bb0b2d6b-8b74-4f0e-88be-fc8dc3d6ff9e.png', distance=0.9368126392364502),\n",
              "  MatchNeighbor(id='4e4c32d0-70ba-4491-8adb-e0d46a92e3b0.png', distance=0.9379324913024902),\n",
              "  MatchNeighbor(id='21b7bde7-b29e-44e7-ac8c-a4f748199342.png', distance=0.9382714629173279),\n",
              "  MatchNeighbor(id='f8714403-6baa-4cec-8757-897a83a04ec6.png', distance=0.9383534789085388),\n",
              "  MatchNeighbor(id='4754616e-cd52-4a5f-a9b7-6f3a9d569d6b.png', distance=0.9387881755828857),\n",
              "  MatchNeighbor(id='de7dafef-67ed-4d3f-9b7d-58b8a8a008d6.png', distance=0.9389169216156006),\n",
              "  MatchNeighbor(id='d460aa8b-a435-4746-b249-5bf32648bddb.png', distance=0.939376175403595),\n",
              "  MatchNeighbor(id='e4430d8c-3200-4b4e-822e-000453a3d5ce.png', distance=0.9404100179672241),\n",
              "  MatchNeighbor(id='b0a566fa-504a-4f67-a2d1-d54169ce19f7.png', distance=0.9415013194084167),\n",
              "  MatchNeighbor(id='87971cfd-ac2c-4ac0-ad9b-e5d999fc0509.png', distance=0.9416283369064331),\n",
              "  MatchNeighbor(id='b6a4ca3e-d98d-4d1b-af97-d54f47806491.png', distance=0.9424576163291931)]]"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define number of neighbors to return\n",
        "NUM_NEIGHBORS = 20\n",
        "\n",
        "response = my_index_endpoint.find_neighbors(\n",
        "    deployed_index_id=DEPLOYED_INDEX_ID,\n",
        "    queries=text_embeddings,\n",
        "    num_neighbors=NUM_NEIGHBORS,\n",
        ")\n",
        "\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1116c7c0d7d"
      },
      "source": [
        "Plot the response and verify that images match the text query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6f4ddb7c8d54"
      },
      "outputs": [],
      "source": [
        "# Sort images and scores by descending order of scores and select the top max_images\n",
        "sorted_data = sorted(response[0], key=lambda x: x.distance, reverse=True)\n",
        "\n",
        "# Create a grid of subplots to display the images\n",
        "fig, axs = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(10, 12))\n",
        "\n",
        "# Loop through the top max_images images and display them in the subplots\n",
        "for i, response in enumerate(sorted_data):\n",
        "    image_path = f\"{image_directory}/{response.id}\"\n",
        "    score = response.distance\n",
        "\n",
        "    # Calculate the row and column index for the current image\n",
        "    row_idx = i // num_cols\n",
        "    col_idx = i % num_cols\n",
        "\n",
        "    # Display the image in the current subplot\n",
        "    if os.path.exists(image_path):\n",
        "        image = copy.deepcopy(Image.open(image_path))\n",
        "        axs[row_idx, col_idx].imshow(image, cmap=\"gray\")\n",
        "\n",
        "        # Set the title of the subplot to the image index and score\n",
        "        axs[row_idx, col_idx].set_title(f\"Rank {i+1}, Score = {score:.2f}\")\n",
        "\n",
        "        # Remove ticks from the subplot\n",
        "        axs[row_idx, col_idx].set_xticks([])\n",
        "        axs[row_idx, col_idx].set_yticks([])\n",
        "\n",
        "# Adjust the spacing between subplots and display the plot\n",
        "plt.subplots_adjust(hspace=0.3, wspace=0.1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "You can also manually delete resources that you created by running the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx_vKniMq9ZX"
      },
      "outputs": [],
      "source": [
        "# Force undeployment of indexes and delete endpoint\n",
        "my_index_endpoint.delete(force=True)\n",
        "\n",
        "# Delete indexes\n",
        "tree_ah_index.delete()\n",
        "\n",
        "# Delete Cloud Storage objects that were created\n",
        "delete_bucket = False\n",
        "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
        "    ! gsutil -m rm -r $BUCKET_URI"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "sdk_matching_engine_create_multimodal_embeddings.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Public Preview] intro_swivel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "environment": {
      "name": "common-cpu.m76",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-cpu:m76"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "source": [
        "# Copyright 2021 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/ai-platform-samples/blob/master/ai-platform-unified/notebooks/notebook_template.ipynb\"\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/ai-platform-samples/blob/master/ai-platform-unified/notebooks/notebook_template.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrate how to train an embedding with Submatrix-wise Vector Embedding Learner ([Swivel](https://arxiv.org/abs/1602.02215)) using Vertex Pipelines. The purpose of the embedding learner is to compute cooccurrences between tokens in a given dataset and to use the cooccurrences to generate embeddings.\n",
        "\n",
        "Vertex AI provides a pipeline template\n",
        "for training with Swivel, so you don't need to design your own pipeline or write\n",
        "your own training code.\n",
        "\n",
        "It will require you provide a bucket where the dataset will be stored.\n",
        "\n",
        "Note: you may incur charges for training, storage or usage of other GCP products (Dataflow) in connection with testing this SDK.\n",
        "\n",
        "### Dataset\n",
        "\n",
        "You will use the following sample datasets in the public bucket **gs://cloud-samples-data/vertex-ai/matching-engine/swivel**:\n",
        "\n",
        "1. **wikipedia**: A text corpus dataset created from a [Wikipedia dump](https://dumps.wikimedia.org/enwiki/) that you can use to create word embeddings.\n",
        "2. **movielens_25m**: A [movie rating dataset](https://grouplens.org/datasets/movielens/25m/) for the items input type that you can use to create embeddings for movies. This dataset is processed so that each line contains the movies that have same rating by the same user. The directory also includes `movies.csv`, which maps the movie ids to their names.\n",
        "\n",
        "### Objective\n",
        "\n",
        "In this notebook, you will learn how to train custom embeddings using Vertex Pipelines and deploy the model for serving. The steps performed include:\n",
        "\n",
        "1. **Setup**: Importing the required libraries and setting your global variables.\n",
        "2. **Configure parameters**: Setting the appropriate parameter values for the pipeline job.\n",
        "3. **Train on Vertex Pipelines**: Create a Swivel job to Vertex Pipelines using pipeline template.\n",
        "4. **Deploy on Vertex Prediction**: Importing and deploying the trained model to a callable endpoint.\n",
        "5. **Predict**: Calling the deployed endpoint using online prediction.\n",
        "6. **Cleaning up**: Deleting resources created by this tutorial.\n",
        "\n",
        "### Costs \n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Dataflow\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), and [Dataflow pricing](https://cloud.google.com/dataflow/pricing), and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze4-nDLfK4pw"
      },
      "source": [
        "### Set up your local development environment\n",
        "\n",
        "**If you are using Colab or Google Cloud Notebooks**, your environment already meets\n",
        "all the requirements to run this notebook. You can skip this step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCuSR8GkAgzl"
      },
      "source": [
        "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
        "You need the following:\n",
        "\n",
        "* The Google Cloud SDK\n",
        "* Git\n",
        "* Python 3\n",
        "* virtualenv\n",
        "* Jupyter notebook running in a virtual environment with Python 3\n",
        "\n",
        "The Google Cloud guide to [Setting up a Python development\n",
        "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
        "installation guide](https://jupyter.org/install) provide detailed instructions\n",
        "for meeting these requirements. The following steps provide a condensed set of\n",
        "instructions:\n",
        "\n",
        "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
        "\n",
        "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
        "\n",
        "1. [Install\n",
        "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
        "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
        "\n",
        "1. To install Jupyter, run `pip3 install jupyter` on the\n",
        "command-line in a terminal shell.\n",
        "\n",
        "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
        "\n",
        "1. Open this notebook in the Jupyter Notebook Dashboard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7EUnXsZhAGF"
      },
      "source": [
        "### Install additional packages\n",
        "\n",
        "Install additional package dependencies not installed in your notebook environment, such as google-cloud-aiplatform, kfp, tensorboard-plugin-profile. Use the latest major GA version of each package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b4ef9b72d43"
      },
      "source": [
        "import os\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyy5Lbnzg5fi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ba8d2f31-b553-4aea-db7a-43915320085a"
      },
      "source": [
        "!pip3 install {USER_FLAG} --upgrade google-cloud-aiplatform\n",
        "!pip3 uninstall {USER_FLAG} kfp\n",
        "!pip3 install {USER_FLAG} kfp==1.6.1 tensorflow==2.4.0\n",
        "!pip3 install {USER_FLAG} --upgrade tensorboard-plugin-profile"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting google-cloud-aiplatform\n",
            "  Downloading google_cloud_aiplatform-1.3.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.26.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-aiplatform) (1.26.3)\n",
            "Collecting proto-plus>=1.10.1\n",
            "  Downloading proto_plus-1.19.0-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-aiplatform) (21.0)\n",
            "Collecting google-cloud-storage<2.0.0dev,>=1.32.0\n",
            "  Downloading google_cloud_storage-1.42.0-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 30.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-aiplatform) (1.21.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (57.4.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.15.0)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.34.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (3.17.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (2018.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.53.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (2.23.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.34.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (4.2.2)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.0.3)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (0.4.1)\n",
            "  Downloading google_cloud_storage-1.41.1-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 17.2 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.41.0-py2.py3-none-any.whl (104 kB)\n",
            "\u001b[K     |████████████████████████████████| 104 kB 21.3 MB/s \n",
            "\u001b[?25hCollecting google-cloud-core<2.0dev,>=1.0.3\n",
            "  Downloading google_cloud_core-1.7.2-py2.py3-none-any.whl (28 kB)\n",
            "Collecting google-cloud-storage<2.0.0dev,>=1.32.0\n",
            "  Downloading google_cloud_storage-1.40.0-py2.py3-none-any.whl (104 kB)\n",
            "\u001b[K     |████████████████████████████████| 104 kB 19.2 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.39.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 13.1 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.38.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 20.8 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.37.1-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 28.1 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.37.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 4.7 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.36.2-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 2.5 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.36.1-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 3.6 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.36.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 3.2 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.35.1-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 3.6 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.35.0-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 2.7 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.34.0-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 2.2 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.33.0-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 5.3 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.32.0-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 5.6 MB/s \n",
            "\u001b[?25hINFO: pip is looking at multiple versions of google-cloud-core to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting google-cloud-core<2.0dev,>=1.0.3\n",
            "  Downloading google_cloud_core-1.7.1-py2.py3-none-any.whl (28 kB)\n",
            "  Downloading google_cloud_core-1.7.0-py2.py3-none-any.whl (28 kB)\n",
            "  Downloading google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB)\n",
            "  Downloading google_cloud_core-1.5.0-py2.py3-none-any.whl (27 kB)\n",
            "  Downloading google_cloud_core-1.4.4-py2.py3-none-any.whl (27 kB)\n",
            "  Downloading google_cloud_core-1.4.3-py2.py3-none-any.whl (27 kB)\n",
            "INFO: pip is looking at multiple versions of google-cloud-core to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading google_cloud_core-1.4.2-py2.py3-none-any.whl (26 kB)\n",
            "  Downloading google_cloud_core-1.4.1-py2.py3-none-any.whl (26 kB)\n",
            "  Downloading google_cloud_core-1.4.0-py2.py3-none-any.whl (26 kB)\n",
            "  Downloading google_cloud_core-1.3.0-py2.py3-none-any.whl (26 kB)\n",
            "  Downloading google_cloud_core-1.2.0-py2.py3-none-any.whl (26 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
            "  Downloading google_cloud_core-1.1.0-py2.py3-none-any.whl (26 kB)\n",
            "  Downloading google_cloud_core-1.0.3-py2.py3-none-any.whl (26 kB)\n",
            "INFO: pip is looking at multiple versions of google-cloud-bigquery to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting google-cloud-bigquery<3.0.0dev,>=1.15.0\n",
            "  Downloading google_cloud_bigquery-2.24.0-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 16.5 MB/s \n",
            "\u001b[?25hCollecting google-resumable-media<3.0dev,>=0.6.0\n",
            "  Downloading google_resumable_media-1.3.3-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 2.8 MB/s \n",
            "\u001b[?25hCollecting google-cloud-bigquery<3.0.0dev,>=1.15.0\n",
            "  Downloading google_cloud_bigquery-2.23.3-py2.py3-none-any.whl (196 kB)\n",
            "\u001b[K     |████████████████████████████████| 196 kB 17.0 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_bigquery-2.23.2-py2.py3-none-any.whl (196 kB)\n",
            "\u001b[K     |████████████████████████████████| 196 kB 23.0 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_bigquery-2.23.1-py2.py3-none-any.whl (196 kB)\n",
            "\u001b[K     |████████████████████████████████| 196 kB 22.1 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_bigquery-2.23.0-py2.py3-none-any.whl (196 kB)\n",
            "\u001b[K     |████████████████████████████████| 196 kB 30.5 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_bigquery-2.22.1-py2.py3-none-any.whl (195 kB)\n",
            "\u001b[K     |████████████████████████████████| 195 kB 13.9 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_bigquery-2.22.0-py2.py3-none-any.whl (194 kB)\n",
            "\u001b[K     |████████████████████████████████| 194 kB 30.3 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_bigquery-2.21.0-py2.py3-none-any.whl (193 kB)\n",
            "\u001b[K     |████████████████████████████████| 193 kB 7.8 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_bigquery-2.20.0-py2.py3-none-any.whl (189 kB)\n",
            "\u001b[K     |████████████████████████████████| 189 kB 34.4 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_bigquery-2.19.0-py2.py3-none-any.whl (188 kB)\n",
            "\u001b[K     |████████████████████████████████| 188 kB 19.3 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_bigquery-2.18.0-py2.py3-none-any.whl (188 kB)\n",
            "\u001b[K     |████████████████████████████████| 188 kB 23.3 MB/s \n",
            "\u001b[?25hCollecting google-crc32c<2.0dev,>=1.0\n",
            "  Downloading google_crc32c-1.1.2-cp37-cp37m-manylinux2014_x86_64.whl (38 kB)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-cloud-aiplatform) (2.4.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (2021.5.30)\n",
            "Installing collected packages: google-crc32c, proto-plus, google-resumable-media, google-cloud-core, google-cloud-storage, google-cloud-bigquery, google-cloud-aiplatform\n",
            "  Attempting uninstall: google-resumable-media\n",
            "    Found existing installation: google-resumable-media 0.4.1\n",
            "    Uninstalling google-resumable-media-0.4.1:\n",
            "      Successfully uninstalled google-resumable-media-0.4.1\n",
            "  Attempting uninstall: google-cloud-core\n",
            "    Found existing installation: google-cloud-core 1.0.3\n",
            "    Uninstalling google-cloud-core-1.0.3:\n",
            "      Successfully uninstalled google-cloud-core-1.0.3\n",
            "  Attempting uninstall: google-cloud-storage\n",
            "    Found existing installation: google-cloud-storage 1.18.1\n",
            "    Uninstalling google-cloud-storage-1.18.1:\n",
            "      Successfully uninstalled google-cloud-storage-1.18.1\n",
            "  Attempting uninstall: google-cloud-bigquery\n",
            "    Found existing installation: google-cloud-bigquery 1.21.0\n",
            "    Uninstalling google-cloud-bigquery-1.21.0:\n",
            "      Successfully uninstalled google-cloud-bigquery-1.21.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.13.3 requires google-cloud-bigquery[bqstorage,pandas]<2.0.0dev,>=1.11.1, but you have google-cloud-bigquery 2.18.0 which is incompatible.\u001b[0m\n",
            "Successfully installed google-cloud-aiplatform-1.3.0 google-cloud-bigquery-2.18.0 google-cloud-core-1.7.2 google-cloud-storage-1.41.1 google-crc32c-1.1.2 google-resumable-media-1.3.3 proto-plus-1.19.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping kfp as it is not installed.\u001b[0m\n",
            "Collecting kfp==1.6.1\n",
            "  Downloading kfp-1.6.1.tar.gz (221 kB)\n",
            "\u001b[K     |████████████████████████████████| 221 kB 4.7 MB/s \n",
            "\u001b[?25hCollecting tensorflow==2.4.0\n",
            "  Downloading tensorflow-2.4.0-cp37-cp37m-manylinux2010_x86_64.whl (394.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 394.7 MB 15 kB/s \n",
            "\u001b[?25hCollecting absl-py<=0.11,>=0.9\n",
            "  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 25.6 MB/s \n",
            "\u001b[?25hCollecting PyYAML<6,>=5.3\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 41.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-storage<2,>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from kfp==1.6.1) (1.41.1)\n",
            "Collecting kubernetes<13,>=8.0.0\n",
            "  Downloading kubernetes-12.0.1-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 44.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client<2,>=1.7.8 in /usr/local/lib/python3.7/dist-packages (from kfp==1.6.1) (1.12.8)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from kfp==1.6.1) (1.34.0)\n",
            "Collecting requests-toolbelt<1,>=0.8.0\n",
            "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle<2,>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from kfp==1.6.1) (1.3.0)\n",
            "Collecting kfp-server-api<2.0.0,>=1.1.2\n",
            "  Downloading kfp-server-api-1.6.0.tar.gz (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting jsonschema<4,>=3.0.1\n",
            "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate<1,>=0.8.6 in /usr/local/lib/python3.7/dist-packages (from kfp==1.6.1) (0.8.9)\n",
            "Requirement already satisfied: click<8,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from kfp==1.6.1) (7.1.2)\n",
            "Collecting Deprecated<2,>=1.2.7\n",
            "  Downloading Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB)\n",
            "Collecting strip-hints<1,>=0.1.8\n",
            "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
            "Collecting docstring-parser<1,>=0.7.3\n",
            "  Downloading docstring_parser-0.10.tar.gz (20 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting kfp-pipeline-spec<0.2.0,>=0.1.7\n",
            "  Downloading kfp_pipeline_spec-0.1.9-py3-none-any.whl (17 kB)\n",
            "Collecting fire<1,>=0.3.1\n",
            "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.13.0 in /usr/local/lib/python3.7/dist-packages (from kfp==1.6.1) (3.17.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (0.37.0)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.12)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.15.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (0.2.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (2.5.0)\n",
            "Collecting tensorflow-estimator<2.5.0,>=2.4.0rc0\n",
            "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 43.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.1.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.6.3)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.19.5)\n",
            "Collecting grpcio~=1.32.0\n",
            "  Downloading grpcio-1.32.0-cp37-cp37m-manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 41.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (3.7.4.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.1.2)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.12.1)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (3.3.0)\n",
            "Collecting h5py~=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 41.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->kfp==1.6.1) (0.17.4)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->kfp==1.6.1) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->kfp==1.6.1) (3.0.1)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->kfp==1.6.1) (1.26.3)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp==1.6.1) (57.4.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp==1.6.1) (2018.9)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp==1.6.1) (21.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp==1.6.1) (1.53.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp==1.6.1) (2.23.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.1->kfp==1.6.1) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.1->kfp==1.6.1) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.1->kfp==1.6.1) (0.2.8)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<2,>=1.20.0->kfp==1.6.1) (1.7.2)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<2,>=1.20.0->kfp==1.6.1) (1.3.3)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.7/dist-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp==1.6.1) (1.1.2)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp==1.6.1) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp==1.6.1) (2.20)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema<4,>=3.0.1->kfp==1.6.1) (0.18.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema<4,>=3.0.1->kfp==1.6.1) (4.6.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema<4,>=3.0.1->kfp==1.6.1) (21.2.0)\n",
            "Requirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.7/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.6.1) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.6.1) (2021.5.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.6.1) (2.8.2)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.7/dist-packages (from kubernetes<13,>=8.0.0->kfp==1.6.1) (1.3.0)\n",
            "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0\n",
            "  Downloading websocket_client-1.2.1-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp==1.6.1) (2.4.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp==1.6.1) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp==1.6.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp==1.6.1) (2.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (0.4.5)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib->kubernetes<13,>=8.0.0->kfp==1.6.1) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->jsonschema<4,>=3.0.1->kfp==1.6.1) (3.5.0)\n",
            "Building wheels for collected packages: kfp, docstring-parser, fire, kfp-server-api, strip-hints\n",
            "  Building wheel for kfp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kfp: filename=kfp-1.6.1-py3-none-any.whl size=301571 sha256=8348597b8dd194328e7820a09ae97b39b899027873a91899bf2bf92d206d5d46\n",
            "  Stored in directory: /root/.cache/pip/wheels/c3/e9/fe/2eb7a670d99e0b638de9882d87695010501512b61ce546d675\n",
            "  Building wheel for docstring-parser (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docstring-parser: filename=docstring_parser-0.10-py3-none-any.whl size=28862 sha256=2b191acae4dc42b03e091b9150e934d390d203da1de3f07f7570cf374a804a81\n",
            "  Stored in directory: /root/.cache/pip/wheels/c2/12/f3/67c96229e15e2fe18709cba1b9e3a6a982699c4f0a7b4c0748\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115943 sha256=1505b4c08eeadef86c2f40f0e3c36ecfe740a09ee681b7b95b6fa76aa573b3e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
            "  Building wheel for kfp-server-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kfp-server-api: filename=kfp_server_api-1.6.0-py3-none-any.whl size=92524 sha256=85671d19e4c089f6252a6379ed48f8767dd4caf8b1318f6ba8a2e3b9747485e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/2f/7c/d5c1cbcb535e30c90b88aa5104b1ee14a0ea9313a543bfdf52\n",
            "  Building wheel for strip-hints (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22302 sha256=7d1d6bfba5649e4686d5caa375ca4c23da5b5a98616b61a25456c1b53661a6fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/14/c3/6e44e9b2545f2d570b03f5b6d38c00b7534aa8abb376978363\n",
            "Successfully built kfp docstring-parser fire kfp-server-api strip-hints\n",
            "Installing collected packages: websocket-client, PyYAML, grpcio, absl-py, tensorflow-estimator, strip-hints, requests-toolbelt, kubernetes, kfp-server-api, kfp-pipeline-spec, jsonschema, h5py, gast, fire, docstring-parser, Deprecated, tensorflow, kfp\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.34.1\n",
            "    Uninstalling grpcio-1.34.1:\n",
            "      Successfully uninstalled grpcio-1.34.1\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 0.12.0\n",
            "    Uninstalling absl-py-0.12.0:\n",
            "      Successfully uninstalled absl-py-0.12.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.5.0\n",
            "    Uninstalling tensorflow-estimator-2.5.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
            "  Attempting uninstall: jsonschema\n",
            "    Found existing installation: jsonschema 2.6.0\n",
            "    Uninstalling jsonschema-2.6.0:\n",
            "      Successfully uninstalled jsonschema-2.6.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.5.0\n",
            "    Uninstalling tensorflow-2.5.0:\n",
            "      Successfully uninstalled tensorflow-2.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "nbclient 0.5.3 requires jupyter-client>=6.1.5, but you have jupyter-client 5.3.5 which is incompatible.\u001b[0m\n",
            "Successfully installed Deprecated-1.2.12 PyYAML-5.4.1 absl-py-0.11.0 docstring-parser-0.10 fire-0.4.0 gast-0.3.3 grpcio-1.32.0 h5py-2.10.0 jsonschema-3.2.0 kfp-1.6.1 kfp-pipeline-spec-0.1.9 kfp-server-api-1.6.0 kubernetes-12.0.1 requests-toolbelt-0.9.1 strip-hints-0.1.10 tensorflow-2.4.0 tensorflow-estimator-2.4.0 websocket-client-1.2.1\n",
            "Collecting tensorboard-plugin-profile\n",
            "  Downloading tensorboard_plugin_profile-2.5.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard-plugin-profile) (1.0.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard-plugin-profile) (1.15.0)\n",
            "Collecting gviz-api>=1.9.0\n",
            "  Downloading gviz_api-1.9.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard-plugin-profile) (57.4.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard-plugin-profile) (3.17.3)\n",
            "Installing collected packages: gviz-api, tensorboard-plugin-profile\n",
            "Successfully installed gviz-api-1.9.0 tensorboard-plugin-profile-2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhq5zEbGg0XX"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzrelQZ22IZj"
      },
      "source": [
        "# Automatically restart kernel after installs\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWEdiXsJg0XY"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF1j6f9HApxa"
      },
      "source": [
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "1. [Enable the Vertex AI API and Dataflow API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,dataflow.googleapis.com).\n",
        "\n",
        "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
        "\n",
        "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WReHDGG5g0XY"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oM1iC_MfAts1"
      },
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"\"\n",
        "\n",
        "# Get your Google Cloud project ID from gcloud\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID: \", PROJECT_ID)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJYoRfYng0XZ"
      },
      "source": [
        "Otherwise, set your project ID here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riG_qUokg0XZ"
      },
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
        "    PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CweX_c7eVSH"
      },
      "source": [
        "#### Timestamp\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append it onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aK4XnlYSeVSI"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr--iN2kAylZ"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using Google Cloud Notebooks**, your environment is already\n",
        "authenticated. Skip this step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBCra4QMA2wR"
      },
      "source": [
        "**If you are using Colab**, run the cell below and follow the instructions\n",
        "when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "1. In the Cloud Console, go to the [**Create service account key**\n",
        "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
        "\n",
        "2. Click **Create service account**.\n",
        "\n",
        "3. In the **Service account name** field, enter a name, and\n",
        "   click **Create**.\n",
        "\n",
        "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
        "into the filter box, and select\n",
        "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "\n",
        "5. Click *Create*. A JSON file that contains your key downloads to your\n",
        "local environment.\n",
        "\n",
        "6. Enter the path to your service account key as the\n",
        "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# If on Google Cloud Notebooks, then don't execute this code\n",
        "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgPO1eR3CYjk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "When you submit a built-in Swivel job using the Cloud SDK, you need a Cloud Storage bucket for storing the input dataset and pipeline artifacts (the trained model).\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. It must be unique across all Cloud Storage buckets.\n",
        "\n",
        "You may also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook. Make sure to [choose a region where Vertex AI services are\n",
        "available](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions). You may\n",
        "not use a Multi-Regional Storage bucket for training with Vertex AI."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzGDU7TWdts_"
      },
      "source": [
        "BUCKET_NAME = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}\n",
        "REGION = \"us-central1\"  # @param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf221059d072"
      },
      "source": [
        "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
        "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EcIXiGsCePi"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIq7R4HZCfIc"
      },
      "source": [
        "! gsutil mb -l $REGION $BUCKET_NAME"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucvCsknMCims"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhOb7YnwClBb"
      },
      "source": [
        "! gsutil ls -al $BUCKET_NAME"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3pGuwT7eVSJ"
      },
      "source": [
        "### Import libraries and define constants\n",
        "Define constants used in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pARPYnT2eVSJ"
      },
      "source": [
        "SOURCE_DATA_PATH = \"{}/swivel\".format(BUCKET_NAME)\n",
        "PIPELINE_ROOT = \"{}/pipeline_root\".format(BUCKET_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg6BigD5eVSJ"
      },
      "source": [
        "Import packages used in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiimME4YeVSJ"
      },
      "source": [
        "from kfp.v2.google import client"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "## Copy sample datasets\n",
        "We provide the following sample datasets in this sample:\n",
        "\n",
        "1. **wikipedia**: A text corpus dataset created from a [Wikipedia dump](https://dumps.wikimedia.org/enwiki/20210201/) that you can use to create word embeddings.\n",
        "2. **movielens_25m**: A [movie rating dataset](https://grouplens.org/datasets/movielens/25m/) for the items input type that you can use to create embeddings for movies. This dataset is processed so that each line contains the movies that have same rating by the same user. The directory also includes `movies.csv`, which maps the movie ids to their names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ll1ij5feVSK"
      },
      "source": [
        "# It will take ~9 min\n",
        "! gsutil cp -r gs://cloud-samples-data/vertex-ai/matching-engine/swivel/movielens_25m/data* {SOURCE_DATA_PATH}/movielens_25m\n",
        "! gsutil cp -r gs://cloud-samples-data/vertex-ai/matching-engine/swivel/wikipedia/* {SOURCE_DATA_PATH}/wikipedia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9Uo3tifg1kx"
      },
      "source": [
        "## Copy and configure the Swivel template\n",
        "\n",
        "Download the Swivel template and configuration script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRUOFELefqf1"
      },
      "source": [
        "!gsutil cp gs://cloud-samples-data/vertex-ai/matching-engine/swivel/pipeline/* ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ElFBL6BeVSK"
      },
      "source": [
        "Change your pipeline configurations: \n",
        "\n",
        "* pipeline_suffix: Suffix of your pipeline name (lowercase and hyphen are allowed).\n",
        "* machine_type: e.g. n1-standard-16.\n",
        "* accelerator_count: Number of GPUs in each machine.\n",
        "* accelerator_type: e.g. NVIDIA_TESLA_P100=2, NVIDIA_TESLA_V100=3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "190tiY-neVSK"
      },
      "source": [
        "YOUR_PIPELINE_SUFFIX = 'my-first-pipeline-wiki' # @param {type:\"string\"}\n",
        "MACHINE_TYPE = 'n1-standard-16' # @param {type:\"string\"}\n",
        "ACCELERATOR_COUNT = 2 # @param {type:\"integer\"}\n",
        "ACCELERATOR_TYPE = 3 # @param {type:\"integer\"}\n",
        "BUCKET = BUCKET_NAME[5:]  # remove \"gs://\" for the following \n",
        "\n",
        "!chmod +x swivel_template_configuration.sh\n",
        "\n",
        "!./swivel_template_configuration.sh -pipeline_suffix {YOUR_PIPELINE_SUFFIX} -project_id {PROJECT_ID} -machine_type {MACHINE_TYPE} -accelerator_count {ACCELERATOR_COUNT} -accelerator_type {ACCELERATOR_TYPE} -pipeline_root {BUCKET}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6OuYKCweVSK"
      },
      "source": [
        "swivel_pipeline.json will be generated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcM5q2wfeVSK"
      },
      "source": [
        "## Create the Swivel job for Wikipedia text embeddings\n",
        "\n",
        "You will submit the pipeline job by passing the compiled spec to the `create_run_from_job_spec()` method. Note that you are passing a `parameter_values` dict that specifies the pipeline input parameters to use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af31EtxxeVSK"
      },
      "source": [
        "The following table shows the runtime parameters required by the Swivel job:\n",
        "\n",
        "| Parameter                  |Data type | Description                                                        | Required               |\n",
        "|----------------------------|----------|--------------------------------------------------------------------|------------------------|\n",
        "| `embedding_dim`            | int      | Dimensions of the embeddings to train.                         | No - Default is 100    |\n",
        "| `input_base`         | string   | Cloud Storage path where the input data is stored.                       | Yes                    |\n",
        "| `input_type`               | string   | Type of the input data.  Can be either 'text' (for wikipedia sample) or 'items'(for movielens sample).      | Yes                    |\n",
        "| `max_vocab_size`               | int      | Maximum vocabulary size to generate embeddings for.                | No - Default is 409600 |\n",
        "|`num_epochs` | int | Number of epochs for training. | No - Default is 20 |\n",
        "\n",
        "In short, the **items** input type means that each line of your input data should be space-separated item ids. Each line is tokenized by splitting on whitespace. The **text** input type means that each line of your input data should be equivalent to a sentence. Each line is tokenized by lowercasing, and splitting on whitespace.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9SnVxxleVSK"
      },
      "source": [
        "# Wikipedia text embedding sample\n",
        "\n",
        "PARAMETER_VALUES={\n",
        "    'embedding_dim': 100,  # <---CHANGE THIS (OPTIONAL)\n",
        "    'input_base': '{}/wikipedia'.format(SOURCE_DATA_PATH),\n",
        "    'input_type': 'text',  # For wikipedia sample\n",
        "    'max_vocab_size': 409600, # <---CHANGE THIS (OPTIONAL)\n",
        "    'num_epochs': 20,  # <---CHANGE THIS (OPTIONAL)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUV5aYtPeVSK"
      },
      "source": [
        "pipeline_client = client.AIPlatformClient(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        ")\n",
        "\n",
        "response = pipeline_client.create_run_from_job_spec(\n",
        "    job_spec_path=\"swivel_pipeline.json\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        "    parameter_values=PARAMETER_VALUES\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhznZuWceVSL"
      },
      "source": [
        "After the job is submitted successfully, you can view its details (including run name that you'll need below) and logs.\n",
        "\n",
        "### Use TensorBoard to check the model\n",
        "\n",
        "After the job finishes successfully (~ 1-2 hours), you can view the trained model in the Vertex ML Metadata browser. It is going to have the following format:\n",
        "\n",
        "* {BUCKET_NAME}/pipeline_root/{PROJECT_NUMBER}/swivel-{TIMESTAMP}/EmbTrainerComponent_-{SOME_NUMBER}/model/\n",
        "\n",
        "Copy this path."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1Na-orVeVSL"
      },
      "source": [
        "MODELOUTPUT_DIR = '{BUCKET_NAME}/{PROJECT_NUMBER}/swivel-{TIMESTAMP}/EmbTrainerComponent_-{SOME_NUMBER}/model' # <---CHANGE THIS (REQUIRED)\n",
        "\n",
        "SAVEDMODEL_DIR  = os.path.join(MODEL_OUTPUT_DIR, 'model')\n",
        "LOGS_DIR = os.path.join(MODEL_OUTPUT_DIR, 'tensorboard')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PY5ipT0feVSL"
      },
      "source": [
        "When the training starts, you can view the logs in TensorBoard:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcswl8-OeVSL"
      },
      "source": [
        "# If on Google Cloud Notebooks, then don't execute this code.\n",
        "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "  if \"google.colab\" in sys.modules:\n",
        "\n",
        "    # Load the TensorBoard notebook extension.\n",
        "    %load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjOzNEQseVSL"
      },
      "source": [
        "# If on Google Cloud Notebooks, then don't execute this code.\n",
        "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "  if \"google.colab\" in sys.modules:\n",
        "\n",
        "    %tensorboard --logdir $LOGS_DIR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3PbqO2IeVSL"
      },
      "source": [
        "For **Google Cloud Notebooks**, you can do the following:\n",
        "\n",
        "1. Open Cloud Shell from the Google Cloud Console.\n",
        "2. Install dependencies: pip3 install tensorflow==2.5.0 tensorboard-plugin-profile==2.5.0.\n",
        "3. Run the following command: tensorboard --logdir <PROFILER_DIR>. You will see a message \"TensorBoard 2.5.0 at http://localhost:<PORT>/ (Press CTRL+C to quit)\" as the output. Take note of the port number.\n",
        "4. You can click on the Web Preview button and view the TensorBoard dashboard and profiling results. You need to configure Web Preview's port to be the same port as you receive from step 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP8psXLneVSL"
      },
      "source": [
        "## Explore the Trained Embeddings\n",
        "\n",
        "Load the SavedModel to lookup embeddings for items. Note the following:\n",
        "* The SavedModel expects a list of string inputs.\n",
        "* Each string input is treated as a list of space-separated tokens.\n",
        "* If the input is text, the string input is lowercased with punctuation removed.\n",
        "* An embedding is generated for each input by looking up the embedding of each token in the input and computing the average embedding per string input.\n",
        "* The embedding of an out-of-vocabulary (OOV) token is a vector of zeros.\n",
        "\n",
        "For example, if the input is ['horror', 'film', 'HORROR! Film'], the output will be three embedding vectors, where the third is the average of the first two."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRzTELX1eVSL"
      },
      "source": [
        "embedding_model = tf.saved_model.load(SAVEDMODEL_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I27u859neVSL"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "input_items = ['horror', 'film', '\"HORROR! Film\"', 'horror-film']\n",
        "output_embeddings = embedding_model(input_items)\n",
        "horror_film_embedding = tf.math.reduce_mean(output_embeddings[:2], axis=0)\n",
        "\n",
        "# Average of embeddings for 'horror' and 'film' equals that for '\"HORROR! Film\"'\n",
        "# since preprocessing cleans punctuation and lowercases.\n",
        "assert tf.math.reduce_all(tf.equal(horror_film_embedding, output_embeddings[2])).numpy()\n",
        "# Embedding for '\"HORROR! Film\"' equal that for 'horror-film' since the\n",
        "# latter contains a hyphenation and thus is a separate token.\n",
        "assert not tf.math.reduce_all(tf.equal(output_embeddings[2], output_embeddings[3])).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPeXWsqreVSL"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Change input_items with your own item tokens\n",
        "input_items = ['man', 'woman', 'glass', 'plastic']\n",
        "\n",
        "output_embeddings = embedding_model(input_items)\n",
        "\n",
        "for idx1 in range(len(input_items)):\n",
        "  item1 = input_items[idx1]\n",
        "  embedding1 = output_embeddings[idx1].numpy()\n",
        "  for idx2 in range(idx1+1, len(input_items)):\n",
        "    item2 = input_items[idx2]\n",
        "    embedding2 = output_embeddings[idx2].numpy()\n",
        "    similarity = round(cosine_similarity([embedding1], [embedding2])[0][0], 5)\n",
        "    print(f\"Similarity between '{item1}' and '{item2}' = {similarity}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ2hI5YAeVSM"
      },
      "source": [
        "You can use the [TensorBoard Embedding Projector](https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin) to graphically represent high dimensional embeddings, which can be helpful in examining and understanding your embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE7pltkVeVSM"
      },
      "source": [
        "## Deploy the embedding model for online serving\n",
        "\n",
        "To deploy the trained model, you will perform the following steps:\n",
        "* Create a model endpoint (if needed).\n",
        "* Upload the trained model to Model resource.\n",
        "* Deploy the Model to the endpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-rHi00XeVSM"
      },
      "source": [
        "ENDPOINT_NAME = f'wikipedia_embedding' # <---CHANGE THIS (OPTIONAL)\n",
        "MODEL_VERSION_NAME = f'wiki-tf2-cpu-2.4'  #<---CHANGE THIS (OPTIONAL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udJ7mBk-eVSM"
      },
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P16dMukCeVSM"
      },
      "source": [
        "endpoint = aiplatform.Endpoint.create(display_name=ENDPOINT_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vxt5Akp3eVSM"
      },
      "source": [
        "# Upload the trained model to Model resource\n",
        "model = aiplatform.Model.upload(\n",
        "    display_name=MODEL_VERSION_NAME,\n",
        "    artifact_uri=SAVEDMODEL_DIR,\n",
        "    serving_container_image_uri='us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-4:latest',\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi00XSkheVSM"
      },
      "source": [
        "# Deploy the Model to the Endpoint\n",
        "model.deploy(\n",
        "    endpoint=endpoint,\n",
        "    machine_type='n1-standard-2',\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDaCXinreVSM"
      },
      "source": [
        "### Look up embedding by making an online prediction request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JTz0i_ieVSM"
      },
      "source": [
        "input_items = ['horror', 'film', '\"HORROR! Film\"', 'horror-film']\n",
        "\n",
        "predictions = endpoint.predict(instances=input_items)\n",
        "embeddings = predictions.predictions\n",
        "print(len(embeddings))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8pgaNbveVSM"
      },
      "source": [
        "## Create the Swivel job for MovieLens items embedding (Optional)\n",
        "\n",
        "This section shows you how to create embeddings for the movies in the movielens-25m dataset using Swivel. You need to do the following steps:\n",
        "1. Configure the swivel template (using the **items** input_type) and create a pipeline job.\n",
        "2. Run the following item embedding exploration code.\n",
        "\n",
        "The following cell overwrites `swivel_pipeline_template.json`; the new pipeline template file is almost identical, but it's labeled with your new pipeline suffix to distinguish it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haz9gXnjeVSM"
      },
      "source": [
        "YOUR_PIPELINE_SUFFIX = 'my-first-pipeline-movie' # @param {type:\"string\"}\n",
        "\n",
        "!./swivel_template_configuration.sh -pipeline_suffix {YOUR_PIPELINE_SUFFIX} -project_id {PROJECT_ID} -machine_type {MACHINE_TYPE} -accelerator_count {ACCELERATOR_COUNT} -accelerator_type {ACCELERATOR_TYPE} -pipeline_root {BUCKET}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2guNOxgjeVSN"
      },
      "source": [
        "# movie_id embedding sample\n",
        "\n",
        "PARAMETER_VALUES={\n",
        "    'embedding_dim': 100,  # <---CHANGE THIS (OPTIONAL)\n",
        "    'input_base': '{}/movielens_25m/train'.format(SOURCE_DATA_PATH),\n",
        "    'input_type': 'items',  # For movielens sample\n",
        "    'max_vocab_size': 409600, # <---CHANGE THIS (OPTIONAL)\n",
        "    'num_epochs': 20,  # <---CHANGE THIS (OPTIONAL)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuEKjKJHeVSN"
      },
      "source": [
        "Submit the pipeline job."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_WZMviseVSN"
      },
      "source": [
        "pipeline_client = client.AIPlatformClient(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        ")\n",
        "\n",
        "response = pipeline_client.create_run_from_job_spec(\n",
        "    job_spec_path=\"swivel_pipeline.json\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        "    parameter_values=PARAMETER_VALUES\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmcx4EADeVSN"
      },
      "source": [
        "After the job is submitted successfully, you can view its details and logs. \n",
        "\n",
        "After the job finishes successfully (~ 1-2 hours), you can view the trained model in your CLoud Storage browser. It is going to have the following format:\n",
        "\n",
        "* {BUCKET_NAME}/{PROJECT_NUMBER}/swivel-{TIMESTAMP}/EmbTrainerComponent_-{SOME_NUMBER}/model/\n",
        "\n",
        "Copy this path."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eALPe9CAeVSN"
      },
      "source": [
        "MODELOUTPUT_DIR = '{BUCKET_NAME}/{PROJECT_NUMBER}/swivel-{TIMESTAMP}/EmbTrainerComponent_-{SOME_NUMBER}/model' # <---CHANGE THIS (REQUIRED)\n",
        "\n",
        "SAVEDMODEL_DIR  = os.path.join(MODEL_OUTPUT_DIR, 'model')\n",
        "movielens_embedding_model = tf.saved_model.load(SAVEDMODEL_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "995NzGSAeVSN"
      },
      "source": [
        "Load the movie ids and titles:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oyMaSvFeVSN"
      },
      "source": [
        "!gsutil cp gs://cloud-samples-data/vertex-ai/matching-engine/swivel/movielens_25m/movies.csv ./movies.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMF_iHuFeVSN"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "movies = pd.read_csv('movies.csv')\n",
        "print(f'Movie count: {len(movies.index)}')\n",
        "movies.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BrQKbvseVSN"
      },
      "source": [
        "# Change to your favourite movies.\n",
        "query_movies = [\n",
        "  'Lion King, The (1994)',\n",
        "  'Aladdin (1992)',\n",
        "  'Star Wars: Episode IV - A New Hope (1977)',\n",
        "  'Star Wars: Episode VI - Return of the Jedi (1983)',\n",
        "  'Terminator 2: Judgment Day (1991)',\n",
        "  'Aliens (1986)',\n",
        "  'Godfather, The (1972)',\n",
        "  'Goodfellas (1990)',\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71mBtTJ2eVSN"
      },
      "source": [
        "Explore movie embedding similarities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNuY44OieVSN"
      },
      "source": [
        "def get_movie_id(title):\n",
        "  return list(movies[movies.title==title].movieId)[0]\n",
        "  \n",
        "input_items = [str(get_movie_id(title)) for title in query_movies]\n",
        "output_embeddings = movielens_embedding_model(input_items)\n",
        "\n",
        "for idx1 in range(0, len(input_items)-1, 2):\n",
        "  item1 = input_items[idx1]\n",
        "  title1 = query_movies[idx1]\n",
        "  print(title1)\n",
        "  print(\"==================\")\n",
        "  embedding1 = output_embeddings[idx1].numpy()\n",
        "  for idx2 in range(0, len(input_items)):\n",
        "    item2 = input_items[idx2]\n",
        "    embedding2 = output_embeddings[idx2].numpy()\n",
        "    similarity = round(cosine_similarity([embedding1], [embedding2])[0][0], 5)\n",
        "    \n",
        "    title1 = query_movies[idx1]\n",
        "    title2 = query_movies[idx2]\n",
        "    print(f\" - Similarity to '{title2}' = {similarity}\")\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GBb7X1reVSN"
      },
      "source": [
        "# Delete endpoint resource\n",
        "# If force is set to True, all deployed models on this Endpoint will be undeployed first.\n",
        "endpoint.delete(force=True)\n",
        "\n",
        "# Delete model resource\n",
        "MODEL_RESOURCE_NAME = model.resource_name\n",
        "! gcloud ai models delete $MODEL_RESOURCE_NAME --region $REGION --quiet\n",
        "\n",
        "# Delete Cloud Storage objects that were created\n",
        "! gsutil -m rm -r $MODELOUTPUT_DIR"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
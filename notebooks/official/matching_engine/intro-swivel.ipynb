{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2021 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/official/matching_engine/intro-swivel.ipynb\"\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/official/matching_engine/intro-swivel.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrate how to train an embedding with Submatrix-wise Vector Embedding Learner ([Swivel](https://arxiv.org/abs/1602.02215)) using Vertex Pipelines. The purpose of the embedding learner is to compute cooccurrences between tokens in a given dataset and to use the cooccurrences to generate embeddings.\n",
        "\n",
        "Vertex AI provides a pipeline template\n",
        "for training with Swivel, so you don't need to design your own pipeline or write\n",
        "your own training code.\n",
        "\n",
        "It will require you provide a bucket where the dataset will be stored.\n",
        "\n",
        "Note: you may incur charges for training, storage or usage of other GCP products (Dataflow) in connection with testing this SDK.\n",
        "\n",
        "### Dataset\n",
        "\n",
        "You will use the following sample datasets in the public bucket **gs://cloud-samples-data/vertex-ai/matching-engine/swivel**:\n",
        "\n",
        "1. **movielens_25m**: A [movie rating dataset](https://grouplens.org/datasets/movielens/25m/) for the items input type that you can use to create embeddings for movies. This dataset is processed so that each line contains the movies that have same rating by the same user. The directory also includes `movies.csv`, which maps the movie ids to their names.\n",
        "2. **wikipedia**: A text corpus dataset created from a [Wikipedia dump](https://dumps.wikimedia.org/enwiki/) that you can use to create word embeddings.\n",
        "\n",
        "### Objective\n",
        "\n",
        "In this notebook, you will learn how to train custom embeddings using Vertex Pipelines and deploy the model for serving. The steps performed include:\n",
        "\n",
        "1. **Setup**: Importing the required libraries and setting your global variables.\n",
        "2. **Configure parameters**: Setting the appropriate parameter values for the pipeline job.\n",
        "3. **Train on Vertex Pipelines**: Create a Swivel job to Vertex Pipelines using pipeline template.\n",
        "4. **Deploy on Vertex Prediction**: Importing and deploying the trained model to a callable endpoint.\n",
        "5. **Predict**: Calling the deployed endpoint using online prediction.\n",
        "6. **Cleaning up**: Deleting resources created by this tutorial.\n",
        "\n",
        "### Costs \n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Dataflow\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), and [Dataflow pricing](https://cloud.google.com/dataflow/pricing), and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze4-nDLfK4pw"
      },
      "source": [
        "### Set up your local development environment\n",
        "\n",
        "**If you are using Colab or Google Cloud Notebooks**, your environment already meets\n",
        "all the requirements to run this notebook. You can skip this step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCuSR8GkAgzl"
      },
      "source": [
        "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
        "You need the following:\n",
        "\n",
        "* The Google Cloud SDK\n",
        "* Git\n",
        "* Python 3\n",
        "* virtualenv\n",
        "* Jupyter notebook running in a virtual environment with Python 3\n",
        "\n",
        "The Google Cloud guide to [Setting up a Python development\n",
        "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
        "installation guide](https://jupyter.org/install) provide detailed instructions\n",
        "for meeting these requirements. The following steps provide a condensed set of\n",
        "instructions:\n",
        "\n",
        "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
        "\n",
        "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
        "\n",
        "1. [Install\n",
        "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
        "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
        "\n",
        "1. To install Jupyter, run `pip3 install jupyter` on the\n",
        "command-line in a terminal shell.\n",
        "\n",
        "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
        "\n",
        "1. Open this notebook in the Jupyter Notebook Dashboard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7EUnXsZhAGF"
      },
      "source": [
        "### Install additional packages\n",
        "\n",
        "Install additional package dependencies not installed in your notebook environment, such as google-cloud-aiplatform, tensorboard-plugin-profile. Use the latest major GA version of each package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b4ef9b72d43"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyy5Lbnzg5fi"
      },
      "outputs": [],
      "source": [
        "!pip3 install {USER_FLAG} --upgrade pip\n",
        "!pip3 install {USER_FLAG} --upgrade scikit-learn\n",
        "!pip3 install {USER_FLAG} --upgrade google-cloud-aiplatform tensorboard-plugin-profile\n",
        "!pip3 install {USER_FLAG} --upgrade tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhq5zEbGg0XX"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzrelQZ22IZj"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWEdiXsJg0XY"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF1j6f9HApxa"
      },
      "source": [
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "1. [Enable the Vertex AI API and Dataflow API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,dataflow.googleapis.com).\n",
        "\n",
        "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
        "\n",
        "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WReHDGG5g0XY"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM1iC_MfAts1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"\"\n",
        "\n",
        "# Get your Google Cloud project ID and project number from gcloud\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID: \", PROJECT_ID)\n",
        "    shell_output = !gcloud projects list --filter=\"$(gcloud config get-value project)\" --format=\"value(PROJECT_NUMBER)\" 2>/dev/null\n",
        "    PROJECT_NUMBER = shell_output[0]\n",
        "    print(\"Project number: \", PROJECT_NUMBER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJYoRfYng0XZ"
      },
      "source": [
        "Otherwise, set your project ID here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riG_qUokg0XZ"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
        "    PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CweX_c7eVSH"
      },
      "source": [
        "#### Timestamp\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append it onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aK4XnlYSeVSI"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr--iN2kAylZ"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using Google Cloud Notebooks**, your environment is already\n",
        "authenticated. Skip this step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBCra4QMA2wR"
      },
      "source": [
        "**If you are using Colab**, run the cell below and follow the instructions\n",
        "when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "1. In the Cloud Console, go to the [**Create service account key**\n",
        "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
        "\n",
        "2. Click **Create service account**.\n",
        "\n",
        "3. In the **Service account name** field, enter a name, and\n",
        "   click **Create**.\n",
        "\n",
        "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
        "into the filter box, and select\n",
        "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "\n",
        "5. Click *Create*. A JSON file that contains your key downloads to your\n",
        "local environment.\n",
        "\n",
        "6. Enter the path to your service account key as the\n",
        "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# If on Google Cloud Notebooks, then don't execute this code\n",
        "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgPO1eR3CYjk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "When you submit a built-in Swivel job using the Cloud SDK, you need a Cloud Storage bucket for storing the input dataset and pipeline artifacts (the trained model).\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. It must be unique across all Cloud Storage buckets.\n",
        "\n",
        "You may also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook. Make sure to [choose a region where Vertex AI services are\n",
        "available](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions). You may\n",
        "not use a Multi-Regional Storage bucket for training with Vertex AI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzGDU7TWdts_"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}\n",
        "REGION = \"us-central1\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf221059d072"
      },
      "outputs": [],
      "source": [
        "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
        "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EcIXiGsCePi"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIq7R4HZCfIc"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucvCsknMCims"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhOb7YnwClBb"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d612cd762261"
      },
      "source": [
        "### Service Account\n",
        "\n",
        "**If you don't know your service account**, try to get your service account using gcloud command by executing the second cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "801acfa0ffbc"
      },
      "outputs": [],
      "source": [
        "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7864c293f9e"
      },
      "outputs": [],
      "source": [
        "if (\n",
        "    SERVICE_ACCOUNT == \"\"\n",
        "    or SERVICE_ACCOUNT is None\n",
        "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
        "):\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = !gcloud auth list 2>/dev/null\n",
        "    SERVICE_ACCOUNT = shell_output[2].split()[1]\n",
        "    print(\"Service Account:\", SERVICE_ACCOUNT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1390d2890e4e"
      },
      "source": [
        "#### Set service account access for Vertex AI Pipelines\n",
        "\n",
        "Run the following commands to grant your service account access to read and write pipeline artifacts in the bucket that you created in the previous step -- you only need to run these once per service account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0f5ef291f226"
      },
      "outputs": [],
      "source": [
        "!gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_NAME\n",
        "\n",
        "!gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3pGuwT7eVSJ"
      },
      "source": [
        "### Import libraries and define constants\n",
        "Define constants used in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pARPYnT2eVSJ"
      },
      "outputs": [],
      "source": [
        "SOURCE_DATA_PATH = \"{}/swivel\".format(BUCKET_NAME)\n",
        "PIPELINE_ROOT = \"{}/pipeline_root\".format(BUCKET_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg6BigD5eVSJ"
      },
      "source": [
        "Import packages used in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiimME4YeVSJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from google.cloud import aiplatform\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9Uo3tifg1kx"
      },
      "source": [
        "## Copy and configure the Swivel template\n",
        "\n",
        "Download the Swivel template and configuration script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRUOFELefqf1"
      },
      "outputs": [],
      "source": [
        "!gsutil cp gs://cloud-samples-data/vertex-ai/matching-engine/swivel/pipeline/* ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ElFBL6BeVSK"
      },
      "source": [
        "Change your pipeline configurations: \n",
        "\n",
        "* pipeline_suffix: Suffix of your pipeline name (lowercase and hyphen are allowed).\n",
        "* machine_type: e.g. n1-standard-16.\n",
        "* accelerator_count: Number of GPUs in each machine.\n",
        "* accelerator_type: e.g. NVIDIA_TESLA_P100, NVIDIA_TESLA_V100.\n",
        "* region: e.g. us-east1 (optional, default is us-central1)\n",
        "* network_name: e.g., my_network_name (optional, otherwise it uses \"default\" network)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "712fffac0757"
      },
      "source": [
        "### VPC Network peering, subnetwork and private IP address configuration\n",
        "\n",
        "Executing the following cell will generate two files:\n",
        "1. `swivel_pipeline_basic.json`: The basic template allows public IPs and default network for the Dataflow job, and doesn't require setting up VPC Network peering for Vertex AI and **you will use it in this notebook sample**.\n",
        "1. `swivel_pipeline.json`: This template enables private IPs and subnet configuration for the Dataflow job, also requires setting up VPC Network peering for the Vertex custom training. This template includes the following args:\n",
        "* \"--subnetwork=regions/%REGION%/subnetworks/%NETWORK_NAME%\",\n",
        "* \"--no_use_public_ips\",\n",
        "* \\\"network\\\": \\\"projects/%PROJECT_NUMBER%/global/networks/%NETWORK_NAME%\\\"\n",
        "\n",
        "**WARNING** In order to specify private IPs and configure VPC network, you need to [set up VPC Network peering for Vertex AI](https://cloud.google.com/vertex-ai/docs/general/vpc-peering#overview) for your subnetwork (e.g. \"default\" network on \"us-central1\") before submitting the following job. This is required for using private IP addresses for DataFlow and Vertex AI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "190tiY-neVSK"
      },
      "outputs": [],
      "source": [
        "YOUR_PIPELINE_SUFFIX = \"swivel-pipeline-movie\"  # @param {type:\"string\"}\n",
        "MACHINE_TYPE = \"n1-standard-16\"  # @param {type:\"string\"}\n",
        "ACCELERATOR_COUNT = 2  # @param {type:\"integer\"}\n",
        "ACCELERATOR_TYPE = \"NVIDIA_TESLA_V100\"  # @param {type:\"string\"}\n",
        "BUCKET = BUCKET_NAME[5:]  # remove \"gs://\" for the following command.\n",
        "\n",
        "!chmod +x swivel_template_configuration*\n",
        "\n",
        "!./swivel_template_configuration_basic.sh -pipeline_suffix {YOUR_PIPELINE_SUFFIX} -project_number {PROJECT_NUMBER} -project_id {PROJECT_ID} -machine_type {MACHINE_TYPE} -accelerator_count {ACCELERATOR_COUNT} -accelerator_type {ACCELERATOR_TYPE} -pipeline_root {BUCKET}\n",
        "!./swivel_template_configuration.sh -pipeline_suffix {YOUR_PIPELINE_SUFFIX} -project_number {PROJECT_NUMBER} -project_id {PROJECT_ID} -machine_type {MACHINE_TYPE} -accelerator_count {ACCELERATOR_COUNT} -accelerator_type {ACCELERATOR_TYPE} -pipeline_root {BUCKET}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cacea95d68c"
      },
      "outputs": [],
      "source": [
        "! sed \"s:\\t:    :g\" swivel_pipeline_basic.json >tmp.json\n",
        "! mv tmp.json swivel_pipeline_basic.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f12048abec2"
      },
      "source": [
        "Both `swivel_pipeline_basic.json` and `swivel_pipeline.json` are generated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcM5q2wfeVSK"
      },
      "source": [
        "## Create the Swivel job for MovieLens items embeddings\n",
        "\n",
        "You will submit the pipeline job by passing the compiled spec to the `create_run_from_job_spec()` method. Note that you are passing a `parameter_values` dict that specifies the pipeline input parameters to use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af31EtxxeVSK"
      },
      "source": [
        "The following table shows the runtime parameters required by the Swivel job:\n",
        "\n",
        "| Parameter                  |Data type | Description                                                        | Required               |\n",
        "|----------------------------|----------|--------------------------------------------------------------------|------------------------|\n",
        "| `embedding_dim`            | int      | Dimensions of the embeddings to train.                         | No - Default is 100    |\n",
        "| `input_base`         | string   | Cloud Storage path where the input data is stored.                       | Yes                    |\n",
        "| `input_type`               | string   | Type of the input data.  Can be either 'text' (for wikipedia sample) or 'items'(for movielens sample).      | Yes                    |\n",
        "| `max_vocab_size`               | int      | Maximum vocabulary size to generate embeddings for.                | No - Default is 409600 |\n",
        "|`num_epochs` | int | Number of epochs for training. | No - Default is 20 |\n",
        "\n",
        "In short, the **items** input type means that each line of your input data should be space-separated item ids. Each line is tokenized by splitting on whitespace. The **text** input type means that each line of your input data should be equivalent to a sentence. Each line is tokenized by lowercasing, and splitting on whitespace.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cd9e3db9bff"
      },
      "outputs": [],
      "source": [
        "# Copy the MovieLens sample dataset\n",
        "! gsutil cp -r gs://cloud-samples-data/vertex-ai/matching-engine/swivel/movielens_25m/train/* {SOURCE_DATA_PATH}/movielens_25m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9SnVxxleVSK"
      },
      "outputs": [],
      "source": [
        "# MovieLens items embedding sample\n",
        "\n",
        "PARAMETER_VALUES = {\n",
        "    \"embedding_dim\": 100,  # <---CHANGE THIS (OPTIONAL)\n",
        "    \"input_base\": \"{}/movielens_25m/train\".format(SOURCE_DATA_PATH),\n",
        "    \"input_type\": \"items\",  # For movielens sample\n",
        "    \"max_vocab_size\": 409600,  # <---CHANGE THIS (OPTIONAL)\n",
        "    \"num_epochs\": 5,  # <---CHANGE THIS (OPTIONAL)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f1cae770338"
      },
      "source": [
        "Submit the pipeline to Vertex AI:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUV5aYtPeVSK"
      },
      "outputs": [],
      "source": [
        "# Instantiate PipelineJob object\n",
        "pl = aiplatform.PipelineJob(\n",
        "    display_name=YOUR_PIPELINE_SUFFIX,\n",
        "    # Whether or not to enable caching\n",
        "    # True = always cache pipeline step result\n",
        "    # False = never cache pipeline step result\n",
        "    # None = defer to cache option for each pipeline component in the pipeline definition\n",
        "    enable_caching=False,\n",
        "    # Local or GCS path to a compiled pipeline definition\n",
        "    template_path=\"swivel_pipeline_basic.json\",\n",
        "    # Dictionary containing input parameters for your pipeline\n",
        "    parameter_values=PARAMETER_VALUES,\n",
        "    # GCS path to act as the pipeline root\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        ")\n",
        "\n",
        "# Submit the Pipeline to Vertex AI\n",
        "# Optionally you may specify the service account below: submit(service_account=SERVICE_ACCOUNT)\n",
        "# You must have iam.serviceAccounts.actAs permission on the service account to use it\n",
        "pl.submit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhznZuWceVSL"
      },
      "source": [
        "After the job is submitted successfully, you can view its details (including run name that you'll need below) and logs.\n",
        "\n",
        "### Use TensorBoard to check the model\n",
        "\n",
        "You may use the TensorBoard to check the model training process. In order to do that, you need to find the path to the trained model artifact. After the job finishes successfully (~ a few hours), you can view the trained model output path in the [Vertex ML Metadata](https://cloud.google.com/vertex-ai/docs/ml-metadata/introduction) browser. It is going to have the following format:\n",
        "\n",
        "* {BUCKET_NAME}/pipeline_root/{PROJECT_NUMBER}/swivel-{TIMESTAMP}/EmbTrainerComponent_-{SOME_NUMBER}/model/\n",
        "\n",
        "You may copy this path for the MODELOUTPUT_DIR below.\n",
        "\n",
        "Alternatively, you can download a pretrained model to `{SOURCE_DATA_PATH}/movielens_model` and proceed. This pretrained model is for demo purpose and not optimized for production usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc20afaf0e8a"
      },
      "outputs": [],
      "source": [
        "! gsutil -m cp -r gs://cloud-samples-data/vertex-ai/matching-engine/swivel/models/movielens/model {SOURCE_DATA_PATH}/movielens_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1Na-orVeVSL"
      },
      "outputs": [],
      "source": [
        "SAVEDMODEL_DIR = os.path.join(SOURCE_DATA_PATH, \"movielens_model/model\")\n",
        "LOGS_DIR = os.path.join(SOURCE_DATA_PATH, \"movielens_model/tensorboard\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PY5ipT0feVSL"
      },
      "source": [
        "When the training starts, you can view the logs in TensorBoard:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcswl8-OeVSL"
      },
      "outputs": [],
      "source": [
        "# If on Google Cloud Notebooks, then don't execute this code.\n",
        "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        # Load the TensorBoard notebook extension.\n",
        "        %load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjOzNEQseVSL"
      },
      "outputs": [],
      "source": [
        "# If on Google Cloud Notebooks, then don't execute this code.\n",
        "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        %tensorboard --logdir $LOGS_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3PbqO2IeVSL"
      },
      "source": [
        "For **Google Cloud Notebooks**, you can do the following:\n",
        "\n",
        "1. Open Cloud Shell from the Google Cloud Console.\n",
        "2. Install dependencies: `pip3 install tensorflow tensorboard-plugin-profile`\n",
        "3. Run the following command: `tensorboard --logdir {LOGS_DIR}`. You will see a message \"TensorBoard 2.x.0 at http://localhost:<PORT>/ (Press CTRL+C to quit)\" as the output. Take note of the port number.\n",
        "4. You can click on the Web Preview button and view the TensorBoard dashboard and profiling results. You need to configure Web Preview's port to be the same port as you receive from step 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE7pltkVeVSM"
      },
      "source": [
        "## Deploy the embedding model for online serving\n",
        "\n",
        "To deploy the trained model, you will perform the following steps:\n",
        "* Create a model endpoint (if needed).\n",
        "* Upload the trained model to Model resource.\n",
        "* Deploy the Model to the endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-rHi00XeVSM"
      },
      "outputs": [],
      "source": [
        "ENDPOINT_NAME = \"swivel_embedding\"  # <---CHANGE THIS (OPTIONAL)\n",
        "MODEL_VERSION_NAME = \"movie-tf2-cpu-2.4\"  # <---CHANGE THIS (OPTIONAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udJ7mBk-eVSM"
      },
      "outputs": [],
      "source": [
        "aiplatform.init(project=PROJECT_ID, location=REGION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P16dMukCeVSM"
      },
      "outputs": [],
      "source": [
        "# Create a model endpoint\n",
        "endpoint = aiplatform.Endpoint.create(display_name=ENDPOINT_NAME)\n",
        "\n",
        "# Upload the trained model to Model resource\n",
        "model = aiplatform.Model.upload(\n",
        "    display_name=MODEL_VERSION_NAME,\n",
        "    artifact_uri=SAVEDMODEL_DIR,\n",
        "    serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-4:latest\",\n",
        ")\n",
        "\n",
        "# Deploy the Model to the Endpoint\n",
        "model.deploy(\n",
        "    endpoint=endpoint,\n",
        "    machine_type=\"n1-standard-2\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9360794f914f"
      },
      "source": [
        "### Load the movie ids and titles for querying embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14fd3ab852a1"
      },
      "outputs": [],
      "source": [
        "!gsutil cp gs://cloud-samples-data/vertex-ai/matching-engine/swivel/movielens_25m/movies.csv ./movies.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24af0e015685"
      },
      "outputs": [],
      "source": [
        "movies = pd.read_csv(\"movies.csv\")\n",
        "print(f\"Movie count: {len(movies.index)}\")\n",
        "movies.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aefb986c6c8"
      },
      "outputs": [],
      "source": [
        "# Change to your favourite movies.\n",
        "query_movies = [\n",
        "    \"Lion King, The (1994)\",\n",
        "    \"Aladdin (1992)\",\n",
        "    \"Star Wars: Episode IV - A New Hope (1977)\",\n",
        "    \"Star Wars: Episode VI - Return of the Jedi (1983)\",\n",
        "    \"Terminator 2: Judgment Day (1991)\",\n",
        "    \"Aliens (1986)\",\n",
        "    \"Godfather, The (1972)\",\n",
        "    \"Goodfellas (1990)\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5aedea722d0"
      },
      "outputs": [],
      "source": [
        "def get_movie_id(title):\n",
        "    return list(movies[movies.title == title].movieId)[0]\n",
        "\n",
        "\n",
        "input_items = [str(get_movie_id(title)) for title in query_movies]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDaCXinreVSM"
      },
      "source": [
        "### Look up embedding by making an online prediction request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JTz0i_ieVSM"
      },
      "outputs": [],
      "source": [
        "predictions = endpoint.predict(instances=input_items)\n",
        "embeddings = predictions.predictions\n",
        "print(len(embeddings))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1c5f572dec7"
      },
      "source": [
        "Explore movie embedding similarities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8c81e656816a"
      },
      "outputs": [],
      "source": [
        "for idx1 in range(0, len(input_items) - 1, 2):\n",
        "    item1 = input_items[idx1]\n",
        "    title1 = query_movies[idx1]\n",
        "    print(title1)\n",
        "    print(\"==================\")\n",
        "    embedding1 = embeddings[idx1]\n",
        "    for idx2 in range(0, len(input_items)):\n",
        "        item2 = input_items[idx2]\n",
        "        embedding2 = embeddings[idx2]\n",
        "        similarity = round(cosine_similarity([embedding1], [embedding2])[0][0], 5)\n",
        "        title1 = query_movies[idx1]\n",
        "        title2 = query_movies[idx2]\n",
        "        print(f\" - Similarity to '{title2}' = {similarity}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8pgaNbveVSM"
      },
      "source": [
        "## Create the Swivel job for Wikipedia text embedding (Optional)\n",
        "\n",
        "This section shows you how to create embeddings for the movies in the wikipedia dataset using Swivel. You need to do the following steps:\n",
        "1. Configure the swivel template (using the **text** input_type) and create a pipeline job.\n",
        "2. Run the following item embedding exploration code.\n",
        "\n",
        "The following cell overwrites `swivel_pipeline_template.json`; the new pipeline template file is almost identical, but it's labeled with your new pipeline suffix to distinguish it. This job will take **a few hours**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02054a96f564"
      },
      "outputs": [],
      "source": [
        "# Copy the wikipedia sample dataset\n",
        "! gsutil -m cp -r gs://cloud-samples-data/vertex-ai/matching-engine/swivel/wikipedia/* {SOURCE_DATA_PATH}/wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haz9gXnjeVSM"
      },
      "outputs": [],
      "source": [
        "YOUR_PIPELINE_SUFFIX = \"my-first-pipeline-wiki\"  # @param {type:\"string\"}\n",
        "\n",
        "!./swivel_template_configuration.sh -pipeline_suffix {YOUR_PIPELINE_SUFFIX} -project_id {PROJECT_ID} -machine_type {MACHINE_TYPE} -accelerator_count {ACCELERATOR_COUNT} -accelerator_type {ACCELERATOR_TYPE} -pipeline_root {BUCKET}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2guNOxgjeVSN"
      },
      "outputs": [],
      "source": [
        "# wikipedia text embedding sample\n",
        "\n",
        "PARAMETER_VALUES = {\n",
        "    \"embedding_dim\": 100,  # <---CHANGE THIS (OPTIONAL)\n",
        "    \"input_base\": \"{}/wikipedia\".format(SOURCE_DATA_PATH),\n",
        "    \"input_type\": \"text\",  # For wikipedia sample\n",
        "    \"max_vocab_size\": 409600,  # <---CHANGE THIS (OPTIONAL)\n",
        "    \"num_epochs\": 20,  # <---CHANGE THIS (OPTIONAL)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmcx4EADeVSN"
      },
      "source": [
        "**Submit the pipeline job through `aiplatform.PipelineJob` object.**\n",
        "\n",
        "After the job finishes successfully (~**a few hours**), you can view the trained model in your CLoud Storage browser. It is going to have the following format:\n",
        "\n",
        "* {BUCKET_NAME}/{PROJECT_NUMBER}/swivel-{TIMESTAMP}/EmbTrainerComponent_-{SOME_NUMBER}/model/\n",
        "\n",
        "You may copy this path for the MODELOUTPUT_DIR below. For demo purpose, you can download a pretrained model to `{SOURCE_DATA_PATH}/wikipedia_model` and proceed. This pretrained model is for demo purpose and not optimized for production usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed94bd036502"
      },
      "outputs": [],
      "source": [
        "! gsutil -m cp -r gs://cloud-samples-data/vertex-ai/matching-engine/swivel/models/wikipedia/model {SOURCE_DATA_PATH}/wikipedia_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eALPe9CAeVSN"
      },
      "outputs": [],
      "source": [
        "SAVEDMODEL_DIR = os.path.join(SOURCE_DATA_PATH, \"wikipedia_model/model\")\n",
        "embedding_model = tf.saved_model.load(SAVEDMODEL_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "995NzGSAeVSN"
      },
      "source": [
        "### Explore the trained text embeddings\n",
        "\n",
        "Load the SavedModel to lookup embeddings for items. Note the following:\n",
        "* The SavedModel expects a list of string inputs.\n",
        "* Each string input is treated as a list of space-separated tokens.\n",
        "* If the input is text, the string input is lowercased with punctuation removed.\n",
        "* An embedding is generated for each input by looking up the embedding of each token in the input and computing the average embedding per string input.\n",
        "* The embedding of an out-of-vocabulary (OOV) token is a vector of zeros.\n",
        "\n",
        "For example, if the input is ['horror', 'film', 'HORROR! Film'], the output will be three embedding vectors, where the third is the average of the first two."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oyMaSvFeVSN"
      },
      "outputs": [],
      "source": [
        "input_items = [\"horror\", \"film\", '\"HORROR! Film\"', \"horror-film\"]\n",
        "output_embeddings = embedding_model(input_items)\n",
        "horror_film_embedding = tf.math.reduce_mean(output_embeddings[:2], axis=0)\n",
        "\n",
        "# Average of embeddings for 'horror' and 'film' equals that for '\"HORROR! Film\"'\n",
        "# since preprocessing cleans punctuation and lowercases.\n",
        "assert tf.math.reduce_all(tf.equal(horror_film_embedding, output_embeddings[2])).numpy()\n",
        "# Embedding for '\"HORROR! Film\"' equal that for 'horror-film' since the\n",
        "# latter contains a hyphenation and thus is a separate token.\n",
        "assert not tf.math.reduce_all(\n",
        "    tf.equal(output_embeddings[2], output_embeddings[3])\n",
        ").numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMF_iHuFeVSN"
      },
      "outputs": [],
      "source": [
        "# Change input_items with your own item tokens\n",
        "\n",
        "input_items = [\"apple\", \"orange\", \"hammer\", \"nails\"]\n",
        "\n",
        "output_embeddings = embedding_model(input_items)\n",
        "\n",
        "for idx1 in range(len(input_items)):\n",
        "    item1 = input_items[idx1]\n",
        "    embedding1 = output_embeddings[idx1].numpy()\n",
        "    for idx2 in range(idx1 + 1, len(input_items)):\n",
        "        item2 = input_items[idx2]\n",
        "        embedding2 = output_embeddings[idx2].numpy()\n",
        "        similarity = round(cosine_similarity([embedding1], [embedding2])[0][0], 5)\n",
        "        print(f\"Similarity between '{item1}' and '{item2}' = {similarity}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71mBtTJ2eVSN"
      },
      "source": [
        "You can use the [TensorBoard Embedding Projector](https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin) to graphically represent high dimensional embeddings, which can be helpful in examining and understanding your embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GBb7X1reVSN"
      },
      "outputs": [],
      "source": [
        "# Delete endpoint resource\n",
        "# If force is set to True, all deployed models on this Endpoint will be undeployed first.\n",
        "endpoint.delete(force=True)\n",
        "\n",
        "# Delete model resource\n",
        "MODEL_RESOURCE_NAME = model.resource_name\n",
        "! gcloud ai models delete $MODEL_RESOURCE_NAME --region $REGION --quiet\n",
        "\n",
        "# Delete Cloud Storage objects that were created\n",
        "! gsutil -m rm -r $SOURCE_DATA_PATH"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "intro-swivel.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

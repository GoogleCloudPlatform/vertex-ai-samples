{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10699312-6eaa-4a89-a37b-e33648fb268e",
      "metadata": {
        "id": "18ebbd838e32"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13acbc6b-7460-4c3a-b9ae-c9a3806d4250",
      "metadata": {
        "id": "16f0f41c2f9c"
      },
      "source": [
        "# Training, Tuning and Deploying a PyTorch Text Sentiment Classification Model on Vertex AI\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pytorch_text_classification/pytorch-text-sentiment-classification-custom-train-deploy.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pytorch_text_classification/pytorch-text-sentiment-classification-custom-train-deploy.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pytorch_text_classification/pytorch-text-sentiment-classification-custom-train-deploy.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "     </a>\n",
        "  </td>\n",
        "</table>\n",
        "<br/><br/><br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38b34b98-8435-43b3-ba53-4618e9c3f8fb",
      "metadata": {
        "id": "4339ebfdf7a0"
      },
      "source": [
        "## Overview\n",
        "\n",
        "### Fine-tuning pre-trained [BERT](https://huggingface.co/bert-base-cased) model for text sentiment classification task \n",
        "\n",
        "This example is inspired from Token-Classification [notebook](https://github.com/huggingface/notebooks/blob/master/examples/token_classification.ipynb) and [run_glue.py](https://github.com/huggingface/transformers/blob/v2.5.0/examples/run_glue.py). \n",
        "We will be fine-tuning **`bert-base-cased`** (pre-trained) model for sentiment classification task.\n",
        "You can find the details about this model at [Hugging Face Hub](https://huggingface.co/bert-base-cased).\n",
        "\n",
        "For more notebooks with the state of the art PyTorch/Tensorflow/JAX, you can explore [Hugging FaceNotebooks](https://huggingface.co/transformers/notebooks.html).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a8d6435-dcc9-46d2-83dd-1c3f591fdf2f",
      "metadata": {
        "id": "9283a2954aef"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn to **Build, Train, Tune and Deploy PyTorch models on [Vertex AI](https://cloud.google.com/vertex-ai)** and emphasize first class support for training and deploying PyTorch models on Vertex AI. \n",
        "\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services:\n",
        "\n",
        "- Vertex AI `Workbench`\n",
        "- Vertex AI `Training`(Custom Python Package Training) \n",
        "- Vertex AI `Model Registry`\n",
        "- Vertex AI `Predictions`\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- `Cloud Storage`\n",
        "- `Container Registry`\n",
        "- `Cloud Build`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6e451d8-a020-49b8-9436-71c756060a2e",
      "metadata": {
        "id": "ab69c72f7c47"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset used for this tutorial is the [Happy Moments dataset](https://www.kaggle.com/ritresearch/happydb) from [Kaggle Datasets](https://www.kaggle.com/ritresearch/happydb). The version of the dataset you use in this tutorial is stored in a public Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebef3b6b-4d82-4621-a365-2922757cf78b",
      "metadata": {
        "id": "181d4dfbf917"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba93e1a8-4d88-48ec-be25-9d23456646c2",
      "metadata": {
        "id": "c6e3174fadc2"
      },
      "source": [
        "### Set up your local development environment\n",
        "\n",
        "If you are using Colab or Google Vertex AI Workbench Notebook, your environment already meets all the requirements to run this notebook. You can skip this step.\n",
        "\n",
        "Otherwise, make sure your environment meets this notebook's requirements. You need the following:\n",
        "\n",
        "- The Cloud Storage SDK\n",
        "- Git\n",
        "- Python 3\n",
        "- virtualenv\n",
        "- Jupyter notebook running in a virtual environment with Python 3\n",
        "\n",
        "The Cloud Storage guide to [Setting up a Python development environment](https://cloud.google.com/python/setup) and the [Jupyter installation guide](https://jupyter.org/install) provide detailed instructions for meeting these requirements. The following steps provide a condensed set of instructions:\n",
        "\n",
        "1. [Install and initialize the SDK](https://cloud.google.com/sdk/docs/).\n",
        "\n",
        "2. [Install Python 3](https://cloud.google.com/python/setup#installing_python).\n",
        "\n",
        "3. [Install virtualenv](Ihttps://cloud.google.com/python/setup#installing_and_using_virtualenv) and create a virtual environment that uses Python 3.\n",
        "\n",
        "4. Activate that environment and run `pip3 install Jupyter` in a terminal shell to install Jupyter.\n",
        "\n",
        "5. Run `jupyter notebook` on the command line in a terminal shell to launch Jupyter.\n",
        "\n",
        "6. Open this notebook in the Jupyter Notebook Dashboard."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b8f1ed1-61db-449c-92e5-4f3dcd6eddcd",
      "metadata": {
        "id": "f3848df1e5b0"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Install the packages required for executing this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68146d4d-7472-4eda-8953-921f47841e8f",
      "metadata": {
        "id": "6f967c29cbed"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Vertex AI Workbench Notebook product has specific requirements\n",
        "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
        "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
        "    \"/opt/deeplearning/metadata/env_version\"\n",
        ")\n",
        "\n",
        "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_WORKBENCH_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\"\n",
        "\n",
        "! pip3 install --upgrade google-cloud-aiplatform {USER_FLAG} -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e259ef0-8e23-404e-8c43-c111751ad07d",
      "metadata": {
        "id": "0c0b2427998a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3ed5c61-e2a6-474b-a411-6f01f7127b28",
      "metadata": {
        "id": "e67eb8c105ac"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### Select a GPU runtime\n",
        "\n",
        "**Make sure you're running this notebook in a GPU runtime if you have that option. In Colab, select \"Runtime --> Change runtime type > GPU\"**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0223af0b-01cf-4adb-8091-e58abf0520e0",
      "metadata": {
        "id": "1a5aeb2d230c"
      },
      "source": [
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "1. Enable following APIs in your project required for running the tutorial\n",
        "    - [Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)\n",
        "    - [Cloud Storage API](https://console.cloud.google.com/flows/enableapi?apiid=storage.googleapis.com)\n",
        "    - [Container Registry API](https://console.cloud.google.com/flows/enableapi?apiid=containerregistry.googleapis.com)\n",
        "    - [Cloud Build API](https://console.cloud.google.com/flows/enableapi?apiid=cloudbuild.googleapis.com)\n",
        "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
        "1. Enter your project ID in the cell below. Then run the cell to make sure the Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dbe83bb-093e-4eca-b688-ded157c5b06a",
      "metadata": {
        "id": "173f251c128d"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e33bcc90-d827-4cf3-b6af-570dec6e3cdd",
      "metadata": {
        "id": "3d22bacc8ea4"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID:\", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56841f56-d17f-407f-8ca5-65375e7a3a70",
      "metadata": {
        "id": "250cb8c648d5"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a25d36d9-8f58-4ee1-a524-ca82a97f13cd",
      "metadata": {
        "id": "2d985c6c4673"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
        "\n",
        "- Americas: `us-central1`\n",
        "- Europe: `europe-west4`\n",
        "- Asia Pacific: `asia-east1`\n",
        "\n",
        "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
        "\n",
        "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beed8307-0efe-45d7-b5cd-9d41f179e350",
      "metadata": {
        "id": "d8b34ef9a3d0"
      },
      "outputs": [],
      "source": [
        "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
        "\n",
        "if REGION == \"[your-region]\":\n",
        "    REGION = \"us-central1\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "472a3631-494e-4c5d-b062-52db81225fb9",
      "metadata": {
        "id": "b45b2839f8b9"
      },
      "source": [
        "#### UUID\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a uuid for each instance session, and append the uuid onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6779fa63-a017-4b67-b3b5-65a1a34b57d5",
      "metadata": {
        "id": "e80050370d51"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "\n",
        "# Generate a uuid of a specifed length(default=8)\n",
        "def generate_uuid(length: int = 8) -> str:\n",
        "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
        "\n",
        "\n",
        "UUID = generate_uuid()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62d8fcf7-11f7-416c-87af-ccdaa8220030",
      "metadata": {
        "id": "1fe0f66f8490"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using Vertex AI Workbench Notebook**, your environment is already authenticated.\n",
        "\n",
        "**If you are using Colab**, run the cell below and follow the instructions\n",
        "when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "1. In the Cloud Console, go to the [**Create service account key**\n",
        "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
        "\n",
        "2. Click **Create service account**.\n",
        "\n",
        "3. In the **Service account name** field, enter a name, and\n",
        "   click **Create**.\n",
        "\n",
        "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
        "into the filter box, and select\n",
        "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "\n",
        "5. Click *Create*. A JSON file that contains your key downloads to your\n",
        "local environment.\n",
        "\n",
        "6. Enter the path to your service account key as the\n",
        "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b7b634e-9d4c-4269-a946-2edbc08b3a1f",
      "metadata": {
        "id": "66e4e2a226ff"
      },
      "outputs": [],
      "source": [
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# If on Vertex AI Workbench, then don't execute this code\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
        "    \"DL_ANACONDA_HOME\"\n",
        "):\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c140460-1aa2-41d4-99e5-9d5e1d93e256",
      "metadata": {
        "id": "b06413d0ecfd"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "When you initialize the Vertex AI SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5914d839-90a5-4598-9505-8907d9ec818f",
      "metadata": {
        "id": "f4417df1dbbc"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15eba06a-b8ca-4ed2-9846-21a3b7ccba24",
      "metadata": {
        "id": "5fdc7d5a5e7f"
      },
      "outputs": [],
      "source": [
        "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
        "    BUCKET_NAME = PROJECT_ID + \"aip-\" + UUID\n",
        "    BUCKET_URI = f\"gs://{BUCKET_NAME}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f85c93c1-277c-43dc-89dd-af110ba94b18",
      "metadata": {
        "id": "58cb4f5895f0"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e6c8cb6-16bb-46d9-8708-f6508b07bbf6",
      "metadata": {
        "id": "b18df61adcf5"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91d7594f-544a-4591-ac7d-89ba7c292a74",
      "metadata": {
        "id": "2065e6fe44dc"
      },
      "source": [
        "##### Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dedd397-8aa6-4e5d-8c78-e383e5fd384e",
      "metadata": {
        "id": "f004d00a7000"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_URI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7d5f6cc-f5bb-45f4-81e6-ec83a9c36812",
      "metadata": {
        "id": "f3a43cbab621"
      },
      "outputs": [],
      "source": [
        "print(f\"PROJECT_ID = {PROJECT_ID}\")\n",
        "print(f\"BUCKET_NAME = {BUCKET_NAME}\")\n",
        "print(f\"REGION = {REGION}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93411664-a221-42c1-993c-0e98edd7ad29",
      "metadata": {
        "id": "4d0f7629c309"
      },
      "source": [
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffe291fd-550b-49d0-ac5a-6bb528936d8f",
      "metadata": {
        "id": "528bfbda0197"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb023b8e-0e4c-4bb6-90b4-6e9d666f2836",
      "metadata": {
        "id": "0f6785535b66"
      },
      "source": [
        "#### Service Account\n",
        "\n",
        "**If you don't know your service account**, try to get your service account using `gcloud` command by executing the second cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3093d9a0-55df-48e8-a938-bc2869c14ee2",
      "metadata": {
        "id": "034c005865b1"
      },
      "outputs": [],
      "source": [
        "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40aa319c-918f-449c-ac09-d2aaad0a63ac",
      "metadata": {
        "id": "785edeb40348"
      },
      "outputs": [],
      "source": [
        "if (\n",
        "    SERVICE_ACCOUNT == \"\"\n",
        "    or SERVICE_ACCOUNT is None\n",
        "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
        "):\n",
        "    # Get your service account from gcloud\n",
        "    if not IS_COLAB:\n",
        "        shell_output = !gcloud auth list 2>/dev/null\n",
        "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
        "\n",
        "    if IS_COLAB:\n",
        "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
        "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "\n",
        "    print(\"Service Account:\", SERVICE_ACCOUNT)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80bd3f1b-41e9-4692-9146-99b386930e73",
      "metadata": {
        "id": "ab78b8709866"
      },
      "source": [
        "## Custom Training on Vertex AI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0055bbc-f869-4c79-89bb-2b3fb4f3536f",
      "metadata": {
        "id": "555df0db139c"
      },
      "source": [
        "#### Recommended Training Application Structure\n",
        "\n",
        "You can structure your training application in any way you like. However, the [following structure](https://cloud.google.com/vertex-ai/docs/training/create-python-pre-built-container#structure) is commonly used in Vertex AI samples, and having your project's organization be similar to the samples can make it easier for you to follow the samples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28a83247-4317-4aa4-b800-f0a2c4946d56",
      "metadata": {
        "id": "0d52388cb4fd"
      },
      "source": [
        "We have `python_package` showing a sample packaging approaches. `README.md` files inside the directory has details on the directory structure and instructions on how to run application locally and on the cloud.\n",
        "\n",
        "```\n",
        "├── python_package\n",
        "│   ├── README.md\n",
        "│   ├── scripts\n",
        "│   │   └── train-cloud.sh\n",
        "│   ├── setup.py\n",
        "│   └── trainer\n",
        "│       ├── __init__.py\n",
        "│       ├── experiment.py\n",
        "│       ├── metadata.py\n",
        "│       ├── model.py\n",
        "│       ├── task.py\n",
        "│       └── utils.py\n",
        "└── pytorch-text-sentiment-classification-custom-train-deploy.ipynb    --> This notebook\n",
        "```\n",
        "\n",
        "1. Main project directory contains your `setup.py` file with the dependencies. \n",
        "2. Use a subdirectory named `trainer` to store your main application module and `scripts` to submit training jobs locally or cloud\n",
        "3. Inside `trainer` directory:\n",
        "    - `task.py` - Main application module 1) initializes and parse task arguments (hyper parameters), and 2) entry point to the trainer\n",
        "    - `model.py` -  Includes function to create model with a sequence classification head from a pre-trained model.\n",
        "    - `experiment.py` - Runs the model training and evaluation experiment, and exports the final model.\n",
        "    - `metadata.py` - Defines metadata for classification task such as predefined model dataset name, target labels\n",
        "    - `utils.py` - Includes utility functions such as data input functions to read data, save model to GCS bucket"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a2e8569-f87b-4d25-9ae1-a2dab79fec99",
      "metadata": {
        "id": "e3dbd659bfc6"
      },
      "source": [
        "### Run Custom Job on Vertex AI Training with a pre-built container"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "412077e3-5097-4278-b78c-dfea416cef60",
      "metadata": {
        "id": "4ca891f1dfc1"
      },
      "source": [
        "Vertex AI provides Docker container images that can be run as [pre-built containers](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers#available_container_images) for custom training. These containers include common dependencies used in training code based on the Machine Learning framework and framework version.\n",
        "\n",
        "In this notebook, we are using Hugging Face Datasets and fine tuning a transformer model from Hugging Face Transformers Library for sentiment analysis task using PyTorch. We will use [pre-built container for PyTorch](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers#pytorch) and package the training application code by adding standard Python dependencies - `transformers`, `datasets` and `tqdm` - in the `setup.py` file. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b4fe41e-0f99-40a5-aa21-62c2976deda9",
      "metadata": {
        "id": "fdf241983d01"
      },
      "outputs": [],
      "source": [
        "APP_NAME = \"finetuned-bert-classifier\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61bfe51b-c63b-4c8f-82bb-12afd15b0a87",
      "metadata": {
        "id": "9022ff0e121b"
      },
      "outputs": [],
      "source": [
        "PRE_BUILT_TRAINING_CONTAINER_IMAGE_URI = (\n",
        "    \"us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-11:latest\"\n",
        ")\n",
        "\n",
        "PYTHON_PACKAGE_APPLICATION_DIR = \"python_package\"\n",
        "\n",
        "source_package_file_name = f\"{PYTHON_PACKAGE_APPLICATION_DIR}/dist/trainer-0.1.tar.gz\"\n",
        "\n",
        "python_package_gcs_uri = (\n",
        "    f\"{BUCKET_URI}/pytorch-on-gcp/{APP_NAME}/train/python_package/trainer-0.1.tar.gz\"\n",
        ")\n",
        "\n",
        "python_module_name = \"trainer.task\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f069cac8-4bc0-4b28-b42e-c758e2e3adc6",
      "metadata": {
        "id": "ae8e41b828e1"
      },
      "source": [
        "### Creating files for each of the steps: preprocess and create model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cce9ed47-a01a-4402-ac0b-95b24b0d324c",
      "metadata": {
        "id": "d257b5c3c6b4"
      },
      "outputs": [],
      "source": [
        "!mkdir -p python_package/trainer\n",
        "!mkdir -p python_package/scripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dcc2e69-0f37-4712-ba15-27b745aecb85",
      "metadata": {
        "id": "432a06d4e41a"
      },
      "outputs": [],
      "source": [
        "!touch ./python_package/trainer/__init__.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9e25a3b-12be-43ae-bab9-e98fb5812fd9",
      "metadata": {
        "id": "112375025f5f"
      },
      "outputs": [],
      "source": [
        "%%writefile ./python_package/trainer/model.py\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from trainer import metadata\n",
        "\n",
        "def create(num_labels):\n",
        "    \"\"\"create the model by loading a pretrained model or define your \n",
        "    own\n",
        "\n",
        "    Args:\n",
        "      num_labels: number of target labels\n",
        "    \"\"\"\n",
        "    # Create the model, loss function, and optimizer\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        metadata.PRETRAINED_MODEL_NAME,\n",
        "        num_labels=num_labels\n",
        "    )\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46b44e47-fe03-4a98-9c5b-f2bb6028320b",
      "metadata": {
        "id": "d34c987cdb27"
      },
      "outputs": [],
      "source": [
        "%%writefile ./python_package/trainer/utils.py\n",
        "\n",
        "import os\n",
        "import datetime\n",
        "import pandas as pd\n",
        "\n",
        "from google.cloud import storage\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset, load_metric, ReadInstruction, DatasetDict, Dataset\n",
        "from trainer import metadata\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        metadata.PRETRAINED_MODEL_NAME,\n",
        "        use_fast=True,\n",
        "    )\n",
        "    \n",
        "    # Tokenize the texts\n",
        "    tokenizer_args = (\n",
        "        (examples['text'],) \n",
        "    )\n",
        "    result = tokenizer(*tokenizer_args, \n",
        "                       padding='max_length', \n",
        "                       max_length=metadata.MAX_SEQ_LENGTH, \n",
        "                       truncation=True)\n",
        "    \n",
        "    # TEMP: We can extract this automatically but Unique method of the dataset\n",
        "    # is not reporting the label -1 which shows up in the pre-processing\n",
        "    # Hence the additional -1 term in the dictionary\n",
        "    \n",
        "    label_to_id = metadata.TARGET_LABELS\n",
        "    \n",
        "    # Map labels to IDs (not necessary for GLUE tasks)\n",
        "    if label_to_id is not None and \"label\" in examples:\n",
        "        result[\"label\"] = [label_to_id[l] for l in examples[\"label\"]]\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def load_data(args):\n",
        "    \"\"\"Loads the data into two different data loaders. (Train, Test)\n",
        "\n",
        "        Args:\n",
        "            args: arguments passed to the python script\n",
        "    \"\"\"\n",
        "    # Dataset loading repeated here to make this cell idempotent\n",
        "    # Since we are over-writing datasets variable\n",
        "    \n",
        "    df_train = pd.read_csv(metadata.TRAIN_DATA)\n",
        "    df_test = pd.read_csv(metadata.TEST_DATA)\n",
        "    \n",
        "    dataset = DatasetDict({\"train\": Dataset.from_pandas(df_train),\"test\": Dataset.from_pandas(df_test)})\n",
        "\n",
        "    dataset = dataset.map(preprocess_function, \n",
        "                          batched=True, \n",
        "                          load_from_cache_file=True)\n",
        "\n",
        "    train_dataset, test_dataset = dataset[\"train\"], dataset[\"test\"]\n",
        "\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "\n",
        "def save_model(args):\n",
        "    \"\"\"Saves the model to Google Cloud Storage or local file system\n",
        "\n",
        "    Args:\n",
        "      args: contains name for saved model.\n",
        "    \"\"\"\n",
        "    scheme = 'gs://'\n",
        "    if args.job_dir.startswith(scheme):\n",
        "        job_dir = args.job_dir.split(\"/\")\n",
        "        bucket_name = job_dir[2]\n",
        "        object_prefix = \"/\".join(job_dir[3:]).rstrip(\"/\")\n",
        "\n",
        "        if object_prefix:\n",
        "            model_path = '{}/{}'.format(object_prefix, args.model_name)\n",
        "        else:\n",
        "            model_path = '{}'.format(args.model_name)\n",
        "\n",
        "        bucket = storage.Client().bucket(bucket_name)    \n",
        "        local_path = os.path.join(\"/tmp\", args.model_name)\n",
        "        files = [f for f in os.listdir(local_path) if os.path.isfile(os.path.join(local_path, f))]\n",
        "        for file in files:\n",
        "            local_file = os.path.join(local_path, file)\n",
        "            blob = bucket.blob(\"/\".join([model_path, file]))\n",
        "            blob.upload_from_filename(local_file)\n",
        "        print(f\"Saved model files in gs://{bucket_name}/{model_path}\")\n",
        "    else:\n",
        "        print(f\"Saved model files at {os.path.join('/tmp', args.model_name)}\")\n",
        "        print(f\"To save model files in GCS bucket, please specify job_dir starting with gs://\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72b35427-2800-4cd9-ade0-6c61d6cbef6b",
      "metadata": {
        "id": "30715ff75292"
      },
      "outputs": [],
      "source": [
        "%%writefile ./python_package/trainer/metadata.py\n",
        "\n",
        "# Task type can be either 'classification', 'regression', or 'custom'.\n",
        "# This is based on the target feature in the dataset.\n",
        "TASK_TYPE = 'classification'\n",
        "\n",
        "# Dataset paths\n",
        "    \n",
        "TRAIN_DATA = \"gs://cloud-samples-data/ai-platform-unified/datasets/text/happydb/happydb_train.csv\"\n",
        "TEST_DATA = \"gs://cloud-samples-data/ai-platform-unified/datasets/text/happydb/happydb_test.csv\"\n",
        "\n",
        "# pre-trained model name\n",
        "PRETRAINED_MODEL_NAME = 'bert-base-cased'\n",
        "\n",
        "# List of the class values (labels) in a classification dataset.\n",
        "TARGET_LABELS = {\"leisure\": 0, \"exercise\":1, \"enjoy_the_moment\":2, \"affection\":3,\"achievement\":4, \"nature\":5, \"bonding\":6}\n",
        "\n",
        "\n",
        "# maximum sequence length\n",
        "MAX_SEQ_LENGTH = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac9b6cf8-538c-48b0-9c1e-ade81e66033d",
      "metadata": {
        "id": "049f50cd8671"
      },
      "outputs": [],
      "source": [
        "%%writefile ./python_package/trainer/experiment.py\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import hypertune\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    EvalPrediction,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    default_data_collator,\n",
        "    TrainerCallback\n",
        ")\n",
        "\n",
        "from trainer import model, metadata, utils\n",
        "\n",
        "\n",
        "class HPTuneCallback(TrainerCallback):\n",
        "    \"\"\"\n",
        "    A custom callback class that reports a metric to hypertuner\n",
        "    at the end of each epoch.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, metric_tag, metric_value):\n",
        "        super(HPTuneCallback, self).__init__()\n",
        "        self.metric_tag = metric_tag\n",
        "        self.metric_value = metric_value\n",
        "        self.hpt = hypertune.HyperTune()\n",
        "        \n",
        "    def on_evaluate(self, args, state, control, **kwargs):\n",
        "        print(f\"HP metric {self.metric_tag}={kwargs['metrics'][self.metric_value]}\")\n",
        "        self.hpt.report_hyperparameter_tuning_metric(\n",
        "            hyperparameter_metric_tag=self.metric_tag,\n",
        "            metric_value=kwargs['metrics'][self.metric_value],\n",
        "            global_step=state.epoch)\n",
        "\n",
        "\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
        "    preds = np.argmax(preds, axis=1)\n",
        "    return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}\n",
        "\n",
        "\n",
        "def train(args, model, train_dataset, test_dataset):\n",
        "    \"\"\"Create the training loop to load pretrained model and tokenizer and \n",
        "    start the training process\n",
        "\n",
        "    Args:\n",
        "      args: read arguments from the runner to set training hyperparameters\n",
        "      model: The neural network that you are training\n",
        "      train_dataset: The training dataset\n",
        "      test_dataset: The test dataset for evaluation\n",
        "    \"\"\"\n",
        "    \n",
        "    # initialize the tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        metadata.PRETRAINED_MODEL_NAME,\n",
        "        use_fast=True,\n",
        "    )\n",
        "    \n",
        "    # set training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        learning_rate=args.learning_rate,\n",
        "        per_device_train_batch_size=args.batch_size,\n",
        "        per_device_eval_batch_size=args.batch_size,\n",
        "        num_train_epochs=args.num_epochs,\n",
        "        weight_decay=args.weight_decay,\n",
        "        output_dir=os.path.join(\"/tmp\", args.model_name)\n",
        "    )\n",
        "    \n",
        "    # initialize our Trainer\n",
        "    trainer = Trainer(\n",
        "        model,\n",
        "        training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset,\n",
        "        data_collator=default_data_collator,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "    \n",
        "    # add hyperparameter tuning callback to report metrics when enabled\n",
        "    if args.hp_tune == \"y\":\n",
        "        trainer.add_callback(HPTuneCallback(\"accuracy\", \"eval_accuracy\"))\n",
        "    \n",
        "    # training\n",
        "    trainer.train()\n",
        "    \n",
        "    return trainer\n",
        "\n",
        "\n",
        "def run(args):\n",
        "    \"\"\"Load the data, train, evaluate, and export the model for serving and\n",
        "     evaluating.\n",
        "\n",
        "    Args:\n",
        "      args: experiment parameters.\n",
        "    \"\"\"\n",
        "    # Open our dataset\n",
        "    train_dataset, test_dataset = utils.load_data(args)\n",
        "\n",
        "    label_list = train_dataset.unique(\"label\")\n",
        "    num_labels = len(label_list)\n",
        "    \n",
        "    # Create the model, loss function, and optimizer\n",
        "    text_classifier = model.create(num_labels=num_labels)\n",
        "    \n",
        "    # Train / Test the model\n",
        "    trainer = train(args, text_classifier, train_dataset, test_dataset)\n",
        "\n",
        "    metrics = trainer.evaluate(eval_dataset=test_dataset)\n",
        "    trainer.save_metrics(\"all\", metrics)\n",
        "\n",
        "    # Export the trained model\n",
        "    trainer.save_model(os.path.join(\"/tmp\", args.model_name))\n",
        "\n",
        "    # Save the model to GCS\n",
        "    if args.job_dir:\n",
        "        utils.save_model(args)\n",
        "    else:\n",
        "        print(f\"Saved model files at {os.path.join('/tmp', args.model_name)}\")\n",
        "        print(f\"To save model files in GCS bucket, please specify job_dir starting with gs://\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f96142ac-0a32-4eb9-bc65-57368ad4c518",
      "metadata": {
        "id": "2ad78a00cba1"
      },
      "outputs": [],
      "source": [
        "%%writefile ./python_package/trainer/task.py\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "from trainer import experiment\n",
        "\n",
        "\n",
        "def get_args():\n",
        "    \"\"\"Define the task arguments with the default values.\n",
        "\n",
        "    Returns:\n",
        "        experiment parameters\n",
        "    \"\"\"\n",
        "    args_parser = argparse.ArgumentParser()\n",
        "\n",
        "\n",
        "    # Experiment arguments\n",
        "    args_parser.add_argument(\n",
        "        '--batch-size',\n",
        "        help='Batch size for each training and evaluation step.',\n",
        "        type=int,\n",
        "        default=16)\n",
        "    args_parser.add_argument(\n",
        "        '--num-epochs',\n",
        "        help=\"\"\"\\\n",
        "        Maximum number of training data epochs on which to train.\n",
        "        If both --train-size and --num-epochs are specified,\n",
        "        --train-steps will be: (train-size/train-batch-size) * num-epochs.\\\n",
        "        \"\"\",\n",
        "        default=1,\n",
        "        type=int,\n",
        "    )\n",
        "    args_parser.add_argument(\n",
        "        '--seed',\n",
        "        help='Random seed (default: 42)',\n",
        "        type=int,\n",
        "        default=42,\n",
        "    )\n",
        "\n",
        "    # Estimator arguments\n",
        "    args_parser.add_argument(\n",
        "        '--learning-rate',\n",
        "        help='Learning rate value for the optimizers.',\n",
        "        default=2e-5,\n",
        "        type=float)\n",
        "    args_parser.add_argument(\n",
        "        '--weight-decay',\n",
        "        help=\"\"\"\n",
        "      The factor by which the learning rate should decay by the end of the\n",
        "      training.\n",
        "\n",
        "      decayed_learning_rate =\n",
        "        learning_rate * decay_rate ^ (global_step / decay_steps)\n",
        "\n",
        "      If set to 0 (default), then no decay will occur.\n",
        "      If set to 0.5, then the learning rate should reach 0.5 of its original\n",
        "          value at the end of the training.\n",
        "      Note that decay_steps is set to train_steps.\n",
        "      \"\"\",\n",
        "        default=0.01,\n",
        "        type=float)\n",
        "\n",
        "    # Enable hyperparameter\n",
        "    args_parser.add_argument(\n",
        "        '--hp-tune',\n",
        "        default=\"n\",\n",
        "        help='Enable hyperparameter tuning. Valida values are: \"y\" - enable, \"n\" - disable')\n",
        "    \n",
        "    # Saved model arguments\n",
        "    args_parser.add_argument(\n",
        "        '--job-dir',\n",
        "        default=os.getenv('AIP_MODEL_DIR'),\n",
        "        help='GCS location to export models')\n",
        "    args_parser.add_argument(\n",
        "        '--model-name',\n",
        "        default=\"finetuned-bert-classifier\",\n",
        "        help='The name of your saved model')\n",
        "\n",
        "    return args_parser.parse_args()\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Setup / Start the experiment\n",
        "    \"\"\"\n",
        "    args = get_args()\n",
        "    print(args)\n",
        "    experiment.run(args)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8ec0ea4-ad15-41f1-a629-8a2514f8f752",
      "metadata": {
        "id": "cba54c974340"
      },
      "outputs": [],
      "source": [
        "%%writefile ./python_package/scripts/train-cloud.sh\n",
        "#!/bin/bash\n",
        "# This script performs cloud training for a PyTorch model.\n",
        "\n",
        "echo \"Submitting Custom Job to Vertex AI to train PyTorch model\"\n",
        "\n",
        "# BUCKET_NAME: Change to your bucket name\n",
        "BUCKET_NAME=\"[your-bucket-name]\" # <-- CHANGE TO YOUR BUCKET NAME\n",
        "\n",
        "# validate bucket name\n",
        "if [ \"${BUCKET_NAME}\" = \"[your-bucket-name]\" ]\n",
        "then\n",
        "  echo \"[ERROR] INVALID VALUE: Please update the variable BUCKET_NAME with valid Cloud Storage bucket name. Exiting the script...\"\n",
        "  exit 1\n",
        "fi\n",
        "\n",
        "# The PyTorch image provided by Vertex AI Training.\n",
        "IMAGE_URI=\"us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-7:latest\"\n",
        "\n",
        "# JOB_NAME: the name of your job running on Vertex AI.\n",
        "JOB_PREFIX=\"finetuned-bert-classifier-pytorch-pkg-ar\"\n",
        "JOB_NAME=${JOB_PREFIX}-$(date +%Y%m%d%H%M%S)-custom-job\n",
        "\n",
        "# REGION: select a region from https://cloud.google.com/vertex-ai/docs/general/locations#available_regions\n",
        "# or use the default '`us-central1`'. The region is where the job will be run.\n",
        "REGION=\"us-central1\"\n",
        "\n",
        "# JOB_DIR: Where to store prepared package and upload output model.\n",
        "JOB_DIR=gs://${BUCKET_NAME}/${JOB_PREFIX}/model/${JOB_NAME}\n",
        "\n",
        "# worker pool spec\n",
        "worker_pool_spec=\"\\\n",
        "replica-count=1,\\\n",
        "machine-type=n1-standard-8,\\\n",
        "accelerator-type=NVIDIA_TESLA_V100,\\\n",
        "accelerator-count=1,\\\n",
        "executor-image-uri=${IMAGE_URI},\\\n",
        "python-module=trainer.task,\\\n",
        "local-package-path=../python_package/\"\n",
        "\n",
        "# Submit Custom Job to Vertex AI\n",
        "gcloud beta ai custom-jobs create \\\n",
        "    --display-name=${JOB_NAME} \\\n",
        "    --region ${REGION} \\\n",
        "    --worker-pool-spec=\"${worker_pool_spec}\" \\\n",
        "    --args=\"--model-name\",\"finetuned-bert-classifier\",\"--job-dir\",$JOB_DIR\n",
        "\n",
        "echo \"After the job is completed successfully, model files will be saved at $JOB_DIR/\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14892d48-48c1-4a28-b378-239549b24d25",
      "metadata": {
        "id": "92a191c72823"
      },
      "source": [
        "#### Following is the `setup.py` file for the training application. The `find_packages()` function inside `setup.py` includes the `trainer` directory in the package as it contains `__init__.py` which tells [Python Setuptools](https://setuptools.readthedocs.io/en/latest/) to include all subdirectories of the parent directory as dependencies. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2d4af5b-6c7f-465a-8aee-a9169dbbeff5",
      "metadata": {
        "id": "1b4a2de28849"
      },
      "outputs": [],
      "source": [
        "%%writefile ./{PYTHON_PACKAGE_APPLICATION_DIR}/setup.py\n",
        "\n",
        "from setuptools import find_packages\n",
        "from setuptools import setup\n",
        "import setuptools\n",
        "\n",
        "from distutils.command.build import build as _build\n",
        "import subprocess\n",
        "\n",
        "\n",
        "REQUIRED_PACKAGES = [\n",
        "    'transformers',\n",
        "    'datasets',\n",
        "    'tqdm',\n",
        "    'cloudml-hypertune'\n",
        "]\n",
        "\n",
        "setup(\n",
        "    name='trainer',\n",
        "    version='0.1',\n",
        "    install_requires=REQUIRED_PACKAGES,\n",
        "    packages=find_packages(),\n",
        "    include_package_data=True,\n",
        "    description='Vertex AI | Training | PyTorch | Text Classification | Python Package'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "950db7bd-d67c-4b16-9a07-8e51d1298805",
      "metadata": {
        "id": "cf62c83bdcbc"
      },
      "source": [
        "#### Run the following command to create a source distribution, dist/trainer-0.1.tar.gz:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b66428b-a993-49d6-b6ee-d3d350ed53b9",
      "metadata": {
        "id": "11d1299bcc12"
      },
      "outputs": [],
      "source": [
        "!cd {PYTHON_PACKAGE_APPLICATION_DIR} && python3 setup.py sdist --formats=gztar"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b6bf4b3-c1d4-46af-abb9-f6ef7ad0c506",
      "metadata": {
        "id": "7e61169bcd7a"
      },
      "source": [
        "Now upload the source distribution with training application to Cloud Storage bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1be9cb1-edbd-41a0-94ae-421a9637337a",
      "metadata": {
        "id": "bff74cab1888"
      },
      "outputs": [],
      "source": [
        "!gsutil cp {source_package_file_name} {python_package_gcs_uri}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c62ef32b-a3ee-4be6-a50b-123f3905ae5d",
      "metadata": {
        "id": "3d00f9beaba7"
      },
      "source": [
        "Validate the source distribution exists on Cloud Storage bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbada642-a3c7-409a-8274-98ee9b24a242",
      "metadata": {
        "id": "376bc46aa1e0"
      },
      "outputs": [],
      "source": [
        "!gsutil ls -l {python_package_gcs_uri}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68f40221-1ee7-48d6-adc1-8e8532d62a1e",
      "metadata": {
        "id": "88e4fd6394a0"
      },
      "source": [
        "### Run custom training job on Vertex AI\n",
        "\n",
        "We use [Vertex AI SDK for Python](https://cloud.google.com/vertex-ai/docs/start/client-libraries#client_libraries) to create and submit training job to the Vertex AI training service."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75164667-7aec-492b-876c-627eefd153b9",
      "metadata": {
        "id": "7d31e7ad1192"
      },
      "source": [
        "### Initialize the Vertex AI SDK for Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1bbe60b-1a2e-4807-b6ee-83f4ac8da6a7",
      "metadata": {
        "id": "0d689f14d93c"
      },
      "outputs": [],
      "source": [
        "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c08362c-11e3-4ee9-89e8-aefe90bd0b7f",
      "metadata": {
        "id": "1c78ddb72229"
      },
      "source": [
        "### Configure and submit Custom Job to Vertex AI Training service"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56f679a1-f875-4a78-927d-6556d8191ff9",
      "metadata": {
        "id": "b94927c47b83"
      },
      "source": [
        "Configure a [Custom Job](https://cloud.google.com/vertex-ai/docs/training/create-custom-job) with the [pre-built container](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers) image for PyTorch and training code packaged as Python source distribution. \n",
        "\n",
        "**NOTE:** When using Vertex AI SDK for Python for submitting a training job, it creates a [Training Pipeline](https://cloud.google.com/vertex-ai/docs/training/create-training-pipeline) which launches the Custom Job on Vertex AI Training service."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10d4dfd7-b24c-4b4a-aec7-5390aaeda20c",
      "metadata": {
        "id": "2faf52ce6839"
      },
      "outputs": [],
      "source": [
        "print(f\"APP_NAME={APP_NAME}\")\n",
        "print(\n",
        "    f\"PRE_BUILT_TRAINING_CONTAINER_IMAGE_URI={PRE_BUILT_TRAINING_CONTAINER_IMAGE_URI}\"\n",
        ")\n",
        "print(f\"python_package_gcs_uri={python_package_gcs_uri}\")\n",
        "print(f\"python_module_name={python_module_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04b2a27a-5710-4067-82a6-f7a24cbf20e7",
      "metadata": {
        "id": "cb06cf1b945d"
      },
      "outputs": [],
      "source": [
        "JOB_NAME = f\"{APP_NAME}-pytorch-pkg-ar-{UUID}\"\n",
        "print(f\"JOB_NAME={JOB_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24cf1978-e2ea-464b-9395-7fd71c08c900",
      "metadata": {
        "id": "c87d5618d366"
      },
      "outputs": [],
      "source": [
        "job = aiplatform.CustomPythonPackageTrainingJob(\n",
        "    display_name=f\"{JOB_NAME}\",\n",
        "    python_package_gcs_uri=python_package_gcs_uri,\n",
        "    python_module_name=python_module_name,\n",
        "    container_uri=PRE_BUILT_TRAINING_CONTAINER_IMAGE_URI,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1dca610-4d10-45d6-9963-b714bfac3141",
      "metadata": {
        "id": "d8ac1bcb31a7"
      },
      "outputs": [],
      "source": [
        "training_args = [\"--num-epochs\", \"2\", \"--model-name\", \"finetuned-bert-classifier\"]\n",
        "\n",
        "model = job.run(\n",
        "    replica_count=1,\n",
        "    machine_type=\"n1-standard-8\",\n",
        "    accelerator_type=\"NVIDIA_TESLA_V100\",\n",
        "    accelerator_count=1,\n",
        "    args=training_args,\n",
        "    sync=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2f07030-629b-4bbd-a574-8ff055682a12",
      "metadata": {
        "id": "7d0f5ea96675"
      },
      "source": [
        "#### Validate the model artifacts written to GCS by the training code after the job completes successfully"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbbad952-5620-4ab5-bc14-a33c6f79da76",
      "metadata": {
        "id": "502123b87fa3"
      },
      "outputs": [],
      "source": [
        "from google.protobuf.json_format import MessageToDict\n",
        "\n",
        "job_response = MessageToDict(job._gca_resource._pb)\n",
        "GCS_MODEL_ARTIFACTS_URI = job_response[\"trainingTaskInputs\"][\"baseOutputDirectory\"][\n",
        "    \"outputUriPrefix\"\n",
        "]\n",
        "print(f\"Model artifacts are available at {GCS_MODEL_ARTIFACTS_URI}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82c5d6d9-fff2-4966-a9ec-c29c9e6fe834",
      "metadata": {
        "id": "3d2205d5224f"
      },
      "outputs": [],
      "source": [
        "!gsutil ls -lr $GCS_MODEL_ARTIFACTS_URI/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e99f981-87df-4f16-9b0e-15a24810c85c",
      "metadata": {
        "id": "d2112c38aaad"
      },
      "source": [
        "## Deploying\n",
        "\n",
        "Deploying a PyTorch model on [Vertex AI Predictions](https://cloud.google.com/vertex-ai/docs/predictions/getting-predictions) requires to use a custom container that serves online predictions. You deploy a container running [PyTorch's TorchServe](https://pytorch.org/serve/) tool in order to serve predictions from a fine-tuned transformer model from Hugging Face Transformers for sentiment analysis task. You can then use Vertex AI Predictions to classify sentiment of input texts. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb1d3bda-aed2-42a9-b31d-1dc5ccd74057",
      "metadata": {
        "id": "9caf6a802e99"
      },
      "source": [
        "### Deploying model on Vertex AI Predictions with custom container\n",
        "\n",
        "To use a custom container to serve predictions from a PyTorch model, you must provide Vertex AI with a Docker container image that runs an HTTP server, such as TorchServe in this case. Please refer to [documentation](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements) that describes the container image requirements to be compatible with Vertex AI Predictions.\n",
        "\n",
        "Essentially, to deploy a PyTorch model on Vertex AI Predictions following are the steps:\n",
        "\n",
        "1. Package the trained model artifacts including [default](https://pytorch.org/serve/#default-handlers) or [custom](https://pytorch.org/serve/custom_service.html) handlers by creating an archive file using [Torch model archiver](https://github.com/pytorch/serve/tree/master/model-archiver)\n",
        "2. Build a [custom container](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements) compatible with Vertex AI Predictions to serve the model using Torchserve\n",
        "3. Upload the model with custom container image to serve predictions as a Vertex AI Model resource\n",
        "4. Create a Vertex AI Endpoint and [deploy the model](https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api) resource"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5f1b9ff-101d-4297-b45a-8367120b069e",
      "metadata": {
        "id": "13c7f8bb6cc1"
      },
      "source": [
        "#### **Create a custom model handler to handle prediction requests**\n",
        "\n",
        "When predicting sentiments of the input text with the fine-tuned transformer model, it requires pre-processing of the input text and post-processing by adding name to the target label along with probability (or confidence). We create a custom handler script that is packaged with the model artifacts and TorchServe executes the code when it runs. \n",
        "\n",
        "Custom handler script does the following:\n",
        "\n",
        "- Pre-process input text before sending it to the model for inference\n",
        "- Customize how the model is invoked for inference\n",
        "- Post-process output from the model before sending back a response\n",
        "\n",
        "Please refer to the [TorchServe documentation](https://pytorch.org/serve/custom_service.html) for defining a custom handler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd6caecd-76e3-4ccc-9724-1c347773686c",
      "metadata": {
        "id": "f109c2317225"
      },
      "outputs": [],
      "source": [
        "!mkdir -p predictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa73eaf1-940d-47ad-a824-73a115d0cdbd",
      "metadata": {
        "id": "e37fd93bcfa5"
      },
      "outputs": [],
      "source": [
        "%%writefile predictor/custom_handler.py\n",
        "\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from ts.torch_handler.base_handler import BaseHandler\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class TransformersClassifierHandler(BaseHandler):\n",
        "    \"\"\"\n",
        "    The handler takes an input string and returns the classification text \n",
        "    based on the serialized transformers checkpoint.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(TransformersClassifierHandler, self).__init__()\n",
        "        self.initialized = False\n",
        "\n",
        "    def initialize(self, ctx):\n",
        "        \"\"\" Loads the model.pt file and initialized the model object.\n",
        "        Instantiates Tokenizer for preprocessor to use\n",
        "        Loads labels to name mapping file for post-processing inference response\n",
        "        \"\"\"\n",
        "        self.manifest = ctx.manifest\n",
        "\n",
        "        properties = ctx.system_properties\n",
        "        model_dir = properties.get(\"model_dir\")\n",
        "        self.device = torch.device(\"cuda:\" + str(properties.get(\"gpu_id\")) if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Read model serialize/pt file\n",
        "        serialized_file = self.manifest[\"model\"][\"serializedFile\"]\n",
        "        model_pt_path = os.path.join(model_dir, serialized_file)\n",
        "        if not os.path.isfile(model_pt_path):\n",
        "            raise RuntimeError(\"Missing the model.pt or pytorch_model.bin file\")\n",
        "        \n",
        "        # Load model\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        logger.debug('Transformer model from path {0} loaded successfully'.format(model_dir))\n",
        "        \n",
        "        # Ensure to use the same tokenizer used during training\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "        # Read the mapping file, index to object name\n",
        "        mapping_file_path = os.path.join(model_dir, \"index_to_name.json\")\n",
        "\n",
        "        if os.path.isfile(mapping_file_path):\n",
        "            with open(mapping_file_path) as f:\n",
        "                self.mapping = json.load(f)\n",
        "        else:\n",
        "            logger.warning('Missing the index_to_name.json file. Inference output will default.')\n",
        "            self.mapping = {\"0\": \"Negative\",  \"1\": \"Positive\"}\n",
        "\n",
        "        self.initialized = True\n",
        "\n",
        "    def preprocess(self, data):\n",
        "        \"\"\" Preprocessing input request by tokenizing\n",
        "            Extend with your own preprocessing steps as needed\n",
        "        \"\"\"\n",
        "        text = data[0].get(\"data\")\n",
        "        if text is None:\n",
        "            text = data[0].get(\"body\")\n",
        "        sentences = text.decode('utf-8')\n",
        "        logger.info(\"Received text: '%s'\", sentences)\n",
        "\n",
        "        # Tokenize the texts\n",
        "        tokenizer_args = ((sentences,))\n",
        "        inputs = self.tokenizer(*tokenizer_args,\n",
        "                                padding='max_length',\n",
        "                                max_length=128,\n",
        "                                truncation=True,\n",
        "                                return_tensors = \"pt\")\n",
        "        return inputs\n",
        "\n",
        "    def inference(self, inputs):\n",
        "        \"\"\" Predict the class of a text using a trained transformer model.\n",
        "        \"\"\"\n",
        "        prediction = self.model(inputs['input_ids'].to(self.device))[0].argmax().item()\n",
        "\n",
        "        if self.mapping:\n",
        "            prediction = self.mapping[str(prediction)]\n",
        "\n",
        "        logger.info(\"Model predicted: '%s'\", prediction)\n",
        "        return [prediction]\n",
        "\n",
        "    def postprocess(self, inference_output):\n",
        "        return inference_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bfb4670-2077-4468-ade2-7448b6dcab82",
      "metadata": {
        "id": "2e504b5529b8"
      },
      "source": [
        "### Generate target label to name file\n",
        "\n",
        "In the custom handler, we refer to a mapping file between target labels and their meaningful names that will be used to format the prediction response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be0ec8ed-e4c1-45cc-98f0-a74626238d18",
      "metadata": {
        "id": "d3c4969c2759"
      },
      "outputs": [],
      "source": [
        "%%writefile ./predictor/index_to_name.json\n",
        "\n",
        "{\n",
        "    \"0\": \"leisure\",\n",
        "    \"1\": \"exercise\",\n",
        "    \"2\": \"enjoy_the_moment\",\n",
        "    \"3\": \"affection\",\n",
        "    \"4\": \"achievement\",\n",
        "    \"5\": \"nature\",\n",
        "    \"6\": \"bonding\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "915bf456-b362-4cae-add1-4a0420e7aaab",
      "metadata": {
        "id": "6f0026a985fa"
      },
      "source": [
        "### Create custom container image to serve predictions\n",
        "\n",
        "We use Cloud Build to create the custom container image with following build steps:\n",
        "\n",
        "#### Download model artifacts\n",
        "\n",
        "Download model artifacts that were saved as part of the training (or hyperparameter tuning) job from Cloud Storage to local directory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb7ea1b3-2abc-4f67-ab7b-f7c53c5eb19f",
      "metadata": {
        "id": "86b272c76889"
      },
      "source": [
        "Validate model artifact files in the Cloud Storage bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7d16875-a9aa-4dae-9983-56c38555b2e7",
      "metadata": {
        "id": "0d160b2911ac"
      },
      "outputs": [],
      "source": [
        "!gsutil ls -r $GCS_MODEL_ARTIFACTS_URI/model/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25b0f8aa-6d61-458c-97e4-4c385162ef38",
      "metadata": {
        "id": "38d171b3eb23"
      },
      "source": [
        "Copy files from Cloud Storage to local directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94f7708f-b07f-469c-a950-6fab67d3850e",
      "metadata": {
        "id": "9138f330c9f2"
      },
      "outputs": [],
      "source": [
        "!gsutil -m cp -r $GCS_MODEL_ARTIFACTS_URI/model/ ./predictor/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fea34b8c-589c-48f5-8007-dc804ddb502f",
      "metadata": {
        "id": "2f9c5a82b1ed"
      },
      "outputs": [],
      "source": [
        "!ls -ltrR ./predictor/model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5709fd2a-0114-41b5-bbd6-27e31a73747f",
      "metadata": {
        "id": "5ec61feb1188"
      },
      "source": [
        "### Build the container image\n",
        "\n",
        "Create a Dockerfile with TorchServe as base image:\n",
        "\n",
        " - **`RUN`**: Installs dependencies such as `transformers`\n",
        " - **`COPY`**: Add model artifacts to `/home/model-server/` directory of the container image\n",
        " - **`COPY`**: Add custom handler script to `/home/model-server/` directory of the container image\n",
        " - **`RUN`**: Create `/home/model-server/config.properties` to define the serving configuration (health and prediction listener ports)\n",
        " - **`RUN`**: Run [Torch model archiver](https://pytorch.org/serve/model-archiver.html) to create a model archive file from the files copied into the image `/home/model-server/`. The model archive is saved in the `/home/model-server/model-store/` with name same as `<model-name>.mar`\n",
        " - **`CMD`**: Launch Torchserve HTTP server referencing the configuration properties and enables serving for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f44afbc8-2900-408f-a5a8-a43d3679f16f",
      "metadata": {
        "id": "6602ec14439c"
      },
      "outputs": [],
      "source": [
        "%%bash -s $APP_NAME\n",
        "\n",
        "APP_NAME=$1\n",
        "\n",
        "cat << EOF > ./predictor/Dockerfile\n",
        "\n",
        "FROM pytorch/torchserve:latest-cpu\n",
        "\n",
        "# install dependencies\n",
        "RUN python3 -m pip install --upgrade pip\n",
        "RUN pip3 install transformers\n",
        "\n",
        "USER model-server\n",
        "\n",
        "# copy model artifacts, custom handler and other dependencies\n",
        "COPY ./custom_handler.py /home/model-server/\n",
        "COPY ./index_to_name.json /home/model-server/\n",
        "COPY ./model/$APP_NAME/ /home/model-server/\n",
        "\n",
        "# create torchserve configuration file\n",
        "USER root\n",
        "RUN printf \"\\nservice_envelope=json\" >> /home/model-server/config.properties\n",
        "RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home/model-server/config.properties\n",
        "RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /home/model-server/config.properties\n",
        "USER model-server\n",
        "\n",
        "# expose health and prediction listener ports from the image\n",
        "EXPOSE 7080\n",
        "EXPOSE 7081\n",
        "\n",
        "# create model archive file packaging model artifacts and dependencies\n",
        "RUN torch-model-archiver -f \\\n",
        "  --model-name=$APP_NAME \\\n",
        "  --version=1.0 \\\n",
        "  --serialized-file=/home/model-server/pytorch_model.bin \\\n",
        "  --handler=/home/model-server/custom_handler.py \\\n",
        "  --extra-files \"/home/model-server/config.json,/home/model-server/tokenizer.json,/home/model-server/training_args.bin,/home/model-server/tokenizer_config.json,/home/model-server/special_tokens_map.json,/home/model-server/vocab.txt,/home/model-server/index_to_name.json\" \\\n",
        "  --export-path=/home/model-server/model-store\n",
        "\n",
        "# run Torchserve HTTP serve to respond to prediction requests\n",
        "CMD [\"torchserve\", \\\n",
        "     \"--start\", \\\n",
        "     \"--ts-config=/home/model-server/config.properties\", \\\n",
        "     \"--models\", \\\n",
        "     \"$APP_NAME=$APP_NAME.mar\", \\\n",
        "     \"--model-store\", \\\n",
        "     \"/home/model-server/model-store\"]\n",
        "EOF\n",
        "\n",
        "echo \"Writing ./predictor/Dockerfile\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "290f71f2-6926-4615-a45a-1556b8b2d10c",
      "metadata": {
        "id": "4601a036e327"
      },
      "source": [
        "### Build the docker image tagged with Container Registry (gcr.io) path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54233d45-65f6-4bf0-9b27-59bc75567e11",
      "metadata": {
        "id": "077f68ee16b0"
      },
      "outputs": [],
      "source": [
        "CUSTOM_PREDICTOR_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/pytorch_predict_{APP_NAME}\"\n",
        "print(f\"CUSTOM_PREDICTOR_IMAGE_URI = {CUSTOM_PREDICTOR_IMAGE_URI}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcff05b4-f3b1-4517-a994-4517591115a9",
      "metadata": {
        "id": "3596d70eb944"
      },
      "outputs": [],
      "source": [
        "!docker build \\\n",
        "  --tag=$CUSTOM_PREDICTOR_IMAGE_URI \\\n",
        "  ./predictor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9adf596-8b2f-425c-97d8-a6b942b0ac59",
      "metadata": {
        "id": "2c331bdaf370"
      },
      "source": [
        "### Deploying the serving container to Vertex AI Predictions\n",
        "\n",
        "We create a model resource on Vertex AI and deploy the model to a Vertex AI Endpoints. You must deploy a model to an endpoint before using the model. The deployed model runs the custom container image to serve predictions. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5f8a605-b720-41e1-abc4-579b9b0cf6e2",
      "metadata": {
        "id": "16dd589dc366"
      },
      "source": [
        "### Push the serving container to Container Registry\n",
        "\n",
        "Push your container image with inference code and dependencies to your Container Registry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5da64ed-07b6-43ec-b23f-5e0664909e7b",
      "metadata": {
        "id": "84c48319fe03"
      },
      "outputs": [],
      "source": [
        "!docker push $CUSTOM_PREDICTOR_IMAGE_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "648d301d-b1d5-485c-b567-094ade18c5a8",
      "metadata": {
        "id": "f77b3d03b2e9"
      },
      "source": [
        "### Create a Model resource with custom serving container"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c612999-df73-4bbf-a1f7-b5870e0325da",
      "metadata": {
        "id": "24edef1346b0"
      },
      "outputs": [],
      "source": [
        "VERSION = 1\n",
        "model_display_name = f\"{APP_NAME}-v{VERSION}\"\n",
        "model_description = \"PyTorch based text classifier with custom container\"\n",
        "\n",
        "MODEL_NAME = APP_NAME\n",
        "health_route = \"/ping\"\n",
        "predict_route = f\"/predictions/{MODEL_NAME}\"\n",
        "serving_container_ports = [7080]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d35bae06-05b6-4446-a25b-7d33ce613f79",
      "metadata": {
        "id": "f5a1472f735b"
      },
      "outputs": [],
      "source": [
        "model = aiplatform.Model.upload(\n",
        "    display_name=model_display_name,\n",
        "    description=model_description,\n",
        "    serving_container_image_uri=CUSTOM_PREDICTOR_IMAGE_URI,\n",
        "    serving_container_predict_route=predict_route,\n",
        "    serving_container_health_route=health_route,\n",
        "    serving_container_ports=serving_container_ports,\n",
        ")\n",
        "\n",
        "model.wait()\n",
        "\n",
        "print(model.display_name)\n",
        "print(model.resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4807385d-4f3a-459e-b376-07a303e8a917",
      "metadata": {
        "id": "a7720d771633"
      },
      "source": [
        "### Create an Endpoint for Model with Custom Container"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ce38952-6df7-4bdf-a6ac-7759eb718aff",
      "metadata": {
        "id": "fc8fe6b64dd1"
      },
      "outputs": [],
      "source": [
        "endpoint_display_name = f\"{APP_NAME}-endpoint\"\n",
        "endpoint = aiplatform.Endpoint.create(display_name=endpoint_display_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1605f2fb-c516-4ab1-8621-77f27ffb2d0c",
      "metadata": {
        "id": "061ab3d05205"
      },
      "source": [
        "### Deploy the Model to Endpoint\n",
        "\n",
        "Deploying a model associates physical resources with the model so it can serve online predictions with low latency. \n",
        "\n",
        "**NOTE:** This step takes few minutes to deploy the resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f010c782-adad-485d-8eff-6f0d06efe9cb",
      "metadata": {
        "id": "d65aae3de6c1"
      },
      "outputs": [],
      "source": [
        "traffic_percentage = 100\n",
        "machine_type = \"n1-standard-4\"\n",
        "deployed_model_display_name = model_display_name\n",
        "min_replica_count = 1\n",
        "max_replica_count = 3\n",
        "sync = True\n",
        "\n",
        "model.deploy(\n",
        "    endpoint=endpoint,\n",
        "    deployed_model_display_name=deployed_model_display_name,\n",
        "    machine_type=machine_type,\n",
        "    traffic_percentage=traffic_percentage,\n",
        "    sync=sync,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7fcc508-a5fe-436b-acb1-a03f58f1d49e",
      "metadata": {
        "id": "c8d7efea5633"
      },
      "source": [
        "### Invoking the Endpoint with deployed Model using Vertex AI SDK to make predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9f442ae-4e17-4a6c-97f7-8dee733917d6",
      "metadata": {
        "id": "094f7b4d8007"
      },
      "source": [
        "#### Formatting input for online prediction\n",
        "\n",
        "This notebook uses [Torchserve's KServe based inference API](https://pytorch.org/serve/inference_api.html#kserve-inference-api) which is also [Vertex AI Predictions compatible format](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#prediction). For online prediction requests, format the prediction input instances as JSON with base64 encoding as shown here:\n",
        "\n",
        "```\n",
        "[\n",
        "    {\n",
        "        \"data\": {\n",
        "            \"b64\": \"<base64 encoded string>\"\n",
        "        }\n",
        "    }\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b33f4e1f-a152-4ab5-b643-293e08a4b5ac",
      "metadata": {
        "id": "b5f0b7f8fc9a"
      },
      "outputs": [],
      "source": [
        "test_instances = [\n",
        "    b\"I went to a meeting that went really well.\",\n",
        "    b\"I ran four miles this morning with a good time.\",\n",
        "    b\"Watching the storms we had yesterday.  The lightning was incredible!\",\n",
        "    b\"The last night I said with her 'I love you '. And she said ' Yes'.\",\n",
        "    b\"I had followed a complex recipe making roasted duck, which took me hours and I had successfully made it.\",\n",
        "    b\"I woke up this morning to birds chirping.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d29cc5f7-bfb5-4d9e-9295-c62cb2bb822f",
      "metadata": {
        "id": "d0a24c94d66c"
      },
      "source": [
        "### Sending an online prediction request\n",
        "\n",
        "Format input text string and call prediction endpoint with formatted input request and get the response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95f61301-d64e-4bbc-a08f-b276cf8c4871",
      "metadata": {
        "id": "515acf48503c"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import json\n",
        "\n",
        "print(\"=\" * 100)\n",
        "for instance in test_instances:\n",
        "    print(f\"Input text: \\n\\t{instance.decode('utf-8')}\\n\")\n",
        "    b64_encoded = base64.b64encode(instance)\n",
        "    test_instance = [{\"data\": {\"b64\": f\"{str(b64_encoded.decode('utf-8'))}\"}}]\n",
        "    print(f\"Formatted input: \\n{json.dumps(test_instance, indent=4)}\\n\")\n",
        "    prediction = endpoint.predict(instances=test_instance)\n",
        "    print(f\"Prediction response: \\n\\t{prediction}\")\n",
        "    print(\"=\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32e9cb26-3586-477c-9177-e62f53c2d92c",
      "metadata": {
        "id": "787f896a1daa"
      },
      "source": [
        "## Cleaning up \n",
        "\n",
        "To clean up all Google Cloud resources used in this notebook, you can [delete the Google Cloud project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:\n",
        "\n",
        "- Training Jobs\n",
        "- Model\n",
        "- Endpoint\n",
        "- Cloud Storage Bucket\n",
        "- Container Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae615410-f163-4eb5-944f-d10aeab92754",
      "metadata": {
        "id": "bdefede69285"
      },
      "outputs": [],
      "source": [
        "delete_custom_job = True\n",
        "delete_endpoint = True\n",
        "delete_model = True\n",
        "delete_bucket = True\n",
        "delete_image = True\n",
        "\n",
        "if delete_custom_job:\n",
        "    job.delete()\n",
        "\n",
        "if delete_endpoint:\n",
        "    endpoint.undeploy_all()\n",
        "    endpoint.delete()\n",
        "\n",
        "if delete_model:\n",
        "    model.delete()\n",
        "\n",
        "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
        "    ! gsutil -m rm -r $BUCKET_URI\n",
        "\n",
        "if delete_image:\n",
        "    gcr_images = !gcloud container images list --repository=gcr.io/$PROJECT_ID --filter=\"name~\"$APP_NAME\n",
        "    for image in gcr_images:\n",
        "        if image != \"NAME\":  # skip header line\n",
        "            print(f\"Deleting image {image} including all tags\")\n",
        "            !gcloud container images delete $image --force-delete-tags --quiet"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "pytorch-text-sentiment-classification-custom-train-deploy.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

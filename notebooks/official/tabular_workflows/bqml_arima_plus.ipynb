{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "copyright"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# Train a BigQuery ML ARIMA_PLUS Model using Vertex AI tabular workflows\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/tabular_workflows/bqml_arima_plus.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"><br> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Ftabular_workflows%2Fbqml_arima_plus.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/tabular_workflows/bqml_arima_plus.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br>\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/tabular_workflows/bqml_arima_plus.ipynb\">\n",
        "        <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br>\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "<br/><br/><br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview:automl"
      },
      "source": [
        "## Overview\n",
        "\n",
        "In this tutorial, you take on the role of a store planner who determines how much inventory for each product needs to be ordered for each store for November 2019. You accomplish this by training a BigQuery ML (BQML) [ARIMA_PLUS](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create-time-series) forecasting model using historical sales data. A BQML ARIMA_PLUS model is useful if you need to perform many quick iterations of model training or if you need an inexpensive baseline to measure other models against.\n",
        "\n",
        "Learn more about [BQML ARIMA+ forecasting for tabular data](https://cloud.google.com/vertex-ai/docs/tabular-data/forecasting-arima/overview)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "objective:automl,training,batch_prediction"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this notebook, you learn how to create the BigQuery ML ARIMA_PLUS model using a training [Vertex AI Pipeline](https://cloud.google.com/vertex-ai/docs/pipelines/introduction) from [Google Cloud Pipeline Components](https://cloud.google.com/vertex-ai/docs/pipelines/components-introduction) (GCPC), and then do a batch prediction using the corresponding prediction pipeline. You then train a Vertex AI forecasting model using the same data and compare the evaluation metrics.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services and resources:\n",
        "\n",
        "- BigQuery\n",
        "- Vertex AI\n",
        "\n",
        "The steps performed are:\n",
        "\n",
        "- Train the BigQuery ML ARIMA_PLUS model.\n",
        "- View BigQuery ML model evaluation.\n",
        "- Make a batch prediction with the BigQuery ML model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset:covid,forecast"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "To demonstrate the tradeoffs between BigQuery ML and Vertex AI forecasting, this tutorial uses a synthetic dataset where product sales are dependent on a variety of factors such as advertisements, holidays, and locations. You see how well a univariate model like ARIMA_PLUS can forecast future sales without knowing information about these factors explicitly, and how well a multivariate model like Vertex AI forecasting can perform when these factors are known."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "costs"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "* BigQuery / BigQuery ML\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), and [BigQuery pricing](https://cloud.google.com/bigquery/pricing) and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2927eacc7883"
      },
      "source": [
        "### Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_aip:mbsdk"
      },
      "source": [
        "### Install Vertex AI SDK for Python and other required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZCCaJsYQEH4"
      },
      "outputs": [],
      "source": [
        "# Install required packages.\n",
        "! pip3 install --quiet --upgrade google-cloud-aiplatform \\\n",
        "                                 google-cloud-bigquery \\\n",
        "                                 google-cloud-pipeline-components \\\n",
        "                                 db-dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "restart"
      },
      "source": [
        "### Restart runtime (Colab only)\n",
        "\n",
        "To use the newly installed packages, you must restart the runtime on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f09b4dff629a"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a2b7b59bbf7"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f82e28c631cc"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "Authenticate your environment on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-ZBOjErv5mM"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "before_you_begin:nogpu"
      },
      "source": [
        "### Set Google Cloud project information\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_project_id"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "DATA_LOCATION = \"US\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bucket:mbsdk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "Create a storage bucket to store intermediate artifacts such as datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bucket"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "autoset_bucket"
      },
      "source": [
        "**If your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91c46850b49b"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85c4ecfd133a"
      },
      "source": [
        "#### Service Account\n",
        "\n",
        "**If you don't know your service account**, try to get your service account using `gcloud` command by executing the second cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77b01a1fdbb4"
      },
      "outputs": [],
      "source": [
        "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f936bebda2d4"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "if (\n",
        "    SERVICE_ACCOUNT == \"\"\n",
        "    or SERVICE_ACCOUNT is None\n",
        "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
        "):\n",
        "    # Get your service account from gcloud\n",
        "    if not IS_COLAB:\n",
        "        shell_output = !gcloud auth list 2>/dev/null\n",
        "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
        "\n",
        "    else:  # IS_COLAB:\n",
        "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
        "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "\n",
        "    print(\"Service Account:\", SERVICE_ACCOUNT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40ef6967cad3"
      },
      "source": [
        "#### Set service account access for Vertex AI Pipelines\n",
        "\n",
        "Run the following commands to grant your service account access to read and write pipeline artifacts in the bucket that you created in the previous step. You only need to run this step once per service account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f88cb0488c08"
      },
      "outputs": [],
      "source": [
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
        "\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_vars"
      },
      "source": [
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import urllib\n",
        "import uuid\n",
        "\n",
        "from google.cloud import aiplatform, bigquery\n",
        "from google_cloud_pipeline_components.v1.automl.forecasting import utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk"
      },
      "source": [
        "## Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fm4Pyn1dQEH7"
      },
      "outputs": [],
      "source": [
        "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNozhksr4Mc-"
      },
      "source": [
        "## Define train and prediction data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TWLAziDiFrL"
      },
      "source": [
        "### Create a BigQuery dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KT1Ik7XqBsxv"
      },
      "outputs": [],
      "source": [
        "arima_dataset_name = \"forecasting_demo_arima\"\n",
        "arima_dataset_path = \".\".join([PROJECT_ID, arima_dataset_name])\n",
        "\n",
        "# Must be same location as TRAINING_DATASET_BQ_PATH.\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "bq_dataset_pre = bigquery.Dataset(arima_dataset_path)\n",
        "bq_dataset_pre.location = DATA_LOCATION\n",
        "try:\n",
        "    bq_dataset = client.create_dataset(bq_dataset_pre)\n",
        "except:\n",
        "    bq_dataset = client.get_dataset(bq_dataset_pre)\n",
        "print(f\"Created bigquery dataset {arima_dataset_path} in {DATA_LOCATION}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_file:u_dataset,csv"
      },
      "source": [
        "### Prepare training data in BigQuery\n",
        "\n",
        "Before training a model, you must first generate our dataset of store sales. This dataset includes multiple products and stores, and it also simulates factors such as advertisements and holiday effects. The data is then split into `TRAIN`, `VALIDATE`, `TEST`, and `PREDICT` sets, where the last three sets are all 1 month in duration.\n",
        "\n",
        "#### Begin by defining the subqueries that creates this base sales data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpJxVD8fo3Rj"
      },
      "outputs": [],
      "source": [
        "base_data_query = \"\"\"\n",
        "  WITH \n",
        "\n",
        "    -- Create time series for each product + store with some covariates.\n",
        "    time_series AS (\n",
        "      SELECT\n",
        "        CONCAT(\"id_\", store_id, \"_\", product_id) AS id,\n",
        "        CONCAT('store_', store_id) AS store,\n",
        "        CONCAT('product_', product_id) AS product,\n",
        "        date,\n",
        "        -- Advertise 1/100 products.\n",
        "        IF(\n",
        "          ABS(MOD(FARM_FINGERPRINT(CONCAT(product_id, date)), 100)) = 0,\n",
        "          1,\n",
        "          0\n",
        "        ) AS advertisement,\n",
        "        -- Mark Thanksgiving sales as holiday sales.\n",
        "        IF(\n",
        "          EXTRACT(DAYOFWEEK FROM date) = 6\n",
        "            AND EXTRACT(MONTH FROM date) = 11\n",
        "            AND EXTRACT(DAY FROM date) BETWEEN 23 AND 29,\n",
        "          1,\n",
        "          0\n",
        "        ) AS holiday,\n",
        "        -- Set when each data split ends.\n",
        "        CASE\n",
        "          WHEN date < '2019-09-01' THEN 'TRAIN'\n",
        "          WHEN date < '2019-10-01' THEN 'VALIDATE'\n",
        "          WHEN date < '2019-11-01' THEN 'TEST'\n",
        "          ELSE 'PREDICT'\n",
        "        END AS split,\n",
        "      -- Generate the sales with one SKU per date.\n",
        "      FROM\n",
        "        UNNEST(GENERATE_DATE_ARRAY('2017-01-01', '2019-12-01')) AS date\n",
        "      CROSS JOIN\n",
        "        UNNEST(GENERATE_ARRAY(0, 10)) AS product_id\n",
        "      CROSS JOIN\n",
        "        UNNEST(GENERATE_ARRAY(0, 3)) AS store_id  \n",
        "    ),\n",
        "    \n",
        "    -- Randomly determine factors that contribute to how syntheic sales are calculated. \n",
        "    time_series_sales_factors AS (\n",
        "      SELECT\n",
        "        *,\n",
        "        ABS(MOD(FARM_FINGERPRINT(product), 10)) AS product_factor,\n",
        "        ABS(MOD(FARM_FINGERPRINT(store), 10)) AS store_factor,\n",
        "        [1.6, 0.6, 0.8, 1.0, 1.2, 1.8, 2.0][\n",
        "          ORDINAL(EXTRACT(DAYOFWEEK FROM date))] AS day_of_week_factor,\n",
        "        1 +  SIN(EXTRACT(MONTH FROM date) * 2.0 * 3.14 / 24.0) AS month_factor,    \n",
        "        -- Advertised products have increased sales factors for 5 days.\n",
        "        CASE\n",
        "          WHEN LAG(advertisement, 0) OVER w = 1.0 THEN 1.2\n",
        "          WHEN LAG(advertisement, 1) OVER w = 1.0 THEN 1.8\n",
        "          WHEN LAG(advertisement, 2) OVER w = 1.0 THEN 2.4\n",
        "          WHEN LAG(advertisement, 3) OVER w = 1.0 THEN 3.0\n",
        "          WHEN LAG(advertisement, 4) OVER w = 1.0 THEN 1.4\n",
        "          ELSE 1.0\n",
        "        END AS advertisement_factor,\n",
        "        IF(holiday = 1.0, 2.0, 1.0) AS holiday_factor,\n",
        "        0.001 * ABS(MOD(FARM_FINGERPRINT(CONCAT(product, store, date)), 100)) AS noise_factor\n",
        "      FROM\n",
        "        time_series\n",
        "      WINDOW w AS (PARTITION BY id ORDER BY date)\n",
        "    ),\n",
        "  \n",
        "    -- Use factors to calculate synthetic sales for each time series. \n",
        "    base_data AS (\n",
        "      SELECT\n",
        "        id,\n",
        "        store,\n",
        "        product,\n",
        "        date,\n",
        "        split,\n",
        "        advertisement,\n",
        "        holiday,\n",
        "        (\n",
        "          (1 + store_factor) \n",
        "          * (1 + product_factor) \n",
        "          * (1 + month_factor + day_of_week_factor) \n",
        "          * (\n",
        "            1.0 \n",
        "            + 2.0 * advertisement_factor \n",
        "            + 3.0 * holiday_factor \n",
        "            + 5.0 * noise_factor\n",
        "          )\n",
        "        ) AS sales\n",
        "      FROM\n",
        "        time_series_sales_factors\n",
        "      )\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdWrtxYtqdsh"
      },
      "source": [
        "Next, convert this base sales data into a dataset you use to train a model, and a dataset you pass to a trained model at serving time. The training dataset includes the `TRAIN`, `VALIDATE`, and `TEST` splits, while the prediction dataset includes the `PREDICT` split and also the `TEST` split to provide context information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvLshoaApviB"
      },
      "outputs": [],
      "source": [
        "TRAINING_DATASET_BQ_PATH = f\"{arima_dataset_path}.train\"\n",
        "PREDICTION_DATASET_BQ_PATH = f\"{arima_dataset_path}.pred\"\n",
        "\n",
        "train_query = f\"\"\"\n",
        "    CREATE OR REPLACE TABLE `{arima_dataset_path}.train` AS\n",
        "    {base_data_query}\n",
        "    SELECT *\n",
        "    FROM base_data\n",
        "    WHERE split != 'PREDICT'\n",
        "\"\"\"\n",
        "client.query(train_query).result()\n",
        "print(f\"Created {TRAINING_DATASET_BQ_PATH}.\")\n",
        "\n",
        "pred_query = f\"\"\"\n",
        "    CREATE OR REPLACE TABLE `{arima_dataset_path}.pred` AS\n",
        "    {base_data_query}\n",
        "    SELECT *\n",
        "    FROM base_data\n",
        "    WHERE split = 'TEST'\n",
        "\n",
        "    UNION ALL\n",
        "\n",
        "    SELECT * EXCEPT (sales), NULL AS sales\n",
        "    FROM base_data\n",
        "    WHERE split = 'PREDICT'\n",
        "\"\"\"\n",
        "client.query(pred_query).result()\n",
        "print(f\"Created {PREDICTION_DATASET_BQ_PATH}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwr0eDQjpR4c"
      },
      "source": [
        "You can take a look at the sales data that was generated. Later in this tutorial, you can see the visualization of the time series along with the forecast.\n",
        "\n",
        "The model is trained with data from January 2017 to October 2019 inclusive.\n",
        "\n",
        "#### Look at the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyBgcsfZpREc"
      },
      "outputs": [],
      "source": [
        "query = f\"SELECT * FROM `{arima_dataset_path}.train` LIMIT 10\"\n",
        "client.query(query).to_dataframe().head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F00dL8oEqqVb"
      },
      "source": [
        "The table used for prediction contains data from November 2019. It also includes actuals from October 2019 as context information.\n",
        "\n",
        "#### Look at the prediction data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOPL6Cv4qIhZ"
      },
      "outputs": [],
      "source": [
        "query = f\"SELECT * FROM `{arima_dataset_path}.pred` LIMIT 10\"\n",
        "client.query(query).to_dataframe().head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tutorial_start:automl"
      },
      "source": [
        "# Create a BigQuery ML ARIMA_PLUS model\n",
        "\n",
        "Now you are ready to start creating your own BigQuery ML ARIMA_PLUS model.\n",
        "\n",
        "Like with Vertex AI forecasting, the pipeline you run trains evaluation models using the training and validation sets and use backtesting to create evaluation metrics on the test set. Finally, a serving model that uses all available data can be produced.\n",
        "\n",
        "**How do you estimate the cost?**\n",
        "\n",
        "Backtesting involves training a single BigQuery ML model for each period in the test set, so the cost is a function of the length of the test set after any downsampling done by the windowing strategy. The cost is also multiplied by the number of candidate models trained, which is determined by `max_order`.\n",
        "\n",
        "According to [BQ pricing](https://cloud.google.com/bigquery-ml/pricing), BigQuery ML model creation costs $250 per TB. You can use a max order of 3, which translates to 20 candidate models when there are multiple time series. The demo dataset is 3 MB in size, and includes 31 test periods. \n",
        "\n",
        "In this tutorial, the model create stage of the pipeline costs `3 MB * ($250 / 1024^2) * (31 / 1) periods * 20 candidates = $0.44`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t04OJzrORmJ4"
      },
      "source": [
        "## Create and run the training job\n",
        "To train a model using the ARIMA pipeline, you perform two steps: \n",
        "\n",
        "1. download the training pipeline from GCPC.\n",
        "1. run the job\n",
        "\n",
        "#### Create training job\n",
        "\n",
        "The training pipeline expects the following parameters:\n",
        "\n",
        "- `bigquery_destination_uri`: (optional) BigQuery Dataset URI. Used to export the metrics table and model. If not given, You can create one for the user.\n",
        "- `data_granularity_unit`: Enum used to specify the time granularity (hour, day, week, month, etc).\n",
        "- `data_source_csv_filenames` or `data_source_bigquery_table_path`: A URI for either a CSV stored in GCR or a BigQuery table, respectively.\n",
        "- `evaluated_examples_destination_uri`: (optional) BigQuery Dataset URI OR Table URI. Used to export the evaluated examples table. Uses bigquery_destination_uri, if not provided.\n",
        "- `forecast_horizon`: Integer number of periods to predict.\n",
        "- A data splitting strategy of either:\n",
        "  - `predefined_split_key`: A column containing `TRAIN`, `VALIDATE`, or `TEST` to denote the splits for each row.\n",
        "  -  `training_fraction`, `validation_fraction`, and `test_fraction` to set the fractions to split on chronologically on the time column.\n",
        "  - `timestamp_split_key` plus the fractions in the previous option to perform fractional splitting on a column other than the time column.\n",
        "- A windowing strategy of either:\n",
        "  - `window_column`: A boolean column decides whether or now each row gets considered when calculating the evaluation metrics.\n",
        "  - `window_stride_length`: Every N rows are used to compute the evaluation metrics.\n",
        "  - `window_max_count`: Downsample rows such that only the given number are used to calculate the evaluation metrics.\n",
        "- `target_column`: Name of target column.\n",
        "- `time_column`: Name of time column.\n",
        "- `time_series_identifier_column`: Name of id column.\n",
        "- `max_order`: Integer between 1 and 5 representing the size of the parameter search space for ARIMA_PLUS. 5 would result in the highest accuracy model, but also the longest training runtime/cost.\n",
        "\n",
        "For a full list of parameters, see the GCPC SDK [documentation](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-2.9.0/api/v1/automl/forecasting.html#v1.automl.forecasting.get_bqml_arima_train_pipeline_and_parameters).\n",
        "\n",
        "The execution of the training pipeline can take up to 20 minutes or more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYaWuqQF4mzZ"
      },
      "outputs": [],
      "source": [
        "time_column = \"date\"  # @param {type: \"string\"}\n",
        "time_series_identifier_column = \"id\"  # @param {type: \"string\"}\n",
        "target_column = \"sales\"  # @param {type: \"string\"}\n",
        "forecast_horizon = 30  # @param {type: \"integer\"}\n",
        "data_granularity_unit = \"day\"  # @param {type: \"string\"}\n",
        "split_column = \"split\"  # @param {type: \"string\"}\n",
        "window_stride_length = 1  # @param {type: \"integer\"}\n",
        "max_order = 3  # @param {type: \"integer\"}\n",
        "override_destination = True  # @param {type: \"boolean\"}\n",
        "\n",
        "(\n",
        "    train_job_spec_path,\n",
        "    train_parameter_values,\n",
        ") = utils.get_bqml_arima_train_pipeline_and_parameters(\n",
        "    project=PROJECT_ID,\n",
        "    location=LOCATION,\n",
        "    root_dir=os.path.join(BUCKET_URI, \"pipeline_root\"),\n",
        "    time_column=time_column,\n",
        "    time_series_identifier_column=time_series_identifier_column,\n",
        "    target_column=target_column,\n",
        "    forecast_horizon=forecast_horizon,\n",
        "    data_granularity_unit=data_granularity_unit,\n",
        "    predefined_split_key=split_column,\n",
        "    data_source_bigquery_table_path=TRAINING_DATASET_BQ_PATH,\n",
        "    window_stride_length=window_stride_length,\n",
        "    bigquery_destination_uri=arima_dataset_path,\n",
        "    override_destination=override_destination,\n",
        "    max_order=max_order,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6RcJ1RS4xmY"
      },
      "source": [
        "### Run the training pipeline\n",
        "\n",
        "Use the Vertex AI Python SDK to kick off a training pipeline run. Once the run has started, the following cell outputs a link that lets you monitor the run. The link should look like this: \n",
        "\n",
        "`https://console.cloud.google.com/vertex-ai/locations/[LOCATION]/pipelines/runs/[TRAIN_DISPLAY_NAME]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiwcDcmN44BG"
      },
      "outputs": [],
      "source": [
        "# The display name should be unique even if this cell is rerun.\n",
        "TRAIN_DISPLAY_NAME = f\"forecasting-demo-train-{str(uuid.uuid1())}\"\n",
        "\n",
        "job = aiplatform.PipelineJob(\n",
        "    job_id=TRAIN_DISPLAY_NAME,\n",
        "    display_name=TRAIN_DISPLAY_NAME,\n",
        "    pipeline_root=os.path.join(BUCKET_URI, TRAIN_DISPLAY_NAME),\n",
        "    template_path=train_job_spec_path,\n",
        "    parameter_values=train_parameter_values,\n",
        "    enable_caching=False,\n",
        ")\n",
        "job.run(service_account=SERVICE_ACCOUNT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5t9pqcw3XDs"
      },
      "source": [
        "## Review model evaluation scores\n",
        "After your model has finished training, you can review the evaluation scores for it.\n",
        "\n",
        "#### Metrics are always reported via the `metrics` table in the destination dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUFvUSax_Hz-"
      },
      "outputs": [],
      "source": [
        "for task_detail in job.gca_resource.job_detail.task_details:\n",
        "    if task_detail.task_name == \"create-metrics-artifact\":\n",
        "        metrics = task_detail.outputs[\"evaluation_metrics\"].artifacts[0].metadata\n",
        "        break\n",
        "else:\n",
        "    raise ValueError(\"Couldn't find the model evaluation task.\")\n",
        "\n",
        "print(\"Evaluation metrics:\\n\")\n",
        "dict(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsRVi6AoiUIE"
      },
      "source": [
        "You can view the predictions used to calculate the evaluation metrics if you want to calculate your own. \n",
        "\n",
        "#### View predictions used to calculate the evaluation metrics\n",
        "\n",
        "This table containing all these predictions is called `evaluated_examples`. In this table, each distinct `predicted_on_date` represents the starting period of a window of predictions. The backtesting metrics make use of all these windows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLjRzPQRqhbo"
      },
      "outputs": [],
      "source": [
        "query = f\"SELECT * FROM `{arima_dataset_path}.evaluated_examples`\"\n",
        "arima_examples = client.query(query).to_dataframe()\n",
        "arima_examples.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtoWftFI3ahi"
      },
      "source": [
        "## Create and run prediction job\n",
        "\n",
        "### Create prediction job\n",
        "Now that your Model resource is trained, you can make a batch prediction using the prediction pipeline, with the following parameters:\n",
        "\n",
        "- `bigquery_destination_uri`: (optional) BigQuery Dataset URI. Used to export the metrics table and model. If not given, You can create one for the user.\n",
        "- `data_source_csv_filenames` or `data_source_bigquery_table_path`: A URI for either a CSV stored in GCR or a BigQuery table, respectively.\n",
        "- `generate_explanation`: If True, the predictions table can have some extra explanations columns.\n",
        "- `model_name`: Name of an existing BigQuery ML ARIMA_PLUS model to use for predictions.\n",
        "\n",
        "For a full list of parameters, see the GCPC SDK [documentation](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-2.9.0/api/v1/automl/forecasting.html#v1.automl.forecasting.get_bqml_arima_predict_pipeline_and_parameters).\n",
        "\n",
        "The execution of the prediction pipeline can take up to 5 minutes or more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDO2aO8o_cwU"
      },
      "outputs": [],
      "source": [
        "# Get the model name programmatically, you can find this by looking at the execution graph in Vertex AI Pipelines.\n",
        "for task_detail in job.gca_resource.job_detail.task_details:\n",
        "    if task_detail.task_name == \"bigquery-create-model-job\":\n",
        "        model_name = task_detail.outputs[\"model\"].artifacts[0].metadata[\"modelId\"]\n",
        "        break\n",
        "else:\n",
        "    raise ValueError(\"Couldn't find the model training task.\")\n",
        "\n",
        "\n",
        "(\n",
        "    predict_job_spec_path,\n",
        "    predict_parameter_values,\n",
        ") = utils.get_bqml_arima_predict_pipeline_and_parameters(\n",
        "    project=PROJECT_ID,\n",
        "    location=LOCATION,\n",
        "    model_name=f\"{arima_dataset_path}.{model_name}\",\n",
        "    data_source_bigquery_table_path=PREDICTION_DATASET_BQ_PATH,\n",
        "    bigquery_destination_uri=arima_dataset_path,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXf2FyVs3fhW"
      },
      "source": [
        "### Run the prediction pipeline\n",
        "\n",
        "Use the Vertex AI Python SDK to kick off a prediction pipeline run. Once the run has started, the following cell outputs a link that lets you monitor the run. The link should look like this: \n",
        "\n",
        "`https://console.cloud.google.com/vertex-ai/locations/[LOCATION]/pipelines/runs/[PRED_DISPLAY_NAME]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhlOwoIQAKVx"
      },
      "outputs": [],
      "source": [
        "# The display name should be unique even if this cell is rerun.\n",
        "PRED_DISPLAY_NAME = f\"forecasting-demo-predict-{str(uuid.uuid1())}\"\n",
        "\n",
        "\n",
        "pred_job = aiplatform.PipelineJob(\n",
        "    job_id=PRED_DISPLAY_NAME,\n",
        "    display_name=PRED_DISPLAY_NAME,\n",
        "    pipeline_root=os.path.join(BUCKET_URI, PRED_DISPLAY_NAME),\n",
        "    template_path=predict_job_spec_path,\n",
        "    parameter_values=predict_parameter_values,\n",
        "    enable_caching=False,\n",
        ")\n",
        "pred_job.run(service_account=SERVICE_ACCOUNT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Vt4LAA2A4qo"
      },
      "source": [
        "### Get the predictions\n",
        "\n",
        "Next, get the results from the completed batch prediction job. These are always written to a table called `predictions` under the output dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6FpuMHsA8Sz"
      },
      "outputs": [],
      "source": [
        "# Get the prediction table programmatically, you can find this by looking at the execution graph in Vertex AI Pipelines.\n",
        "for task_detail in pred_job.gca_resource.job_detail.task_details:\n",
        "    if task_detail.task_name == \"bigquery-query-job\":\n",
        "        pred_table = (\n",
        "            task_detail.outputs[\"destination_table\"].artifacts[0].metadata[\"tableId\"]\n",
        "        )\n",
        "        break\n",
        "else:\n",
        "    raise ValueError(\"Couldn't find the prediction task.\")\n",
        "\n",
        "query = f\"SELECT * FROM `{arima_dataset_path}.{pred_table}`\"\n",
        "arima_preds = client.query(query).to_dataframe()\n",
        "arima_preds.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvczFPe0kcgX"
      },
      "source": [
        "## Visualize the forecasts\n",
        "\n",
        "Lastly, follow the given link to visualize the generated forecasts in [Data Studio](https://support.google.com/datastudio/answer/6283323?hl=en).\n",
        "The code block included in this section dynamically generates a Data Studio link that specifies the template, the location of the forecasts, and the query to generate the chart. The data is populated from the forecasts generated earlier.\n",
        "\n",
        "You can inspect the used template at https://datastudio.google.com/c/u/0/reporting/067f70d2-8cd6-4a4c-a099-292acd1053e8. This was created by Google specifically to view forecasting predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ibEs8z38Q9k"
      },
      "outputs": [],
      "source": [
        "def _sanitize_bq_uri(bq_uri: str):\n",
        "    if bq_uri.startswith(\"bq://\"):\n",
        "        bq_uri = bq_uri[5:]\n",
        "    return bq_uri.replace(\":\", \".\")\n",
        "\n",
        "\n",
        "def get_data_studio_link(\n",
        "    batch_prediction_bq_input_uri: str,\n",
        "    batch_prediction_bq_output_uri: str,\n",
        "    time_column: str,\n",
        "    time_series_identifier_column: str,\n",
        "    target_column: str,\n",
        "):\n",
        "    \"\"\"Creates a link that fills in the demo Data Studio template.\"\"\"\n",
        "    batch_prediction_bq_input_uri = _sanitize_bq_uri(batch_prediction_bq_input_uri)\n",
        "    batch_prediction_bq_output_uri = _sanitize_bq_uri(batch_prediction_bq_output_uri)\n",
        "    query = f\"\"\"\n",
        "        SELECT\n",
        "          CAST(input.{time_column} as DATETIME) timestamp_col,\n",
        "          CAST(input.{time_series_identifier_column} as STRING) time_series_identifier_col,\n",
        "          CAST(input.{target_column} as NUMERIC) historical_values,\n",
        "          CAST(predicted_{target_column}.value as NUMERIC) predicted_values,\n",
        "        FROM `{batch_prediction_bq_input_uri}` input\n",
        "        LEFT JOIN `{batch_prediction_bq_output_uri}` output\n",
        "          ON\n",
        "            TIMESTAMP(input.{time_column}) = TIMESTAMP(output.{time_column})\n",
        "            AND CAST(input.{time_series_identifier_column} as STRING) = CAST(\n",
        "              output.{time_series_identifier_column} as STRING)\n",
        "    \"\"\"\n",
        "    params = {\n",
        "        \"templateId\": \"067f70d2-8cd6-4a4c-a099-292acd1053e8\",\n",
        "        \"ds0.connector\": \"BIG_QUERY\",\n",
        "        \"ds0.projectId\": PROJECT_ID,\n",
        "        \"ds0.billingProjectId\": PROJECT_ID,\n",
        "        \"ds0.type\": \"CUSTOM_QUERY\",\n",
        "        \"ds0.sql\": query,\n",
        "    }\n",
        "    base_url = \"https://datastudio.google.com/c/u/0/reporting\"\n",
        "    url_params = urllib.parse.urlencode({\"params\": json.dumps(params)})\n",
        "    return f\"{base_url}?{url_params}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a91HWQmdByqo"
      },
      "outputs": [],
      "source": [
        "actuals_table = f\"{arima_dataset_path}.actuals\"\n",
        "query = f\"\"\"\n",
        "    CREATE OR REPLACE TABLE `{actuals_table}` AS\n",
        "    {base_data_query}\n",
        "    SELECT *\n",
        "    FROM base_data\n",
        "    WHERE split != 'TRAIN'\n",
        "\"\"\"\n",
        "client.query(query).result()\n",
        "print(f\"Created {actuals_table}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMDQSc5Fqgco"
      },
      "outputs": [],
      "source": [
        "print(\"Click the link below to view ARIMA predictions:\")\n",
        "print(\n",
        "    get_data_studio_link(\n",
        "        batch_prediction_bq_input_uri=actuals_table,\n",
        "        batch_prediction_bq_output_uri=f\"{arima_dataset_path}.{pred_table}\",\n",
        "        time_column=time_column,\n",
        "        time_series_identifier_column=time_series_identifier_column,\n",
        "        target_column=target_column,\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cleanup:mbsdk"
      },
      "source": [
        "## Clean up Vertex AI and BigQuery resources\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources that you created in this tutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJa9R2zyQEH-"
      },
      "outputs": [],
      "source": [
        "# Delete output datasets\n",
        "client.delete_dataset(arima_dataset_path, delete_contents=True, not_found_ok=True)\n",
        "\n",
        "job.delete()\n",
        "pred_job.delete()\n",
        "\n",
        "delete_bucket = True\n",
        "if delete_bucket:\n",
        "    ! gsutil -m rm -r $BUCKET_URI"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "bqml_arima_plus.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

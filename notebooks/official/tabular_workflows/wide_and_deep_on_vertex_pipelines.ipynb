{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18ebbd838e32"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mThXALJl9Yue"
   },
   "source": [
    "# Wide and Deep pipeline for tabular workflows in Vertex AI\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/tabular_workflows/wide_and_deep_on_vertex_pipelines.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/tabular_workflows/wide_and_deep_on_vertex_pipelines.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/tabular_workflows/wide_and_deep_on_vertex_pipelines.ipynb\">\n",
    "        <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcc745968395"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook showcases how to run the Wide & Deep algorithm using Vertex AI Tabular Workflows.This workflows includes configuring custom job for hyperparameters to use for training and hyperparameter tuning job to get the best set of hyperparameters for your dataset.Learn more about [Wide & Deep algorithm](https://cloud.google.com/ai-platform/training/docs/algorithms/wide-and-deep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f887ec5c06c5"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you create two classification models using Vertex AI Wide & Deep Tabular Workflows. Each workflow is a managed instance of [Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction).\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services and resources:\n",
    "\n",
    "- Vertex AI Training\n",
    "- Vertex AI Pipelines\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "   - Define Helper Functions : helper functions for getting gcs buket name and to write in to gcs etc. \n",
    "   - Configuring Dataset.\n",
    "   - Configure feature transformation : This transformations specified using Feature Transform Engine(FTE). \n",
    "   - Training Configuration:Configuring the training arguements with values.  \n",
    "   - VPC Configuration : Configuring subnetwork name and  defining any public IP addresses used.\n",
    "   - Configuring CustomJob : This is the best option if you know which hyperparameters to use for training.\n",
    "   - Configuring HyperparameterTuningJob : This allows you to get the best set of hyperparameters for your  dataset.\n",
    "\n",
    "After training, each pipeline returns a link to the Vertex Model UI. You can use the UI to deploy the model, get online predictions, or run batch prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eac26958afe8"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset you use is [Bank Marketing](https://archive.ics.uci.edu/ml/datasets/bank+marketing).\n",
    "The data is for direct marketing campaigns (phone calls) of a Portuguese banking institution. The binary classification goal is to predict if a client will subscribe a term deposit. For this notebook, we randomly selected 90% of the rows in the original dataset and saved them in a train.csv file hosted on a public Cloud Storage bucket. To download the file, Learn more about [bank-marketing-dataset](https://storage.googleapis.com/cloud-samples-data/vertex-ai/tabular-workflows/datasets/bank-marketing/train.csv)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "181d4dfbf917"
   },
   "source": [
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing) and [Dataflow](https://cloud.google.com/dataflow/pricing) and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuOonx6suOb7"
   },
   "source": [
    "### Set up your local development environment\n",
    "\n",
    "**If you are using Colab or Vertex AI Workbench Notebooks**, your environment already meets\n",
    "all the requirements to run this notebook. You can skip this step.\n",
    "\n",
    "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
    "You need the following:\n",
    "\n",
    "* The Google Cloud SDK\n",
    "\n",
    "The Google Cloud guide to [Setting up a Python development\n",
    "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
    "installation guide](https://jupyter.org/install) provide detailed instructions\n",
    "for meeting these requirements. The following steps provide a condensed set of\n",
    "instructions:\n",
    "\n",
    "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "1. [Install\n",
    "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
    "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
    "\n",
    "1. To install Jupyter, run `pip3 install jupyter` on the\n",
    "command-line in a terminal shell.\n",
    "\n",
    "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
    "\n",
    "1. Open this notebook in the Jupyter Notebook Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## Installation\n",
    "\n",
    "Install the following packages required to execute this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform \\\n",
    "                         google-cloud-pipeline-components {USER_FLAG} -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bj5O0S5RTxzY"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "023DMKUaTypt"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfEglUHQk9S3"
   },
   "source": [
    "## Before you begin\n",
    "\n",
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "2. [Make sure that billing is enabled for your project.](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
    "\n",
    "3. [Enable the following APIs: Vertex AI, Dataflow,Compute Engine,Cloud Storage and APIs](https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,dataflow.googleapis.com,compute_component,storage-component.googleapis.com)\n",
    "\n",
    "4. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    "\n",
    "5. Enter your project ID in the cell below. Then run the  cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zebLBGXOky2A"
   },
   "source": [
    "#### Notes about service account and permission\n",
    "\n",
    "**By default no configuration is required**, if you run into any permission related issue, please make sure the service accounts above have the required roles:\n",
    "\n",
    "|Service account email|Description|Roles|\n",
    "|---|---|---|\n",
    "|PROJECT_NUMBER-compute@developer.gserviceaccount.com|Compute Engine default service account|Dataflow Admin, Dataflow Worker, Storage Admin, BigQuery Admin, Vertex AI User|\n",
    "|service-PROJECT_NUMBER@gcp-sa-aiplatform.iam.gserviceaccount.com|AI Platform Service Agent|Vertex AI Service Agent|\n",
    "\n",
    "\n",
    "1. Go to [Cloud IAM console](https://console.cloud.google.com/iam-admin/iam.).\n",
    "2. Check the \"Include Google-provided role grants\" checkbox.\n",
    "3. Find the above emails.\n",
    "4. Grant the corresponding roles.\n",
    "\n",
    "#### Using data source from a different project\n",
    "- For the BQ data source, grant both the service accounts(listed above) with the **BigQuery Data Viewer** role.\n",
    "- For the CSV data source, grant both the service accounts(listed above) with the **Storage Object Viewer** role."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95cb7ffd6895"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cd85f5c794e5"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5a604eeffc32"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "30e64c0eda41"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b12f508d97c6"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex AI. It is recommended that you choose the region closest to you.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
    "\n",
    "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8e8b7997de7a"
   },
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "070f83c35863"
   },
   "source": [
    "#### UUID\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a uuid for each instance session, and append it onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e87d5856317d"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eu0e2TRVxjHb"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "**If you are using Vertex AI Workbench Notebooks**, your environment is already\n",
    "authenticated. \n",
    "\n",
    "**If you are using Colab**, run the cell below and follow the instructions\n",
    "when prompted to authenticate your account via oAuth.\n",
    "\n",
    "**Otherwise**, follow these steps:\n",
    "\n",
    "1. In the Cloud Console, go to the [**Create service account key**\n",
    "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
    "\n",
    "2. Click **Create service account**.\n",
    "\n",
    "3. In the **Service account name** field, enter a name, and\n",
    "   click **Create**.\n",
    "\n",
    "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
    "into the filter box, and select\n",
    "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
    "\n",
    "5. Click *Create*. A JSON file that contains your key downloads to your\n",
    "local environment.\n",
    "\n",
    "6. Enter the path to your service account key as the\n",
    "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_nJ1VqdTxosw"
   },
   "outputs": [],
   "source": [
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If on Vertex AI Workbench, then don't execute this code\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
    "    \"DL_ANACONDA_HOME\"\n",
    "):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS '[your-service-account-key-path]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUfwWir9yPNV"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "When you submit a training job using the Vertex AI SDK, you upload a Python package\n",
    "containing your training code to a Cloud Storage bucket. Vertex AI runs\n",
    "the code from this package. In this tutorial, Vertex AI also saves the\n",
    "trained model that results from your job in the same bucket. Using this model artifact, you can then\n",
    "create Vertex AI model and endpoint resources in order to serve\n",
    "online predictions.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
    "Cloud Storage buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5325f437b46a"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a30ea8551126"
   },
   "source": [
    "Create the bucket if it doesn't already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0217c63ed87f"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
    "    BUCKET_NAME = PROJECT_ID + \"aip-\" + UUID\n",
    "    BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "\n",
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4cf2cdebb50"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96ad3d416327"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4b9febbaebe6"
   },
   "source": [
    "#### Service Account\n",
    "\n",
    "If you do not want to use your project's Compute Engine service account, set `SERVICE_ACCOUNT` to another service account ID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9eb734b2dc2a"
   },
   "outputs": [],
   "source": [
    "IS_COLAB = False\n",
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b219715427fe"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your service account from gcloud\n",
    "    if not IS_COLAB:\n",
    "        shell_output = !gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "    else:  # IS_COLAB:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b2d479f5c9a"
   },
   "source": [
    "#### Set service account access for Vertex AI Pipelines\n",
    "\n",
    "Run the following commands to grant your service account access to in the bucket that you created in the previous step. You only need to run this step once per service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f88cb0488c08"
   },
   "outputs": [],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbbc3479a1da"
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8G6YmJT1yqkV"
   },
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import json\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from google.cloud import aiplatform, storage\n",
    "from google_cloud_pipeline_components.experimental.automl.tabular import \\\n",
    "    utils as automl_tabular_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0423f260423"
   },
   "source": [
    "### Initialize Vertex SDK for Python\n",
    "\n",
    "Initialize the Vertex SDK for Python for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ad69f2590268"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3LWH3PRF5o2v"
   },
   "source": [
    "### Define helper functions\n",
    "\n",
    "Defining helper functions for getting gcs buket name and to write in to gcs etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g9FPFT8c5oC0"
   },
   "outputs": [],
   "source": [
    "def get_model_artifacts_path(task_details: List[Dict[str, Any]], task_name: str) -> str:\n",
    "    task = get_task_detail(task_details, task_name)\n",
    "    return task.outputs[\"unmanaged_container_model\"].artifacts[0].uri\n",
    "\n",
    "\n",
    "def get_model_uri(task_details: List[Dict[str, Any]]) -> str:\n",
    "    task = get_task_detail(task_details, \"model-upload\")\n",
    "    # in format https://<location>-aiplatform.googleapis.com/v1/projects/<project_number>/locations/<location>/models/<model_id>\n",
    "    model_id = task.outputs[\"model\"].artifacts[0].uri.split(\"/\")[-1]\n",
    "    return f\"https://console.cloud.google.com/vertex-ai/locations/{REGION}/models/{model_id}?project={PROJECT_ID}\"\n",
    "\n",
    "\n",
    "def get_bucket_name_and_path(uri: str) -> str:\n",
    "    no_prefix_uri = uri[len(\"gs://\") :]\n",
    "    splits = no_prefix_uri.split(\"/\")\n",
    "    return splits[0], \"/\".join(splits[1:])\n",
    "\n",
    "\n",
    "def download_from_gcs(uri: str) -> str:\n",
    "    bucket_name, path = get_bucket_name_and_path(uri)\n",
    "    storage_client = storage.Client(project=PROJECT_ID)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(path)\n",
    "    return blob.download_as_string()\n",
    "\n",
    "\n",
    "def write_to_gcs(uri: str, content: str):\n",
    "    bucket_name, path = get_bucket_name_and_path(uri)\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(path)\n",
    "    blob.upload_from_string(content)\n",
    "\n",
    "\n",
    "def get_task_detail(\n",
    "    task_details: List[Dict[str, Any]], task_name: str\n",
    ") -> List[Dict[str, Any]]:\n",
    "    for task_detail in task_details:\n",
    "        if task_detail.task_name == task_name:\n",
    "            return task_detail\n",
    "\n",
    "\n",
    "def get_model_name(job_id: str) -> str:\n",
    "    pipeline_task_details = aiplatform.PipelineJob.get(\n",
    "        job_id\n",
    "    ).gca_resource.job_detail.task_details\n",
    "    upload_task_details = get_task_detail(pipeline_task_details, \"model-upload\")\n",
    "    return upload_task_details.outputs[\"model\"].artifacts[0].metadata[\"resourceName\"]\n",
    "\n",
    "\n",
    "def get_evaluation_metrics(\n",
    "    task_details: List[Dict[str, Any]],\n",
    ") -> str:\n",
    "    ensemble_task = get_task_detail(task_details, \"model-evaluation\")\n",
    "    return download_from_gcs(\n",
    "        ensemble_task.outputs[\"evaluation_metrics\"].artifacts[0].uri\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a7332a3f8e2"
   },
   "source": [
    "### Define training specification and configure dataset\n",
    "\n",
    "You define either of the following parameters:\n",
    "\n",
    "- `data_source_csv_filenames`: The CSV data source.\n",
    "- `data_source_bigquery_table_path`: The BigQuery data source.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b6dd1af1d336"
   },
   "outputs": [],
   "source": [
    "data_source_csv_filenames = \"gs://cloud-samples-data/vertex-ai/tabular-workflows/datasets/bank-marketing/train.csv\"\n",
    "data_source_bigquery_table_path = None  # @param {type:\"string\"},"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bf417de96807"
   },
   "source": [
    "### Configure feature transformation\n",
    "\n",
    "Transformations can be specified using Feature Transform Engine (FTE) specific configurations. Below, you configure full auto transformations (i.e., `auto_transform_config`). FTE automatically configures a set of built-in transformations for each input column based on its data statistics. \n",
    "\n",
    "Learn more about [feature transformation configs and examples](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.15/google_cloud_pipeline_components.experimental.automl.tabular.html#google_cloud_pipeline_components.experimental.automl.tabular.FeatureTransformEngineOp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fce334e09df6"
   },
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"age\",\n",
    "    \"job\",\n",
    "    \"marital\",\n",
    "    \"education\",\n",
    "    \"default\",\n",
    "    \"balance\",\n",
    "    \"housing\",\n",
    "    \"loan\",\n",
    "    \"contact\",\n",
    "    \"day\",\n",
    "    \"month\",\n",
    "    \"duration\",\n",
    "    \"campaign\",\n",
    "    \"pdays\",\n",
    "    \"previous\",\n",
    "    \"poutcome\",\n",
    "]\n",
    "\n",
    "auto_transform_config = {\"auto_transforms\": features}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4b28b609b259"
   },
   "source": [
    "### Set up training configuration\n",
    "\n",
    "Plese find the description of arguments which you can define in below section.\n",
    "\n",
    "- `target_column`: The target column name.\n",
    "- `prediction_type`: The type of prediction the model is to produce.\n",
    "  'classification' or 'regression'.\n",
    "- `transform_config`: The path to a GCS file containing the transformations to apply.\n",
    "- `predefined_split_key`: The predefined_split column name.\n",
    "- `timestamp_split_key`: The timestamp_split column name.\n",
    "- `stratified_split_key`: The stratified_split column name.\n",
    "- `training_fraction`: The training fraction.\n",
    "- `validation_fraction`: The validation fraction.\n",
    "- `test_fraction`: The test fraction.\n",
    "- `weight_column`: The weight column name.\n",
    "- `run_evaluation`: Whether to run evaluation steps during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eV4JrwB8wAkg"
   },
   "outputs": [],
   "source": [
    "run_evaluation = True  # @param {type:\"boolean\"}\n",
    "prediction_type = \"classification\"\n",
    "target_column = \"deposit\"\n",
    "\n",
    "# Fraction split\n",
    "training_fraction = 0.8\n",
    "validation_fraction = 0.1\n",
    "test_fraction = 0.1\n",
    "\n",
    "timestamp_split_key = None  # timestamp column name when using timestamp split\n",
    "stratified_split_key = None  # target column name when using stratified split\n",
    "training_fraction = 0.8\n",
    "validation_fraction = 0.1\n",
    "test_fraction = 0.1\n",
    "\n",
    "predefined_split_key = None\n",
    "if predefined_split_key:\n",
    "    training_fraction = None\n",
    "    validation_fraction = None\n",
    "    test_fraction = None\n",
    "\n",
    "weight_column = None\n",
    "\n",
    "transform_config = auto_transform_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyWGg2s09xOk"
   },
   "source": [
    "### VPC related configuration\n",
    "\n",
    "Plese find the description of arguments which you can define in below section.\n",
    "\n",
    "- `dataflow_subnetwork`: Dataflow's fully qualified subnetwork name, when empty the default subnetwork is used. Look at a [sample network and subnetwork specifications](https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications)\n",
    "- `dataflow_use_public_ips`: Specifies whether Dataflow workers use public IP\n",
    "  addresses.\n",
    "\n",
    "If you need to use a custom Dataflow subnetwork, you can set it through the `dataflow_subnetwork` parameter. The requirements are:\n",
    "1. `dataflow_subnetwork` must be fully qualified subnetwork name.\n",
    "   Learn more about [subnetwork](https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications)\n",
    "1. The following service accounts must have [Compute Network User role](https://cloud.google.com/compute/docs/access/iam#compute.networkUser) assigned on the specified dataflow subnetwork. \n",
    " Compute Engine default service account: `PROJECT_NUMBER-compute@developer.gserviceaccount.com` and `service-PROJECT_NUMBER@dataflow-service-producer-prod.iam.gserviceaccount.com`.\n",
    "\n",
    "Learn more about [specifying-networks](https://cloud.google.com/dataflow/docs/guides/specifying-networks#shared).\n",
    "\n",
    "If your project has VPC-SC enabled, please make sure:\n",
    "\n",
    "1. The dataflow subnetwork used in VPC-SC is configured properly for Dataflow.\n",
    "   Learn more about [subnetwork](https://cloud.google.com/dataflow/docs/guides/routes-firewall)\n",
    "1. `dataflow_use_public_ips` is set to False.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_TePNlLl9v1q"
   },
   "outputs": [],
   "source": [
    "dataflow_subnetwork = \"\"  # @param {type:\"string\"}\n",
    "dataflow_use_public_ips = True  # @param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-iXXE14voyR"
   },
   "source": [
    "## Configure custom job and create a pipeline\n",
    "\n",
    "This step is perfect when you know exactly which hyperparameter values to use for model training. It uses fewer training resources than a HyperparameterTuningJob. \n",
    "\n",
    "In the example below, you configure the following:\n",
    "\n",
    "- `root_dir`: The root GCS directory for the pipeline components.\n",
    "- `worker_pool_specs_override`: The dictionary for overriding training and evaluation worker pool specs. The dictionary should be of [this format]( https://github.com/googleapis/googleapis/blob/4e836c7c257e3e20b1de14d470993a2b1f4736a8/google/cloud/aiplatform/v1beta1/custom_job.proto#L172).\n",
    "- `learning_rate`: The learning rate used by the linear optimizer.\n",
    "- `dnn_learning_rate`: The learning rate for training the deep part of the\n",
    "model.\n",
    "- `max_steps`: Number of steps to run the trainer for.\n",
    "- `max_train_secs`: Amount of time in seconds to run the trainer for.\n",
    "\n",
    "Learn more about the [pipeline inputs and model hyperparameters](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.23/google_cloud_pipeline_components.experimental.automl.tabular.html#google_cloud_pipeline_components.experimental.automl.tabular.utils.get_tabnet_trainer_pipeline_and_parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sG46cXVueb66"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pipeline_job_root_dir = os.path.join(BUCKET_URI, \"wide_and_deep_custom_job\")\n",
    "\n",
    "# max_steps and/or max_train_secs must be set. If both are\n",
    "# specified, training will stop after either condition is met.\n",
    "# By default, max_train_secs is set to -1.\n",
    "\n",
    "max_steps = 1000\n",
    "max_train_secs = -1\n",
    "\n",
    "learning_rate = 0.01\n",
    "dnn_learning_rate = 0.01\n",
    "\n",
    "transform_config_path = os.path.join(pipeline_job_root_dir, \"transform_config.json\")\n",
    "write_to_gcs(transform_config_path, json.dumps(transform_config))\n",
    "\n",
    "worker_pool_specs_override = [\n",
    "    {\"machine_spec\": {\"machine_type\": \"c2-standard-16\"}}  # Override for TF chief node\n",
    "]\n",
    "\n",
    "\n",
    "# If your system does not use Python, you can save the JSON file (`template_path`),\n",
    "# and use another programming language to submit the pipeline.\n",
    "(\n",
    "    template_path,\n",
    "    parameter_values,\n",
    ") = automl_tabular_utils.get_wide_and_deep_trainer_pipeline_and_parameters(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    root_dir=pipeline_job_root_dir,\n",
    "    max_steps=max_steps,\n",
    "    max_train_secs=max_train_secs,\n",
    "    learning_rate=learning_rate,\n",
    "    dnn_learning_rate=dnn_learning_rate,\n",
    "    target_column=target_column,\n",
    "    prediction_type=prediction_type,\n",
    "    transform_config=transform_config_path,\n",
    "    training_fraction=training_fraction,\n",
    "    validation_fraction=validation_fraction,\n",
    "    test_fraction=test_fraction,\n",
    "    data_source_csv_filenames=data_source_csv_filenames,\n",
    "    data_source_bigquery_table_path=data_source_bigquery_table_path,\n",
    "    worker_pool_specs_override=worker_pool_specs_override,\n",
    "    dataflow_use_public_ips=dataflow_use_public_ips,\n",
    "    dataflow_subnetwork=dataflow_subnetwork,\n",
    "    run_evaluation=run_evaluation,\n",
    ")\n",
    "\n",
    "pipeline_job_id = f\"wide-and-deep-{UUID}\"\n",
    "\n",
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=pipeline_job_id,\n",
    "    template_path=template_path,\n",
    "    job_id=pipeline_job_id,\n",
    "    pipeline_root=pipeline_job_root_dir,\n",
    "    parameter_values=parameter_values,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "pipeline_job.run(service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbf2cf98842b"
   },
   "source": [
    "### Go to the Vertex AI console\n",
    "From the link generated from below cell, you can deploy the model and test online prediction or run batch prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "719a784573ce"
   },
   "outputs": [],
   "source": [
    "wide_and_deep_trainer_pipeline_task_details = aiplatform.PipelineJob.get(\n",
    "    pipeline_job_id\n",
    ").gca_resource.job_detail.task_details\n",
    "CUSTOM_JOB_MODEL = get_model_name(pipeline_job_id)\n",
    "print(\"model uri:\", get_model_uri(wide_and_deep_trainer_pipeline_task_details))\n",
    "print(\n",
    "    \"model artifacts:\",\n",
    "    get_model_artifacts_path(\n",
    "        wide_and_deep_trainer_pipeline_task_details, \"wide-and-deep-trainer\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sF8a2RKtRhg"
   },
   "source": [
    "## Configure hyperparameter tuning job and create a pipeline\n",
    "\n",
    "To get the best set of hyperparameters for your dataset, you can run HyperparameterTuningJob.\n",
    "\n",
    "Hyperparameters that can be tuned are set in the optional `study_spec_parameters_override` parameter. you provide a helper function called `get_wide_and_deep_study_spec_parameters_override` to get these hyperparameters. The function returns a list of hyperparameters and ranges. `study_spec_parameters_override` can be empty or one or more of these hyperparameters can be specified. For hyperparameters not specified in `study_spec_parameters_override`, you set ranges in the pipeline. For a full list of hyperparameters available for tuning, Learn more about [hyperparameters](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.23/google_cloud_pipeline_components.experimental.automl.tabular.html#google_cloud_pipeline_components.experimental.automl.tabular.utils.get_wide_and_deep_trainer_pipeline_and_parameters).\n",
    "\n",
    "In addition to hyperparameters, Vertex AI's `HyperparameterTuningJob` takes the following parameters:\n",
    "\n",
    "- `root_dir`: The root GCS directory for the pipeline components.\n",
    "- `worker_pool_specs_override`: The dictionary for overriding training and evaluation worker pool specs. The dictionary should be of [this format]( https://github.com/googleapis/googleapis/blob/4e836c7c257e3e20b1de14d470993a2b1f4736a8/google/cloud/aiplatform/v1beta1/custom_job.proto#L172).\n",
    "- `study_spec_metric_id`: Metric to optimize, possible values: ['loss', 'average_loss', 'rmse', 'mae', 'mql', 'accuracy', 'auc', 'precision', 'recall'].\n",
    "- `study_spec_metric_goal`: Optimization goal of the metric, possible values: \"MAXIMIZE\", \"MINIMIZE\".\n",
    "- `max_trial_count`: The desired total number of trials.\n",
    "- `parallel_trial_count`: The desired number of trials to run in parallel.\n",
    "- `max_failed_trial_count`: The number of failed trials that need to be seen before failing the HyperparameterTuningJob. If set to 0, Vertex AI decides how many trials must fail before the whole job fails.\n",
    "- `study_spec_algorithm`: The search algorithm specified for the study. One of\n",
    "'ALGORITHM_UNSPECIFIED', 'GRID_SEARCH', or 'RANDOM_SEARCH'.\n",
    "\n",
    "Learn more about the [hyperparameters available for tuning](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.23/google_cloud_pipeline_components.experimental.automl.tabular.html#google_cloud_pipeline_components.experimental.automl.tabular.utils.get_wide_and_deep_hyperparameter_tuning_job_pipeline_and_parameters).\n",
    "\n",
    "Multiple trials can be configured. The pipeline returns the best trial based on the metric configured in `study_spec_metrics`. In the example below, you return the trial with the lowest loss value. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8hlPps2Rtpq-"
   },
   "outputs": [],
   "source": [
    "pipeline_job_root_dir = os.path.join(\n",
    "    BUCKET_URI, \"wide_and_deep_hyperparameter_tuning_job\"\n",
    ")\n",
    "\n",
    "worker_pool_specs_override = [\n",
    "    {\"machine_spec\": {\"machine_type\": \"c2-standard-16\"}}  # Override for TF chief node\n",
    "]\n",
    "\n",
    "\n",
    "study_spec_metric_id = \"loss\"\n",
    "study_spec_metric_goal = \"MINIMIZE\"\n",
    "\n",
    "# max_steps and/or max_train_secs must be set. If both are\n",
    "# specified, training will stop after either condition is met.\n",
    "# By default, max_train_secs is set to -1 and max_steps is set to\n",
    "# an appropriate range given dataset_size and training budget.\n",
    "study_spec_parameters_override = (\n",
    "    automl_tabular_utils.get_wide_and_deep_study_spec_parameters_override()\n",
    ")\n",
    "\n",
    "# If your system does not use Python, you can save the JSON file (`template_path`),\n",
    "# and use another programming language to submit the pipeline.\n",
    "(\n",
    "    template_path,\n",
    "    parameter_values,\n",
    ") = automl_tabular_utils.get_wide_and_deep_hyperparameter_tuning_job_pipeline_and_parameters(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    root_dir=pipeline_job_root_dir,\n",
    "    target_column=target_column,\n",
    "    prediction_type=prediction_type,\n",
    "    transform_config=transform_config_path,\n",
    "    training_fraction=training_fraction,\n",
    "    validation_fraction=validation_fraction,\n",
    "    test_fraction=test_fraction,\n",
    "    data_source_csv_filenames=data_source_csv_filenames,\n",
    "    data_source_bigquery_table_path=data_source_bigquery_table_path,\n",
    "    study_spec_metric_id=study_spec_metric_id,\n",
    "    study_spec_metric_goal=study_spec_metric_goal,\n",
    "    study_spec_parameters_override=study_spec_parameters_override,\n",
    "    max_trial_count=1,\n",
    "    parallel_trial_count=1,\n",
    "    max_failed_trial_count=0,\n",
    "    worker_pool_specs_override=worker_pool_specs_override,\n",
    "    dataflow_use_public_ips=dataflow_use_public_ips,\n",
    "    dataflow_subnetwork=dataflow_subnetwork,\n",
    "    run_evaluation=True,\n",
    ")\n",
    "\n",
    "pipeline_job_id = f\"wide-and-deep-hpt-{UUID}\"\n",
    "\n",
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=pipeline_job_id,\n",
    "    template_path=template_path,\n",
    "    job_id=pipeline_job_id,\n",
    "    pipeline_root=pipeline_job_root_dir,\n",
    "    parameter_values=parameter_values,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "pipeline_job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2749d8cec287"
   },
   "source": [
    "### Go to the Vertex AI console\n",
    "From the link below, you can deploy the model and test online prediction or run batch prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "730af2836871"
   },
   "outputs": [],
   "source": [
    "wide_and_deep_hpt_pipeline_task_details = aiplatform.PipelineJob.get(\n",
    "    pipeline_job_id\n",
    ").gca_resource.job_detail.task_details\n",
    "HPT_JOB_MODEL = get_model_name(pipeline_job_id)\n",
    "print(\"model uri:\", get_model_uri(wide_and_deep_hpt_pipeline_task_details))\n",
    "print(\n",
    "    \"model artifacts:\",\n",
    "    get_model_artifacts_path(\n",
    "        wide_and_deep_hpt_pipeline_task_details,\n",
    "        \"get-best-hyperparameter-tuning-job-trial\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43342a43176e"
   },
   "source": [
    "## Clean up Vertex AI and BigQuery resources\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n",
    "- Cloud Storage Bucket [Set `delete_bucket` to **True** to delete your bucket]\n",
    "- Model from CustomJob pipeline\n",
    "- Model from HyperparameterTuningJob pipeline\n",
    "- Vertex AI Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ad8d12061a65"
   },
   "outputs": [],
   "source": [
    "# Delete model resources\n",
    "custom_job_model = aiplatform.Model(CUSTOM_JOB_MODEL)\n",
    "hpt_job_model = aiplatform.Model(HPT_JOB_MODEL)\n",
    "custom_job_model.delete()\n",
    "hpt_job_model.delete()\n",
    "\n",
    "# Delete pieline job\n",
    "pipeline_job.delete()\n",
    "\n",
    "# Delete bucket\n",
    "delete_bucket = False\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "wide_and_deep_on_vertex_pipelines.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

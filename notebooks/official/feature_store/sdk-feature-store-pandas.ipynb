{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Using Vertex AI Feature Store with Pandas Dataframe\n",
        "\n",
        "<table align=\"left\">\n",
        "    <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/feature_store/sdk-feature-store-pandas.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "    \n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/feature_store/sdk-feature-store-pandas.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> \n",
        "        Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/feature_store/sdk-feature-store-pandas.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook introduces Pandas support for Feature Store using Vertex AI SDK. For pre-requisites and introduction on Vertex AI SDK and Feature Store native support, please go through this [Colab notebook](https://colab.sandbox.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/feature_store/sdk-feature-store.ipynb). \n",
        "\n",
        "Learn more about [Vertex AI Feature Store](https://cloud.google.com/vertex-ai/docs/featurestore)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxF5JWRVT5PP"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this notebook, you learn how to use `Vertex AI Feature Store` with pandas Dataframe.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services and resources:\n",
        "\n",
        "- Vertex AI Feature Store\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Ingest Feature values from Pandas DataFrame into Feature Store's Entity types.\n",
        "- Read Entity feature values from Online Feature Store into Pandas DataFrame.\n",
        "- Batch serve feature values from your Feature Store into Pandas DataFrame.\n",
        "\n",
        "You also learn how Vertex AI Feature Store can be useful in the below scenarios:\n",
        "\n",
        "- Online serving with updated feature values.\n",
        "- Point-in-time correctness to fetch feature values for training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4ZNLaf6T0lN"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "This tutorial is a part of the Feature Store tutorial notebooks. It uses a movie recommendation dataset as an example for demonstrating various functionalities of Feature Store. The original task is to train a model to predict if a user is going to watch a movie, and serve the model online."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W2Bj_QpT2Ud"
      },
      "source": [
        "### Costs \n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWEdiXsJg0XY"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7EUnXsZhAGF"
      },
      "source": [
        "### Install additional packages\n",
        "\n",
        "To run this notebook, you need to install the following packages for Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b4ef9b72d43"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Vertex AI Workbench Notebook product has specific requirements\n",
        "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
        "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
        "    \"/opt/deeplearning/metadata/env_version\"\n",
        ")\n",
        "\n",
        "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_WORKBENCH_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\"\n",
        "    \n",
        "! pip install -U {USER_FLAG} --upgrade google-cloud-aiplatform \\\n",
        "                                        google-cloud-bigquery \\\n",
        "                                        google-cloud-bigquery-storage \\\n",
        "                                        avro \\\n",
        "                                        pyarrow \\\n",
        "                                        pandas -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhq5zEbGg0XX"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "After you install the packages, you need to restart the notebook kernel so that it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzrelQZ22IZj"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF1j6f9HApxa"
      },
      "source": [
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "1. [Enable the Vertex AI API and Compute Engine API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component).\n",
        "\n",
        "1. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
        "\n",
        "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WReHDGG5g0XY"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcdfccf50581"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM1iC_MfAts1"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID:\", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09021c90b34c"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f41eda68c379"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook.  Below are regions supported for Vertex AI. It is recommended that you choose the region closest to you.\n",
        "\n",
        "- Americas: `us-central1`\n",
        "- Europe: `europe-west4`\n",
        "- Asia Pacific: `asia-east1`\n",
        "\n",
        "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
        "\n",
        "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c615e53149f"
      },
      "outputs": [],
      "source": [
        "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
        "\n",
        "if REGION == \"[your-region]\":\n",
        "    REGION = \"us-central1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr--iN2kAylZ"
      },
      "source": [
        "#### UUID\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a uuid for each instance session, and append it onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4e166d927e36"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "\n",
        "# Generate a uuid of a specifed length(default=8)\n",
        "def generate_uuid(length: int = 8) -> str:\n",
        "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
        "\n",
        "\n",
        "UUID = generate_uuid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBCra4QMA2wR"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using Vertex AI Workbench Notebooks**, your environment is already\n",
        "authenticated. \n",
        "\n",
        "**If you are using Colab**, run the cell below and follow the instructions\n",
        "when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "1. In the Cloud Console, go to the [**Create service account key**\n",
        "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
        "\n",
        "2. Click **Create service account**.\n",
        "\n",
        "3. In the **Service account name** field, enter a name, and\n",
        "   click **Create**.\n",
        "\n",
        "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
        "into the filter box, and select\n",
        "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "\n",
        "5. Click *Create*. A JSON file that contains your key downloads to your\n",
        "local environment.\n",
        "\n",
        "6. Enter the path to your service account key as the\n",
        "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "outputs": [],
      "source": [
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# If on Vertex AI Workbench, then don't execute this code\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
        "    \"DL_ANACONDA_HOME\"\n",
        "):\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS '[your-service-account-key-path]'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cdct_Lm7x2I_"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "import pandas as pd\n",
        "from avro.datafile import DataFileReader\n",
        "from avro.io import DatumReader\n",
        "from google.cloud import aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "138407556b22"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project and region."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d2077ffee78"
      },
      "outputs": [],
      "source": [
        "aiplatform.init(project=PROJECT_ID, location=REGION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buQBIv3ZL3A0"
      },
      "source": [
        "## Create a Feature Store\n",
        "\n",
        "The method to create a Feature Store in Vertex AI returns a\n",
        "[long-running operation](https://google.aip.dev/151) (LRO). An LRO starts an asynchronous job. LROs are returned for other API methods too, such as updating or deleting a featurestore. \n",
        "\n",
        "Running the code cell below creates a featurestore and prints the process' logs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6uIWQeoBSr8"
      },
      "outputs": [],
      "source": [
        "# Create featurestore\n",
        "movie_predictions_feature_store = aiplatform.Featurestore.create(\n",
        "    featurestore_id=f\"movie_predictions_{UUID}\", online_store_fixed_node_count=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpmJq75zXjmT"
      },
      "source": [
        "## Create Entity types\n",
        "\n",
        "Entity types can be created within the Featurestore class. Below, you create the `Users` entity type and `Movies` entity type. Process logs are printed in the output for each cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GU0oXvINBgPV"
      },
      "outputs": [],
      "source": [
        "# Create users entity type\n",
        "users_entity_type = movie_predictions_feature_store.create_entity_type(\n",
        "    entity_type_id=\"users\",\n",
        "    description=\"Users entity\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPCGFznrFwFy"
      },
      "outputs": [],
      "source": [
        "# Create movies entity type\n",
        "movies_entity_type = movie_predictions_feature_store.create_entity_type(\n",
        "    entity_type_id=\"movies\",\n",
        "    description=\"Movies entity\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJW4q-0jO2Xf"
      },
      "source": [
        "## Create Features\n",
        "Features can be created within each entity type. Add defined features to the `Users` entity type and `Movies` entity type by using the following methods.\n",
        "\n",
        "### Add features using *create_feature* method\n",
        "Provide the feature information like id, type and description to the `create_feature` method of entity type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvjwT84iVSps"
      },
      "outputs": [],
      "source": [
        "# Create age feature\n",
        "users_feature_age = users_entity_type.create_feature(\n",
        "    feature_id=\"age\",\n",
        "    value_type=\"INT64\",\n",
        "    description=\"User age\",\n",
        ")\n",
        "\n",
        "# Create gender feature\n",
        "users_feature_gender = users_entity_type.create_feature(\n",
        "    feature_id=\"gender\",\n",
        "    value_type=\"STRING\",\n",
        "    description=\"User gender\",\n",
        ")\n",
        "\n",
        "# Create liked_genres feature\n",
        "users_feature_liked_genres = users_entity_type.create_feature(\n",
        "    feature_id=\"liked_genres\",\n",
        "    value_type=\"STRING_ARRAY\",\n",
        "    description=\"An array of genres this user liked\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecb141839033"
      },
      "source": [
        "### Add features using batch method\n",
        "You can also create features using a config map in a dictionary format and the `batch_create_features` method. This way, you can add multiple features at once. \n",
        "\n",
        "Below, you define and create *title*, *genres* and *average_rating* features using the batch method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llTT9_Dgbac2"
      },
      "outputs": [],
      "source": [
        "movies_feature_configs = {\n",
        "    \"title\": {\n",
        "        \"value_type\": \"STRING\",\n",
        "        \"description\": \"The title of the movie\",\n",
        "    },\n",
        "    \"genres\": {\n",
        "        \"value_type\": \"STRING\",\n",
        "        \"description\": \"The genre of the movie\",\n",
        "    },\n",
        "    \"average_rating\": {\n",
        "        \"value_type\": \"DOUBLE\",\n",
        "        \"description\": \"The average rating for the movie, range is [1.0-5.0]\",\n",
        "    },\n",
        "}\n",
        "\n",
        "movie_features = movies_entity_type.batch_create_features(\n",
        "    feature_configs=movies_feature_configs,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3n5XdK8Xjmw"
      },
      "source": [
        "## Ingest Feature values into Entity types from dataframes\n",
        "\n",
        "You need to ingest feature values into your entity type containing the features. It is so that you can later `read` (online) or `batch serve` (offline) the feature values from the entity type. \n",
        "\n",
        "In this step, you learn how to ingest feature values from a Pandas dataframe into an entity type. You can also import feature values from BigQuery or Google Cloud Storage.\n",
        "\n",
        "### Get data from source\n",
        "\n",
        "Define the public data sources for users and movies and copy them locally into *avro* files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uNrHqiGXrff"
      },
      "outputs": [],
      "source": [
        "GCS_USERS_AVRO_URI = (\n",
        "    \"gs://cloud-samples-data-us-central1/vertex-ai/feature-store/datasets/users.avro\"\n",
        ")\n",
        "GCS_MOVIES_AVRO_URI = (\n",
        "    \"gs://cloud-samples-data-us-central1/vertex-ai/feature-store/datasets/movies.avro\"\n",
        ")\n",
        "\n",
        "USERS_AVRO_FN = \"users.avro\"\n",
        "MOVIES_AVRO_FN = \"movies.avro\"\n",
        "\n",
        "! gsutil cp $GCS_USERS_AVRO_URI $USERS_AVRO_FN\n",
        "! gsutil cp $GCS_MOVIES_AVRO_URI $MOVIES_AVRO_FN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd6Z0jfR5OW5"
      },
      "source": [
        "### Load data from avro files \n",
        "\n",
        "Load users and movies data from avro files into Pandas dataframes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrB7bnqbZYaC"
      },
      "outputs": [],
      "source": [
        "# Define a class for reading the avro data\n",
        "class AvroReader:\n",
        "    def __init__(self, data_file):\n",
        "        self.avro_reader = DataFileReader(open(data_file, \"rb\"), DatumReader())\n",
        "\n",
        "    def to_dataframe(self):\n",
        "        records = [record for record in self.avro_reader]\n",
        "        return pd.DataFrame.from_records(data=records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdlWJhUt5OW5"
      },
      "outputs": [],
      "source": [
        "# Load users data from avro file\n",
        "users_avro_reader = AvroReader(data_file=USERS_AVRO_FN)\n",
        "users_source_df = users_avro_reader.to_dataframe()\n",
        "print(users_source_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZ49cPS35OW5"
      },
      "outputs": [],
      "source": [
        "# Load movies data from avro file\n",
        "movies_avro_reader = AvroReader(data_file=MOVIES_AVRO_FN)\n",
        "movies_source_df = movies_avro_reader.to_dataframe()\n",
        "print(movies_source_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgb0WGwX5OW6"
      },
      "source": [
        "### Ingest Feature values into Entity types\n",
        "\n",
        "Load the feature values into `users` entity type providing the id fields and time field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76b813uj5OW6"
      },
      "outputs": [],
      "source": [
        "users_entity_type.ingest_from_df(\n",
        "    feature_ids=[\"age\", \"gender\", \"liked_genres\"],\n",
        "    feature_time=\"update_time\",\n",
        "    df_source=users_source_df,\n",
        "    entity_id_field=\"user_id\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCAdQ3cF5OW6"
      },
      "source": [
        "Load the feature values into `movie` entity type providing the id fields and time field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DYlKe4e5OW6"
      },
      "outputs": [],
      "source": [
        "movies_entity_type.ingest_from_df(\n",
        "    feature_ids=[\"average_rating\", \"title\", \"genres\"],\n",
        "    feature_time=\"update_time\",\n",
        "    df_source=movies_source_df,\n",
        "    entity_id_field=\"movie_id\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIYLZwao5OW6"
      },
      "source": [
        "## Read/serve Entity's feature values online from Feature Store\n",
        "\n",
        "Feature Store allows [online serving](https://cloud.google.com/vertex-ai/docs/featurestore/serving-online)\n",
        "which lets you read feature values for small batches of entities. It works well when you want to read values of selected features from an entity or multiple entities in an entity type.\n",
        "\n",
        "### Read feature values for users"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrR-SY3i58rh"
      },
      "outputs": [],
      "source": [
        "users_read_df = users_entity_type.read(\n",
        "    entity_ids=[\"dave\", \"alice\", \"charlie\", \"bob\", \"eve\"],\n",
        ")\n",
        "print(users_read_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2cfa09ef11d"
      },
      "source": [
        "### Read feature values for movies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTW6kBxN5OW7"
      },
      "outputs": [],
      "source": [
        "movies_read_df = movies_entity_type.read(\n",
        "    entity_ids=[\"movie_01\", \"movie_02\", \"movie_03\", \"movie_04\"],\n",
        "    feature_ids=[\"title\", \"genres\", \"average_rating\"],\n",
        ")\n",
        "print(movies_read_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AK2Glzkq5OW7"
      },
      "source": [
        "## Batch serve feature values from Feature Store\n",
        "\n",
        "Batch Serving is used to fetch a large batch of feature values for high-throughput, and is typically used for training a model or batch prediction. In this section, you learn how to prepare training examples by using the Feature Store's batch serve function.\n",
        "\n",
        "### Read instances from source file\n",
        "\n",
        "Define the source file and destination file. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4k2QVN-5OW7"
      },
      "outputs": [],
      "source": [
        "GCS_READ_INSTANCES_CSV_URI = \"gs://cloud-samples-data-us-central1/vertex-ai/feature-store/datasets/movie_prediction.csv\"\n",
        "READ_INSTANCES_CSV_FN = \"data.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f2c558b649f"
      },
      "source": [
        "Copy the instances from the source file to the destination file locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rr8XDjk_5OW7"
      },
      "outputs": [],
      "source": [
        "! gsutil cp $GCS_READ_INSTANCES_CSV_URI $READ_INSTANCES_CSV_FN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5DW1MFt5OW7"
      },
      "source": [
        "### Load the instances\n",
        "\n",
        "Load the instances from CSV file into a Pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqQVfRnC5OW7"
      },
      "outputs": [],
      "source": [
        "read_instances_df = pd.read_csv(READ_INSTANCES_CSV_FN)\n",
        "print(read_instances_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsgNNH8G5OW8"
      },
      "source": [
        "### Change the data type\n",
        "\n",
        "Change the data type of the timestamp field from `Timestamp` to `Datetime64`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vb9ntNEA5OW8"
      },
      "outputs": [],
      "source": [
        "print(\"before: \", read_instances_df[\"timestamp\"].dtype)\n",
        "read_instances_df = read_instances_df.astype({\"timestamp\": \"datetime64\"})\n",
        "print(\"after:  \", read_instances_df[\"timestamp\"].dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ao1dC5Pc5OW8"
      },
      "source": [
        "### Batch serve feature values from Feature Store\n",
        "\n",
        "Serve the batch response to a dataframe and display the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZSJ-Sbl5OW8"
      },
      "outputs": [],
      "source": [
        "movie_predictions_df = movie_predictions_feature_store.batch_serve_to_df(\n",
        "    serving_feature_ids={\n",
        "        \"users\": [\"age\", \"gender\", \"liked_genres\"],\n",
        "        \"movies\": [\"title\", \"average_rating\", \"genres\"],\n",
        "    },\n",
        "    read_instances_df=read_instances_df,\n",
        ")\n",
        "movie_predictions_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN84znoI5OW8"
      },
      "source": [
        "## Read the latest feature values\n",
        "\n",
        "In Feature Store, you access the latest or the last available feature values unless a specific time is provided. Now, you test this feature by ingesting new data to the entity types and reading it from the Feature Store.\n",
        "\n",
        "### Ingest updated feature values\n",
        "\n",
        "Now, you update the feature values by running the following cell. \n",
        "\n",
        "**Note:** For comparison, you can try printing the feature values read from the entity types earlier (those in `movies_read_df` variable). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Y-iMUFH5OW9"
      },
      "outputs": [],
      "source": [
        "# Create a dataframe for the new data\n",
        "update_movies_df = pd.DataFrame(\n",
        "    data=[[\"movie_03\", 4.3], [\"movie_04\", 4.8]],\n",
        "    columns=[\"movie_id\", \"average_rating\"],\n",
        ")\n",
        "\n",
        "# Ingest the new data from the dataframe\n",
        "movies_entity_type.ingest_from_df(\n",
        "    feature_ids=[\"average_rating\"],\n",
        "    feature_time=datetime.datetime.now(),\n",
        "    df_source=update_movies_df,\n",
        "    entity_id_field=\"movie_id\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s47WCIvL5OW9"
      },
      "source": [
        "### Fetch the latest feature values\n",
        "\n",
        "Reading from the entity type gives you the updated feature values from the latest ingestion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2IPEY7S5OW9"
      },
      "outputs": [],
      "source": [
        "update_movies_read_df = movies_entity_type.read(\n",
        "    entity_ids=[\"movie_01\", \"movie_02\", \"movie_03\", \"movie_04\"],\n",
        "    feature_ids=[\"title\", \"genres\", \"average_rating\"],\n",
        ")\n",
        "print(update_movies_read_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1YGRNsW5OW9"
      },
      "source": [
        "## Point-in-time correctness\n",
        "\n",
        "Vertex AI Feature Store captures feature values for a feature at a specific point in time. In case there are missing values in your past data, you can backfill them using batch serving.\n",
        "\n",
        "### Missing data\n",
        "Recall that response from the batch serve from last ingestion has some missing data in it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueXYomBr5OW-"
      },
      "outputs": [],
      "source": [
        "# Print the response\n",
        "print(movie_predictions_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abQRF6mx5OW-"
      },
      "source": [
        "### Backfill/correct point-in-time data\n",
        "\n",
        "Impute the missing data based on the timestamps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehZhc4ZP5OW-"
      },
      "outputs": [],
      "source": [
        "# Impute the users data\n",
        "backfill_users_df = pd.DataFrame(\n",
        "    data=[[\"bob\", 34, \"Male\", [\"Drama\"], \"2020-02-13 09:35:15\"]],\n",
        "    columns=[\"user_id\", \"age\", \"gender\", \"liked_genres\", \"update_time\"],\n",
        ")\n",
        "backfill_users_df = backfill_users_df.astype({\"update_time\": \"datetime64\"})\n",
        "print(backfill_users_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhdTzl5k5OW-"
      },
      "outputs": [],
      "source": [
        "# Impute the movies data\n",
        "backfill_movies_df = pd.DataFrame(\n",
        "    data=[[\"movie_04\", 4.2, \"The Dark Knight\", \"Action\", \"2020-02-13 09:35:15\"]],\n",
        "    columns=[\"movie_id\", \"average_rating\", \"title\", \"genres\", \"update_time\"],\n",
        ")\n",
        "backfill_movies_df = backfill_movies_df.astype({\"update_time\": \"datetime64\"})\n",
        "print(backfill_movies_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXb4JUhu5OW-"
      },
      "source": [
        "### Ingest the backfilled/corrected data\n",
        "\n",
        "Ingest the imputed point-in-time data from dataframe to the entity types in feature store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vM1ejZMa5OW-"
      },
      "outputs": [],
      "source": [
        "# Ingest the users data\n",
        "users_entity_type.ingest_from_df(\n",
        "    feature_ids=[\"age\", \"gender\", \"liked_genres\"],\n",
        "    feature_time=\"update_time\",\n",
        "    df_source=backfill_users_df,\n",
        "    entity_id_field=\"user_id\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBnrNbv75OW-"
      },
      "outputs": [],
      "source": [
        "# Ingest the users data\n",
        "movies_entity_type.ingest_from_df(\n",
        "    feature_ids=[\"average_rating\", \"title\", \"genres\"],\n",
        "    feature_time=\"update_time\",\n",
        "    df_source=backfill_movies_df,\n",
        "    entity_id_field=\"movie_id\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e62Ku6W5OW_"
      },
      "source": [
        "### Fetch the latest data\n",
        "Batch serve the latest ingested data with backfill/correction to a dataframe to ensure the feature store is updated. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3njvLv2wbZok"
      },
      "outputs": [],
      "source": [
        "backfill_movie_predictions_df = movie_predictions_feature_store.batch_serve_to_df(\n",
        "    serving_feature_ids={\n",
        "        \"users\": [\"age\", \"gender\", \"liked_genres\"],\n",
        "        \"movies\": [\"title\", \"average_rating\", \"genres\"],\n",
        "    },\n",
        "    read_instances_df=read_instances_df,\n",
        ")\n",
        "print(backfill_movie_predictions_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBTNfN8vxz4x"
      },
      "outputs": [],
      "source": [
        "# Delete the feature store\n",
        "movie_predictions_feature_store.delete(force=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "sdk-feature-store-pandas.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

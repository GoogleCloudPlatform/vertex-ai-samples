{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EcdxqUnftBL"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awoLZ5dc5bcG"
      },
      "source": [
        "# Vertex AI Feature Store Based LLM Grounding Tutorial - Offline Version\n",
        "\n",
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/feature_store/vertex_ai_feature_store_based_llm_grounding_tutorial.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/feature_store/vertex_ai_feature_store_based_llm_grounding_tutorial.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/feature_store/vertex_ai_feature_store_based_llm_grounding_tutorial.ipynb.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>                                                                                               \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWNCLbZZ6MLi"
      },
      "source": [
        "## Overview\n",
        "\n",
        "In this tutorial, you learn how to chunk user-provided data, and then generate embedding vectors for each chunk using a Vertex LLM (Large Language Model) having embedding generation capabilities. The resulting embedding vector dataset can then be loaded into Vertex AI Feature Store, enabling fast feature retrieval and efficient online serving.\n",
        "\n",
        "Learn more about [Vertex AI Feature Store](https://cloud.google.com/vertex-ai/docs/featurestore/overview)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBeo3dIqJVDd"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to create and use an offline feature store instance to host and serve data in `BigQuery` with `Vertex AI Feature Store`.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services and resources:\n",
        "\n",
        "- `Vertex AI Feature Store`\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Provision an online feature store instance to host and serve data.\n",
        "- Create an online feature store instance to serve a `BigQuery` table.\n",
        "- Perform CRUD operations\n",
        "- Note the data lineage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1t0xA5XB_3n"
      },
      "source": [
        "### Note\n",
        "This is a Preview release. By using the feature, you acknowledge that you're aware of the open issues and that this preview is provided “as is” under the pre-GA terms of service.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbCa7Pcpqgaz"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "This tutorial uses the [Google Patents Public Data](https://console.cloud.google.com/marketplace/product/google_patents_public_datasets/google-patents-public-data) dataset from the `BigQuery` public datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrggkyCUrhZM"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* `Vertex AI`\n",
        "* `BigQuery`\n",
        "* `Cloud Storage`\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing),\n",
        "[BigQuery pricing](https://cloud.google.com/bigquery/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing),\n",
        "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTJiDCrYsOmT"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Install the following packages required to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAqbE5Z2sTVM"
      },
      "outputs": [],
      "source": [
        "# Install the packages\n",
        "! pip3 install --upgrade --quiet google-cloud-aiplatform\\\n",
        "                                 google-cloud-bigquery\\\n",
        "                                 db-dtypes\n",
        "\n",
        "! pip3 install --upgrade kfp -q --no-warn-conflicts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np60_uuCs7X5"
      },
      "source": [
        "### Colab only: Uncomment the following cell to restart the kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_u0aEgaSs-3v"
      },
      "outputs": [],
      "source": [
        "# # Automatically restart the kernel after installation so that your environment can access the new packages\n",
        "# import IPython\n",
        "\n",
        "# app = IPython.Application.instance()\n",
        "# app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdqw6ADTtJRI"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "3. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "4. If you're running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFANidV0tPbo"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, try the following:\n",
        "* Run `gcloud config list`.\n",
        "* Run `gcloud projects list`.\n",
        "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5cutPRQtQ7m"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"your-project-id\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfY8yWnbtZ0K"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations). Note that the new Feature Store capability showed in the colab is currently only available in the following regions:\n",
        "* `us-central1`\n",
        "* `us-east1`\n",
        "* `us-west1`\n",
        "* `europe-west4`\n",
        "* `asia-southeast1`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6iMMALZthFM"
      },
      "outputs": [],
      "source": [
        "REGION = \"us-central1\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni5jx6RGtzG3"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "Depending on your Jupyter environment, you might have to manually authenticate. Follow these instructions:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzsMphY2t4-v"
      },
      "source": [
        "**1. Vertex AI Workbench**\n",
        "* Do nothing as you are already authenticated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_OnJm_Yt8bw"
      },
      "source": [
        "**2. Local JupyterLab instance, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4ybIfzhuAOc"
      },
      "outputs": [],
      "source": [
        "! gcloud auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmnRqX6BuBnx"
      },
      "source": [
        "**3. Colab, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uN9JoC1buE9P"
      },
      "outputs": [],
      "source": [
        "# from google.colab import auth\n",
        "# auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNAvMVJjuH5b"
      },
      "source": [
        "**4. Service account or other**\n",
        "* See how to grant Cloud Storage permissions to your service account at https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMCl0avIusKl"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifbIQuN1uz2r"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "from google.cloud import aiplatform, bigquery\n",
        "from google.cloud.aiplatform_v1beta1 import (\n",
        "    FeatureRegistryServiceClient,\n",
        "    CreateFeatureGroupRequest,\n",
        "    FeatureGroup,\n",
        "    CreateFeatureRequest,\n",
        "    ListFeatureGroupsRequest,\n",
        "    ListFeaturesRequest,\n",
        "    DeleteFeatureGroupRequest,\n",
        ")\n",
        "\n",
        "from google.cloud.aiplatform_v1beta1.types import (\n",
        "    Feature,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ooJNSOvu6Q5"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLTm3pquu9ar"
      },
      "outputs": [],
      "source": [
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "API_ENDPOINT = f\"{REGION}-aiplatform.googleapis.com\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYzwb095DJTl"
      },
      "source": [
        "## Set up and start online serving\n",
        "\n",
        "To serve embedding data in Vertex AI Feature Store, do the following:\n",
        "\n",
        "1. Prepare the data source in BigQuery.\n",
        "2. Create an FeatureOnlineStore instance to host the data.\n",
        "3. Define the data (`FeatureView`) to be served by the newly-created instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNv9jdEhV0u6"
      },
      "source": [
        "### Prepare BigQuery data source for feature view creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PBDNyHnY_OB"
      },
      "outputs": [],
      "source": [
        "GCS_BUCKET = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OV9dADJb63o"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-9B3v7kcOAF"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l {REGION} -p {PROJECT_ID} {GCS_BUCKET}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTykneRCbDOU"
      },
      "source": [
        "#### Prepare data in Google Cloud Storage (GCS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmL7Z66xb9Sx"
      },
      "outputs": [],
      "source": [
        "INPUT_TEXT_GCS_DIR = f\"{GCS_BUCKET}/fs_grounding/data\"\n",
        "\n",
        "import tarfile\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "\n",
        "def untar(file_name):\n",
        "    output_folder_name = file_name[:-7]\n",
        "    file = tarfile.open(file_name)\n",
        "    file.extractall(output_folder_name)\n",
        "    return output_folder_name\n",
        "\n",
        "\n",
        "# Download data from https://www.cl.uni-heidelberg.de/statnlpgroup/nfcorpus/nfcorpus.tar.gz\n",
        "url = \"https://www.cl.uni-heidelberg.de/statnlpgroup/nfcorpus/nfcorpus.tar.gz\"\n",
        "filename = \"nfcorpus.tar.gz\"\n",
        "path, _ = urlretrieve(url, filename)\n",
        "print(f\"Downloaded {path}\")\n",
        "\n",
        "# Copy text files to GCS.\n",
        "output_folder_name = f\"{untar(path)}/nfcorpus\"\n",
        "dev_all_queries = f\"{output_folder_name}/dev.all.queries\"\n",
        "dev_docs = f\"{output_folder_name}/dev.docs\"\n",
        "! gsutil cp {dev_all_queries} {INPUT_TEXT_GCS_DIR}/queries\n",
        "! gsutil cp {dev_docs} {INPUT_TEXT_GCS_DIR}/docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4rxyUV4XU7q"
      },
      "source": [
        "#### Create BigQuery dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdIPzLkoW5mc"
      },
      "outputs": [],
      "source": [
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "BQ_DATASET_ID = \"fs_grounding\"  # @param {type:\"string\"}\n",
        "dataset = bigquery.Dataset(f\"{PROJECT_ID}.{BQ_DATASET_ID}\")\n",
        "dataset.location = REGION\n",
        "dataset = bq_client.create_dataset(\n",
        "    dataset, exists_ok=True, timeout=30\n",
        ")  # Make an API request.\n",
        "\n",
        "# Confirm dataset created.\n",
        "print(f\"Created dataset {dataset}.{BQ_DATASET_ID}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlKyrvIAXf5O"
      },
      "source": [
        "#### Launch pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Knz8N5iIXpl9"
      },
      "outputs": [],
      "source": [
        "run_id = str(uuid.uuid4())\n",
        "\n",
        "PIPELINE_TEMPLATE_URI = \"gs://vertex-evaluation-pipeline-templates/20240117_0005/feature_store_grounding_pipeline_pipeline.yaml\"\n",
        "BIGQUERY_BP_INPUT_URI = f\"bq://{PROJECT_ID}.{BQ_DATASET_ID}.batch_predict_input\"\n",
        "BIGQUERY_BP_OUTPUT_URI = f\"bq://{PROJECT_ID}.{BQ_DATASET_ID}.batch_predict_output\"\n",
        "\n",
        "PARAMS = {\n",
        "    \"project\": PROJECT_ID,\n",
        "    \"location\": REGION,\n",
        "    \"bigquery_bp_input_uri\": BIGQUERY_BP_INPUT_URI,\n",
        "    \"bigquery_bp_output_uri\": BIGQUERY_BP_OUTPUT_URI,\n",
        "    \"input_text_gcs_dir\": INPUT_TEXT_GCS_DIR,\n",
        "    \"output_text_gcs_dir\": f\"{GCS_BUCKET}/fs_grounding_{run_id}/chunking_output\",\n",
        "    \"output_error_file_path\": f\"{GCS_BUCKET}/fs_grounding_{run_id}/chunking_error_output\",\n",
        "    \"model_name\": \"publishers/google/models/textembedding-gecko@003\",\n",
        "    \"generation_threshold_microseconds\": \"0\",\n",
        "}\n",
        "\n",
        "\n",
        "def run_pipeline(\n",
        "    parameters: dict,\n",
        "    project: str,\n",
        "    pipeline_root: str,\n",
        "    location: str = \"us-central1\",\n",
        ") -> aiplatform.PipelineJob:\n",
        "    aiplatform.init(\n",
        "        project=project,\n",
        "        location=location,\n",
        "    )\n",
        "\n",
        "    test_prefix = \"your-test-prefix\"  # @param {type:\"string\"}\n",
        "    pipeline_name = \"feature-store-grounding-pipeline\"  # @param {type:\"string\"}\n",
        "\n",
        "    test_name = f\"{test_prefix}-{pipeline_name}-{run_id}\"\n",
        "    job = aiplatform.PipelineJob(\n",
        "        display_name=test_name,\n",
        "        template_path=PIPELINE_TEMPLATE_URI,\n",
        "        job_id=test_name,\n",
        "        pipeline_root=pipeline_root,\n",
        "        parameter_values=parameters,\n",
        "        enable_caching=False,\n",
        "    )\n",
        "\n",
        "    job.submit()\n",
        "\n",
        "    return job\n",
        "\n",
        "\n",
        "job = run_pipeline(\n",
        "    parameters=PARAMS,\n",
        "    project=PROJECT_ID,\n",
        "    pipeline_root=f\"{GCS_BUCKET}/fs_based/pipeline_root\",\n",
        "    location=REGION,\n",
        ")\n",
        "job.wait()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7v96efXpBxf"
      },
      "source": [
        "#### BQ format conversion\n",
        "\n",
        "This table is loading data with embeddings. It also has a few extra special fields that assist with such as `feature_timestamp` which provides tells FS what the latest value of a given `entity_id`. Note that you can map different fields to surrogate for the default `entity_id` by using the `entity_id_columns` below.\n",
        "\n",
        "For any BigQuery table or view that you associate with a feature group, you need to [ensure the following](https://cloud.google.com/vertex-ai/docs/featurestore/latest/create-featuregroup):\n",
        "\n",
        "- The schema of the data source conforms to the Data source preparation guidelines.\n",
        "\n",
        "- The data source contains the entity IDs as string values in a column named entity_id.\n",
        "\n",
        "- The data source contains the feature timestamps of type timestamp in a column called feature_timestamp."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vv27Q9AanTCO"
      },
      "outputs": [],
      "source": [
        "def compose_bq_query_format_conversion(\n",
        "    bigquery_bp_input_uri: str, bigquery_bp_output_uri: str\n",
        ") -> str:\n",
        "    \"\"\"Compose the BQ query for format conversion.\n",
        "\n",
        "    Args:\n",
        "      bigquery_bp_input_uri: The URI to a bigquery table as the input for the\n",
        "        batch prediction component. The chunking component will populate data to\n",
        "        this uri first before batch prediction.\n",
        "      bigquery_bp_output_uri: The URI to a bigquery table as the output for the\n",
        "        batch prediction component.\n",
        "\n",
        "    Returns:\n",
        "      The composed query for BigQuery format conversion.\n",
        "    \"\"\"\n",
        "\n",
        "    if bigquery_bp_input_uri.startswith(\"bq://\"):\n",
        "        bigquery_bp_input_uri = bigquery_bp_input_uri.replace(\"bq://\", \"\")\n",
        "\n",
        "    if bigquery_bp_output_uri.startswith(\"bq://\"):\n",
        "        bigquery_bp_output_uri = bigquery_bp_output_uri.replace(\"bq://\", \"\")\n",
        "\n",
        "    inseration_query = (\n",
        "        f\"UPDATE `{bigquery_bp_input_uri}` destTable\"\n",
        "        \" SET embedding=ARRAY( select cast (str_element as float64) from\"\n",
        "        \" unnest(JSON_VALUE_ARRAY(prediction, '$.embeddings.values')) as\"\n",
        "        \" str_element)\"\n",
        "    )\n",
        "    fetch_data_query = (\n",
        "        \"FROM (SELECT vertex_generated_chunk_id, prediction FROM\"\n",
        "        f\" `{bigquery_bp_output_uri}` cross join\"\n",
        "        \" unnest(JSON_EXTRACT_ARRAY(predictions)) as prediction) sourceTable\"\n",
        "        \" WHERE\"\n",
        "        \" destTable.vertex_generated_chunk_id=sourceTable.vertex_generated_chunk_id\"\n",
        "    )\n",
        "    return f\"{inseration_query} {fetch_data_query};\"\n",
        "\n",
        "\n",
        "bq_query = compose_bq_query_format_conversion(\n",
        "    bigquery_bp_input_uri=BIGQUERY_BP_INPUT_URI,\n",
        "    bigquery_bp_output_uri=BIGQUERY_BP_OUTPUT_URI,\n",
        ")\n",
        "DATA_SOURCE = BIGQUERY_BP_INPUT_URI\n",
        "bq_job = bq_client.query(bq_query)\n",
        "bq_job.result()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Goal Offline use of embeddings using SDK CRUD ops\n",
        "\n",
        "1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set parent\n",
        "parent = f\"projects/{PROJECT_ID}/locations/{REGION}\"\n",
        "\n",
        "print(parent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 0. Configure the client and explore SDK options\n",
        "\n",
        "Not required but you can understand the options of the admin client by exploring the methods and classes in the module below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "admin_client_offline_fs = FeatureRegistryServiceClient(\n",
        "    client_options={\"api_endpoint\": API_ENDPOINT}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# playground - lots of useful things in here\n",
        "admin_client_offline_fs.create_feature()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Make a feature group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Our bq table for the feature group: \", DATA_SOURCE)\n",
        "FG_ID = 'my-embedding-feature-group'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Dict, List\n",
        "\n",
        "\n",
        "def sample_create_feature_group(\n",
        "    bq_uri: str,\n",
        "    parent: str,\n",
        "    feature_group_name: str,\n",
        "    entity_id_columns: List[str],\n",
        "    client_options: Dict[str, str] = {\"api_endpoint\": API_ENDPOINT},\n",
        "):\n",
        "    # Create a client\n",
        "    client = FeatureRegistryServiceClient(client_options=client_options)\n",
        "\n",
        "    # Initialize request argument(s)\n",
        "    feature_group = FeatureGroup()\n",
        "    feature_group.big_query.big_query_source.input_uri = bq_uri\n",
        "    feature_group.big_query.entity_id_columns = entity_id_columns\n",
        "\n",
        "    request = CreateFeatureGroupRequest(\n",
        "        parent=parent,\n",
        "        feature_group=feature_group,\n",
        "        feature_group_id=feature_group_name,\n",
        "    )\n",
        "\n",
        "    # Make the request\n",
        "    operation = client.create_feature_group(request=request)\n",
        "\n",
        "    print(\"Waiting for operation to complete...\")\n",
        "\n",
        "    response = operation.result()\n",
        "\n",
        "    # Handle the response\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_create_feature_group(\n",
        "    bq_uri=DATA_SOURCE,\n",
        "    entity_id_columns=[\"vertex_generated_chunk_id\"],\n",
        "    parent=parent,\n",
        "    feature_group_name=FG_ID,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Add features to the Feature Group\n",
        "\n",
        "\n",
        " IMPORTANT NOTE:\n",
        "\n",
        " BILLING SERVICES ENABLED: Lineage requires both Data Lineage API and Data Catalog API to be enabled in project _____. Enabling them may have billing implications .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_create_feature(\n",
        "    parent: str,\n",
        "    feature_group: str,\n",
        "    feature_name: str,\n",
        "    feature_description: str,\n",
        "    client_options: Dict[str, str] = {\"api_endpoint\": API_ENDPOINT},\n",
        "):\n",
        "    # Create a client\n",
        "    client = FeatureRegistryServiceClient(client_options=client_options)\n",
        "    # projects/{project}/locations/{location}/featureGroups/{feature_group}/features/{feature}\n",
        "    parent_feature_group = f\"{parent}/featureGroups/{feature_group}\"\n",
        "    fqn = f\"{parent_feature_group}/features/{feature_name}\"\n",
        "    feature = Feature(name=fqn, description=feature_description)\n",
        "    # Initialize request argument(s)\n",
        "    request = CreateFeatureRequest(\n",
        "        parent=parent_feature_group, feature=feature, feature_id=feature_name\n",
        "    )\n",
        "\n",
        "    # Make the request\n",
        "    operation = client.create_feature(request=request)\n",
        "\n",
        "    print(\"Waiting for operation to complete...\")\n",
        "\n",
        "    response = operation.result()\n",
        "\n",
        "    # Handle the response\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_create_feature(\n",
        "    parent=parent,\n",
        "    feature_group=FG_ID,\n",
        "    feature_name=\"embedding\",\n",
        "    feature_description=\"text gecko003 experimental test i did\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_create_feature(\n",
        "    parent=parent,\n",
        "    feature_group=FG_ID,\n",
        "    feature_name=\"content\",\n",
        "    feature_description=\"the source text for the text gecko test i did\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_create_feature(\n",
        "    parent=parent,\n",
        "    feature_group=FG_ID,\n",
        "    feature_name=\"chunk_size\",\n",
        "    feature_description=\"chunk size for the embedding passage\",\n",
        ")\n",
        "sample_create_feature(\n",
        "    parent=parent,\n",
        "    feature_group=FG_ID,\n",
        "    feature_name=\"overlap_size\",\n",
        "    feature_description=\"number of characters the chunks overlapped for the embedding passages\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. list feature groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_list_feature_groups(\n",
        "    parent: str, client_options: Dict[str, str] = {\"api_endpoint\": API_ENDPOINT}\n",
        "):\n",
        "    # Create a client\n",
        "    client = FeatureRegistryServiceClient(client_options=client_options)\n",
        "\n",
        "    # Initialize request argument(s)\n",
        "    request = ListFeatureGroupsRequest(\n",
        "        parent=parent,\n",
        "    )\n",
        "\n",
        "    # Make the request\n",
        "    page_result = client.list_feature_groups(request=request)\n",
        "\n",
        "    # Handle the response\n",
        "    for response in page_result:\n",
        "        print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fgs = sample_list_feature_groups(parent=parent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4. List Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_list_features(\n",
        "    parent: str,\n",
        "    feature_group_name: str,\n",
        "    client_options: Dict[str, str] = {\"api_endpoint\": API_ENDPOINT},\n",
        "):\n",
        "    # Create a client\n",
        "    client = FeatureRegistryServiceClient(client_options=client_options)\n",
        "\n",
        "    fqn = f\"{parent}/featureGroups/{feature_group_name}\"\n",
        "    # Initialize request argument(s)\n",
        "    request = ListFeaturesRequest(\n",
        "        parent=fqn,\n",
        "    )\n",
        "\n",
        "    # Make the request\n",
        "    page_result = client.list_features(request=request)\n",
        "\n",
        "    # Handle the response\n",
        "    for response in page_result:\n",
        "        print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_list_features(parent=parent, feature_group_name=FG_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OR1ve2OyyiKq"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, delete the individual resources you created in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Um27J_vvvzGc"
      },
      "outputs": [],
      "source": [
        "# Delete Feature Group\n",
        "\n",
        "def sample_delete_feature_group(parent: str, feature_group_name: str, client_options: Dict [str,str ]= {\"api_endpoint\": API_ENDPOINT}):\n",
        "                # Create a client\n",
        "                client = FeatureRegistryServiceClient(client_options=client_options)\n",
        "                fqn = f'{parent}/featureGroups/{feature_group_name}'\n",
        "                # Initialize request argument(s)\n",
        "                request = DeleteFeatureGroupRequest(\n",
        "                    name=fqn,\n",
        "                )\n",
        "\n",
        "                # Make the request\n",
        "                operation = client.delete_feature_group(request=request)\n",
        "\n",
        "                print(\"Waiting for operation to complete...\")\n",
        "\n",
        "                response = operation.result()\n",
        "\n",
        "                # Handle the response\n",
        "                print(response)\n",
        "\n",
        "sample_delete_feature_group(\n",
        "    parent=parent, feature_group_name=FG_NAME)\n",
        "\n",
        "\n",
        "# Delete Cloud Storage objects that were created\n",
        "import os\n",
        "\n",
        "delete_bucket = False\n",
        "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
        "    ! gsutil -m rm -r $GCS_BUCKET"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "vertex_ai_feature_store_based_llm_grounding_tutorial.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2026 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy LoRA Fine-tuned Models on Vertex AI with Custom vLLM Container\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/prediction/vertexai_serving_vllm/vertexai_serving_vllm_mistral_7b_lora_custom_container.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Fprediction%2Fvertexai_serving_vllm%2Fvertexai_serving_vllm_mistral_7b_lora_custom_container.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>    \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/prediction/vertexai_serving_vllm/vertexai_serving_vllm_mistral_7b_lora_custom_container.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/prediction/vertexai_serving_vllm/vertexai_serving_vllm_mistral_7b_lora_custom_container.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://raw.githubusercontent.com/primer/octicons/refs/heads/main/icons/mark-github-24.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to deploy a LoRA (Low-Rank Adaptation) fine-tuned model on Vertex AI using a custom-built vLLM container. This approach gives you more control over the vLLM configuration and allows you to customize the serving environment.\n",
    "\n",
    "### Models used in this example\n",
    "\n",
    "**Base Model:** [Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)\n",
    "\n",
    "Mistral-7B-Instruct-v0.3 is an instruction-tuned version of the Mistral-7B model, optimized for following instructions and conversational tasks. It features a 32k context window and uses the v3 tokenizer with extended vocabulary.\n",
    "\n",
    "**LoRA Adapter:** [Research-Reasoner-7B-v0.3](https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3)\n",
    "\n",
    "Research-Reasoner-7B-v0.3 is a LoRA adapter fine-tuned on top of Mistral-7B to enhance research planning and reasoning capabilities. It enables the model to provide structured, step-by-step research methodologies.\n",
    "\n",
    "### What you will learn\n",
    "\n",
    "In this tutorial, you will learn how to:\n",
    "\n",
    "* Set up Artifact Registry for custom container storage\n",
    "* Build and push a custom vLLM container using Cloud Build\n",
    "* Configure vLLM serving arguments for LoRA deployment\n",
    "* Register the model in Vertex AI Model Registry\n",
    "* Deploy the model to a Vertex AI Endpoint\n",
    "* Run inference using both the base model and LoRA adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required packages\n",
    "\n",
    "Install the Vertex AI SDK for Python. This package provides the `aiplatform` module for:\n",
    "\n",
    "- Uploading models to the Model Registry\n",
    "- Creating and managing endpoints\n",
    "- Deploying models for online predictions\n",
    "\n",
    "**Note**: Unlike the prebuilt container notebook, we don't need `huggingface_hub` here because the models are assumed to already be in GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install --upgrade --quiet google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import vertexai\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "Authenticate your environment on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Google Cloud project information\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
    "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type: \"string\", placeholder: \"[your-bucket-name]\", isTemplate: true}\n",
    "if not BUCKET_NAME or BUCKET_NAME == \"[your-bucket-name]\":\n",
    "    BUCKET_NAME = f\"{PROJECT_ID}-vertex-ai-models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"shamika-customer-ml-dev\"  # <--- REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET_NAME = \"shamika-ml-dev-vertex-ai-artifacts\" # <--- REPLACE WITH YOUR BUCKET NAME\n",
    "LOCATION = \"us-central1\"\n",
    "SERVICE_ACCOUNT = \"shamika-ml-dev-vertex-deployer@shamika-customer-ml-dev.iam.gserviceaccount.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Vertex AI for project: shamika-customer-ml-dev\n"
     ]
    }
   ],
   "source": [
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "print(f\"Initialized Vertex AI for project: {PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure model paths\n",
    "\n",
    "Define the GCS paths where your base model and LoRA adapter are stored.\n",
    "\n",
    "**Prerequisites**: Before running this notebook, ensure that:\n",
    "1. The Mistral-7B-Instruct-v0.3 base model is uploaded to `gs://{BUCKET_NAME}/deployments/lora/basemodels/`\n",
    "2. The Research-Reasoner LoRA adapter is uploaded to `gs://{BUCKET_NAME}/deployments/lora/adapters/`\n",
    "\n",
    "You can use the prebuilt container notebook to download and upload these models, or use the Hugging Face CLI:\n",
    "```bash\n",
    "huggingface-cli download mistralai/Mistral-7B-Instruct-v0.3 --local-dir ./model\n",
    "gsutil -m cp -r ./model gs://{BUCKET_NAME}/deployments/lora/basemodels/mistralai/Mistral-7B-Instruct-v0.3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_BASE_MODEL_URI = f\"gs://{BUCKET_NAME}/deployments/lora/basemodels/mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "GCS_ADAPTER_URI = f\"gs://{BUCKET_NAME}/deployments/lora/adapters/Raymond-dev-546730/Research-Reasoner-7B-v0.3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable required APIs\n",
    "\n",
    "Enable the Google Cloud APIs needed for building and deploying custom containers:\n",
    "\n",
    "- **Artifact Registry API**: For storing Docker container images\n",
    "- **Cloud Build API**: For building Docker images in the cloud\n",
    "\n",
    "These APIs are required to build the custom vLLM container and push it to your project's Artifact Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation \"operations/acat.p2-39042042168-21db2887-d325-4b3b-87bb-c22286db35b4\" finished successfully.\n"
     ]
    }
   ],
   "source": [
    "! gcloud services enable artifactregistry.googleapis.com cloudbuild.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Artifact Registry repository\n",
    "\n",
    "Create a Docker repository in Artifact Registry to store your custom vLLM container image.\n",
    "\n",
    "**What is Artifact Registry?**\n",
    "Artifact Registry is Google Cloud's package management service for storing, managing, and securing container images and other artifacts. It integrates with Cloud Build and Vertex AI for seamless deployment workflows.\n",
    "\n",
    "**Repository configuration:**\n",
    "- **Format**: Docker (for container images)\n",
    "- **Location**: Same region as your Vertex AI deployment for lower latency\n",
    "- **Access**: Vertex AI automatically has access to pull images from Artifact Registry in the same project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create request issued for: [vllm-lora-vertex-ai-docker-repo]\n",
      "Waiting for operation [projects/shamika-customer-ml-dev/locations/us-central1/o\n",
      "perations/d786b8f9-ecac-4a3f-bfec-e92b986cb382] to complete...done.            \n",
      "Created repository [vllm-lora-vertex-ai-docker-repo].\n"
     ]
    }
   ],
   "source": [
    "DOCKER_REPOSITORY = \"vllm-lora-vertex-ai-docker-repo\"\n",
    "\n",
    "! gcloud artifacts repositories create {DOCKER_REPOSITORY} \\\n",
    "    --repository-format=docker \\\n",
    "    --location={LOCATION} \\\n",
    "    --description=\"vLLM LoRA Vertex AI Docker repository\" \\\n",
    "    --quiet || echo \"Repository may already exist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build custom vLLM container\n",
    "\n",
    "Build a custom vLLM Docker container using Cloud Build. This approach offers several advantages over the prebuilt container:\n",
    "\n",
    "**Why use a custom container?**\n",
    "- Install specific vLLM versions or patches\n",
    "- Add custom dependencies or modifications\n",
    "- Include model-specific optimizations\n",
    "- Bundle additional tools for debugging\n",
    "\n",
    "**Cloud Build configuration:**\n",
    "\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `--config` | Path to cloudbuild.yaml defining build steps |\n",
    "| `--region` | Build region (should match deployment region) |\n",
    "| `--timeout` | Maximum build time (2 hours for large images) |\n",
    "| `--machine-type` | Build machine (e2-highcpu-32 for faster builds) |\n",
    "| `--substitutions` | Variables passed to cloudbuild.yaml |\n",
    "\n",
    "**Required files in `custom_container/` directory:**\n",
    "- `Dockerfile`: Container build instructions\n",
    "- `cloudbuild.yaml`: Cloud Build configuration\n",
    "\n",
    "The build process typically takes 10-15 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary archive of 5 file(s) totalling 12.5 KiB before compression.\n",
      "Uploading tarball of [.] to [gs://shamika-ml-dev-vertex-ai-artifacts/cloudbuild-staging/1770056864.082014-67ee5944961e4c8e97f5eaa3a40b26fc.tgz]\n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.builds.submit) INVALID_ARGUMENT: could not resolve source: googleapi: Error 403: 39042042168-compute@developer.gserviceaccount.com does not have storage.objects.get access to the Google Cloud Storage object. Permission 'storage.objects.get' denied on resource (or it may not exist)., forbidden\n",
      "- '@type': type.googleapis.com/google.rpc.DebugInfo\n",
      "  detail: \"could not resolve source: googleapi: Error 403: 39042042168-compute@developer.gserviceaccount.com\\\n",
      "    \\ does not have storage.objects.get access to the Google Cloud Storage object.\\\n",
      "    \\ Permission 'storage.objects.get' denied on resource (or it may not exist).,\\\n",
      "    \\ forbidden\"\n"
     ]
    }
   ],
   "source": [
    "! cd ./custom_container \\\n",
    "    && gcloud builds submit \\\n",
    "        --config=cloudbuild.yaml \\\n",
    "        --region={LOCATION} \\\n",
    "        --timeout=\"2h\" \\\n",
    "        --machine-type=e2-highcpu-32 \\\n",
    "        --gcs-source-staging-dir=gs://{BUCKET_NAME}/cloudbuild-staging \\\n",
    "        --substitutions=_REPOSITORY={DOCKER_REPOSITORY} \\\n",
    "        --gcs-log-dir=gs://{BUCKET_NAME}/cloudbuild-staging/logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy model to Vertex AI Endpoint\n",
    "\n",
    "Deployment to Vertex AI involves three main steps:\n",
    "\n",
    "1. **Upload model to Model Registry**: Register your model configuration including container image, serving arguments, and routes\n",
    "2. **Create an Endpoint**: Create a Vertex AI Endpoint resource that will host the deployed model\n",
    "3. **Deploy model to Endpoint**: Deploy the model to the endpoint with specified hardware configuration\n",
    "\n",
    "This separation allows you to:\n",
    "- Reuse the same model across multiple endpoints\n",
    "- Update endpoints independently of model versions\n",
    "- Manage traffic splitting between model versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure deployment settings\n",
    "\n",
    "Define the hardware configuration and container image URI for deployment.\n",
    "\n",
    "**Hardware selection:**\n",
    "\n",
    "| Parameter | Value | Description |\n",
    "|-----------|-------|-------------|\n",
    "| `machine_type` | g2-standard-12 | 12 vCPUs, 48GB RAM, 1 NVIDIA L4 GPU |\n",
    "| `accelerator_type` | NVIDIA_L4 | Cost-effective GPU with 24GB VRAM |\n",
    "| `accelerator_count` | 1 | Single GPU (sufficient for 7B model) |\n",
    "\n",
    "**Why NVIDIA L4?**\n",
    "- 24GB GPU memory fits 7B models with LoRA adapters\n",
    "- Good price-performance ratio for inference workloads\n",
    "- Supports bfloat16 for efficient inference\n",
    "\n",
    "**Container URI format:**\n",
    "`{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE_NAME}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_type = \"g2-standard-12\"\n",
    "accelerator_type = \"NVIDIA_L4\"\n",
    "accelerator_count = 1\n",
    "\n",
    "DOCKER_URI = f\"{LOCATION}-docker.pkg.dev/{PROJECT_ID}/{DOCKER_REPOSITORY}/vllm-gpu\"\n",
    "model_name = \"mistral-7B-instruct-v0.3-lora-custom-container\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload model to Model Registry\n",
    "\n",
    "Register the model in Vertex AI Model Registry with the custom container and vLLM serving arguments.\n",
    "\n",
    "**vLLM serving parameters:**\n",
    "\n",
    "| Parameter | Value | Description |\n",
    "|-----------|-------|-------------|\n",
    "| `--model` | GCS URI | Path to base model in GCS |\n",
    "| `--served-model-name` | mistral-base-custom-container | Model name for API calls |\n",
    "| `--tensor-parallel-size` | 1 | GPUs for model parallelism |\n",
    "| `--gpu-memory-utilization` | 0.90 | Use 90% of GPU memory |\n",
    "| `--max-model-len` | 8192 | Maximum context length |\n",
    "| `--max-num-seqs` | 64 | Concurrent request batching |\n",
    "| `--dtype` | bfloat16 | Use bfloat16 precision |\n",
    "\n",
    "**LoRA configuration:**\n",
    "\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `--enable-lora` | Enable LoRA adapter support |\n",
    "| `--max-loras` | Max adapters in GPU memory |\n",
    "| `--max-lora-rank` | Maximum LoRA rank (64) |\n",
    "| `--lora-modules` | Register adapter as `researcher={GCS_URI}` |\n",
    "\n",
    "**Key difference from prebuilt container**: With a custom container, the LoRA adapter is loaded directly from GCS using the full GCS URI, rather than from a local path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_model_len = 8192\n",
    "gpu_memory_utilization = 0.90\n",
    "max_num_seqs = 64\n",
    "\n",
    "vllm_args = [\n",
    "    \"python3\",\n",
    "    \"-m\",\n",
    "    \"vllm.entrypoints.openai.api_server\",\n",
    "    \"--host=0.0.0.0\",\n",
    "    \"--port=8080\",\n",
    "    f\"--model={GCS_BASE_MODEL_URI}\",\n",
    "    \"--served-model-name=mistral-base-custom-container\",\n",
    "    f\"--tensor-parallel-size={accelerator_count}\",\n",
    "    \"--swap-space=16\",\n",
    "    f\"--gpu-memory-utilization={gpu_memory_utilization}\",\n",
    "    f\"--max-model-len={max_model_len}\",\n",
    "    f\"--max-num-seqs={max_num_seqs}\",\n",
    "    \"--dtype=bfloat16\",\n",
    "    # LoRA configuration\n",
    "    \"--enable-lora\",\n",
    "    \"--max-loras=1\",\n",
    "    \"--max-cpu-loras=1\",\n",
    "    \"--max-lora-rank=64\",\n",
    "    f\"--lora-modules=researcher={GCS_ADAPTER_URI}\"\n",
    "]\n",
    "\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=model_name,\n",
    "    serving_container_image_uri=DOCKER_URI,\n",
    "    serving_container_args=vllm_args,\n",
    "    serving_container_ports=[8080],\n",
    "    serving_container_predict_route=\"/v1/completions\",\n",
    "    serving_container_health_route=\"/health\",\n",
    "    serving_container_shared_memory_size_mb=(16 * 1024),\n",
    "    serving_container_deployment_timeout=7200,\n",
    ")\n",
    "print(f\"Model uploaded: {model.resource_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vertex AI Endpoint\n",
    "\n",
    "Create a Vertex AI Endpoint resource. An endpoint is a cloud resource that provides a URL for serving online predictions.\n",
    "\n",
    "**Why separate endpoint creation?**\n",
    "- Endpoints can host multiple deployed models for A/B testing\n",
    "- Traffic can be split between model versions\n",
    "- Endpoints persist even when models are undeployed\n",
    "- Enables blue-green deployments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
    "print(f\"Endpoint created: {endpoint.resource_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model to endpoint\n",
    "\n",
    "Deploy the model to the endpoint with autoscaling configuration.\n",
    "\n",
    "**Deployment parameters:**\n",
    "\n",
    "| Parameter | Value | Description |\n",
    "|-----------|-------|-------------|\n",
    "| `min_replica_count` | 1 | Minimum running replicas |\n",
    "| `max_replica_count` | 4 | Maximum replicas for autoscaling |\n",
    "| `autoscaling_target_accelerator_duty_cycle` | 60 | Scale up when GPU utilization exceeds 60% |\n",
    "| `traffic_percentage` | 100 | Send all traffic to this model |\n",
    "| `deploy_request_timeout` | 1800 | 30 minutes for deployment |\n",
    "\n",
    "**Autoscaling behavior:**\n",
    "- Vertex AI monitors GPU utilization\n",
    "- When average utilization exceeds 60%, new replicas are added (up to max)\n",
    "- When utilization drops, replicas are removed (down to min)\n",
    "- Scaling decisions are made based on a rolling average\n",
    "\n",
    "**Note**: Deployment takes 10-20 minutes as the container downloads model weights and loads them into GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=model_name,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=4,\n",
    "    autoscaling_target_accelerator_duty_cycle=60,\n",
    "    traffic_percentage=100,\n",
    "    deploy_request_timeout=1800,\n",
    "    sync=True,\n",
    ")\n",
    "print(f\"Model deployed to endpoint: {endpoint.resource_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference\n",
    "\n",
    "Test the deployed endpoint by sending prompts to both the base model and the LoRA adapter.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "The vLLM server exposes an OpenAI-compatible API. You can select which model to use via the `model` field:\n",
    "- `\"model\": \"mistral-base-custom-container\"` - Uses the base Mistral model\n",
    "- `\"model\": \"researcher\"` - Uses the base model with the Research-Reasoner LoRA adapter\n",
    "\n",
    "**Expected differences:**\n",
    "\n",
    "| Aspect | Base Model | LoRA Adapter |\n",
    "|--------|------------|--------------|\n",
    "| Response style | General instruction-following | Structured research methodology |\n",
    "| Output format | Variable | Step-by-step research plan |\n",
    "| Domain focus | Broad | Research planning & reasoning |\n",
    "\n",
    "The `raw_predict` method sends raw HTTP requests to the vLLM container, enabling the use of the OpenAI-compatible `/v1/completions` endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(endpoint, prompt, model_name):\n",
    "    \"\"\"Send a prompt to the model and get a response.\"\"\"\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"temperature\": 0.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"top_k\": 1\n",
    "    }\n",
    "\n",
    "    payload_bytes = json.dumps(payload).encode(\"utf-8\")\n",
    "\n",
    "    response = endpoint.raw_predict(\n",
    "        body=payload_bytes,\n",
    "        headers={\"Content-Type\": \"application/json\"}\n",
    "    )\n",
    "\n",
    "    result = json.loads(response.content)\n",
    "    generated_text = result[\"choices\"][0][\"text\"]\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"<s>[INST] Research Topic: \\\"Hybrid Quantum-Classical Algorithms for Scalable Variational Quantum Simulation of Strongly Correlated Materials\\\"\\nLet's think step by step:? [/INST]\"\n",
    "\n",
    "# Get response from base model\n",
    "base_response = get_response(endpoint, prompt, \"mistral-base-custom-container\")\n",
    "print(\"Base Model Response:\")\n",
    "print(base_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get response from LoRA adapter\n",
    "adapter_response = get_response(endpoint, prompt, \"researcher\")\n",
    "print(\"LoRA Adapter Response:\")\n",
    "print(adapter_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources created in this tutorial:\n",
    "\n",
    "**Resources to clean up:**\n",
    "1. **Vertex AI Endpoint**: Undeploy models and delete the endpoint\n",
    "2. **Vertex AI Model**: Delete from Model Registry\n",
    "3. **Artifact Registry**: Delete the Docker repository (optional)\n",
    "4. **Cloud Storage**: Delete uploaded model artifacts (optional)\n",
    "\n",
    "**Cost considerations:**\n",
    "- Running endpoints incur costs even with no traffic\n",
    "- GPU instances (g2-standard-12) are billed per second\n",
    "- Artifact Registry and Cloud Storage have storage costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to True to delete resources\n",
    "delete_resources = False\n",
    "\n",
    "if delete_resources:\n",
    "    # Undeploy model from endpoint and delete endpoint\n",
    "    endpoint.undeploy_all()\n",
    "    endpoint.delete()\n",
    "    print(\"Endpoint deleted.\")\n",
    "\n",
    "    # Delete the model from Model Registry\n",
    "    model.delete()\n",
    "    print(\"Model deleted.\")\n",
    "\n",
    "    # Delete Artifact Registry repository\n",
    "    # ! gcloud artifacts repositories delete {DOCKER_REPOSITORY} --location={LOCATION} --quiet\n",
    "    # print(\"Artifact Registry repository deleted.\")\n",
    "\n",
    "    # Optionally delete GCS artifacts\n",
    "    # ! gsutil -m rm -r gs://{BUCKET_NAME}/deployments/lora/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

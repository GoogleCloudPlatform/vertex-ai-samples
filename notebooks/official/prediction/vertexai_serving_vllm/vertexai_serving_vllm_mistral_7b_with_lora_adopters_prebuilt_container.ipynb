{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2026 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy LoRA Fine-tuned Models on Vertex AI with Prebuilt vLLM Container\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/prediction/vertexai_serving_vllm/vertexai_serving_vllm_mistral_7b_lora_prebuilt_container.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Fprediction%2Fvertexai_serving_vllm%2Fvertexai_serving_vllm_mistral_7b_lora_prebuilt_container.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>    \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/prediction/vertexai_serving_vllm/vertexai_serving_vllm_mistral_7b_lora_prebuilt_container.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/prediction/vertexai_serving_vllm/vertexai_serving_vllm_mistral_7b_lora_prebuilt_container.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://raw.githubusercontent.com/primer/octicons/refs/heads/main/icons/mark-github-24.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to deploy a LoRA (Low-Rank Adaptation) fine-tuned model on Vertex AI using the prebuilt vLLM serving container. LoRA is an efficient fine-tuning technique that allows you to adapt large language models with minimal additional parameters.\n",
    "\n",
    "### Models used in this Example\n",
    "\n",
    "**Base Model:** [Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)\n",
    "\n",
    "Mistral-7B-Instruct-v0.3 is an instruction-tuned version of the Mistral-7B model, optimized for following instructions and conversational tasks. It features a 32k context window and uses the v3 tokenizer with extended vocabulary.\n",
    "\n",
    "**LoRA Adapter:** [Research-Reasoner-7B-v0.3](https://huggingface.co/Raymond-dev-546730/Research-Reasoner-7B-v0.3)\n",
    "\n",
    "Research-Reasoner-7B-v0.3 is a LoRA adapter fine-tuned on top of Mistral-7B to enhance research planning and reasoning capabilities. It enables the model to provide structured, step-by-step research methodologies.\n",
    "\n",
    "### What you will learn\n",
    "\n",
    "In this tutorial, you will learn how to:\n",
    "\n",
    "* Download a base model and LoRA adapter from Hugging Face\n",
    "* Upload model artifacts to Google Cloud Storage\n",
    "* Configure vLLM serving arguments for LoRA deployment\n",
    "* Register the model in Vertex AI Model Registry\n",
    "* Deploy the model to a Vertex AI Endpoint\n",
    "* Run inference using both the base model and LoRA adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required packages\n",
    "\n",
    "Install the necessary Python packages:\n",
    "\n",
    "- **google-cloud-aiplatform**: The Vertex AI SDK for model deployment and management\n",
    "- **google-cloud-storage**: For uploading model artifacts to Google Cloud Storage\n",
    "- **transformers**: Hugging Face library for working with transformer models\n",
    "- **huggingface_hub**: For downloading models from the Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install --upgrade --quiet google-cloud-aiplatform google-cloud-storage transformers huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n",
    "from huggingface_hub import snapshot_download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "Authenticate your environment on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Google Cloud project information\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment).\n",
    "\n",
    "**Service Account Requirements:**\n",
    "\n",
    "For Vertex AI deployment, you need a service account with the following roles:\n",
    "- `roles/storage.objectViewer` - To read model artifacts from GCS buckets\n",
    "- `roles/aiplatform.user` - To deploy models to Vertex AI\n",
    "\n",
    "You can find your service account details in [IAM & Admin > Service Accounts](https://console.cloud.google.com/iam-admin/serviceaccounts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
    "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type: \"string\", placeholder: \"[your-bucket-name]\", isTemplate: true}\n",
    "if not BUCKET_NAME or BUCKET_NAME == \"[your-bucket-name]\":\n",
    "    BUCKET_NAME = f\"{PROJECT_ID}-vertex-ai-models\"\n",
    "\n",
    "# Service account for Vertex AI deployment - must have storage.objectViewer access to BUCKET_NAME\n",
    "# Format: your-service-account@your-project.iam.gserviceaccount.com\n",
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type: \"string\", placeholder: \"[your-service-account@your-project.iam.gserviceaccount.com]\", isTemplate: true}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
    "print(f\"Initialized Vertex AI for project: {PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper functions\n",
    "\n",
    "The following helper function uploads a local directory to Google Cloud Storage (GCS). This is needed because Vertex AI requires model artifacts to be stored in GCS for deployment. The function:\n",
    "\n",
    "1. Parses the GCS URI to extract the bucket name and path prefix\n",
    "2. Walks through all files in the local directory\n",
    "3. Uploads each file to the corresponding path in GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_directory_to_gcs(local_path, gcs_uri):\n",
    "    \"\"\"Upload a directory to GCS.\"\"\"\n",
    "    gcs_path = gcs_uri.replace(\"gs://\", \"\")\n",
    "    bucket_name = gcs_path.split(\"/\")[0]\n",
    "    prefix = \"/\".join(gcs_path.split(\"/\")[1:])\n",
    "\n",
    "    client = storage.Client(project=PROJECT_ID)\n",
    "    bucket = client.bucket(bucket_name)\n",
    "\n",
    "    for root, dirs, files in os.walk(local_path):\n",
    "        for file in files:\n",
    "            local_file = os.path.join(root, file)\n",
    "            relative_path = os.path.relpath(local_file, local_path)\n",
    "            blob_name = f\"{prefix}/{relative_path}\"\n",
    "\n",
    "            blob = bucket.blob(blob_name)\n",
    "            blob.upload_from_filename(local_file)\n",
    "            print(f\"Uploaded {relative_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure model paths\n",
    "\n",
    "Define the Hugging Face model identifiers and construct the GCS paths where the models will be stored.\n",
    "\n",
    "- **BASE_MODEL_NAME**: The Hugging Face repository ID for the base Mistral model\n",
    "- **LORA_MODEL_ADAPTER**: The Hugging Face repository ID for the LoRA adapter\n",
    "- **GCS_BASE_MODEL_URI**: The GCS path where the base model will be uploaded\n",
    "- **GCS_ADAPTER_URI**: The GCS path where the LoRA adapter will be uploaded\n",
    "\n",
    "The models are organized in a hierarchical structure in GCS, with adapters stored under the base model directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "LORA_MODEL_ADAPTER = \"Raymond-dev-546730/Research-Reasoner-7B-v0.3\"\n",
    "\n",
    "GCS_BASE_MODEL_PATH = f\"{BUCKET_NAME}/deployments/lora/models/{BASE_MODEL_NAME}\"\n",
    "GCS_ADAPTER_PATH = f\"{GCS_BASE_MODEL_PATH}/adapters/{LORA_MODEL_ADAPTER}\"\n",
    "\n",
    "GCS_BASE_MODEL_URI = f\"gs://{GCS_BASE_MODEL_PATH}\"\n",
    "GCS_ADAPTER_URI = f\"{GCS_BASE_MODEL_URI}/adapters/{LORA_MODEL_ADAPTER}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the base model\n",
    "\n",
    "Download the Mistral-7B-Instruct-v0.3 base model from Hugging Face Hub using `snapshot_download()`. This function:\n",
    "\n",
    "- Downloads all model files (weights, tokenizer, config) to your local cache\n",
    "- Returns the local path where the model is stored\n",
    "- Supports resumable downloads if interrupted\n",
    "\n",
    "**Note**: The Mistral-7B model is approximately 14GB in size. Ensure you have sufficient disk space and bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_base_model_path = snapshot_download(repo_id=BASE_MODEL_NAME)\n",
    "print(f\"Base model downloaded to: {local_base_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the LoRA adapter\n",
    "\n",
    "Download only the LoRA adapter weights from the Research-Reasoner repository. The `allow_patterns` parameter filters the download to include only the LoRA adapter files, which are much smaller than the full model (typically a few hundred MB).\n",
    "\n",
    "LoRA adapters contain:\n",
    "- **adapter_config.json**: Configuration specifying the LoRA rank, alpha, and target modules\n",
    "- **adapter_model.safetensors**: The trained low-rank weight matrices\n",
    "- **Tokenizer files**: If the adapter uses a modified tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_repo_path = snapshot_download(\n",
    "    repo_id=LORA_MODEL_ADAPTER,\n",
    "    allow_patterns=[\n",
    "        \"Model_Weights/LoRA_adapter/**\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "local_adapter_path = f\"{local_repo_path}/Model_Weights/LoRA_adapter\"\n",
    "print(f\"Adapter downloaded to: {local_adapter_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload model and adapter to GCS\n",
    "\n",
    "Upload both the base model and LoRA adapter to Google Cloud Storage. Vertex AI will load these artifacts during container startup.\n",
    "\n",
    "**Why GCS?**\n",
    "- Vertex AI prediction containers automatically mount GCS paths to `/tmp/model_dir/`\n",
    "- This allows the vLLM server to access the model weights without bundling them in the container\n",
    "- Separating model storage from the container enables easier model updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_directory_to_gcs(local_base_model_path, GCS_BASE_MODEL_URI)\n",
    "upload_directory_to_gcs(local_adapter_path, GCS_ADAPTER_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy with prebuilt vLLM container\n",
    "\n",
    "This section deploys the model using Google's prebuilt vLLM serving container from Vertex AI Model Garden.\n",
    "\n",
    "**Architecture:** Prebuilt vLLM Container + Single LoRA loaded on startup.\n",
    "\n",
    "**Why use the prebuilt container?**\n",
    "- No need to build or maintain custom Docker images\n",
    "- Optimized for Vertex AI with proper health checks and logging\n",
    "- Regularly updated with the latest vLLM optimizations\n",
    "- Supports LoRA adapters out of the box\n",
    "\n",
    "The container URI points to Google's Artifact Registry where the prebuilt vLLM image is hosted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20251216_0916_RC01\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure vLLM serving arguments\n",
    "\n",
    "Configure the vLLM server with optimal settings for serving the Mistral model with LoRA.\n",
    "\n",
    "**Key parameters explained:**\n",
    "\n",
    "| Parameter | Value | Description |\n",
    "|-----------|-------|-------------|\n",
    "| `--model` | GCS URI | Path to the base model in GCS |\n",
    "| `--served-model-name` | mistral-base | Name used to reference the base model in API calls |\n",
    "| `--tensor-parallel-size` | 1 | Number of GPUs for tensor parallelism (1 for single L4) |\n",
    "| `--gpu-memory-utilization` | 0.90 | Fraction of GPU memory to use (90%) |\n",
    "| `--max-model-len` | 8192 | Maximum sequence length for generation |\n",
    "| `--max-num-seqs` | 64 | Maximum concurrent sequences for batching |\n",
    "| `--dtype` | bfloat16 | Use bfloat16 for faster inference with minimal quality loss |\n",
    "\n",
    "**LoRA-specific parameters:**\n",
    "\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `--enable-lora` | Enable LoRA adapter support |\n",
    "| `--max-loras` | Maximum number of LoRA adapters to load in GPU memory |\n",
    "| `--max-cpu-loras` | Maximum adapters to keep in CPU memory for swapping |\n",
    "| `--max-lora-rank` | Maximum LoRA rank supported (64 for this adapter) |\n",
    "| `--lora-modules` | Register adapter with format `name=path` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_ADAPTER_PATH = f\"/tmp/model_dir/{GCS_ADAPTER_PATH}\"\n",
    "\n",
    "max_model_len = 8192\n",
    "gpu_memory_utilization = 0.90\n",
    "max_num_seqs = 64\n",
    "\n",
    "vllm_args = [\n",
    "    \"python\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n",
    "    \"--host=0.0.0.0\",\n",
    "    \"--port=8080\",\n",
    "    f\"--model={GCS_BASE_MODEL_URI}\",\n",
    "    \"--served-model-name=mistral-base\",\n",
    "    \"--tensor-parallel-size=1\",\n",
    "    \"--swap-space=16\",\n",
    "    f\"--gpu-memory-utilization={gpu_memory_utilization}\",\n",
    "    f\"--max-model-len={max_model_len}\",\n",
    "    f\"--max-num-seqs={max_num_seqs}\",\n",
    "    \"--dtype=bfloat16\",\n",
    "    # LoRA configuration\n",
    "    \"--enable-lora\",\n",
    "    \"--max-loras=1\",\n",
    "    \"--max-cpu-loras=1\",\n",
    "    \"--max-lora-rank=64\",\n",
    "    f\"--lora-modules=researcher={LOCAL_ADAPTER_PATH}\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload model to Vertex AI Model Registry\n",
    "\n",
    "Register the model in Vertex AI Model Registry. This creates a model resource that can be deployed to endpoints.\n",
    "\n",
    "**Key configuration options:**\n",
    "\n",
    "- **serving_container_image_uri**: The prebuilt vLLM Docker image\n",
    "- **serving_container_args**: Command-line arguments passed to vLLM\n",
    "- **serving_container_ports**: Port 8080 for the OpenAI-compatible API\n",
    "- **serving_container_predict_route**: `/v1/completions` for the completions endpoint\n",
    "- **serving_container_health_route**: `/health` for Vertex AI health checks\n",
    "- **serving_container_shared_memory_size_mb**: 16GB shared memory for model loading\n",
    "- **serving_container_deployment_timeout**: 2 hours to allow for large model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=\"mistral-7B-instruct-v0.3-with-lora-adapter-prebuilt\",\n",
    "    serving_container_image_uri=VLLM_DOCKER_URI,\n",
    "    serving_container_args=vllm_args,\n",
    "    serving_container_ports=[8080],\n",
    "    serving_container_predict_route=\"/v1/completions\",\n",
    "    serving_container_health_route=\"/health\",\n",
    "    serving_container_shared_memory_size_mb=(16 * 1024),\n",
    "    serving_container_deployment_timeout=7200,\n",
    ")\n",
    "print(f\"Model uploaded: {model.resource_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model to endpoint\n",
    "\n",
    "Deploy the model to a Vertex AI Endpoint for online predictions.\n",
    "\n",
    "**Hardware configuration:**\n",
    "\n",
    "- **machine_type**: `g2-standard-12` provides 12 vCPUs, 48GB RAM, and 1 NVIDIA L4 GPU\n",
    "- **accelerator_type**: `NVIDIA_L4` - A cost-effective GPU with 24GB memory, suitable for 7B parameter models\n",
    "- **accelerator_count**: 1 GPU (matching `tensor-parallel-size=1`)\n",
    "\n",
    "**Scaling configuration:**\n",
    "\n",
    "- **min_replica_count**: 1 - Always keep at least one replica running\n",
    "- **max_replica_count**: 1 - Fixed scaling for this example (increase for production)\n",
    "- **deploy_request_timeout**: 1800 seconds (30 minutes) for deployment to complete\n",
    "\n",
    "**Note**: Deployment takes approximately 10-15 minutes as the container starts, downloads model weights from GCS, and loads them into GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = model.deploy(\n",
    "    machine_type=\"g2-standard-12\",  # 1x NVIDIA L4\n",
    "    accelerator_type=\"NVIDIA_L4\",\n",
    "    accelerator_count=1,\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1,\n",
    "    sync=True,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    deploy_request_timeout=1800\n",
    ")\n",
    "print(f\"Model deployed to endpoint: {endpoint.resource_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference\n",
    "\n",
    "Test the deployed endpoint by sending prompts to both the base model and the LoRA adapter.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "The vLLM server exposes an OpenAI-compatible API. You can select which model to use via the `model` field in your request:\n",
    "- `\"model\": \"mistral-base\"` - Uses the base Mistral model without any adapter\n",
    "- `\"model\": \"researcher\"` - Uses the base model with the Research-Reasoner LoRA adapter applied\n",
    "\n",
    "The `raw_predict` method sends HTTP requests directly to the container, bypassing Vertex AI's standard prediction format.\n",
    "\n",
    "**Request parameters:**\n",
    "\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `model` | Model name to use (base or LoRA adapter name) |\n",
    "| `prompt` | Input text to complete |\n",
    "| `max_tokens` | Maximum tokens to generate |\n",
    "| `temperature` | Controls randomness (0.0 = deterministic) |\n",
    "| `top_p` | Nucleus sampling parameter |\n",
    "| `top_k` | Top-k sampling parameter |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(endpoint, prompt, model_name):\n",
    "    \"\"\"Send a prompt to the model and get a response.\"\"\"\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"temperature\": 0.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"top_k\": 1\n",
    "    }\n",
    "\n",
    "    payload_bytes = json.dumps(payload).encode(\"utf-8\")\n",
    "\n",
    "    response = endpoint.raw_predict(\n",
    "        body=payload_bytes,\n",
    "        headers={\"Content-Type\": \"application/json\"}\n",
    "    )\n",
    "\n",
    "    result = json.loads(response.content)\n",
    "    generated_text = result[\"choices\"][0][\"text\"]\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENDPOINT_ID = \"8812523024461856768\"\n",
    "# endpoint = aiplatform.Endpoint(f\"projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/{ENDPOINT_ID}\")\n",
    "# print(f\"Using endpoint: {endpoint.resource_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"<s>[INST] Research Topic: \\\"Hybrid Quantum-Classical Algorithms for Scalable Variational Quantum Simulation of Strongly Correlated Materials\\\"\\nLet's think step by step:? [/INST]\"\n",
    "\n",
    "\n",
    "# Get response from base model\n",
    "base_response = get_response(endpoint, prompt, \"mistral-base\")\n",
    "print(\"Base Model Response:\")\n",
    "print(base_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get response from LoRA adapter\n",
    "adapter_response = get_response(endpoint, prompt, \"researcher\")\n",
    "print(\"LoRA Adapter Response:\")\n",
    "print(adapter_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources created in this tutorial:\n",
    "\n",
    "**Resources to clean up:**\n",
    "1. **Vertex AI Endpoint**: Undeploy models and delete the endpoint\n",
    "2. **Vertex AI Model**: Delete from Model Registry\n",
    "3. **Cloud Storage**: Delete uploaded model artifacts (optional)\n",
    "\n",
    "**Cost considerations:**\n",
    "- Running endpoints incur costs even with no traffic\n",
    "- GPU instances (g2-standard-12) are billed per second while the endpoint is active\n",
    "- Cloud Storage has minimal storage costs for model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to True to delete resources\n",
    "delete_resources = True\n",
    "\n",
    "if delete_resources:\n",
    "    # Undeploy model from endpoint and delete endpoint\n",
    "    endpoint.undeploy_all()\n",
    "    endpoint.delete()\n",
    "    print(\"Endpoint deleted.\")\n",
    "\n",
    "    # Delete the model from Model Registry\n",
    "    model.delete()\n",
    "    print(\"Model deleted.\")\n",
    "\n",
    "    # Optionally delete GCS artifacts\n",
    "    # ! gsutil -m rm -r gs://{BUCKET_NAME}/deployments/lora/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

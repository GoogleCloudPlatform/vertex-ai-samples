{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f705f4be70e9"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c9b0667d10c"
      },
      "source": [
        "# Serving Open Models on Vertex AI using vLLM with CPU\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/prediction/vertexai_serving_vllm/vertexai_serving_vllm_cpu_llama3_2_3B.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Fprediction%2Fvertexai_serving_vllm%2Fvertexai_serving_vllm_cpu_llama3_2_3B.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/prediction/vertexai_serving_vllm/vertexai_serving_vllm_cpu_llama3_2_3B.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24d72ac84336"
      },
      "source": [
        "## Overview\n",
        "\n",
        "There are multiple ways of serving open models (including open source and open weight) such as Llama 3.2 on Google Cloud Vertex AI. The Llama models are available in [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/llama) and Model Garden allows a single click self-deployment of the models. This notebooks demonstrates how Llama 3.2 3B model can be served via Vertex AI Endpoint using a custom vLLM container image built for the CPU. This notebook does the following:\n",
        "\n",
        "- Builds a custom docker container image using vLLM source code\n",
        "- Uploads the model to Model Registry using custom docker container image\n",
        "- Creates a public Endpoint for Online Prediction\n",
        "- Deploys model to the Endpoint\n",
        "- Llama 3.2 3B model is downloaded from Hugging Face during deployment\n",
        "- This custom container image can also be used for downloading model from Google Storage\n",
        "\n",
        "The code in this notebook can be used for serving other open models supported by vLLM. This notebook has been tested with Python 3.10 and `google-cloud-aiplatform` SDK Version `1.106.0`.\n",
        "\n",
        "To download the models from the Hugging Face, you need a Hugging Face token.\n",
        "  1.  Create a [Hugging Face account](https://huggingface.co/) if you don't have one.\n",
        "  2.  For **gated models** like Llama 3.2, ensure you have requested and been granted access on Hugging Face before proceeding.\n",
        "  3.  Generate an Access Token: Go to **Your Profile > Settings > Access Tokens**.\n",
        "  4.  Select **New Token**.\n",
        "  5.  Specify a Name and a Role of at least Read.\n",
        "  6.  Select **Generate a token**.\n",
        "  7.  Set the token in hf_token env below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36df818a34bd"
      },
      "source": [
        "## Get Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26645caf62fe"
      },
      "source": [
        "### Install Vertex AI SDK for Python and other required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4cf289f0d99"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --quiet google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "848322ec177e"
      },
      "source": [
        "### Restart runtime (Colab only)\n",
        "\n",
        "To use the newly installed packages, you must restart the runtime on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8d49bb74a53"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f332441fe51"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11947ae0fe5e"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "Authenticate your environment on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "015bf6d5da75"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "722a10c66085"
      },
      "source": [
        "### Set Google Cloud project and initialize Vertex AI SDK for Python\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66156945acb1"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "DEVICE_TYPE = \"cpu\"  # @param {type:\"string\"}\n",
        "\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "202ff51ba5de"
      },
      "source": [
        "Set Project ID in active gcloud configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc8b4506d7ea"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cce406746d5"
      },
      "source": [
        "## Create vLLM Customer Container Image for Vertex AI\n",
        "\n",
        "Vertex AI requires [requests](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#inference) and [responses](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#response_requirements) in specific formats. vLLM API server implements OpenAI API protocol and therefore, it does not support the Vertex AI request and response requirements. Therefore, the vLLM API server (vllm.entrypoints.openai.api_server.py) needs to be updated to support Vertex AI request and response formats."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f7ec1c7bdf8"
      },
      "source": [
        "### Enable Artifact Registry API\n",
        "Enable the Artifact Registry API service for the Google cloud project. This tutorial requires [gcloud CLI](https://cloud.google.com/sdk/docs/install) installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e93c586de15"
      },
      "outputs": [],
      "source": [
        "! gcloud components update --quiet && gcloud services enable artifactregistry.googleapis.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c4e1d4d5e66"
      },
      "source": [
        "### Create a private Docker repository\n",
        "Create a Docker repository in [Artifact Registry](https://cloud.google.com/artifact-registry/docs/overview)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f21a9e18e5c"
      },
      "outputs": [],
      "source": [
        "DOCKER_REPOSITORY = \"my-docker-repo\"\n",
        "! gcloud artifacts repositories create {DOCKER_REPOSITORY} --repository-format=docker --location={LOCATION} --description=\"Vertex AI Docker repository\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "479fbec1d5e0"
      },
      "source": [
        "### Build vLLM Custom Docker Container Image for CPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bed9ed248a5e"
      },
      "source": [
        "Clone vertex-ai-samples code reposistory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13df52a033d3"
      },
      "outputs": [],
      "source": [
        "! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b89cf1a29b80"
      },
      "source": [
        "Build image using Cloud Build"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8c00680ff433"
      },
      "outputs": [],
      "source": [
        "! cd vertex-ai-samples/notebooks/official/prediction/vertexai_serving_vllm/cloud-build \\\n",
        "    && gcloud builds submit --config=cloudbuild.yaml --region={LOCATION} --timeout \"2h\" --machine-type=e2-highcpu-32 --substitutions=_REPOSITORY={DOCKER_REPOSITORY},_DEVICE_TYPE={DEVICE_TYPE},_BASE_IMAGE=vllm-cpu-base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95c5ddf6f5c6"
      },
      "source": [
        "## Deploy Model to Vertex AI Endpoint\n",
        "\n",
        "Following steps are required to serve model via a Vertex AI Prediction Endpoint:\n",
        "- import model to model registry\n",
        "- create a Online Prediction Endpoint\n",
        "- Deploy the model to endpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd851913ae85"
      },
      "source": [
        "### Define Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b6b731880ed"
      },
      "outputs": [],
      "source": [
        "hf_token = \"[your-hugging-face-auth-token]\"  # @param {type:\"string\"}\n",
        "model_name = \"cpu-llama3_2_3B-serve-vllm\"  # @param {type:\"string\"}\n",
        "model_id = \"meta-llama/Llama-3.2-3B\"  # @param {type:\"string\"}\n",
        "machine_type = \"c2-standard-16\"  # @param {type:\"string\"}\n",
        "DOCKER_URI = (\n",
        "    f\"{LOCATION}-docker.pkg.dev/{PROJECT_ID}/{DOCKER_REPOSITORY}/vllm-{DEVICE_TYPE}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0c1bb82c9e0"
      },
      "source": [
        "### Import model to Model Registry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d36036e3f6f7"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "\n",
        "def upload_model(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    hf_token: str,\n",
        "    docker_uri: str,\n",
        ") -> aiplatform.Model:\n",
        "\n",
        "    vllm_args = [\n",
        "        \"python3\",\n",
        "        \"-m\",\n",
        "        \"vllm.entrypoints.openai.api_server\",\n",
        "        \"--host=0.0.0.0\",\n",
        "        \"--port=8080\",\n",
        "        f\"--model={model_id}\",\n",
        "        \"--max-model-len=2048\",\n",
        "    ]\n",
        "\n",
        "    env_vars = {\n",
        "        \"HF_TOKEN\": hf_token,\n",
        "    }\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=docker_uri,\n",
        "        serving_container_args=vllm_args,\n",
        "        serving_container_ports=[8080],\n",
        "        serving_container_predict_route=\"/v1/completions\",\n",
        "        serving_container_health_route=\"/health\",\n",
        "        serving_container_environment_variables=env_vars,\n",
        "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
        "        serving_container_deployment_timeout=1800,\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "vertexai_model = upload_model(\n",
        "    model_name=model_name, model_id=model_id, hf_token=hf_token, docker_uri=DOCKER_URI\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "124c8611bab3"
      },
      "source": [
        "### Create Vertex AI Endpoint for Online Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91591af558cf"
      },
      "outputs": [],
      "source": [
        "def create_model_endpoint(model_name: str) -> aiplatform.Endpoint:\n",
        "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
        "    return endpoint\n",
        "\n",
        "\n",
        "vertexai_endpoint = create_model_endpoint(model_name=model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15a1ba904c6e"
      },
      "source": [
        "### Deploy Model to Endpoint\n",
        "**NOTE**: The model deployment may take around 30 minutes to complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f77f1ede9afb"
      },
      "outputs": [],
      "source": [
        "def deploy_model(\n",
        "    model: aiplatform.Model,\n",
        "    endpoint: aiplatform.Endpoint,\n",
        "    model_name: str,\n",
        "    machine_type: str,\n",
        "):\n",
        "    print(\n",
        "        f\"Deploying {model_name} to endpoint: {endpoint.resource_name} using machine type: {machine_type}\"\n",
        "    )\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        deployed_model_display_name=model_name,\n",
        "        machine_type=machine_type,\n",
        "        min_replica_count=1,\n",
        "        max_replica_count=4,\n",
        "        autoscaling_target_cpu_utilization=60,\n",
        "        traffic_percentage=100,\n",
        "        deploy_request_timeout=1800,\n",
        "    )\n",
        "\n",
        "\n",
        "deploy_model(\n",
        "    model=vertexai_model,\n",
        "    endpoint=vertexai_endpoint,\n",
        "    model_name=model_name,\n",
        "    machine_type=machine_type,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13f1059160b9"
      },
      "source": [
        "## Test Endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ec42d0c4422"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "PROMPT = \"Distance of moon from earth is \"\n",
        "request_body = json.dumps(\n",
        "    {\n",
        "        \"prompt\": PROMPT,\n",
        "        \"temperature\": 0.0,\n",
        "    },\n",
        ")\n",
        "\n",
        "raw_response = vertexai_endpoint.raw_predict(\n",
        "    body=request_body, headers={\"Content-Type\": \"application/json\"}\n",
        ")\n",
        "assert raw_response.status_code == 200\n",
        "result = json.loads(raw_response.text)\n",
        "\n",
        "for choice in result[\"choices\"]:\n",
        "    print(choice)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdc1973ace2f"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, delete the resources created in this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c8cc0c481e8"
      },
      "source": [
        "### Delete Vertex AI Prediction Endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6615b3d9888d"
      },
      "outputs": [],
      "source": [
        "vertexai_endpoint.delete(force=True, sync=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebd767d8e848"
      },
      "source": [
        "### Delete Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2089ddbdaf35"
      },
      "outputs": [],
      "source": [
        "vertexai_model.delete(sync=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e35ebc98df3"
      },
      "source": [
        "### Delete private docker repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "795a05a169bc"
      },
      "outputs": [],
      "source": [
        "! gcloud artifacts repositories delete {DOCKER_REPOSITORY} --location={LOCATION} --quiet"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "vertexai_serving_vllm_cpu_llama3_2_3B.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

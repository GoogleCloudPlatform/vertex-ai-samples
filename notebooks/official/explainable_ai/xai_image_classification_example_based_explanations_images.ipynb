{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Get started with Vertex Explainable AI Example Based API - Custom training image classification model\n",
        "\n",
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/explainable_ai/get_started_with_vertex_xai_example_based_images.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/explainable_ai/get_started_with_vertex_xai_example_based_images.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/explainable_ai/get_started_with_vertex_xai_example_based_images.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24743cf4a1e1"
      },
      "source": [
        "**_NOTE_**: This notebook has been tested in the following environment:\n",
        "\n",
        "* Python version = 3.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates how to get example-based explanations for an image classification model. With example-based explanations, Vertex AI uses nearest neighbor search to return a list of examples (typically from the training set) that are most similar to the input.\n",
        "\n",
        "Learn more about [Vertex Explainable AI](https://cloud.google.com/vertex-ai/docs/explainable-ai/overview)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d975e698c9a4"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to get Example-Based explanations from Vertex Explainable AI services.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services and resources:\n",
        "\n",
        "- `Vertex AI Model Registry`\n",
        "- `Vertex Explainable AI`\n",
        "- `Vertex AI Prediction`\n",
        "\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Prepare training data\n",
        "- Fine tune a image classication model to get embeddings\n",
        "- Register the model in Vertex AI Model Registry\n",
        "- Deploy the model in Vertex AI Endpoint\n",
        "- Request explanations using Example-Based Explanation API\n",
        "- Analyze the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08d289fa873f"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "For this notebook, you use the [beans dataset](https://github.com/AI-Lab-Makerere/ibean/) downloaded through [TF Datasets](https://www.tensorflow.org/datasets/catalog/beans)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aed92deeb4a0"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing)\n",
        "and [Cloud Storage pricing](https://cloud.google.com/storage/pricing),\n",
        "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7EUnXsZhAGF"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Install the following packages required to execute this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b4ef9b72d43"
      },
      "outputs": [],
      "source": [
        "# Install the packages.\n",
        "# Notice Tensorflow version has to be alligned with Vertex AI prebuild serving container.\n",
        "\n",
        "USER = \"\"\n",
        "! pip3 install {USER} --upgrade numpy tensorflow_datasets tensorflow==2.11.0  -q --no-warn-conflicts\n",
        "! pip3 install {USER} --upgrade google-cloud-aiplatform -q --no-warn-conflicts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58707a750154"
      },
      "source": [
        "### Colab only: Uncomment the following cell to restart the kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f200f10a1da3"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "# import IPython\n",
        "\n",
        "# app = IPython.Application.instance()\n",
        "# app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF1j6f9HApxa"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "3. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "4. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WReHDGG5g0XY"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, try the following:\n",
        "* Run `gcloud config list`.\n",
        "* Run `gcloud projects list`.\n",
        "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM1iC_MfAts1"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ie2gspgMndAu"
      },
      "outputs": [],
      "source": [
        "REGION = \"us-central1\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBCra4QMA2wR"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74ccc9e52986"
      },
      "source": [
        "**1. Vertex AI Workbench**\n",
        "* Do nothing as you are already authenticated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de775a3773ba"
      },
      "source": [
        "**2. Local JupyterLab instance, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "254614fa0c46"
      },
      "outputs": [],
      "source": [
        "# ! gcloud auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef21552ccea8"
      },
      "source": [
        "**3. Colab, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "603adbbf0532"
      },
      "outputs": [],
      "source": [
        "# from google.colab import auth\n",
        "# auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6b2ccc891ed"
      },
      "source": [
        "**4. Service account or other**\n",
        "* See how to grant Cloud Storage permissions to your service account at https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgPO1eR3CYjk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "Create a storage bucket to store intermediate artifacts such as datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzGDU7TWdts_"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = f\"your-bucket-name-{PROJECT_ID}-unique\" # @param {type:\"string\"}\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EcIXiGsCePi"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIq7R4HZCfIc"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlICvTdOtyLN"
      },
      "source": [
        "### Set up project template\n",
        "Set the folder you use in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxRaeiQIt6kn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "TUTORIAL_DIR = os.path.join(os.getcwd(), \"sdk_xai_example_based_tutorial\")\n",
        "DATA_DIR = os.path.join(TUTORIAL_DIR, \"data\")\n",
        "MODEL_DIR = os.path.join(TUTORIAL_DIR, \"model\")\n",
        "\n",
        "for path in TUTORIAL_DIR, DATA_DIR, MODEL_DIR:\n",
        "    os.makedirs(path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set the dataset and the model to explain\n",
        "\n",
        "Indicate the dataset and the bucket uri of the model to explain"
      ],
      "metadata": {
        "id": "0h4g7Pmoswhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_NAME = \"beans\"\n",
        "MODEL_FILE_NAME = f\"mobilenetv2-{DATASET_NAME}.tar.gz\"\n",
        "SOURCE_MODEL_URI = BUCKET_URI + \"/model/\" + MODEL_FILE_NAME"
      ],
      "metadata": {
        "id": "cAf66fVLs6Kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "960505627ddf"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "outputs": [],
      "source": [
        "# General\n",
        "import io\n",
        "from PIL import Image\n",
        "import base64\n",
        "from io import BytesIO\n",
        "from google.cloud import storage\n",
        "import tarfile\n",
        "import json\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Vertex AI\n",
        "from google.cloud import aiplatform as vertex_ai\n",
        "from google.cloud import aiplatform_v1beta1 as vertex_ai_v1beta1\n",
        "from google.cloud.aiplatform_v1beta1.types import io as io_pb2\n",
        "from google.protobuf import json_format\n",
        "from google.protobuf.struct_pb2 import Value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3yttwolcarS"
      },
      "source": [
        "### Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvxP35URk-MQ"
      },
      "outputs": [],
      "source": [
        "# API service endpoint\n",
        "API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
        "\n",
        "# Vertex location root path for your dataset, model and endpoint resources\n",
        "PARENT = \"projects/\" + PROJECT_ID + \"/locations/\" + REGION\n",
        "\n",
        "# Training\n",
        "CHANNELS = 3\n",
        "SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "NUM_BATCHES = -1\n",
        "\n",
        "# Serving\n",
        "MODEL_SOURCE_FILE_NAME = \"model/\" + MODEL_FILE_NAME\n",
        "MODEL_DESTINATION_FILE_NAME = os.path.join(MODEL_DIR, MODEL_FILE_NAME)\n",
        "MODEL_FOLDER_DIR = os.path.join(MODEL_DIR, f\"mobilenetv2-{DATASET_NAME}\")\n",
        "ENBEDDINGS_URI = BUCKET_URI + f\"/model/mobilenetv2-{DATASET_NAME}\"\n",
        "TRAIN_DATASET_FILE = DATASET_NAME + \"train-images.jsonl\"\n",
        "TRAIN_SOURCE_JSON_PATH = os.path.join(DATA_DIR, TRAIN_DATASET_FILE)\n",
        "TRAIN_DESTINATION_JSON_PATH = 'data/' + TRAIN_DATASET_FILE\n",
        "TRAIN_DATASET_URI = BUCKET_URI + '/' + TRAIN_DESTINATION_JSON_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzwU0n_Wk1u6"
      },
      "source": [
        "### Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ypLir2Rk6T5"
      },
      "outputs": [],
      "source": [
        "def create_index_to_name_map(ds_info):\n",
        "    \"\"\"\n",
        "    Creates a map from label name to numerical index.\n",
        "    Args:\n",
        "        ds_info: DatasetInfo object.\n",
        "    Returns:\n",
        "        index_to_name_map: dict. Map from name to index.\n",
        "    \"\"\"\n",
        "    index_to_name = {}\n",
        "    num_classes = ds_info.features[\"label\"].num_classes\n",
        "    names = ds_info.features[\"label\"].names\n",
        "    for i in range(num_classes):\n",
        "        index_to_name[i] = names[i]\n",
        "    return index_to_name\n",
        "\n",
        "\n",
        "def extract_images_and_labels(ds, num_batches):\n",
        "    \"\"\"\n",
        "    Extract images and labels from a dataset.\n",
        "    Args:\n",
        "        ds: A dataset.\n",
        "        num_batches: The number of batches to extract. -1 uses the whole dataset\n",
        "    Returns:\n",
        "        images: A numpy structure of images.\n",
        "        labels: A numpy structure of labels.\n",
        "    \"\"\"\n",
        "    data_slice = ds.take(num_batches)\n",
        "    images = []\n",
        "    labels = []\n",
        "    for image, label in data_slice:\n",
        "        images.append(image)\n",
        "        labels.append(label)\n",
        "    images = tf.concat(images, 0)\n",
        "    labels = tf.concat(labels, 0)\n",
        "    return images.numpy(), labels.numpy()\n",
        "\n",
        "def get_instance(index, image):\n",
        "    \"\"\"\n",
        "    Get the instance to send to the model\n",
        "    Args:\n",
        "        index: The index associated with image\n",
        "        image: The image to send to the model\n",
        "    Returns:\n",
        "        The instance to send to the model\n",
        "    \"\"\"\n",
        "    img_bytes = io.BytesIO()\n",
        "    img = Image.fromarray(image.astype(np.uint8))\n",
        "    img.save(img_bytes, format=\"PNG\")\n",
        "    instance = {\n",
        "                \"id\": str(index),\n",
        "                \"bytes_inputs\": {\n",
        "                    \"b64\": base64.b64encode(img_bytes.getvalue()).decode(\"utf-8\")\n",
        "                },\n",
        "              }\n",
        "    return instance\n",
        "\n",
        "def write_jsonl(saved_jsonl_path, images):\n",
        "    \"\"\"\n",
        "    Write the jsonl file to send to the model\n",
        "    Args:\n",
        "        saved_jsonl_path: The path to save the jsonl file\n",
        "        images: The images to send to the model\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    with open(saved_jsonl_path, \"w\") as f:\n",
        "        for i, im in enumerate(images):\n",
        "            instance = get_instance(i, im)\n",
        "            json.dump(\n",
        "                instance,\n",
        "                f,\n",
        "            )\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "def upload_model(model_configuration):\n",
        "    \"\"\"\n",
        "    Upload the model to Vertex AI\n",
        "    Args:\n",
        "        model_configuration: The model configuration\n",
        "    Returns:\n",
        "        The uploaded model\n",
        "    \"\"\"\n",
        "\n",
        "    model = vertex_ai_v1beta1.Model(\n",
        "        display_name=model_configuration[\"display_name\"],\n",
        "        artifact_uri=model_configuration[\"artifact_uri\"],\n",
        "        metadata_schema_uri=model_configuration[\"metadata_schema_uri\"],\n",
        "        explanation_spec=model_configuration[\"explanation_spec\"],\n",
        "        container_spec=model_configuration[\"container_spec\"],\n",
        "    )\n",
        "\n",
        "    response = clients[\"model\"].upload_model(parent=PARENT, model=model)\n",
        "    print(\"Long running operation:\", response.operation.name)\n",
        "    uploaded_model = response.result(timeout=10000)\n",
        "    print(\"upload_model_response\")\n",
        "    print(\" model:\", uploaded_model)\n",
        "    return uploaded_model\n",
        "\n",
        "def create_endpoint(endpoint_config):\n",
        "    \"\"\"\n",
        "    Create an endpoint\n",
        "    Args:\n",
        "        endpoint_config: The endpoint configuration\n",
        "    Returns:\n",
        "        The created endpoint\n",
        "    \"\"\"\n",
        "    response = clients[\"endpoint\"].create_endpoint(parent=PARENT, endpoint=endpoint_config)\n",
        "    print(\"Long running operation:\", response.operation.name)\n",
        "    endpoint = response.result()\n",
        "    print(\"create_endpoint_response\")\n",
        "    print(\" endpoint:\", endpoint)\n",
        "    return endpoint\n",
        "\n",
        "def deploy_model(\n",
        "        model, endpoint, deploy_config\n",
        "):\n",
        "    \"\"\"\n",
        "    Deploy a model to an endpoint\n",
        "    Args:\n",
        "        model: The model to deploy\n",
        "        endpoint: The endpoint to deploy the model\n",
        "        deploy_config: The model deployment configuration\n",
        "    Returns:\n",
        "        The deployed model\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    if deploy_config[\"deploy_gpu\"]:\n",
        "        machine_spec = {\n",
        "            \"machine_type\": deploy_config[\"deploy_compute\"],\n",
        "            \"accelerator_type\": deploy_config[\"deploy_gpu\"],\n",
        "            \"accelerator_count\": deploy_config[\"deploy_ngpu\"],\n",
        "        }\n",
        "    else:\n",
        "        machine_spec = {\n",
        "            \"machine_type\": deploy_config[\"deploy_compute\"],\n",
        "        }\n",
        "\n",
        "    deployed_model = {\n",
        "        \"model\": model,\n",
        "        \"display_name\": deploy_config[\"deployed_model_display_name\"],\n",
        "        \"dedicated_resources\": {\n",
        "            \"min_replica_count\": deploy_config[\"min_nodes\"],\n",
        "            \"max_replica_count\": deploy_config[\"max_nodes\"],\n",
        "            \"machine_spec\": machine_spec,\n",
        "        },\n",
        "        \"enable_container_logging\": False,\n",
        "    }\n",
        "\n",
        "    response = clients[\"endpoint\"].deploy_model(\n",
        "        endpoint=endpoint, deployed_model=deployed_model, traffic_split=deploy_config[\"traffic_split\"]\n",
        "    )\n",
        "\n",
        "    print(\"Long running operation:\", response.operation.name)\n",
        "    deployed_model = response.result(timeout=10000)\n",
        "    print(\"deploy_model_response\")\n",
        "    print(\" deployed_model:\", deployed_model)\n",
        "\n",
        "    return deployed_model\n",
        "\n",
        "def explain_image(formatted_data, endpoint, parameters, deployed_model_id):\n",
        "    \"\"\"\n",
        "    Get example based explanations an image\n",
        "    Args:\n",
        "        formatted_data: The data to send to the model\n",
        "        endpoint: The endpoint to send the data to\n",
        "        parameters: The parameters to send to the model\n",
        "        deployed_model_id: The deployed model id\n",
        "    Returns:\n",
        "        The response from the model\n",
        "    \"\"\"\n",
        "\n",
        "    # The format of each instance should conform to the deployed model's prediction input schema.\n",
        "    instances_list = formatted_data\n",
        "    instances = [\n",
        "        json_format.ParseDict(instance, Value()) for instance in instances_list\n",
        "    ]\n",
        "\n",
        "    response = clients[\"prediction\"].explain(\n",
        "        endpoint=endpoint,\n",
        "        instances=instances,\n",
        "        parameters=parameters,\n",
        "        deployed_model_id=deployed_model_id,\n",
        "    )\n",
        "    print(\"response\")\n",
        "    print(\" deployed_model_id:\", response.deployed_model_id)\n",
        "    predictions = response.predictions\n",
        "    print(\"predictions\")\n",
        "    for prediction in predictions:\n",
        "        print(\" prediction:\", prediction)\n",
        "\n",
        "    explanations = response.explanations\n",
        "    print(\"explanations\")\n",
        "    for explanation in explanations:\n",
        "        print(\" explanation:\", explanation)\n",
        "\n",
        "    return response\n",
        "\n",
        "def plot_input_and_neighbors(\n",
        "    val_img_idx,\n",
        "    all_train_images,\n",
        "    val_images,\n",
        "    all_train_labels,\n",
        "    val_labels,\n",
        "    label_index_to_name,\n",
        "    data_with_neighbors,\n",
        "):\n",
        "    \"\"\"\n",
        "    Plot the input image and its neighbors.\n",
        "    Args:\n",
        "        val_img_idx: Index of the input image.\n",
        "        all_train_images: All training images.\n",
        "        val_images: Validation images.\n",
        "        all_train_labels: All training labels.\n",
        "        val_labels: Validation labels.\n",
        "        label_index_to_name: Dictionary mapping label indices to names.\n",
        "        data_with_neighbors: Data with neighbors.\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    image = val_images[val_img_idx]\n",
        "    fig = plt.figure(figsize=(24, 12))\n",
        "    ax_list = fig.subplots(3, 5)\n",
        "    ax_list[0, 0].axis(\"off\")\n",
        "    ax_list[0, 1].axis(\"off\")\n",
        "    ax_list[0, 3].axis(\"off\")\n",
        "    ax_list[0, 4].axis(\"off\")\n",
        "    ax = ax_list[0, 2]\n",
        "    class_label = val_labels[val_img_idx]\n",
        "    ax.set_title(\n",
        "        f\"{class_label}:{label_index_to_name[class_label]} (example index: {val_img_idx})\",\n",
        "        fontsize=15,\n",
        "    )\n",
        "    ax.axis(\"off\")\n",
        "    ax.imshow(image.astype(\"uint8\"))\n",
        "\n",
        "    neighbor_list = data_with_neighbors[val_img_idx][\"neighbors\"]\n",
        "    num_neighbors = len(neighbor_list)\n",
        "    for n in range(num_neighbors):\n",
        "        neighbor = neighbor_list[n]\n",
        "        neighbor_idx = int(neighbor[\"neighborId\"])\n",
        "        neighbor_dist = neighbor[\"neighborDistance\"]\n",
        "        ax = ax_list[1 + n // 5, n % 5]\n",
        "        class_label = all_train_labels[neighbor_idx]\n",
        "        ax.set_title(\n",
        "            f\"{class_label}:{label_index_to_name[class_label]} (dist: {neighbor_dist:.3f})\",\n",
        "            fontsize=15,\n",
        "        )\n",
        "        ax.axis(\"off\")\n",
        "        ax.imshow(all_train_images[neighbor_idx].astype(\"uint8\"))\n",
        "\n",
        "def undeploy_model(deployed_model_id, endpoint):\n",
        "    \"\"\"\n",
        "    Undeploy a model from an endpoint\n",
        "    Args:\n",
        "        deployed_model_id: The deployed model id\n",
        "        endpoint: The endpoint to undeploy the model from\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    response = clients[\"endpoint\"].undeploy_model(\n",
        "        endpoint=endpoint, deployed_model_id=deployed_model_id, traffic_split={}\n",
        "    )\n",
        "    print(response)\n",
        "\n",
        "def delete_endpoint(endpoint_id):\n",
        "    \"\"\"\n",
        "    Delete an endpoint\n",
        "    Args:\n",
        "        endpoint_id: The name of endpoint to delete\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    response = clients[\"endpoint\"].delete_endpoint(name=endpoint_id)\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HigWeoD1ndAw"
      },
      "outputs": [],
      "source": [
        "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAXCt-Ldplv2"
      },
      "source": [
        "### Set up clients\n",
        "\n",
        "The Vertex AI client library works as a client/server model. Then you need to set clients to use different services.\n",
        "\n",
        "You will use different clients in this tutorial for different steps in the workflow. So set them all up upfront.\n",
        "\n",
        "- Model Service for `Model` resources.\n",
        "- Endpoint Service for deployment.\n",
        "- Job Service for batch jobs and custom training.\n",
        "- Prediction Service for serving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vq5UsPfqPiKi"
      },
      "outputs": [],
      "source": [
        "# client options same for all services\n",
        "client_options = {\"api_endpoint\": API_ENDPOINT}\n",
        "\n",
        "\n",
        "def create_model_client():\n",
        "    client = vertex_ai_v1beta1.ModelServiceClient(client_options=client_options)\n",
        "    return client\n",
        "\n",
        "\n",
        "def create_endpoint_client():\n",
        "    client = vertex_ai_v1beta1.EndpointServiceClient(client_options=client_options)\n",
        "    return client\n",
        "\n",
        "\n",
        "def create_prediction_client():\n",
        "    client = vertex_ai_v1beta1.PredictionServiceClient(client_options=client_options)\n",
        "    return client\n",
        "\n",
        "\n",
        "clients = {}\n",
        "clients[\"model\"] = create_model_client()\n",
        "clients[\"endpoint\"] = create_endpoint_client()\n",
        "clients[\"prediction\"] = create_prediction_client()\n",
        "\n",
        "for client in clients.items():\n",
        "    print(client)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmquyf6oHQwB"
      },
      "source": [
        "## Working with Vertex Explainable Example-based API\n",
        "\n",
        "Vertex Explainable Example-based API provides an highly performant ANN service for returning similar examples to new predictions/instances.\n",
        "\n",
        "To leverage Vertex AI Example-based explanations, you need to cover the following steps:\n",
        "\n",
        "- Index the entire dataset: It requires to provide a path to an embedding model in a GCS bucket, training data stored in a GCS bucket and the config file for example-based explanation\n",
        "\n",
        "- Deploy index and model: You need to specify the machine to use and the model identifier from the model upload set\n",
        "\n",
        "- Query for similar examples: You need to make the explain query and model will return similar examples\n",
        "\n",
        "Below you use a `MobileNetV2` [Keras Application](https://keras.io/api/applications/) deep learning model that is available alongside pre-trained weights for fine-tuning the model which will be used to create embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gX-aXQYumVR"
      },
      "source": [
        "### Download and visualize the data\n",
        "\n",
        "Download the data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_ds, ds_info = tfds.load(\n",
        "    DATASET_NAME,\n",
        "    split=[\"train\", \"test\"],\n",
        "    as_supervised=True,\n",
        "    with_info=True,\n",
        "    shuffle_files=False,\n",
        "    data_dir=DATA_DIR,\n",
        ")\n",
        "train_ds, validation_ds = split_ds"
      ],
      "metadata": {
        "id": "uocPs8Ht1Sj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the dataset."
      ],
      "metadata": {
        "id": "LrCqBhSo3iYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfds.show_examples(ds=train_ds, ds_info=ds_info)"
      ],
      "metadata": {
        "id": "PA4867DF3mpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download the model to explain\n",
        "\n",
        "For your convenience, you copy and extract a pretrained MobileNetV2 model for this exercise."
      ],
      "metadata": {
        "id": "jACz3XE6HRZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! gsutil cp {SOURCE_MODEL_URI} {MODEL_DESTINATION_FILE_NAME}"
      ],
      "metadata": {
        "id": "4NkydSTjsB47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgeprhhXhCPq"
      },
      "outputs": [],
      "source": [
        "with tarfile.open(MODEL_DESTINATION_FILE_NAME) as file:\n",
        "  file.extractall(MODEL_DIR)\n",
        "  file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print a summary of the model to the console, including the number of layers and the number of neurons in each layer"
      ],
      "metadata": {
        "id": "Cc0NcisZIIpz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDawpX57zc9o"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model(MODEL_FOLDER_DIR)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k964kdaam4la"
      },
      "source": [
        "### Index the entire dataset\n",
        "\n",
        "To index the dataset you will use to get similar examples, you provide:\n",
        "\n",
        "- embedding model\n",
        "- training dataset\n",
        "- config file for example-based explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN8KlWBvpEIp"
      },
      "source": [
        "#### Extract embeddings\n",
        "\n",
        "To generate example-based explanations, you need to extract embedding model from the model you want to evaluate.\n",
        "\n",
        "In this case, you skip the data augmentation layer and drop the softmax layer to get to the embeddings from the model you previously trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53v6mwLU9lEi"
      },
      "outputs": [],
      "source": [
        "embedding_model = keras.Sequential()\n",
        "# Loop over layers\n",
        "for layer in model.layers[:-1]:\n",
        "    # Skip data augmentation layer\n",
        "    if \"sequential\" not in layer.name:\n",
        "        embedding_model.add(layer)\n",
        "embedding_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7F8XLBiX29W"
      },
      "source": [
        "####  Prepare embeddings for Vertex Explainable AI\n",
        "\n",
        "Next, you need to upload enbeddings layer of the TF.Keras model to Vertex AI Model Registry as Vertex `Model` resource.\n",
        "\n",
        "During the index deployment process, the model will be served to transform images into embeddings and create the index.\n",
        "\n",
        "As you can imagine, images need some common preprocessing. When you use a TensorFlow pre-built container to serve the model and you want to include preprocessing, you need to define a serving function to convert data to the format your embeddings expects. You specify the input layer of the serving function as the signature `serving_default` and saved it back with the underlying model using `tf.saved_model.save`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FoYsNA29FM5"
      },
      "outputs": [],
      "source": [
        "CONCRETE_INPUT = \"numpy_inputs\"\n",
        "\n",
        "def _preprocess(bytes_input):\n",
        "    \"\"\"\n",
        "    The preprocess function.\n",
        "    Args:\n",
        "        bytes_input: The input image in bytes.\n",
        "    Returns:\n",
        "        The preprocessed image in numpy array.\n",
        "    \"\"\"\n",
        "    decoded = tf.io.decode_jpeg(bytes_input, channels=CHANNELS)\n",
        "    decoded = tf.image.convert_image_dtype(decoded, tf.float32)\n",
        "    resized = tf.image.resize(decoded, size=SIZE)\n",
        "    rescale = tf.cast(resized, tf.float32)\n",
        "    return rescale\n",
        "\n",
        "\n",
        "@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])\n",
        "def preprocess_fn(bytes_inputs):\n",
        "    \"\"\"\n",
        "    Preprocess the input image.\n",
        "    Args:\n",
        "        bytes_inputs: A list of raw image bytes.\n",
        "    Returns:\n",
        "        A list of preprocessed images.\n",
        "    \"\"\"\n",
        "    decoded_images = tf.nest.map_structure(\n",
        "        tf.stop_gradient, tf.map_fn(_preprocess, bytes_inputs, dtype=tf.float32)\n",
        "    )\n",
        "    return {CONCRETE_INPUT: decoded_images}\n",
        "\n",
        "\n",
        "@tf.function(\n",
        "    input_signature=[tf.TensorSpec([None], tf.string), tf.TensorSpec([None], tf.string)]\n",
        ")\n",
        "def serving_fn(id, bytes_inputs):\n",
        "    \"\"\"\n",
        "    This function is used to serve the embeddings.\n",
        "    Args:\n",
        "        id: The id of the input.\n",
        "        bytes_inputs: The input image.\n",
        "    Returns:\n",
        "        The output of the model.\n",
        "    \"\"\"\n",
        "    images = preprocess_fn(bytes_inputs)\n",
        "    embedding = m_call(**images)\n",
        "    return {\"id\": id, \"embedding\": embedding}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqhkrG23Y7ZA"
      },
      "source": [
        "#### Export Embeddings for Vertex Explainable AI\n",
        "\n",
        "After you specify input signatures, you export embeddings as a SavedModel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uF6m34g09liu"
      },
      "outputs": [],
      "source": [
        "SIZE_3D = (None, 224, 224, 3)\n",
        "\n",
        "m_call = tf.function(embedding_model.call).get_concrete_function(\n",
        "    [tf.TensorSpec(shape=SIZE_3D, dtype=tf.float32, name=CONCRETE_INPUT)]\n",
        ")\n",
        "\n",
        "tf.saved_model.save(\n",
        "    embedding_model,\n",
        "    ENBEDDINGS_URI,\n",
        "    signatures={\n",
        "        \"serving_default\": serving_fn,\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeYFx4__WjhD"
      },
      "source": [
        "In this tutorial, you will use a TensorFlow pre-built container on Vertex AI to serve example-based explanation. When you use a TensorFlow pre-built container to serve predictions, you need to provide the names of the input tensors and the output tensor of your model. These names will be part of an ExplanationMetadata message when you configure a Model for Vertex Explainable AI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19Rc7ldz95f1"
      },
      "outputs": [],
      "source": [
        "embedding_model_loaded = tf.saved_model.load(ENBEDDINGS_URI)\n",
        "\n",
        "serving_input = list(\n",
        "    embedding_model_loaded.signatures[\"serving_default\"]\n",
        "    .structured_input_signature[1]\n",
        "    .keys()\n",
        ")[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcO_eZwRbTpO"
      },
      "outputs": [],
      "source": [
        "print(\"Serving function input:\", serving_input)\n",
        "serving_output = list(\n",
        "    embedding_model_loaded.signatures[\"serving_default\"].structured_outputs.keys()\n",
        ")[0]\n",
        "print(\"Serving function output:\", serving_output)\n",
        "\n",
        "input_name = model.input.name\n",
        "print(\"Model input name:\", input_name)\n",
        "output_name = model.output.name\n",
        "print(\"Model output name:\", output_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prepare the training dataset\n",
        "\n",
        "Now that you get embeddings, you need to prepare the training dataset by converting images into jsonl file."
      ],
      "metadata": {
        "id": "_gQ_Womf7aLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_ds.map(lambda x, y: (tf.image.resize(x, SIZE), y))\n",
        "train_ds = train_ds.batch(BATCH_SIZE).prefetch(buffer_size=10)\n",
        "train_images, train_labels = extract_images_and_labels(\n",
        "    train_ds, num_batches=NUM_BATCHES\n",
        ")\n",
        "write_jsonl(TRAIN_SOURCE_JSON_PATH, train_images)"
      ],
      "metadata": {
        "id": "5uyxaVXiUu1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! gsutil cp {TRAIN_SOURCE_JSON_PATH} {TRAIN_DATASET_URI}"
      ],
      "metadata": {
        "id": "vVWrypzesXsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "explanation_spec"
      },
      "source": [
        "#### Example-based explanation configuration\n",
        "\n",
        "Finally, you need to define the example-based explanation configuration.\n",
        "\n",
        "In particular, you need to specify:\n",
        "\n",
        "- `parameters` which indicate the explainability algorithm to use for explanations on your model. In this tutorial, you will use `Examples`\n",
        "\n",
        "- `metadata` which indicate how the algorithm is applied on your custom model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DJ2FcXJc-bI"
      },
      "source": [
        "##### Parameters\n",
        "\n",
        "About `Parameters` of example based explanations, you need to provide `examples` which define conditions to return the nearest neighbors from the provided dataset.\n",
        "\n",
        "With Example-based explanations, you have a new explanation method with associated parameter configuration. Below you have the list of the main properties you have to define.\n",
        "\n",
        "- `dimensions` : The dimension of the embedding.\n",
        "- `approximateNeighborsCount` : Number of neighbors to return.\n",
        "- `distanceMeasureType` : The distance metric by which to measure nearness of examples. You can choose between ``SQUARED_L2_DISTANCE,  L1_DISTANCE, COSINE_DISTANCE and DOT_PRODUCT_DISTANCE``.\n",
        "- `featureNormType` : Normalize the embeddings so that it has a unit length. You can choose between ``UNIT_L2_NORM or NONE``.\n",
        "- `treeAhConfig`: Parameters controlling the trade-off between quality of approximation and speed. See the paper for technical details. Under the hood, it creates a shallow tree where the number of leaves is controlled by leafNodeEmbeddingCount and the search recall/speed tradeoff is controlled by leafNodesToSearchPercent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sf_do58iXnK9"
      },
      "outputs": [],
      "source": [
        "dimensions = embedding_model.output.shape[1]\n",
        "\n",
        "NUM_NEIGHBORS_TO_RETURN = 10\n",
        "\n",
        "nearest_neighbor_search_config = {\n",
        "    \"contentsDeltaUri\": \"\",\n",
        "    \"config\": {\n",
        "        \"dimensions\": dimensions,\n",
        "        \"approximateNeighborsCount\": NUM_NEIGHBORS_TO_RETURN,\n",
        "        \"distanceMeasureType\": \"SQUARED_L2_DISTANCE\",\n",
        "        \"featureNormType\": \"NONE\",\n",
        "        \"algorithmConfig\": {\n",
        "            \"treeAhConfig\": {\n",
        "                \"leafNodeEmbeddingCount\": 1000,\n",
        "                \"leafNodesToSearchPercent\": 100,\n",
        "            }\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "examples = vertex_ai_v1beta1.Examples(\n",
        "    nearest_neighbor_search_config=nearest_neighbor_search_config,\n",
        "    gcs_source=io_pb2.GcsSource(uris=[TRAIN_DATASET_URI]),\n",
        "    neighbor_count=NUM_NEIGHBORS_TO_RETURN,\n",
        ")\n",
        "\n",
        "parameters = vertex_ai_v1beta1.ExplanationParameters(examples=examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Dn_QepydOFv"
      },
      "source": [
        "##### Explanation Metadata\n",
        "\n",
        "About metadata, you need to indicate\n",
        "\n",
        "- `outputs`: It is represented by Map from output names to output metadata. In this case you expect embeddings.\n",
        "\n",
        "- `inputs`: It is represented by Metadata of the input of a feature. In this case you have the encoded image and the id associated to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSMePRWgUaYs"
      },
      "outputs": [],
      "source": [
        "# for encoding parameter, 1 stands for 'IDENTITY'\n",
        "IMAGE_INPUT_TENSOR_NAME = \"bytes_inputs\"\n",
        "ID_INPUT_TENSOR_NAME = \"id\"\n",
        "OUTPUT_TENSOR_NAME = \"embedding\"\n",
        "\n",
        "explanation_inputs = {\n",
        "    \"my_input\": vertex_ai_v1beta1.ExplanationMetadata.InputMetadata(\n",
        "        {\n",
        "            \"input_tensor_name\": IMAGE_INPUT_TENSOR_NAME,\n",
        "            \"encoding\": vertex_ai_v1beta1.ExplanationMetadata.InputMetadata.Encoding(1),\n",
        "            \"modality\": \"image\",\n",
        "        }\n",
        "    ),\n",
        "    \"id\": vertex_ai_v1beta1.ExplanationMetadata.InputMetadata(\n",
        "        {\n",
        "            \"input_tensor_name\": ID_INPUT_TENSOR_NAME,\n",
        "            \"encoding\": vertex_ai_v1beta1.ExplanationMetadata.InputMetadata.Encoding(1),\n",
        "        }\n",
        "    ),\n",
        "}\n",
        "\n",
        "explanation_outputs = {\n",
        "    \"embedding\": vertex_ai_v1beta1.ExplanationMetadata.OutputMetadata(\n",
        "        {\"output_tensor_name\": OUTPUT_TENSOR_NAME}\n",
        "    )\n",
        "}\n",
        "\n",
        "explanation_meta_config = vertex_ai_v1beta1.ExplanationMetadata(\n",
        "    inputs=explanation_inputs, outputs=explanation_outputs\n",
        ")\n",
        "\n",
        "explanation_spec = vertex_ai_v1beta1.ExplanationSpec(\n",
        "    parameters=parameters, metadata=explanation_meta_config\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upload_the_model:explanation"
      },
      "source": [
        "### Deploy model and index\n",
        "\n",
        "Now you are ready to deploy your model.\n",
        "\n",
        "To deploy the model on Vertex AI, you need to create a `Model` resource. Then deploy the model to a `Endpoint` resource."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc-E8crVieKJ"
      },
      "source": [
        "#### Upload the model\n",
        "\n",
        "You can use `upload_model` helper function to upload your model, stored in SavedModel format, up to the `Model` service, which will instantiate a Vertex `Model` resource instance for your model. Below the parameters you need to define:\n",
        "\n",
        "- `display_name`: A human readable name for the `Model` resource.\n",
        "- `metadata_schema_uri`: Since your model was built without an Vertex `Dataset` resource, you will leave this blank (`''`).\n",
        "- `artificat_uri`: The Cloud Storage path where the embeddings is stored in SavedModel format.\n",
        "- `container_spec`: This is the specification for the Docker container that will be installed on the `Endpoint` resource, from which the `Model` resource will serve predictions.\n",
        "- `explanation_spec`: This is the specification for enabling explainability for your model.\n",
        "\n",
        "Uploading a model into a Vertex Model resource returns a long running operation which would take time. With example-based explanations, uploading a model triggers a batch prediction job to calculate embeddings and index them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xATLh3ADhK-w"
      },
      "source": [
        "##### Define serving container configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5ujqbVxhK-w"
      },
      "outputs": [],
      "source": [
        "DEPLOY_IMAGE_URI = \"gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-11:latest\"\n",
        "\n",
        "container_config = {\"image_uri\": DEPLOY_IMAGE_URI}\n",
        "container_spec = vertex_ai_v1beta1.ModelContainerSpec(container_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0CR8R1KhTBV"
      },
      "source": [
        "##### Define Model configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7byz4C8wiRRj"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = f\"mobilenetv2-{DATASET_NAME}-similarity\"\n",
        "model_config = {\n",
        "    \"display_name\": MODEL_NAME,\n",
        "    \"artifact_uri\": ENBEDDINGS_URI,\n",
        "    \"metadata_schema_uri\": \"\",\n",
        "    \"container_spec\": container_spec,\n",
        "    \"explanation_spec\": explanation_spec,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Upload the model\n",
        "\n",
        "Upload the model would take more than 1 hour.\n"
      ],
      "metadata": {
        "id": "FuKPBfXJbRcn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmFqtIO5OzvZ"
      },
      "outputs": [],
      "source": [
        "uploaded_model = upload_model(model_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_endpoint:custom"
      },
      "source": [
        "#### Deploy the `Model` resource\n",
        "\n",
        "To deploy the registered Vertex `Model` resource, you need to create an `Endpoint` resource. And then you deploy the `Model` resource to the `Endpoint` resource."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_endpoint"
      },
      "source": [
        "##### Create an `Endpoint` resource\n",
        "\n",
        "You use `create_endpoint` to create an endpoint for serving the model. Below the configuration you have to specify with the name of the `Endpoint` resource and some additional information.\n",
        "\n",
        "Creating an `Endpoint` resource returns a long running operation, since it may take a few moments to provision the `Endpoint` resource for serving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Qzc7H0jOzva"
      },
      "outputs": [],
      "source": [
        "ENDPOINT_NAME = f\"{MODEL_NAME}-similarity-endpoint\"\n",
        "DESCRIPTION = \"An endpoint for the similarity model\"\n",
        "LABELS = {\"env\": \"prod\", \"status\": \"online\"}\n",
        "\n",
        "endpoint_config = {\n",
        "        \"display_name\": ENDPOINT_NAME,\n",
        "        \"description\": DESCRIPTION,\n",
        "        \"labels\": LABELS,\n",
        "    }\n",
        "\n",
        "endpoint = create_endpoint(endpoint_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deploy_model:dedicated,v1beta1"
      },
      "source": [
        "##### Deploy model to endpoint\n",
        "\n",
        "You use `deploy_model` helper function to deploy the model to the endpoint you created. Below the parameters you have to define:\n",
        "\n",
        "- `model`: The Vertex fully qualified identifier of the `Model` resource to upload (deploy) from the training pipeline.\n",
        "- `endpoint`: The Vertex fully qualified `Endpoint` resource identifier to deploy the `Model` resource to.\n",
        "- `deploy_config`: The deployment configuration to define the deployment resources (GPUs, machine type) and some other conditions such as traffic split policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wtq7Xyn2Ozvb"
      },
      "outputs": [],
      "source": [
        "DEPLOYED_MODEL_NAME = f\"{MODEL_NAME}-deployed\"\n",
        "uploaded_model_id = uploaded_model.model\n",
        "endpoint_id = endpoint.name\n",
        "\n",
        "deploy_config = {\n",
        "        \"deployed_model_display_name\": DEPLOYED_MODEL_NAME,\n",
        "        \"deploy_gpu\": None,\n",
        "        \"deploy_ngpu\": 0,\n",
        "        \"deploy_compute\": 'n1-standard-4',\n",
        "        \"min_nodes\" : 1,\n",
        "        \"max_nodes\" : 1,\n",
        "        \"traffic_split\" : {\"0\": 100}\n",
        "        }\n",
        "\n",
        "deployed_model = deploy_model(uploaded_model_id, endpoint_id, deploy_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "make_prediction"
      },
      "source": [
        "### Query for similar examples\n",
        "\n",
        "Lastly you can run an online prediction request to your deployed model to get your similar examples using a sample of validation dataset.\n",
        "\n",
        "In this tutorial, you send STL10 images as compressed and encoded PNG image into base 64, instead of the raw uncompressed bytes that has been previously created.\n",
        "\n",
        "Each instance in the prediction request is a dictionary entry of the form:\n",
        "\n",
        "                        {`id`:, `bytes_inputs`: {'b64': content}}\n",
        "\n",
        "- `id`: the unique identifier associated to the image.\n",
        "- `bytes_inputs` : A map to contain decoded inputs.\n",
        "- `'b64'`: A key that indicates the content is base 64 encoded.\n",
        "- `content`: The compressed JPG image bytes as a base 64 encoded string.\n",
        "\n",
        "You use `get_instance` helper function to create the prediction instances for the prediction request."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_ds = validation_ds.map(lambda x, y: (tf.image.resize(x, SIZE), y))\n",
        "validation_ds = validation_ds.batch(BATCH_SIZE).prefetch(buffer_size=10)\n",
        "val_images, val_labels = extract_images_and_labels(\n",
        "    validation_ds, num_batches=NUM_BATCHES\n",
        ")"
      ],
      "metadata": {
        "id": "YY3OorYz06U7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_data = []\n",
        "\n",
        "for i, im in enumerate(val_images):\n",
        "  val_instance = get_instance(i, im)\n",
        "  val_data.append(val_instance)"
      ],
      "metadata": {
        "id": "J2krlRRi0Gpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "send_explain_request:image"
      },
      "source": [
        "#### Send the prediction with explanation request\n",
        "\n",
        "To send the prediction with explanation request you use `explain_image` helper function, which takes the parameters:\n",
        "\n",
        "- `image`: A list of test image data as a numpy array.\n",
        "- `endpoint`: The Vertex fully qualified identifier for the `Endpoint` resource where the `Model` resource was deployed.\n",
        "- `parameters_dict`: Additional parameters for serving.\n",
        "- `deployed_model_id`: The Vertex fully qualified identifier for the deployed model, when more than one model is deployed at the endpoint. Otherwise, if only one model deployed, can be set to `None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m76sOJ4803Qe"
      },
      "outputs": [],
      "source": [
        "INSTANCE_SIZE = 8\n",
        "NUM_VAL_DATA = 16\n",
        "deployed_model_id = deployed_model.deployed_model.id\n",
        "\n",
        "all_neighbors = []\n",
        "\n",
        "for data_idx in range(0, NUM_VAL_DATA, INSTANCE_SIZE):\n",
        "    end_idx = min(data_idx + INSTANCE_SIZE, NUM_VAL_DATA)\n",
        "    formatted_data = val_data[data_idx:end_idx]\n",
        "    response = explain_image(formatted_data, endpoint_id, None, deployed_model_id)\n",
        "    all_neighbors = (\n",
        "        all_neighbors + json_format.MessageToDict(response._pb)[\"explanations\"]\n",
        "    )\n",
        "\n",
        "print(f\"\\nExamples processed: {len(all_neighbors)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZiJ1ykWIAWR"
      },
      "source": [
        "#### Save input ids and the corresponding neighbors\n",
        "\n",
        "For each input image you sent, we create a dictionary with corrisponding neighbors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILMgp_4LH9TO"
      },
      "outputs": [],
      "source": [
        "# Save input ids and the corresponding neighbors\n",
        "data_with_neighbors = []\n",
        "input_data_list = val_data[:NUM_VAL_DATA]\n",
        "\n",
        "for i, input_data in enumerate(input_data_list):\n",
        "    neighbor_dict = all_neighbors[i]\n",
        "    neighbor_dict[\"input\"] = input_data[\"id\"]\n",
        "    data_with_neighbors.append(neighbor_dict)\n",
        "\n",
        "DEBUG = False\n",
        "if DEBUG:\n",
        "    val_idx = 0\n",
        "    print(data_with_neighbors[val_idx])\n",
        "    print(data_with_neighbors[val_idx][\"neighbors\"])\n",
        "    print(data_with_neighbors[val_idx][\"input\"])\n",
        "    print(len(data_with_neighbors[val_idx][\"neighbors\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9Gv-HxLlDOq"
      },
      "source": [
        "#### Visualize the images with explanations\n",
        "\n",
        "In the following representation, you will see for each image sent the ten closer examples the API generated according the distance you define.\n",
        "\n",
        "As you can verify, although the `example index` results closed to image classified in the same category, in some cases the model wrongly indenfies the category. And you can easily visualize them by leveraging distances."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_index_to_name = create_index_to_name_map(ds_info)"
      ],
      "metadata": {
        "id": "FqdxnG6L1iKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x79xz2XrxaNO"
      },
      "outputs": [],
      "source": [
        "VAL_IMG_INDICES = [1, 2, 10]  # images to visually explore\n",
        "for val_img_idx in VAL_IMG_INDICES:\n",
        "    if val_img_idx > NUM_VAL_DATA - 1:\n",
        "        raise ValueError(\n",
        "            f\"Data index {val_img_idx} does not exist in the requested explanations\"\n",
        "        )\n",
        "    plot_input_and_neighbors(\n",
        "        val_img_idx,\n",
        "        train_images,\n",
        "        val_images,\n",
        "        train_labels,\n",
        "        val_labels,\n",
        "        label_index_to_name,\n",
        "        data_with_neighbors,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLmS2M6dWsAI"
      },
      "source": [
        "### Further exploration\n",
        "If you want to continue exploring, here are some ideas:\n",
        "1.   Isolate test points where the model is making mistakes (cat mislabed as bird), and visualize the example-based explanations to see if you can find any common patterns.\n",
        "2.   If through this analysis, you find your training data is lacking in some representative cases (overhead images of cats), you can try adding such images to your dataset to see if that improves model performance.\n",
        "3.   [Fine-tune](https://keras.io/guides/transfer_learning/) the lower layers of the model to see if you can improve the quality of example-based explanations by enabling the model to learn a better latent representation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "undeploy_model"
      },
      "source": [
        "\n",
        "You can undeploy your `Model` resource from the serving `Endpoint` resoure with `undeploy_model` helper function, which takes the following parameters:\n",
        "\n",
        "- `deployed_model_id`: The model deployment identifier returned by the endpoint service when the `Model` resource was deployed to.\n",
        "- `endpoint`: The Vertex fully qualified identifier for the `Endpoint` resource where the `Model` is deployed to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx_vKniMq9ZX"
      },
      "outputs": [],
      "source": [
        "# delete flags\n",
        "undeploy_model_flag = False\n",
        "delete_endpoint_flag = False\n",
        "delete_bucket_flag = False\n",
        "\n",
        "# Undeploy model resource\n",
        "if undeploy_model_flag:\n",
        "  undeploy_model(deployed_model_id, endpoint_id)\n",
        "\n",
        "# Delete endpoint resource\n",
        "if delete_endpoint_flag:\n",
        "  delete_endpoint(endpoint_id)\n",
        "\n",
        "# Delete Cloud Storage objects that were created\n",
        "if delete_bucket_flag or os.getenv(\"IS_TESTING\"):\n",
        "    ! gsutil -m rm -r $BUCKET_URI"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Get started with Vertex Explainable AI Example Based API - Custom training text classification model\n",
    "\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24743cf4a1e1"
   },
   "source": [
    "**_NOTE_**: This notebook has been tested in the following environment:\n",
    "\n",
    "* Python version = 3.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to get example-based explanations for an text classification model. With example-based explanations, Vertex AI returns a list of examples (typically from the training set) that are most similar to the input.\n",
    "\n",
    "Learn more about [Vertex Explainable AI](https://cloud.google.com/vertex-ai/docs/explainable-ai/overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d975e698c9a4"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you learn how to get Example-Based explanations from Vertex Explainable AI services.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services and resources:\n",
    "\n",
    "- `Vertex AI Model Registry`\n",
    "- `Vertex Explainable AI`\n",
    "- `Vertex AI Prediction`\n",
    "\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Get data and model to generate example-based explainations\n",
    "- Index the entire dataset\n",
    "- Deploy model and index\n",
    "- Query for similar examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08d289fa873f"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "For this notebook, you use the [ag_news_subset dataset](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html) downloaded through [TF Datasets](https://www.tensorflow.org/datasets/catalog/ag_news_subset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aed92deeb4a0"
   },
   "source": [
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing)\n",
    "and [Cloud Storage pricing](https://cloud.google.com/storage/pricing),\n",
    "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## Installation\n",
    "\n",
    "Install the following packages required to execute this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "# Install the packages\n",
    "import os\n",
    "\n",
    "USER = \"\"\n",
    "! pip3 install {USER} --upgrade numpy tensorflow_datasets tensorflow==2.11.0  -q --no-warn-conflicts\n",
    "! pip3 install {USER} --upgrade google-cloud-aiplatform -q --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58707a750154"
   },
   "source": [
    "### Colab only: Uncomment the following cell to restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f200f10a1da3"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs so that your environment can access the new packages\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "## Before you begin\n",
    "\n",
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "3. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
    "\n",
    "4. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, try the following:\n",
    "* Run `gcloud config list`.\n",
    "* Run `gcloud projects list`.\n",
    "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAup21nz6LHk"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74ccc9e52986"
   },
   "source": [
    "**1. Vertex AI Workbench**\n",
    "* Do nothing as you are already authenticated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de775a3773ba"
   },
   "source": [
    "**2. Local JupyterLab instance, uncomment and run:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "254614fa0c46"
   },
   "outputs": [],
   "source": [
    "# ! gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef21552ccea8"
   },
   "source": [
    "**3. Colab, uncomment and run:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "603adbbf0532"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6b2ccc891ed"
   },
   "source": [
    "**4. Service account or other**\n",
    "* See how to grant Cloud Storage permissions to your service account at https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "Create a storage bucket to store intermediate artifacts such as datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = f\"your-bucket-name-{PROJECT_ID}-unique\" # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlICvTdOtyLN"
   },
   "source": [
    "### Set up project template\n",
    "Set the folder you use in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxRaeiQIt6kn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "TUTORIAL_DIR = os.path.join(os.getcwd(), \"sdk_xai_example_based_tutorial\")\n",
    "DATA_DIR = os.path.join(TUTORIAL_DIR, \"data\")\n",
    "MODEL_DIR = os.path.join(TUTORIAL_DIR, \"model\")\n",
    "\n",
    "for path in TUTORIAL_DIR, DATA_DIR, MODEL_DIR:\n",
    "    os.makedirs(path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960505627ddf"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "# General\n",
    "import io\n",
    "import time\n",
    "import tarfile\n",
    "import json\n",
    "import textwrap\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Vertex AI\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import aiplatform_v1beta1 as vertex_ai_v1beta1\n",
    "from google.cloud.aiplatform_v1beta1.types import io as io_pb2\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3yttwolcarS"
   },
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvxP35URk-MQ"
   },
   "outputs": [],
   "source": [
    "# API service endpoint\n",
    "API_ENDPOINT = f\"{REGION}-aiplatform.googleapis.com\"\n",
    "\n",
    "# Vertex location root path for your dataset, model and endpoint resources\n",
    "PARENT = \"projects/\" + PROJECT_ID + \"/locations/\" + REGION\n",
    "\n",
    "# Dataset\n",
    "DATASET_NAME = \"ag_news_subset\"\n",
    "TEXT_WIDTH = 200\n",
    "BATCH_SIZE = 512\n",
    "NUM_BATCHES = -1 # -1 is the default values to take all batches\n",
    "\n",
    "# Serving\n",
    "MODEL_FILE_NAME = f\"nnlm-en-dim50-{DATASET_NAME}.tar.gz\"\n",
    "SOURCE_MODEL_URI = BUCKET_URI + \"/model/\" + MODEL_FILE_NAME\n",
    "MODEL_SOURCE_FILE_NAME = \"model/\" + MODEL_FILE_NAME\n",
    "MODEL_DESTINATION_FILE_NAME = os.path.join(MODEL_DIR, MODEL_FILE_NAME)\n",
    "MODEL_FOLDER_DIR = os.path.join(MODEL_DIR, f\"nnlm-en-dim50-{DATASET_NAME}\")\n",
    "\n",
    "ENBEDDINGS_URI = BUCKET_URI + \"/model/nnlm-en-dim50-\" + DATASET_NAME\n",
    "TRAIN_DATASET_FILE = f\"{DATASET_NAME}-train-images.jsonl\"\n",
    "TRAIN_SOURCE_JSON_PATH = os.path.join(DATA_DIR, TRAIN_DATASET_FILE)\n",
    "TRAIN_DESTINATION_JSON_PATH = \"data/\" + TRAIN_DATASET_FILE\n",
    "TRAIN_DATASET_URI = BUCKET_URI + \"/\" + TRAIN_DESTINATION_JSON_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQ07ujzCsZCq"
   },
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CjYGKrUGsVlj"
   },
   "outputs": [],
   "source": [
    "def create_index_to_name_map(ds_info):\n",
    "    \"\"\"\n",
    "    Creates a map from label name to numerical index.\n",
    "    Args:\n",
    "        ds_info: DatasetInfo object.\n",
    "    Returns:\n",
    "        index_to_name_map: dict. Map from name to index.\n",
    "    \"\"\"\n",
    "    index_to_name = {}\n",
    "    num_classes = ds_info.features[\"label\"].num_classes\n",
    "    names = ds_info.features[\"label\"].names\n",
    "    for i in range(num_classes):\n",
    "        index_to_name[i] = names[i]\n",
    "    return index_to_name\n",
    "\n",
    "def extract_examples_and_labels(ds, num_batches):\n",
    "  \"\"\"\n",
    "  Extract examples and labels from a dataset.\n",
    "  Args:\n",
    "      ds: A dataset.\n",
    "      num_batches: The number of batches to extract. -1 uses the whole dataset\n",
    "  Returns:\n",
    "      examples: A numpy structure of examples.\n",
    "      labels: A numpy structure of labels.\n",
    "  \"\"\"\n",
    "  data_slice = ds.take(num_batches)  # -1 uses the whole dataset\n",
    "  examples = []\n",
    "  labels = []\n",
    "  for example, label in data_slice:\n",
    "      examples.append(example)\n",
    "      labels.append(label)\n",
    "  examples = tf.concat(examples,0)\n",
    "  labels = tf.concat(labels,0)\n",
    "  return examples.numpy(), labels.numpy()\n",
    "\n",
    "def get_instance(index, text):\n",
    "    \"\"\"\n",
    "    Get the instance to send to the model\n",
    "    Args:\n",
    "        index: The index associated with image\n",
    "        text: The text to send to the model\n",
    "    Returns:\n",
    "        The instance to send to the model\n",
    "    \"\"\"\n",
    "    instance = {\n",
    "        \"id\": str(index),\n",
    "        \"input_text\": str(text.decode(\"utf-8\"))\n",
    "        }\n",
    "    return instance\n",
    "\n",
    "def write_jsonl(saved_jsonl_path, text_instances):\n",
    "    \"\"\"\n",
    "    Write the jsonl file to send to the model\n",
    "    Args:\n",
    "        saved_jsonl_path: The path to save the jsonl file\n",
    "        text_instances: The text instances to send to the model\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(saved_jsonl_path, \"w\") as f:\n",
    "        for i, tx in enumerate(text_instances):\n",
    "            instance = get_instance(i, tx)\n",
    "            json.dump(\n",
    "                instance,\n",
    "                f,\n",
    "            )\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "def upload_model(model_configuration):\n",
    "    \"\"\"\n",
    "    Upload the model to Vertex AI\n",
    "    Args:\n",
    "        model_configuration: The model configuration\n",
    "    Returns:\n",
    "        The uploaded model\n",
    "    \"\"\"\n",
    "\n",
    "    model = vertex_ai_v1beta1.Model(\n",
    "        display_name=model_configuration[\"display_name\"],\n",
    "        artifact_uri=model_configuration[\"artifact_uri\"],\n",
    "        metadata_schema_uri=model_configuration[\"metadata_schema_uri\"],\n",
    "        explanation_spec=model_configuration[\"explanation_spec\"],\n",
    "        container_spec=model_configuration[\"container_spec\"],\n",
    "    )\n",
    "\n",
    "    response = clients[\"model\"].upload_model(parent=PARENT, model=model)\n",
    "    print(\"Long running operation:\", response.operation.name)\n",
    "    uploaded_model = response.result(timeout=10000)\n",
    "    print(\"upload_model_response\")\n",
    "    print(\" model:\", uploaded_model)\n",
    "    return uploaded_model\n",
    "\n",
    "\n",
    "def create_endpoint(endpoint_config):\n",
    "    \"\"\"\n",
    "    Create an endpoint\n",
    "    Args:\n",
    "        endpoint_config: The endpoint configuration\n",
    "    Returns:\n",
    "        The created endpoint\n",
    "    \"\"\"\n",
    "    response = clients[\"endpoint\"].create_endpoint(parent=PARENT, endpoint=endpoint_config)\n",
    "    print(\"Long running operation:\", response.operation.name)\n",
    "    endpoint = response.result()\n",
    "    print(\"create_endpoint_response\")\n",
    "    print(\" endpoint:\", endpoint)\n",
    "    return endpoint\n",
    "\n",
    "def deploy_model(\n",
    "        model, endpoint, deploy_config\n",
    "):\n",
    "    \"\"\"\n",
    "    Deploy a model to an endpoint\n",
    "    Args:\n",
    "        model: The model to deploy\n",
    "        endpoint: The endpoint to deploy the model\n",
    "        deploy_config: The model deployment configuration\n",
    "    Returns:\n",
    "        The deployed model\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if deploy_config[\"deploy_gpu\"]:\n",
    "        machine_spec = {\n",
    "            \"machine_type\": deploy_config[\"deploy_compute\"],\n",
    "            \"accelerator_type\": deploy_config[\"deploy_gpu\"],\n",
    "            \"accelerator_count\": deploy_config[\"deploy_ngpu\"],\n",
    "        }\n",
    "    else:\n",
    "        machine_spec = {\n",
    "            \"machine_type\": deploy_config[\"deploy_compute\"],\n",
    "        }\n",
    "\n",
    "    deployed_model = {\n",
    "        \"model\": model,\n",
    "        \"display_name\": deploy_config[\"deployed_model_display_name\"],\n",
    "        \"dedicated_resources\": {\n",
    "            \"min_replica_count\": deploy_config[\"min_nodes\"],\n",
    "            \"max_replica_count\": deploy_config[\"max_nodes\"],\n",
    "            \"machine_spec\": machine_spec,\n",
    "        },\n",
    "        \"enable_container_logging\": False,\n",
    "    }\n",
    "\n",
    "    response = clients[\"endpoint\"].deploy_model(\n",
    "        endpoint=endpoint, deployed_model=deployed_model, traffic_split=deploy_config[\"traffic_split\"]\n",
    "    )\n",
    "\n",
    "    print(\"Long running operation:\", response.operation.name)\n",
    "    deployed_model = response.result(timeout=10000)\n",
    "    print(\"deploy_model_response\")\n",
    "    print(\" deployed_model:\", deployed_model)\n",
    "\n",
    "    return deployed_model\n",
    "\n",
    "def explain_text(formatted_data, endpoint, parameters, deployed_model_id):\n",
    "    \"\"\"\n",
    "    Get example based explanations an image\n",
    "    Args:\n",
    "        formatted_data: The data to send to the model\n",
    "        endpoint: The endpoint to send the data to\n",
    "        parameters: The parameters to send to the model\n",
    "        deployed_model_id: The deployed model id\n",
    "    Returns:\n",
    "        The response from the model\n",
    "    \"\"\"\n",
    "\n",
    "    # The format of each instance should conform to the deployed model's prediction input schema.\n",
    "    instances_list = formatted_data\n",
    "    instances = [\n",
    "        json_format.ParseDict(instance, Value()) for instance in instances_list\n",
    "    ]\n",
    "\n",
    "    response = clients[\"prediction\"].explain(\n",
    "        endpoint=endpoint,\n",
    "        instances=instances,\n",
    "        parameters=parameters,\n",
    "        deployed_model_id=deployed_model_id,\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "def get_explanations(val_data, num_val_data, batch_size, endpoint_id, deployed_model_id):\n",
    "    \"\"\"\n",
    "    Get explanations for data.\n",
    "    Args:\n",
    "        val_data: list of data to explain\n",
    "        num_val_data: number of data to explain\n",
    "        batch_size: batch size for explanation\n",
    "        endpoint_id: endpoint id\n",
    "        deployed_model_id: deployed model id\n",
    "    Returns:\n",
    "        all_neighbors: list of explanations\n",
    "        examples_processed_each_iter: list of number of examples processed each iteration\n",
    "    \"\"\"\n",
    "    dataset_size = len(val_data)\n",
    "    all_neighbors = []\n",
    "    examples_processed_each_iter = []\n",
    "    if num_val_data > dataset_size:\n",
    "        print(f\"Requesting {num_val_data} explanations while the dataset is only {dataset_size}\")\n",
    "    for data_idx in range(0, num_val_data, batch_size):\n",
    "        end_idx = min(data_idx + batch_size, num_val_data)\n",
    "        formatted_data = val_data[data_idx:end_idx]\n",
    "        response = explain_text(formatted_data, endpoint_id, None, deployed_model_id)\n",
    "        examples_processed_each_iter.append(len(formatted_data))\n",
    "        all_neighbors = (\n",
    "            all_neighbors + json_format.MessageToDict(response._pb)[\"explanations\"]\n",
    "        )\n",
    "    return all_neighbors, examples_processed_each_iter\n",
    "\n",
    "def inspect_input_and_neighbors(val_example_idx, all_train_examples, val_examples, all_train_labels, val_labels, label_index_to_name, data_with_neighbors):\n",
    "    \"\"\"\n",
    "    Inspect the input example and its neighbors.\n",
    "    Args:\n",
    "        val_example_idx: The index of the input example in the dataset.\n",
    "        all_train_examples: A list of all the training examples.\n",
    "        val_examples: A list of all the validation examples.\n",
    "        all_train_labels: A list of all the training labels.\n",
    "        val_labels: A list of all the validation labels.\n",
    "        label_index_to_name: A dictionary mapping label indices to label names.\n",
    "        data_with_neighbors: A list of dictionaries, where each dictionary contains the input example and its neighbors.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    example = val_examples[val_example_idx]\n",
    "    class_label = val_labels[val_example_idx]\n",
    "    print(f\"Input label (true): {label_index_to_name[class_label]}({class_label}). \"\n",
    "          f\"Input index: {val_example_idx}. Input example:\\n{textwrap.fill(example.decode('utf-8'), width = TEXT_WIDTH)}\\n\")\n",
    "\n",
    "    neighbor_list = data_with_neighbors[val_example_idx]['neighbors']\n",
    "    num_neighbors = len(neighbor_list)\n",
    "    for n in range(num_neighbors):\n",
    "        neighbor = neighbor_list[n]\n",
    "        neighbor_idx = int(neighbor['neighborId'])\n",
    "        neighbor_example = all_train_examples[neighbor_idx]\n",
    "        neighbor_dist = neighbor['neighborDistance']\n",
    "\n",
    "        class_label = all_train_labels[neighbor_idx]\n",
    "        print(f\"Neighbor label: {label_index_to_name[class_label]}({class_label}). Neighbor index: {neighbor_idx}. \"\n",
    "            f\"Neighbor distance: {neighbor_dist:.3f}. Neighbor example:\\n {textwrap.fill(neighbor_example.decode('utf-8'), width = TEXT_WIDTH)}\\n\")\n",
    "    print(\"*****************\\n\\n\")\n",
    "\n",
    "def undeploy_model(deployed_model_id, endpoint):\n",
    "    \"\"\"\n",
    "    Undeploy a model from an endpoint\n",
    "    Args:\n",
    "        deployed_model_id: The deployed model id\n",
    "        endpoint: The endpoint to undeploy the model from\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    response = clients[\"endpoint\"].undeploy_model(\n",
    "        endpoint=endpoint, deployed_model_id=deployed_model_id, traffic_split={}\n",
    "    )\n",
    "    print(response)\n",
    "\n",
    "def delete_endpoint(endpoint_id):\n",
    "    \"\"\"\n",
    "    Delete an endpoint\n",
    "    Args:\n",
    "        endpoint_id: The name of endpoint to delete\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    response = clients[\"endpoint\"].delete_endpoint(name=endpoint_id)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xfFTb9Tu6LHm"
   },
   "outputs": [],
   "source": [
    "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAXCt-Ldplv2"
   },
   "source": [
    "### Set up clients\n",
    "\n",
    "The Vertex AI client library works as a client/server model. Then you need to set clients to use different services.\n",
    "\n",
    "You will use different clients in this tutorial for different steps in the workflow. So set them all up upfront.\n",
    "\n",
    "- Model Service for `Model` resources.\n",
    "- Endpoint Service for deployment.\n",
    "- Job Service for batch jobs and custom training.\n",
    "- Prediction Service for serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vq5UsPfqPiKi"
   },
   "outputs": [],
   "source": [
    "# client options same for all services\n",
    "client_options = {\"api_endpoint\": API_ENDPOINT}\n",
    "\n",
    "\n",
    "def create_model_client():\n",
    "    client = vertex_ai_v1beta1.ModelServiceClient(client_options=client_options)\n",
    "    return client\n",
    "\n",
    "\n",
    "def create_endpoint_client():\n",
    "    client = vertex_ai_v1beta1.EndpointServiceClient(client_options=client_options)\n",
    "    return client\n",
    "\n",
    "\n",
    "def create_prediction_client():\n",
    "    client = vertex_ai_v1beta1.PredictionServiceClient(client_options=client_options)\n",
    "    return client\n",
    "\n",
    "\n",
    "clients = {}\n",
    "clients[\"model\"] = create_model_client()\n",
    "clients[\"endpoint\"] = create_endpoint_client()\n",
    "clients[\"prediction\"] = create_prediction_client()\n",
    "\n",
    "for client in clients.items():\n",
    "    print(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmquyf6oHQwB"
   },
   "source": [
    "## Working with Vertex Explainable Example-based API\n",
    "\n",
    "Vertex Explainable Example-based API provides an highly performant ANN service for returning similar examples to new predictions/instances.\n",
    "\n",
    "To leverage Vertex AI Example-based explanations, you need to cover the following steps:\n",
    "\n",
    "- Index the entire dataset: It requires to provide a path to an embedding model in a GCS bucket, training data stored in a GCS bucket and the config file for example-based explanation\n",
    "\n",
    "- Deploy index and model: You need to specify the machine to use and the model identifier from the model upload set\n",
    "\n",
    "- Query for similar examples: You need to make the explain query and model will return similar examples\n",
    "\n",
    "Below you use a `nnlm-en-dim50` [model](https://tfhub.dev/google/nnlm-en-dim50/2) which is a token based text embedding trained on English Google News 7B corpus. It is available alongside pre-trained weights for fine-tuning the model which will be used to create embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gX-aXQYumVR"
   },
   "source": [
    "### Get data and model to generate example-based explainations\n",
    "\n",
    "Load tutorial data and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZLRjgjLTIw8f"
   },
   "outputs": [],
   "source": [
    "split_ds, ds_info = tfds.load(\n",
    "    DATASET_NAME,\n",
    "    split=[\"train\", \"test\"],\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    "    shuffle_files=False,\n",
    "    data_dir=DATA_DIR\n",
    ")\n",
    "train_ds, validation_ds = split_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4QGLBtxJie_"
   },
   "source": [
    "Check out the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B69zY7TBJkz8"
   },
   "outputs": [],
   "source": [
    "tfds.as_dataframe(ds=train_ds.take(5), ds_info=ds_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuwEYfLyJ3Ho"
   },
   "source": [
    "Get the model using the TF.Keras `model.load_model()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k6v2wLEpPzl6"
   },
   "outputs": [],
   "source": [
    "! gsutil cp {SOURCE_MODEL_URI} {MODEL_DESTINATION_FILE_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "meR7tO-hJrv-"
   },
   "outputs": [],
   "source": [
    "with tarfile.open(MODEL_DESTINATION_FILE_NAME) as file:\n",
    "  file.extractall(MODEL_DIR)\n",
    "  file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cc0NcisZIIpz"
   },
   "source": [
    "Check out the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDawpX57zc9o"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(MODEL_FOLDER_DIR)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k964kdaam4la"
   },
   "source": [
    "### Index the entire dataset\n",
    "\n",
    "To index the dataset you will use to get similar examples, you provide:\n",
    "\n",
    "- embedding model\n",
    "- training dataset\n",
    "- config file for example-based explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kN8KlWBvpEIp"
   },
   "source": [
    "#### Extract embeddings\n",
    "\n",
    "To generate example-based explanations, you need to extract embeddings from the model you want to evaluate.\n",
    "\n",
    "In this case, you skip the data augmentation layer and drop the softmax layer to get to the embeddings from the model you previously trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_zffjYVKj-3"
   },
   "outputs": [],
   "source": [
    "embedding_model = keras.models.Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer('task_embedding_layer').output, name='embedding_model')\n",
    "embedding_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBJbhNtNQVsa"
   },
   "source": [
    "####  Prepare embeddings for Vertex Explainable AI\n",
    "\n",
    "Next, you need to upload enbeddings layer of the TF.Keras model to Vertex AI Model Registry as Vertex `Model` resource.\n",
    "\n",
    "During the index deployment process, the model will be served to transform images into embeddings and create the index.\n",
    "\n",
    "You use a TensorFlow pre-built container to serve the model. And because you need to return index and embeddings, you need to define a serving function. You specify the input layer of the serving function as the signature `serving_default` and saved it back with the underlying model using `tf.saved_model.save`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mPB4m_k0QLzm"
   },
   "outputs": [],
   "source": [
    "@tf.function(input_signature=[tf.TensorSpec([None], tf.string), tf.TensorSpec(shape=[None], dtype=tf.string)])\n",
    "def serving_fn(id, input_text):\n",
    "    embedding = embedding_model(input_text)\n",
    "    return {\"id\": id, \"embedding\": embedding}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqhkrG23Y7ZA"
   },
   "source": [
    "#### Export Embeddings for Vertex Explainable AI\n",
    "\n",
    "After you specify input signatures, you export embeddings as a SavedModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uF6m34g09liu"
   },
   "outputs": [],
   "source": [
    "tf.saved_model.save(\n",
    "    embedding_model,\n",
    "    ENBEDDINGS_URI,\n",
    "    signatures={\n",
    "        \"serving_default\": serving_fn,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gQ_Womf7aLZ"
   },
   "source": [
    "#### Prepare the training dataset\n",
    "\n",
    "Now that you get embeddings, you need to prepare the training dataset by converting images into jsonl file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5uyxaVXiUu1O"
   },
   "outputs": [],
   "source": [
    "train_ds = train_ds.batch(BATCH_SIZE).prefetch(buffer_size=10)\n",
    "train_examples, train_labels = extract_examples_and_labels(\n",
    "    train_ds, num_batches=NUM_BATCHES\n",
    ")\n",
    "write_jsonl(TRAIN_SOURCE_JSON_PATH, train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PVub6JrndE7y"
   },
   "outputs": [],
   "source": [
    "! gsutil cp {TRAIN_SOURCE_JSON_PATH} {TRAIN_DATASET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "explanation_spec"
   },
   "source": [
    "#### Example-based explanation configuration\n",
    "\n",
    "When you use a TensorFlow pre-built container to serve predictions, you need to provide the names of the input tensors and the output tensor of your model. These names will be part of an ExplanationMetadata message when you configure a Model for Vertex Explainable AI.\n",
    "\n",
    "Also, you need to define the example-based explanation configuration.\n",
    "\n",
    "In particular, you need to specify:\n",
    "\n",
    "- `parameters` which indicate the explainability algorithm to use for explanations on your model. In this tutorial, you will use `Examples`\n",
    "\n",
    "- `metadata` which indicate how the algorithm is applied on your custom model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DJ2FcXJc-bI"
   },
   "source": [
    "##### Parameters\n",
    "\n",
    "About `Parameters` of example based explanations, you need to provide `examples` which define conditions to return the nearest neighbors from the provided dataset.\n",
    "\n",
    "With Example-based explanations, you have a new explanation method with associated parameter configuration. [See](https://cloud.google.com/vertex-ai/docs/reference/rpc/google.cloud.aiplatform.v1beta1#google.cloud.aiplatform.v1beta1.Presets) the documentation to know the main properties you have to define.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHFa4cpcmkWu"
   },
   "outputs": [],
   "source": [
    "dimensions = embedding_model.output.shape[1]\n",
    "\n",
    "PRESET_CONFIG = {\n",
    "     \"modality\": \"TEXT\",\n",
    "     \"query\": \"FAST\"\n",
    "}\n",
    "\n",
    "NUM_NEIGHBORS_TO_RETURN = 10\n",
    "\n",
    "examples = vertex_ai_v1beta1.Examples(\n",
    "    presets=PRESET_CONFIG,\n",
    "    gcs_source=io_pb2.GcsSource(uris=[TRAIN_DATASET_URI]),\n",
    "    neighbor_count=NUM_NEIGHBORS_TO_RETURN,\n",
    ")\n",
    "\n",
    "parameters = vertex_ai_v1beta1.ExplanationParameters(examples=examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Dn_QepydOFv"
   },
   "source": [
    "##### Explanation Metadata\n",
    "\n",
    "About metadata, you need to indicate\n",
    "\n",
    "- `outputs`: It is represented by Map from output names to output metadata. In this case you expect embeddings.\n",
    "\n",
    "- `inputs`: It is represented by Metadata of the input of a feature. In this case you have the encoded image and the id associated to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tSMePRWgUaYs"
   },
   "outputs": [],
   "source": [
    "TEXT_INPUT_TENSOR_NAME = \"input_text\"\n",
    "ID_INPUT_TENSOR_NAME = \"id\"\n",
    "OUTPUT_TENSOR_NAME = \"embedding\"\n",
    "\n",
    "explanation_inputs = {\n",
    "    \"my_input\": vertex_ai_v1beta1.ExplanationMetadata.InputMetadata(\n",
    "        {\n",
    "            \"input_tensor_name\": TEXT_INPUT_TENSOR_NAME,\n",
    "            \"encoding\": vertex_ai_v1beta1.ExplanationMetadata.InputMetadata.Encoding(1), # for encoding parameter, 1 stands for 'IDENTITY'\n",
    "        }\n",
    "    ),\n",
    "    \"id\": vertex_ai_v1beta1.ExplanationMetadata.InputMetadata(\n",
    "        {\n",
    "            \"input_tensor_name\": ID_INPUT_TENSOR_NAME,\n",
    "            \"encoding\": vertex_ai_v1beta1.ExplanationMetadata.InputMetadata.Encoding(1), # for encoding parameter, 1 stands for 'IDENTITY'\n",
    "        }\n",
    "    ),\n",
    "}\n",
    "\n",
    "explanation_outputs = {\n",
    "    \"embedding\": vertex_ai_v1beta1.ExplanationMetadata.OutputMetadata(\n",
    "        {\"output_tensor_name\": OUTPUT_TENSOR_NAME}\n",
    "    )\n",
    "}\n",
    "\n",
    "explanation_meta_config = vertex_ai_v1beta1.ExplanationMetadata(\n",
    "    inputs=explanation_inputs, outputs=explanation_outputs\n",
    ")\n",
    "\n",
    "explanation_spec = vertex_ai_v1beta1.ExplanationSpec(\n",
    "    parameters=parameters, metadata=explanation_meta_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_the_model:explanation"
   },
   "source": [
    "### Deploy model and index\n",
    "\n",
    "Now you are ready to deploy your model.\n",
    "\n",
    "To deploy the model on Vertex AI, you need to create a `Model` resource. Then deploy the model to a `Endpoint` resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qc-E8crVieKJ"
   },
   "source": [
    "#### Upload the model\n",
    "\n",
    "You can use `upload_model` helper function to upload your model, stored in SavedModel format, up to the `Model` service, which will instantiate a Vertex `Model` resource instance for your model. Below the parameters you need to define:\n",
    "\n",
    "- `display_name`: A human readable name for the `Model` resource.\n",
    "- `metadata_schema_uri`: Since your model was built without an Vertex `Dataset` resource, you will leave this blank (`''`).\n",
    "- `artificat_uri`: The Cloud Storage path where the embeddings is stored in SavedModel format.\n",
    "- `container_spec`: This is the specification for the Docker container that will be installed on the `Endpoint` resource, from which the `Model` resource will serve predictions.\n",
    "- `explanation_spec`: This is the specification for enabling explainability for your model.\n",
    "\n",
    "Uploading a model into a Vertex Model resource returns a long running operation which would take time. With example-based explanations, uploading a model triggers a batch prediction job to calculate embeddings and the associated index.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xATLh3ADhK-w"
   },
   "source": [
    "##### Define serving container configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5ujqbVxhK-w"
   },
   "outputs": [],
   "source": [
    "DEPLOY_IMAGE_URI = \"gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-11:latest\"\n",
    "\n",
    "container_config = {\"image_uri\": DEPLOY_IMAGE_URI}\n",
    "container_spec = vertex_ai_v1beta1.ModelContainerSpec(container_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0CR8R1KhTBV"
   },
   "source": [
    "##### Define Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7byz4C8wiRRj"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = f\"nnlm-en-dim50-{DATASET_NAME}-similarity\"\n",
    "\n",
    "model_config = {\n",
    "    \"display_name\": MODEL_NAME,\n",
    "    \"artifact_uri\": ENBEDDINGS_URI,\n",
    "    \"metadata_schema_uri\": \"\",\n",
    "    \"container_spec\": container_spec,\n",
    "    \"explanation_spec\": explanation_spec,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FuKPBfXJbRcn"
   },
   "source": [
    "##### Upload the model\n",
    "\n",
    "Upload the model would take more than 1 hour.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmFqtIO5OzvZ"
   },
   "outputs": [],
   "source": [
    "uploaded_model = upload_model(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_endpoint:custom"
   },
   "source": [
    "#### Deploy the `Model` resource\n",
    "\n",
    "To deploy the registered Vertex `Model` resource, you need to create an `Endpoint` resource. And then you deploy the `Model` resource to the `Endpoint` resource. This allows you to generate online predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_endpoint"
   },
   "source": [
    "##### Create an `Endpoint` resource\n",
    "\n",
    "You use `create_endpoint` to create an endpoint for serving the model. Below the configuration you have to specify with the name of the `Endpoint` resource and some additional information.\n",
    "\n",
    "Creating an `Endpoint` resource returns a long running operation, since it may take a few moments to provision the `Endpoint` resource for serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Qzc7H0jOzva"
   },
   "outputs": [],
   "source": [
    "ENDPOINT_NAME = f\"nnlm-en-dim50-{DATASET_NAME}-similarity-endpoint\"\n",
    "DESCRIPTION = \"An endpoint for the similarity model\"\n",
    "LABELS = {\"env\": \"prod\", \"status\": \"online\"}\n",
    "\n",
    "endpoint_config = {\n",
    "        \"display_name\": ENDPOINT_NAME,\n",
    "        \"description\": DESCRIPTION,\n",
    "        \"labels\": LABELS,\n",
    "    }\n",
    "\n",
    "endpoint = create_endpoint(endpoint_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deploy_model:dedicated,v1beta1"
   },
   "source": [
    "##### Deploy model to endpoint\n",
    "\n",
    "You use `deploy_model` helper function to deploy the model to the endpoint you created. Below the parameters you have to define:\n",
    "\n",
    "- `model`: The Vertex fully qualified identifier of the `Model` resource to upload (deploy) from the training pipeline.\n",
    "- `endpoint`: The Vertex fully qualified `Endpoint` resource identifier to deploy the `Model` resource to.\n",
    "- `deploy_config`: The deployment configuration to define the deployment resources (GPUs, machine type) and some other conditions such as traffic split policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wtq7Xyn2Ozvb"
   },
   "outputs": [],
   "source": [
    "DEPLOYED_MODEL_NAME = f\"{MODEL_NAME}-deployed\"\n",
    "uploaded_model_id = uploaded_model.model\n",
    "endpoint_id = endpoint.name\n",
    "\n",
    "deploy_config = {\n",
    "        \"deployed_model_display_name\": DEPLOYED_MODEL_NAME,\n",
    "        \"deploy_gpu\": None,\n",
    "        \"deploy_ngpu\": 0,\n",
    "        \"deploy_compute\": 'n1-standard-4',\n",
    "        \"min_nodes\" : 1,\n",
    "        \"max_nodes\" : 1,\n",
    "        \"traffic_split\" : {\"0\": 100}\n",
    "        }\n",
    "\n",
    "deployed_model = deploy_model(uploaded_model_id, endpoint_id, deploy_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "make_prediction"
   },
   "source": [
    "### Query for similar examples\n",
    "\n",
    "Lastly you can run an online prediction request to your deployed model to get your similar examples using a sample of validation dataset.\n",
    "\n",
    "Each instance in the prediction request is a dictionary entry of the form:\n",
    "\n",
    "                  {`id`:, `input_text`: `content`}\n",
    "\n",
    "- `id`: the unique identifier associated to the image.\n",
    "- `input_text` : the key contains text to find similarities.\n",
    "\n",
    "You use `get_instance` helper function to create the prediction instances for the prediction request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQcfmNQeli9E"
   },
   "outputs": [],
   "source": [
    "validation_ds = validation_ds.batch(BATCH_SIZE).prefetch(buffer_size=10)\n",
    "val_examples, val_labels = extract_examples_and_labels(\n",
    "    validation_ds, num_batches=NUM_BATCHES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jvSutXzPmH1y"
   },
   "outputs": [],
   "source": [
    "val_data = []\n",
    "\n",
    "for i, tx in enumerate(val_examples):\n",
    "  instance = get_instance(i, tx)\n",
    "  val_data.append(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "send_explain_request:image"
   },
   "source": [
    "#### Send the prediction with explanation request\n",
    "\n",
    "To send the prediction with explanation request you use `get_explanations` helper function, which takes the parameters:\n",
    "\n",
    "*   `val_data`: list of data to explain\n",
    "*   `num_val_data`: number of data to explain\n",
    "*   `batch_size`: batch size for explanation\n",
    "*   `endpoint_id`: endpoint id\n",
    "*   `deployed_model_id`: deployed model id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "obvD5PHDkgaH"
   },
   "outputs": [],
   "source": [
    "INSTANCE_SIZE = 8\n",
    "NUM_VAL_DATA = 16\n",
    "deployed_model_id = deployed_model.deployed_model.id\n",
    "\n",
    "all_neighbors, examples_processed_each_iter = get_explanations(val_data, NUM_VAL_DATA, INSTANCE_SIZE, endpoint_id, deployed_model_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZiJ1ykWIAWR"
   },
   "source": [
    "#### Save input ids and the corresponding neighbors\n",
    "\n",
    "For each input text you sent, we create a dictionary with corrisponding neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KlBQTiaQpADX"
   },
   "outputs": [],
   "source": [
    "data_with_neighbors = []\n",
    "input_data_list = val_data[:NUM_VAL_DATA]\n",
    "\n",
    "for i, input_data in enumerate(input_data_list):\n",
    "    neighbor_dict = all_neighbors[i]\n",
    "    neighbor_dict[\"input\"] = input_data[\"id\"]\n",
    "    data_with_neighbors.append(neighbor_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9Gv-HxLlDOq"
   },
   "source": [
    "#### Visualize text with explanations\n",
    "\n",
    "In the following representation, you will see for each text sent the closer examples the API generated according the distance you define.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iwUdWpvorfU3"
   },
   "outputs": [],
   "source": [
    "label_index_to_name = create_index_to_name_map(ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCBWolRFpD8e"
   },
   "outputs": [],
   "source": [
    "val_example_indices = [0, 2, 10,] # examples to inspect\n",
    "for val_example_idx in val_example_indices:\n",
    "    if val_example_idx > NUM_VAL_DATA - 1:\n",
    "        print(f'\\n\\n****Data index {val_example_idx} does not exist in the requested explanations***\\n\\n')\n",
    "        continue\n",
    "    inspect_input_and_neighbors(val_example_idx,\n",
    "                                train_examples,\n",
    "                                val_examples,\n",
    "                                train_labels,\n",
    "                                val_labels,\n",
    "                                label_index_to_name,\n",
    "                                data_with_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLmS2M6dWsAI"
   },
   "source": [
    "## Further exploration\n",
    "If you want to continue exploring, here are some ideas:\n",
    "1.   Isolate test points where the model is making mistakes (Sci/Tech mislabed as World), and visualize the example-based explanations to see if you can find any common patterns.\n",
    "2.   If through this analysis, you find your training data is lacking in some representative cases (articles on conversational AI), you can try adding such examples to your dataset to see if that improves model performance.\n",
    "3.   [Fine-tune](https://keras.io/guides/transfer_learning/) the lower layers of the model to see if you can improve the quality of example-based explanations by enabling the model to learn a better latent representation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLQ7tdGouz8A"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YXpKjz_1R2ck"
   },
   "outputs": [],
   "source": [
    "# delete flags\n",
    "undeploy_model_flag = True\n",
    "delete_endpoint_flag = True\n",
    "delete_bucket_flag = True\n",
    "\n",
    "# Undeploy model resource\n",
    "if undeploy_model_flag:\n",
    "  undeploy_model(deployed_model_id, endpoint_id)\n",
    "\n",
    "# Delete endpoint resource\n",
    "if delete_endpoint_flag:\n",
    "  delete_endpoint(endpoint_id)\n",
    "\n",
    "# Delete Cloud Storage objects that were created\n",
    "if delete_bucket_flag or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

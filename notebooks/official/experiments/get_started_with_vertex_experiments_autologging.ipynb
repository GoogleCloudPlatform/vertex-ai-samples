{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Vertex AI Experiments: Autologging\n",
        "\n",
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/experiments/autologging.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/experiments/autologging.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/experiments/autologging.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24743cf4a1e1"
      },
      "source": [
        "**_NOTE_**: This notebook has been tested in the following environment:\n",
        "\n",
        "* Python version = 3.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "As part of the Data science team, you may want to try different modeling approaches during experimentation phase.To guarantee reproducibility, each approach has different parameters that you need to manually track This is a time consuming task. To address this challenge, Vertex AI SDK introduces autologging, a one-line code SDK capability which leverages MLflow to provide automatic metrics and parameters tracking associated with your Vertex Experiments and Experiment Runs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d975e698c9a4"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to use Vertex AI `autologging`.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services and resources:\n",
        "\n",
        "- Vertex AI Workbench\n",
        "- Vertex AI Experiments\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Enable autologging in the Vertex AI SDK.\n",
        "- Train Sklearn model and see the resulting experiment run with metrics and parameters autologged to Vertex AI Experiments without setting an experiment run.\n",
        "- Train Tensorflow model, check autologged metrics and parameters to Vertex AI Experiments by manually setting an experiment run with `aiplatform.start_run()` and `aiplatform.end_run()`.\n",
        "- Disable autologging in the Vertex AI SDK, train a Pytorch model and check that none of the parameters or metrics has been logged.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08d289fa873f"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset is the [UCI Car Evaluation dataset](https://archive-beta.ics.uci.edu/dataset/19/car+evaluation), which is derived from simple hierarchical decision model and it contains attributions to predict car evaluation class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aed92deeb4a0"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI Experiments\n",
        "* Vertex AI Tensorboard\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing),\n",
        "and [Cloud Storage pricing](https://cloud.google.com/storage/pricing),\n",
        "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7EUnXsZhAGF"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Install the following packages required to execute this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b4ef9b72d43"
      },
      "outputs": [],
      "source": [
        "# Install the packages\n",
        "! pip3 install --user --upgrade pandas scikit-learn category_encoders tensorflow torch torchdata torchmetrics mlflow\n",
        "! pip3 install --user --upgrade google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58707a750154"
      },
      "source": [
        "### Colab only: Uncomment the following cell to restart the kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f200f10a1da3"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "# import IPython\n",
        "\n",
        "# app = IPython.Application.instance()\n",
        "# app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF1j6f9HApxa"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "3. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "4. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WReHDGG5g0XY"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, try the following:\n",
        "* Run `gcloud config list`.\n",
        "* Run `gcloud projects list`.\n",
        "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM1iC_MfAts1"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWz5v12n3jb3"
      },
      "outputs": [],
      "source": [
        "REGION = \"us-central1\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06571eb4063b"
      },
      "source": [
        "#### UUID\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a uuid for each instance session, and append it onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "697568e92bd6"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "\n",
        "# Generate a uuid of length 8\n",
        "def generate_uuid():\n",
        "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=8))\n",
        "\n",
        "\n",
        "UUID = generate_uuid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBCra4QMA2wR"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74ccc9e52986"
      },
      "source": [
        "**1. Vertex AI Workbench**\n",
        "* Do nothing as you are already authenticated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de775a3773ba"
      },
      "source": [
        "**2. Local JupyterLab instance, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "254614fa0c46"
      },
      "outputs": [],
      "source": [
        "# ! gcloud auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef21552ccea8"
      },
      "source": [
        "**3. Colab, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "603adbbf0532"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6b2ccc891ed"
      },
      "source": [
        "**4. Service account or other**\n",
        "* See how to grant Cloud Storage permissions to your service account at https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgPO1eR3CYjk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "Create a storage bucket to store intermediate artifacts such as datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzGDU7TWdts_"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = \"gs://your-bucket-name-unique\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EcIXiGsCePi"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIq7R4HZCfIc"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmXN14--y9xU"
      },
      "source": [
        "### Set up project template\n",
        "\n",
        "Set the folder you use in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ohz3wHe4zFNb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "tutorial_path = os.path.join(os.getcwd(), \"sdk_autologging_tutorial\")\n",
        "data_path = os.path.join(tutorial_path, \"data\")\n",
        "\n",
        "for path in tutorial_path, data_path:\n",
        "    os.makedirs(path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dAKzp-i1UJX"
      },
      "source": [
        "### Download dataset\n",
        "\n",
        "Download the car evaluation dataset from the official Google Cloud bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ukztj7v1XZw"
      },
      "outputs": [],
      "source": [
        "from urllib import request\n",
        "\n",
        "DATA_URL = \"http://cloud-samples-data.storage.googleapis.com/vertex-ai/dataset-management/datasets/uci_car_eval/car_evaluation_preprocessed.csv\"\n",
        "data_filepath = os.path.join(data_path, \"car_evaluation_data.csv\")\n",
        "request.urlretrieve(DATA_URL, data_filepath)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "COLUMN_NAMES = [\"buying\", \"maint\", \"doors\", \"persons\", \"lug_boot\", \"safety\", \"class\"]\n",
        "df = pd.read_csv(data_filepath)\n",
        "df[\"class\"] = df[\"class\"].replace({\"unacc\": 0, \"acc\": 0, \"good\": 1, \"vgood\": 1})\n",
        "\n",
        "processed_data_filepath = os.path.join(data_path, \"car_evaluation_preprocessed.csv\")\n",
        "df.to_csv(processed_data_filepath, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFjBFK5a3f4b"
      },
      "outputs": [],
      "source": [
        "!head {processed_data_filepath} -n 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "960505627ddf"
      },
      "source": [
        "### Import libraries\n",
        "\n",
        "Import the Vertex AI library to log experiments in Vertex AI Experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform as vertex_ai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mwQMym-Rteu"
      },
      "source": [
        "### Helpers\n",
        "\n",
        "To run experiments it is not uncommon to define experiment helpers, one per each modelling approach you plan to evaluate. Below you have the following experiment helpers:\n",
        "\n",
        "*   `train_sklearn_model`: A helper function to train a Decision Tree model using Sklearn.\n",
        "*   `train_tensorflow_model`: A helper function to train a simple model using Tensorflow.\n",
        "*   `train_pytorch_model`: A helper function to train a simple neural network using PyTorch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5tWyGgeRvXC"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed: int):\n",
        "    \"\"\"\n",
        "    A function to set the seed for reproducibility.\n",
        "    Args:\n",
        "        seed: Seed to be set\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    import random\n",
        "\n",
        "    import numpy as np\n",
        "    import tensorflow as tf\n",
        "    import torch\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "\n",
        "def train_sklearn_model(data_path: str, test_size: int, max_depth: int):\n",
        "    \"\"\"\n",
        "    A function to train a Decision Tree model using sklearn.\n",
        "    Args:\n",
        "        data_path: Path to the data\n",
        "        test_size: Size of the test set\n",
        "        max_depth: Maximum depth of the Decision Tree\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    # Libraries\n",
        "    import pandas as pd\n",
        "    from category_encoders import OrdinalEncoder\n",
        "    from sklearn.metrics import accuracy_score\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.pipeline import Pipeline\n",
        "    from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "    # Read data\n",
        "    print(\"Reading data...\")\n",
        "    df = pd.read_csv(data_path)\n",
        "\n",
        "    # Train, test split\n",
        "    print(\"Generating train and test data...\")\n",
        "    x = df[[\"buying\", \"maint\", \"doors\", \"persons\", \"lug_boot\", \"safety\"]]\n",
        "    y = df[[\"class\"]]\n",
        "    x_train, x_test, y_train, y_test = train_test_split(\n",
        "        x, y, test_size=test_size, shuffle=True\n",
        "    )\n",
        "\n",
        "    # Build pipeline\n",
        "    print(\"Building pipeline...\")\n",
        "    pipe = Pipeline(\n",
        "        [\n",
        "            (\"encoder\", OrdinalEncoder()),\n",
        "            (\"model\", DecisionTreeClassifier(criterion=\"gini\", max_depth=max_depth)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    print(\"Training model...\")\n",
        "    pipe.fit(x_train, y_train)\n",
        "\n",
        "    # Evaluate model\n",
        "    print(\"Evaluating model...\")\n",
        "    y_pred = pipe.predict(x_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(\"accurancy\", round(accuracy, 3))\n",
        "\n",
        "\n",
        "def train_tensorflow_model(\n",
        "    data_path: str, test_size: float, batch_size: int, epochs: int\n",
        "):\n",
        "    \"\"\"\n",
        "    A function to train a TF model.\n",
        "    Args:\n",
        "        data_path: Path to the data\n",
        "        test_size: Size of the test set\n",
        "        batch_size: Batch size\n",
        "        epochs: Number of epochs\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Libraries\n",
        "    import tensorflow as tf\n",
        "\n",
        "    # Variables\n",
        "    dataset_size = 1729\n",
        "    features_values = {\n",
        "        \"buying\": [\"vhigh\", \"high\", \"med\", \"low\"],\n",
        "        \"maint\": [\"vhigh\", \"high\", \"med\", \"low\"],\n",
        "        \"doors\": [\"2\", \"3\", \"4\", \"5more\"],\n",
        "        \"persons\": [\"2\", \"4\", \"more\"],\n",
        "        \"lug_boot\": [\"small\", \"med\", \"big\"],\n",
        "        \"safety\": [\"low\", \"med\", \"high\"],\n",
        "    }\n",
        "\n",
        "    # Helpers\n",
        "    def get_input_layer(features_vocabulary):\n",
        "        input_map = {}\n",
        "        for cat_name, cat_values in features_vocabulary.items():\n",
        "            input_map[cat_name] = tf.keras.Input(\n",
        "                shape=(1,), name=cat_name, dtype=\"string\"\n",
        "            )\n",
        "        return input_map\n",
        "\n",
        "    def get_features_layer(inputs_map, features_vocabulary):\n",
        "        features_map = {}\n",
        "        for cat_name, cat_values in features_vocabulary.items():\n",
        "            # Calculate categories\n",
        "            cat_index = tf.keras.layers.StringLookup(\n",
        "                vocabulary=cat_values, max_tokens=5\n",
        "            )(inputs_map[cat_name])\n",
        "            # Create encoding layer\n",
        "            cat_layer = tf.keras.layers.CategoryEncoding(num_tokens=5)(cat_index)\n",
        "            features_map[cat_name] = cat_layer\n",
        "        return features_map\n",
        "\n",
        "    # Read data\n",
        "    print(\"Reading data...\")\n",
        "    car_dataset = tf.data.experimental.make_csv_dataset(\n",
        "        data_path,\n",
        "        column_names=[\n",
        "            \"buying\",\n",
        "            \"maint\",\n",
        "            \"doors\",\n",
        "            \"persons\",\n",
        "            \"lug_boot\",\n",
        "            \"safety\",\n",
        "            \"class\",\n",
        "        ],\n",
        "        label_name=\"class\",\n",
        "        batch_size=batch_size,\n",
        "    )\n",
        "\n",
        "    # Generating Train, test split\n",
        "    print(\"Generating train and test data...\")\n",
        "    train_size = int(0.8 * dataset_size)\n",
        "    test_size = int(test_size * dataset_size)\n",
        "    train_dataset = car_dataset.take(train_size)\n",
        "    test_dataset = car_dataset.skip(train_size).take(test_size)\n",
        "\n",
        "    # Build model\n",
        "    print(\"Building model...\")\n",
        "    inputs_layer = get_input_layer(features_values)\n",
        "    features_layer = get_features_layer(inputs_layer, features_values)\n",
        "    x = tf.keras.layers.Concatenate()(features_layer.values())\n",
        "    x = tf.keras.layers.Dense(10, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Dense(5, activation=\"relu\")(x)\n",
        "    output_layer = tf.keras.layers.Dense(1)(x)\n",
        "    model = tf.keras.Model(inputs=inputs_layer.values(), outputs=output_layer)\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "\n",
        "    # Fit the model\n",
        "    print(\"Training model...\")\n",
        "    model.fit(\n",
        "        train_dataset,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_data=test_dataset,\n",
        "    )\n",
        "\n",
        "\n",
        "def train_pytorch_model(\n",
        "    data_path: str, test_size: float, batch_size: int, lr: float, epochs: int, seed: int\n",
        "):\n",
        "\n",
        "    # Libraries\n",
        "    import numpy as np\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torchmetrics\n",
        "    from torch.utils.data import DataLoader\n",
        "    from torchdata import datapipes\n",
        "\n",
        "    # Variables\n",
        "    seed = 8\n",
        "    features_map = {\n",
        "        0: {\"low\": 0, \"med\": 1, \"high\": 2, \"vhigh\": 3},\n",
        "        1: {\"low\": 0, \"med\": 1, \"high\": 2, \"vhigh\": 3},\n",
        "        2: {\"2\": 0, \"3\": 1, \"4\": 2, \"5more\": 3},\n",
        "        3: {\"2\": 0, \"4\": 1, \"more\": 2},\n",
        "        4: {\"small\": 0, \"med\": 1, \"big\": 2},\n",
        "        5: {\"low\": 0, \"med\": 1, \"high\": 2},\n",
        "    }\n",
        "    dataset_length = 1729\n",
        "\n",
        "    # Helpers\n",
        "    def row_processor(r):\n",
        "        for i, value in enumerate(r[:-1]):\n",
        "            r[i] = features_map[i][value]\n",
        "        return {\n",
        "            \"data\": np.array(r[:-1], dtype=np.float64),\n",
        "            \"labels\": np.array(r[-1], dtype=np.float64),\n",
        "        }\n",
        "\n",
        "    # Model definition\n",
        "    class SimpleNetwork(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.linear_relu = nn.Sequential(\n",
        "                nn.Linear(6, 12, dtype=torch.float64),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(12, 6, dtype=torch.float64),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(6, 3, dtype=torch.float64),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(3, 1, dtype=torch.float64),\n",
        "            )\n",
        "\n",
        "        def forward(self, x):\n",
        "            logits = self.linear_relu(x)\n",
        "            return logits\n",
        "\n",
        "    # Read data\n",
        "    print(\"Reading and preparing data...\")\n",
        "    read_dp = datapipes.iter.FileLister(data_path)\n",
        "    open_dp = datapipes.iter.FileOpener(read_dp)\n",
        "    parse_dp = datapipes.iter.CSVParser(open_dp, delimiter=\",\", skip_lines=1)\n",
        "    train_dp, test_dp = datapipes.iter.RandomSplitter(\n",
        "        parse_dp,\n",
        "        weights={\"train\": 1 - test_size, \"test\": test_size},\n",
        "        total_length=dataset_length,\n",
        "        seed=seed,\n",
        "    )\n",
        "    map_train_dp = datapipes.iter.Mapper(train_dp, row_processor)\n",
        "    map_test_dp = datapipes.iter.Mapper(test_dp, row_processor)\n",
        "    train_dataloader = DataLoader(map_train_dp, batch_size=batch_size, shuffle=True)\n",
        "    test_dataloader = DataLoader(map_test_dp, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Build model\n",
        "    print(\"Building model...\")\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = SimpleNetwork().to(device)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "    # Train model\n",
        "    print(\"Training model...\")\n",
        "    model.train()\n",
        "    for t in range(epochs):\n",
        "        batch = 0\n",
        "        for row in iter(train_dataloader):\n",
        "            features, labels = row[\"data\"].to(device), row[\"labels\"].to(device)\n",
        "            train_predictions = model(features)\n",
        "            train_prediction, _ = torch.max(train_predictions, 1)\n",
        "            train_loss = loss_fn(train_prediction, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            batch += 1\n",
        "            print(f\"Epoch {t + 1} - Batch {batch} - Loss {train_loss.item():.4f}\")\n",
        "\n",
        "    # Test model\n",
        "    print(\"Evaluating model...\")\n",
        "    metric = torchmetrics.classification.BinaryAccuracy()\n",
        "    metric_values = []\n",
        "    model.eval()\n",
        "    for t in range(epochs):\n",
        "        batch = 0\n",
        "        with torch.no_grad():\n",
        "            for row in iter(test_dataloader):\n",
        "                features, labels = row[\"data\"].to(device), row[\"labels\"].to(device)\n",
        "                val_predictions = model(features)\n",
        "                val_prediction, _ = torch.max(val_predictions, 1)\n",
        "                metric.update(val_prediction, labels)\n",
        "        accuracy = metric.compute()\n",
        "        metric_values.append(accuracy)\n",
        "        metric.reset()\n",
        "\n",
        "        batch += 1\n",
        "        print(f\"Epoch {t + 1} - Batch {batch} - Accuracy {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python and set seed for reproducibility\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project and set seed to guarantee reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1l-7Wft3jb6"
      },
      "outputs": [],
      "source": [
        "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mDnjvTV9Dlt"
      },
      "outputs": [],
      "source": [
        "set_seed(8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "004aGQ1BSLFm"
      },
      "source": [
        "## Model Experimentation using autologging with Vertex AI Experiments\n",
        "\n",
        "Vertex AI Experiments Autologging allows to run experiments and autologging parameters and metrics of different ML frameworks.\n",
        "\n",
        "After initiating an Vertex AI Experiment, enable autologging using `vertex_ai.autolog()`.\n",
        "\n",
        "There are two ways to use Autologging:\n",
        "\n",
        "1.   *With automatic experiment run creation*\n",
        "2.   *With user experiment run creation*\n",
        "\n",
        "With *automatic experiment run creation*, you run an experiment. Vertex AI SDK automatically creates an experiment run by logging all paramenters and metrics in Vertex AI Experiments.\n",
        "\n",
        "With *user experiment run creation*, you create an experiment using `vertex_ai.start_run(your-experiment-run-name)` and run the experiment. Then you get access to resulting paramentes and metrics after you end the experiment run with `vertex_ai.end_run()`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjJVARe5V-MG"
      },
      "source": [
        "#### Create an experiment for tracking training parameters and metrics\n",
        "\n",
        "To start, initiate an experiment using the `init()` method.\n",
        "\n",
        "Because some model types like Tensorflow and Keras result in autologging time series metrics, you need to create a Tensorboard instance.\n",
        "\n",
        "To create a Tensorboard instance, you can use `vertex_ai.Tensorboard.create()`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRoiPPk1XCWi"
      },
      "source": [
        "<div class=\"alert alert-danger\">Notice that if you did not activate yet, Vertex AI TensorBoard charges a monthly fee of $300 per unique active user. Learn more about [TensorBoard overview](https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-overview). </div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2z-VaIVTtI5"
      },
      "outputs": [],
      "source": [
        "autologged_experiment_name = f\"autologging-experiment-{UUID}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqM2pIO8_4Y7"
      },
      "outputs": [],
      "source": [
        "experiment_tensorboard = vertex_ai.Tensorboard.create()\n",
        "vertex_ai.init(\n",
        "    project=PROJECT_ID,\n",
        "    location=REGION,\n",
        "    staging_bucket=BUCKET_URI,\n",
        "    experiment=autologged_experiment_name,\n",
        "    experiment_tensorboard=experiment_tensorboard,\n",
        "    experiment_description=\"autolog-experiment-with-automatic-run\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCoYnaY594V7"
      },
      "source": [
        "#### Autologging an experiment with automatic experiment run creation\n",
        "\n",
        "In this section, Vertex AI SDK automatically creates an experiment run for you by logging all paramenters and training and post-training metrics in Vertex AI Experiments.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VGUnqk6Zvb6"
      },
      "source": [
        "##### Enable autologging\n",
        "\n",
        "First, enable autologging using `vertex_ai.autolog()` method.\n",
        "\n",
        "After calling `vertex_ai.autolog()`, any metrics and parameters from\n",
        "model training calls with supported ML frameworks will be automatically\n",
        "logged to Vertex Experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0H9kw74-5ob"
      },
      "outputs": [],
      "source": [
        "vertex_ai.autolog()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWWr08gZZzMb"
      },
      "source": [
        "##### Run baseline experiment\n",
        "\n",
        "Next, define your baseline model by running a Sklearn model experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCM9wLvRSaKc"
      },
      "outputs": [],
      "source": [
        "sklearn_config = dict(data_path=processed_data_filepath, test_size=0.2, max_depth=5)\n",
        "train_sklearn_model(**sklearn_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQh3v_85aGWP"
      },
      "source": [
        "##### Get the experiment results\n",
        "\n",
        "Then, use the method `get_experiment_df()` to get the results of the experiment as a pandas dataframe.\n",
        "\n",
        "Notice how all paramenters and metrics are logged in Vertex AI Experiments.\n",
        "\n",
        "In particular, the `run_name` has been automatically assigned and the `accurancy_score` metrics you defined has been logged too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuAl1etkLwAe"
      },
      "outputs": [],
      "source": [
        "experiment_df = vertex_ai.get_experiment_df()\n",
        "experiment_df = experiment_df.T\n",
        "experiment_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQlwVV_VMjGZ"
      },
      "source": [
        "#### Autologging with user experiment run creation\n",
        "\n",
        "As mentioned above, autologging automatically assigned running experiment to an experiment run.\n",
        "\n",
        "But you can always initialize a run within the experiment using `start_run()`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXU_UY5kcPdR"
      },
      "source": [
        "##### Initialize a new experiment run\n",
        "\n",
        "Track a specific run within the experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IxWrkgUQ7KL"
      },
      "outputs": [],
      "source": [
        "autologged_manual_run_experiment_name = f\"autologging-tf-experiment-{UUID}\"\n",
        "vertex_ai.start_run(autologged_manual_run_experiment_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFDLCAlqfYW7"
      },
      "source": [
        "##### Run an new experiment run\n",
        "\n",
        "Next, run a new experiment run with Tensorflow model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOu7DxpVVMMY"
      },
      "outputs": [],
      "source": [
        "tf_config = dict(\n",
        "    data_path=processed_data_filepath, test_size=0.2, batch_size=5, epochs=3\n",
        ")\n",
        "train_tensorflow_model(**tf_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAQUlys2j0UN"
      },
      "source": [
        "##### Compare experiment results\n",
        "\n",
        "After the experiment run finishes, call `end_run()` method to complete the logging for that run and you can use `get_experiment_df()` again to get the results of the experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rp5rNvZeePwk"
      },
      "outputs": [],
      "source": [
        "vertex_ai.end_run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XcCQUCgdTmG"
      },
      "outputs": [],
      "source": [
        "experiment_df = vertex_ai.get_experiment_df()\n",
        "experiment_df.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEt-8xvk9czW"
      },
      "source": [
        "### Running experiment without autologging\n",
        "\n",
        "Enabling autologging is optional. You can always disable autologging using `vertex_ai.autolog(disable=True)`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SeTs1Oavn2j"
      },
      "source": [
        "#### Disable autologging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHp-6wcN9jOq"
      },
      "outputs": [],
      "source": [
        "vertex_ai.autolog(disable=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c--rC7i3wFWM"
      },
      "source": [
        "#### Run a final experiment using Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3QKpkXc-N5o"
      },
      "outputs": [],
      "source": [
        "pt_config = dict(\n",
        "    data_path=processed_data_filepath,\n",
        "    test_size=0.2,\n",
        "    batch_size=64,\n",
        "    epochs=2,\n",
        "    lr=0.01,\n",
        "    seed=8,\n",
        ")\n",
        "train_pytorch_model(**pt_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygTmq9Tawfc-"
      },
      "source": [
        "#### Check experiment results\n",
        "\n",
        "Use `vertex_ai.get_experiment_df()` to check that the PyTorch experiment run hasn't been recorded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTOltQJbacLn"
      },
      "outputs": [],
      "source": [
        "experiment_df = vertex_ai.get_experiment_df()\n",
        "experiment_df.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I1hPpV63yyQ"
      },
      "source": [
        "Notice that Vertex AI SDK autologging uses MLFlow's autologging in its implementation, which support Pytorch Lightning only. Then you need to manually log metrics and models for tracking this Pytorch experiment. Check out the [Vertex AI Experiment documentation](https://cloud.google.com/vertex-ai/docs/experiments/intro-vertex-ai-experiments) to know more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx_vKniMq9ZX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Delete experiment\n",
        "experiment_list = vertex_ai.Experiment.list()\n",
        "for experiment in experiment_list:\n",
        "    experiment.delete()\n",
        "\n",
        "# Delete Cloud Storage objects that were created\n",
        "delete_bucket = True\n",
        "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
        "    ! gsutil -m rm -r $BUCKET_URI"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "get_started_with_vertex_experiments_autologging.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

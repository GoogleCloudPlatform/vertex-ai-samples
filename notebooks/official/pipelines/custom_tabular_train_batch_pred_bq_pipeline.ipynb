{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86d2d4c-6785-40e3-bd9c-423a47d05b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ada523-399a-466c-8d97-a7c62b880b59",
   "metadata": {},
   "source": [
    "# Vertex AI Pipelines: Training and batch prediction with BigQuery source and destinantion for a custom tabular classification model \n",
    "\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/custom_tabular_classification_bq_io_pipeline.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/custom_tabular_classification_bq_io_pipeline.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/custom_tabular_classification_bq_io_pipeline.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>                                                                                               \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fd9940-345b-480e-ba9c-5a8b95b6935e",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates performing training and batch prediction for a custom tabular classification model inside a Vertex AI pipeline. The batch prediction job takes data from a BigQuery source and writes the results to a BigQuery destination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de1a404-c3b8-4537-9b4f-43a622304205",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you train a scikit-learn tabular classification model and create batch prediction job for it through a Vertex AI pipeline using `google_cloud_pipeline_components`. The source and destination data for the batch prediction job is served in BigQuery.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services and resources:\n",
    "\n",
    "- Vertex AI `Pipelines`\n",
    "- Vertex AI `Datasets`\n",
    "- Vertex AI `Training`\n",
    "- Vertex AI `Model Registry`\n",
    "- Vertex AI `Batch Predictions`\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Create a dataset in BigQuery.\n",
    "- Set some data aside from the source dataset for batch prediction.\n",
    "- Create a custom python package for training application.\n",
    "- Upload the python package to Cloud Storage.\n",
    "- Create a Vertex AI Pipeline that:\n",
    "    - creates a Vertex AI Dataset from the source dataset.\n",
    "    - trains a scikit-learn RandomForest classification model on the dataset.\n",
    "    - uploads the trained model to Vertex AI Model Registry.\n",
    "    - runs a batch prediction job with the model on the test data.\n",
    "- Check the prediction results from the destination table in BigQuery.\n",
    "- Clean up the resources created in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01a9e9b-73f7-49fb-b3cb-3595ef6cb44f",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The [Census Income Data Set](https://archive.ics.uci.edu/ml/datasets/Census+Income) that this notebook uses for training is available publicly at the BigQuery location `bigquery-public-data.ml_datasets.census_adult_income`. It consists of the following fields:\n",
    "\n",
    "- `age`: Age.\n",
    "- `workclass`: Nature of employment.\n",
    "- `functional_weight`: Sample weight of the individual from the original Census data. How likely they were to be included in this dataset, based on their demographic characteristics vs. whole-population estimates.\n",
    "- `education`: Level of education completed.\n",
    "- `education_num`: Estimated years of education completed based on the value of the education field.\n",
    "- `marital_status`: Marital status.\n",
    "- `occupation`: Occupation category.\n",
    "- `relationship`: Relationship to the household.\n",
    "- `race`: Race.\n",
    "- `sex`: Gender.\n",
    "- `capital_gain`: Amount of capital gains.\n",
    "- `capital_loss`: Amount of capital loss.\n",
    "- `hours_per_week`: Hours worked per week.\n",
    "- `native_country`: Country of birth.\n",
    "- `income_bracket`: Either \" >50K\" or \" <=50K\" based on income."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e49cf9a-bcc7-4be5-bff9-cc86929b2314",
   "metadata": {},
   "source": [
    "### Costs \n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* BigQuery\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing), [BigQuery pricing](https://cloud.google.com/bigquery/pricing), [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbf90ae-a2af-4012-b256-0094005c9ed1",
   "metadata": {},
   "source": [
    "### Set up your local development environment\n",
    "\n",
    "**If you are using Colab or Vertex AI Workbench Notebooks**, your environment already meets\n",
    "all the requirements to run this notebook.\n",
    "\n",
    "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
    "You need the following:\n",
    "\n",
    "* The Google Cloud SDK\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* Jupyter notebook running in a virtual environment with Python 3\n",
    "\n",
    "The Google Cloud guide to [Setting up a Python development\n",
    "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
    "installation guide](https://jupyter.org/install) provide detailed instructions\n",
    "for meeting these requirements. The following steps provide a condensed set of\n",
    "instructions:\n",
    "\n",
    "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "1. [Install\n",
    "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
    "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
    "\n",
    "1. To install Jupyter, run `pip3 install jupyter` on the\n",
    "command-line in a terminal shell.\n",
    "\n",
    "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
    "\n",
    "1. Open this notebook in the Jupyter Notebook Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc47b5d7-ea76-49f3-b32b-ca075def77cd",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Install the following packages required to execute this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a6e9a4-c381-4dfd-a559-2507dffee62a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "# if IS_WORKBENCH_NOTEBOOK:\n",
    "#     USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform \\\n",
    "                            google-cloud-bigquery \\\n",
    "                            kfp \\\n",
    "                            google-cloud-pipeline-components {USER_FLAG} -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67e5aac-2ebd-4673-819d-99095b0567f8",
   "metadata": {},
   "source": [
    "### Restart the kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c180cf94-93ce-4be0-8570-6e5f1954c73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eead195-57de-4eb7-bd78-bb788bac5418",
   "metadata": {},
   "source": [
    "## Before you begin\n",
    "\n",
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. [Enable the Vertex AI, Compute Engine, Artifact Registry, BigQuery and Cloud Build APIs](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute.googleapis.com,artifactregistry.googleapis.com,bigquery.googleapis.com,cloudbuild.googleapis.com).\n",
    "\n",
    "1. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    "\n",
    "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e5f7dc-a665-4b7f-a03f-e20809bb3db1",
   "metadata": {},
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baafa58-1cdb-4d21-93c8-c1f8ebbc9425",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3300a6b5-602f-4f94-a686-71e0571b9f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8094bce0-2531-45fa-8408-78b977ae11fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9736ef-24c6-4bd5-acce-dda88e22b4e8",
   "metadata": {},
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex AI. It is recommended that you choose the region closest to you.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
    "\n",
    "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bebc78-9ab5-46c5-b69d-caaf8086eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6178e8f0-f6d5-4a43-8968-51f1d70cef1a",
   "metadata": {},
   "source": [
    "#### UUID\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a uuid for each instance session, and append it onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98d258c-fc56-45ce-bbd3-93e8227b9192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ca1822-6891-4b4a-8a1d-bbbfd447d66f",
   "metadata": {},
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "**If you are using Vertex AI Workbench Notebooks**, your environment is already\n",
    "authenticated. \n",
    "\n",
    "**If you are using Colab**, run the cell below and follow the instructions\n",
    "when prompted to authenticate your account via oAuth.\n",
    "\n",
    "**Otherwise**, follow these steps:\n",
    "\n",
    "1. In the Cloud Console, go to the [**Create service account key**\n",
    "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
    "\n",
    "2. Click **Create service account**.\n",
    "\n",
    "3. In the **Service account name** field, enter a name, and\n",
    "   click **Create**.\n",
    "\n",
    "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
    "into the filter box, and select\n",
    "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
    "\n",
    "5. Click *Create*. A JSON file that contains your key downloads to your\n",
    "local environment.\n",
    "\n",
    "6. Enter the path to your service account key as the\n",
    "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df1568a-8def-499a-bece-3deee9d071a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If on Vertex AI Workbench, then don't execute this code\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
    "    \"DL_ANACONDA_HOME\"\n",
    "):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS '[your-service-account-key-path]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e347faa-e17d-44e8-9e31-0c916bdf2a1d",
   "metadata": {},
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "When you submit a Pipeline or a Batch prediction job using the Vertex AI SDK, Vertex AI uses a Cloud Storage bucket as a staging location. Instead of providing while running the job, the staging location can also be provided to Vertex AI while initializing. In this tutorial, Vertex AI is initialized with a staging bucket that you create in the next steps.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
    "Cloud Storage buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e4e917-cef9-4037-92c5-1414db6e55b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1a4fd2-ab06-444b-9327-6950006e2671",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
    "    BUCKET_NAME = PROJECT_ID + \"aip-\" + UUID\n",
    "    BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe18fcf-4501-409b-a521-1020ca77fd80",
   "metadata": {},
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58370793-fd31-49e2-ae83-db549ea34fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2d396a-b45e-4473-b739-04c2e7bdfbd0",
   "metadata": {},
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400a1b50-cd04-4a8a-bd96-06a212aadd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee747771-7af2-4283-b6c5-c8e6527f2cc4",
   "metadata": {},
   "source": [
    "#### Service Account\n",
    "\n",
    "You use a service account to create Vertex AI Pipeline jobs. If you do not want to use your project's Compute Engine service account, set `SERVICE_ACCOUNT` to another service account ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164c1bb7-4b2b-4f58-95be-5a4106fd15b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f71ab4-3669-4977-b8c4-6d6b91305f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your service account from gcloud\n",
    "    if not IS_COLAB:\n",
    "        shell_output = !gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "    else:  # IS_COLAB:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf2e036-9df1-4142-9b9f-160f9754a34a",
   "metadata": {},
   "source": [
    "#### Set service account access for Vertex AI Pipelines\n",
    "\n",
    "Run the following commands to grant your service account access to read and write pipeline artifacts in the bucket that you created in the previous step. You only need to run this step once per service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f88356-a0ad-4502-9ef0-82dacd93376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d1c617-9813-4eef-bd8e-87bae76e9160",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "\n",
    "Import the Vertex AI Python SDK and other required Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e92c0f-bf42-40bc-9c41-678c6fdf3f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import aiplatform\n",
    "from kfp.dsl import pipeline\n",
    "from kfp.v2 import compiler\n",
    "from google.cloud.aiplatform_v1 import types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a657cf-681b-4483-9628-a79a128879c8",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31b9951-68b1-4aca-bdd1-ba55ff2f3c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI SDK\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)\n",
    "\n",
    "# Initialize BigQuery client \n",
    "bq_client = bigquery.Client(\n",
    "    project=PROJECT_ID,\n",
    "    credentials=aiplatform.initializer.global_config.credentials,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854b5634-dde6-41e1-a342-027aaf5c62d6",
   "metadata": {},
   "source": [
    "### Define constants\n",
    "\n",
    "Set constants that you need while training the model, creating and running the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fa59e3-31f5-4cb4-8214-61f2a674ae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source of the dataset\n",
    "DATA_SOURCE = \"bq://bigquery-public-data.ml_datasets.census_adult_income\"\n",
    "# Set name for the managed Vertex AI dataset \n",
    "DATASET_DISPLAY_NAME = f\"adult_census_dataset_{UUID}\"\n",
    "# BigQuery Dataset name\n",
    "BQ_DATASET_ID = f\"income_prediction_{UUID}\"\n",
    "# Set name for the BigQuery source table for batch prediction\n",
    "BQ_INPUT_TABLE = f\"income_test_data_{UUID}\"\n",
    "# Set the size(%) of the train set\n",
    "TRAIN_SPLIT = 0.9\n",
    "# Provide the container for training the model\n",
    "TRAINING_CONTAINER = \"us-docker.pkg.dev/vertex-ai/training/scikit-learn-cpu.0-23:latest\"\n",
    "# Provide the container for serving the model\n",
    "SERVING_CONTAINER = \"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-23:latest\"\n",
    "# Set the display name for training job\n",
    "TRAINING_JOB_DISPLAY_NAME = f\"income_classify_train_job_{UUID}\"\n",
    "# Model display name for Vertex AI Model Registry\n",
    "MODEL_DISPLAY_NAME = f\"income_classify_model_{UUID}\"\n",
    "# Set the name for batch prediction job\n",
    "BATCH_PREDICTION_JOB_NAME = f\"income_classify_batch_pred_{UUID}\"\n",
    "# Dispaly name for the Vertex AI Pipeline\n",
    "PIPELINE_DISPLAY_NAME = f\"income_classfiy_batch_pred_pipeline_{UUID}\"\n",
    "# Filename to compile the pipeline to\n",
    "PIPELINE_FILE_NAME = f\"{PIPELINE_DISPLAY_NAME}.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69705a87-b762-4b2b-9876-b2d53e6e7eca",
   "metadata": {},
   "source": [
    "## Create a BigQuery dataset\n",
    "For this tutorial, your input and output for the batch prediction job need to lie in BigQuery. So, you create a dataset in BigQuery to store them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4314fbf2-4c98-4448-8c49-3dbe9dd76913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BQ dataset\n",
    "bq_dataset = bigquery.Dataset(f\"{PROJECT_ID}.{BQ_DATASET_ID}\")\n",
    "bq_dataset = bq_client.create_dataset(bq_dataset)\n",
    "print(f\"Created dataset {bq_client.project}.{bq_dataset.dataset_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884483a9-f992-4f2b-94ef-d367a23dde88",
   "metadata": {},
   "source": [
    "## Create test data for batch prediction\n",
    "\n",
    "Query the public dataset source and create a test set in the created BigQuery dataset.\n",
    "\n",
    "For batch prediction, your test set is created by randomly selecting a small fraction (1-`TRAIN_SPLIT`) of the source dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6b7185-1f4f-4d84-8c0b-28c106ec0705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to create a test set from the source table\n",
    "query = f'''\n",
    "CREATE OR REPLACE TABLE\n",
    "  `{PROJECT_ID}.{BQ_DATASET_ID}.{BQ_INPUT_TABLE}` AS\n",
    "\n",
    "SELECT\n",
    "  * EXCEPT (pseudo_random, income_bracket)\n",
    "FROM (\n",
    "  SELECT\n",
    "    *,\n",
    "    RAND() AS pseudo_random \n",
    "  FROM\n",
    "    `bigquery-public-data.ml_datasets.census_adult_income` )\n",
    "WHERE pseudo_random > {TRAIN_SPLIT}\n",
    "'''\n",
    "# Run the query\n",
    "_ = bq_client.query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b925f04-56e5-498e-bf87-0a9ce13c47a6",
   "metadata": {},
   "source": [
    "## Create a Python package for your training application\n",
    "\n",
    "Before you perform the batch prediction task, you train the Random Forest classification model on the income census dataset. You perform the training through using a pre-built container in Vertex AI. For this purpose, you package the training application in the following steps.\n",
    "\n",
    "Learn more about [creating a Python training application for a pre-built container](https://cloud.google.com/vertex-ai/docs/training/create-python-pre-built-container).\n",
    "\n",
    "### Prepare the source directory\n",
    "\n",
    "Create a source directory named `python_package` with a `trainer` subfolder inside. Next, create a `__init__.py` file in the `trainer` folder to make it a package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288d54ab-92ad-4174-b786-e5bb8f949f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p python_package\n",
    "!mkdir -p python_package/trainer\n",
    "!touch python_package/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04246b73-dbb9-4fea-9fa9-97a8edde1cab",
   "metadata": {},
   "source": [
    "### Create the trainer task\n",
    "Within `trainer/`, create a module named `task.py` that serves as the entrypoint for your training code.\n",
    "\n",
    "The trainer code below preprocesses the train set and store the preprocessing transforms in a scikit-learn pipeline. Further, a [Random Forest model is trained](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) on the preprocessed train data and added as an estimator to the pipeline. After saving the model, it is uploaded to the Cloud Storage bucket for deployment.\n",
    "\n",
    "An advantage of using a scikit-learn pipeline is that it saves you from the trouble of writing additional scripts for preprocessing the data while generating predictions. \n",
    "\n",
    "Learn more about [scikit-learn pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc79fbda-4ef8-455b-b00e-7b460101dcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile python_package/trainer/task.py\n",
    "import os\n",
    "import joblib\n",
    "import argparse\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Read environmental variables\n",
    "PROJECT = os.getenv(\"CLOUD_ML_PROJECT_ID\")\n",
    "TRAINING_DATA_URI = os.getenv(\"AIP_TRAINING_DATA_URI\")\n",
    "\n",
    "# Set Bigquery Client\n",
    "bq_client = bigquery.Client(project=PROJECT)\n",
    "storage_client = storage.Client(project=PROJECT)\n",
    "\n",
    "# Define the constants\n",
    "TARGET = 'income_bracket'\n",
    "ARTIFACTS_PATH = os.getenv(\"AIP_MODEL_DIR\")\n",
    "# Get the bucket name from the model dir\n",
    "BUCKET_NAME = ARTIFACTS_PATH.replace(\"gs://\",\"\").split(\"/\")[0]\n",
    "\n",
    "MODEL_FILENAME = 'model.joblib' \n",
    "# Define the format of your input data, excluding the target column.\n",
    "# These are the columns from the census data files.\n",
    "COLUMNS = [\n",
    "    'age',\n",
    "    'workclass',\n",
    "    'functional_weight',\n",
    "    'education',\n",
    "    'education_num',\n",
    "    'marital_status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'capital_gain',\n",
    "    'capital_loss',\n",
    "    'hours_per_week',\n",
    "    'native_country'\n",
    "]\n",
    "# Categorical columns are columns that need to be turned into a numerical value to be used by scikit-learn\n",
    "CATEGORICAL_COLUMNS = [\n",
    "    'workclass',\n",
    "    'education',\n",
    "    'marital_status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'native_country'\n",
    "]\n",
    "\n",
    "# Function to fetch the data from BigQuery\n",
    "def download_table(bq_table_uri: str):\n",
    "    prefix = \"bq://\"\n",
    "    if bq_table_uri.startswith(prefix):\n",
    "        bq_table_uri = bq_table_uri[len(prefix):]\n",
    "\n",
    "    table = bigquery.TableReference.from_string(bq_table_uri)\n",
    "    rows = bq_client.list_rows(\n",
    "        table,\n",
    "    )\n",
    "    return rows.to_dataframe(create_bqstorage_client=False)\n",
    "\n",
    "# Function to upload local files to GCS\n",
    "def upload_model(bucket_name: str,\n",
    "                filename: str):\n",
    "     # Upload the saved model file to GCS\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    storage_path = os.path.join(ARTIFACTS_PATH, filename)\n",
    "    blob = storage.blob.Blob.from_string(storage_path, client=storage_client)\n",
    "    blob.upload_from_filename(filename)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Load the training data\n",
    "    X_train = download_table(TRAINING_DATA_URI)\n",
    "\n",
    "    # Remove the column we are trying to predict ('income-level') from our features list\n",
    "    # Convert the Dataframe to a lists of lists\n",
    "    train_features = X_train.drop(TARGET, axis=1).to_numpy().tolist()\n",
    "    # Create our training labels list, convert the Dataframe to a lists of lists\n",
    "    train_labels = X_train[TARGET].to_numpy().tolist()\n",
    "\n",
    "    # Since the census data set has categorical features, we need to convert\n",
    "    # them to numerical values. We use a list of pipelines to convert each\n",
    "    # categorical column and then use FeatureUnion to combine them before calling\n",
    "    # the RandomForestClassifier.\n",
    "    categorical_pipelines = []\n",
    "\n",
    "    # Each categorical column needs to be extracted individually and converted to a numerical value.\n",
    "    # To do this, each categorical column use a pipeline that extracts one feature column via\n",
    "    # SelectKBest(k=1) and a LabelBinarizer() to convert the categorical value to a numerical one.\n",
    "    # A scores array (created below) selects and extracts the feature column. The scores array is\n",
    "    # created by iterating over the COLUMNS and checking if it is a CATEGORICAL_COLUMN.\n",
    "    for i, col in enumerate(COLUMNS):\n",
    "        if col in CATEGORICAL_COLUMNS:\n",
    "            # Create a scores array to get the individual categorical column.\n",
    "            # Example:\n",
    "            #  data = [39, 'State-gov', 77516, 'Bachelors', 13, 'Never-married', 'Adm-clerical',\n",
    "            #         'Not-in-family', 'White', 'Male', 2174, 0, 40, 'United-States']\n",
    "            #  scores = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "            #\n",
    "            # Returns: [['Sate-gov']]\n",
    "            scores = []\n",
    "            # Build the scores array\n",
    "            for j in range(len(COLUMNS)):\n",
    "                if i == j: # This column is the categorical column we want to extract.\n",
    "                    scores.append(1) # Set to 1 to select this column\n",
    "                else: # Every other column should be ignored.\n",
    "                    scores.append(0)\n",
    "            skb = SelectKBest(k=1)\n",
    "            skb.scores_ = scores\n",
    "            # Convert the categorical column to a numerical value\n",
    "            lbn = LabelBinarizer()\n",
    "            r = skb.transform(train_features)\n",
    "            lbn.fit(r)\n",
    "            # Create the pipeline to extract the categorical feature\n",
    "            categorical_pipelines.append(\n",
    "                ('categorical-{}'.format(i), Pipeline([\n",
    "                    ('SKB-{}'.format(i), skb),\n",
    "                    ('LBN-{}'.format(i), lbn)])))\n",
    "\n",
    "    # Create pipeline to extract the numerical features\n",
    "    skb = SelectKBest(k=6)\n",
    "    # From COLUMNS use the features that are numerical\n",
    "    skb.scores_ = [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0]\n",
    "    categorical_pipelines.append(('numerical', skb))\n",
    "\n",
    "    # Combine all the features using FeatureUnion\n",
    "    preprocess = FeatureUnion(categorical_pipelines)\n",
    "\n",
    "    # Create the classifier\n",
    "    classifier = RandomForestClassifier()\n",
    "\n",
    "    # Transform the features and fit them to the classifier\n",
    "    classifier.fit(preprocess.transform(train_features), train_labels)\n",
    "\n",
    "    # Create the overall model as a single pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('union', preprocess),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "\n",
    "    # Save the pipeline locally\n",
    "    joblib.dump(pipeline, MODEL_FILENAME)\n",
    "    \n",
    "    # Upload the locally saved model to GCS\n",
    "    upload_model(bucket_name = BUCKET_NAME, \n",
    "                 filename=MODEL_FILENAME\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366e334d-15ac-4ba1-9a00-916b48820911",
   "metadata": {},
   "source": [
    "### Create a setup file\n",
    "Create a `setup.py` file that tells Setuptools how to create the source distribution. You also specify your application's standard dependencies as part of the `setup.py` file. Vertex AI uses pip to install your training application on the replicas that it allocates for your job. \n",
    "\n",
    "Learn more about [Setuptools](https://setuptools.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e062e01d-51bc-4394-8d6f-3db43d36ef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile python_package/setup.py\n",
    "\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = ['pandas','pyarrow']\n",
    "\n",
    "setup(\n",
    "    name='trainer',\n",
    "    version='0.1',\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='My training application.'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9d78f9-f376-4870-a388-83222afc8ab8",
   "metadata": {},
   "source": [
    "### Create the source distribution\n",
    "\n",
    "Run the following command to create a source distribution, `dist/trainer-0.1.tar.gz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb51f9c-a2ff-4a59-bee3-378dd513b3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd python_package && python3 setup.py sdist --formats=gztar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d98a69-3872-4bb6-9d92-27d4c5bb6274",
   "metadata": {},
   "source": [
    "### Copy the source distribution to Cloud Storage\n",
    "\n",
    "To train the custom classification model using a pre-built container, copy the source distribution of your training application to a Cloud Storage path. While training you let the Vertex AI SDK locate the package through the `python_package_gcs_uri` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0516ea72-5bde-4148-857b-84cada711ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r python_package/dist/* $BUCKET_URI/training_package/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4862c2-de09-4a24-89a8-e9856605008a",
   "metadata": {},
   "source": [
    "## Create and run the pipeline\n",
    "\n",
    "All the preparations have been done for your pipeline. In the current step, you create a Vertex AI Pipeline that comprises the following components each serving their own purpose in order:\n",
    "\n",
    "- `TabularDatasetCreateOp`: Creates a new managed tabular dataset in Vertex AI. \n",
    "- `CustomPythonPackageTrainingJobRunOp`: Creates and runs a custom training job in Vertex AI using a Python package.\n",
    "- `ModelBatchPredictOp`: Creates a batch prediction job in Vertex AI and waits for it to complete.\n",
    "\n",
    "All the above components are imported from the `google-cloud-pipeline-components` Python library. Learn more about [Google Cloud Pipeline Components](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.26/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50db936-6fb5-44fc-8726-0f2c8e3d9b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "@pipeline(name='custom-model-bq-batch-prediction-pipeline')\n",
    "def custom_model_bq_batch_prediction_pipeline(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    dataset_display_name: str,\n",
    "    dataset_bq_source: str,\n",
    "    training_job_dispaly_name: str,\n",
    "    gcs_staging_directory: str,\n",
    "    python_package_gcs_uri: str,\n",
    "    python_package_module_name: str,\n",
    "    training_split: float,\n",
    "    test_split: float,\n",
    "    training_container_uri: str,\n",
    "    serving_container_uri: str,\n",
    "    training_bigquery_destination: str,\n",
    "    model_display_name: str,\n",
    "    batch_prediction_display_name: str,\n",
    "    batch_prediction_instances_format: str,\n",
    "    batch_prediction_predictions_format: str,\n",
    "    batch_prediction_source_uri: str,\n",
    "    batch_prediction_destination_uri: str,\n",
    "    batch_prediction_machine_type: str = \"n1-standard-4\",\n",
    "    batch_prediction_batch_size: int = 1000\n",
    "    \n",
    "):\n",
    "    from google_cloud_pipeline_components.aiplatform import TabularDatasetCreateOp\n",
    "    from google_cloud_pipeline_components.aiplatform import CustomPythonPackageTrainingJobRunOp\n",
    "    from google_cloud_pipeline_components.aiplatform import ModelBatchPredictOp\n",
    "    \n",
    "    # Create the dataset\n",
    "    dataset_create_op = TabularDatasetCreateOp(\n",
    "        project=project,\n",
    "        location=location,\n",
    "        display_name=dataset_display_name,\n",
    "        bq_source=dataset_bq_source\n",
    "    )\n",
    "    \n",
    "    # Run the training task\n",
    "    train_op = CustomPythonPackageTrainingJobRunOp(\n",
    "        display_name=training_job_dispaly_name,\n",
    "        python_package_gcs_uri=python_package_gcs_uri,\n",
    "        python_module_name=python_package_module_name,\n",
    "        container_uri=training_container_uri,\n",
    "        model_display_name=model_display_name,\n",
    "        model_serving_container_image_uri=serving_container_uri,\n",
    "        dataset=dataset_create_op.outputs[\"dataset\"],\n",
    "        base_output_dir=gcs_staging_directory,\n",
    "        bigquery_destination=training_bigquery_destination,\n",
    "        training_fraction_split=training_split,\n",
    "        test_fraction_split=test_split,\n",
    "        staging_bucket=gcs_staging_directory\n",
    "    )\n",
    "    \n",
    "    # Run the batch prediction task\n",
    "    batch_predict_task = ModelBatchPredictOp(\n",
    "                            project=project,\n",
    "                            location=location,\n",
    "                            model=train_op.outputs['model'],\n",
    "                            instances_format=batch_prediction_instances_format,\n",
    "                            bigquery_source_input_uri=batch_prediction_source_uri,\n",
    "                            predictions_format=batch_prediction_predictions_format,\n",
    "                            bigquery_destination_output_uri=batch_prediction_destination_uri,\n",
    "                            job_display_name=batch_prediction_display_name,\n",
    "                            machine_type=batch_prediction_machine_type,\n",
    "                            manual_batch_tuning_parameters_batch_size=batch_prediction_batch_size\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e43179a-fd33-48de-88db-8f217fd1e526",
   "metadata": {},
   "source": [
    "### Compile the pipeline\n",
    "\n",
    "After defining your pipeline, compile it to a file (`PIPELINE_FILE_NAME`) in JSON or YAML format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7f563a-ac02-4c84-b532-859197d16356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the pipeline\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=custom_model_bq_batch_prediction_pipeline,\n",
    "    package_path=PIPELINE_FILE_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b74c11-fe73-4be1-854b-8c327c5898d2",
   "metadata": {},
   "source": [
    "### Set the parameters\n",
    "\n",
    "Now, define the paramters to run your pipeline.\n",
    "\n",
    "To pass the required arguments to the individual components in your pipeline, you define the following paramters:\n",
    "- `project`: Project ID for the Google Cloud project where the pipeline needs to run.\n",
    "- `location`: Region where the pipeline needs to run.\n",
    "- `dataset_display_name`: Display name for the managed dataset resource in Vertex AI.\n",
    "- `dataset_bq_source`: BigQuery table URI to serve as a source for the managed dataset in Vertex AI.\n",
    "- `training_job_dispaly_name`: Display name for the the custom python package training job.\n",
    "- `gcs_staging_directory`: Staging directory for Vertex AI to store training artifacts.\n",
    "- `python_package_gcs_uri`: Cloud Storage path to the Python package for training.\n",
    "- `python_package_module_name`: Module name (trainer task) inside the Python package for training.\n",
    "- `training_split`: Percentage of the total data to be considered for training.\n",
    "- `test_split`: Percentage of the total data to be considered for testing. Split percentage parameters provided for the **CustomPythonPackageTrainingJobRunOp** component should always sum up to 1.\n",
    "- `training_container_uri`: Pre-built container image URI for training the model. \n",
    "- `serving_container_uri`: Pre-built container image URI for serving the model on Vertex AI.\n",
    "- `training_bigquery_destination`: The BigQuery project location where the training data is to be written to during training.\n",
    "- `model_display_name`: Dispaly name for the model to be deployed in Vertex AI Model Registry.\n",
    "- `batch_prediction_display_name`: Dispaly name for the batch prediction job.\n",
    "- `batch_prediction_instances_format`: Format of the input instances for batch prediction.\n",
    "- `batch_prediction_predictions_format`: Format of the results from the batch prediction.\n",
    "- `batch_prediction_source_uri`: Source URI of the input data.\n",
    "- `batch_prediction_destination_uri`: Destination URI where the batch prediction results need to be stored.\n",
    "\n",
    "**Note:** Though a test split percentage is provided, test data is not used during the training process. This test data is different from the test data created in earlier steps for batch prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddb7194-f270-4130-8b92-9eaba9d303a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters for running the pipeline\n",
    "parameters = {\n",
    "            \"project\": PROJECT_ID,\n",
    "            \"location\": REGION,\n",
    "            \"dataset_display_name\": DATASET_DISPLAY_NAME,\n",
    "            \"dataset_bq_source\": DATA_SOURCE,\n",
    "            \"training_job_dispaly_name\": TRAINING_JOB_DISPLAY_NAME,\n",
    "            \"gcs_staging_directory\": BUCKET_URI,\n",
    "            \"python_package_gcs_uri\": f\"{BUCKET_URI}/training_package/trainer-0.1.tar.gz\",\n",
    "            \"python_package_module_name\": \"trainer.task\",\n",
    "            \"training_split\": TRAIN_SPLIT,\n",
    "            \"test_split\": 1 - TRAIN_SPLIT,\n",
    "            \"training_container_uri\": TRAINING_CONTAINER,\n",
    "            \"serving_container_uri\": SERVING_CONTAINER,\n",
    "            \"training_bigquery_destination\": f\"bq://{PROJECT_ID}\",\n",
    "            \"model_display_name\": MODEL_DISPLAY_NAME,\n",
    "            \"batch_prediction_display_name\": BATCH_PREDICTION_JOB_NAME,\n",
    "            \"batch_prediction_instances_format\": 'bigquery',\n",
    "            \"batch_prediction_predictions_format\": 'bigquery',\n",
    "            \"batch_prediction_source_uri\": f\"bq://{PROJECT_ID}.{BQ_DATASET_ID}.{BQ_INPUT_TABLE}\",\n",
    "            \"batch_prediction_destination_uri\": f\"bq://{PROJECT_ID}.{BQ_DATASET_ID}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03649acf-b0a1-4cbb-9108-f4d4402e204c",
   "metadata": {},
   "source": [
    "### Run the pipeline\n",
    "\n",
    "Create a Vertex AI Pipeline job and run it using the `PipelineJob` class.\n",
    "\n",
    "The `PipelineJob` class takes the following parameters:\n",
    "\n",
    "- `display_name`: The display name of the Vertex AI Pipeline.\n",
    "- `template_path`: The path of PipelineJob or PipelineSpec JSON (or YAML) file.\n",
    "- `parameter_values`: The mapping from runtime parameter names to its values that control the pipeline run.\n",
    "- `enable_caching`: Whether to turn on caching for the run.\n",
    "\n",
    "Learn more about the `PipelineJob` class from [Vertex AI PipelineJob documentation](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.PipelineJob)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabe1bef-040a-48d1-b38b-8f352bdd5b4a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Vertex AI Pipeline job\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=PIPELINE_DISPLAY_NAME,\n",
    "    template_path=PIPELINE_FILE_NAME,\n",
    "    parameter_values=parameters,\n",
    "    enable_caching=True\n",
    ")\n",
    "# Run the pipeline job\n",
    "job.run(service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5926e01-3c8f-49b9-b1ab-ebb3d442d1cb",
   "metadata": {},
   "source": [
    "## Fetch results from the predictions table\n",
    "\n",
    "After the Vertex AI pipeline job is finished successfully, fetch the results from batch prediction into a dataframe.\n",
    "\n",
    "### Get the table name\n",
    "\n",
    "Run the below cell to get the name of the predictions table from the pipeline job's artifact details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e724a20-4ca2-487d-adf6-b6b93a078153",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTPUT_TABLE = None\n",
    "# Load the batch prediction job details using the display name\n",
    "[batch_prediction_job] = aiplatform.BatchPredictionJob.list(filter=f'display_name=\"{BATCH_PREDICTION_JOB_NAME}\"')\n",
    "# Fetch the name of the output table\n",
    "OUTPUT_TABLE = batch_prediction_job.output_info.bigquery_output_table\n",
    "print(\"Predictions table ID:\", OUTPUT_TABLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eee6779-12be-4150-a1f8-1bd9c19dd0ee",
   "metadata": {},
   "source": [
    "### Query the results table\n",
    "\n",
    "Fetch a specified number of rows from the predictions table using the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023bd359-3213-4b53-95b5-0fcfdae16c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the needed no.of rows\n",
    "ROWS = 10\n",
    "# Define the query\n",
    "query = f'''\n",
    "    Select prediction from `{PROJECT_ID}.{BQ_DATASET_ID}.{OUTPUT_TABLE}` limit {ROWS}\n",
    "'''\n",
    "# Fetch the data into a dataframe\n",
    "df = bq_client.query(query).to_dataframe()\n",
    "# Display the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca070e31-54b3-4c22-b83a-b890cbc10926",
   "metadata": {},
   "source": [
    "### Fetch the resources for deletion\n",
    "\n",
    "Using the display names of the individual resources, load the resources created inside the pipeline for the clean up step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccdc144-f0f4-40e7-88b1-0572ae81b32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Vertex AI tabular dataset using the display name\n",
    "[dataset] = aiplatform.TabularDataset.list(filter=f'display_name=\"{DATASET_DISPLAY_NAME}\"')\n",
    "\n",
    "# Load the Vertex AI model resource using the display name\n",
    "[model] = aiplatform.Model.list(filter=f'display_name=\"{MODEL_DISPLAY_NAME}\"')\n",
    "\n",
    "# Load the custom training job using the display name\n",
    "[training_job] = aiplatform.CustomPythonPackageTrainingJob.list(filter=f'display_name=\"{TRAINING_JOB_DISPLAY_NAME}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992372ee-70f3-4793-bde3-2aaaf9199c88",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n",
    "- Vertex AI Pipeline Job\n",
    "- Vertex AI TabularDataset\n",
    "- Vertex AI Model\n",
    "- Vertex AI Training job\n",
    "- Vertex AI Batch Prediction job\n",
    "- BigQuery dataset\n",
    "- Cloud Storage bucket (Set `delete_bucket` to **True** to delete the Cloud Storage bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb155781-a268-49ee-8e6e-a6de349bc902",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "delete_bucket = False\n",
    "\n",
    "# Delete the Vertex AI Pipeline job\n",
    "job.delete()\n",
    "\n",
    "# Delete the Vertex AI TabularDataset\n",
    "dataset.delete()\n",
    "\n",
    "# Delete the Vertex AI Model\n",
    "model.delete()\n",
    "\n",
    "# Delete the Vertex AI Training job\n",
    "training_job.delete()\n",
    "\n",
    "# Delete the Vertex AI Batch prediction job\n",
    "batch_prediction_job.delete()\n",
    "\n",
    "# Delete the BigQuery dataset\n",
    "! bq rm -r -f -d $PROJECT_ID:$BQ_DATASET_ID\n",
    "\n",
    "# Delete Cloud Storage objects\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-bq_io-py",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Python [conda env:bq_io]",
   "language": "python",
   "name": "conda-env-bq_io-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18ebbd838e32"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "978ab06a7e3f"
      },
      "source": [
        "# Vertex AI Pipelines: Training and batch prediction with BigQuery source and destination for a custom tabular classification model \n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/custom_tabular_train_batch_pred_bq_pipeline.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Fpipelines%2Fcustom_tabular_train_batch_pred_bq_pipeline.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/custom_tabular_train_batch_pred_bq_pipeline.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "<a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/pipelines/custom_tabular_train_batch_pred_bq_pipeline.ipynb\" target='_blank'>\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "     </a>\n",
        "   </td>\n",
        "</table>\n",
        "<br/><br/><br/><br/>\n",
        "\n",
        "*Note: This notebook uses KFP 1.x and GCPC 1.x. We recommend using 2.x*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1153bb181b8c"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates performing training and batch prediction for a custom tabular classification model inside a Vertex AI pipeline. The batch prediction job takes data from a BigQuery source and writes the results to a BigQuery destination.\n",
        "\n",
        "Learn more about [Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction) and [Vertex AI Batch Prediction components](https://cloud.google.com/vertex-ai/docs/pipelines/batchprediction-component)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87c2947ffa97"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you train a scikit-learn tabular classification model and create a batch prediction job for it through a Vertex AI pipeline using google_cloud_pipeline_components. The source and destination data for the batch prediction job is served in BigQuery.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services and resources:\n",
        "\n",
        "- Vertex AI Pipelines\n",
        "- Vertex AI dataset\n",
        "- Vertex AI Training\n",
        "- Vertex AI Model Registry\n",
        "- Vertex AI batch prediction\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Create a dataset in BigQuery.\n",
        "- Set some data aside from the source dataset for batch prediction.\n",
        "- Create a custom python package for training application.\n",
        "- Upload the python package to Cloud Storage.\n",
        "- Create a Vertex AI Pipeline that:\n",
        "    - creates a Vertex AI dataset from the source dataset.\n",
        "    - trains a scikit-learn RandomForest classification model on the dataset.\n",
        "    - uploads the trained model to Vertex AI Model Registry.\n",
        "    - runs a batch prediction job with the model on the test data.\n",
        "- Check the prediction results from the destination table in BigQuery.\n",
        "- Clean up the resources created in this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aef4f59195ad"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The [Census Income Data Set](https://archive.ics.uci.edu/ml/datasets/Census+Income) that this notebook uses for training is available publicly at the BigQuery location bigquery-public-data.ml_datasets.census_adult_income. It consists of the following fields:\n",
        "\n",
        "- age: Age.\n",
        "- workclass: Nature of employment.\n",
        "- functional_weight: Sample weight of the individual from the original Census data. How likely they were to be included in this dataset, based on their demographic characteristics vs. whole-population estimates.\n",
        "- education: Level of education completed.\n",
        "- education_num: Estimated years of education completed based on the value of the education field.\n",
        "- marital_status: Marital status.\n",
        "- occupation: Occupation category.\n",
        "- relationship: Relationship to the household.\n",
        "- race: Race.\n",
        "- sex: Gender.\n",
        "- capital_gain: Amount of capital gains.\n",
        "- capital_loss: Amount of capital loss.\n",
        "- hours_per_week: Hours worked per week.\n",
        "- native_country: Country of birth.\n",
        "- income_bracket: Either \" >50K\" or \" <=50K\" based on income."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fcafeb3489d"
      },
      "source": [
        "### Costs \n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* BigQuery\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), \n",
        "[BigQuery pricing](https://cloud.google.com/bigquery/pricing), \n",
        "[Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the \n",
        "[Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b887879da0a"
      },
      "source": [
        "## Get started\n",
        "\n",
        "\n",
        "Install Vertex AI SDK for Python and other required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bbc49622f3c"
      },
      "outputs": [],
      "source": [
        "! pip3 install --upgrade --quiet google-cloud-aiplatform \\\n",
        "                                 google-cloud-bigquery \\\n",
        "                                 pandas \\\n",
        "                                 pyarrow \\\n",
        "                                 'kfp<2' \\\n",
        "                                 'google-cloud-pipeline-components<2' \\\n",
        "                                 db-dtypes "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2f55b22b5fa"
      },
      "source": [
        "### Restart runtime (Colab only)\n",
        "\n",
        "To use the newly installed packages, you must restart the runtime on Google Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcc98768955f"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4de1bd77992b"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">,\n",
        "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>,\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c06fe092778"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "Authenticate your environment on Google Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c97be6a73155"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d74b65fa97ed"
      },
      "source": [
        "### Set Google Cloud project information\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project. Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM1iC_MfAts1"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgPO1eR3CYjk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "Create a storage bucket to store intermediate artifacts such as datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzGDU7TWdts_"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EcIXiGsCePi"
      },
      "source": [
        "**If your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIq7R4HZCfIc"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $LOCATION -p $PROJECT_ID $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8debaa04cb14"
      },
      "source": [
        "#### Service Account\n",
        "\n",
        "You use a service account to create Vertex AI Pipeline jobs. If you don't want to use your project's Compute Engine service account, set `SERVICE_ACCOUNT` to another service account ID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77b01a1fdbb4"
      },
      "outputs": [],
      "source": [
        "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f936bebda2d4"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "if (\n",
        "    SERVICE_ACCOUNT == \"\"\n",
        "    or SERVICE_ACCOUNT is None\n",
        "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
        "):\n",
        "    # Get your service account from gcloud\n",
        "    if not IS_COLAB:\n",
        "        shell_output = !gcloud auth list 2>/dev/null\n",
        "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
        "\n",
        "    else:  # IS_COLAB:\n",
        "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
        "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "\n",
        "    print(\"Service Account:\", SERVICE_ACCOUNT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40ef6967cad3"
      },
      "source": [
        "#### Set service account access for Vertex AI Pipelines\n",
        "\n",
        "Run the following commands to grant your service account access to read and write pipeline artifacts in the bucket that you created in the previous step. You only need to run this step once per service account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f88cb0488c08"
      },
      "outputs": [],
      "source": [
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
        "\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a61dae37929"
      },
      "source": [
        "### Import libraries\n",
        "\n",
        "Import the Vertex AI Python SDK and other required Python libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ff2006a7c9a"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform, bigquery\n",
        "from kfp.dsl import pipeline\n",
        "from kfp.v2 import compiler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "782ab8eecedf"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0b876b97113"
      },
      "outputs": [],
      "source": [
        "# Initialize Vertex AI SDK\n",
        "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_URI)\n",
        "\n",
        "# Initialize BigQuery client\n",
        "bq_client = bigquery.Client(\n",
        "    project=PROJECT_ID,\n",
        "    credentials=aiplatform.initializer.global_config.credentials,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb0eafd9b733"
      },
      "source": [
        "### Define constants\n",
        "\n",
        "Set constants that you need while training the model, creating and running the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "891800f8c3ab"
      },
      "outputs": [],
      "source": [
        "# Source of the dataset\n",
        "DATA_SOURCE = \"bq://bigquery-public-data.ml_datasets.census_adult_income\"\n",
        "# Set name for the managed Vertex AI dataset\n",
        "DATASET_DISPLAY_NAME = \"adult_census_dataset_unique\"\n",
        "# BigQuery Dataset name\n",
        "BQ_DATASET_ID = \"income_prediction_unique1\"\n",
        "# Set name for the BigQuery source table for batch prediction\n",
        "BQ_INPUT_TABLE = \"income_test_data_unique\"\n",
        "# Set the size(%) of the train set\n",
        "TRAIN_SPLIT = 0.9\n",
        "# Provide the container for training the model\n",
        "TRAINING_CONTAINER = \"us-docker.pkg.dev/vertex-ai/training/scikit-learn-cpu.0-23:latest\"\n",
        "# Provide the container for serving the model\n",
        "SERVING_CONTAINER = \"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-23:latest\"\n",
        "# Set the display name for training job\n",
        "TRAINING_JOB_DISPLAY_NAME = \"income_classify_train_job_unique\"\n",
        "# Model display name for Vertex AI Model Registry\n",
        "MODEL_DISPLAY_NAME = \"income_classify_model_unique\"\n",
        "# Set the name for batch prediction job\n",
        "BATCH_PREDICTION_JOB_NAME = \"income_classify_batch_pred_unique\"\n",
        "# Dispaly name for the Vertex AI Pipeline\n",
        "PIPELINE_DISPLAY_NAME = \"income_classfiy_batch_pred_pipeline_unique\"\n",
        "# Filename to compile the pipeline to\n",
        "PIPELINE_FILE_NAME = f\"{PIPELINE_DISPLAY_NAME}.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e40fa6f1c674"
      },
      "source": [
        "## Create a BigQuery dataset\n",
        "For this tutorial, your input and output for the batch prediction job need to lie in BigQuery. So, you create a dataset in BigQuery to store them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74d8750d8fe9"
      },
      "outputs": [],
      "source": [
        "# Create a BQ dataset\n",
        "bq_dataset = bigquery.Dataset(f\"{PROJECT_ID}.{BQ_DATASET_ID}\")\n",
        "bq_dataset = bq_client.create_dataset(bq_dataset)\n",
        "print(f\"Created dataset {bq_client.project}.{bq_dataset.dataset_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "042747a05fd1"
      },
      "source": [
        "## Create test data for batch prediction\n",
        "\n",
        "Query the public dataset source and create a test set in the created BigQuery dataset.\n",
        "\n",
        "For batch prediction, test set is created by randomly selecting a small fraction (1-TRAIN_SPLIT) of the source dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89cb2a5c5044"
      },
      "outputs": [],
      "source": [
        "# Query to create a test set from the source table\n",
        "query = f\"\"\"\n",
        "CREATE OR REPLACE TABLE\n",
        "  `{PROJECT_ID}.{BQ_DATASET_ID}.{BQ_INPUT_TABLE}` AS\n",
        "\n",
        "SELECT\n",
        "  * EXCEPT (pseudo_random, income_bracket)\n",
        "FROM (\n",
        "  SELECT\n",
        "    *,\n",
        "    RAND() AS pseudo_random \n",
        "  FROM\n",
        "    `bigquery-public-data.ml_datasets.census_adult_income` )\n",
        "WHERE pseudo_random > {TRAIN_SPLIT}\n",
        "\"\"\"\n",
        "# Run the query\n",
        "_ = bq_client.query(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45595b3d8037"
      },
      "source": [
        "## Create a Python package for your training application\n",
        "\n",
        "Before you perform the batch prediction task, you train the Random Forest classification model on the income census dataset. You perform the training through using a prebuilt container in Vertex AI. For this purpose, you package the training application in the following steps.\n",
        "\n",
        "Learn more about [creating a Python training application for a prebuilt container](https://cloud.google.com/vertex-ai/docs/training/create-python-prebuilt-container).\n",
        "\n",
        "### Prepare the source directory\n",
        "\n",
        "Create a source directory named python_package with a trainer subfolder inside. Next, create a __init__.py file in the trainer folder to make it a package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3190d7d2afdf"
      },
      "outputs": [],
      "source": [
        "!mkdir -p python_package\n",
        "!mkdir -p python_package/trainer\n",
        "!touch python_package/trainer/__init__.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88fa9c49bebd"
      },
      "source": [
        "### Create the trainer task\n",
        "Within trainer/, create a module named task.py that serves as the entrypoint for your training code.\n",
        "\n",
        "The trainer code below preprocesses the train set and stores the preprocessing transforms in a scikit-learn pipeline. Further, a [Random Forest model is trained](https://scikitlearn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) on the preprocessed train data and added as an estimator to the pipeline. After saving the model, it's uploaded to the Cloud Storage bucket for deployment.\n",
        "\n",
        "An advantage of using a scikit-learn pipeline is that it saves you from the trouble of writing additional scripts for preprocessing the data while generating predictions. \n",
        "\n",
        "Learn more about [scikit-learn pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b2c76999db9"
      },
      "outputs": [],
      "source": [
        "%%writefile python_package/trainer/task.py\n",
        "import os\n",
        "import joblib\n",
        "import argparse\n",
        "from google.cloud import storage\n",
        "from google.cloud import bigquery\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Read environmental variables\n",
        "PROJECT = os.getenv(\"CLOUD_ML_PROJECT_ID\")\n",
        "TRAINING_DATA_URI = os.getenv(\"AIP_TRAINING_DATA_URI\")\n",
        "\n",
        "# Set Bigquery Client\n",
        "bq_client = bigquery.Client(project=PROJECT)\n",
        "storage_client = storage.Client(project=PROJECT)\n",
        "\n",
        "# Define the constants\n",
        "TARGET = 'income_bracket'\n",
        "ARTIFACTS_PATH = os.getenv(\"AIP_MODEL_DIR\")\n",
        "# Get the bucket name from the model dir\n",
        "BUCKET_NAME = ARTIFACTS_PATH.replace(\"gs://\",\"\").split(\"/\")[0]\n",
        "\n",
        "MODEL_FILENAME = 'model.joblib' \n",
        "# Define the format of your input data, excluding the target column.\n",
        "# These are the columns from the census data files.\n",
        "COLUMNS = [\n",
        "    'age',\n",
        "    'workclass',\n",
        "    'functional_weight',\n",
        "    'education',\n",
        "    'education_num',\n",
        "    'marital_status',\n",
        "    'occupation',\n",
        "    'relationship',\n",
        "    'race',\n",
        "    'sex',\n",
        "    'capital_gain',\n",
        "    'capital_loss',\n",
        "    'hours_per_week',\n",
        "    'native_country'\n",
        "]\n",
        "# Categorical columns are columns that need to be turned into a numerical value to be used by scikit-learn\n",
        "CATEGORICAL_COLUMNS = [\n",
        "    'workclass',\n",
        "    'education',\n",
        "    'marital_status',\n",
        "    'occupation',\n",
        "    'relationship',\n",
        "    'race',\n",
        "    'sex',\n",
        "    'native_country'\n",
        "]\n",
        "\n",
        "# Function to fetch the data from BigQuery\n",
        "def download_table(bq_table_uri: str):\n",
        "    prefix = \"bq://\"\n",
        "    if bq_table_uri.startswith(prefix):\n",
        "        bq_table_uri = bq_table_uri[len(prefix):]\n",
        "\n",
        "    table = bigquery.TableReference.from_string(bq_table_uri)\n",
        "    rows = bq_client.list_rows(\n",
        "        table,\n",
        "    )\n",
        "    return rows.to_dataframe(create_bqstorage_client=False)\n",
        "\n",
        "# Function to upload local files to GCS\n",
        "def upload_model(bucket_name: str,\n",
        "                filename: str):\n",
        "     # Upload the saved model file to GCS\n",
        "    bucket = storage_client.get_bucket(bucket_name)\n",
        "    storage_path = os.path.join(ARTIFACTS_PATH, filename)\n",
        "    blob = storage.blob.Blob.from_string(storage_path, client=storage_client)\n",
        "    blob.upload_from_filename(filename)\n",
        "    \n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Load the training data\n",
        "    X_train = download_table(TRAINING_DATA_URI)\n",
        "\n",
        "    # Remove the column we are trying to predict ('income-level') from our features list\n",
        "    # Convert the Dataframe to a lists of lists\n",
        "    train_features = X_train.drop(TARGET, axis=1).to_numpy().tolist()\n",
        "    # Create our training labels list, convert the Dataframe to a lists of lists\n",
        "    train_labels = X_train[TARGET].to_numpy().tolist()\n",
        "\n",
        "    # Since the census data set has categorical features, we need to convert\n",
        "    # them to numerical values. We use a list of pipelines to convert each\n",
        "    # categorical column and then use FeatureUnion to combine them before calling\n",
        "    # the RandomForestClassifier.\n",
        "    categorical_pipelines = []\n",
        "\n",
        "    # Each categorical column needs to be extracted individually and converted to a numerical value.\n",
        "    # To do this, each categorical column use a pipeline that extracts one feature column via\n",
        "    # SelectKBest(k=1) and a LabelBinarizer() to convert the categorical value to a numerical one.\n",
        "    # A scores array (created below) selects and extracts the feature column. The scores array is\n",
        "    # created by iterating over the COLUMNS and checking if it's a CATEGORICAL_COLUMN.\n",
        "    for i, col in enumerate(COLUMNS):\n",
        "        if col in CATEGORICAL_COLUMNS:\n",
        "            # Create a scores array to get the individual categorical column.\n",
        "            # Example:\n",
        "            #  data = [39, 'State-gov', 77516, 'Bachelors', 13, 'Never-married', 'Adm-clerical',\n",
        "            #         'Not-in-family', 'White', 'Male', 2174, 0, 40, 'United-States']\n",
        "            #  scores = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "            #\n",
        "            # Returns: [['Sate-gov']]\n",
        "            scores = []\n",
        "            # Build the scores array\n",
        "            for j in range(len(COLUMNS)):\n",
        "                if i == j: # This column is the categorical column we want to extract.\n",
        "                    scores.append(1) # Set to 1 to select this column\n",
        "                else: # Every other column should be ignored.\n",
        "                    scores.append(0)\n",
        "            skb = SelectKBest(k=1)\n",
        "            skb.scores_ = scores\n",
        "            # Convert the categorical column to a numerical value\n",
        "            lbn = LabelBinarizer()\n",
        "            r = skb.transform(train_features)\n",
        "            lbn.fit(r)\n",
        "            # Create the pipeline to extract the categorical feature\n",
        "            categorical_pipelines.append(\n",
        "                ('categorical-{}'.format(i), Pipeline([\n",
        "                    ('SKB-{}'.format(i), skb),\n",
        "                    ('LBN-{}'.format(i), lbn)])))\n",
        "\n",
        "    # Create pipeline to extract the numerical features\n",
        "    skb = SelectKBest(k=6)\n",
        "    # From COLUMNS use the features that are numerical\n",
        "    skb.scores_ = [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0]\n",
        "    categorical_pipelines.append(('numerical', skb))\n",
        "\n",
        "    # Combine all the features using FeatureUnion\n",
        "    preprocess = FeatureUnion(categorical_pipelines)\n",
        "\n",
        "    # Create the classifier\n",
        "    classifier = RandomForestClassifier()\n",
        "\n",
        "    # Transform the features and fit them to the classifier\n",
        "    classifier.fit(preprocess.transform(train_features), train_labels)\n",
        "\n",
        "    # Create the overall model as a single pipeline\n",
        "    pipeline = Pipeline([\n",
        "        ('union', preprocess),\n",
        "        ('classifier', classifier)\n",
        "    ])\n",
        "\n",
        "    # Save the pipeline locally\n",
        "    joblib.dump(pipeline, MODEL_FILENAME)\n",
        "    \n",
        "    # Upload the locally saved model to GCS\n",
        "    upload_model(bucket_name = BUCKET_NAME, \n",
        "                 filename=MODEL_FILENAME\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaaf2a432b4e"
      },
      "source": [
        "### Create a setup file\n",
        "Create a setup.py file that tells Setuptools how to create the source distribution. You also specify your application's standard dependencies as part of the setup.py file. Vertex AI uses pip to install your training application on the replicas that it allocates for your job. \n",
        "\n",
        "Learn more about [Setuptools](https://setuptools.readthedocs.io/en/latest/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ee443878b01"
      },
      "outputs": [],
      "source": [
        "%%writefile python_package/setup.py\n",
        "\n",
        "from setuptools import find_packages\n",
        "from setuptools import setup\n",
        "\n",
        "REQUIRED_PACKAGES = ['pandas','pyarrow']\n",
        "\n",
        "setup(\n",
        "    name='trainer',\n",
        "    version='0.1',\n",
        "    packages=find_packages(),\n",
        "    include_package_data=True,\n",
        "    description='My training application.'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf70c1e585ec"
      },
      "source": [
        "### Create the source distribution\n",
        "\n",
        "Run the following command to create a source distribution, dist/trainer-0.1.tar.gz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3601e4802a8"
      },
      "outputs": [],
      "source": [
        "!cd python_package && python3 setup.py sdist --formats=gztar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdff577e26dd"
      },
      "source": [
        "### Copy the source distribution to Cloud Storage\n",
        "\n",
        "To train the custom classification model using a prebuilt container, copy the source distribution of your training application to a Cloud Storage path. While training you let the Vertex AI SDK locate the package through the python_package_gcs_uri parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbf36204476c"
      },
      "outputs": [],
      "source": [
        "!gsutil cp -r python_package/dist/* $BUCKET_URI/training_package/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9c16e6aef37"
      },
      "source": [
        "## Create and run the pipeline\n",
        "\n",
        "All the preparations have been done for your pipeline. In the current step, you create a Vertex AI Pipeline that comprises the following components each serving their own purpose in order:\n",
        "\n",
        "- TabularDatasetCreateOp: Creates a new managed tabular dataset in Vertex AI. \n",
        "- CustomPythonPackageTrainingJobRunOp: Creates and runs a custom training job in Vertex AI using a Python package.\n",
        "- ModelBatchPredictOp: Creates a batch prediction job in Vertex AI and waits for it to complete.\n",
        "\n",
        "All the above components are imported from the google-cloud-pipeline-components Python library. Learn more about [Google Cloud Pipeline Components](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.26/index.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2861ae02f12"
      },
      "outputs": [],
      "source": [
        "# Define the pipeline\n",
        "\n",
        "\n",
        "@pipeline(name=\"custom-model-bq-batch-prediction-pipeline\")\n",
        "def custom_model_bq_batch_prediction_pipeline(\n",
        "    project: str,\n",
        "    location: str,\n",
        "    dataset_display_name: str,\n",
        "    dataset_bq_source: str,\n",
        "    training_job_dispaly_name: str,\n",
        "    gcs_staging_directory: str,\n",
        "    python_package_gcs_uri: str,\n",
        "    python_package_module_name: str,\n",
        "    training_split: float,\n",
        "    test_split: float,\n",
        "    training_container_uri: str,\n",
        "    serving_container_uri: str,\n",
        "    training_bigquery_destination: str,\n",
        "    model_display_name: str,\n",
        "    batch_prediction_display_name: str,\n",
        "    batch_prediction_instances_format: str,\n",
        "    batch_prediction_predictions_format: str,\n",
        "    batch_prediction_source_uri: str,\n",
        "    batch_prediction_destination_uri: str,\n",
        "    batch_prediction_machine_type: str = \"n1-standard-4\",\n",
        "    batch_prediction_batch_size: int = 1000,\n",
        "):\n",
        "    from google_cloud_pipeline_components.aiplatform import (\n",
        "        CustomPythonPackageTrainingJobRunOp, ModelBatchPredictOp,\n",
        "        TabularDatasetCreateOp)\n",
        "\n",
        "    # Create the dataset\n",
        "    dataset_create_op = TabularDatasetCreateOp(\n",
        "        project=project,\n",
        "        location=location,\n",
        "        display_name=dataset_display_name,\n",
        "        bq_source=dataset_bq_source,\n",
        "    )\n",
        "\n",
        "    # Run the training task\n",
        "    train_op = CustomPythonPackageTrainingJobRunOp(\n",
        "        display_name=training_job_dispaly_name,\n",
        "        python_package_gcs_uri=python_package_gcs_uri,\n",
        "        python_module_name=python_package_module_name,\n",
        "        container_uri=training_container_uri,\n",
        "        model_display_name=model_display_name,\n",
        "        model_serving_container_image_uri=serving_container_uri,\n",
        "        dataset=dataset_create_op.outputs[\"dataset\"],\n",
        "        base_output_dir=gcs_staging_directory,\n",
        "        bigquery_destination=training_bigquery_destination,\n",
        "        training_fraction_split=training_split,\n",
        "        test_fraction_split=test_split,\n",
        "        staging_bucket=gcs_staging_directory,\n",
        "    )\n",
        "\n",
        "    # Run the batch prediction task\n",
        "    _ = ModelBatchPredictOp(\n",
        "        project=project,\n",
        "        location=location,\n",
        "        model=train_op.outputs[\"model\"],\n",
        "        instances_format=batch_prediction_instances_format,\n",
        "        bigquery_source_input_uri=batch_prediction_source_uri,\n",
        "        predictions_format=batch_prediction_predictions_format,\n",
        "        bigquery_destination_output_uri=batch_prediction_destination_uri,\n",
        "        job_display_name=batch_prediction_display_name,\n",
        "        machine_type=batch_prediction_machine_type,\n",
        "        manual_batch_tuning_parameters_batch_size=batch_prediction_batch_size,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0150d697ac31"
      },
      "source": [
        "### Compile the pipeline\n",
        "\n",
        "After defining your pipeline, compile it to a file (PIPELINE_FILE_NAME) in `JSON` or `YAML` format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2e755cae3948"
      },
      "outputs": [],
      "source": [
        "# Compile the pipeline\n",
        "compiler.Compiler().compile(\n",
        "    pipeline_func=custom_model_bq_batch_prediction_pipeline,\n",
        "    package_path=PIPELINE_FILE_NAME,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96edd12acf43"
      },
      "source": [
        "### Set the parameters\n",
        "\n",
        "Now, define the paramters to run your pipeline.\n",
        "\n",
        "To pass the required arguments to the individual components in your pipeline, you define the following paramters:\n",
        "- project: Project ID for the Google Cloud project where the pipeline needs to run.\n",
        "- location: Region where the pipeline needs to run.\n",
        "- dataset_display_name: Display name for the managed dataset resource in Vertex AI.\n",
        "- dataset_bq_source: BigQuery table URI to serve as a source for the managed dataset in Vertex AI.\n",
        "- training_job_dispaly_name: Display name for the the custom python package training job.\n",
        "- gcs_staging_directory: Staging directory for Vertex AI to store training artifacts.\n",
        "- python_package_gcs_uri: Cloud Storage path to the Python package for training.\n",
        "- python_package_module_name: Module name (trainer task) inside the Python package for training.\n",
        "- training_split: Percentage of the total data to be considered for training.\n",
        "- test_split: Percentage of the total data to be considered for testing. Split percentage parameters provided for the **CustomPythonPackageTrainingJobRunOp** component should always sum up to 1.\n",
        "- training_container_uri: Prebuilt container image URI for training the model. \n",
        "- serving_container_uri: Prebuilt container image URI for serving the model on Vertex AI.\n",
        "- training_bigquery_destination: The BigQuery project location where the training data is to be written to during training.\n",
        "- model_display_name: Dispaly name for the model to be deployed in Vertex AI Model Registry.\n",
        "- batch_prediction_display_name: Dispaly name for the batch prediction job.\n",
        "- batch_prediction_instances_format: Format of the input instances for batch prediction.\n",
        "- batch_prediction_predictions_format: Format of the results from the batch prediction.\n",
        "- batch_prediction_source_uri: Source URI of the input data.\n",
        "- batch_prediction_destination_uri: Destination URI where the batch prediction results need to be stored.\n",
        "\n",
        "**Note:** Though a test split percentage is provided, test data isn't used during the training process. This test data is different from the test data created in earlier steps for batch prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33462dc29ac6"
      },
      "outputs": [],
      "source": [
        "# Define the parameters for running the pipeline\n",
        "parameters = {\n",
        "    \"project\": PROJECT_ID,\n",
        "    \"location\": LOCATION,\n",
        "    \"dataset_display_name\": DATASET_DISPLAY_NAME,\n",
        "    \"dataset_bq_source\": DATA_SOURCE,\n",
        "    \"training_job_dispaly_name\": TRAINING_JOB_DISPLAY_NAME,\n",
        "    \"gcs_staging_directory\": BUCKET_URI,\n",
        "    \"python_package_gcs_uri\": f\"{BUCKET_URI}/training_package/trainer-0.1.tar.gz\",\n",
        "    \"python_package_module_name\": \"trainer.task\",\n",
        "    \"training_split\": TRAIN_SPLIT,\n",
        "    \"test_split\": 1 - TRAIN_SPLIT,\n",
        "    \"training_container_uri\": TRAINING_CONTAINER,\n",
        "    \"serving_container_uri\": SERVING_CONTAINER,\n",
        "    \"training_bigquery_destination\": f\"bq://{PROJECT_ID}\",\n",
        "    \"model_display_name\": MODEL_DISPLAY_NAME,\n",
        "    \"batch_prediction_display_name\": BATCH_PREDICTION_JOB_NAME,\n",
        "    \"batch_prediction_instances_format\": \"bigquery\",\n",
        "    \"batch_prediction_predictions_format\": \"bigquery\",\n",
        "    \"batch_prediction_source_uri\": f\"bq://{PROJECT_ID}.{BQ_DATASET_ID}.{BQ_INPUT_TABLE}\",\n",
        "    \"batch_prediction_destination_uri\": f\"bq://{PROJECT_ID}.{BQ_DATASET_ID}\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15a1270831ca"
      },
      "source": [
        "### Run the pipeline\n",
        "\n",
        "Create a Vertex AI Pipeline job and run it using the `PipelineJob` class.\n",
        "\n",
        "The PipelineJob class takes the following parameters:\n",
        "\n",
        "- display_name: The display name of the Vertex AI pipeline.\n",
        "- template_path: The path of PipelineJob or PipelineSpec (JSON or YAML) file.\n",
        "- parameter_values: The mapping from runtime parameter names to its values that control the pipeline run.\n",
        "- enable_caching: Whether to turn on caching for the run.\n",
        "\n",
        "Learn more about the `PipelineJob` class from [Vertex AI PipelineJob documentation](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.PipelineJob)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7efd80523c99"
      },
      "outputs": [],
      "source": [
        "# Create a Vertex AI Pipeline job\n",
        "job = aiplatform.PipelineJob(\n",
        "    display_name=PIPELINE_DISPLAY_NAME,\n",
        "    template_path=PIPELINE_FILE_NAME,\n",
        "    parameter_values=parameters,\n",
        "    enable_caching=True,\n",
        ")\n",
        "# Run the pipeline job\n",
        "job.run(service_account=SERVICE_ACCOUNT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfeb56af21ae"
      },
      "source": [
        "## Fetch results from the predictions table\n",
        "\n",
        "After the Vertex AI pipeline job is finished successfully, fetch the results from batch prediction into a dataframe.\n",
        "\n",
        "### Get the table name\n",
        "\n",
        "Run the below cell to get the name of the predictions table from the pipeline job's artifact details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6e00bc95662"
      },
      "outputs": [],
      "source": [
        "OUTPUT_TABLE = None\n",
        "# Load the batch prediction job details using the display name\n",
        "[batch_prediction_job] = aiplatform.BatchPredictionJob.list(\n",
        "    filter=f'display_name=\"{BATCH_PREDICTION_JOB_NAME}\"'\n",
        ")\n",
        "# Fetch the name of the output table\n",
        "OUTPUT_TABLE = batch_prediction_job.output_info.bigquery_output_table\n",
        "print(\"Predictions table ID:\", OUTPUT_TABLE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bab610ea646d"
      },
      "source": [
        "### Query the results table\n",
        "\n",
        "Fetch a specified number of rows from the predictions table using the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99738dd9e4aa"
      },
      "outputs": [],
      "source": [
        "# Specify the needed no.of rows\n",
        "ROWS = 10\n",
        "# Define the query\n",
        "query = f\"\"\"\n",
        "    Select prediction from `{PROJECT_ID}.{BQ_DATASET_ID}.{OUTPUT_TABLE}` limit {ROWS}\n",
        "\"\"\"\n",
        "# Fetch the data into a dataframe\n",
        "df = bq_client.query(query).to_dataframe()\n",
        "# Display the dataframe\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a0e1c095133"
      },
      "source": [
        "### Fetch the resources for deletion\n",
        "\n",
        "Using the display names of the individual resources, load the resources created inside the pipeline for the clean up step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "171452c4157b"
      },
      "outputs": [],
      "source": [
        "# Load the Vertex AI tabular dataset using the display name\n",
        "[dataset] = aiplatform.TabularDataset.list(\n",
        "    filter=f'display_name=\"{DATASET_DISPLAY_NAME}\"'\n",
        ")\n",
        "\n",
        "# Load the Vertex AI model resource using the display name\n",
        "[model] = aiplatform.Model.list(filter=f'display_name=\"{MODEL_DISPLAY_NAME}\"')\n",
        "\n",
        "# Load the custom training job using the display name\n",
        "[training_job] = aiplatform.CustomPythonPackageTrainingJob.list(\n",
        "    filter=f'display_name=\"{TRAINING_JOB_DISPLAY_NAME}\"'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adac675f7ac3"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:\n",
        "\n",
        "- Vertex AI Pipeline job\n",
        "- Vertex AI TabularDataset\n",
        "- Vertex AI model\n",
        "- Vertex AI Training job\n",
        "- Vertex AI batch prediction job\n",
        "- BigQuery dataset\n",
        "- Cloud Storage bucket (Set `delete_bucket` to **True** to delete the Cloud Storage bucket)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53c239fc8667"
      },
      "outputs": [],
      "source": [
        "delete_bucket = False\n",
        "\n",
        "# Delete the Vertex AI Pipeline job\n",
        "job.delete()\n",
        "\n",
        "# Delete the Vertex AI TabularDataset\n",
        "dataset.delete()\n",
        "\n",
        "# Delete the Vertex AI Model\n",
        "model.delete()\n",
        "\n",
        "# Delete the Vertex AI Training job\n",
        "training_job.delete()\n",
        "\n",
        "# Delete the Vertex AI Batch prediction job\n",
        "batch_prediction_job.delete()\n",
        "\n",
        "# Delete the BigQuery dataset\n",
        "! bq rm -r -f -d $PROJECT_ID:$BQ_DATASET_ID\n",
        "\n",
        "# Delete Cloud Storage objects\n",
        "if delete_bucket:\n",
        "    ! gsutil -m rm -r $BUCKET_URI\n",
        "\n",
        "! rm $PIPELINE_FILE_NAME"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "custom_tabular_train_batch_pred_bq_pipeline.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

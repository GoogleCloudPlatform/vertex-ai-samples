{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86d2d4c-6785-40e3-bd9c-423a47d05b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ada523-399a-466c-8d97-a7c62b880b59",
   "metadata": {},
   "source": [
    "# Vertex AI Pipelines: Batch Prediction with BigQuery source and destinantion from a Custom Tabular Classification model\n",
    "\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/custom_tabular_classification_bq_io_pipeline.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/custom_tabular_classification_bq_io_pipeline.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/custom_tabular_classification_bq_io_pipeline.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>                                                                                               \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fd9940-345b-480e-ba9c-5a8b95b6935e",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to do a batch prediction with a custom tabular classification model inside a Vertex AI pipeline. The pipeline is constructed with the `google_cloud_pipeline_components` Python library and the batch prediction is configured with a bigquery source and destination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de1a404-c3b8-4537-9b4f-43a622304205",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you train a scikit-learn Tabular Classification model and learn how to create batch prediction job for it through a Vertex AI pipeline job using `google_cloud_pipeline_components`. The source and destination data for the batch prediction job is served in BigQuery.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services and resources:\n",
    "\n",
    "- Vertex AI `Model Registry`\n",
    "- Vertex AI `Pipelines`\n",
    "- Vertex AI `Batch Predictions`\n",
    "\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Save data to BigQuery for batch prediction.\n",
    "- Train a scikit-learn RandomForest classification model on the dataset.\n",
    "- Upload the scikit-learn model to Vertex AI Model Registry.\n",
    "- Create a Vertex AI Pipeline job to fetch the model and run a batch prediction job.\n",
    "- Run the Vertex AI Pipeline job.\n",
    "- Check the prediction results from the destination BigQuery table.\n",
    "- Clean up the resources created in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01a9e9b-73f7-49fb-b3cb-3595ef6cb44f",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The [Census Income Data Set](https://archive.ics.uci.edu/ml/datasets/Census+Income) that this notebook uses for training is available publicly at the BigQuery location `bigquery-public-data.ml_datasets.census_adult_income`. It consists of the following fields:\n",
    "\n",
    "- `age`: Age.\n",
    "- `workclass`: Nature of employment.\n",
    "- `functional_weight`: Sample weight of the individual from the original Census data. How likely they were to be included in this dataset, based on their demographic characteristics vs. whole-population estimates.\n",
    "- `education`: Level of education completed.\n",
    "- `education_num`: Estimated years of education completed based on the value of the education field.\n",
    "- `marital_status`: Marital status.\n",
    "- `occupation`: Occupation category.\n",
    "- `relationship`: Relationship to the household.\n",
    "- `race`: Race.\n",
    "- `sex`: Gender.\n",
    "- `capital_gain`: Amount of capital gains.\n",
    "- `capital_loss`: Amount of capital loss.\n",
    "- `hours_per_week`: Hours worked per week.\n",
    "- `native_country`: Country of birth.\n",
    "- `income_bracket`: Either \" >50K\" or \" <=50K\" based on income."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e49cf9a-bcc7-4be5-bff9-cc86929b2314",
   "metadata": {},
   "source": [
    "### Costs \n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* BigQuery\n",
    "* Cloud Storage\n",
    "* Artifact Registry\n",
    "* Cloud Build\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing), [BigQuery pricing](https://cloud.google.com/bigquery/pricing), [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), [Artifact Registry pricing](https://cloud.google.com/artifact-registry/pricing), [Cloud Build pricing](https://cloud.google.com/build/pricing) and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbf90ae-a2af-4012-b256-0094005c9ed1",
   "metadata": {},
   "source": [
    "### Set up your local development environment\n",
    "\n",
    "**If you are using Colab or Vertex AI Workbench Notebooks**, your environment already meets\n",
    "all the requirements to run this notebook.\n",
    "\n",
    "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
    "You need the following:\n",
    "\n",
    "* The Google Cloud SDK\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* Jupyter notebook running in a virtual environment with Python 3\n",
    "\n",
    "The Google Cloud guide to [Setting up a Python development\n",
    "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
    "installation guide](https://jupyter.org/install) provide detailed instructions\n",
    "for meeting these requirements. The following steps provide a condensed set of\n",
    "instructions:\n",
    "\n",
    "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "1. [Install\n",
    "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
    "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
    "\n",
    "1. To install Jupyter, run `pip3 install jupyter` on the\n",
    "command-line in a terminal shell.\n",
    "\n",
    "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
    "\n",
    "1. Open this notebook in the Jupyter Notebook Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc47b5d7-ea76-49f3-b32b-ca075def77cd",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Install the following packages required to execute this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a6e9a4-c381-4dfd-a559-2507dffee62a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform \\\n",
    "                            google-cloud-bigquery \\\n",
    "                            kfp \\\n",
    "                            google-cloud-pipeline-components \\\n",
    "                            pyarrow \\\n",
    "                            pandas {USER_FLAG} -q\n",
    "! pip3 install scikit-learn==1.0  {USER_FLAG} -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67e5aac-2ebd-4673-819d-99095b0567f8",
   "metadata": {},
   "source": [
    "### Restart the kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c180cf94-93ce-4be0-8570-6e5f1954c73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eead195-57de-4eb7-bd78-bb788bac5418",
   "metadata": {},
   "source": [
    "## Before you begin\n",
    "\n",
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. [Enable the Vertex AI, Compute Engine, Artifact Registry, BigQuery and Cloud Build APIs](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute.googleapis.com,artifactregistry.googleapis.com,bigquery.googleapis.com,cloudbuild.googleapis.com).\n",
    "\n",
    "1. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    "\n",
    "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e5f7dc-a665-4b7f-a03f-e20809bb3db1",
   "metadata": {},
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baafa58-1cdb-4d21-93c8-c1f8ebbc9425",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3300a6b5-602f-4f94-a686-71e0571b9f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8094bce0-2531-45fa-8408-78b977ae11fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9736ef-24c6-4bd5-acce-dda88e22b4e8",
   "metadata": {},
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex AI. It is recommended that you choose the region closest to you.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
    "\n",
    "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bebc78-9ab5-46c5-b69d-caaf8086eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6178e8f0-f6d5-4a43-8968-51f1d70cef1a",
   "metadata": {},
   "source": [
    "#### UUID\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a uuid for each instance session, and append it onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98d258c-fc56-45ce-bbd3-93e8227b9192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ca1822-6891-4b4a-8a1d-bbbfd447d66f",
   "metadata": {},
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "**If you are using Vertex AI Workbench Notebooks**, your environment is already\n",
    "authenticated. \n",
    "\n",
    "**If you are using Colab**, run the cell below and follow the instructions\n",
    "when prompted to authenticate your account via oAuth.\n",
    "\n",
    "**Otherwise**, follow these steps:\n",
    "\n",
    "1. In the Cloud Console, go to the [**Create service account key**\n",
    "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
    "\n",
    "2. Click **Create service account**.\n",
    "\n",
    "3. In the **Service account name** field, enter a name, and\n",
    "   click **Create**.\n",
    "\n",
    "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
    "into the filter box, and select\n",
    "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
    "\n",
    "5. Click *Create*. A JSON file that contains your key downloads to your\n",
    "local environment.\n",
    "\n",
    "6. Enter the path to your service account key as the\n",
    "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df1568a-8def-499a-bece-3deee9d071a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If on Vertex AI Workbench, then don't execute this code\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
    "    \"DL_ANACONDA_HOME\"\n",
    "):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS '[your-service-account-key-path]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e347faa-e17d-44e8-9e31-0c916bdf2a1d",
   "metadata": {},
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "When you submit a Pipeline or a Batch prediction job using the Vertex AI SDK, Vertex AI uses a Cloud Storage bucket as a staging location. Instead of providing while running the job, the staging location can also be provided to Vertex AI while initializing. In this tutorial, Vertex AI is initialized with a staging bucket that you create in the next steps.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
    "Cloud Storage buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e4e917-cef9-4037-92c5-1414db6e55b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1a4fd2-ab06-444b-9327-6950006e2671",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
    "    BUCKET_NAME = PROJECT_ID + \"aip-\" + UUID\n",
    "    BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe18fcf-4501-409b-a521-1020ca77fd80",
   "metadata": {},
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58370793-fd31-49e2-ae83-db549ea34fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2d396a-b45e-4473-b739-04c2e7bdfbd0",
   "metadata": {},
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400a1b50-cd04-4a8a-bd96-06a212aadd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee747771-7af2-4283-b6c5-c8e6527f2cc4",
   "metadata": {},
   "source": [
    "#### Service Account\n",
    "\n",
    "You use a service account to create Vertex AI Pipeline jobs. If you do not want to use your project's Compute Engine service account, set `SERVICE_ACCOUNT` to another service account ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164c1bb7-4b2b-4f58-95be-5a4106fd15b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f71ab4-3669-4977-b8c4-6d6b91305f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your service account from gcloud\n",
    "    if not IS_COLAB:\n",
    "        shell_output = !gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "    else:  # IS_COLAB:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf2e036-9df1-4142-9b9f-160f9754a34a",
   "metadata": {},
   "source": [
    "#### Set service account access for Vertex AI Pipelines\n",
    "\n",
    "Run the following commands to grant your service account access to read and write pipeline artifacts in the bucket that you created in the previous step. You only need to run this step once per service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f88356-a0ad-4502-9ef0-82dacd93376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d1c617-9813-4eef-bd8e-87bae76e9160",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "\n",
    "Import the Vertex AI Python SDK and other required Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e92c0f-bf42-40bc-9c41-678c6fdf3f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform_v1 import types\n",
    "import kfp\n",
    "from kfp.v2 import compiler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a657cf-681b-4483-9628-a79a128879c8",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31b9951-68b1-4aca-bdd1-ba55ff2f3c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI SDK\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)\n",
    "\n",
    "# Initialize BigQuery client \n",
    "bq_client = bigquery.Client(\n",
    "    project=PROJECT_ID,\n",
    "    credentials=aiplatform.initializer.global_config.credentials,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854b5634-dde6-41e1-a342-027aaf5c62d6",
   "metadata": {},
   "source": [
    "### Define constants\n",
    "\n",
    "Set constants that you need while training the model, creating and running the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fa59e3-31f5-4cb4-8214-61f2a674ae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SOURCE = \"bigquery-public-data.ml_datasets.census_adult_income\" # Source of the dataset\n",
    "TEST_SIZE = 0.25 # size(%) of the test set\n",
    "RANDOM_STATE = 36 # Random state\n",
    "\n",
    "# Define the format of your input data, excluding the target column.\n",
    "# These are the columns from the census data files.\n",
    "COLUMNS = [\n",
    "    'age',\n",
    "    'workclass',\n",
    "    'functional_weight',\n",
    "    'education',\n",
    "    'education_num',\n",
    "    'marital_status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'capital_gain',\n",
    "    'capital_loss',\n",
    "    'hours_per_week',\n",
    "    'native_country'\n",
    "]\n",
    "\n",
    "# Categorical columns are columns that need to be turned into a numerical value to be used by scikit-learn\n",
    "CATEGORICAL_COLUMNS = [\n",
    "    'workclass',\n",
    "    'education',\n",
    "    'marital_status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'native_country'\n",
    "]\n",
    "\n",
    "# Target column for training\n",
    "TARGET = 'income_bracket'\n",
    "\n",
    "# BigQuery Dataset name\n",
    "BQ_DATASET_ID = f\"income_prediction_{UUID}\"\n",
    "\n",
    "# name of the BigQuery source table for batch prediction\n",
    "BQ_INPUT_TABLE = f\"income_test_data_{UUID}\"\n",
    "\n",
    "# name of the BigQuery output table for batch prediction\n",
    "BQ_OUTPUT_TABLE = f\"{BQ_INPUT_TABLE}_predictions\"\n",
    "\n",
    "# Filename to save the model locally\n",
    "MODEL_FILENAME = \"model.joblib\"\n",
    "\n",
    "# Model display name for Vertex AI Model Registry\n",
    "MODEL_DISPLAY_NAME = f\"income_pred_skl_{UUID}\"\n",
    "\n",
    "# Dispaly name for the Vertex AI Pipeline\n",
    "PIPELINE_DISPLAY_NAME = f\"income_classifier_pipeline_{UUID}\"\n",
    "\n",
    "# Filename to compile the pipeline to\n",
    "PIPELINE_FILENAME = \"income_classification_pipeline.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884483a9-f992-4f2b-94ef-d367a23dde88",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "\n",
    "### Fetch the Dataset\n",
    "\n",
    "Get the data from the public BigQuery dataset to train your model locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6b7185-1f4f-4d84-8c0b-28c106ec0705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to fetch the data from BQ\n",
    "query = f'''\n",
    "Select * from `{DATA_SOURCE}`\n",
    "'''\n",
    "\n",
    "# Fetch the data into a dataframe\n",
    "df = bq_client.query(query).to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85531f27-0175-4938-8e32-c9f740b7b54d",
   "metadata": {},
   "source": [
    "### Split into train and test sets\n",
    "\n",
    "Split the fetched dataset into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c28df8-bb99-4629-9968-e81972fd3e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test\n",
    "X_train, X_test = train_test_split(\n",
    "    df, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc278d8b-bacc-428c-9053-cbd6063455f8",
   "metadata": {},
   "source": [
    "### Save test data to BigQuery\n",
    "\n",
    "To run a batch prediction using BigQuery input, save the test data to a BigQuery table. \n",
    "\n",
    "#### Create Schema configuration\n",
    "\n",
    "To create a table in BigQuery with proper data types, define the schema configuration of the table based on the data-types available in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae545285-a9b2-467e-b750-2acaba39cd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema configuration\n",
    "schema_config = []\n",
    "for i in COLUMNS:\n",
    "    if X_test[i].dtype == \"int64\":\n",
    "        schema_config.append(bigquery.SchemaField(i, 'INTEGER'))\n",
    "    elif X_test[i].dtype in ['object', 'category']:\n",
    "        schema_config.append(bigquery.SchemaField(i, 'STRING'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ed9804-eb3f-4e98-8ad1-e2c41217c498",
   "metadata": {},
   "source": [
    "#### Create a BigQuery Dataset\n",
    "\n",
    "Run the below cell to create a dataset in BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc933100-99b0-40a5-a25a-991a8d3aca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BQ dataset\n",
    "bq_dataset = bigquery.Dataset(f\"{PROJECT_ID}.{BQ_DATASET_ID}\")\n",
    "bq_dataset = bq_client.create_dataset(bq_dataset)\n",
    "print(f\"Created dataset {bq_client.project}.{bq_dataset.dataset_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55e8a5c-d290-47be-bcdd-cc619408c784",
   "metadata": {},
   "source": [
    "#### Create the table from dataframe\n",
    "\n",
    "Create the BigQuery table and load the test data from dataframe.\n",
    "\n",
    "**Note:** You save only the features to the table excluding the target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f271bdf-a4ca-44f8-b99c-d7bbed7fddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BQ table from dataframe\n",
    "table_ref = bq_dataset.table(BQ_INPUT_TABLE)\n",
    "job_config = bigquery.LoadJobConfig(schema=schema_config,\n",
    "                                           write_disposition=\"WRITE_TRUNCATE\" )\n",
    "\n",
    "job = bq_client.load_table_from_dataframe(X_test[COLUMNS], table_ref, location = REGION.split('-')[0].upper())\n",
    "\n",
    "job.result()  # Waits for table load to complete.\n",
    "print(\"Loaded dataframe to {}\".format(table_ref.path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aa7454-4e33-4ff1-8646-db6f3374be68",
   "metadata": {},
   "source": [
    "## Preprocess the data\n",
    "\n",
    "Run the code below to preprocess the train set and store the preprocessing steps in a scikit-learn pipeline. \n",
    "\n",
    "The scikit-learn pipeline saves you from the trouble of writing additional scripts to preprocess the test data for generating predictions. \n",
    "\n",
    "Learn more about [scikit-learn pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135c612b-e17c-4325-9921-0a6f145b2378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the column we are trying to predict ('income-level') from our features list\n",
    "# Convert the Dataframe to a lists of lists\n",
    "train_features = X_train.drop(TARGET, axis=1).to_numpy().tolist()\n",
    "# Create our training labels list, convert the Dataframe to a lists of lists\n",
    "train_labels = X_train[TARGET].to_numpy().tolist()\n",
    "\n",
    "# Since the census data set has categorical features, we need to convert\n",
    "# them to numerical values. We use a list of pipelines to convert each\n",
    "# categorical column and then use FeatureUnion to combine them before calling\n",
    "# the RandomForestClassifier.\n",
    "categorical_pipelines = []\n",
    "\n",
    "# Each categorical column needs to be extracted individually and converted to a numerical value.\n",
    "# To do this, each categorical column use a pipeline that extracts one feature column via\n",
    "# SelectKBest(k=1) and a LabelBinarizer() to convert the categorical value to a numerical one.\n",
    "# A scores array (created below) selects and extracts the feature column. The scores array is\n",
    "# created by iterating over the COLUMNS and checking if it is a CATEGORICAL_COLUMN.\n",
    "for i, col in enumerate(COLUMNS):\n",
    "    if col in CATEGORICAL_COLUMNS:\n",
    "        # Create a scores array to get the individual categorical column.\n",
    "        # Example:\n",
    "        #  data = [39, 'State-gov', 77516, 'Bachelors', 13, 'Never-married', 'Adm-clerical',\n",
    "        #         'Not-in-family', 'White', 'Male', 2174, 0, 40, 'United-States']\n",
    "        #  scores = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        #\n",
    "        # Returns: [['Sate-gov']]\n",
    "        scores = []\n",
    "        # Build the scores array\n",
    "        for j in range(len(COLUMNS)):\n",
    "            if i == j: # This column is the categorical column we want to extract.\n",
    "                scores.append(1) # Set to 1 to select this column\n",
    "            else: # Every other column should be ignored.\n",
    "                scores.append(0)\n",
    "        skb = SelectKBest(k=1)\n",
    "        skb.scores_ = scores\n",
    "        # Convert the categorical column to a numerical value\n",
    "        lbn = LabelBinarizer()\n",
    "        r = skb.transform(train_features)\n",
    "        lbn.fit(r)\n",
    "        # Create the pipeline to extract the categorical feature\n",
    "        categorical_pipelines.append(\n",
    "            ('categorical-{}'.format(i), Pipeline([\n",
    "                ('SKB-{}'.format(i), skb),\n",
    "                ('LBN-{}'.format(i), lbn)])))\n",
    "\n",
    "# Create pipeline to extract the numerical features\n",
    "skb = SelectKBest(k=6)\n",
    "# From COLUMNS use the features that are numerical\n",
    "skb.scores_ = [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0]\n",
    "categorical_pipelines.append(('numerical', skb))\n",
    "\n",
    "# Combine all the features using FeatureUnion\n",
    "preprocess = FeatureUnion(categorical_pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a0b6ce-3c86-4b8e-b118-f63d529d6995",
   "metadata": {},
   "source": [
    "## Create a RandomForest model\n",
    "\n",
    "Using  scikit-learn's `RandomForestClassifier`, fit a Random-forest model on the train data. \n",
    "\n",
    "Further, add the trained classifier to the scikit-learn pipeline.\n",
    "\n",
    "Learn more about the RandomForestClassifer from [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eb7b02-3d8c-4620-a112-6cc5d46ebaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classifier\n",
    "classifier = RandomForestClassifier()\n",
    "\n",
    "# Transform the features and fit them to the classifier\n",
    "classifier.fit(preprocess.transform(train_features), train_labels)\n",
    "\n",
    "# Create the overall model as a single pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('union', preprocess),\n",
    "    ('classifier', classifier)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fdbe13-375d-470a-9669-0742492c64d1",
   "metadata": {},
   "source": [
    "Test the trained classifier on a few samples from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43c662a-0316-472e-91a5-92431aaa7d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances= X_train[COLUMNS].sample(5).to_numpy().tolist()\n",
    "pipeline.predict(instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cece496-d1b8-46a2-b6fd-34fc22a8f76d",
   "metadata": {},
   "source": [
    "Save the model pipeline to a file using `joblib` Python library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eb0649-7d80-4bb4-b135-df7326ee8f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pipeline\n",
    "joblib.dump(pipeline, MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82233f8b-8e37-40c5-9964-002473432cbf",
   "metadata": {},
   "source": [
    "## Upload the model to Vertex AI\n",
    "\n",
    "Create a Vertex AI model resource from the saved model file.\n",
    "\n",
    "For this step, you can use Vertex AI SDK's `Model.upload_scikit_learn_model_file` method that lets you create a Vertex AI model resource directly from your scikit-learn model file. It takes the following arguments:\n",
    "\n",
    "- `model_file_path`: Local file path of the model.\n",
    "- `sklearn_version`: The version of the Scikit-learn serving container. You use \"1.0\" which is the latest supported version in this notebook.\n",
    "- `display_name`: The display name of the Vertex AI Model.\n",
    "\n",
    "Learn more details about Vertex AI Model class from [`Vertex AI Model documentation`](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f4d9dc-0eed-4028-b724-2daa4465057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Vertex AI model resource\n",
    "model = aiplatform.Model.upload_scikit_learn_model_file(\n",
    "            model_file_path=MODEL_FILENAME,\n",
    "            display_name=MODEL_DISPLAY_NAME,\n",
    "            sklearn_version=\"1.0\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e590542-dc01-4704-b910-f519f5aab8f5",
   "metadata": {},
   "source": [
    "Get the resource name of the created Vertex AI Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e67fed-2b04-4959-b9a7-2f721fb392f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_RESOURCE_NAME = model.resource_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badf6d6b-a8a9-4d46-88ec-8020c9688d06",
   "metadata": {},
   "source": [
    "## Create Vertex AI Pipeline\n",
    "\n",
    "Next, you create a Vertex AI pipeline using `google-cloud-pipeline-components`. This pipeline fetches the Vertex AI model resource and runs a batch prediction job with it.\n",
    "\n",
    "### Define the Pipeline \n",
    "\n",
    "The pipeline uses the following components:\n",
    "\n",
    "- `GetVertexModelOp`: Gets a Vertex AI Model Artifact. Learn more about [GetVertexModelOp component](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.23/google_cloud_pipeline_components.experimental.evaluation.html#google_cloud_pipeline_components.experimental.evaluation.GetVertexModelOp).\n",
    "\n",
    "- `ModelBatchPredictOp`: Creates a Google Cloud Vertex AI batch prediction job and waits for it to complete. Learn more about [ModelBatchPredictOp component](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.23/google_cloud_pipeline_components.aiplatform.html#google_cloud_pipeline_components.aiplatform.ModelBatchPredictOp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b6c5d7-300e-4748-9103-ceac1db7471b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "@kfp.dsl.pipeline(\n",
    "    name='custom-model-batch-prediction-pipeline')\n",
    "def custom_model_batch_prediction_pipeline(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    model_name: str,\n",
    "    bigquery_source_input_uri: str,\n",
    "    bigquery_destination_output_uri: str,\n",
    "    batch_prediction_display_name: str = \"model-registry-batch-prediction\",\n",
    "    batch_predict_machine_type: str = \"n1-standard-4\",\n",
    "    \n",
    "):\n",
    "  \n",
    "    from google_cloud_pipeline_components.experimental.evaluation import GetVertexModelOp\n",
    "    from google_cloud_pipeline_components.aiplatform import ModelBatchPredictOp\n",
    "    \n",
    "    # Get the Vertex AI model resource\n",
    "    get_model_task = GetVertexModelOp(model_resource_name=model_name)\n",
    "    \n",
    "    # Run Batch Predictions\n",
    "    _ = ModelBatchPredictOp(\n",
    "                            project=project,\n",
    "                            location=location,\n",
    "                            model=get_model_task.outputs['model'],\n",
    "                            instances_format='bigquery',\n",
    "                            bigquery_source_input_uri=bigquery_source_input_uri,\n",
    "                            predictions_format='bigquery',\n",
    "                            bigquery_destination_output_uri=bigquery_destination_output_uri,\n",
    "                            job_display_name=batch_prediction_display_name,\n",
    "                            machine_type=batch_predict_machine_type\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841ab517-e475-4ef0-b238-bd63a31adae6",
   "metadata": {},
   "source": [
    "### Compile the pipeline\n",
    "\n",
    "Compile the pipeline to a `json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d0c168-ada1-45d1-bc73-df90d57354a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=custom_model_batch_prediction_pipeline,\n",
    "    package_path=PIPELINE_FILENAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd241b3-d10d-4b08-8149-bbd5251987d6",
   "metadata": {},
   "source": [
    "### Run the Pipeline\n",
    "\n",
    "Define the parameters to create a Vertex AI Pipeline job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e77ded4-0a01-446f-9177-cca279b513bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "            'project':PROJECT_ID,\n",
    "            'location':REGION,\n",
    "            'model_name':MODEL_RESOURCE_NAME,\n",
    "            'bigquery_source_input_uri':f\"bq://{PROJECT_ID}.{table_ref.dataset_id}.{table_ref.table_id}\",\n",
    "            'bigquery_destination_output_uri':f\"bq://{PROJECT_ID}.{table_ref.dataset_id}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0099660b-1566-4d9e-a6d9-61443aa6be66",
   "metadata": {},
   "source": [
    "Create a Vertex AI Pipeline job and run it using `PipelineJob` class.\n",
    "\n",
    "The `PipelineJob` class takes the following parameters as arguments:\n",
    "\n",
    "- `display_name`: The display name of the Vertex AI Pipeline.\n",
    "- `template_path`: The path of PipelineJob or PipelineSpec JSON (or YAML) file.\n",
    "- `parameter_values`: The mapping from runtime parameter names to its values that control the pipeline run.\n",
    "- `enable_caching`: Whether to turn on caching for the run.\n",
    "\n",
    "Learn more about the `PipelineJob` class from [Vertex AI PipelineJob documentation](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.PipelineJob)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c81f00-fbfa-4d5e-b70b-3b09cc083db6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "    display_name=PIPELINE_DISPLAY_NAME,\n",
    "    template_path=PIPELINE_FILENAME,\n",
    "    parameter_values=parameters,\n",
    "    enable_caching=False\n",
    ")\n",
    "\n",
    "job.run(service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5926e01-3c8f-49b9-b1ab-ebb3d442d1cb",
   "metadata": {},
   "source": [
    "## Fetch results from the predictions table\n",
    "\n",
    "After the Vertex AI pipeline job is finished successfully, fetch the results from batch prediction into a dataframe.\n",
    "\n",
    "### Get the table name\n",
    "\n",
    "Run the below cell to get the name of the predictions table from the pipeline job's artifact details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e724a20-4ca2-487d-adf6-b6b93a078153",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTPUT_TABLE = None\n",
    "\n",
    "# Iterate over the pipeline tasks\n",
    "for task in job._gca_resource.job_detail.task_details:\n",
    "    if  task.task_name == \"model-batch-predict\" and (\n",
    "            task.state == types.PipelineTaskDetail.State.SUCCEEDED\n",
    "            or task.state == types.PipelineTaskDetail.State.SKIPPED\n",
    "        ):\n",
    "    \n",
    "        # Obtain the artifacts from the batch-prediction task\n",
    "        OUTPUT_TABLE = task.outputs[\"bigquery_output_table\"].artifacts[0].metadata[\"tableId\"]\n",
    "        \n",
    "\n",
    "print(\"Predictions table ID:\", OUTPUT_TABLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eee6779-12be-4150-a1f8-1bd9c19dd0ee",
   "metadata": {},
   "source": [
    "### Query the predictions table\n",
    "\n",
    "Fetch a specified number of rows from the predictions table using the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023bd359-3213-4b53-95b5-0fcfdae16c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS = 10 # specify the needed no.of rows\n",
    "query = f'''\n",
    "    Select prediction from `{PROJECT_ID}.{BQ_DATASET_ID}.{OUTPUT_TABLE}` limit {ROWS}\n",
    "'''\n",
    "\n",
    "# Fetch the data into a dataframe\n",
    "df = bq_client.query(query).to_dataframe()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992372ee-70f3-4793-bde3-2aaaf9199c88",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial.\n",
    "\n",
    "Set `delete_bucket` to **True** to delete the Cloud Storage bucket used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb155781-a268-49ee-8e6e-a6de349bc902",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "delete_bucket = False\n",
    "\n",
    "# Delete the Vertex AI Pipeline job\n",
    "job.delete()\n",
    "\n",
    "# Delete the Vertex AI Model\n",
    "model.delete()\n",
    "\n",
    "# Delete the BigQuery dataset\n",
    "! bq rm -r -f -d $PROJECT_ID:$BQ_DATASET_ID\n",
    "\n",
    "# Delete Cloud Storage objects\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-bq_io-py",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Python [conda env:bq_io]",
   "language": "python",
   "name": "conda-env-bq_io-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

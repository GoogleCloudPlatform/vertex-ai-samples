{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "d3069d95"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "546c53de"
      },
      "source": [
        "# Vertex AI Batch Prediction with Model Monitoring\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/model_monitoring/batch_prediction_model_monitoring.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Fmodel_monitoring%2Fbatch_prediction_model_monitoring.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "<a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/model_monitoring/batch_prediction_model_monitoring.ipynb\" target='_blank'>\n",
        "       <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
        "    </a>\n",
        "  </td> \n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/model_monitoring/batch_prediction_model_monitoring.ipynb\">\n",
        "        <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53fd1070"
      },
      "source": [
        "## Overview\n",
        "\n",
        "In this notebook, you learn how to use Model Monitoring with batch prediction requests on a deployed Vertex AI model resource. In a companion notebook, <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/model_monitoring/model_monitoring.ipynb\" target=\"_blank\">Vertex AI Model Monitoring with Explainable AI feature attributions</a>, you can learn about how to apply model monitoring to streaming, real-time predictions.\n",
        "\n",
        "Learn more about [Vertex AI Model Monitoring for batch predictions](https://cloud.google.com/vertex-ai/docs/model-monitoring/model-monitoring-batch-predictions)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b26c855"
      },
      "source": [
        "### Objective\n",
        "In this notebook, you learn to use the Vertex AI model monitoring service to detect drift and anomalies in batch prediction.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services:\n",
        "\n",
        "- Vertex AI model monitoring\n",
        "- Vertex AI batch prediction\n",
        "- Vertex AI model resource\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Upload a pre-trained model as a Vertex AI model resource.\n",
        "- Generate batch prediction requests.\n",
        "- Interpret the statistics, visualizations, other data reported by the model monitoring feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35b52a5fba4a"
      },
      "source": [
        "### Model\n",
        "\n",
        "This tutorial uses a pre-trained model, where the model artifacts are stored in a public Cloud Storage bucket. The model predicts for an online gaming site, the probability that a player may churn, i.e. stop being an active player."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5508f7979954"
      },
      "source": [
        "### Costs \n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertext AI\n",
        "* Google Cloud Storage\n",
        "\n",
        "Learn about [Vertext AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d52ba95b"
      },
      "source": [
        "### What is Vertex AI batch prediction?\n",
        "\n",
        "<a href=\"https://cloud.google.com/vertex-ai/docs/predictions/batch-predictions\" target=\"_blank\">Batch Prediction</a> is a service that processes a collection (i.e. a batch) of machine learning inference requests in bulk, with less stringent response time requirements than real-time or streaming predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e64fb18a"
      },
      "source": [
        "### What is Vertex AI model monitoring?\n",
        "\n",
        "<a href=\"https://cloud.google.com/vertex-ai/docs/model-monitoring\" target=\"_blank\">Model Monitoring</a> is a service that automatically determines \n",
        "whether production machine learning traffic differs from  training data, or varies substantially over time, in terms of model predictions or feature attributions. When that happens, you can be alerted automatically and responsively, whereby, you can detect model decay which may negatively impact your operations -- such as your customer experience and/or revenue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d839347"
      },
      "source": [
        "### The example model\n",
        "\n",
        "The model you use in this notebook is based on [this blog post](https://cloud.google.com/blog/topics/developers-practitioners/churn-prediction-game-developers-using-google-analytics-4-ga4-and-bigquery-ml). The idea behind this model is that your company has extensive log data describing how your game users have interacted with the site. The raw data contains the following categories of information:\n",
        "\n",
        "- identity - unique player identitity numbers\n",
        "- demographic features - information about the player, such as the geographic region in which a player is located\n",
        "- behavioral features - counts of the number of times a  player has triggered certain game events, such as reaching a new level\n",
        "- churn propensity - this is the label or target feature, it provides an estimated probability that this player churns, i.e. stop being an active player.\n",
        "\n",
        "The blog article referenced above explains how to use BigQuery to store the raw data, pre-process it for use in machine learning, and train a model. Because this notebook focuses on model monitoring, rather than training models, you're going to reuse a pre-trained version of this model, which has been exported to Google Cloud Storage. In the next section, you setup your environment and import this model into your own project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "738fce1f"
      },
      "source": [
        "## Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28932aae0027"
      },
      "source": [
        "### Install Vertex AI SDK for Python and other required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4536fe4e"
      },
      "outputs": [],
      "source": [
        "# Install Python package dependencies.\n",
        "! pip3 install --upgrade --quiet google-cloud-aiplatform\n",
        "\n",
        "! pip3 install --upgrade --quiet tensorflow-data-validation\n",
        "\n",
        "! pip3 install --quiet cachetools==5.2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bc15a58ba2c"
      },
      "source": [
        "### Restart runtime (Colab only)\n",
        "\n",
        "To use the newly installed packages, you must restart the runtime on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57f0efda334f"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bf09d288e2a"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95feee7495bd"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "Authenticate your environment on Google Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ddb29fecd89"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e866da832fbc"
      },
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI SDK for Python\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_project_id"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}\n",
        "\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71404c9f"
      },
      "source": [
        "#### Set your email address\n",
        "This is used for delivering model monitoring notifications.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b1d2b69"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "EMAIL_ADDRESS = \"[your-email-address]\"  # @param {type:\"string\"}\n",
        "if not EMAIL_ADDRESS or EMAIL_ADDRESS == \"[your-email-address]\":\n",
        "    if os.getenv(\"IS_TESTING\"):\n",
        "        EMAIL_ADDRESS = \"noreply@google.com\"\n",
        "    else:\n",
        "        print(\"EMAIL_ADDRESS not specified, please correct before proceeding.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bucket:mbsdk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "Create a storage bucket to store intermediate artifacts such as datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bucket"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "autoset_bucket"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91c46850b49b"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0d294ff6d10"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bd7a633296eb"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "import google.cloud.aiplatform as aiplatform\n",
        "import tensorflow_data_validation as tfdv\n",
        "from google.cloud.aiplatform_v1beta1.services.job_service import \\\n",
        "    JobServiceClient\n",
        "from google.cloud.aiplatform_v1beta1.types import (\n",
        "    BatchDedicatedResources, BatchPredictionJob, GcsDestination, GcsSource,\n",
        "    MachineSpec, ModelMonitoringAlertConfig, ModelMonitoringConfig,\n",
        "    ModelMonitoringObjectiveConfig, ThresholdConfig)\n",
        "from tensorflow_data_validation.utils import io_util\n",
        "from tensorflow_metadata.proto.v0 import statistics_pb2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "outputs": [],
      "source": [
        "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "accelerators:training,prediction"
      },
      "source": [
        "#### Set hardware accelerators\n",
        "\n",
        "You can set hardware accelerators for prediction (e.g., GPUs) or choose not to use any (CPU). Hardware accelertors lower the latency response for a prediction request. When choosing a hardware accelerators, consider the additional cost trade-off over latency.\n",
        "\n",
        "Set the variables `DEPLOY_GPU/DEPLOY_NGPU` to use a container image supporting a GPU and the number of GPUs allocated to the virtual machine (VM) instance. For example, to use a GPU container image with 4 Nvidia Tesla T4 GPUs allocated to each VM, you would specify:\n",
        "\n",
        "    (aiplatform.gapic.AcceleratorType.NVIDIA_TESLA_T4, 4)\n",
        "\n",
        "See the [locations where accelerators are available](https://cloud.google.com/vertex-ai/docs/general/locations#accelerators).\n",
        "\n",
        "Otherwise specify `(None, None)` to use a container image to run on a CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xd5PLXDTlugv"
      },
      "outputs": [],
      "source": [
        "GPU = False\n",
        "if GPU:\n",
        "    DEPLOY_GPU, DEPLOY_NGPU = (aiplatform.gapic.AcceleratorType.NVIDIA_TESLA_T4, 1)\n",
        "else:\n",
        "    DEPLOY_GPU, DEPLOY_NGPU = (None, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "container:training,prediction"
      },
      "source": [
        "#### Set pre-built containers\n",
        "\n",
        "Set the pre-built Docker container image for prediction.\n",
        "\n",
        "For the latest list, see [Pre-built containers for prediction](https://cloud.google.com/ai-platform-unified/docs/predictions/pre-built-containers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1u1mr18jlugv"
      },
      "outputs": [],
      "source": [
        "if GPU:\n",
        "    DEPLOY_VERSION = \"tf2-gpu.2-13\"\n",
        "else:\n",
        "    DEPLOY_VERSION = \"tf2-cpu.2-13\"\n",
        "\n",
        "DEPLOY_IMAGE = \"{}-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(\n",
        "    LOCATION.split(\"-\")[0], DEPLOY_VERSION\n",
        ")\n",
        "\n",
        "print(\"Deployment:\", DEPLOY_IMAGE, DEPLOY_GPU, DEPLOY_NGPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "machine:training,prediction"
      },
      "source": [
        "#### Set machine types\n",
        "\n",
        "Next, set the machine types to use for training and prediction.\n",
        "\n",
        "- Set the variable `DEPLOY_COMPUTE` to configure your compute resources for prediction.\n",
        " - `machine type`\n",
        "     - `n1-standard`: 3.75GB of memory per vCPU\n",
        "     - `n1-highmem`: 6.5GB of memory per vCPU\n",
        "     - `n1-highcpu`: 0.9 GB of memory per vCPU\n",
        " - `vCPUs`: number of \\[2, 4, 8, 16, 32, 64, 96 \\]\n",
        "\n",
        "*Note: You may also use n2 and e2 machine types for training and deployment, but they do not support GPUs*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAXwbqKKlugv"
      },
      "outputs": [],
      "source": [
        "TRAIN_COMPUTE = \"n1-standard-4\"\n",
        "print(\"Train machine type\", TRAIN_COMPUTE)\n",
        "\n",
        "DEPLOY_COMPUTE = \"n1-standard-4\"\n",
        "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bf06cd476e9"
      },
      "source": [
        "### Upload the model artifacts as a Vertex AI model resource\n",
        "\n",
        "First, you upload the pre-trained custom tabular model artifacts as a Vertex AI model resource using the `upload()` method, with the following parameters:\n",
        "\n",
        "- `display_name`: The human readable name for the model resource.\n",
        "- `artifact_uri`: The Cloud Storage location of the model artifacts.\n",
        "- `serving_container_image`: The serving container image to use when the model is deployed to a Vertex AI endpoint resource.\n",
        "- `sync`: Whether to wait for the process to complete, or return immediately (async)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0193f247e216"
      },
      "outputs": [],
      "source": [
        "MODEL_ARTIFACT_URI = \"gs://mco-mm/churn\"\n",
        "\n",
        "model = aiplatform.Model.upload(\n",
        "    display_name=\"churn\",\n",
        "    artifact_uri=MODEL_ARTIFACT_URI,\n",
        "    serving_container_image_uri=DEPLOY_IMAGE,\n",
        "    sync=True,\n",
        ")\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4305ddf"
      },
      "source": [
        "## Submit a batch prediction request with model monitoring enabled\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "053fde99"
      },
      "source": [
        "### Create Cloud Storage buckets\n",
        "\n",
        "This cell creates two Cloud Storage buckets:\n",
        "\n",
        "- `gs://PROJECT_ID_bp_mm_input` contains the batch prediction request data.\n",
        "- `gs://PROJECT_ID_bp_mm_output` contains the batch prediction output as well as the model monitoring results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b832ad31"
      },
      "outputs": [],
      "source": [
        "# Copy files to your projects gs bucket to avoid permission issues.\n",
        "# Ignore any error(s) for bucket already exists.\n",
        "OUTPUT_GS_PATH = f\"{BUCKET_URI}/bp_mm_output\"\n",
        "INPUT_GS_PATH = f\"{BUCKET_URI}/bp_mm_input\"\n",
        "PUBLIC_TRAINING_DATASET = \"gs://bp_mm_public_data/churn/churn_bp_insample.csv\"\n",
        "TRAINING_DATASET = f\"{INPUT_GS_PATH}/churn_bp_insample.csv\"\n",
        "TRAINING_DATASET_FORMAT = \"csv\"\n",
        "\n",
        "! gsutil copy $PUBLIC_TRAINING_DATASET $TRAINING_DATASET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34c95126"
      },
      "source": [
        "### Create batch prediction job\n",
        "\n",
        "This step parameterizes and builds the data structure representing the batch prediction request, with model monitoring enabled.\n",
        "\n",
        "The BatchPredictionJob object specifies the input source, the data format, and the computing resources requests for the batch prediction. Learn more about <a href=\"https://cloud.google.com/vertex-ai/docs/samples/aiplatform-create-batch-prediction-job-sample\" target=\"_blank\">BatchPredictionJob</a>.\n",
        "\n",
        "The ModelMonitoringConfig object specifies the alerting email address, the training dataset, the features to be monitored, and their associated alerting thresholds. Learn more about <a href=\"https://cloud.google.com/vertex-ai/docs/model-monitoring\" target=\"_blank\">ModelMonitoringConfig</a>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3a54368a"
      },
      "outputs": [],
      "source": [
        "INPUT_URI = \"gs://bp_mm_public_data/churn/churn_bp_outsample.jsonl\"\n",
        "OUTPUT_URI = OUTPUT_GS_PATH\n",
        "INSTANCES_FORMAT = \"jsonl\"\n",
        "PREDICTIONS_FORMAT = \"jsonl\"\n",
        "JOB_NAME_PREFIX = \"bp_mm_demo\"\n",
        "MODEL_NAME = model.resource_name\n",
        "BATCH_PREDICTION_JOB_NAME = JOB_NAME_PREFIX\n",
        "\n",
        "batch_prediction_job = BatchPredictionJob(\n",
        "    display_name=BATCH_PREDICTION_JOB_NAME,\n",
        "    model=MODEL_NAME,\n",
        "    input_config=BatchPredictionJob.InputConfig(\n",
        "        instances_format=INSTANCES_FORMAT, gcs_source=GcsSource(uris=[INPUT_URI])\n",
        "    ),\n",
        "    output_config=BatchPredictionJob.OutputConfig(\n",
        "        predictions_format=PREDICTIONS_FORMAT,\n",
        "        gcs_destination=GcsDestination(output_uri_prefix=OUTPUT_URI),\n",
        "    ),\n",
        "    dedicated_resources=BatchDedicatedResources(\n",
        "        machine_spec=MachineSpec(machine_type=DEPLOY_COMPUTE),\n",
        "        starting_replica_count=1,\n",
        "        max_replica_count=1,\n",
        "    ),\n",
        "    # Model monitoring service is triggerred if provide following configs.\n",
        "    model_monitoring_config=ModelMonitoringConfig(\n",
        "        alert_config=ModelMonitoringAlertConfig(\n",
        "            email_alert_config=ModelMonitoringAlertConfig.EmailAlertConfig(\n",
        "                user_emails=[EMAIL_ADDRESS]\n",
        "            )\n",
        "        ),\n",
        "        objective_configs=[\n",
        "            ModelMonitoringObjectiveConfig(\n",
        "                training_dataset=ModelMonitoringObjectiveConfig.TrainingDataset(\n",
        "                    data_format=TRAINING_DATASET_FORMAT,\n",
        "                    gcs_source=GcsSource(uris=[TRAINING_DATASET]),\n",
        "                ),\n",
        "                training_prediction_skew_detection_config=ModelMonitoringObjectiveConfig.TrainingPredictionSkewDetectionConfig(\n",
        "                    skew_thresholds={\n",
        "                        \"cnt_user_engagement\": ThresholdConfig(value=0.001),\n",
        "                        \"julianday\": ThresholdConfig(value=0.001),\n",
        "                    }\n",
        "                ),\n",
        "            )\n",
        "        ],\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cae39778"
      },
      "source": [
        "### Submit batch prediction job\n",
        "\n",
        "This step submits the batch prediction request created in the previous step. If successful, it returns a JSON document summarizing the request, which is displayed in the cell output below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcdd4a47"
      },
      "outputs": [],
      "source": [
        "API_ENDPOINT = f\"{LOCATION}-aiplatform.googleapis.com\"\n",
        "client = JobServiceClient(client_options={\"api_endpoint\": API_ENDPOINT})\n",
        "out = client.create_batch_prediction_job(\n",
        "    parent=f\"projects/{PROJECT_ID}/locations/{LOCATION}\",\n",
        "    batch_prediction_job=batch_prediction_job,\n",
        ")\n",
        "BATCH_PREDICTION_JOB_ID = out.name.split(\"/\")[-1]\n",
        "print(\"BATCH_PREDICTION_JOB_ID:\", BATCH_PREDICTION_JOB_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49ec90a0"
      },
      "source": [
        "## Verify prediction results\n",
        "\n",
        "The batch prediction request gets completed after about **17 mins**, and the model monitoring result is available **10 mins** post that. The request below obtains the batch prediction job id, which is a unique number associated with asynchronous requests like this one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c30496b5"
      },
      "outputs": [],
      "source": [
        "# If auto-testing, wait for request completion\n",
        "if os.getenv(\"IS_TESTING\"):\n",
        "    time.sleep(3000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "831651c2"
      },
      "source": [
        "### Browse storage bucket\n",
        "\n",
        "Click on the link below, which opens the Cloud Storage object viewer on the output bucket you created above, to see the results of your batch prediction request.\n",
        "\n",
        "When everything is ready, you see two folders in the bucket:\n",
        "\n",
        "- `prediction-batch_prediction_monitoring_test_model_<timestmap>` - this folder contains the your batch prediction results, i.e. the predictions produced by your model for each input in the batch\n",
        "- `job-<id>` - this folder contains the model monitoring results, including the model schema, monitoring thresholds and other config settings, statistics, and anomalies\n",
        "\n",
        "*Note:* Until the request and the monitoring job are completed, you may see empty or partial results in this bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a705c10b"
      },
      "outputs": [],
      "source": [
        "print(f\"https://console.cloud.google.com/storage/browser/{OUTPUT_URI.lstrip('gs://')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bbdddac"
      },
      "source": [
        "### Visualize the batch prediction result\n",
        "\n",
        "Run the following cell to examine the batch prediction results with a tabular and visual analysis using the <a href=\"https://www.tensorflow.org/tfx/data_validation/get_started\" target=\"_blank\">TensorFlow Data Validation</a> package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6c674e9"
      },
      "outputs": [],
      "source": [
        "! rm -f ./training_stats.pb\n",
        "! rm -f ./prediction_stats.pb\n",
        "\n",
        "TRAINING_STATS_SUBPATH = \"stats_training/stats/training_stats\"\n",
        "PREDICTION_STATS_SUBPATH = \"stats_and_anomalies/stats/current_stats\"\n",
        "STATS_GCS_FOLDER = OUTPUT_URI = (\n",
        "    OUTPUT_GS_PATH + \"/job-\" + BATCH_PREDICTION_JOB_ID + \"/bp_monitoring/\"\n",
        ")\n",
        "TRAINING_STATS_GCS_PATH = STATS_GCS_FOLDER + TRAINING_STATS_SUBPATH\n",
        "print(\"Looking up statistics from: \" + TRAINING_STATS_GCS_PATH)\n",
        "PREDICTION_STATS_GCS_PATH = STATS_GCS_FOLDER + PREDICTION_STATS_SUBPATH\n",
        "print(\"Looking up statistics from: \" + PREDICTION_STATS_GCS_PATH)\n",
        "\n",
        "! gsutil cp $TRAINING_STATS_GCS_PATH ./training_stats.pb\n",
        "! gsutil cp $PREDICTION_STATS_GCS_PATH ./prediction_stats.pb\n",
        "\n",
        "\n",
        "# util function to load stats binary file from GCS\n",
        "def load_stats_binary(input_path):\n",
        "    stats_proto = statistics_pb2.DatasetFeatureStatisticsList()\n",
        "    stats_proto.ParseFromString(\n",
        "        io_util.read_file_to_string(input_path, binary_mode=True)\n",
        "    )\n",
        "    return stats_proto\n",
        "\n",
        "\n",
        "tfdv.visualize_statistics(load_stats_binary(\"./training_stats.pb\"))\n",
        "tfdv.visualize_statistics(load_stats_binary(\"./prediction_stats.pb\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "233b1266"
      },
      "source": [
        "### Check skew results\n",
        "\n",
        "Finally, you can check the skew results by examining a file in the output bucket. The JSON file contains a report indicating how much the batch prediction data deviates from the training data, on a feature-by-feature basis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4e8c00a7"
      },
      "outputs": [],
      "source": [
        "SKEW_GS_PATH = (\n",
        "    STATS_GCS_FOLDER\n",
        "    + \"stats_and_anomalies/anomalies/training_prediction_skew_anomalies\"\n",
        ")\n",
        "! gsutil cat $SKEW_GS_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aa0219d"
      },
      "source": [
        "## Learn more\n",
        "\n",
        "**Congratulations!** You've now learned how to monitor batch predictions, and how to find and interpret the results. Check out the following resources to learn more about model monitoring and ML Ops.\n",
        "\n",
        "- [TensorFlow Data Validation](https://www.tensorflow.org/tfx/guide/tfdv)\n",
        "- [Data Understanding, Validation, and Monitoring At Scale](https://blog.tensorflow.org/2018/09/introducing-tensorflow-data-validation.html)\n",
        "- [Vertex Product Documentation](https://cloud.google.com/vertex-ai)\n",
        "- [Vertex AI Model Monitoring Reference docs](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.modelDeploymentMonitoringJobs)\n",
        "- [Vertex AI Model Monitoring blog article](https://cloud.google.com/blog/topics/developers-practitioners/monitor-models-training-serving-skew-vertex-ai)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "497a0016"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:\n",
        "\n",
        "- Vertex AI Model\n",
        "- Batch prediction job\n",
        "- Cloud Storage bucket (set `delete_bucket` to *True* for deletion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eabc3f81"
      },
      "outputs": [],
      "source": [
        "# Delete model resource\n",
        "model.delete()\n",
        "\n",
        "# Get the batch prediction job resource\n",
        "batch_job = aiplatform.BatchPredictionJob(\n",
        "    batch_prediction_job_name=BATCH_PREDICTION_JOB_ID\n",
        ")\n",
        "# Delete the job\n",
        "batch_job.delete()\n",
        "\n",
        "# Delete Cloud Storage bucket\n",
        "delete_bucket = False\n",
        "if delete_bucket:\n",
        "    ! gsutil -m rm -r $BUCKET_URI\n",
        "\n",
        "! rm -f ./training_stats.pb\n",
        "! rm -f ./prediction_stats.pb"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "batch_prediction_model_monitoring.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

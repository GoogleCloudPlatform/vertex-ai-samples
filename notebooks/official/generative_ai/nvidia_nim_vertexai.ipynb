{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHI7XiOc8L9W"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXIc5UUP0IdQ"
      },
      "source": [
        "# **NVIDIA NIM on Google Cloud Vertex AI**\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Fgenerative_ai%2Fnvidia_nim_vertexai.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/generative_ai/nvidia_nim_vertexai.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>  \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSsj1dJ48bgo"
      },
      "source": [
        "[Vertex AI](https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform) is Google Cloud's unified machine learning platform. It streamlines the process of building, training, and deploying AI models, making it easier to bring your AI projects to life.\n",
        "\n",
        "[NVIDIA Inference Microservices (NIM)](https://www.nvidia.com/en-us/ai/) are pre-trained and optimized AI models packaged as microservices. They're designed to simplify the deployment of high-performance, production-ready AI into applications.\n",
        "\n",
        "This Colab notebook provides a demonstration of deploying a meta/llama-3.1-8b-instruct NIM on Vertex AI, leveraging NVIDIA GPUs. We will illustrate how to perform inference tasks using both batch and streaming modes. To execute this, you can utilize Colab Enterprise within Vertex AI. The NVIDIA NIM is available as container images, which you'll need to pull into your Google Cloud environment and subsequently deploy to a Vertex AI endpoint. These endpoints, accessible via REST, can then be integrated into your applications for various use cases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpTYRzExDnXR"
      },
      "source": [
        "## Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeziXkLk5IbE"
      },
      "source": [
        "### Hardware\n",
        "<a name=\"hardware\"></a>\n",
        "To run the meta/llama-3.1-8b-instruct NIM, you will need a Google Cloud G2 VM family with 2 `g2-standard-24` VMs, which provides access to the required [NVIDIA L4 GPU](https://cloud.google.com/compute/docs/gpus#l4-gpus) accelerator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrAzEokzD1Yh"
      },
      "source": [
        "### Software\n",
        "<a name=\"software\"></a>\n",
        "1. [Google Cloud Project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#creating_a_project) with a billing ID\n",
        "2. [Vertex AI](https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform)\n",
        " - [Vertex AI Model Resource](https://cloud.google.com/vertex-ai/docs/model-registry/introduction)\n",
        " - [Vertex AI Endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment)\n",
        "3. [Colab Enterprise](https://cloud.google.com/colab/docs/create-console-quickstart) using a default Runtime\n",
        "4. [Artifact Registry](https://cloud.google.com/artifact-registry)\n",
        "5. NVIDIA NGC API Key\n",
        "\n",
        "  **Note:** Please sign up for the [NVIDIA Developer Program](https://developer.nvidia.com/developer-program) which provides developers with tools and resources to build more efficiently and quickly using NVIDIA technology. You will get a NGC API Key that is required to access NIM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBpy7sGpD8oC"
      },
      "source": [
        "### Security roles and permissions\n",
        "<a name=\"security\"></a>\n",
        "\n",
        "To successfully run this colab, your user account needs specific permissions. Request the following roles from your administrator:\n",
        " - Colab Enterprise Admin (*roles/aiplatform.colabEnterpriseAdmin*)\n",
        " - Vertex AI Platform User (*roles/aiplatform.user*)\n",
        "\n",
        "Additionally, the Vertex AI Workbench instance operates under the default Compute Engine service account [<PROJECT_NUMBER>-compute@developer.gserviceaccount.com]. To ensure proper functionality, ask your administrator to assign the following role(s) to this service account:\n",
        " - Artifact Registry Writer (*roles/artifactregistry.writer*)\n",
        " - Compute Network Admin (*roles/compute.networkAdmin*) [Optional, if there is no default network]\n",
        "\n",
        "\n",
        "## Outline\n",
        "<a name=\"outline\"></a>\n",
        "\n",
        "1. [Getting started](#step1): To deploy the NIM container image to Vertex AI Workbench, first download the image to the Artifact Registry. Vertex AI Workbench instances come with Docker pre-installed, simplifying the process of pulling, tagging, and pushing images to repositories like Artifact Registry.\n",
        "\n",
        " *If you already have the NIM image in Google's Artifact Registry, you can move to [Step 3](#step3).*\n",
        "\n",
        "2. [Prerequisites](#step2): Enable API's for Google Cloud Products and authenticate for subsequent steps.\n",
        "\n",
        "4. [Configure](#step3): Parameters such as GPU Accelerator types, machine types are configured to host the NIM in Vertex AI\n",
        "\n",
        "5. [Deploy](#step4): The model needs to be uploaded to Vertex AI Model and deployed to a Vertex AI endpoint.\n",
        "\n",
        "6. [Test inference](#step7): Use sample prompts to test the model inferencing in [batch](#step7a) and [streaming](#step7b) mode.\n",
        "\n",
        "6. [Teardown](#step8): Teardown all the resources such as Vertex AI Endpoint, Model, Artifact registry repo and Workbench instance.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-dYYYhtiyZx"
      },
      "source": [
        "## Step 1: Getting Started\n",
        "<a name=\"step1\"></a>\n",
        "\n",
        "### a. Enable APIs\n",
        "Enable the APIs listed below from the Google Cloud Console.\n",
        "\n",
        "  - [Artifact Registry](https://console.cloud.google.com/flows/enableapi?apiid=artifactregistry.googleapis.com&redirect=https://console.cloud.google.com&_ga=2.153348347.214183506.1726972544-2083916923.1726802364)\n",
        "  - [Compute Engine](https://console.cloud.google.com/flows/enableapi?apiid=compute.googleapis.com&redirect=https://console.cloud.google.com&_ga=2.153677051.214183506.1726972544-2083916923.1726802364)\n",
        "  - [Dataform](https://console.cloud.google.com/flows/enableapi?apiid=dataform.googleapis.com&redirect=https://console.cloud.google.com&_ga=2.203088210.214183506.1726972544-2083916923.1726802364)\n",
        "  - [Notebook](https://console.cloud.google.com/flows/enableapi?apiid=notebooks.googleapis.com&redirect=https://console.cloud.google.com&_ga=2.153677051.214183506.1726972544-2083916923.1726802364)\n",
        "  - [Vertex AI](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com&redirect=https://console.cloud.google.com&_ga=2.153348347.214183506.1726972544-2083916923.1726802364)\n",
        "\n",
        "### b. Colab Notebook\n",
        "\n",
        "Follow [this link](https://cloud.google.com/colab/docs/create-runtime#create) to create a runtime in Colab Enterprise. Make sure to [connect to the runtime](https://cloud.google.com/colab/docs/connect-to-runtime#existing).\n",
        "\n",
        "### c. Setup environment\n",
        "Set up your environment by installing the required Python packages as detailed below.\n",
        "\n",
        "<sub><p align=\"right\">[go to top](#outline)</p></sub>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNU6x_VFBYVi"
      },
      "outputs": [],
      "source": [
        "! pip install --upgrade --user --quiet \\\n",
        "    google-cloud-aiplatform \\\n",
        "    google-cloud-artifact-registry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0hgKwc6CLrb"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPe2WjwQzut-"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "ZONE = \"us-central1-a\"  # @param [\"us-central1-a\"] {\"allow-input\":true}\n",
        "LOCATION = \"-\".join(ZONE.split(\"-\")[:-1])\n",
        "\n",
        "REPOSITORY_NAME = \"nim\"\n",
        "\n",
        "WORKBENCH_NAME = \"wb-nim\"  # @param {\"type\":\"string\"}\n",
        "MACHINE_TYPE = \"e2-standard-4\"  # @param {\"type\":\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ng0UmyfrD8VV"
      },
      "outputs": [],
      "source": [
        "! gcloud artifacts repositories create {REPOSITORY_NAME} \\\n",
        "  --repository-format=docker \\\n",
        "  --location={LOCATION} \\\n",
        "  --project={PROJECT_ID}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aUBUukfN2dH"
      },
      "outputs": [],
      "source": [
        "import google.cloud.aiplatform as aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CncZoi96CoWz"
      },
      "source": [
        "## Step 2: Create Vertex AI Workbench instance\n",
        "<a name=\"step2\"></a>\n",
        "\n",
        "A Vertex AI Workbench instance is temporarily created to retrieve the Docker image from the NVIDIA GPU Catalog (NGC) using the NGC API Key. You have the option to create and specify a default network if one is not already set up.\n",
        "\n",
        "**Note:**\n",
        "Click on the link below that opens up the new Vertex AI Workbench instance.\n",
        "<sub><p align=\"right\">[go to top](#outline)</p></sub>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTd24cMfkqtV"
      },
      "outputs": [],
      "source": [
        "# Optional to create network and specify if there is no default network created\n",
        "NETWORK_NAME = f\"vpc-{WORKBENCH_NAME}\"\n",
        "\n",
        "! gcloud compute networks create {NETWORK_NAME} \\\n",
        "  --project={PROJECT_ID} \\\n",
        "  --subnet-mode=auto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igcBIoHW9RnQ"
      },
      "outputs": [],
      "source": [
        "! gcloud workbench instances create {WORKBENCH_NAME} \\\n",
        "  --project={PROJECT_ID} \\\n",
        "  --location={ZONE} \\\n",
        "  --machine-type={MACHINE_TYPE}\n",
        "  #--network={NETWORK_NAME}\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "url = !gcloud workbench instances describe {WORKBENCH_NAME} \\\n",
        "  --project={PROJECT_ID} \\\n",
        "  --location={ZONE} \\\n",
        "  --format=\"value(gceSetup.metadata.proxy-url)\"\n",
        "\n",
        "url = url[0]\n",
        "display(HTML(f'<a href=\"https://{url}\" target=\"_blank\">{url}</a>'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bciR3s8p56QF"
      },
      "source": [
        "## Step 3: Push image to Artifact registry\n",
        "<a name=\"step3\"></a>\n",
        "\n",
        "This code comprises two sections:\n",
        " - [Configure NIM models](#step3a): Execute this in both **this Colab notebook** and **the Vertex AI workbench**.\n",
        " - [Pull and push Docker image](#step3b): Execute this section exclusively within the **Vertex AI workbench only**. You will need your NGC API Key. This section will fail if executed in Colab. To execute within the Vertex AI Workbench instance:\n",
        "   - Navigate to Notebooks -> Python3 (ipykernel)\n",
        "   - Paste the code from both cells\n",
        "   - If not already defined, provide values for `PROJECT_ID` and `LOCATION`\n",
        "   - Execute the code\n",
        "\n",
        "**Note:** The image size will determine how long this step takes, which may range from 20 to 30 minutes.\n",
        "\n",
        "<sub><p align=\"right\">[go to top](#outline)</p></sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phbVb2UWiblT"
      },
      "source": [
        "### a. Configure NIM model\n",
        "<a name=\"step3a\"></a>\n",
        "\n",
        "Note:: Execute both in Colab and the Vertex AI Workbench notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1u5-MWu3wTc"
      },
      "outputs": [],
      "source": [
        "LOCATION = \"[GCP region from Step 1c]\"\n",
        "PROJECT_ID = \"[GCP project_id from Step 1c]\"\n",
        "\n",
        "NVCR_REGISTRY = \"nvcr.io\"\n",
        "REPOSITORY_NAME = \"nim\"\n",
        "NIM_MODEL = \"meta/llama-3.1-8b-instruct:1.2.2\"  # @param [\"meta/llama-3.1-8b-instruct:1.2.2\",\"meta/llama-3.1-70b-instruct:1.2.1\",\"meta/llama-3.1-405b-instruct:1.2.0\",\"meta/llama3-8b-instruct:1.0.3\",\"meta/llama3-70b-instruct:1.0.3\"] {\"allow-input\":true}\n",
        "NGC_API_KEY = \"[nvidia-ngc-api-key]\"  # @param {\"type\":\"string\"}\n",
        "\n",
        "NIM_IMAGE = NVCR_REGISTRY + \"/\" + REPOSITORY_NAME + \"/\" + NIM_MODEL\n",
        "NIM_IMAGE_GAR = (\n",
        "    LOCATION + \"-docker.pkg.dev/\" + PROJECT_ID + \"/\" + REPOSITORY_NAME + \"/\" + NIM_MODEL\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJK5VHrZinb_"
      },
      "source": [
        "### b. Pull and push Docker image\n",
        "<a name=\"step3b\"></a>\n",
        "\n",
        "**Note:**: Execute only in the Vertex AI Workbench notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7WQ5XzviAyX"
      },
      "outputs": [],
      "source": [
        "! gcloud auth configure-docker {LOCATION}-docker.pkg.dev --quiet\n",
        "\n",
        "! docker login -u '$oauthtoken' --password-stdin nvcr.io <<< \"$NGC_API_KEY\"\n",
        "! docker pull $NIM_IMAGE\n",
        "! docker tag $NIM_IMAGE $NIM_IMAGE_GAR\n",
        "! docker push $NIM_IMAGE_GAR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ow_OnrCaXN_f"
      },
      "source": [
        "## Step 4. Upload NIM to Vertex AI\n",
        "<a name=\"step4\"></a>\n",
        "Uploading a NIM to Vertex AI provides a streamlined and efficient way to deploy and manage your generative AI models.\n",
        "\n",
        "**Note:** The image size will determine how long this step takes, which may range from 20 to 30 minutes.\n",
        "\n",
        "<sub><p align=\"right\">[go to top](#outline)</p></sub>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XV_BtH9qMrxf"
      },
      "outputs": [],
      "source": [
        "MACHINE_TYPE = \"g2-standard-24\"  # @param {type:\"string\"}\n",
        "GPU_ACCELERATOR_TYPE = \"NVIDIA_L4\"  # @param {type:\"string\"}\n",
        "GPU_ACCELERATOR_COUNT = 2  # @param {\"type\":\"number\"}\n",
        "\n",
        "SELECTED_PROFILE = \"vllm-fp16-tp2\"  # @param {type:\"string\"}\n",
        "API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(LOCATION)\n",
        "\n",
        "endpoint_name = NIM_MODEL.replace(\":\", \"_\")\n",
        "model_wo_tag = NIM_MODEL.split(\":\")[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiE5bGiMIkkE"
      },
      "outputs": [],
      "source": [
        "from google.api_core.future.polling import DEFAULT_POLLING\n",
        "from google.cloud.aiplatform import Endpoint, Model\n",
        "\n",
        "DEFAULT_POLLING._timeout = 360000\n",
        "model = None\n",
        "models = Model.list(filter=f'displayName=\"{NIM_MODEL}\"')\n",
        "\n",
        "if models:\n",
        "    model = models[0]\n",
        "else:\n",
        "    try:\n",
        "        model = aiplatform.Model.upload(\n",
        "            display_name=f\"{NIM_MODEL}\",\n",
        "            serving_container_image_uri=f\"{NIM_IMAGE_GAR}\",\n",
        "            serving_container_predict_route=\"/v1/chat/completions\",\n",
        "            serving_container_health_route=\"/v1/health/ready\",\n",
        "            serving_container_environment_variables={\n",
        "                \"NGC_API_KEY\": f\"{NGC_API_KEY}\",\n",
        "                \"PORT\": \"8000\",\n",
        "                \"shm-size\": \"16GB\",\n",
        "            },\n",
        "            serving_container_shared_memory_size_mb=16000,\n",
        "            serving_container_ports=[8000],\n",
        "            sync=True,\n",
        "        )\n",
        "        model.wait()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "if model:\n",
        "    print(\"Model:\")\n",
        "    print(f\"\\tDisplay name: {model.display_name}\")\n",
        "    print(f\"\\tResource name: {model.resource_name}\")\n",
        "    MODEL_ID = model.resource_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkJ9kWaGZVnN"
      },
      "source": [
        "## Step 5. Create Vertex Endpoint\n",
        "<a name=\"step5\"></a>\n",
        "The Vertex AI Endpoint components expose the functionalities of the Vertex AI Model through an Endpoint resource.\n",
        "<sub><p align=\"right\">[go to top](#outline)</p></sub>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-ekGzFGQomr"
      },
      "outputs": [],
      "source": [
        "endpoints = Endpoint.list(filter=f'displayName=\"{endpoint_name}\"')\n",
        "print(endpoints)\n",
        "if endpoints:\n",
        "    endpoint = endpoints[0]\n",
        "else:\n",
        "    print(f\"Endpoint {endpoint_name} doesn't exist, creating...\")\n",
        "    endpoint = aiplatform.Endpoint.create(display_name=endpoint_name)\n",
        "\n",
        "if endpoint:\n",
        "    print(\"Endpoint:\")\n",
        "    print(f\"\\tDisplay name: {endpoint.display_name}\")\n",
        "    print(f\"\\tResource name: {endpoint.resource_name}\")\n",
        "\n",
        "    ENDPOINT_ID = endpoint.resource_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALnX28YdZacH"
      },
      "source": [
        "## Step 6. Deploy NIM\n",
        "<a name=\"step6\"></a>\n",
        "\n",
        "To use models for online predictions, they need to be deployed to an endpoint.\n",
        "\n",
        "**Note:** This step can take 20-30 minutes.\n",
        "<sub><p align=\"right\">[go to top](#outline)</p></sub>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPQ-3kMoRFPr"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        deployed_model_display_name=f\"{NIM_MODEL}\",\n",
        "        traffic_percentage=100,\n",
        "        machine_type=f\"{MACHINE_TYPE}\",\n",
        "        min_replica_count=1,\n",
        "        max_replica_count=1,\n",
        "        accelerator_type=f\"{GPU_ACCELERATOR_TYPE}\",\n",
        "        accelerator_count=GPU_ACCELERATOR_COUNT,\n",
        "        enable_access_logging=True,\n",
        "        sync=True,\n",
        "    )\n",
        "    print(f\"Model {model.display_name} deployed at endpoint {endpoint.display_name}.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkMEzG2GZepL"
      },
      "source": [
        "## Step 7. Run Inference\n",
        "<a name=\"step7\"></a>\n",
        "To execute a sample inference, structure your input instance in JSON format.\n",
        "\n",
        "<sub><p align=\"right\">[go to top](#outline)</p></sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pDhek1LqQTm"
      },
      "source": [
        "### a. Create Payload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sby_UTdWWXGo"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"content\": \"You are a polite and respectful chatbot helping people plan a vacation.\",\n",
        "        \"role\": \"system\",\n",
        "    },\n",
        "    {\"content\": \"What should I do for a 4 day vacation in Spain?\", \"role\": \"user\"},\n",
        "]\n",
        "\n",
        "payload = {\"model\": model_wo_tag, \"messages\": messages, \"max_tokens\": 4096, \"top_p\": 1}\n",
        "\n",
        "with open(\"request.json\", \"w\") as outfile:\n",
        "    json.dump(payload, outfile)\n",
        "\n",
        "# Streaming\n",
        "payload_s = {\n",
        "    \"model\": model_wo_tag,\n",
        "    \"messages\": messages,\n",
        "    \"max_tokens\": 4096,\n",
        "    \"top_p\": 1,\n",
        "    \"stream\": True,\n",
        "}\n",
        "\n",
        "with open(\"request_stream.json\", \"w\") as outfile:\n",
        "    json.dump(payload_s, outfile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJbAepESZkWH"
      },
      "source": [
        "### b. Test Inference\n",
        "<a name=\"step7a\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WR-AjyX0WgTf"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pprint import pprint\n",
        "\n",
        "from google.api import httpbody_pb2\n",
        "from google.cloud import aiplatform_v1\n",
        "\n",
        "client_options = {\"api_endpoint\": API_ENDPOINT}\n",
        "\n",
        "http_body = httpbody_pb2.HttpBody(\n",
        "    data=json.dumps(payload).encode(\"utf-8\"),\n",
        "    content_type=\"application/json\",\n",
        ")\n",
        "\n",
        "try:\n",
        "    req = aiplatform_v1.RawPredictRequest(\n",
        "        http_body=http_body, endpoint=endpoint.resource_name\n",
        "    )\n",
        "\n",
        "    print(\"Request:\")\n",
        "    pprint(json.loads(req.http_body.data))\n",
        "\n",
        "    pred_client = aiplatform.gapic.PredictionServiceClient(\n",
        "        client_options=client_options\n",
        "    )\n",
        "\n",
        "    response = pred_client.raw_predict(req)\n",
        "\n",
        "    print(\n",
        "        \"--------------------------------------------------------------------------------------\"\n",
        "    )\n",
        "    print(\"Response:\")\n",
        "    pprint(json.loads(response.data))\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEHodX6eZs-R"
      },
      "source": [
        "### c. Test Inferencing (Streaming)\n",
        "<a name=\"step7b\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bs2g_UJnWqYx"
      },
      "outputs": [],
      "source": [
        "# Streaming\n",
        "\n",
        "import json\n",
        "from pprint import pprint\n",
        "\n",
        "from google.api import httpbody_pb2\n",
        "from google.cloud import aiplatform_v1\n",
        "\n",
        "client_options = {\"api_endpoint\": API_ENDPOINT}\n",
        "\n",
        "http_body = httpbody_pb2.HttpBody(\n",
        "    data=json.dumps(payload_s).encode(\"utf-8\"),\n",
        "    content_type=\"application/json\",\n",
        ")\n",
        "\n",
        "try:\n",
        "    req = aiplatform_v1.RawPredictRequest(\n",
        "        http_body=http_body, endpoint=endpoint.resource_name\n",
        "    )\n",
        "\n",
        "    print(\"Request:\")\n",
        "    pprint(json.loads(req.http_body.data))\n",
        "\n",
        "    pred_client = aiplatform.gapic.PredictionServiceClient(\n",
        "        client_options=client_options\n",
        "    )\n",
        "\n",
        "    response = pred_client.raw_predict(req)\n",
        "    print(\n",
        "        \"--------------------------------------------------------------------------------------\"\n",
        "    )\n",
        "    print(\"Response:\")\n",
        "    print(response.data.decode(\"utf-8\"))\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xYIV31DCZ8k"
      },
      "source": [
        "## Step 8. Teardown (optional)\n",
        "<a name=\"step8\"></a>\n",
        "\n",
        "The infrastructure provisioned in the previous steps can be deleted in this optional step.\n",
        "<sub><p align=\"right\">[go to top](#outline)</p></sub>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2y_n_dSCgKt"
      },
      "outputs": [],
      "source": [
        "if endpoint:\n",
        "  print(f\"Deleting endpoint {endpoint.display_name}\")\n",
        "  endpoint.undeploy_all()\n",
        "  endpoint.delete()\n",
        "\n",
        "if model:\n",
        "  print(f\"Deleting model {model.display_name}\")\n",
        "  model.delete()\n",
        "\n",
        "! gcloud artifacts docker images delete \\\n",
        "  --delete-tags {NIM_IMAGE_GAR} \\\n",
        "  --quiet\n",
        "\n",
        "! gcloud artifacts repositories delete {REPOSITORY_NAME} \\\n",
        "  --location={LOCATION} \\\n",
        "  --quiet\n",
        "\n",
        "! gcloud workbench instances delete {WORKBENCH_NAME} \\\n",
        "  --project={PROJECT_ID} \\\n",
        "  --location={ZONE}\n",
        "\n",
        "! gcloud compute networks delete {NETWORK_NAME} \\\n",
        "  --project={PROJECT_ID} \\\n",
        "  --quiet"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "nvidia_nim_vertexai.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9A9NkTRTfo2I"
      },
      "outputs": [],
      "source": [
        "# Copyright 2026 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e771d196b58"
      },
      "source": [
        "# Getting Started with `Llama Nemotron` Models\n",
        "\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/generative_ai/llama_nemotron_intro.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Fgenerative_ai%2Fllama_nemotron_intro.ipynb\\\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>    \n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/generative_ai/llama_nemotron_intro.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/generative_ai/llama_nemotron_intro.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://raw.githubusercontent.com/primer/octicons/refs/heads/main/icons/mark-github-24.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fK_rdvvx1iZ"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates how to deploy `Llama Nemotron` NVIDIA Inference Microservices (NIM) to Google Cloud Platform (GCP) Vertex AI.\n",
        "\n",
        "You’ll learn how to run an NVIDIA NIM container on a Vertex AI compute instance, register it as a model in the Vertex AI Model Registry, deploy it to an Endpoint, and perform real-time predictions — all within a managed and scalable GCP environment.\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Set up Vertex AI environment and authentication.\n",
        "- Upload the NVIDIA NIM model to the Vertex AI Model Registry.\n",
        "- Create a Vertex AI Endpoint.\n",
        "- Deploy the Model to the Endpoint.\n",
        "- Send a prediction request via API and Python SDK.\n",
        "\n",
        "### `Llama Nemotron` on Vertex AI\n",
        "\n",
        "You can deploy the `Llama Nemotron` models in your own endpoint.\n",
        "\n",
        "### Available `Llama Nemotron` models\n",
        "\n",
        "#### `Llama-3.3-Nemotron-Super-49B-v1.5`\n",
        "**Llama-3.3-Nemotron-Super-49B-v1.5** is a reasoning model that is post trained for reasoning, human chat preferences, and agentic tasks, such as Retrieval-Augmented Generation (RAG) and tool calling. The model supports a context length of 128K tokens. Llama-3.3-Nemotron-Super-49B-v1.5 is a model which offers a great tradeoff between model accuracy and efficiency. Efficiency (throughput) directly translates to savings. Using a novel Neural Architecture Search (NAS) approach, we greatly reduce the model’s memory footprint, enabling larger workloads, as well as fitting the model on a single GPU at high workloads (H200). This NAS approach enables the selection of a desired point in the accuracy-efficiency tradeoff. For more information on the NAS approach, please refer to this [paper](https://arxiv.org/abs/2411.19146).\n",
        "\n",
        "## Objective\n",
        "\n",
        "This notebook shows how to use **Vertex AI API** to deploy the `Llama Nemotron` models.\n",
        "\n",
        "For more information, see the [NIM documentation](https://build.nvidia.com/nvidia/llama-3_3-nemotron-super-49b-v1_5/modelcard).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwYvaaW25jYS"
      },
      "source": [
        "## Get Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d10e8895d2d4"
      },
      "source": [
        "### Install Vertex AI SDK for Python or other required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08dd6d2ac629"
      },
      "outputs": [],
      "source": [
        "! pip3 install --upgrade --quiet google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "754611260f53"
      },
      "outputs": [],
      "source": [
        "! pip3 install -U -q httpx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9f4c57a43f6"
      },
      "source": [
        "### Restart runtime (Colab only)\n",
        "\n",
        "To use the newly installed packages, you must restart the runtime on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b9119a60525"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e767418763cd"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a5bea26f60f"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "Authenticate your environment on Google Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c97be6a73155"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fxZn4SAbxdl"
      },
      "source": [
        "### Select one of `publisher name` models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Y8X70FTSbx7U"
      },
      "outputs": [],
      "source": [
        "PUBLISHER_NAME = \"nvidia\"  # @param {type:\"string\"}\n",
        "PUBLISHER_MODEL_NAME = (\n",
        "    \"llama-3.3-nemotron-super-49b-v1.5\"  # @param [\"llama-3.3-nemotron-super-49b-v1.5\"]\n",
        ")\n",
        "\n",
        "if PUBLISHER_MODEL_NAME == \"llama-3.3-nemotron-super-49b-v1.5\":\n",
        "    available_regions = [\n",
        "        \"asia-northeast1\",\n",
        "        \"asia-northeast3\",\n",
        "        \"asia-south1\",\n",
        "        \"asia-south2\",\n",
        "        \"asia-southeast1\",\n",
        "        \"australia-southeast1\",\n",
        "        \"europe-north1\",\n",
        "        \"europe-west1\",\n",
        "        \"europe-west2\",\n",
        "        \"europe-west3\",\n",
        "        \"europe-west4\",\n",
        "        \"me-west1\",\n",
        "        \"northamerica-northeast2\",\n",
        "        \"us-central1\",\n",
        "        \"us-east1\",\n",
        "        \"us-east4\",\n",
        "        \"us-east5\",\n",
        "        \"us-south1\",\n",
        "        \"us-west1\",\n",
        "        \"us-west2\",\n",
        "        \"us-west3\",\n",
        "        \"us-west4\",\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpuX3sKtexlK"
      },
      "source": [
        "### Select a location and a version from the dropdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHl8xW45ex_O"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "dropdown_loc = widgets.Dropdown(\n",
        "    options=available_regions,\n",
        "    description=\"Select a location:\",\n",
        "    font_weight=\"bold\",\n",
        "    style={\"description_width\": \"initial\"},\n",
        ")\n",
        "\n",
        "\n",
        "def dropdown_loc_eventhandler(change):\n",
        "    global LOCATION\n",
        "    if change[\"type\"] == \"change\" and change[\"name\"] == \"value\":\n",
        "        LOCATION = change.new\n",
        "        print(\"Selected:\", change.new)\n",
        "\n",
        "\n",
        "LOCATION = dropdown_loc.value\n",
        "dropdown_loc.observe(dropdown_loc_eventhandler, names=\"value\")\n",
        "display(dropdown_loc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bfb7dd22f59"
      },
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI SDK for Python\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8acca7fbea99"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "ENDPOINT = f\"https://{LOCATION}-aiplatform.googleapis.com\"\n",
        "\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    raise ValueError(\"Please set your PROJECT_ID\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NAstKRFBt4N"
      },
      "source": [
        "### Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZEFLE6a6bqy"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d98c7d020a2"
      },
      "source": [
        "## Using Vertex AI API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjsDpa8jlTRu"
      },
      "source": [
        "### Upload Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1R2BRsBlu-k"
      },
      "outputs": [],
      "source": [
        "UPLOAD_MODEL_PAYLOAD = {\n",
        "    \"model\": {\n",
        "        \"displayName\": \"ModelGarden_LaunchPad_Model_\" + time.strftime(\"%Y%m%d-%H%M%S\"),\n",
        "        \"baseModelSource\": {\n",
        "            \"modelGardenSource\": {\n",
        "                \"publicModelName\": f\"publishers/{PUBLISHER_NAME}/models/{PUBLISHER_MODEL_NAME}\",\n",
        "            }\n",
        "        },\n",
        "    }\n",
        "}\n",
        "\n",
        "request = json.dumps(UPLOAD_MODEL_PAYLOAD)\n",
        "\n",
        "! curl -X POST -H \"Authorization: Bearer $(gcloud auth print-access-token)\" -H \"Content-Type: application/json\" {ENDPOINT}/v1beta1/projects/{PROJECT_ID}/locations/{LOCATION}/models:upload -d '{request}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2j0nVGwlf9b"
      },
      "source": [
        "### Get Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d78f893c5b9"
      },
      "source": [
        "After uploading the model to the Vertex AI Model Registry, extract its Model ID by specifying the target region and matching the display name pattern."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0d475526767"
      },
      "outputs": [],
      "source": [
        "# Adjust filter to include exact timestamp if needed\n",
        "MODEL_ID = !gcloud ai models list --region=$LOCATION --filter=\"display_name~'.*ModelGarden_LaunchPad_Model.*'\" --format=\"value(name)\" | head -n 1\n",
        "MODEL_ID = MODEL_ID[1]\n",
        "print(MODEL_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxwM0GXTmQhh"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"YOUR_MODEL_ID\"  # @param {type: \"string\"}\n",
        "\n",
        "! curl -X GET -H \"Authorization: Bearer $(gcloud auth print-access-token)\" -H \"Content-Type: application/json\" {ENDPOINT}/v1/projects/{PROJECT_ID}/locations/{LOCATION}/models/{MODEL_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q3ygq8VlZAp"
      },
      "source": [
        "### Create Endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1ChDOt7mPBQ"
      },
      "outputs": [],
      "source": [
        "CREATE_ENDPOINT_PAYLOAD = {\n",
        "    \"displayName\": \"ModelGarden_LaunchPad_Endpoint_\" + time.strftime(\"%Y%m%d-%H%M%S\"),\n",
        "}\n",
        "\n",
        "request = json.dumps(CREATE_ENDPOINT_PAYLOAD)\n",
        "\n",
        "! curl -X POST -H \"Authorization: Bearer $(gcloud auth print-access-token)\" -H \"Content-Type: application/json\" {ENDPOINT}/v1/projects/{PROJECT_ID}/locations/{LOCATION}/endpoints -d '{request}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuMZCdhmlpCE"
      },
      "source": [
        "### Get Endpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39c72d20a82a"
      },
      "source": [
        "After creating the Endpoint, extract its Endpoint ID by specifying the target region and matching the display name pattern."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "087f338851d5"
      },
      "outputs": [],
      "source": [
        "# Adjust filter to include exact timestamp if needed\n",
        "ENDPOINT_ID = !gcloud ai endpoints list --region=$LOCATION --filter=\"DISPLAY_NAME ~ .*ModelGarden_LaunchPad_Endpoint.*\" --format=\"value(name)\" | head -n 1\n",
        "ENDPOINT_ID = ENDPOINT_ID[1]\n",
        "print(ENDPOINT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHq_cLT6mPp_"
      },
      "outputs": [],
      "source": [
        "ENDPOINT_ID = \"YOUR_ENDPOINT_ID\"  # @param {type: \"string\"}\n",
        "\n",
        "! curl -X GET -H \"Authorization: Bearer $(gcloud auth print-access-token)\" -H \"Content-Type: application/json\" {ENDPOINT}/v1/projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/{ENDPOINT_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0amEPXolbP7"
      },
      "source": [
        "### Deploy Model\n",
        "\n",
        "Deploy the model to the endpoint. The deployment process will take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fd863f6ec91"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Configure machine types (please change if needed)\n",
        "\n",
        "MACHINE_CONFIG = {\n",
        "    \"a2-highgpu-8g\": {\"type\": \"NVIDIA_TESLA_A100\", \"count\": 8},\n",
        "    \"a2-ultragpu-8g\": {\"type\": \"NVIDIA_A100_80GB\", \"count\": 8},\n",
        "    \"a3-highgpu-4g\": {\"type\": \"NVIDIA_H100_80GB\", \"count\": 4},\n",
        "    \"g4-standard-384\": {\"type\": \"NVIDIA_RTX_PRO_6000\", \"count\": 8},\n",
        "}\n",
        "\n",
        "COMPATIBLE_MACHINES = list(MACHINE_CONFIG.keys())\n",
        "\n",
        "# Setup Widgets\n",
        "\n",
        "dropdown_machine = widgets.Dropdown(\n",
        "    options=COMPATIBLE_MACHINES,\n",
        "    description=\"Machine type:\",\n",
        "    style={\"description_width\": \"initial\"},\n",
        ")\n",
        "\n",
        "label_accelerator_type = widgets.HTML()\n",
        "label_accelerator_count = widgets.HTML()\n",
        "\n",
        "\n",
        "def update_labels(machine_type: str):\n",
        "    \"\"\"Update accelerator labels based on the selected machine.\"\"\"\n",
        "    info = MACHINE_CONFIG.get(machine_type, {\"type\": \"Unknown\", \"count\": 0})\n",
        "    label_accelerator_type.value = f\"<b>Accelerator type:</b> {info['type']}\"\n",
        "    label_accelerator_count.value = f\"<b>Accelerator count:</b> {info['count']}\"\n",
        "    return info\n",
        "\n",
        "\n",
        "# Event Handling\n",
        "\n",
        "\n",
        "def on_machine_change(change):\n",
        "    \"\"\"Handle machine selection change.\"\"\"\n",
        "    if change[\"name\"] == \"value\" and change[\"type\"] == \"change\":\n",
        "        global MACHINE_TYPE, ACCELERATOR_TYPE, ACCELERATOR_COUNT\n",
        "        MACHINE_TYPE = change.new\n",
        "        info = update_labels(MACHINE_TYPE)\n",
        "        ACCELERATOR_TYPE = info[\"type\"]\n",
        "        ACCELERATOR_COUNT = info[\"count\"]\n",
        "\n",
        "\n",
        "# Initiate Global Variables\n",
        "\n",
        "MACHINE_TYPE = dropdown_machine.value\n",
        "initial_info = update_labels(MACHINE_TYPE)\n",
        "ACCELERATOR_TYPE = initial_info[\"type\"]\n",
        "ACCELERATOR_COUNT = initial_info[\"count\"]\n",
        "\n",
        "dropdown_machine.observe(on_machine_change, names=\"value\")\n",
        "\n",
        "display(\n",
        "    widgets.VBox(\n",
        "        [\n",
        "            dropdown_machine,\n",
        "            label_accelerator_type,\n",
        "            label_accelerator_count,\n",
        "        ]\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGTyCQQhlrAR"
      },
      "outputs": [],
      "source": [
        "DEPLOY_PAYLOAD = {\n",
        "    \"deployedModel\": {\n",
        "        \"model\": f\"projects/{PROJECT_ID}/locations/{LOCATION}/models/{MODEL_ID}\",\n",
        "        \"displayName\": \"ModelGarden_LaunchPad_DeployedModel_\"\n",
        "        + time.strftime(\"%Y%m%d-%H%M%S\"),\n",
        "        \"dedicatedResources\": {\n",
        "            \"machineSpec\": {\n",
        "                \"machineType\": MACHINE_TYPE,\n",
        "                \"acceleratorType\": ACCELERATOR_TYPE,\n",
        "                \"acceleratorCount\": ACCELERATOR_COUNT,\n",
        "            },\n",
        "            \"minReplicaCount\": 1,\n",
        "            \"maxReplicaCount\": 1,\n",
        "        },\n",
        "    },\n",
        "    \"trafficSplit\": {\"0\": 100},\n",
        "}\n",
        "\n",
        "request = json.dumps(DEPLOY_PAYLOAD)\n",
        "print(\"Request payload to Deploy Model:\")\n",
        "print(json.dumps(DEPLOY_PAYLOAD, indent=2))\n",
        "print(\"\\nResult:\")\n",
        "! curl -X POST -H \"Authorization: Bearer $(gcloud auth print-access-token)\" -H \"Content-Type: application/json\" {ENDPOINT}/v1/projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/{ENDPOINT_ID}:deployModel -d '{request}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ahw-uFjCAbo"
      },
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61107099357a"
      },
      "source": [
        "#### Unary call\n",
        "\n",
        "Sends a POST request to the specified API endpoint to get a response from the model for a limerick writing using the provided payload, with reasoning on / off."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zFz260B50oi"
      },
      "outputs": [],
      "source": [
        "# Reasoning off \n",
        "\n",
        "PAYLOAD = {\n",
        "    \"model\": PUBLISHER_NAME + \"/\" + PUBLISHER_MODEL_NAME,\n",
        "    \"messages\": [\n",
        "       {\"role\": \"system\", \"content\": \"/no_think\"}, \n",
        "       {\"role\":\"user\", \"content\":\"How many 'r's are in 'strawberry'?\"}\n",
        "    ],\n",
        "    \"temperature\": 0.6,\n",
        "    \"top_p\": 0.95,\n",
        "    \"max_tokens\": 1024,\n",
        "    \"stream\": False\n",
        "}\n",
        "\n",
        "request = json.dumps(PAYLOAD)\n",
        "\n",
        "!curl -X POST \\\n",
        "  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
        "  -H \"Content-Type: application/json\" {ENDPOINT}/v1/projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/{ENDPOINT_ID}:rawPredict \\\n",
        "  -d '{request}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "608382a0e83d"
      },
      "outputs": [],
      "source": [
        "# Reasoning on\n",
        "\n",
        "PAYLOAD = {\n",
        "    \"model\": PUBLISHER_NAME + \"/\" + PUBLISHER_MODEL_NAME,\n",
        "    \"messages\": [\n",
        "       {\"role\": \"system\", \"content\": \"/think\"}, \n",
        "       {\"role\":\"user\", \"content\":\"How many 'r's are in 'strawberry'?\"}\n",
        "    ],\n",
        "    \"temperature\": 0.6,\n",
        "    \"top_p\": 0.95,\n",
        "    \"max_tokens\": 1024,\n",
        "    \"stream\": False\n",
        "}\n",
        "\n",
        "request = json.dumps(PAYLOAD)\n",
        "\n",
        "!curl -X POST \\\n",
        "  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
        "  -H \"Content-Type: application/json\" {ENDPOINT}/v1/projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/{ENDPOINT_ID}:rawPredict \\\n",
        "  -d '{request}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6f52fae9379"
      },
      "source": [
        "#### Streaming call\n",
        "\n",
        "Sends a POST request to the specified API endpoint to stream a response from the model for a limerick writing using provided payload, with reasoning on / off."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c99761dcd7da"
      },
      "outputs": [],
      "source": [
        "# Reasoning off \n",
        "\n",
        "PAYLOAD = {\n",
        "    \"model\": PUBLISHER_NAME + \"/\" + PUBLISHER_MODEL_NAME,\n",
        "    \"messages\": [\n",
        "       {\"role\": \"system\", \"content\": \"/no_think\"}, \n",
        "       {\"role\":\"user\", \"content\":\"How many 'r's are in 'strawberry'?\"}\n",
        "    ],\n",
        "    \"temperature\": 0.6,\n",
        "    \"top_p\": 0.95,\n",
        "    \"max_tokens\": 1024,\n",
        "    \"stream\": True\n",
        "}\n",
        "\n",
        "request = json.dumps(PAYLOAD)\n",
        "!curl -X POST \\\n",
        "  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
        "  -H \"Content-Type: application/json\" {ENDPOINT}/v1/projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/{ENDPOINT_ID}:streamRawPredict \\\n",
        "  -d '{request}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d48704dda0a2"
      },
      "outputs": [],
      "source": [
        "# Reasoning on\n",
        "\n",
        "PAYLOAD = {\n",
        "    \"model\": PUBLISHER_NAME + \"/\" + PUBLISHER_MODEL_NAME,\n",
        "    \"messages\": [\n",
        "       {\"role\": \"system\", \"content\": \"/think\"}, \n",
        "       {\"role\":\"user\", \"content\":\"How many 'r's are in 'strawberry'?\"}\n",
        "    ],\n",
        "    \"temperature\": 0.6,\n",
        "    \"top_p\": 0.95,\n",
        "    \"max_tokens\": 1024,\n",
        "    \"stream\": True\n",
        "}\n",
        "\n",
        "request = json.dumps(PAYLOAD)\n",
        "!curl -X POST \\\n",
        "  -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
        "  -H \"Content-Type: application/json\" {ENDPOINT}/v1/projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/{ENDPOINT_ID}:streamRawPredict \\\n",
        "  -d '{request}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35f4dff97caa"
      },
      "source": [
        "## Using Vertex AI SDK for *Python*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e02fdb9ff02a"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6c0f4fbeb1e"
      },
      "outputs": [],
      "source": [
        "aiplatform.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2755acb7abd4"
      },
      "source": [
        "### Upload Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb35f67f32db"
      },
      "outputs": [],
      "source": [
        "model = aiplatform.Model.upload(\n",
        "    display_name=\"ModelGarden_LaunchPad_Endpoint_\" + time.strftime(\"%Y%m%d-%H%M%S\"),\n",
        "    model_garden_source_model_name=f\"publishers/{PUBLISHER_NAME}/models/{PUBLISHER_MODEL_NAME}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fffa89e57c84"
      },
      "source": [
        "### Create Endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff78186a6178"
      },
      "outputs": [],
      "source": [
        "my_endpoint = aiplatform.Endpoint.create(\n",
        "    display_name=\"ModelGarden_LaunchPad_Endpoint_\" + time.strftime(\"%Y%m%d-%H%M%S\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57df41f27203"
      },
      "source": [
        "### Deploy Model\n",
        "\n",
        "Deploy the model to the endpoint. The deployment process will take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e12dc6c390d9"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Configure machine types (please change if needed)\n",
        "\n",
        "MACHINE_CONFIG = {\n",
        "    \"a2-highgpu-8g\": {\"type\": \"NVIDIA_TESLA_A100\", \"count\": 8},\n",
        "    \"a2-ultragpu-8g\": {\"type\": \"NVIDIA_A100_80GB\", \"count\": 8},\n",
        "    \"a3-highgpu-4g\": {\"type\": \"NVIDIA_H100_80GB\", \"count\": 4},\n",
        "    \"g4-standard-384\": {\"type\": \"NVIDIA_RTX_PRO_6000\", \"count\": 8},\n",
        "}\n",
        "\n",
        "COMPATIBLE_MACHINES = list(MACHINE_CONFIG.keys())\n",
        "\n",
        "# Setup Widgets\n",
        "\n",
        "dropdown_machine = widgets.Dropdown(\n",
        "    options=COMPATIBLE_MACHINES,\n",
        "    description=\"Machine type:\",\n",
        "    style={\"description_width\": \"initial\"},\n",
        ")\n",
        "\n",
        "label_accelerator_type = widgets.HTML()\n",
        "label_accelerator_count = widgets.HTML()\n",
        "\n",
        "\n",
        "def update_labels(machine_type: str):\n",
        "    \"\"\"Update accelerator labels based on the selected machine.\"\"\"\n",
        "    info = MACHINE_CONFIG.get(machine_type, {\"type\": \"Unknown\", \"count\": 0})\n",
        "    label_accelerator_type.value = f\"<b>Accelerator type:</b> {info['type']}\"\n",
        "    label_accelerator_count.value = f\"<b>Accelerator count:</b> {info['count']}\"\n",
        "    return info\n",
        "\n",
        "\n",
        "# Event Handling\n",
        "\n",
        "\n",
        "def on_machine_change(change):\n",
        "    \"\"\"Handle machine selection change.\"\"\"\n",
        "    if change[\"name\"] == \"value\" and change[\"type\"] == \"change\":\n",
        "        global MACHINE_TYPE, ACCELERATOR_TYPE, ACCELERATOR_COUNT\n",
        "        MACHINE_TYPE = change.new\n",
        "        info = update_labels(MACHINE_TYPE)\n",
        "        ACCELERATOR_TYPE = info[\"type\"]\n",
        "        ACCELERATOR_COUNT = info[\"count\"]\n",
        "\n",
        "\n",
        "# Initiate Global Variables\n",
        "\n",
        "MACHINE_TYPE = dropdown_machine.value\n",
        "initial_info = update_labels(MACHINE_TYPE)\n",
        "ACCELERATOR_TYPE = initial_info[\"type\"]\n",
        "ACCELERATOR_COUNT = initial_info[\"count\"]\n",
        "\n",
        "dropdown_machine.observe(on_machine_change, names=\"value\")\n",
        "\n",
        "display(\n",
        "    widgets.VBox(\n",
        "        [\n",
        "            dropdown_machine,\n",
        "            label_accelerator_type,\n",
        "            label_accelerator_count,\n",
        "        ]\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2084cd94cc6"
      },
      "outputs": [],
      "source": [
        "model.deploy(\n",
        "    endpoint=my_endpoint,\n",
        "    deployed_model_display_name=\"ModelGarden_LaunchPad_DeployedModel_\"\n",
        "    + time.strftime(\"%Y%m%d-%H%M%S\"),\n",
        "    traffic_split={\"0\": 100},\n",
        "    machine_type=MACHINE_TYPE,\n",
        "    accelerator_type=ACCELERATOR_TYPE,\n",
        "    accelerator_count=ACCELERATOR_COUNT,\n",
        "    min_replica_count=1,\n",
        "    max_replica_count=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d458707ce79"
      },
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aa4287e76ed"
      },
      "source": [
        "#### Unary call\n",
        "\n",
        "Sends a POST request to the specified API endpoint to get a response from the model for a limerick writing using the provided payload, with reasoning on / off."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88016fa06a5f"
      },
      "outputs": [],
      "source": [
        "# Reasoning off\n",
        "\n",
        "PAYLOAD = {\n",
        "    \"model\": PUBLISHER_NAME + \"/\" + PUBLISHER_MODEL_NAME,\n",
        "    \"messages\": [\n",
        "        {\"role\": \"system\", \"content\": \"/no_think\"},\n",
        "        {\"role\": \"user\", \"content\": \"How many 'r's are in 'strawberry'?\"},\n",
        "    ],\n",
        "    \"temperature\": 0.6,\n",
        "    \"top_p\": 0.95,\n",
        "    \"max_tokens\": 1024,\n",
        "    \"stream\": False,\n",
        "}\n",
        "\n",
        "request = json.dumps(PAYLOAD)\n",
        "\n",
        "response = my_endpoint.raw_predict(\n",
        "    body=request, headers={\"Content-Type\": \"application/json\"}\n",
        ")\n",
        "\n",
        "result = json.loads(response.text)\n",
        "print(json.dumps(result[\"choices\"][0][\"message\"][\"content\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "375c05f8155f"
      },
      "outputs": [],
      "source": [
        "# Reasoning on\n",
        "\n",
        "PAYLOAD = {\n",
        "    \"model\": PUBLISHER_NAME + \"/\" + PUBLISHER_MODEL_NAME,\n",
        "    \"messages\": [\n",
        "        {\"role\": \"system\", \"content\": \"/think\"},\n",
        "        {\"role\": \"user\", \"content\": \"How many 'r's are in 'strawberry'?\"},\n",
        "    ],\n",
        "    \"temperature\": 0.6,\n",
        "    \"top_p\": 0.95,\n",
        "    \"max_tokens\": 1024,\n",
        "    \"stream\": False,\n",
        "}\n",
        "\n",
        "request = json.dumps(PAYLOAD)\n",
        "\n",
        "response = my_endpoint.raw_predict(\n",
        "    body=request, headers={\"Content-Type\": \"application/json\"}\n",
        ")\n",
        "\n",
        "result = json.loads(response.text)\n",
        "print(json.dumps(result[\"choices\"][0][\"message\"][\"content\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "724de41dc485"
      },
      "source": [
        "#### Streaming call\n",
        "\n",
        "Sends a POST request to the specified API endpoint to stream a response from the model for a limerick writing using provided payload, with reasoning on / off."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02fdb37d4015"
      },
      "outputs": [],
      "source": [
        "# Reasoning off\n",
        "\n",
        "PAYLOAD = {\n",
        "    \"model\": PUBLISHER_NAME + \"/\" + PUBLISHER_MODEL_NAME,\n",
        "    \"messages\": [\n",
        "        {\"role\": \"system\", \"content\": \"/no_think\"},\n",
        "        {\"role\": \"user\", \"content\": \"How many 'r's are in 'strawberry'?\"},\n",
        "    ],\n",
        "    \"temperature\": 0.6,\n",
        "    \"top_p\": 0.95,\n",
        "    \"max_tokens\": 1024,\n",
        "    \"stream\": True,\n",
        "}\n",
        "\n",
        "request = json.dumps(PAYLOAD)\n",
        "\n",
        "for stream_response in my_endpoint.stream_raw_predict(\n",
        "    body=request, headers={\"Content-Type\": \"application/json\"}\n",
        "):\n",
        "    print(stream_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14da6e58cd37"
      },
      "outputs": [],
      "source": [
        "# Reasoning on\n",
        "\n",
        "PAYLOAD = {\n",
        "    \"model\": PUBLISHER_NAME + \"/\" + PUBLISHER_MODEL_NAME,\n",
        "    \"messages\": [\n",
        "        {\"role\": \"system\", \"content\": \"/think\"},\n",
        "        {\"role\": \"user\", \"content\": \"How many 'r's are in 'strawberry'?\"},\n",
        "    ],\n",
        "    \"temperature\": 0.6,\n",
        "    \"top_p\": 0.95,\n",
        "    \"max_tokens\": 1024,\n",
        "    \"stream\": True,\n",
        "}\n",
        "\n",
        "request = json.dumps(PAYLOAD)\n",
        "\n",
        "for stream_response in my_endpoint.stream_raw_predict(\n",
        "    body=request, headers={\"Content-Type\": \"application/json\"}\n",
        "):\n",
        "    print(stream_response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "llama_nemotron_intro.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

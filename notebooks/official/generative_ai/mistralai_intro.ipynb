{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9A9NkTRTfo2I"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPprg6Oz0QDs"
      },
      "source": [
        "# Getting Started with Mistral AI Models\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/generative_ai/mistralai_intro.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Fgenerative_ai%2Fmistralai_intro.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">                                                                             \n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/generative_ai/mistralai_intro.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/generative_ai/mistralai_intro.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fK_rdvvx1iZ"
      },
      "source": [
        "## Overview\n",
        "\n",
        "### Mistral AI on Vertex AI\n",
        "\n",
        "Mistral AI models on Vertex AI offer fully managed and serverless models are offered as managed APIs. To use a Mistral AI model on Vertex AI, send a request directly to the Vertex AI API endpoint.\n",
        "\n",
        "You can stream your Mistral AI model responses to reduce the end-user latency perception. A streamed response uses server-sent events (SSE) to incrementally stream the response.\n",
        "\n",
        "Learn more about [Vertex AI](https://cloud.google.com/vertex-ai).\n",
        "\n",
        "### Available Mistral AI models\n",
        "\n",
        "*   ### Mistral Large (2407)\n",
        "Complex tasks that require large reasoning capabilities or are highly specialized (synthetic text Generation, code generation, RAG, or agents). [Blog Post](https://mistral.ai/news/mistral-large-2407/)\n",
        "\n",
        "*   ### Mistral Nemo\n",
        "Reasoning, world knowledge, and coding performance are state-of-the-art in its size category.\n",
        "\n",
        "*   ### Codestral\n",
        "Coding specific tasks to enhance developers productivity with code completion and fill-in-the-middle capabilities.\n",
        "\n",
        "\n",
        "## Objective\n",
        "\n",
        "This notebook shows how to use **Vertex AI API** to call the Mistral AI models on Vertex AI API with the Large, Nemo, and Codestral models.\n",
        "\n",
        "For more information, see the [Use Mistral's](https://docs.mistral.ai/) documentation and [Mistral's models](https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/mistral) on Google Cloud.\n",
        "\n",
        "- Mistral on Model Garden supports the same API calls as Mistral’s own API endpoints, except for the `safe_prompt` parameter that will return an error if specified in the input. So do not include `safe_prompt` in input requests.\n",
        "- Documentation links\n",
        "  - [Mistral APIs](https://docs.mistral.ai/api/)\n",
        "  - [Chat Completion](https://docs.mistral.ai/api/#operation/createChatCompletion) operations supported by Mistral Large, Mistral Nemo and Codestral\n",
        "  - [Fill-in-the-middle](https://docs.mistral.ai/api/#operation/createFIMCompletion) operations supported by Codestral"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcJCV6Dw5usD"
      },
      "source": [
        "## Vertex AI API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwYvaaW25jYS"
      },
      "source": [
        "## Get Started - Required first steps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a5bea26f60f"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c97be6a73155"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fxZn4SAbxdl"
      },
      "source": [
        "#### Select one of Mistral AI models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8X70FTSbx7U"
      },
      "outputs": [],
      "source": [
        "MODEL = \"mistral-large\"  # @param [\"mistral-large\", \"mistral-nemo\", \"codestral\"]\n",
        "if MODEL == \"mistral-large\":\n",
        "    available_regions = [\"europe-west4\", \"us-central1\"]\n",
        "    available_versions = [\"latest\", \"2407\"]\n",
        "elif MODEL == \"mistral-nemo\":\n",
        "    available_regions = [\"europe-west4\", \"us-central1\"]\n",
        "    available_versions = [\"latest\", \"2407\"]\n",
        "elif MODEL == \"codestral\":\n",
        "    available_regions = [\"europe-west4\", \"us-central1\"]\n",
        "    available_versions = [\"latest\", \"2405\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpuX3sKtexlK"
      },
      "source": [
        "#### Select a location and a version from the dropdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHl8xW45ex_O"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "dropdown_loc = widgets.Dropdown(\n",
        "    options=available_regions,\n",
        "    description=\"Select a location:\",\n",
        "    font_weight=\"bold\",\n",
        "    style={\"description_width\": \"initial\"},\n",
        ")\n",
        "\n",
        "dropdown_ver = widgets.Dropdown(\n",
        "    options=available_versions,\n",
        "    description=\"Select the model version (optional):\",\n",
        "    font_weight=\"bold\",\n",
        "    style={\"description_width\": \"initial\"},\n",
        ")\n",
        "\n",
        "\n",
        "def dropdown_loc_eventhandler(change):\n",
        "    global LOCATION\n",
        "    if change[\"type\"] == \"change\" and change[\"name\"] == \"value\":\n",
        "        LOCATION = change.new\n",
        "        print(\"Selected:\", change.new)\n",
        "\n",
        "\n",
        "def dropdown_ver_eventhandler(change):\n",
        "    global MODEL_VERSION\n",
        "    if change[\"type\"] == \"change\" and change[\"name\"] == \"value\":\n",
        "        MODEL_VERSION = change.new\n",
        "        print(\"Selected:\", change.new)\n",
        "\n",
        "\n",
        "LOCATION = dropdown_loc.value\n",
        "dropdown_loc.observe(dropdown_loc_eventhandler, names=\"value\")\n",
        "display(dropdown_loc)\n",
        "\n",
        "MODEL_VERSION = dropdown_ver.value\n",
        "dropdown_ver.observe(dropdown_ver_eventhandler, names=\"value\")\n",
        "display(dropdown_ver)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q58icinBjoK"
      },
      "source": [
        "#### Set Google Cloud project and model information\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hltNx33t6cSZ"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "ENDPOINT = f\"https://{LOCATION}-aiplatform.googleapis.com\"\n",
        "SELECTED_MODEL_VERSION = \"\" if MODEL_VERSION == \"latest\" else f\"@{MODEL_VERSION}\"\n",
        "\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    raise ValueError(\"Please set your PROJECT_ID\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NAstKRFBt4N"
      },
      "source": [
        "#### Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZEFLE6a6bqy"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import subprocess\n",
        "\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVVnOhvE_PA6"
      },
      "source": [
        "### Sample Requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ahw-uFjCAbo"
      },
      "source": [
        "#### Text generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61107099357a"
      },
      "source": [
        "##### Unary call\n",
        "\n",
        "Sends a POST request to the specified API endpoint to get a response from the model using the provided payload."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zFz260B50oi"
      },
      "outputs": [],
      "source": [
        "PAYLOAD = {\n",
        "    \"model\": MODEL,\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"who is the best French painter?\"}],\n",
        "    \"max_tokens\": 100,\n",
        "    \"stream\": False,\n",
        "}\n",
        "\n",
        "request = json.dumps(PAYLOAD)\n",
        "!curl -X POST -H \"Authorization: Bearer $(gcloud auth print-access-token)\" -H \"Content-Type: application/json\" {ENDPOINT}/v1/projects/{PROJECT_ID}/locations/{LOCATION}/publishers/mistralai/models/{MODEL}{SELECTED_MODEL_VERSION}:rawPredict -d '{request}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf56XLe1EZos"
      },
      "source": [
        "With a pretty response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L90Kdr-PEYr3"
      },
      "outputs": [],
      "source": [
        "# Get the access token\n",
        "process = subprocess.Popen(\n",
        "    \"gcloud auth print-access-token\", stdout=subprocess.PIPE, shell=True\n",
        ")\n",
        "(access_token_bytes, err) = process.communicate()\n",
        "access_token = access_token_bytes.decode(\"utf-8\").strip()  # Strip newline\n",
        "\n",
        "# Define query headers\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {access_token}\",\n",
        "    \"Accept\": \"application/json\",\n",
        "}\n",
        "\n",
        "# Replace with your actual values\n",
        "url = f\"{ENDPOINT}/v1/projects/{PROJECT_ID}/locations/{LOCATION}/publishers/mistralai/models/{MODEL}{SELECTED_MODEL_VERSION}:rawPredict\"\n",
        "data = {\n",
        "    \"model\": MODEL,\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"who is the best French painter?\"}],\n",
        "    \"stream\": False,\n",
        "}\n",
        "\n",
        "# Make the POST request\n",
        "response = requests.post(url, headers=headers, json=data)\n",
        "\n",
        "# Check status code and try to parse the response as JSON\n",
        "if response.status_code == 200:\n",
        "    try:\n",
        "        response_dict = response.json()\n",
        "        print(response_dict[\"choices\"][0][\"message\"][\"content\"])\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(\"Error decoding JSON:\", e)\n",
        "        print(\"Raw response:\", response.text)  # Print raw response if parsing fails\n",
        "else:\n",
        "    print(f\"Request failed with status code: {response.status_code}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6f52fae9379"
      },
      "source": [
        "##### Streaming call\n",
        "\n",
        "Sends a POST request to the specified API endpoint to stream a response from the model using the provided payload."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c99761dcd7da"
      },
      "outputs": [],
      "source": [
        "PAYLOAD = {\n",
        "    \"model\": MODEL,\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"who is the best French painter?\"}],\n",
        "    \"max_tokens\": 100,\n",
        "    \"stream\": True,\n",
        "}\n",
        "\n",
        "request = json.dumps(PAYLOAD)\n",
        "!curl -X POST -H \"Authorization: Bearer $(gcloud auth print-access-token)\" -H \"Content-Type: application/json\" {ENDPOINT}/v1/projects/{PROJECT_ID}/locations/{LOCATION}/publishers/mistralai/models/{MODEL}{SELECTED_MODEL_VERSION}:streamRawPredict -d '{request}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6HolebUhShT"
      },
      "source": [
        "#### Code generation\n",
        "\n",
        "Mistral Large, Mistral Nemo and Codestral support code generation with the Chat Completion operations covered above.\n",
        "\n",
        "With Codestral, you can also do Fill-in-the-middle operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deUZQwgSheEr"
      },
      "source": [
        "##### Fill-in-the-middle (FIM)\n",
        "With this feature, users can define the starting point of the code using a `prompt`, and the ending point of the code using an optional `suffix` and an optional `stop`.\n",
        "\n",
        "The Codestral model will then generate the code that fits in between, making it ideal for tasks that require a specific piece of code to be generated.\n",
        "\n",
        "More information on FIM:\n",
        "- [Mistral API Documentation FIM](https://docs.mistral.ai/api/#operation/createFIMCompletion)\n",
        "- [Mistral FIM Documentation](https://docs.mistral.ai/capabilities/code_generation/#fill-in-the-middle-endpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JzsH7TmujqR"
      },
      "source": [
        "Example 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zXL4RrnhRLG"
      },
      "outputs": [],
      "source": [
        "MODEL = \"codestral\"\n",
        "SELECTED_MODEL_VERSION = \"\"\n",
        "\n",
        "PAYLOAD = {\n",
        "    \"model\": MODEL,\n",
        "    \"prompt\": \"def say_hello(name: str) -> str\",\n",
        "    \"suffix\": \"return n_words\",\n",
        "}\n",
        "\n",
        "request = json.dumps(PAYLOAD)\n",
        "!curl -X POST -H \"Authorization: Bearer $(gcloud auth print-access-token)\" -H \"Content-Type: application/json\" {ENDPOINT}/v1/projects/{PROJECT_ID}/locations/{LOCATION}/publishers/mistralai/models/{MODEL}{SELECTED_MODEL_VERSION}:streamRawPredict -d '{request}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6kMTcg5ulJD"
      },
      "source": [
        "Example 2 with pretty response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciJKueNfDman"
      },
      "outputs": [],
      "source": [
        "MODEL = \"codestral\"\n",
        "\n",
        "# Get the access token\n",
        "process = subprocess.Popen(\n",
        "    \"gcloud auth print-access-token\", stdout=subprocess.PIPE, shell=True\n",
        ")\n",
        "(access_token_bytes, err) = process.communicate()\n",
        "access_token = access_token_bytes.decode(\"utf-8\").strip()  # Strip newline\n",
        "\n",
        "# Define query headers\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {access_token}\",\n",
        "    \"Accept\": \"application/json\",\n",
        "}\n",
        "\n",
        "# Replace with your actual values\n",
        "url = f\"{ENDPOINT}/v1/projects/{PROJECT_ID}/locations/{LOCATION}/publishers/mistralai/models/{MODEL}:rawPredict\"\n",
        "data = {\n",
        "    \"model\": MODEL,\n",
        "    \"prompt\": \"def f(\",\n",
        "    \"suffix\": \"return a + b\",\n",
        "    \"max_tokens\": 64,\n",
        "    \"temperature\": 0,\n",
        "}\n",
        "\n",
        "# Make the POST request\n",
        "response = requests.post(url, headers=headers, json=data)\n",
        "\n",
        "# Check status code and try to parse the response as JSON\n",
        "if response.status_code == 200:\n",
        "    try:\n",
        "        response_dict = response.json()\n",
        "        print(response_dict[\"choices\"][0][\"message\"][\"content\"])\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(\"Error decoding JSON:\", e)\n",
        "        print(\"Raw response:\", response.text)  # Print raw response if parsing fails\n",
        "else:\n",
        "    print(f\"Request failed with status code: {response.status_code}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G8kI3S_FUjM"
      },
      "source": [
        "## Using Mistral AI's Vertex SDK for *Python*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdzqqcJKOYHa"
      },
      "source": [
        "## Get Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TGDKo-vFn5F"
      },
      "source": [
        "### Install Mistral's Vertex SDK for Python and other required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AdfzWYfFw3u"
      },
      "outputs": [],
      "source": [
        "! pip3 install -U -q 'mistralai[gcp]>=1.0.3'\n",
        "! pip3 install -U -q httpx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9EvH5iez_n_"
      },
      "outputs": [],
      "source": [
        "# Get the access token\n",
        "import subprocess\n",
        "\n",
        "process = subprocess.Popen(\n",
        "    \"gcloud auth print-access-token\", stdout=subprocess.PIPE, shell=True\n",
        ")\n",
        "(access_token_bytes, err) = process.communicate()\n",
        "access_token = access_token_bytes.decode(\"utf-8\").strip()  # Strip newline\n",
        "\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {access_token}\",\n",
        "    \"Content-Type\": \"application/json\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTbP9RNlF1nO"
      },
      "source": [
        "### Restart runtime (Colab only)\n",
        "\n",
        "To use the newly installed packages, you must restart the runtime on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCjYtEc8F437"
      },
      "outputs": [],
      "source": [
        "# Restart kernel after installs so that your environment can access the new packages\n",
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fgq8ZhyuF8wx"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3nepikRF_Gt"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "Authenticate your environment on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qgHPk0IGBZX"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ICjT5ZNGF2c"
      },
      "source": [
        "#### Select one of Mistral AI models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBRTMXhmGIlN"
      },
      "outputs": [],
      "source": [
        "MODEL = \"mistral-large\"  # @param [\"mistral-large\", \"mistral-nemo\", \"codestral\"]\n",
        "if MODEL == \"mistral-large\":\n",
        "    available_regions = [\"europe-west4\", \"us-central1\"]\n",
        "    available_versions = [\"2407\"]\n",
        "elif MODEL == \"mistral-nemo\":\n",
        "    available_regions = [\"europe-west4\", \"us-central1\"]\n",
        "    available_versions = [\"2407\"]\n",
        "elif MODEL == \"codestral\":\n",
        "    available_regions = [\"europe-west4\", \"us-central1\"]\n",
        "    available_versions = [\"2405\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "227_bG2OGNec"
      },
      "source": [
        "#### Select a location and a version from the dropdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoP1c89sGP7k"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "dropdown_loc = widgets.Dropdown(\n",
        "    options=available_regions,\n",
        "    description=\"Select a location:\",\n",
        "    font_weight=\"bold\",\n",
        "    style={\"description_width\": \"initial\"},\n",
        ")\n",
        "\n",
        "dropdown_ver = widgets.Dropdown(\n",
        "    options=available_versions,\n",
        "    description=\"Select the model version (optional):\",\n",
        "    font_weight=\"bold\",\n",
        "    style={\"description_width\": \"initial\"},\n",
        ")\n",
        "\n",
        "\n",
        "def dropdown_loc_eventhandler(change):\n",
        "    global LOCATION\n",
        "    if change[\"type\"] == \"change\" and change[\"name\"] == \"value\":\n",
        "        LOCATION = change.new\n",
        "        print(\"Selected:\", change.new)\n",
        "\n",
        "\n",
        "def dropdown_ver_eventhandler(change):\n",
        "    global MODEL_VERSION\n",
        "    if change[\"type\"] == \"change\" and change[\"name\"] == \"value\":\n",
        "        MODEL_VERSION = change.new\n",
        "        print(\"Selected:\", change.new)\n",
        "\n",
        "\n",
        "LOCATION = dropdown_loc.value\n",
        "dropdown_loc.observe(dropdown_loc_eventhandler, names=\"value\")\n",
        "display(dropdown_loc)\n",
        "\n",
        "MODEL_VERSION = dropdown_ver.value\n",
        "dropdown_ver.observe(dropdown_ver_eventhandler, names=\"value\")\n",
        "display(dropdown_ver)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mghCnsVbK5YP"
      },
      "source": [
        "#### Set Google Cloud project and model information\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NB-_kGiJLDcT"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "ENDPOINT = f\"https://{LOCATION}-aiplatform.googleapis.com\"\n",
        "\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    raise ValueError(\"Please set your PROJECT_ID\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD9oKNOGMA18"
      },
      "source": [
        "#### Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgmj4wwLMYgd"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQnpW1WLO3uX"
      },
      "source": [
        "### Sample Requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tH0QFYIXMZNZ"
      },
      "source": [
        "#### Text generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm2rUyXtPgp2"
      },
      "source": [
        "##### Unary call\n",
        "\n",
        "Initializes a client for Mistral AI's Vertex AI, sends a request to generate the content, and prints the response in a formatted JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRqiGVUdMgXs"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "from mistralai_gcp import MistralGoogleCloud\n",
        "\n",
        "client = MistralGoogleCloud(\n",
        "    access_token=access_token, region=LOCATION, project_id=PROJECT_ID\n",
        ")\n",
        "\n",
        "try:\n",
        "    resp = client.chat.complete(\n",
        "        model=f\"{MODEL}-{MODEL_VERSION}\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Who is the best French painter? Answer in one short sentence.\",\n",
        "            }\n",
        "        ],\n",
        "    )\n",
        "    print(resp.choices[0].message.content)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWkLsn5GQrqu"
      },
      "source": [
        "##### Streaming call\n",
        "\n",
        "Initializes a client for Mistral AI's Vertex AI, sends a streaming request to generate the content, and continuously prints the received text as it is streamed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uOfrmmRQ-rJ"
      },
      "outputs": [],
      "source": [
        "from mistralai_gcp import MistralGoogleCloud\n",
        "\n",
        "client = MistralGoogleCloud(\n",
        "    access_token=access_token, region=LOCATION, project_id=PROJECT_ID\n",
        ")\n",
        "\n",
        "try:\n",
        "    stream = client.chat.stream(\n",
        "        model=f\"{MODEL}-{MODEL_VERSION}\",\n",
        "        max_tokens=1024,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Who is the best French painter? Answer in one short sentence.\",\n",
        "            }\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    for chunk in stream:\n",
        "        print(chunk.data.choices[0].delta.content)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEZcdznmRxT6"
      },
      "source": [
        "#### Code generation\n",
        "\n",
        "Mistral Large, Mistral Nemo and Codestral support code generation with the Chat Completion operations covered above.\n",
        "\n",
        "With Codestral, you can also do Fill-in-the-middle operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWl3pMDOXdEZ"
      },
      "source": [
        "##### Fill-in-the-middle (FIM)\n",
        "With this feature, users can define the starting point of the code using a `prompt`, and the ending point of the code using an optional `suffix` and an optional `stop`.\n",
        "\n",
        "The Codestral model will then generate the code that fits in between, making it ideal for tasks that require a specific piece of code to be generated.\n",
        "\n",
        "More information on FIM:\n",
        "- [Mistral API Documentation FIM](https://docs.mistral.ai/api/#operation/createFIMCompletion)\n",
        "- [Mistral FIM Documentation](https://docs.mistral.ai/capabilities/code_generation/#fill-in-the-middle-endpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o61l9pOsTUc-"
      },
      "outputs": [],
      "source": [
        "from mistralai_gcp import MistralGoogleCloud\n",
        "\n",
        "client = MistralGoogleCloud(\n",
        "    access_token=access_token, region=LOCATION, project_id=PROJECT_ID\n",
        ")\n",
        "\n",
        "MODEL = \"codestral\"\n",
        "MODEL_VERSION = \"2405\"\n",
        "\n",
        "try:\n",
        "    resp = client.fim.complete(\n",
        "        model=f\"{MODEL}-{MODEL_VERSION}\",\n",
        "        prompt=\"def count_words_in_file(file_path: str) -> int\",\n",
        "        suffix=\"return n_words\",\n",
        "    )\n",
        "\n",
        "    print(resp.choices[0].message.content)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsJLkqyyztR_"
      },
      "source": [
        "## Model Capabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfmwOYBXxcQG"
      },
      "source": [
        "### Function Calling with Mistral Large\n",
        "\n",
        "Function calling allows Mistral models to connect to external tools. By integrating Mistral models with external tools such as user defined functions or APIs, users can easily build applications catering to specific use cases and practical problems.\n",
        "\n",
        "This guide is the one Mistral provides [here](https://docs.mistral.ai/capabilities/function_calling/). We write two functions for tracking payment status and payment date. We can use these two tools to provide answers for payment-related queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLmoam_z0OfN"
      },
      "source": [
        "#### Step 1. User: specify tools\n",
        "\n",
        "Define sample data like this was stored in a sample database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oak2cRcgx7VB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming we have the following data\n",
        "data = {\n",
        "    \"transaction_id\": [\"T1001\", \"T1002\", \"T1003\", \"T1004\", \"T1005\"],\n",
        "    \"customer_id\": [\"C001\", \"C002\", \"C003\", \"C002\", \"C001\"],\n",
        "    \"payment_amount\": [125.50, 89.99, 120.00, 54.30, 210.20],\n",
        "    \"payment_date\": [\n",
        "        \"2021-10-05\",\n",
        "        \"2021-10-06\",\n",
        "        \"2021-10-07\",\n",
        "        \"2021-10-05\",\n",
        "        \"2021-10-08\",\n",
        "    ],\n",
        "    \"payment_status\": [\"Paid\", \"Unpaid\", \"Paid\", \"Paid\", \"Pending\"],\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxb7_m9UxxmW"
      },
      "source": [
        "Define the functions that will be used as tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfOuDWlYxdvL"
      },
      "outputs": [],
      "source": [
        "def retrieve_payment_status(df: data, transaction_id: str) -> str:\n",
        "    if transaction_id in df.transaction_id.values:\n",
        "        return json.dumps(\n",
        "            {\"status\": df[df.transaction_id == transaction_id].payment_status.item()}\n",
        "        )\n",
        "    return json.dumps({\"error\": \"transaction id not found.\"})\n",
        "\n",
        "\n",
        "def retrieve_payment_date(df: data, transaction_id: str) -> str:\n",
        "    if transaction_id in df.transaction_id.values:\n",
        "        return json.dumps(\n",
        "            {\"date\": df[df.transaction_id == transaction_id].payment_date.item()}\n",
        "        )\n",
        "    return json.dumps({\"error\": \"transaction id not found.\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66OSfAKVyJnn"
      },
      "source": [
        "Define the tools for those functions following the right JSON format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5xBTtIGxzRS"
      },
      "outputs": [],
      "source": [
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"retrieve_payment_status\",\n",
        "            \"description\": \"Get payment status of a transaction\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"transaction_id\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The transaction id.\",\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"transaction_id\"],\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"retrieve_payment_date\",\n",
        "            \"description\": \"Get payment date of a transaction\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"transaction_id\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The transaction id.\",\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"transaction_id\"],\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DCJP8hN2X-c"
      },
      "source": [
        "#### Step 2. Model: Generate the right tool and arguments with Mistral Large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmhX-3FDy1vS"
      },
      "outputs": [],
      "source": [
        "url = f\"{ENDPOINT}/v1/projects/{PROJECT_ID}/locations/{LOCATION}/publishers/mistralai/models/{MODEL}:rawPredict\"\n",
        "data = {\n",
        "    \"model\": MODEL,\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": \"What is the status of my transaction T1001?\"}\n",
        "    ],\n",
        "    \"tools\": tools,\n",
        "    \"tool_choice\": \"any\",\n",
        "}\n",
        "function_name = None\n",
        "function_params = None\n",
        "\n",
        "# Make the POST request\n",
        "response = requests.post(url, headers=headers, json=data)\n",
        "\n",
        "# Check status code and try to parse the response as JSON\n",
        "if response.status_code == 200:\n",
        "    try:\n",
        "        response_dict = response.json()\n",
        "        tool_call = response_dict[\"choices\"][0][\"message\"][\"tool_calls\"][0]\n",
        "        function_name = tool_call[\"function\"][\"name\"]\n",
        "        function_params = json.loads(tool_call[\"function\"][\"arguments\"])\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(\"Error decoding JSON:\", e)\n",
        "        print(\"Raw response:\", response.text)  # Print raw response if parsing fails\n",
        "else:\n",
        "    print(f\"Request failed with status code: {response.status_code}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkG59Jsg1hnE"
      },
      "source": [
        "#### Step 3. User: Extract the tool function name, the params and execute the tool function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQHeV3Qz032W"
      },
      "outputs": [],
      "source": [
        "if function_name and function_params:\n",
        "    print(\"\\nfunction_name: \", function_name, \"\\nfunction_params: \", function_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-nOJR6hCpYz"
      },
      "source": [
        "Map function names returned by Mistral model to the actual function object in the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W30ACckn2N_F"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "\n",
        "names_to_functions = {\n",
        "    \"retrieve_payment_status\": functools.partial(retrieve_payment_status, df=df),\n",
        "    \"retrieve_payment_date\": functools.partial(retrieve_payment_date, df=df),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78S2e-TOCyQn"
      },
      "source": [
        "Call the right function with the parameters suggested by Mistral's model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Pk7XszE1nXI"
      },
      "outputs": [],
      "source": [
        "if function_name and function_params:\n",
        "    function_result = names_to_functions[function_name](**function_params)\n",
        "    function_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbEuWr3M922W"
      },
      "source": [
        "### JSON Output Mode\n",
        "\n",
        "You can force the response format to JSON by adding `\"response_format\": {\"type\": \"json_object\"}` in the JSON payload of the request\n",
        "See Mistral's documentation on JSON mode\n",
        "\n",
        "*   See Mistral's [documentation](https://docs.mistral.ai/capabilities/json_mode/) on JSON mode\n",
        "*   See Mistral's API [documentation](https://docs.mistral.ai/api/#operation/createChatCompletion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-695Dyt-Q9V"
      },
      "outputs": [],
      "source": [
        "PAYLOAD = {\n",
        "    \"model\": MODEL,\n",
        "    \"messages\": [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What is the best French cheese? Return the product and produce location in JSON format\",\n",
        "        }\n",
        "    ],\n",
        "    \"response_format\": {\"type\": \"json_object\"},\n",
        "}\n",
        "\n",
        "request = json.dumps(PAYLOAD)\n",
        "!curl -X POST -H \"Authorization: Bearer $(gcloud auth print-access-token)\" -H \"Content-Type: application/json\" {ENDPOINT}/v1/projects/{PROJECT_ID}/locations/{LOCATION}/publishers/mistralai/models/{MODEL}:rawPredict -d '{request}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DQlJjz7DXDu"
      },
      "source": [
        "Pretty response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ky1TDqhc-bur"
      },
      "outputs": [],
      "source": [
        "# Get the access token\n",
        "process = subprocess.Popen(\n",
        "    \"gcloud auth print-access-token\", stdout=subprocess.PIPE, shell=True\n",
        ")\n",
        "(access_token_bytes, err) = process.communicate()\n",
        "access_token = access_token_bytes.decode(\"utf-8\").strip()  # Strip newline\n",
        "\n",
        "# Replace with your actual values\n",
        "url = f\"{ENDPOINT}/v1/projects/{PROJECT_ID}/locations/{LOCATION}/publishers/mistralai/models/{MODEL}:rawPredict\"\n",
        "data = {\n",
        "    \"model\": MODEL,\n",
        "    \"messages\": [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What is the best French cheese? Return the product and produce location in JSON format\",\n",
        "        }\n",
        "    ],\n",
        "    \"response_format\": {\"type\": \"json_object\"},\n",
        "}\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {access_token}\",\n",
        "    \"Content-Type\": \"application/json\",\n",
        "}\n",
        "\n",
        "# Make the POST request\n",
        "response = requests.post(url, headers=headers, json=data)\n",
        "\n",
        "# Check status code and try to parse the response as JSON\n",
        "if response.status_code == 200:\n",
        "    try:\n",
        "        response_dict = response.json()\n",
        "        print(response_dict[\"choices\"][0][\"message\"][\"content\"])\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(\"Error decoding JSON:\", e)\n",
        "        print(\"Raw response:\", response.text)  # Print raw response if parsing fails\n",
        "else:\n",
        "    print(f\"Request failed with status code: {response.status_code}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "mistralai_intro.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

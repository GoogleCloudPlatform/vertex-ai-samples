{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eec5cc39a59"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da28efe88436"
      },
      "source": [
        "# Getting Started with Mistral AI OCR\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/generative_ai/mistralai_ocr.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Fgenerative_ai%2Fmistralai_ocr.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">                                                                             \n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/generative_ai/mistralai_ocr.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/generative_ai/mistralai_ocr.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccc3e15e21c5"
      },
      "source": [
        "NOTE: This notebook has been tested in the following environment:\n",
        "\n",
        "Python version = 3.10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e9bfd2d8658"
      },
      "source": [
        "## Objective\n",
        "\n",
        "Mistral OCR (25.05) is a model specialized in extracting text and images from documents. It is specifically built to preserve the structure of the document pages and automatically formats the extracted text in Markdown.\n",
        "\n",
        "The objective of this notebook is to provide a summary overview of the Mistral OCR (25.05) model's capabilities and how to leverage it using the Vertex AI platform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "200afb22a42b"
      },
      "outputs": [],
      "source": [
        "%pip install -U -q httpx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "563f89d0115e"
      },
      "source": [
        "## Getting started\n",
        "\n",
        "Before proceeding further, fill in the following information:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "85932a54e4db"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"\"\n",
        "REGION = \"\"\n",
        "MODEL_NAME = \"mistral-ocr\"\n",
        "MODEL_VERSION = \"2505\"\n",
        "TEST_DOC_URL = \"https://arxiv.org/pdf/2410.07073\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5544ad27e670"
      },
      "source": [
        "As a developer, your first step is to authenticate your notebook environment. If you are using Google Colab to run it, the following cell should take care of the authentication, otherwise it will run the `gcloud` command to retrieve the access token needed to authenticate your API calls:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e127418e326"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()\n",
        "else:\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"gcloud\", \"auth\", \"print-access-token\"],\n",
        "            check=True,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            text=True,\n",
        "        )\n",
        "        access_token = result.stdout.strip()\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error while running command: {e.stderr}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9206ed0b71c"
      },
      "source": [
        "## Calling the OCR model (HTTP)\n",
        "\n",
        "Start by defining a simple function that will help building the URL of your model endpoint:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70651e83cff5"
      },
      "outputs": [],
      "source": [
        "def build_endpoint_url(\n",
        "    region: str, project_id: str, model_name: str, model_version: str\n",
        ") -> str:\n",
        "    base_url = f\"https://{region}-aiplatform.googleapis.com/v1\"\n",
        "    endpoint_url_segments = [\n",
        "        base_url,\n",
        "        f\"projects/{project_id}\",\n",
        "        f\"locations/{region}\",\n",
        "        \"publishers/mistralai\",\n",
        "        f\"models/{model_name}-{model_version}\",\n",
        "    ]\n",
        "    specifier = \"rawPredict\"  # Streaming is not supported\n",
        "    endpoint_url = \"/\".join(endpoint_url_segments) + f\":{specifier}\"\n",
        "    return endpoint_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab383be0128c"
      },
      "source": [
        "You can now send your HTTP request to the model endpoint:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc3b5bd0e27a"
      },
      "source": [
        "Calling the OCR model is done via a HTTP POST request where the document to be processed is passed in the payload as a base64-encoded string. The following cell defines another helper function that downloads a given PDF file from a URL and encodes it in base64. If you already have your own documents at hand you can easily modify it to only keep the encoding part.\n",
        "\n",
        "**Warning**: the larger the document, the bigger the payload and the longer the model will take to handle it. To avoid timeout issues it is advised to split the document into smaller chunks. The number and size of chunks will depend on the total volume of documents you wish to process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57561631be74"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "\n",
        "\n",
        "def download_pdf_and_base64_encode(pdf_url: str) -> str:\n",
        "    resp = httpx.get(pdf_url)\n",
        "    resp.raise_for_status()\n",
        "    content_bytes = resp.content\n",
        "    content_encoded_pdf = base64.b64encode(content_bytes).decode(\"utf-8\")\n",
        "    return content_encoded_pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87092165402d"
      },
      "source": [
        "You can now send the HTTP request to the model endpoint. Note that you can also optionally:\n",
        "\n",
        "- limit the number of scanned pages,\n",
        "- retrieved any image detected by the model, in the form of base64-encoded strings.\n",
        "\n",
        "Check the detailed list of available options and values in the [Mistral AI API documentation](https://docs.mistral.ai/api/#tag/ocr)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98ecd45c2aec"
      },
      "outputs": [],
      "source": [
        "import httpx\n",
        "\n",
        "# URL\n",
        "url = build_endpoint_url(\n",
        "    region=REGION,\n",
        "    project_id=PROJECT_ID,\n",
        "    model_name=MODEL_NAME,\n",
        "    model_version=MODEL_VERSION,\n",
        ")\n",
        "\n",
        "# Headers\n",
        "headers = {\"Content-Type\": \"application/json\", \"Accept\": \"application/json\"}\n",
        "if \"google.colab\" not in sys.modules:\n",
        "    headers[\"Authorization\"] = f\"Bearer {access_token}\"\n",
        "\n",
        "# Payload\n",
        "encoded_doc = download_pdf_and_base64_encode(TEST_DOC_URL)\n",
        "payload = {\n",
        "    \"model\": f\"{MODEL_NAME}-{MODEL_VERSION}\",\n",
        "    \"document\": {\n",
        "        \"type\": \"document_url\",\n",
        "        \"document_url\": f\"data:application/pdf;base64,{encoded_doc}\",\n",
        "    },\n",
        "}\n",
        "\n",
        "# Request\n",
        "model_resp = httpx.post(url=url, headers=headers, json=payload, timeout=3600)\n",
        "\n",
        "# Response\n",
        "model_resp.raise_for_status()\n",
        "if model_resp.status_code == 200:\n",
        "    scanned_doc = model_resp.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c23e104bb19"
      },
      "source": [
        "## Parsing the results\n",
        "\n",
        "If your request was successful, the `scanned_doc` variable should contain:\n",
        "\n",
        "* `.pages` : a list of dicts containing, for each scanned page, the Markdown-formatted text detected.\n",
        "* `.usage_info`: the total count of pages processed as well as the scanned document's size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cfe8e16ddef"
      },
      "outputs": [],
      "source": [
        "# Beginning of the first page's content\n",
        "\n",
        "content_extract = scanned_doc[\"pages\"][0][\"markdown\"][:256]\n",
        "print(content_extract)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e475ab265ae4"
      },
      "outputs": [],
      "source": [
        "content_info = scanned_doc[\"usage_info\"]\n",
        "print(content_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d619af71b78"
      },
      "source": [
        "## (Advanced) Combining OCR with a multimodal model\n",
        "\n",
        "In more elaborate scenarios, you may want to annotate the images of a document in addition to retrieving the document's text. This is made possible by adding a multimodal model such as `mistral-small-2503` to the mix and have it analyze the image extracted from the OCR operation, making the overall operation a two-step process which takes longer but yields more information on the document's content.\n",
        "\n",
        "To make the code more modular you can start by packaging the OCR call into a `call_ocr_model()` function that:\n",
        "- downloads the document,\n",
        "- converts it into a base64-encoded string,\n",
        "- passes that string to the OCR model. We added a `pages` argument to optionally limit the number of pages processed, to shorten processing time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5290a17ed1ca"
      },
      "outputs": [],
      "source": [
        "from typing import Any, Dict, Optional\n",
        "\n",
        "\n",
        "def call_ocr_model(\n",
        "    endpoint_url: str,\n",
        "    pdf_url: str,\n",
        "    with_image_outputs: bool = False,\n",
        "    pages: Optional[str] = None,\n",
        "    access_token: Optional[str] = None,\n",
        ") -> Dict[str, Any]:\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Accept\": \"application/json\",\n",
        "    }\n",
        "    if access_token:\n",
        "        headers[\"Authorization\"] = f\"Bearer {access_token}\"\n",
        "    encoded_doc = download_pdf_and_base64_encode(pdf_url)\n",
        "    payload = {\n",
        "        \"model\": f\"{MODEL_NAME}-{MODEL_VERSION}\",\n",
        "        \"document\": {\n",
        "            \"type\": \"document_url\",\n",
        "            \"document_url\": f\"data:application/pdf;base64,{encoded_doc}\",\n",
        "        },\n",
        "        \"include_image_base64\": with_image_outputs,\n",
        "    }\n",
        "    if pages:\n",
        "        payload[\"pages\"] = pages\n",
        "    with httpx.Client() as client:\n",
        "        ocr_resp = client.post(\n",
        "            url=endpoint_url, headers=headers, json=payload, timeout=3600\n",
        "        )\n",
        "        ocr_resp.raise_for_status()\n",
        "        if ocr_resp.status_code == 200:\n",
        "            return ocr_resp.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1b7b5c79a55"
      },
      "source": [
        "To illustrate how the system works, you can scan the document pointed at by the `TEST_DOC_URL` URL as such:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49138e10d101"
      },
      "outputs": [],
      "source": [
        "scanned_doc = call_ocr_model(\n",
        "    endpoint_url=url,\n",
        "    pdf_url=TEST_DOC_URL,\n",
        "    pages=\"1-10\",\n",
        "    with_image_outputs=True,\n",
        "    access_token=access_token,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0e81cf53e9c"
      },
      "source": [
        "Note that `with_image_outputs` is set to `True` because you will want to annotate the figures/images in the documents, so you need to retrieve their base64-encoded representation.\n",
        "\n",
        "The next step is to define how the multimodal model will be called to analyze the image content. To do so, you will query another Mistral model available on Vertex AI: `mistral-small-2503`, which can process both text and image inputs. This is a basic system message you can pass to it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46a275c654bc"
      },
      "outputs": [],
      "source": [
        "VLM_NAME = \"mistral-small\"\n",
        "VLM_VERSION = \"2503\"\n",
        "\n",
        "ANNOTATION_SYSTEM_PROMPT = \"\"\"\n",
        "Your mission is to provide a clear description to each image you will see.\n",
        "Describe its features and key components.\n",
        "Return your answer in a well-structured JSON object.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "287daf8c15ae"
      },
      "source": [
        "To ensure that the image annotation will stick to a specific format, you will leverage another feature called _structured outputs_, which enforces strict schema rules when you require JSON output from the model. In practice, you can define your output structure with a Pydantic model, then later convert it into a JSON Schema dictionary when passing it to the API, here is an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25ae77088850"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class AnnotatedImage(BaseModel):\n",
        "    short_desc: str = Field(\n",
        "        ..., description=\"A short one-sentence summary of the image\"\n",
        "    )\n",
        "    long_desc: str = Field(\n",
        "        ..., description=\"A longer detailed description of the image\"\n",
        "    )\n",
        "\n",
        "\n",
        "schema = AnnotatedImage.model_json_schema()\n",
        "print(json.dumps(schema, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c7cec2f02ff"
      },
      "source": [
        "You can read more about structured outputs in the [Mistral documentation](https://docs.mistral.ai/capabilities/structured-output/custom_structured_output).\n",
        "\n",
        "From there, you can write an `annotate_with_vlm_model()` function that will call the multimodal model and pass it a base64-encoded image to retrieve its description in a well-structured format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2729970e01df"
      },
      "outputs": [],
      "source": [
        "def annotate_with_vlm_model(\n",
        "    endpoint_url: str,\n",
        "    annotation_structure: BaseModel,\n",
        "    image_base64: str,\n",
        "    access_token: Optional[str] = None,\n",
        "    debug: bool = False,\n",
        ") -> Dict[str, Any]:\n",
        "\n",
        "    # Headers\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Accept\": \"application/json\",\n",
        "    }\n",
        "    if access_token:  # Non-Colab environments only\n",
        "        headers[\"Authorization\"] = f\"Bearer {access_token}\"\n",
        "\n",
        "    # JSON output schema\n",
        "    annotation_schema = annotation_structure.model_json_schema()\n",
        "    annotation_schema[\"additionalProperties\"] = False\n",
        "    payload = {\n",
        "        \"model\": f\"{VLM_NAME}-{VLM_VERSION}\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": ANNOTATION_SYSTEM_PROMPT},\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [{\"type\": \"image_url\", \"image_url\": image_base64}],\n",
        "            },\n",
        "        ],\n",
        "        \"response_format\": {\n",
        "            \"type\": \"json_schema\",\n",
        "            \"json_schema\": {\n",
        "                \"schema\": annotation_schema,\n",
        "                \"name\": \"image_schema\",\n",
        "                \"strict\": True,\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "\n",
        "    # Request & response\n",
        "    with httpx.Client() as client:\n",
        "        vlm_resp = client.post(\n",
        "            url=endpoint_url, headers=headers, json=payload, timeout=3600\n",
        "        )\n",
        "        vlm_resp.raise_for_status()\n",
        "        if vlm_resp.status_code == 200:\n",
        "            vlm_out = vlm_resp.json()\n",
        "            annotation = json.loads(vlm_out[\"choices\"][0][\"message\"][\"content\"])\n",
        "        return annotation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eee496c724f1"
      },
      "source": [
        "You can test your function on a page of the scanned document that contains one or more images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb5b24399943"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "page_idx = 0\n",
        "images_b64 = [item[\"image_base64\"] for item in scanned_doc[\"pages\"][page_idx][\"images\"]]\n",
        "vlm_url = build_endpoint_url(\n",
        "    model_name=VLM_NAME, model_version=VLM_VERSION, project_id=PROJECT_ID, region=REGION\n",
        ")\n",
        "annotations: List[Dict[str, Any]] = []\n",
        "for imgb64 in images_b64:\n",
        "    annotations.append(\n",
        "        annotate_with_vlm_model(\n",
        "            endpoint_url=vlm_url,\n",
        "            annotation_structure=AnnotatedImage,\n",
        "            image_base64=imgb64,\n",
        "            access_token=access_token,\n",
        "        )\n",
        "    )\n",
        "print(f\"Page {page_idx}:\")\n",
        "print(annotations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8136937e22ce"
      },
      "source": [
        "Finally, in order to stict everything together, you can run the following code that will edit in-place the `scanned_doc` variable and add a `annotations` field where each detected image will have its description written:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b75a52c7b35"
      },
      "outputs": [],
      "source": [
        "for idx, page in enumerate(scanned_doc[\"pages\"]):\n",
        "    annotations: List[Dict[str, Any]] = []\n",
        "    for img in page[\"images\"]:\n",
        "        annotations.append(\n",
        "            {\n",
        "                \"id\": img[\"id\"],\n",
        "                \"annotation\": annotate_with_vlm_model(\n",
        "                    endpoint_url=vlm_url,\n",
        "                    annotation_structure=AnnotatedImage,\n",
        "                    image_base64=img[\"image_base64\"],\n",
        "                    access_token=access_token,\n",
        "                ),\n",
        "            }\n",
        "        )\n",
        "    scanned_doc[\"pages\"][idx][\"annotations\"] = annotations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae5bcfff22cc"
      },
      "source": [
        "You can now retrieve both text and image descriptions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "238a4eaef404"
      },
      "outputs": [],
      "source": [
        "page_idx = 6\n",
        "text = scanned_doc[\"pages\"][page_idx][\"markdown\"]\n",
        "annotations = scanned_doc[\"pages\"][page_idx][\"annotations\"]\n",
        "print(text)\n",
        "print(80 * \"_\")\n",
        "print(annotations)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "mistralai_ocr.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

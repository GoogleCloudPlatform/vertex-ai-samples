{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Using the Vertex SDK with Vertex AI Large Language Models\n",
        "\n",
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/generative_ai/vertex_sdk_llm_snippets.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/generative_ai/vertex_sdk_llm_snippets.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/generative_ai/vertex_sdk_llm_snippets.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>                                                                                               \n",
        "</table>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**_NOTE_**: This notebook has been tested in the following environment:\n",
        "\n",
        "* Python version = 3.9"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial demonstrates how to use the Vertex AI SDK to run Large Language Models on Vertex AI via the PaLM API. You will find sample code to test, tune, and deploy generative AI language models. Get started by exploring examples of content summarization, sentiment snalysis, and chat, as well as text embedding and prompt tuning."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to provide text input to Large Language Models (LLMs) available on Vertex AI to test, tune, and deploy generative AI language models.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services and resources:\n",
        "\n",
        "- Vertex AI PaLM API\n",
        "- Vertex AI SDK\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Use the predict endpoints of Vertex AI PaLM API receive generative AI responses to a message.\n",
        "- Use the text embedding endpoint to receive a vector representation of a message.\n",
        "- Perform prompt tuning of an LLM, based on input/output training data."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Costs \n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation\n",
        "\n",
        "Install the following packages required to execute this notebook. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install the packages\n",
        "! pip3 install --upgrade --quiet google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Before you begin\n",
        "\n",
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "3. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). {TODO: Update the APIs needed for your tutorial. Edit the API names, and update the link to append the API IDs, separating each one with a comma. For example, container.googleapis.com,cloudbuild.googleapis.com}\n",
        "\n",
        "4. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Before you begin\n",
        "\n",
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "3. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). {TODO: Update the APIs needed for your tutorial. Edit the API names, and update the link to append the API IDs, separating each one with a comma. For example, container.googleapis.com,cloudbuild.googleapis.com}\n",
        "\n",
        "4. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOabLACbseoE"
      },
      "source": [
        "# Summarization examples: Transcript summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmnFXfnhJ9K-"
      },
      "outputs": [],
      "source": [
        "from vertexai.preview.language_models import TextGenerationModel\n",
        "\n",
        "\n",
        "def text_summarization_example(temperature=.2):\n",
        "    \"\"\"Summarization Example with a Large Language Model\"\"\"\n",
        "    model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "    response = model.predict(\n",
        "        '''Provide a summary with about two sentences for the following article:\n",
        "The efficient-market hypothesis (EMH) is a hypothesis in financial \\\n",
        "economics that states that asset prices reflect all available \\\n",
        "information. A direct implication is that it is impossible to \\\n",
        "\"beat the market\" consistently on a risk-adjusted basis since market \\\n",
        "prices should only react to new information. Because the EMH is \\\n",
        "formulated in terms of risk adjustment, it only makes testable \\\n",
        "predictions when coupled with a particular model of risk. As a \\\n",
        "result, research in financial economics since at least the 1990s has \\\n",
        "focused on market anomalies, that is, deviations from specific \\\n",
        "models of risk. The idea that financial market returns are difficult \\\n",
        "to predict goes back to Bachelier, Mandelbrot, and Samuelson, but \\\n",
        "is closely associated with Eugene Fama, in part due to his \\\n",
        "influential 1970 review of the theoretical and empirical research. \\\n",
        "The EMH provides the basic logic for modern risk-based theories of \\\n",
        "asset prices, and frameworks such as consumption-based asset pricing \\\n",
        "and intermediary asset pricing can be thought of as the combination \\\n",
        "of a model of risk with the EMH. Many decades of empirical research \\\n",
        "on return predictability has found mixed evidence. Research in the \\\n",
        "1950s and 1960s often found a lack of predictability (e.g. Ball and \\\n",
        "Brown 1968; Fama, Fisher, Jensen, and Roll 1969), yet the \\\n",
        "1980s-2000s saw an explosion of discovered return predictors (e.g. \\\n",
        "Rosenberg, Reid, and Lanstein 1985; Campbell and Shiller 1988; \\\n",
        "Jegadeesh and Titman 1993). Since the 2010s, studies have often \\\n",
        "found that return predictability has become more elusive, as \\\n",
        "predictability fails to work out-of-sample (Goyal and Welch 2008), \\\n",
        "or has been weakened by advances in trading technology and investor \\\n",
        "learning (Chordia, Subrahmanyam, and Tong 2014; McLean and Pontiff \\\n",
        "2016; Martineau 2021).\n",
        "Summary:\n",
        "''',\n",
        "        temperature=temperature,\n",
        "        max_output_tokens=256,\n",
        "        top_k=40,\n",
        "        top_p=.95,\n",
        "    )\n",
        "    print(f\"Response from Model: {response.text}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdxJX2dNE7t3"
      },
      "source": [
        "# Classification examples: Classification headline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2fw1_3H-tjX"
      },
      "outputs": [],
      "source": [
        "from vertexai.preview.language_models import TextGenerationModel\n",
        "\n",
        "\n",
        "def classify_news_items_example(temperature=0):\n",
        "    \"\"\"Text Classification Example with a Large Language Model\"\"\"\n",
        "    model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "    response = model.predict(\n",
        "      '''What is the topic for a given news headline?\n",
        "- business\n",
        "- entertainment\n",
        "- health\n",
        "- sports\n",
        "- technology\n",
        "\n",
        "Text: Pixel 7 Pro Expert Hands On Review, the Most Helpful Google Phones.\n",
        "The answer is: technology\n",
        "\n",
        "Text: Quit smoking?\n",
        "The answer is: health\n",
        "\n",
        "Text: Roger Federer reveals why he touched Rafael Nadals hand while they were crying\n",
        "The answer is: sports\n",
        "\n",
        "Text: Business relief from Arizona minimum-wage hike looking more remote\n",
        "The answer is: business\n",
        "\n",
        "Text: #TomCruise has arrived in Bari, Italy for #MissionImpossible.\n",
        "The answer is: entertainment\n",
        "\n",
        "Text: CNBC Reports Rising Digital Profit as Print Advertising Falls\n",
        "The answer is:\n",
        "''',\n",
        "      temperature=temperature,\n",
        "      max_output_tokens=5,\n",
        "      top_k=1,\n",
        "      top_p=0,\n",
        "    )\n",
        "    print(f\"Response from Model: {response.text}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKsuhKli70q-"
      },
      "source": [
        "# Classification examples: Sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipqROPufuhDr"
      },
      "outputs": [],
      "source": [
        "from vertexai.preview.language_models import TextGenerationModel\n",
        "\n",
        "\n",
        "def sentiment_analysis_example(temperature=0):\n",
        "    \"\"\"Sentiment analysis example with a Large Language Model.\"\"\"\n",
        "    model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "    response = model.predict(\n",
        "      '''I had to compare two versions of Hamlet for my Shakespeare class and \\\n",
        "unfortunately I picked this version. Everything from the acting (the actors \\\n",
        "deliver most of their lines directly to the camera) to the camera shots (all \\\n",
        "medium or close up shots...no scenery shots and very little back ground in the \\\n",
        "shots) were absolutely terrible. I watched this over my spring break and it is \\\n",
        "very safe to say that I feel that I was gypped out of 114 minutes of my \\\n",
        "vacation. Not recommended by any stretch of the imagination.\n",
        "Classify the sentiment of the message: negative\n",
        "\n",
        "Something surprised me about this movie - it was actually original. It was not \\\n",
        "the same old recycled crap that comes out of Hollywood every month. I saw this \\\n",
        "movie on video because I did not even know about it before I saw it at my \\\n",
        "local video store. If you see this movie available - rent it - you will not \\\n",
        "regret it.\n",
        "Classify the sentiment of the message: positive\n",
        "\n",
        "My family has watched Arthur Bach stumble and stammer since the movie first \\\n",
        "came out. We have most lines memorized. I watched it two weeks ago and still \\\n",
        "get tickled at the simple humor and view-at-life that Dudley Moore portrays. \\\n",
        "Liza Minelli did a wonderful job as the side kick - though I\\'m not her \\\n",
        "biggest fan. This movie makes me just enjoy watching movies. My favorite scene \\\n",
        "is when Arthur is visiting his fiancée\\'s house. His conversation with the \\\n",
        "butler and Susan\\'s father is side-spitting. The line from the butler, \\\n",
        "\"Would you care to wait in the Library\" followed by Arthur\\'s reply, \\\n",
        "\"Yes I would, the bathroom is out of the question\", is my NEWMAIL \\\n",
        "notification on my computer.\n",
        "Classify the sentiment of the message: positive\n",
        "\n",
        "This Charles outing is decent but this is a pretty low-key performance. Marlon \\\n",
        "Brando stands out. There\\'s a subplot with Mira Sorvino and Donald Sutherland \\\n",
        "that forgets to develop and it hurts the film a little. I\\'m still trying to \\\n",
        "figure out why Charlie want to change his name.\n",
        "Classify the sentiment of the message: negative\n",
        "\n",
        "Tweet: The Pixel 7 Pro, is too big to fit in my jeans pocket, so I bought \\\n",
        "new jeans.\n",
        "Classify the sentiment of the message: ''',\n",
        "      max_output_tokens=5,\n",
        "      temperature=temperature,\n",
        "      top_k=1,\n",
        "      top_p=0,\n",
        "    )\n",
        "    print(f\"Response from Model: {response.text}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh0tLRggE5H1"
      },
      "source": [
        "# Extraction examples: Extractive Question Answering\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPqZ38QDoBeh"
      },
      "outputs": [],
      "source": [
        "from vertexai.preview.language_models import TextGenerationModel\n",
        "\n",
        "\n",
        "def extractive_question_answering_example(temperature=0):\n",
        "    \"\"\"Extractive Question Answering with a Large Language Model.\"\"\"\n",
        "    model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "    response = model.predict(\n",
        "      '''Background: There is evidence that there have been significant changes \\\n",
        "in Amazon rainforest vegetation over the last 21,000 years through the Last \\\n",
        "Glacial Maximum (LGM) and subsequent deglaciation. Analyses of sediment \\\n",
        "deposits from Amazon basin paleo lakes and from the Amazon Fan indicate that \\\n",
        "rainfall in the basin during the LGM was lower than for the present, and this \\\n",
        "was almost certainly associated with reduced moist tropical vegetation cover \\\n",
        "in the basin. There is debate, however, over how extensive this reduction \\\n",
        "was. Some scientists argue that the rainforest was reduced to small, isolated \\\n",
        "refugia separated by open forest and grassland; other scientists argue that \\\n",
        "the rainforest remained largely intact but extended less far to the north, \\\n",
        "south, and east than is seen today. This debate has proved difficult to \\\n",
        "resolve because the practical limitations of working in the rainforest mean \\\n",
        "that data sampling is biased away from the center of the Amazon basin, and \\\n",
        "both explanations are reasonably well supported by the available data.\n",
        "\n",
        "Q: What does LGM stands for?\n",
        "A: Last Glacial Maximum.\n",
        "\n",
        "Q: What did the analysis from the sediment deposits indicate?\n",
        "A: Rainfall in the basin during the LGM was lower than for the present.\n",
        "\n",
        "Q: What are some of scientists arguments?\n",
        "A: The rainforest was reduced to small, isolated refugia separated by open forest and grassland.\n",
        "\n",
        "Q: There have been major changes in Amazon rainforest vegetation over the last how many years?\n",
        "A: 21,000.\n",
        "\n",
        "Q: What caused changes in the Amazon rainforest vegetation?\n",
        "A: The Last Glacial Maximum (LGM) and subsequent deglaciation\n",
        "\n",
        "Q: What has been analyzed to compare Amazon rainfall in the past and present?\n",
        "A: Sediment deposits.\n",
        "\n",
        "Q: What has the lower rainfall in the Amazon during the LGM been attributed to?\n",
        "A:''',\n",
        "      temperature=temperature,\n",
        "      max_output_tokens=256,\n",
        "      top_k=1,\n",
        "      top_p=0,\n",
        "    )\n",
        "    print(f\"Response from Model: {response.text}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV08j9H-Rbr7"
      },
      "source": [
        "# Ideation examples: Interview questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofBsWYxt_cl3"
      },
      "outputs": [],
      "source": [
        "from vertexai.preview.language_models import TextGenerationModel\n",
        "\n",
        "\n",
        "def interview_example(temperature=.2):\n",
        "    \"\"\"Ideation example with a Large Language Model\"\"\"\n",
        "    model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "    response = model.predict(\n",
        "        'Give me ten interview questions for the role of program manager.',\n",
        "        temperature=temperature,\n",
        "        max_output_tokens=256,\n",
        "        top_k=40,\n",
        "        top_p=.8,\n",
        "    )\n",
        "    print(f\"Response from Model: {response.text}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpeXOD_bsCAX"
      },
      "source": [
        "# Chat examples: Science tutoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1my0ebNpsKGT",
        "outputId": "ed564737-f853-473f-e312-9d4f420cf7e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The names of the moons of Pluto were chosen by the discoverers of the moons. Charon was named after the ferryman who carried the souls of the dead across the River Styx in Greek mythology. Nix and Hydra were named after the goddess of the night and the nine-headed serpent, respectively, in Greek mythology. Kerberos and Styx were named after the three-headed dog and the river of the underworld, respectively, in Greek mythology.\r\n",
            "\r\n",
            "The names of the moons of Eris were chosen by the discoverers of the dwarf planet. Dysnomia was named after the daughter of Eris in Greek mythology.\r\n",
            "\r\n",
            "The names of the moons of Haumea were chosen by the discoverers of the dwarf planet. Hiʻiaka was named after the daughter of Haumea in Hawaiian mythology. Namaka was named after the goddess of the sea in Hawaiian mythology.\r\n",
            "\r\n",
            "The names of the moons of dwarf planets are often chosen to reflect the mythology or culture of the discoverers.\n"
          ]
        }
      ],
      "source": [
        "from vertexai.preview.language_models import ChatModel, InputOutputTextPair\n",
        "\n",
        "def science_tutoring_chat(temperature=.2):\n",
        "    \"\"\"Chat example with a Large Language Model\"\"\"\n",
        "    chat_model = ChatModel.from_pretrained(\"chat-bison@001\")\n",
        "    parameters = {\n",
        "      \"temperature\": 0.2,\n",
        "      \"max_output_tokens\": 256,\n",
        "      \"top_p\": 0.95,\n",
        "      \"top_k\": 40,\n",
        "    }\n",
        "\n",
        "    chat = chat_model.start_chat(\n",
        "        context=\"My name is Miles. You are an astronomer, knowledgeable about the solar system.\",\n",
        "        examples=[\n",
        "            InputOutputTextPair(\n",
        "                input_text='How many moons does Mars have?',\n",
        "                output_text='The planet Mars has two moons, Phobos and Deimos.',\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    response = chat.send_message(\"How many planets are there in the solar system?\", **parameters)\n",
        "    response = chat.send_message(\"When I learned about the planets in school, there were nine. When did that change?\", **parameters)\n",
        "    response = chat.send_message(\"Does Pluto have any moons? What about other dwarf planets?\", **parameters)\n",
        "    response = chat.send_message(\"Who chose all of these cool names?!\", **parameters)\n",
        "    print(response.text)\n",
        "\n",
        "science_tutoring_chat()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ag5Og6Y3xfvM"
      },
      "source": [
        "# Text Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNstE8FOxmm2"
      },
      "outputs": [],
      "source": [
        "from vertexai.preview.language_models import TextEmbeddingModel\n",
        "\n",
        "\n",
        "def text_embedding_example():\n",
        "  \"\"\"Text embedding with a Large Language Model.\"\"\"\n",
        "  model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")\n",
        "  embeddings = model.get_embeddings([\"What is life?\"])\n",
        "  for embedding in embeddings:\n",
        "      vector = embedding.values\n",
        "      print(f'Length of Embedding Vector: {len(vector)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CI-dpBTr6LGH"
      },
      "source": [
        "# List tuned models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-MuqTjTA0zk"
      },
      "outputs": [],
      "source": [
        "import vertexai\n",
        "from vertexai.preview.language_models import TextGenerationModel\n",
        "\n",
        "\n",
        "def list_tuned_models(project_id, location):\n",
        "  \"\"\"List tuned models.\"\"\"\n",
        "  vertexai.init(project=project_id, location=location)\n",
        "  model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "  tuned_model_names = model.list_tuned_model_names()\n",
        "  print(tuned_model_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYvbJ9PrFF2g"
      },
      "source": [
        "# Tune a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoEVAfGCFOnT"
      },
      "outputs": [],
      "source": [
        "from typing import Union\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import vertexai\n",
        "from vertexai.preview.language_models import TextGenerationModel\n",
        "\n",
        "\n",
        "def tune_model(\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    training_data: Union[pd.DataFrame, str],\n",
        "    train_steps: int = 10,\n",
        "):\n",
        "  \"\"\"Tune a new model, based on a prompt-response data.\n",
        "\n",
        "  \"training_data\" can be either the GCS URI of a file formatted in JSONL format\n",
        "  (for example: training_data=f'gs://{bucket}/{filename}.jsonl'), or a pandas\n",
        "  DataFrame. Each training example should be JSONL record with two keys, for\n",
        "  example:\n",
        "    {\n",
        "      \"input_text\": <input prompt>,\n",
        "      \"output_text\": <associated output>\n",
        "    },\n",
        "  or the pandas DataFame should contain two columns:\n",
        "    ['input_text', 'output_text']\n",
        "  with rows for each training example.\n",
        "\n",
        "  Args:\n",
        "    project_id: GCP Project ID, used to initialize aiplatform\n",
        "    location: GCP Region, used to initialize aiplatform\n",
        "    training_data: GCS URI of training file or pandas dataframe of training data\n",
        "    train_steps: Number of training steps to use when tuning the model.\n",
        "  \"\"\"\n",
        "  vertexai.init(project=project_id, location=location)\n",
        "  model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "\n",
        "  model.tune_model(\n",
        "      training_data=training_data,\n",
        "      train_steps=train_steps,\n",
        "      tuning_job_location=\"europe-west4\",\n",
        "      tuned_model_location=\"us-central1\",\n",
        "  )\n",
        "\n",
        "  # Test the tuned model:\n",
        "  print(\n",
        "      model.predict(\"Tell me some ideas combining VR and fitness:\")\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaDO8x3bfuoP",
        "outputId": "7a23f28b-2818-4499-e99b-434c87c2ee58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The names of the dwarf planets and their moons are chosen by the discoverers. In some cases, the discoverers have chosen to name the objects after mythological figures or characters from literature. For example, Pluto is named after the Roman god of the underworld, and Charon is named after the ferryman who transported souls across the River Styx to the underworld. In other cases, the discoverers have chosen to name the objects after places or people that are significant to them. For example, Eris is named after the Greek goddess of strife, and Makemake is named after the creator god of the Rapa Nui people.\r\n",
            "\r\n",
            "The names of the dwarf planets and their moons are important because they help us to remember and learn about these fascinating objects. They also help us to understand the history of their discovery and the cultures that have been inspired by them.\n"
          ]
        }
      ],
      "source": [
        "science_tutoring_chat()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbGh-7nxf_yT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

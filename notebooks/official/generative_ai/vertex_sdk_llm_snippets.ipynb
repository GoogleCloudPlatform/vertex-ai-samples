{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d9bbf86da5e"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a8301bf64d4"
      },
      "source": [
        "# Using the Vertex AI SDK with Large Language Models\n",
        "\n",
        "<table align=\"left\">\n",
        "\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/generative_ai/vertex_sdk_llm_snippets.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fblob%2Fmain%2Fnotebooks%2Fofficial%2Fgenerative_ai%2Fvertex_sdk_llm_snippets.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/generative_ai/vertex_sdk_llm_snippets.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/generative_ai/vertex_sdk_llm_snippets.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>                                                                                               \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5d108e0f262"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial demonstrates how to use the Vertex AI SDK to run Large Language Models on Vertex AI via the PaLM API. You find sample code to test, tune, and deploy generative AI language models. Get started by exploring examples of content summarization, sentiment snalysis, and chat, as well as text embedding and prompt tuning. \n",
        "\n",
        "Learn more about [PaLM API](https://ai.google/discover/palm2/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "825a6b6ffb84"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to provide text input to Large Language Models (LLMs) available on Vertex AI to test, tune, and deploy generative AI language models.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services and resources:\n",
        "\n",
        "- Vertex AI PaLM API\n",
        "- Vertex AI SDK\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Use the predict endpoints of Vertex AI PaLM API to receive generative AI responses to a message.\n",
        "- Use the text embedding endpoint to receive a vector representation of a message.\n",
        "- Perform prompt tuning of an LLM, based on input/output training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "946810630936"
      },
      "source": [
        "### Costs \n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8925ff9e165e"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e150bf471f1b"
      },
      "source": [
        "### Install Vertex AI SDK and other required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a85627de3636"
      },
      "outputs": [],
      "source": [
        "# Install the packages\n",
        "! pip3 install --upgrade --quiet google-cloud-aiplatform \"shapely<2.0.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d98bc9fdd80d"
      },
      "source": [
        "### Restart runtime (Colab only)\n",
        "\n",
        "To use the newly installed packages, you must restart the runtime on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dbf29389c65"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b49231643e4"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7176ea64999b"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "Authenticate your environment on Google Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7de6ef0fac42"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e487ec618b5e"
      },
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI SDK\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e61aaa036444"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type: \"string\"}\n",
        "\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acedaa0065d1"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9865d6d81fa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from vertexai.preview.language_models import (ChatModel, InputOutputTextPair,\n",
        "                                              TextEmbeddingModel,\n",
        "                                              TextGenerationModel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOabLACbseoE"
      },
      "source": [
        "### Summarization examples: transcript summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmnFXfnhJ9K-"
      },
      "outputs": [],
      "source": [
        "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "response = model.predict(\n",
        "    \"\"\"Provide a summary with about two sentences for the following article:\n",
        "The efficient-market hypothesis (EMH) is a hypothesis in financial \\\n",
        "economics that states that asset prices reflect all available \\\n",
        "information. A direct implication is that it is impossible to \\\n",
        "\"beat the market\" consistently on a risk-adjusted basis since market \\\n",
        "prices should only react to new information. Because the EMH is \\\n",
        "formulated in terms of risk adjustment, it only makes testable \\\n",
        "predictions when coupled with a particular model of risk. As a \\\n",
        "result, research in financial economics since at least the 1990s has \\\n",
        "focused on market anomalies, that is, deviations from specific \\\n",
        "models of risk. The idea that financial market returns are difficult \\\n",
        "to predict goes back to Bachelier, Mandelbrot, and Samuelson, but \\\n",
        "is closely associated with Eugene Fama, in part due to his \\\n",
        "influential 1970 review of the theoretical and empirical research. \\\n",
        "The EMH provides the basic logic for modern risk-based theories of \\\n",
        "asset prices, and frameworks such as consumption-based asset pricing \\\n",
        "and intermediary asset pricing can be thought of as the combination \\\n",
        "of a model of risk with the EMH. Many decades of empirical research \\\n",
        "on return predictability has found mixed evidence. Research in the \\\n",
        "1950s and 1960s often found a lack of predictability (e.g. Ball and \\\n",
        "Brown 1968; Fama, Fisher, Jensen, and Roll 1969), yet the \\\n",
        "1980s-2000s saw an explosion of discovered return predictors (e.g. \\\n",
        "Rosenberg, Reid, and Lanstein 1985; Campbell and Shiller 1988; \\\n",
        "Jegadeesh and Titman 1993). Since the 2010s, studies have often \\\n",
        "found that return predictability has become more elusive, as \\\n",
        "predictability fails to work out-of-sample (Goyal and Welch 2008), \\\n",
        "or has been weakened by advances in trading technology and investor \\\n",
        "learning (Chordia, Subrahmanyam, and Tong 2014; McLean and Pontiff \\\n",
        "2016; Martineau 2021).\n",
        "Summary:\n",
        "\"\"\",\n",
        "    temperature=0.2,\n",
        "    max_output_tokens=256,\n",
        "    top_k=40,\n",
        "    top_p=0.95,\n",
        ")\n",
        "print(f\"Response from Model: {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdxJX2dNE7t3"
      },
      "source": [
        "### Classification examples: classification headline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2fw1_3H-tjX"
      },
      "outputs": [],
      "source": [
        "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "response = model.predict(\n",
        "    \"\"\"What is the topic for a given news headline?\n",
        "- business\n",
        "- entertainment\n",
        "- health\n",
        "- sports\n",
        "- technology\n",
        "\n",
        "Text: Pixel 7 Pro Expert Hands On Review, the Most Helpful Google Phones.\n",
        "The answer is: technology\n",
        "\n",
        "Text: Quit smoking?\n",
        "The answer is: health\n",
        "\n",
        "Text: Roger Federer reveals why he touched Rafael Nadals hand while they were crying\n",
        "The answer is: sports\n",
        "\n",
        "Text: Business relief from Arizona minimum-wage hike looking more remote\n",
        "The answer is: business\n",
        "\n",
        "Text: #TomCruise has arrived in Bari, Italy for #MissionImpossible.\n",
        "The answer is: entertainment\n",
        "\n",
        "Text: CNBC Reports Rising Digital Profit as Print Advertising Falls\n",
        "The answer is:\n",
        "\"\"\",\n",
        "    temperature=0.2,\n",
        "    max_output_tokens=5,\n",
        "    top_k=1,\n",
        "    top_p=0,\n",
        ")\n",
        "print(f\"Response from Model: {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKsuhKli70q-"
      },
      "source": [
        "### Classification examples: sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipqROPufuhDr"
      },
      "outputs": [],
      "source": [
        "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "response = model.predict(\n",
        "    \"\"\"I had to compare two versions of Hamlet for my Shakespeare class and \\\n",
        "unfortunately I picked this version. Everything from the acting (the actors \\\n",
        "deliver most of their lines directly to the camera) to the camera shots (all \\\n",
        "medium or close up shots...no scenery shots and very little back ground in the \\\n",
        "shots) were absolutely terrible. I watched this over my spring break and it is \\\n",
        "very safe to say that I feel that I was gypped out of 114 minutes of my \\\n",
        "vacation. Not recommended by any stretch of the imagination.\n",
        "Classify the sentiment of the message: negative\n",
        "\n",
        "Something surprised me about this movie - it was actually original. It was not \\\n",
        "the same old recycled crap that comes out of Hollywood every month. I saw this \\\n",
        "movie on video because I did not even know about it before I saw it at my \\\n",
        "local video store. If you see this movie available - rent it - you will not \\\n",
        "regret it.\n",
        "Classify the sentiment of the message: positive\n",
        "\n",
        "My family has watched Arthur Bach stumble and stammer since the movie first \\\n",
        "came out. We have most lines memorized. I watched it two weeks ago and still \\\n",
        "get tickled at the simple humor and view-at-life that Dudley Moore portrays. \\\n",
        "Liza Minelli did a wonderful job as the side kick - though I\\'m not her \\\n",
        "biggest fan. This movie makes me just enjoy watching movies. My favorite scene \\\n",
        "is when Arthur is visiting his fiancée\\'s house. His conversation with the \\\n",
        "butler and Susan\\'s father is side-spitting. The line from the butler, \\\n",
        "\"Would you care to wait in the Library\" followed by Arthur\\'s reply, \\\n",
        "\"Yes I would, the bathroom is out of the question\", is my NEWMAIL \\\n",
        "notification on my computer.\n",
        "Classify the sentiment of the message: positive\n",
        "\n",
        "This Charles outing is decent but this is a pretty low-key performance. Marlon \\\n",
        "Brando stands out. There\\'s a subplot with Mira Sorvino and Donald Sutherland \\\n",
        "that forgets to develop and it hurts the film a little. I\\'m still trying to \\\n",
        "figure out why Charlie want to change his name.\n",
        "Classify the sentiment of the message: negative\n",
        "\n",
        "Tweet: The Pixel 7 Pro, is too big to fit in my jeans pocket, so I bought \\\n",
        "new jeans.\n",
        "Classify the sentiment of the message: \"\"\",\n",
        "    max_output_tokens=5,\n",
        "    temperature=0.2,\n",
        "    top_k=1,\n",
        "    top_p=0,\n",
        ")\n",
        "print(f\"Response from Model: {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh0tLRggE5H1"
      },
      "source": [
        "### Extraction examples: extractive question answering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPqZ38QDoBeh"
      },
      "outputs": [],
      "source": [
        "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "response = model.predict(\n",
        "    \"\"\"Background: There is evidence that there have been significant changes \\\n",
        "in Amazon rainforest vegetation over the last 21,000 years through the Last \\\n",
        "Glacial Maximum (LGM) and subsequent deglaciation. Analyses of sediment \\\n",
        "deposits from Amazon basin paleo lakes and from the Amazon Fan indicate that \\\n",
        "rainfall in the basin during the LGM was lower than for the present, and this \\\n",
        "was almost certainly associated with reduced moist tropical vegetation cover \\\n",
        "in the basin. There is debate, however, over how extensive this reduction \\\n",
        "was. Some scientists argue that the rainforest was reduced to small, isolated \\\n",
        "refugia separated by open forest and grassland; other scientists argue that \\\n",
        "the rainforest remained largely intact but extended less far to the north, \\\n",
        "south, and east than is seen today. This debate has proved difficult to \\\n",
        "resolve because the practical limitations of working in the rainforest mean \\\n",
        "that data sampling is biased away from the center of the Amazon basin, and \\\n",
        "both explanations are reasonably well supported by the available data.\n",
        "\n",
        "Q: What does LGM stands for?\n",
        "A: Last Glacial Maximum.\n",
        "\n",
        "Q: What did the analysis from the sediment deposits indicate?\n",
        "A: Rainfall in the basin during the LGM was lower than for the present.\n",
        "\n",
        "Q: What are some of scientists arguments?\n",
        "A: The rainforest was reduced to small, isolated refugia separated by open forest and grassland.\n",
        "\n",
        "Q: There have been major changes in Amazon rainforest vegetation over the last how many years?\n",
        "A: 21,000.\n",
        "\n",
        "Q: What caused changes in the Amazon rainforest vegetation?\n",
        "A: The Last Glacial Maximum (LGM) and subsequent deglaciation\n",
        "\n",
        "Q: What has been analyzed to compare Amazon rainfall in the past and present?\n",
        "A: Sediment deposits.\n",
        "\n",
        "Q: What has the lower rainfall in the Amazon during the LGM been attributed to?\n",
        "A:\"\"\",\n",
        "    temperature=0.2,\n",
        "    max_output_tokens=256,\n",
        "    top_k=1,\n",
        "    top_p=0,\n",
        ")\n",
        "print(f\"Response from Model: {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV08j9H-Rbr7"
      },
      "source": [
        "### Ideation examples: interview questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofBsWYxt_cl3"
      },
      "outputs": [],
      "source": [
        "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "response = model.predict(\n",
        "    \"Give me ten interview questions for the role of program manager.\",\n",
        "    temperature=0.2,\n",
        "    max_output_tokens=256,\n",
        "    top_k=40,\n",
        "    top_p=0.8,\n",
        ")\n",
        "print(f\"Response from Model: {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpeXOD_bsCAX"
      },
      "source": [
        "### Chat examples: science tutoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1my0ebNpsKGT"
      },
      "outputs": [],
      "source": [
        "chat_model = ChatModel.from_pretrained(\"chat-bison@001\")\n",
        "parameters = {\n",
        "    \"temperature\": 0.2,\n",
        "    \"max_output_tokens\": 256,\n",
        "    \"top_p\": 0.95,\n",
        "    \"top_k\": 40,\n",
        "}\n",
        "\n",
        "chat = chat_model.start_chat(\n",
        "    context=\"My name is Miles. You are an astronomer, knowledgeable about the solar system.\",\n",
        "    examples=[\n",
        "        InputOutputTextPair(\n",
        "            input_text=\"How many moons does Mars have?\",\n",
        "            output_text=\"The planet Mars has two moons, Phobos and Deimos.\",\n",
        "        ),\n",
        "    ],\n",
        ")\n",
        "\n",
        "response = chat.send_message(\n",
        "    \"How many planets are there in the solar system?\", **parameters\n",
        ")\n",
        "response = chat.send_message(\n",
        "    \"When I learned about the planets in school, there were nine. When did that change?\",\n",
        "    **parameters,\n",
        ")\n",
        "response = chat.send_message(\n",
        "    \"Does Pluto have any moons? What about other dwarf planets?\", **parameters\n",
        ")\n",
        "response = chat.send_message(\"Who chose all of these cool names?!\", **parameters)\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ag5Og6Y3xfvM"
      },
      "source": [
        "### Text embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNstE8FOxmm2"
      },
      "outputs": [],
      "source": [
        "model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")\n",
        "embeddings = model.get_embeddings([\"What is life?\"])\n",
        "for embedding in embeddings:\n",
        "    vector = embedding.values\n",
        "    print(f\"Length of Embedding Vector: {len(vector)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CI-dpBTr6LGH"
      },
      "source": [
        "### List tuned models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-MuqTjTA0zk"
      },
      "outputs": [],
      "source": [
        "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "tuned_model_names = model.list_tuned_model_names()\n",
        "print(tuned_model_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYvbJ9PrFF2g"
      },
      "source": [
        "### Tune a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoEVAfGCFOnT"
      },
      "outputs": [],
      "source": [
        "def tuning(\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    model_display_name: str,\n",
        "    training_data: pd.DataFrame,\n",
        "    train_steps: int = 10,\n",
        ") -> TextGenerationModel:\n",
        "    \"\"\"Tune a new model, based on a prompt-response data.\n",
        "\n",
        "    \"training_data\" can be either the GCS URI of a file formatted in JSONL format\n",
        "    (for example: training_data=f'gs://{bucket}/{filename}.jsonl'), or a pandas\n",
        "    DataFrame. Each training example should be JSONL record with two keys, for\n",
        "    example:\n",
        "      {\n",
        "        \"input_text\": <input prompt>,\n",
        "        \"output_text\": <associated output>\n",
        "      },\n",
        "    or the pandas DataFame should contain two columns:\n",
        "      ['input_text', 'output_text']\n",
        "    with rows for each training example.\n",
        "\n",
        "    Args:\n",
        "      project_id: GCP Project ID, used to initialize vertexai\n",
        "      location: GCP Region, used to initialize vertexai\n",
        "      model_display_name: Customized Tuned LLM model name.\n",
        "      training_data: GCS URI of jsonl file or pandas dataframe of training data\n",
        "      train_steps: Number of training steps to use when tuning the model.\n",
        "    \"\"\"\n",
        "    vertexai.init(project=project_id, location=location)\n",
        "    model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "\n",
        "    model.tune_model(\n",
        "        training_data=training_data,\n",
        "        # Optional:\n",
        "        model_display_name=model_display_name,\n",
        "        train_steps=train_steps,\n",
        "        tuning_job_location=\"europe-west4\",\n",
        "        tuned_model_location=location,\n",
        "    )\n",
        "\n",
        "    print(model._job.status)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acc05a4e4fe5"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "vertex_sdk_llm_snippets.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

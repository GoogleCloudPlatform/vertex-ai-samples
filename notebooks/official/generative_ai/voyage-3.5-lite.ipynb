{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 MongoDB, Inc\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": "# Voyage 3.5 Lite\n\nThis notebook demonstrates how to deploy and use the Voyage 3.5 Lite embedding model.\n\n<table align=\"left\">\n  <td style=\"text-align: center\">\n    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/generative_ai/voyage-3.5-lite.ipynb\">\n      <img src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n    </a>\n  </td>\n  <td style=\"text-align: center\">\n    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Fgenerative_ai%2Fvoyage-3.5-lite.ipynb\">\n      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n    </a>\n  </td>\n  <td style=\"text-align: center\">\n    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/generative_ai/voyage-3.5-lite.ipynb\">\n      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Workbench\n    </a>\n  </td>\n  <td style=\"text-align: center\">\n    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/generative_ai/voyage-3.5-lite.ipynb\">\n      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n    </a>\n  </td>\n</table>"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "**Voyage 3.5 Lite** is a high-performance text embedding model designed for efficient semantic search, retrieval-augmented generation (RAG), and clustering tasks. This model provides:\n",
        "\n",
        "* **High Quality Embeddings**: State-of-the-art semantic understanding\n",
        "* **Efficient Performance**: Optimized for speed and resource usage\n",
        "* **1024-dimensional vectors**: Compact yet powerful representations\n",
        "* **Maximum 32K tokens input**: Support for long documents\n",
        "\n",
        "### What you'll learn\n",
        "\n",
        "In this notebook, you will:\n",
        "\n",
        "* Deploy the Voyage 3.5 Lite model to a Vertex AI endpoint\n",
        "* Generate embeddings for single and multiple texts\n",
        "* Use the embeddings for semantic similarity tasks\n",
        "* Clean up resources after use\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI Model Garden\n",
        "* Vertex AI Prediction endpoints\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Vertex AI SDK for Python and other required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFy3H3aPgx12"
      },
      "outputs": [],
      "source": [
        "! pip3 install --upgrade --quiet google-cloud-aiplatform numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "### Restart runtime (Colab only)\n",
        "\n",
        "To use the newly installed packages, you must restart the runtime on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XRvKdaPDTznN"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmM4z7FOBpM"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "Authenticate your environment on Google Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI SDK for Python\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": "# @title Setup Google Cloud project\n\n# Set your Google Cloud project ID and region below:\n\nimport os\n\nimport vertexai\n\n# @markdown Enter your project ID if not auto-detected:\nPROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\nif not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n    PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n\n# @markdown Select your region:\nLOCATION = \"us-central1\"  # @param [\"us-central1\", \"us-east1\", \"us-west1\", \"europe-west1\", \"europe-west4\", \"asia-east1\", \"asia-southeast1\"]\n\nprint(f\"Project ID: {PROJECT_ID}\")\nprint(f\"Location: {LOCATION}\")\n\nvertexai.init(project=PROJECT_ID, location=LOCATION)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23acca3ed72b"
      },
      "source": [
        "## Deploy model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdvJRUWRNGHE"
      },
      "source": [
        "### Initialize the Model\n",
        "\n",
        "Initialize the Voyage 3.5 Lite model from Model Garden.\n",
        "\n",
        "Use the `list_deploy_options()` method to view the verified deployment configurations for your selected model. This helps ensure you have sufficient resources (e.g., GPU quota) available to deploy it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7b994ad7bee1"
      },
      "outputs": [],
      "source": [
        "from vertexai import model_garden\n",
        "\n",
        "MODEL_NAME = \"mongodb/voyage-3.5-lite\"\n",
        "model = model_garden.OpenModel(MODEL_NAME)\n",
        "\n",
        "deploy_options = model.list_deploy_options(concise=True)\n",
        "print(deploy_options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddad02227127"
      },
      "source": [
        "### Deploy the Model\n",
        "\n",
        "Now that you've reviewed the deployment options, use the `deploy()` method to serve the Voyage 3.5 Lite model to a Vertex AI endpoint. Deployment time may vary depending on infrastructure requirements.\n",
        "\n",
        "You can either deploy a new model or use an existing endpoint. Set `use_dedicated_endpoint` to False if you don't want to use [dedicated endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment#create-dedicated-endpoint)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c797fed2dd9a"
      },
      "outputs": [],
      "source": [
        "# @title Deploy or connect to endpoint\n",
        "# @markdown Choose whether to deploy a new model or use an existing endpoint:\n",
        "\n",
        "deployment_option = \"deploy_new\"  # @param [\"deploy_new\", \"use_existing\"]\n",
        "use_dedicated_endpoint = True  # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown If using existing endpoint, provide the endpoint ID:\n",
        "ENDPOINT_ID = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "if deployment_option == \"deploy_new\":\n",
        "    print(\"Deploying new model...\")\n",
        "    endpoint = model.deploy(\n",
        "        accept_eula=True,\n",
        "        use_dedicated_endpoint=use_dedicated_endpoint,\n",
        "    )\n",
        "    print(f\"Endpoint deployed: {endpoint.display_name}\")\n",
        "    print(f\"Endpoint resource name: {endpoint.resource_name}\")\n",
        "else:\n",
        "    if not ENDPOINT_ID:\n",
        "        raise ValueError(\"Please provide an ENDPOINT_ID when using existing endpoint\")\n",
        "\n",
        "    from google.cloud import aiplatform\n",
        "\n",
        "    print(f\"Connecting to existing endpoint: {ENDPOINT_ID}\")\n",
        "    endpoint = aiplatform.Endpoint(\n",
        "        endpoint_name=f\"projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/{ENDPOINT_ID}\"\n",
        "    )\n",
        "    print(f\"Using endpoint: {endpoint.display_name}\")\n",
        "    print(f\"Endpoint resource name: {endpoint.resource_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c2a64096489"
      },
      "source": [
        "### Advanced Deployment Configuration (Optional)\n",
        "\n",
        "To further customize your deployment, you can configure:\n",
        "\n",
        "- **Compute Resources**: Machine type, replica count (min/max), accelerator type and quantity.\n",
        "- **Infrastructure**: Use Spot VMs, reservation affinity, or dedicated endpoints.\n",
        "- **Serving Container**: Customize container image, ports, health checks, and environment variables.\n",
        "\n",
        "See the [Model Garden SDK README](https://github.com/googleapis/python-aiplatform/blob/main/vertexai/model_garden/README.md) for advanced configuration options."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w__t1DyldLXl"
      },
      "source": [
        "## Generate embeddings with Voyage 3.5 Lite\n",
        "\n",
        "Now that the model is deployed, you can generate embeddings for your text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJkrCQLSdLXm"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Multiple texts to embed\n",
        "texts = [\n",
        "    \"Machine learning enables computers to learn from data.\",\n",
        "    \"Natural language processing helps computers understand human language.\",\n",
        "    \"Computer vision allows machines to interpret visual information.\",\n",
        "    \"Deep learning uses neural networks with multiple layers.\",\n",
        "]\n",
        "\n",
        "# Prepare the batch request and make invoke call\n",
        "body = {\"input\": texts, \"output_dimension\": 1024, \"input_type\": \"document\"}\n",
        "response = endpoint.invoke(\n",
        "    request_path=\"/embeddings\",\n",
        "    body=json.dumps(body).encode(\"utf-8\"),\n",
        "    headers={\"Content-Type\": \"application/json\"},\n",
        ")\n",
        "\n",
        "# Extract embeddings\n",
        "result = response.json()\n",
        "embeddings = [item[\"embedding\"] for item in result[\"data\"]]\n",
        "\n",
        "print(f\"Number of texts embedded: {len(embeddings)}\")\n",
        "print(f\"Embedding dimension: {len(embeddings[0])}\")\n",
        "print(f\"\\nFirst embedding (first 5 values): {embeddings[0][:5]}\")\n",
        "print(f\"Second embedding (first 5 values): {embeddings[1][:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRSllL4RdLXm"
      },
      "source": [
        "### Semantic similarity\n",
        "\n",
        "Use embeddings to compute semantic similarity between text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njfQ4jBSdLXm"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
        "    vec1 = np.array(vec1)\n",
        "    vec2 = np.array(vec2)\n",
        "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "\n",
        "# Example texts\n",
        "query = \"How do computers learn from examples?\"\n",
        "documents = [\n",
        "    \"Machine learning enables computers to learn from data.\",\n",
        "    \"The weather today is sunny and warm.\",\n",
        "    \"Neural networks can recognize patterns in data.\",\n",
        "    \"I enjoy cooking Italian food.\",\n",
        "]\n",
        "\n",
        "# Get embeddings - using invoke with /embeddings endpoint\n",
        "all_texts = [query] + documents\n",
        "body = {\"input\": all_texts, \"output_dimension\": 1024, \"input_type\": \"document\"}\n",
        "response = endpoint.invoke(\n",
        "    request_path=\"/embeddings\",\n",
        "    body=json.dumps(body).encode(\"utf-8\"),\n",
        "    headers={\"Content-Type\": \"application/json\"},\n",
        ")\n",
        "result = response.json()\n",
        "all_embeddings = [item[\"embedding\"] for item in result[\"data\"]]\n",
        "\n",
        "query_embedding = all_embeddings[0]\n",
        "doc_embeddings = all_embeddings[1:]\n",
        "\n",
        "# Calculate similarities\n",
        "print(f\"Query: {query}\\n\")\n",
        "print(\"Similarity scores:\")\n",
        "for i, doc in enumerate(documents):\n",
        "    similarity = cosine_similarity(query_embedding, doc_embeddings[i])\n",
        "    print(f\"{similarity:.4f} - {doc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d60e2e833ea"
      },
      "source": [
        "### Advanced parameters\n",
        "\n",
        "Let's explore the advanced parameters that Voyage 3.5 Lite supports to optimize your embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c6c1f7d045d"
      },
      "source": [
        "#### Understanding input_type: Query vs Document\n",
        "\n",
        "The `input_type` parameter optimizes embeddings for retrieval tasks:\n",
        "\n",
        "* **`query`**: Use this when the text represents a search query or question. The model prepends \"Represent the query for retrieving supporting documents: \" to optimize for retrieval.\n",
        "* **`document`**: Use this when the text represents a document or passage to be searched. The model prepends \"Represent the document for retrieval: \" to optimize for indexing.\n",
        "* **`null`** (default): No special prompt is added. Use for general-purpose embeddings.\n",
        "\n",
        "**Best Practice**: For retrieval/search applications, use `input_type=\"query\"` for your search queries and `input_type=\"document\"` for the documents you're indexing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aad248deffe2"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Example: Using input_type for retrieval\n",
        "query_text = \"What is machine learning?\"\n",
        "document_texts = [\n",
        "    \"Machine learning enables computers to learn from data.\",\n",
        "    \"Natural language processing helps computers understand human language.\",\n",
        "    \"Computer vision allows machines to interpret visual information.\",\n",
        "]\n",
        "\n",
        "# Generate query embedding with input_type=\"query\"\n",
        "query_body = {\n",
        "    \"input\": [query_text],\n",
        "    \"output_dimension\": 1024,\n",
        "    \"input_type\": \"query\",  # Optimize for search queries\n",
        "}\n",
        "query_response = endpoint.invoke(\n",
        "    request_path=\"/embeddings\",\n",
        "    body=json.dumps(query_body).encode(\"utf-8\"),\n",
        "    headers={\"Content-Type\": \"application/json\"},\n",
        ")\n",
        "query_result = query_response.json()\n",
        "query_embedding = query_result[\"data\"][0][\"embedding\"]\n",
        "\n",
        "# Generate document embeddings with input_type=\"document\"\n",
        "doc_body = {\n",
        "    \"input\": document_texts,\n",
        "    \"output_dimension\": 1024,\n",
        "    \"input_type\": \"document\",  # Optimize for document indexing\n",
        "}\n",
        "doc_response = endpoint.invoke(\n",
        "    request_path=\"/embeddings\",\n",
        "    body=json.dumps(doc_body).encode(\"utf-8\"),\n",
        "    headers={\"Content-Type\": \"application/json\"},\n",
        ")\n",
        "doc_result = doc_response.json()\n",
        "doc_embeddings = [item[\"embedding\"] for item in doc_result[\"data\"]]\n",
        "\n",
        "print(f\"Query: {query_text}\")\n",
        "print(f\"Query embedding dimension: {len(query_embedding)}\")\n",
        "print(f\"\\nNumber of documents embedded: {len(doc_embeddings)}\")\n",
        "print(f\"Document embedding dimension: {len(doc_embeddings[0])}\")\n",
        "print(f\"\\nQuery embedding (first 5 values): {query_embedding[:5]}\")\n",
        "print(f\"First document embedding (first 5 values): {doc_embeddings[0][:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca376ae3706d"
      },
      "source": [
        "#### Using different output dimensions\n",
        "\n",
        "Voyage 3.5 Lite supports multiple output dimensions: 256, 512, 1024 (default), and 2048. Smaller dimensions reduce storage and computation costs, while larger dimensions may provide better accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2ffb067c42b"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "text = \"Machine learning enables computers to learn from data.\"\n",
        "\n",
        "# Test different output dimensions\n",
        "dimensions = [256, 512, 1024, 2048]\n",
        "\n",
        "print(\"Comparing different output dimensions:\\n\")\n",
        "for dim in dimensions:\n",
        "    body = {\"input\": [text], \"output_dimension\": dim, \"input_type\": \"document\"}\n",
        "    response = endpoint.invoke(\n",
        "        request_path=\"/embeddings\",\n",
        "        body=json.dumps(body).encode(\"utf-8\"),\n",
        "        headers={\"Content-Type\": \"application/json\"},\n",
        "    )\n",
        "    result = response.json()\n",
        "    embedding = result[\"data\"][0][\"embedding\"]\n",
        "\n",
        "    print(f\"Dimension {dim}:\")\n",
        "    print(f\"  Length: {len(embedding)}\")\n",
        "    print(f\"  First 5 values: {embedding[:5]}\")\n",
        "    print(f\"  Storage size: ~{len(embedding) * 4} bytes (float32)\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06f976b98112"
      },
      "source": [
        "#### Using different output data types\n",
        "\n",
        "Voyage 3.5 Lite supports multiple output data types to optimize for storage and performance:\n",
        "\n",
        "* **`float`** (default): 32-bit floating-point numbers, highest precision\n",
        "* **`int8`**: 8-bit signed integers (-128 to 127), 4x smaller than float\n",
        "* **`uint8`**: 8-bit unsigned integers (0 to 255), 4x smaller than float\n",
        "* **`binary`**: Bit-packed signed integers (int8), 32x smaller than float\n",
        "* **`ubinary`**: Bit-packed unsigned integers (uint8), 32x smaller than float\n",
        "\n",
        "Quantized formats (int8, uint8, binary, ubinary) trade some precision for significant storage savings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78a7545a01a1"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "text = \"Machine learning enables computers to learn from data.\"\n",
        "\n",
        "# Test different output data types\n",
        "output_dtypes = [\"float\", \"int8\", \"uint8\", \"binary\", \"ubinary\"]\n",
        "\n",
        "print(\"Comparing different output data types:\\n\")\n",
        "for dtype in output_dtypes:\n",
        "    body = {\n",
        "        \"input\": [text],\n",
        "        \"output_dimension\": 1024,\n",
        "        \"output_dtype\": dtype,\n",
        "        \"input_type\": \"document\",\n",
        "    }\n",
        "    response = endpoint.invoke(\n",
        "        request_path=\"/embeddings\",\n",
        "        body=json.dumps(body).encode(\"utf-8\"),\n",
        "        headers={\"Content-Type\": \"application/json\"},\n",
        "    )\n",
        "    result = response.json()\n",
        "    embedding = result[\"data\"][0][\"embedding\"]\n",
        "\n",
        "    # Calculate actual storage size\n",
        "    if dtype == \"float\":\n",
        "        storage_bytes = len(embedding) * 4  # 4 bytes per float32\n",
        "    elif dtype in [\"int8\", \"uint8\"]:\n",
        "        storage_bytes = len(embedding) * 1  # 1 byte per int8/uint8\n",
        "    elif dtype in [\"binary\", \"ubinary\"]:\n",
        "        storage_bytes = len(embedding) * 1  # bit-packed, 1/8 of dimension\n",
        "\n",
        "    print(f\"Output dtype: {dtype}\")\n",
        "    print(f\"  Length: {len(embedding)}\")\n",
        "    print(f\"  Value type: {type(embedding[0]).__name__}\")\n",
        "    print(f\"  First 5 values: {embedding[:5]}\")\n",
        "    print(f\"  Storage size: ~{storage_bytes} bytes\")\n",
        "\n",
        "    # Calculate compression ratio vs float\n",
        "    if dtype != \"float\":\n",
        "        compression_ratio = (1024 * 4) / storage_bytes\n",
        "        print(f\"  Compression: {compression_ratio:.1f}x smaller than float\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ca2bff08f63"
      },
      "source": [
        "#### Combining output_dimension and output_dtype\n",
        "\n",
        "You can combine different dimensions and data types to optimize for your use case.\n",
        "\n",
        "Please refer to our guide for details on [offset binary](https://docs.voyageai.com/docs/flexible-dimensions-and-quantization#offset-binary) and [binary embeddings](https://docs.voyageai.com/docs/flexible-dimensions-and-quantization#quantization). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d0d719659f9"
      },
      "outputs": [],
      "source": "import json\n\ntext = \"Machine learning enables computers to learn from data.\"\n\n# Example: Ultra-compact embeddings (256 dimensions + ubinary)\ncompact_body = {\n    \"input\": [text],\n    \"output_dimension\": 256,\n    \"output_dtype\": \"ubinary\",  # Most compact format\n    \"input_type\": \"document\",\n}\ncompact_response = endpoint.invoke(\n    request_path=\"/embeddings\",\n    body=json.dumps(compact_body).encode(\"utf-8\"),\n    headers={\"Content-Type\": \"application/json\"},\n)\ncompact_result = compact_response.json()\ncompact_embedding = compact_result[\"data\"][0][\"embedding\"]\n\n# Example: High-precision embeddings (2048 dimensions + float)\nprecise_body = {\n    \"input\": [text],\n    \"output_dimension\": 2048,\n    \"output_dtype\": \"float\",  # Highest precision\n    \"input_type\": \"document\",\n}\nprecise_response = endpoint.invoke(\n    request_path=\"/embeddings\",\n    body=json.dumps(precise_body).encode(\"utf-8\"),\n    headers={\"Content-Type\": \"application/json\"},\n)\nprecise_result = precise_response.json()\nprecise_embedding = precise_result[\"data\"][0][\"embedding\"]\n\n# Compare storage requirements\ncompact_storage = len(compact_embedding) * 1  # binary is bit-packed\nprecise_storage = len(precise_embedding) * 4  # float32\n\nprint(\"Storage comparison:\\n\")\nprint(\"Ultra-compact (256-dim ubinary):\")\nprint(\"  Dimension: 256\")\nprint(f\"  Storage: ~{compact_storage} bytes\")\nprint(f\"  First 5 values: {compact_embedding[:5]}\\n\")\n\nprint(\"High-precision (2048-dim float):\")\nprint(f\"  Dimension: {len(precise_embedding)}\")\nprint(f\"  Storage: ~{precise_storage} bytes\")\nprint(f\"  First 5 values: {precise_embedding[:5]}\\n\")\n\nprint(f\"Storage ratio: {precise_storage / compact_storage:.1f}x\")\nprint(\"\\nFor 1 million vectors:\")\nprint(f\"  Ultra-compact: ~{compact_storage * 1_000_000 / (1024**2):.1f} MB\")\nprint(f\"  High-precision: ~{precise_storage * 1_000_000 / (1024**2):.1f} MB\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e378dcf4085"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To avoid incurring charges to your Google Cloud account for the resources used in this tutorial, delete the endpoint and undeploy the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fd9195c876f"
      },
      "outputs": [],
      "source": [
        "# Delete the endpoint (this will also undeploy all models)\n",
        "print(f\"Deleting endpoint: {endpoint.display_name}\")\n",
        "endpoint.delete(force=True)\n",
        "print(\"Endpoint deleted successfully!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "voyage-3.5-lite.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Vertex AI SDK: Using PyTorch torchrun to simplify multi-node training with custom containers\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/training/sdk_pytorch_torchrun_custom_container_training_imagenet.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Ftraining%2Fsdk_pytorch_torchrun_custom_container_training_imagenet.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>   \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/training/sdk_pytorch_torchrun_custom_container_training_imagenet.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br>\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>  \n",
    " <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/training/sdk_pytorch_torchrun_custom_container_training_imagenet.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br>\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c119189aa978"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This tutorial uses the Tiny ImageNet dataset to run multi-node distributed training on Vertex AI with Torchrun. It runs distributed training on multiple nodes with GPUs.\n",
    "\n",
    "Learn more about [Distributed training](https://cloud.google.com/vertex-ai/docs/training/distributed-training)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26f217af913b"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you learn how to train an Imagenet model using PyTorch's Torchrun on multiple nodes.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services:\n",
    "\n",
    "- Vertex AI Training(Custom Python Package Training) \n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "    * Create a shell script to start an ETCD cluster on the master node\n",
    "    * Create a training script using code from PyTorch Elastic's Github repository\n",
    "    * Create containers that download the data, and start an ETCD cluster on the host\n",
    "    * Train the model using multiple nodes with GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6a298ceea8e"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "For the sake of training time, the Tiny ImageNet dataset is used in this tutorial: https://image-net.org/data/tiny-imagenet-200.zip\n",
    "\n",
    "This dataset consists of many small (~2KB) images. To avoid network bottlenecks with the large volume of network transfers from Cloud Storage to the GPUs, download this dataset to the containers\n",
    "\n",
    "The training code is based on this PyTorch Torchrun example for ImageNet: https://github.com/pytorch/elastic/blob/master/examples/imagenet/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "### Costs \n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI Training w/ GPUs\n",
    "* Vertex AI TensorBoard\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "### Install Vertex AI SDK for Python and other required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2b4ef9b72d43",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install the packages\n",
    "! pip3 install --upgrade --quiet google-cloud-aiplatform \\\n",
    "                                 python-etcd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58707a750154"
   },
   "source": [
    "### Restart runtime (Colab only)\n",
    "\n",
    "To use the newly installed packages, you must restart the runtime on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "f200f10a1da3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "Authenticate your environment on Google Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "Create a storage bucket to store intermediate artifacts such as datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MzGDU7TWdts_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "**If your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NIq7R4HZCfIc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://k-bucket-vertexai-service-project-torchrun-custom/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'k-bucket-vertexai-service-project-torchrun-custom' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### Set Google Cloud project information and initialize Vertex AI SDK for Python\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PyQmSRbKA8r-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7a84e2e4c4e"
   },
   "source": [
    "### Service Account\n",
    "\n",
    "You use a service account to create the Vertex AI Training job. If you do not want to use your project's Compute Engine service account, set SERVICE_ACCOUNT to another service account ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "e0c9c4f84849",
    "tags": []
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2d9440bb3017"
   },
   "source": [
    "If you do not provide a service account, run the code below to get the Compute Engine service account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "304a9ea0b6d0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service Account: 314481442207-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your service account from gcloud\n",
    "    if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "        shell_output = !gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "    if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gar_enable_api"
   },
   "source": [
    "### Enable Artifact Registry API\n",
    "\n",
    "First, you must enable the Artifact Registry API service for your project.\n",
    "\n",
    "Learn more about [Enabling service](https://cloud.google.com/artifact-registry/docs/enable-service)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gar_enable_api",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "To take a quick anonymous survey, run:\n",
      "  $ gcloud survey\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! gcloud services enable artifactregistry.googleapis.com\n",
    "\n",
    "if os.getenv(\"IS_TESTING\"):\n",
    "    ! sudo apt-get update --yes && sudo apt-get --only-upgrade --yes install google-cloud-sdk-cloud-run-proxy google-cloud-sdk-harbourbridge google-cloud-sdk-cbt google-cloud-sdk-gke-gcloud-auth-plugin google-cloud-sdk-kpt google-cloud-sdk-local-extract google-cloud-sdk-minikube google-cloud-sdk-app-engine-java google-cloud-sdk-app-engine-go google-cloud-sdk-app-engine-python google-cloud-sdk-spanner-emulator google-cloud-sdk-bigtable-emulator google-cloud-sdk-nomos google-cloud-sdk-package-go-module google-cloud-sdk-firestore-emulator kubectl google-cloud-sdk-datastore-emulator google-cloud-sdk-app-engine-python-extras google-cloud-sdk-cloud-build-local google-cloud-sdk-kubectl-oidc google-cloud-sdk-anthos-auth google-cloud-sdk-app-engine-grpc google-cloud-sdk-pubsub-emulator google-cloud-sdk-datalab google-cloud-sdk-skaffold google-cloud-sdk google-cloud-sdk-terraform-tools google-cloud-sdk-config-connector\n",
    "    ! gcloud components update --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gar_create_repo"
   },
   "source": [
    "### Create a private Docker repository\n",
    "\n",
    "Your first step is to create your own Docker repository in Artifact Registry.\n",
    "\n",
    "1. Run the `gcloud artifacts repositories create` command to create a new Docker repository with your region with the description \"docker repository\".\n",
    "\n",
    "2. Run the `gcloud artifacts repositories list` command to verify that your repository was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ea7cf85d87d2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "REPOSITORY = \"torchrun-imagenet-repo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "gar_create_repo",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create request issued for: [torchrun-imagenet-repo]\n",
      "Waiting for operation [projects/vertexai-service-project/locations/us-central1/\n",
      "operations/45c5f580-6535-4a85-8792-c9748878c02e] to complete...done.           \n",
      "Created repository [torchrun-imagenet-repo].\n",
      "Listing items under project vertexai-service-project, across all locations.\n",
      "\n",
      "                                                                         ARTIFACT_REGISTRY\n",
      "REPOSITORY              FORMAT  MODE                 DESCRIPTION        LOCATION     LABELS  ENCRYPTION          CREATE_TIME          UPDATE_TIME          SIZE (MB)\n",
      "torchrun-imagenet-repo  DOCKER  STANDARD_REPOSITORY  Docker repository  us-central1          Google-managed key  2024-06-25T09:29:49  2024-06-25T09:29:49  0\n"
     ]
    }
   ],
   "source": [
    "! gcloud artifacts repositories create {REPOSITORY} --repository-format=docker --location={LOCATION} --description=\"Docker repository\"\n",
    "\n",
    "! gcloud artifacts repositories list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gar_auth"
   },
   "source": [
    "### Configure authentication to your private repo\n",
    "\n",
    "Before you push or pull container images, configure Docker to use the `gcloud` command-line tool to authenticate requests to `Artifact Registry` for your region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "gar_auth",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/home/jupyter/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"gcr.io\": \"gcloud\",\n",
      "    \"us.gcr.io\": \"gcloud\",\n",
      "    \"eu.gcr.io\": \"gcloud\",\n",
      "    \"asia.gcr.io\": \"gcloud\",\n",
      "    \"staging-k8s.gcr.io\": \"gcloud\",\n",
      "    \"marketplace.gcr.io\": \"gcloud\",\n",
      "    \"us-central1-docker.pkg.dev\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: us-central1-docker.pkg.dev\n",
      "gcloud credential helpers already registered correctly.\n"
     ]
    }
   ],
   "source": [
    "! gcloud auth configure-docker {LOCATION}-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8f3ea1210749"
   },
   "source": [
    "## Vertex AI Training with GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2f6e7336d1a0"
   },
   "source": [
    "### Create files for the host container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "fd9228f35579",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%mkdir -p trainer\n",
    "%cat /dev/null > trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70886930375a"
   },
   "source": [
    "#### Create the Dockerfile\n",
    "Installs necessary libraries, and downloads the tiny ImageNet data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "b7da2e37e767",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/Dockerfile\n",
    "FROM gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m102\n",
    "\n",
    "RUN curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - && \\\n",
    "    # Install reduction server plugin on GPU containers. google-fast-socket is\n",
    "    # previously installed in GPU dlenv containers only and it is not compatible\n",
    "    # with google-reduction-server.\n",
    "    if dpkg -s google-fast-socket; then \\\n",
    "      apt remove -y google-fast-socket && \\\n",
    "      apt install -y google-reduction-server; \\\n",
    "    fi\n",
    "\n",
    "RUN rm -f /etc/apt/sources.list.d/cuda.list && \\\n",
    "    rm -f /etc/apt/sources.list.d/nvidia-ml.list\n",
    "\n",
    "RUN apt-key del 7fa2af80 && \\\n",
    "    apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub && \\\n",
    "    apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n",
    "\n",
    "RUN apt-get update --allow-releaseinfo-change\n",
    "RUN apt-get update -y && \\\n",
    "    apt-get install -y curl gnupg telnet nano net-tools iputils-ping\n",
    "\n",
    "# Set ETCD version\n",
    "ARG ETCD_VER=v2.3.0\n",
    "# Choose either URL\n",
    "ARG GOOGLE_URL=https://storage.googleapis.com/etcd\n",
    "ARG GITHUB_URL=https://github.com/etcd-io/etcd/releases/download\n",
    "# Set ETCD URL to download from\n",
    "ARG DOWNLOAD_URL=$GOOGLE_URL\n",
    "\n",
    "# Install ETCD\n",
    "RUN mkdir -p /tmp/etcd-download-test && \\\n",
    "    curl -L ${DOWNLOAD_URL}/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz -o /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz && \\\n",
    "    tar xzvf /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz -C /tmp/etcd-download-test --strip-components=1 && \\\n",
    "    rm -f /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz\n",
    "\n",
    "# Copy training application code\n",
    "COPY . /trainer\n",
    "\n",
    "WORKDIR /trainer\n",
    "\n",
    "# Install dependencies\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "RUN chmod 777 main.sh\n",
    "\n",
    "# Download data to the container\n",
    "RUN wget -q -P /trainer/data https://image-net.org/data/tiny-imagenet-200.zip\n",
    "RUN unzip -q /trainer/data/tiny-imagenet-200.zip\n",
    "RUN rm /trainer/data/tiny-imagenet-200.zip\n",
    "\n",
    "CMD [\"/bin/bash\", \"main.sh\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "cbcfa70f5d54",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/requirements.txt\n",
    "torch==1.13.0\n",
    "torchvision==0.14.0\n",
    "tensorboard==2.5.0\n",
    "protobuf==3.20.*\n",
    "python-etcd\n",
    "python-json-logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "127c2963c3b8"
   },
   "source": [
    "#### Create the main.sh file \n",
    "Starts the ETCD server on the host, saves the host IP to Cloud Storage (for the workers), and calls torchrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "77dba1b6bf95",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer/main.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/main.sh\n",
    "#!/bin/bash\n",
    "# Copyright 2022 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# Provision the prerequisites for running a job on TPU VMs in GKE \n",
    "# using a Vertex AI Pipeline \n",
    "# USAGE:  ./install.sh PROJECT_ID GKE_CLUSTER NAME_PREFIX [ZONE=us-central1-b]\n",
    "# ./install.sh your-project-id gke-tpu-cluster gke-tpu us-central1-b\n",
    "# Set up a global error handler\n",
    "err_handler() {\n",
    "    echo \"Error on line: $1\"\n",
    "    echo \"Caused by: $2\"\n",
    "    echo \"That returned exit status: $3\"\n",
    "    echo \"Aborting...\"\n",
    "    exit $3\n",
    "}\n",
    "\n",
    "trap 'err_handler \"$LINENO\" \"$BASH_COMMAND\" \"$?\"' ERR\n",
    "\n",
    "setup_etcd() {\n",
    "    HOST_IP=$1\n",
    "    # Start a local instane of ETCD v2 \n",
    "    export ETCD_ENABLE_V2=true\n",
    "    export ETCDCTL_API=2\n",
    "\n",
    "    /tmp/etcd-download-test/etcd --name s1 --data-dir /tmp/etcd-download-test/s1  \\\n",
    "    --listen-client-urls http://0.0.0.0:2379 --advertise-client-urls http://$HOST_IP:2379 \\\n",
    "    --listen-peer-urls http://0.0.0.0:2380 --initial-advertise-peer-urls http://$HOST_IP:2380 \\\n",
    "    --initial-cluster s1=http://$HOST_IP:2380 --initial-cluster-token tkn \\\n",
    "    --initial-cluster-state new &> /tmp/etcd-download-test/node.log &\n",
    "\n",
    "    /tmp/etcd-download-test/etcd --version\n",
    "    /tmp/etcd-download-test/etcdctl --version\n",
    "}\n",
    "\n",
    "\n",
    "# Process and print passed in variables\n",
    "while getopts e:a:b:d:t:w:v:u:i:p:n:r:c: option\n",
    "do \n",
    "    case \"${option}\"\n",
    "        in\n",
    "        e)epochs=${OPTARG};;\n",
    "        a)arch=${OPTARG};;\n",
    "        b)batchsize=${OPTARG};;\n",
    "        d)distbackend=${OPTARG};;\n",
    "        t)data=${OPTARG};;\n",
    "        w)workers=${OPTARG};;\n",
    "        v)env=${OPTARG};;\n",
    "        u)rdvzbackend=${OPTARG};;\n",
    "        i)rdvzid=${OPTARG};;\n",
    "        p)endpoint=${OPTARG};;\n",
    "        n)nnodes=${OPTARG};;\n",
    "        r)nprocpernode=${OPTARG};;\n",
    "        c)ischief=${OPTARG};;\n",
    "    esac\n",
    "done\n",
    "\n",
    "echo \"epochs : $epochs\"\n",
    "echo \"arch : $arch\"\n",
    "echo \"batchsize : $batchsize\"\n",
    "echo \"distbackend : $distbackend\"\n",
    "echo \"data : $data\"\n",
    "echo \"workers : $workers\"\n",
    "echo \"env : $env\"\n",
    "echo \"rdvzbackend : $rdvzbackend\"\n",
    "echo \"rdvzid : $rdvzid\"\n",
    "echo \"endpoint : $endpoint\"\n",
    "echo \"nnodes : $nnodes\"\n",
    "echo \"nprocpernode : $nprocpernode\"\n",
    "echo \"ischief : $ischief\"\n",
    "\n",
    "# parse cluster config\n",
    "IFS=' ' read -a conf <<< $(python parse_cluster_config.py)\n",
    "WORKERPOOL_TYPE=\"${conf[0]}\"\n",
    "\n",
    "echo \"WORKERPOOL_TYPE=${WORKERPOOL_TYPE}\"\n",
    "echo \"CLUSTER_SPEC=${CLUSTER_SPEC}\"\n",
    "\n",
    "gcsfilepath=\"${env//\\/gcs\\//gs://}\"\n",
    "\n",
    "if [ \"$WORKERPOOL_TYPE\" == \"workerpool0\" ] || [ \"$WORKERPOOL_TYPE\" == \"chief\" ]; then\n",
    "    HOST_IP=$(hostname -i)\n",
    "    echo \"HOST_IP=\"$HOST_IP\n",
    "    echo \"Writing host IP address to \"$gcsfilepath\n",
    "    echo $HOST_IP| gsutil cp - $gcsfilepath\n",
    "    setup_etcd $HOST_IP\n",
    "else\n",
    "    echo \"Wait 60s for the host server to come online\"\n",
    "    sleep 60\n",
    "    echo \"reading host IP address from \"$gcsfilepath\n",
    "    HOST_IP=$(gsutil cat $gcsfilepath)\n",
    "    echo \"HOST_IP=\"$HOST_IP\n",
    "fi\n",
    "\n",
    "env=\"env://\"\n",
    "ping -c 1 $HOST_IP\n",
    "\n",
    "set -x\n",
    "\n",
    "torchrun --rdzv_backend $rdvzbackend --rdzv_id $rdvzid --rdzv_endpoint $HOST_IP:2379 \\\n",
    "--nnodes $nnodes --nproc_per_node $nprocpernode --master_addr $HOST_IP --master_port 2379 \\\n",
    "main.py --epochs $epochs --arch $arch --batch-size $batchsize --dist-backend $distbackend \\\n",
    "--data $data \\\n",
    "--env $env \\\n",
    "--hostip $HOST_IP \\\n",
    "--hostipport 2379 \\\n",
    "--workers $workers \\\n",
    "--ischief $ischief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "3294e388117a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trainer/parse_cluster_config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/parse_cluster_config.py\n",
    "import os\n",
    "import json\n",
    "\n",
    "cluster_config_str = os.environ.get('CLUSTER_SPEC')\n",
    "cluster_config_dict  = json.loads(cluster_config_str)\n",
    "workerpool_type = cluster_config_dict['task']['type']\n",
    "\n",
    "print(workerpool_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10fdcf6db6bb"
   },
   "source": [
    "#### Create the main.py file \n",
    "Main trainer for the ImageNet training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "24793c94ebff",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trainer/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/main.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the BSD-style license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "r\"\"\"\n",
    "Source: `pytorch imagenet example <https://github.com/pytorch/examples/blob/master/imagenet/main.py>`_ # noqa B950\n",
    "Modified and simplified to make the original pytorch example compatible with\n",
    "torchelastic.distributed.launch.\n",
    "Changes:\n",
    "1. Removed ``rank``, ``gpu``, ``multiprocessing-distributed``, ``dist_url`` options.\n",
    "   These are obsolete parameters when using ``torchelastic.distributed.launch``.\n",
    "2. Removed ``seed``, ``evaluate``, ``pretrained`` options for simplicity.\n",
    "3. Removed ``resume``, ``start-epoch`` options.\n",
    "   Loads the most recent checkpoint by default.\n",
    "4. ``batch-size`` is now per GPU (worker) batch size rather than for all GPUs.\n",
    "5. Defaults ``workers`` (num data loader workers) to ``0``.\n",
    "Usage\n",
    "::\n",
    " >>> python -m torchelastic.distributed.launch\n",
    "        --nnodes=$NUM_NODES\n",
    "        --nproc_per_node=$WORKERS_PER_NODE\n",
    "        --rdzv_id=$JOB_ID\n",
    "        --rdzv_backend=etcd\n",
    "        --rdzv_endpoint=$ETCD_HOST:$ETCD_PORT\n",
    "        main.py\n",
    "        --arch resnet18\n",
    "        --epochs 20\n",
    "        --batch-size 32\n",
    "        <DATA_DIR>\n",
    "\"\"\"\n",
    "\n",
    "import traceback\n",
    "import argparse\n",
    "import io\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from datetime import timedelta\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.distributed.elastic.utils.data import ElasticDistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "model_names = sorted(\n",
    "    name\n",
    "    for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\") and callable(models.__dict__[name])\n",
    ")\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"PyTorch Elastic ImageNet Training\")\n",
    "parser.add_argument(\"--data\", metavar=\"DIR\", help=\"path to dataset\")\n",
    "parser.add_argument(\n",
    "    \"-a\",\n",
    "    \"--arch\",\n",
    "    metavar=\"ARCH\",\n",
    "    default=\"resnet18\",\n",
    "    choices=model_names,\n",
    "    help=\"model architecture: \" + \" | \".join(model_names) + \" (default: resnet18)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-j\",\n",
    "    \"--workers\",\n",
    "    default=0,\n",
    "    type=int,\n",
    "    metavar=\"N\",\n",
    "    help=\"number of data loading workers\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--epochs\", default=90, type=int, metavar=\"N\", help=\"number of total epochs to run\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-b\",\n",
    "    \"--batch-size\",\n",
    "    default=32,\n",
    "    type=int,\n",
    "    metavar=\"N\",\n",
    "    help=\"mini-batch size (default: 32), per worker (GPU)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr\",\n",
    "    \"--learning-rate\",\n",
    "    default=0.1,\n",
    "    type=float,\n",
    "    metavar=\"LR\",\n",
    "    help=\"initial learning rate\",\n",
    "    dest=\"lr\",\n",
    ")\n",
    "parser.add_argument(\"--momentum\", default=0.9, type=float, metavar=\"M\", help=\"momentum\")\n",
    "parser.add_argument(\n",
    "    \"--wd\",\n",
    "    \"--weight-decay\",\n",
    "    default=1e-4,\n",
    "    type=float,\n",
    "    metavar=\"W\",\n",
    "    help=\"weight decay (default: 1e-4)\",\n",
    "    dest=\"weight_decay\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-p\",\n",
    "    \"--print-freq\",\n",
    "    default=10,\n",
    "    type=int,\n",
    "    metavar=\"N\",\n",
    "    help=\"print frequency (default: 10)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dist-backend\",\n",
    "    default=\"nccl\",\n",
    "    choices=[\"nccl\", \"gloo\"],\n",
    "    type=str,\n",
    "    help=\"distributed backend\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--checkpoint-file\",\n",
    "    default=\"/tmp/checkpoint.pth.tar\",\n",
    "    type=str,\n",
    "    help=\"checkpoint file path, to load and save to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--env\",\n",
    "    default=\"env://\",\n",
    "    type=str,\n",
    "    help=\"setting for init_method for torch.distributed.init_process_group. Leave default unless you want to pass a shared gcs path\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hostip\",\n",
    "    default=\"localhost\",\n",
    "    type=str,\n",
    "    help=\"setting for etcd host ip\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hostipport\",\n",
    "    default=2379,\n",
    "    type=int,\n",
    "    help=\"setting for etcd host ip port\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ischief\",\n",
    "    default=\"n\", \n",
    "    type=str,\n",
    "    help='is this cheif or worker')\n",
    "\n",
    "def main():\n",
    "    args = parser.parse_args()\n",
    "    print(args)\n",
    "    device_id = int(os.environ[\"LOCAL_RANK\"])\n",
    "    torch.cuda.set_device(device_id)\n",
    "    print(f\"=> set cuda device = {device_id}\")\n",
    "\n",
    "    LOCAL_RANK=int(os.environ[\"LOCAL_RANK\"])\n",
    "    RANK=int(os.environ[\"RANK\"])\n",
    "    WORLD_SIZE=int(os.environ[\"WORLD_SIZE\"])\n",
    "\n",
    "    print (f\"LOCAL_RANK={os.environ['LOCAL_RANK']} RANK={os.environ['RANK']} WORLD_SIZE={os.environ['WORLD_SIZE']}\")\n",
    "    print (f\"args env= {args.env}\")\n",
    "    \n",
    "    print (f\"Host address: {os.environ['MASTER_ADDR']}:{os.environ['MASTER_PORT']}\")\n",
    "    os.environ['MASTER_ADDR']=args.hostip\n",
    "    print (f\"Updated IPv4 host address: {os.environ['MASTER_ADDR']}:{os.environ['MASTER_PORT']}\")\n",
    "    \n",
    "    print ('Initialize process group')\n",
    "    if args.env == \"env://\":\n",
    "        dist.init_process_group(\n",
    "            backend=args.dist_backend, init_method=f\"{args.env}\", timeout=timedelta(seconds=120)\n",
    "        )\n",
    "    else:\n",
    "        if args.ischief.lower() == 'y':\n",
    "            print ('Setting store')\n",
    "            #STORE = dist.FileStore(args.env, WORLD_SIZE)\n",
    "            STORE = dist.TCPStore(host_name=args.hostip, port=args.hostipport, world_size=WORLD_SIZE, is_master=True, timeout=timedelta(seconds=30))\n",
    "            print (f'Store set = {STORE}')        \n",
    "            dist.init_process_group(\n",
    "                backend=args.dist_backend, store=STORE, timeout=timedelta(seconds=30),\n",
    "                rank=RANK, world_size=WORLD_SIZE\n",
    "            )\n",
    "\n",
    "        dist.init_process_group(\n",
    "            backend=args.dist_backend, init_method=f\"tcp://{args.hostip}:{args.hostipport}\", timeout=timedelta(seconds=120), rank=RANK, world_size=WORLD_SIZE\n",
    "        )\n",
    "    print ('Process initialized')\n",
    "\n",
    "    model, criterion, optimizer = initialize_model(\n",
    "        args.arch, args.lr, args.momentum, args.weight_decay, device_id\n",
    "    )\n",
    "\n",
    "    train_loader, val_loader = initialize_data_loader(\n",
    "        args.data, args.batch_size, args.workers\n",
    "    )\n",
    "\n",
    "    # resume from checkpoint if one exists;\n",
    "    state = load_checkpoint(\n",
    "        args.checkpoint_file, device_id, args.arch, model, optimizer\n",
    "    )\n",
    "\n",
    "    start_epoch = state.epoch + 1\n",
    "    print(f\"=> start_epoch: {start_epoch}, best_acc1: {state.best_acc1}\")\n",
    "\n",
    "    print_freq = args.print_freq\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "        state.epoch = epoch\n",
    "        train_loader.batch_sampler.sampler.set_epoch(epoch)\n",
    "        adjust_learning_rate(optimizer, epoch, args.lr)\n",
    "\n",
    "        # train for one epoch\n",
    "        train(train_loader, model, criterion, optimizer, epoch, device_id, print_freq)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        acc1 = validate(val_loader, model, criterion, device_id, print_freq)\n",
    "\n",
    "        # remember best acc@1 and save checkpoint\n",
    "        is_best = acc1 > state.best_acc1\n",
    "        state.best_acc1 = max(acc1, state.best_acc1)\n",
    "\n",
    "        if device_id == 0:\n",
    "            save_checkpoint(state, is_best, args.checkpoint_file)\n",
    "\n",
    "\n",
    "class State:\n",
    "    \"\"\"\n",
    "    Container for objects that we want to checkpoint. Represents the\n",
    "    current \"state\" of the worker. This object is mutable.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, arch, model, optimizer):\n",
    "        self.epoch = -1\n",
    "        self.best_acc1 = 0\n",
    "        self.arch = arch\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def capture_snapshot(self):\n",
    "        \"\"\"\n",
    "        Essentially a ``serialize()`` function, returns the state as an\n",
    "        object compatible with ``torch.save()``. The following should work\n",
    "        ::\n",
    "        snapshot = state_0.capture_snapshot()\n",
    "        state_1.apply_snapshot(snapshot)\n",
    "        assert state_0 == state_1\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"epoch\": self.epoch,\n",
    "            \"best_acc1\": self.best_acc1,\n",
    "            \"arch\": self.arch,\n",
    "            \"state_dict\": self.model.state_dict(),\n",
    "            \"optimizer\": self.optimizer.state_dict(),\n",
    "        }\n",
    "\n",
    "    def apply_snapshot(self, obj, device_id):\n",
    "        \"\"\"\n",
    "        The complimentary function of ``capture_snapshot()``. Applies the\n",
    "        snapshot object that was returned by ``capture_snapshot()``.\n",
    "        This function mutates this state object.\n",
    "        \"\"\"\n",
    "\n",
    "        self.epoch = obj[\"epoch\"]\n",
    "        self.best_acc1 = obj[\"best_acc1\"]\n",
    "        self.state_dict = obj[\"state_dict\"]\n",
    "        self.model.load_state_dict(obj[\"state_dict\"])\n",
    "        self.optimizer.load_state_dict(obj[\"optimizer\"])\n",
    "\n",
    "    def save(self, f):\n",
    "        torch.save(self.capture_snapshot(), f)\n",
    "\n",
    "    def load(self, f, device_id):\n",
    "        # Map model to be loaded to specified single gpu.\n",
    "        snapshot = torch.load(f, map_location=f\"cuda:{device_id}\")\n",
    "        self.apply_snapshot(snapshot, device_id)\n",
    "\n",
    "\n",
    "def initialize_model(\n",
    "    arch: str, lr: float, momentum: float, weight_decay: float, device_id: int\n",
    "):\n",
    "    print(f\"=> creating model: {arch}\")\n",
    "    model = models.__dict__[arch]()\n",
    "    # For multiprocessing distributed, DistributedDataParallel constructor\n",
    "    # should always set the single device scope, otherwise,\n",
    "    # DistributedDataParallel will use all available devices.\n",
    "    model.cuda(device_id)\n",
    "    cudnn.benchmark = True\n",
    "    model = DistributedDataParallel(model, device_ids=[device_id])\n",
    "    # define loss function (criterion) and optimizer\n",
    "    criterion = nn.CrossEntropyLoss().cuda(device_id)\n",
    "    optimizer = SGD(\n",
    "        model.parameters(), lr, momentum=momentum, weight_decay=weight_decay\n",
    "    )\n",
    "    return model, criterion, optimizer\n",
    "\n",
    "\n",
    "def initialize_data_loader(\n",
    "    data_dir, batch_size, num_data_workers\n",
    ") -> Tuple[DataLoader, DataLoader]:\n",
    "    traindir = os.path.join(data_dir, \"train\")\n",
    "    valdir = os.path.join(data_dir, \"val\")\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "    train_dataset = datasets.ImageFolder(\n",
    "        traindir,\n",
    "        transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomResizedCrop(224),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "    train_sampler = ElasticDistributedSampler(train_dataset)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_data_workers,\n",
    "        pin_memory=True,\n",
    "        sampler=train_sampler,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        datasets.ImageFolder(\n",
    "            valdir,\n",
    "            transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize(256),\n",
    "                    transforms.CenterCrop(224),\n",
    "                    transforms.ToTensor(),\n",
    "                    normalize,\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_data_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def load_checkpoint(\n",
    "    checkpoint_file: str,\n",
    "    device_id: int,\n",
    "    arch: str,\n",
    "    model: DistributedDataParallel,\n",
    "    optimizer,  # SGD\n",
    ") -> State:\n",
    "    \"\"\"\n",
    "    Loads a local checkpoint (if any). Otherwise, checks to see if any of\n",
    "    the neighbors have a non-zero state. If so, restore the state\n",
    "    from the rank that has the most up-to-date checkpoint.\n",
    "    .. note:: when your job has access to a globally visible persistent storage\n",
    "              (e.g. nfs mount, S3) you can simply have all workers load\n",
    "              from the most recent checkpoint from such storage. Since this\n",
    "              example is expected to run on vanilla hosts (with no shared\n",
    "              storage) the checkpoints are written to local disk, hence\n",
    "              we have the extra logic to broadcast the checkpoint from a\n",
    "              surviving node.\n",
    "    \"\"\"\n",
    "\n",
    "    state = State(arch, model, optimizer)\n",
    "\n",
    "    if os.path.isfile(checkpoint_file):\n",
    "        print(f\"=> loading checkpoint file: {checkpoint_file}\")\n",
    "        state.load(checkpoint_file, device_id)\n",
    "        print(f\"=> loaded checkpoint file: {checkpoint_file}\")\n",
    "\n",
    "    # logic below is unnecessary when the checkpoint is visible on all nodes!\n",
    "    # create a temporary cpu pg to broadcast most up-to-date checkpoint\n",
    "    with tmp_process_group(backend=\"gloo\") as pg:\n",
    "        rank = dist.get_rank(group=pg)\n",
    "\n",
    "        # get rank that has the largest state.epoch\n",
    "        epochs = torch.zeros(dist.get_world_size(), dtype=torch.int32)\n",
    "        epochs[rank] = state.epoch\n",
    "        dist.all_reduce(epochs, op=dist.ReduceOp.SUM, group=pg)\n",
    "        t_max_epoch, t_max_rank = torch.max(epochs, dim=0)\n",
    "        max_epoch = t_max_epoch.item()\n",
    "        max_rank = t_max_rank.item()\n",
    "\n",
    "        # max_epoch == -1 means no one has checkpointed return base state\n",
    "        if max_epoch == -1:\n",
    "            print(f\"=> no workers have checkpoints, starting from epoch 0\")\n",
    "            return state\n",
    "\n",
    "        # broadcast the state from max_rank (which has the most up-to-date state)\n",
    "        # pickle the snapshot, convert it into a byte-blob tensor\n",
    "        # then broadcast it, unpickle it and apply the snapshot\n",
    "        print(f\"=> using checkpoint from rank: {max_rank}, max_epoch: {max_epoch}\")\n",
    "\n",
    "        with io.BytesIO() as f:\n",
    "            torch.save(state.capture_snapshot(), f)\n",
    "            raw_blob = numpy.frombuffer(f.getvalue(), dtype=numpy.uint8)\n",
    "\n",
    "        blob_len = torch.tensor(len(raw_blob))\n",
    "        dist.broadcast(blob_len, src=max_rank, group=pg)\n",
    "        print(f\"=> checkpoint broadcast size is: {blob_len}\")\n",
    "\n",
    "        if rank != max_rank:\n",
    "            # pyre-fixme[6]: For 1st param expected `Union[List[int], Size,\n",
    "            #  typing.Tuple[int, ...]]` but got `Union[bool, float, int]`.\n",
    "            blob = torch.zeros(blob_len.item(), dtype=torch.uint8)\n",
    "        else:\n",
    "            blob = torch.as_tensor(raw_blob, dtype=torch.uint8)\n",
    "\n",
    "        dist.broadcast(blob, src=max_rank, group=pg)\n",
    "        print(f\"=> done broadcasting checkpoint\")\n",
    "\n",
    "        if rank != max_rank:\n",
    "            with io.BytesIO(blob.numpy()) as f:\n",
    "                snapshot = torch.load(f)\n",
    "            state.apply_snapshot(snapshot, device_id)\n",
    "\n",
    "        # wait till everyone has loaded the checkpoint\n",
    "        dist.barrier(group=pg)\n",
    "\n",
    "    print(f\"=> done restoring from previous checkpoint\")\n",
    "    return state\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def tmp_process_group(backend):\n",
    "    cpu_pg = dist.new_group(backend=backend)\n",
    "    try:\n",
    "        yield cpu_pg\n",
    "    finally:\n",
    "        dist.destroy_process_group(cpu_pg)\n",
    "\n",
    "\n",
    "def save_checkpoint(state: State, is_best: bool, filename: str):\n",
    "    checkpoint_dir = os.path.dirname(filename)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    # save to tmp, then commit by moving the file in case the job\n",
    "    # gets interrupted while writing the checkpoint\n",
    "    tmp_filename = filename + \".tmp\"\n",
    "    torch.save(state.capture_snapshot(), tmp_filename)\n",
    "    os.rename(tmp_filename, filename)\n",
    "    print(f\"=> saved checkpoint for epoch {state.epoch} at {filename}\")\n",
    "    if is_best:\n",
    "        best = os.path.join(checkpoint_dir, \"model_best.pth.tar\")\n",
    "        print(f\"=> best model found at epoch {state.epoch} saving to {best}\")\n",
    "        shutil.copyfile(filename, best)\n",
    "\n",
    "\n",
    "def train(\n",
    "    train_loader: DataLoader,\n",
    "    model: DistributedDataParallel,\n",
    "    criterion,  # nn.CrossEntropyLoss\n",
    "    optimizer,  # SGD,\n",
    "    epoch: int,\n",
    "    device_id: int,\n",
    "    print_freq: int,\n",
    "):\n",
    "    batch_time = AverageMeter(\"Time\", \":6.3f\")\n",
    "    data_time = AverageMeter(\"Data\", \":6.3f\")\n",
    "    losses = AverageMeter(\"Loss\", \":.4e\")\n",
    "    top1 = AverageMeter(\"Acc@1\", \":6.2f\")\n",
    "    top5 = AverageMeter(\"Acc@5\", \":6.2f\")\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses, top1, top5],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch+1),\n",
    "    )\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        images = images.cuda(device_id, non_blocking=True)\n",
    "        target = target.cuda(device_id, non_blocking=True)\n",
    "\n",
    "        # compute output\n",
    "        output = model(images)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        top1.update(acc1[0], images.size(0))\n",
    "        top5.update(acc5[0], images.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            progress.display(i)\n",
    "\n",
    "\n",
    "def validate(\n",
    "    val_loader: DataLoader,\n",
    "    model: DistributedDataParallel,\n",
    "    criterion,  # nn.CrossEntropyLoss\n",
    "    device_id: int,\n",
    "    print_freq: int,\n",
    "):\n",
    "    batch_time = AverageMeter(\"Time\", \":6.3f\")\n",
    "    losses = AverageMeter(\"Loss\", \":.4e\")\n",
    "    top1 = AverageMeter(\"Acc@1\", \":6.2f\")\n",
    "    top5 = AverageMeter(\"Acc@5\", \":6.2f\")\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader), [batch_time, losses, top1, top5], prefix=\"Test: \"\n",
    "    )\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "            if device_id is not None:\n",
    "                images = images.cuda(device_id, non_blocking=True)\n",
    "            target = target.cuda(device_id, non_blocking=True)\n",
    "\n",
    "            # compute output\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            top1.update(acc1[0], images.size(0))\n",
    "            top5.update(acc5[0], images.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:\n",
    "                progress.display(i)\n",
    "\n",
    "        # TODO: this should also be done with the ProgressMeter\n",
    "        print(\n",
    "            \" * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}\".format(top1=top1, top5=top5)\n",
    "        )\n",
    "\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, name: str, fmt: str = \":f\"):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1) -> None:\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = \"{name} {val\" + self.fmt + \"} ({avg\" + self.fmt + \"})\"\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches: int, meters: List[AverageMeter], prefix: str = \"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch: int) -> None:\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print(\"\\t\".join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches: int) -> str:\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = \"{:\" + str(num_digits) + \"d}\"\n",
    "        return \"[\" + fmt + \"/\" + fmt.format(num_batches) + \"]\"\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch: int, lr: float) -> None:\n",
    "    \"\"\"\n",
    "    Sets the learning rate to the initial LR decayed by 10 every 30 epochs\n",
    "    \"\"\"\n",
    "    learning_rate = lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = learning_rate\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"\n",
    "    Computes the accuracy over the k top predictions for the specified values of k\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(1, -1).view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        trace_str = ''.join(traceback.format_tb(e.__traceback__))\n",
    "        print(trace_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93002a20a2a6"
   },
   "source": [
    "### Build custom container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "8ea6fc98b2a9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONTENT_NAME = \"pytorch-torchrun-imagenet-multi-node\"\n",
    "CONTAINER_NAME = CONTENT_NAME + \"-gpu\"\n",
    "TAG = \"latest\"\n",
    "\n",
    "custom_container_host_image_uri = (\n",
    "    f\"{LOCATION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{CONTAINER_NAME}:{TAG}\"  # noqa\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "ee1a0a06d0b4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary archive of 7 file(s) totalling 35.8 KiB before compression.\n",
      "Uploading tarball of [trainer] to [gs://vertexai-service-project_cloudbuild/source/1719308467.317051-f8a6f615c0bc46478be1dd0c2aeb87ba.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/vertexai-service-project/locations/us-central1/builds/cb3d5135-a728-4a5f-bb4e-8213f8e99259].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds;region=us-central1/cb3d5135-a728-4a5f-bb4e-8213f8e99259?project=314481442207 ].\n",
      "Waiting for build to complete. Polling interval: 1 second(s).\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"cb3d5135-a728-4a5f-bb4e-8213f8e99259\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://vertexai-service-project_cloudbuild/source/1719308467.317051-f8a6f615c0bc46478be1dd0c2aeb87ba.tgz#1719308467748245\n",
      "Copying gs://vertexai-service-project_cloudbuild/source/1719308467.317051-f8a6f615c0bc46478be1dd0c2aeb87ba.tgz#1719308467748245...\n",
      "/ [1 files][ 12.3 KiB/ 12.3 KiB]                                                \n",
      "Operation completed over 1 objects/12.3 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  43.01kB\n",
      "Step 1/19 : FROM gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m102\n",
      "m102: Pulling from deeplearning-platform-release/pytorch-gpu.1-13\n",
      "eaead16dc43b: Pulling fs layer\n",
      "66505a2e0e5b: Pulling fs layer\n",
      "a655a22c474a: Pulling fs layer\n",
      "a296e3e832e8: Pulling fs layer\n",
      "c1c3ca40938b: Pulling fs layer\n",
      "aa8799b38d43: Pulling fs layer\n",
      "7a83033ad051: Pulling fs layer\n",
      "e508c64fd858: Pulling fs layer\n",
      "c711afa7022f: Pulling fs layer\n",
      "eced3f37c25e: Pulling fs layer\n",
      "8e7602fdce33: Pulling fs layer\n",
      "ff60b2bba5f8: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "ebd800816a17: Pulling fs layer\n",
      "a798f9ec3f11: Pulling fs layer\n",
      "9c1c6c434334: Pulling fs layer\n",
      "c2ccb3ba2311: Pulling fs layer\n",
      "76a2e4e12ef8: Pulling fs layer\n",
      "b336ce8db20f: Pulling fs layer\n",
      "53e0c73b7a09: Pulling fs layer\n",
      "f1615116c881: Pulling fs layer\n",
      "b885cde9b114: Pulling fs layer\n",
      "a296e3e832e8: Waiting\n",
      "0949c75a4dde: Pulling fs layer\n",
      "55cd1e2d875c: Pulling fs layer\n",
      "c1c3ca40938b: Waiting\n",
      "aa8799b38d43: Waiting\n",
      "f6e57611c727: Pulling fs layer\n",
      "35252cfce065: Pulling fs layer\n",
      "7a83033ad051: Waiting\n",
      "f4777e8e02cd: Pulling fs layer\n",
      "e508c64fd858: Waiting\n",
      "c711afa7022f: Waiting\n",
      "3cb270247f6f: Pulling fs layer\n",
      "8fd949e557d0: Pulling fs layer\n",
      "eced3f37c25e: Waiting\n",
      "2dc11453d343: Pulling fs layer\n",
      "8e7602fdce33: Waiting\n",
      "c9f50f95c770: Pulling fs layer\n",
      "555778361e36: Pulling fs layer\n",
      "ff60b2bba5f8: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "0949c75a4dde: Waiting\n",
      "ebd800816a17: Waiting\n",
      "55cd1e2d875c: Waiting\n",
      "f6e57611c727: Waiting\n",
      "a798f9ec3f11: Waiting\n",
      "9c1c6c434334: Waiting\n",
      "35252cfce065: Waiting\n",
      "76a2e4e12ef8: Waiting\n",
      "f4777e8e02cd: Waiting\n",
      "b336ce8db20f: Waiting\n",
      "3cb270247f6f: Waiting\n",
      "c2ccb3ba2311: Waiting\n",
      "b885cde9b114: Waiting\n",
      "8fd949e557d0: Waiting\n",
      "2dc11453d343: Waiting\n",
      "53e0c73b7a09: Waiting\n",
      "c9f50f95c770: Waiting\n",
      "555778361e36: Waiting\n",
      "66505a2e0e5b: Verifying Checksum\n",
      "66505a2e0e5b: Download complete\n",
      "a655a22c474a: Verifying Checksum\n",
      "a655a22c474a: Download complete\n",
      "eaead16dc43b: Verifying Checksum\n",
      "eaead16dc43b: Download complete\n",
      "a296e3e832e8: Download complete\n",
      "c1c3ca40938b: Verifying Checksum\n",
      "c1c3ca40938b: Download complete\n",
      "7a83033ad051: Verifying Checksum\n",
      "7a83033ad051: Download complete\n",
      "c711afa7022f: Verifying Checksum\n",
      "c711afa7022f: Download complete\n",
      "eaead16dc43b: Pull complete\n",
      "66505a2e0e5b: Pull complete\n",
      "a655a22c474a: Pull complete\n",
      "a296e3e832e8: Pull complete\n",
      "c1c3ca40938b: Pull complete\n",
      "e508c64fd858: Verifying Checksum\n",
      "e508c64fd858: Download complete\n",
      "aa8799b38d43: Verifying Checksum\n",
      "aa8799b38d43: Download complete\n",
      "ff60b2bba5f8: Verifying Checksum\n",
      "ff60b2bba5f8: Download complete\n",
      "8e7602fdce33: Verifying Checksum\n",
      "8e7602fdce33: Download complete\n",
      "4f4fb700ef54: Download complete\n",
      "a798f9ec3f11: Verifying Checksum\n",
      "a798f9ec3f11: Download complete\n",
      "ebd800816a17: Verifying Checksum\n",
      "ebd800816a17: Download complete\n",
      "c2ccb3ba2311: Verifying Checksum\n",
      "c2ccb3ba2311: Download complete\n",
      "9c1c6c434334: Verifying Checksum\n",
      "9c1c6c434334: Download complete\n",
      "b336ce8db20f: Verifying Checksum\n",
      "b336ce8db20f: Download complete\n",
      "76a2e4e12ef8: Verifying Checksum\n",
      "76a2e4e12ef8: Download complete\n",
      "f1615116c881: Download complete\n",
      "b885cde9b114: Verifying Checksum\n",
      "b885cde9b114: Download complete\n",
      "0949c75a4dde: Verifying Checksum\n",
      "0949c75a4dde: Download complete\n",
      "55cd1e2d875c: Download complete\n",
      "f6e57611c727: Verifying Checksum\n",
      "f6e57611c727: Download complete\n",
      "35252cfce065: Verifying Checksum\n",
      "35252cfce065: Download complete\n",
      "f4777e8e02cd: Download complete\n",
      "eced3f37c25e: Verifying Checksum\n",
      "eced3f37c25e: Download complete\n",
      "3cb270247f6f: Verifying Checksum\n",
      "3cb270247f6f: Download complete\n",
      "2dc11453d343: Download complete\n",
      "8fd949e557d0: Verifying Checksum\n",
      "8fd949e557d0: Download complete\n",
      "53e0c73b7a09: Verifying Checksum\n",
      "53e0c73b7a09: Download complete\n",
      "555778361e36: Download complete\n",
      "aa8799b38d43: Pull complete\n",
      "7a83033ad051: Pull complete\n",
      "c9f50f95c770: Verifying Checksum\n",
      "c9f50f95c770: Download complete\n",
      "e508c64fd858: Pull complete\n",
      "c711afa7022f: Pull complete\n",
      "eced3f37c25e: Pull complete\n",
      "8e7602fdce33: Pull complete\n",
      "ff60b2bba5f8: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "ebd800816a17: Pull complete\n",
      "a798f9ec3f11: Pull complete\n",
      "9c1c6c434334: Pull complete\n",
      "c2ccb3ba2311: Pull complete\n",
      "76a2e4e12ef8: Pull complete\n",
      "b336ce8db20f: Pull complete\n",
      "53e0c73b7a09: Pull complete\n",
      "f1615116c881: Pull complete\n",
      "b885cde9b114: Pull complete\n",
      "0949c75a4dde: Pull complete\n",
      "55cd1e2d875c: Pull complete\n",
      "f6e57611c727: Pull complete\n",
      "35252cfce065: Pull complete\n",
      "f4777e8e02cd: Pull complete\n",
      "3cb270247f6f: Pull complete\n",
      "8fd949e557d0: Pull complete\n",
      "2dc11453d343: Pull complete\n",
      "c9f50f95c770: Pull complete\n",
      "555778361e36: Pull complete\n",
      "Digest: sha256:0ff4fadfc5da720a115d67886741d6b437033cadf3e16d95a3e266d7b2bdc129\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m102\n",
      " ---> 47bc03eb96fb\n",
      "Step 2/19 : RUN curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - &&     if dpkg -s google-fast-socket; then       apt remove -y google-fast-socket &&       apt install -y google-reduction-server;     fi\n",
      " ---> Running in 2bb4cf37197d\n",
      "\u001b[91m  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2659  100  2659    0     0  45725      0 --:--:-- --:--:-- --:--:-- 45844:--     0\u001b[0m\u001b[91m\n",
      "\u001b[0m\u001b[91mWarning: apt-key output should not be parsed (stdout is not a terminal)\n",
      "\u001b[0mOK\n",
      "Package: google-fast-socket\n",
      "Status: install ok installed\n",
      "Priority: optional\n",
      "Section: contrib/devel\n",
      "Maintainer: Chang Lan <changlan@google.com>\n",
      "Architecture: amd64\n",
      "Version: 0.0.5\n",
      "Recommends: libnccl2\n",
      "Description: Fast Socket for NCCL 2\n",
      "\u001b[91m\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\u001b[0mReading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following packages will be REMOVED:\n",
      "  google-fast-socket\n",
      "0 upgraded, 0 newly installed, 1 to remove and 4 not upgraded.\n",
      "After this operation, 0 B of additional disk space will be used.\n",
      "(Reading database ... 90763 files and directories currently installed.)\n",
      "Removing google-fast-socket (0.0.5) ...\n",
      "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
      "\u001b[91m\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\u001b[0mReading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following NEW packages will be installed:\n",
      "  google-reduction-server\n",
      "0 upgraded, 1 newly installed, 0 to remove and 4 not upgraded.\n",
      "Need to get 514 kB of archives.\n",
      "After this operation, 0 B of additional disk space will be used.\n",
      "Get:1 https://packages.cloud.google.com/apt google-fast-socket/main amd64 google-reduction-server amd64 2.2.3 [514 kB]\n",
      "\u001b[91mdebconf: unable to initialize frontend: Dialog\n",
      "debconf: (TERM is not set, so the dialog frontend is not usable.)\n",
      "debconf: falling back to frontend: Readline\n",
      "\u001b[0m\u001b[91mdebconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "\u001b[0m\u001b[91mdpkg-preconfigure: unable to re-open stdin: \n",
      "\u001b[0mFetched 514 kB in 0s (6245 kB/s)\n",
      "Selecting previously unselected package google-reduction-server.\n",
      "(Reading database ... 90760 files and directories currently installed.)\n",
      "Preparing to unpack .../google-reduction-server_2.2.3_amd64.deb ...\n",
      "Unpacking google-reduction-server (2.2.3) ...\n",
      "Setting up google-reduction-server (2.2.3) ...\n",
      "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
      "Removing intermediate container 2bb4cf37197d\n",
      " ---> 389f90d6f755\n",
      "Step 3/19 : RUN rm -f /etc/apt/sources.list.d/cuda.list &&     rm -f /etc/apt/sources.list.d/nvidia-ml.list\n",
      " ---> Running in 934041f5563f\n",
      "Removing intermediate container 934041f5563f\n",
      " ---> 6492c5e29d6b\n",
      "Step 4/19 : RUN apt-key del 7fa2af80 &&     apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub &&     apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n",
      " ---> Running in 2e290f366e8b\n",
      "OK\n",
      "\u001b[91mWarning: apt-key output should not be parsed (stdout is not a terminal)\n",
      "\u001b[0mExecuting: /tmp/apt-key-gpghome.yZWpHjcan0/gpg.1.sh --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub\n",
      "\u001b[91mgpg: requesting key from 'http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub'\n",
      "\u001b[0m\u001b[91mgpg: key A4B469963BF863CC: \"cudatools <cudatools@nvidia.com>\" not changed\n",
      "gpg: Total number processed: 1\n",
      "gpg:              unchanged: 1\n",
      "\u001b[0m\u001b[91mWarning: apt-key output should not be parsed (stdout is not a terminal)\n",
      "\u001b[0mExecuting: /tmp/apt-key-gpghome.naKVtsCmj3/gpg.1.sh --fetch-keys http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\n",
      "\u001b[91mgpg: requesting key from 'http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub'\n",
      "\u001b[0m\u001b[91mgpg: key F60F4B3D7FA2AF80: public key \"cudatools <cudatools@nvidia.com>\" imported\n",
      "\u001b[0m\u001b[91mgpg: Total number processed: 1\n",
      "gpg:               imported: 1\n",
      "\u001b[0mRemoving intermediate container 2e290f366e8b\n",
      " ---> 2992f5b82cc3\n",
      "Step 5/19 : RUN apt-get update --allow-releaseinfo-change\n",
      " ---> Running in 06fee02b0b2f\n",
      "Get:1 http://packages.cloud.google.com/apt gcsfuse-focal InRelease [1225 B]\n",
      "Get:2 https://packages.cloud.google.com/apt cloud-sdk InRelease [1616 B]\n",
      "Get:3 https://packages.cloud.google.com/apt google-fast-socket InRelease [5015 B]\n",
      "Get:4 http://security.ubuntu.com/ubuntu focal-security InRelease [128 kB]\n",
      "Hit:5 http://archive.ubuntu.com/ubuntu focal InRelease\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease [128 kB]\n",
      "Get:7 http://packages.cloud.google.com/apt gcsfuse-focal/main amd64 Packages [22.5 kB]\n",
      "Get:8 http://packages.cloud.google.com/apt gcsfuse-focal/main all Packages [750 B]\n",
      "Err:2 https://packages.cloud.google.com/apt cloud-sdk InRelease\n",
      "  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY C0BA5CE6DC6315A3\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal-backports InRelease [128 kB]\n",
      "Get:10 https://packages.cloud.google.com/apt google-fast-socket/main amd64 Packages [447 B]\n",
      "Get:11 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [3711 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [4182 kB]\n",
      "Get:13 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [29.8 kB]\n",
      "Get:14 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1214 kB]\n",
      "Get:15 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [3651 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [3821 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [33.5 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1512 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [28.6 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]\n",
      "Fetched 18.7 MB in 2s (9962 kB/s)\n",
      "Reading package lists...\n",
      "\u001b[91mW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://packages.cloud.google.com/apt cloud-sdk InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY C0BA5CE6DC6315A3\n",
      "W: Failed to fetch https://packages.cloud.google.com/apt/dists/cloud-sdk/InRelease  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY C0BA5CE6DC6315A3\n",
      "W: Some index files failed to download. They have been ignored, or old ones used instead.\n",
      "\u001b[0mRemoving intermediate container 06fee02b0b2f\n",
      " ---> 4a580a3f8fe5\n",
      "Step 6/19 : RUN apt-get update -y &&     apt-get install -y curl gnupg telnet nano net-tools iputils-ping\n",
      " ---> Running in c592ee52cc21\n",
      "Get:1 https://packages.cloud.google.com/apt cloud-sdk InRelease [1616 B]\n",
      "Hit:2 http://packages.cloud.google.com/apt gcsfuse-focal InRelease\n",
      "Hit:3 https://packages.cloud.google.com/apt google-fast-socket InRelease\n",
      "Hit:4 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
      "Hit:5 http://archive.ubuntu.com/ubuntu focal InRelease\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease\n",
      "Err:1 https://packages.cloud.google.com/apt cloud-sdk InRelease\n",
      "  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY C0BA5CE6DC6315A3\n",
      "Hit:7 http://archive.ubuntu.com/ubuntu focal-backports InRelease\n",
      "Reading package lists...\n",
      "\u001b[91mW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://packages.cloud.google.com/apt cloud-sdk InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY C0BA5CE6DC6315A3\n",
      "W: Failed to fetch https://packages.cloud.google.com/apt/dists/cloud-sdk/InRelease  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY C0BA5CE6DC6315A3\n",
      "W: Some index files failed to download. They have been ignored, or old ones used instead.\n",
      "\u001b[0mReading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "gnupg is already the newest version (2.2.19-3ubuntu2.2).\n",
      "The following additional packages will be installed:\n",
      "  libcurl4 libcurl4-openssl-dev netbase\n",
      "Suggested packages:\n",
      "  libcurl4-doc libidn11-dev libkrb5-dev libldap2-dev librtmp-dev libssh2-1-dev\n",
      "  libssl-dev hunspell\n",
      "The following NEW packages will be installed:\n",
      "  iputils-ping nano net-tools netbase telnet\n",
      "The following packages will be upgraded:\n",
      "  curl libcurl4 libcurl4-openssl-dev\n",
      "3 upgraded, 5 newly installed, 0 to remove and 168 not upgraded.\n",
      "Need to get 1301 kB of archives.\n",
      "After this operation, 2053 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 iputils-ping amd64 3:20190709-3ubuntu1 [40.0 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal/main amd64 netbase all 6.1 [13.1 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal/main amd64 nano amd64 4.8-1ubuntu1 [269 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal/main amd64 telnet amd64 0.17-41.2build1 [64.0 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libcurl4-openssl-dev amd64 7.68.0-1ubuntu2.22 [322 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 curl amd64 7.68.0-1ubuntu2.22 [161 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libcurl4 amd64 7.68.0-1ubuntu2.22 [235 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal/main amd64 net-tools amd64 1.60+git20180626.aebd88e-1ubuntu1 [196 kB]\n",
      "\u001b[91mdebconf: unable to initialize frontend: Dialog\n",
      "debconf: (TERM is not set, so the dialog frontend is not usable.)\n",
      "debconf: falling back to frontend: Readline\n",
      "\u001b[0m\u001b[91mdebconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "\u001b[0m\u001b[91mdpkg-preconfigure: unable to re-open stdin: \n",
      "\u001b[0mFetched 1301 kB in 0s (4076 kB/s)\n",
      "Selecting previously unselected package iputils-ping.\n",
      "(Reading database ... 90761 files and directories currently installed.)\n",
      "Preparing to unpack .../0-iputils-ping_3%3a20190709-3ubuntu1_amd64.deb ...\n",
      "Unpacking iputils-ping (3:20190709-3ubuntu1) ...\n",
      "Selecting previously unselected package netbase.\n",
      "Preparing to unpack .../1-netbase_6.1_all.deb ...\n",
      "Unpacking netbase (6.1) ...\n",
      "Selecting previously unselected package nano.\n",
      "Preparing to unpack .../2-nano_4.8-1ubuntu1_amd64.deb ...\n",
      "Unpacking nano (4.8-1ubuntu1) ...\n",
      "Selecting previously unselected package telnet.\n",
      "Preparing to unpack .../3-telnet_0.17-41.2build1_amd64.deb ...\n",
      "Unpacking telnet (0.17-41.2build1) ...\n",
      "Preparing to unpack .../4-libcurl4-openssl-dev_7.68.0-1ubuntu2.22_amd64.deb ...\n",
      "Unpacking libcurl4-openssl-dev:amd64 (7.68.0-1ubuntu2.22) over (7.68.0-1ubuntu2.14) ...\n",
      "Preparing to unpack .../5-curl_7.68.0-1ubuntu2.22_amd64.deb ...\n",
      "Unpacking curl (7.68.0-1ubuntu2.22) over (7.68.0-1ubuntu2.14) ...\n",
      "Preparing to unpack .../6-libcurl4_7.68.0-1ubuntu2.22_amd64.deb ...\n",
      "Unpacking libcurl4:amd64 (7.68.0-1ubuntu2.22) over (7.68.0-1ubuntu2.14) ...\n",
      "Selecting previously unselected package net-tools.\n",
      "Preparing to unpack .../7-net-tools_1.60+git20180626.aebd88e-1ubuntu1_amd64.deb ...\n",
      "Unpacking net-tools (1.60+git20180626.aebd88e-1ubuntu1) ...\n",
      "Setting up net-tools (1.60+git20180626.aebd88e-1ubuntu1) ...\n",
      "Setting up nano (4.8-1ubuntu1) ...\n",
      "update-alternatives: using /bin/nano to provide /usr/bin/editor (editor) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/editor.1.gz because associated file /usr/share/man/man1/nano.1.gz (of link group editor) doesn't exist\n",
      "update-alternatives: using /bin/nano to provide /usr/bin/pico (pico) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/pico.1.gz because associated file /usr/share/man/man1/nano.1.gz (of link group pico) doesn't exist\n",
      "Setting up libcurl4:amd64 (7.68.0-1ubuntu2.22) ...\n",
      "Setting up netbase (6.1) ...\n",
      "Setting up curl (7.68.0-1ubuntu2.22) ...\n",
      "Setting up iputils-ping (3:20190709-3ubuntu1) ...\n",
      "Setting up telnet (0.17-41.2build1) ...\n",
      "update-alternatives: using /usr/bin/telnet.netkit to provide /usr/bin/telnet (telnet) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/telnet.1.gz because associated file /usr/share/man/man1/telnet.netkit.1.gz (of link group telnet) doesn't exist\n",
      "Setting up libcurl4-openssl-dev:amd64 (7.68.0-1ubuntu2.22) ...\n",
      "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
      "Processing triggers for man-db (2.9.1-1) ...\n",
      "Processing triggers for install-info (6.7.0.dfsg.2-5) ...\n",
      "Removing intermediate container c592ee52cc21\n",
      " ---> 3569f690cf0f\n",
      "Step 7/19 : ARG ETCD_VER=v2.3.0\n",
      " ---> Running in 647ea687f609\n",
      "Removing intermediate container 647ea687f609\n",
      " ---> 7a16a02b8026\n",
      "Step 8/19 : ARG GOOGLE_URL=https://storage.googleapis.com/etcd\n",
      " ---> Running in cffdd7772ff1\n",
      "Removing intermediate container cffdd7772ff1\n",
      " ---> 94983e252b65\n",
      "Step 9/19 : ARG GITHUB_URL=https://github.com/etcd-io/etcd/releases/download\n",
      " ---> Running in 89738730860d\n",
      "Removing intermediate container 89738730860d\n",
      " ---> 6c9079c18c71\n",
      "Step 10/19 : ARG DOWNLOAD_URL=$GOOGLE_URL\n",
      " ---> Running in 2a0be26e8c21\n",
      "Removing intermediate container 2a0be26e8c21\n",
      " ---> c140f6e4ee45\n",
      "Step 11/19 : RUN mkdir -p /tmp/etcd-download-test &&     curl -L ${DOWNLOAD_URL}/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz -o /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz &&     tar xzvf /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz -C /tmp/etcd-download-test --strip-components=1 &&     rm -f /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz\n",
      " ---> Running in 11dbc3c50cdb\n",
      "\u001b[91m  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 7946k  100 7946k    0     0  23.7M      0 --:--:-- --:--:-- --:--:-- 23.8M\u001b[0m\u001b[91m\n",
      "\u001b[0metcd-v2.3.0-linux-amd64/Documentation/\n",
      "etcd-v2.3.0-linux-amd64/Documentation/runtime-configuration.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/admin_guide.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/tuning.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/glossary.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/rfc/\n",
      "etcd-v2.3.0-linux-amd64/Documentation/rfc/v3api.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/discovery_protocol.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/errorcode.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/metrics.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/security.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/configuration.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/docker_guide.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/dev/\n",
      "etcd-v2.3.0-linux-amd64/Documentation/dev/release.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/members_api.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/auth_api.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/backward_compatibility.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/platforms/\n",
      "etcd-v2.3.0-linux-amd64/Documentation/platforms/freebsd.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/libraries-and-tools.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/implementation-faq.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/reporting_bugs.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/upgrade_2_2.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/internal-protocol-versioning.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/upgrade_2_1.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/faq.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/api_v3.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/runtime-reconf-design.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/clustering.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/proxy.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/branch_management.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/other_apis.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/benchmarks/\n",
      "etcd-v2.3.0-linux-amd64/Documentation/benchmarks/etcd-2-2-0-rc-memory-benchmarks.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/benchmarks/etcd-3-demo-benchmarks.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/benchmarks/etcd-2-2-0-benchmarks.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/benchmarks/etcd-2-1-0-alpha-benchmarks.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/benchmarks/etcd-storage-memory-benchmark.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/benchmarks/README.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/benchmarks/etcd-2-2-0-rc-benchmarks.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/benchmarks/etcd-3-watch-memory-benchmark.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/api.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/authentication.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/04_to_2_snapshot_migration.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/upgrade_2_3.md\n",
      "etcd-v2.3.0-linux-amd64/Documentation/production-users.md\n",
      "etcd-v2.3.0-linux-amd64/README-etcdctl.md\n",
      "etcd-v2.3.0-linux-amd64/etcdctl\n",
      "etcd-v2.3.0-linux-amd64/etcd\n",
      "etcd-v2.3.0-linux-amd64/README.md\n",
      "Removing intermediate container 11dbc3c50cdb\n",
      " ---> 812303d8dda9\n",
      "Step 12/19 : COPY . /trainer\n",
      " ---> ead0ef20c009\n",
      "Step 13/19 : WORKDIR /trainer\n",
      " ---> Running in d2239a9ad2d1\n",
      "Removing intermediate container d2239a9ad2d1\n",
      " ---> 082ff95be981\n",
      "Step 14/19 : RUN pip install -r requirements.txt\n",
      " ---> Running in 9bee6f53793a\n",
      "Requirement already satisfied: torch==1.13.0 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (1.13.0)\n",
      "Collecting torchvision==0.14.0\n",
      "  Downloading torchvision-0.14.0-cp37-cp37m-manylinux1_x86_64.whl (24.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.3/24.3 MB 56.4 MB/s eta 0:00:00\n",
      "Collecting tensorboard==2.5.0\n",
      "  Downloading tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.0/6.0 MB 56.5 MB/s eta 0:00:00\n",
      "Collecting protobuf==3.20.*\n",
      "  Downloading protobuf-3.20.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 62.3 MB/s eta 0:00:00\n",
      "Collecting python-etcd\n",
      "  Downloading python-etcd-0.4.5.tar.gz (37 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting python-json-logger\n",
      "  Downloading python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch==1.13.0->-r requirements.txt (line 1)) (11.7.99)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.13.0->-r requirements.txt (line 1)) (4.4.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.7/site-packages (from torch==1.13.0->-r requirements.txt (line 1)) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.7/site-packages (from torch==1.13.0->-r requirements.txt (line 1)) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch==1.13.0->-r requirements.txt (line 1)) (11.7.99)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision==0.14.0->-r requirements.txt (line 2)) (9.3.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision==0.14.0->-r requirements.txt (line 2)) (2.28.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision==0.14.0->-r requirements.txt (line 2)) (1.21.6)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorboard==2.5.0->-r requirements.txt (line 3)) (0.38.4)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.3/781.3 kB 51.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard==2.5.0->-r requirements.txt (line 3)) (65.5.1)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 152.9/152.9 kB 19.9 MB/s eta 0:00:00\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.9/4.9 MB 70.9 MB/s eta 0:00:00\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 233.6/233.6 kB 26.3 MB/s eta 0:00:00\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.7/site-packages (from tensorboard==2.5.0->-r requirements.txt (line 3)) (1.3.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94.2/94.2 kB 11.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard==2.5.0->-r requirements.txt (line 3)) (1.51.1)\n",
      "Requirement already satisfied: urllib3>=1.7.1 in /opt/conda/lib/python3.7/site-packages (from python-etcd->-r requirements.txt (line 5)) (1.26.13)\n",
      "Collecting dnspython>=1.13.0\n",
      "  Downloading dnspython-2.3.0-py3-none-any.whl (283 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 283.7/283.7 kB 27.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard==2.5.0->-r requirements.txt (line 3)) (4.9)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard==2.5.0->-r requirements.txt (line 3)) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard==2.5.0->-r requirements.txt (line 3)) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.5.0->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard==2.5.0->-r requirements.txt (line 3)) (5.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.14.0->-r requirements.txt (line 2)) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.14.0->-r requirements.txt (line 2)) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.14.0->-r requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard==2.5.0->-r requirements.txt (line 3)) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==2.5.0->-r requirements.txt (line 3)) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard==2.5.0->-r requirements.txt (line 3)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.5.0->-r requirements.txt (line 3)) (3.2.2)\n",
      "Building wheels for collected packages: python-etcd\n",
      "  Building wheel for python-etcd (setup.py): started\n",
      "  Building wheel for python-etcd (setup.py): finished with status 'done'\n",
      "  Created wheel for python-etcd: filename=python_etcd-0.4.5-py3-none-any.whl size=38483 sha256=64c14216732af9e4666de2e99b9ed21ae90654ecde5a48859efb217737783bcd\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/62/c9/cecc0af9a7946f154dfed012432cc43f030977a30e879f823e\n",
      "Successfully built python-etcd\n",
      "Installing collected packages: tensorboard-plugin-wit, werkzeug, tensorboard-data-server, python-json-logger, protobuf, dnspython, cachetools, python-etcd, markdown, google-auth, google-auth-oauthlib, torchvision, tensorboard\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.6\n",
      "    Uninstalling protobuf-3.19.6:\n",
      "      Successfully uninstalled protobuf-3.19.6\n",
      "  Attempting uninstall: cachetools\n",
      "    Found existing installation: cachetools 5.2.0\n",
      "    Uninstalling cachetools-5.2.0:\n",
      "      Successfully uninstalled cachetools-5.2.0\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.15.0\n",
      "    Uninstalling google-auth-2.15.0:\n",
      "      Successfully uninstalled google-auth-2.15.0\n",
      "  Attempting uninstall: google-auth-oauthlib\n",
      "    Found existing installation: google-auth-oauthlib 0.8.0\n",
      "    Uninstalling google-auth-oauthlib-0.8.0:\n",
      "      Successfully uninstalled google-auth-oauthlib-0.8.0\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.13.1+cu113\n",
      "    Uninstalling torchvision-0.13.1+cu113:\n",
      "      Successfully uninstalled torchvision-0.13.1+cu113\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorboardx 2.5.1 requires protobuf<=3.20.1,>=3.8.0, but you have protobuf 3.20.3 which is incompatible.\n",
      "google-cloud-bigquery 3.4.1 requires packaging<22.0.0dev,>=14.3, but you have packaging 22.0 which is incompatible.\n",
      "\u001b[0mSuccessfully installed cachetools-4.2.4 dnspython-2.3.0 google-auth-1.35.0 google-auth-oauthlib-0.4.6 markdown-3.4.4 protobuf-3.20.3 python-etcd-0.4.5 python-json-logger-2.0.7 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 torchvision-0.14.0 werkzeug-2.2.3\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 9bee6f53793a\n",
      " ---> 1fb5d074a008\n",
      "Step 15/19 : RUN chmod 777 main.sh\n",
      " ---> Running in 3b26f61020af\n",
      "Removing intermediate container 3b26f61020af\n",
      " ---> 98fb2c449cd9\n",
      "Step 16/19 : RUN wget -q -P /trainer/data https://image-net.org/data/tiny-imagenet-200.zip\n",
      " ---> Running in 82bab431a04b\n",
      "Removing intermediate container 82bab431a04b\n",
      " ---> 7db09ec66608\n",
      "Step 17/19 : RUN unzip -q /trainer/data/tiny-imagenet-200.zip\n",
      " ---> Running in 6deec3466133\n",
      "Removing intermediate container 6deec3466133\n",
      " ---> 0c943297079f\n",
      "Step 18/19 : RUN rm /trainer/data/tiny-imagenet-200.zip\n",
      " ---> Running in 305e015fb6d6\n",
      "Removing intermediate container 305e015fb6d6\n",
      " ---> e82dfbd38c24\n",
      "Step 19/19 : CMD [\"/bin/bash\", \"main.sh\"]\n",
      " ---> Running in f18afe2e55b0\n",
      "Removing intermediate container f18afe2e55b0\n",
      " ---> 8c7a2e01466a\n",
      "Successfully built 8c7a2e01466a\n",
      "Successfully tagged us-central1-docker.pkg.dev/vertexai-service-project/torchrun-imagenet-repo/pytorch-torchrun-imagenet-multi-node-gpu:latest\n",
      "PUSH\n",
      "Pushing us-central1-docker.pkg.dev/vertexai-service-project/torchrun-imagenet-repo/pytorch-torchrun-imagenet-multi-node-gpu:latest\n",
      "The push refers to repository [us-central1-docker.pkg.dev/vertexai-service-project/torchrun-imagenet-repo/pytorch-torchrun-imagenet-multi-node-gpu]\n",
      "52412e7d53c7: Preparing\n",
      "2bd19354a27e: Preparing\n",
      "60575184de11: Preparing\n",
      "ca9fc8b23236: Preparing\n",
      "35707bcced3f: Preparing\n",
      "001136dc3978: Preparing\n",
      "f1ffdb5563de: Preparing\n",
      "5df8b4cc645c: Preparing\n",
      "3ceb32ba401d: Preparing\n",
      "2bc882d812d1: Preparing\n",
      "4d43d6de7c14: Preparing\n",
      "128e38bc512c: Preparing\n",
      "e15faef6b0c6: Preparing\n",
      "55a8be34ac87: Preparing\n",
      "17d3bfcc5741: Preparing\n",
      "739e5e43a72a: Preparing\n",
      "7e66bd8b5b8d: Preparing\n",
      "10535f2fe91d: Preparing\n",
      "38aaf0adbb42: Preparing\n",
      "9192d6b86332: Preparing\n",
      "2b55ab636d0a: Preparing\n",
      "f14c2d2eb39e: Preparing\n",
      "060f5a6593f6: Preparing\n",
      "74115b901496: Preparing\n",
      "81ac346eb187: Preparing\n",
      "7365f548d2be: Preparing\n",
      "8d14dba85309: Preparing\n",
      "fc09c92af34e: Preparing\n",
      "6e36bffef97d: Preparing\n",
      "31a4ee0dfc1f: Preparing\n",
      "988e9d26d6ee: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "b763c1c026b0: Preparing\n",
      "9f24c36cbd64: Preparing\n",
      "3ceb32ba401d: Waiting\n",
      "99799594069a: Preparing\n",
      "2bc882d812d1: Waiting\n",
      "d812321bae2e: Preparing\n",
      "e39fe845e186: Preparing\n",
      "4d43d6de7c14: Waiting\n",
      "0c0255fe70c0: Preparing\n",
      "3f68638ae737: Preparing\n",
      "128e38bc512c: Waiting\n",
      "6aa81253de72: Preparing\n",
      "7e66bd8b5b8d: Waiting\n",
      "25e27b6ba1ab: Preparing\n",
      "2e699e937b48: Preparing\n",
      "3f09125d08e2: Preparing\n",
      "10535f2fe91d: Waiting\n",
      "e15faef6b0c6: Waiting\n",
      "f4462d5b2da2: Preparing\n",
      "55a8be34ac87: Waiting\n",
      "060f5a6593f6: Waiting\n",
      "7365f548d2be: Waiting\n",
      "17d3bfcc5741: Waiting\n",
      "74115b901496: Waiting\n",
      "8d14dba85309: Waiting\n",
      "739e5e43a72a: Waiting\n",
      "fc09c92af34e: Waiting\n",
      "81ac346eb187: Waiting\n",
      "6e36bffef97d: Waiting\n",
      "38aaf0adbb42: Waiting\n",
      "2b55ab636d0a: Waiting\n",
      "31a4ee0dfc1f: Waiting\n",
      "9192d6b86332: Waiting\n",
      "988e9d26d6ee: Waiting\n",
      "f14c2d2eb39e: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "b763c1c026b0: Waiting\n",
      "9f24c36cbd64: Waiting\n",
      "99799594069a: Waiting\n",
      "d812321bae2e: Waiting\n",
      "e39fe845e186: Waiting\n",
      "0c0255fe70c0: Waiting\n",
      "3f68638ae737: Waiting\n",
      "6aa81253de72: Waiting\n",
      "f1ffdb5563de: Waiting\n",
      "25e27b6ba1ab: Waiting\n",
      "001136dc3978: Waiting\n",
      "5df8b4cc645c: Waiting\n",
      "2e699e937b48: Waiting\n",
      "f4462d5b2da2: Waiting\n",
      "52412e7d53c7: Pushed\n",
      "ca9fc8b23236: Pushed\n",
      "001136dc3978: Pushed\n",
      "5df8b4cc645c: Pushed\n",
      "f1ffdb5563de: Pushed\n",
      "2bc882d812d1: Pushed\n",
      "4d43d6de7c14: Pushed\n",
      "128e38bc512c: Pushed\n",
      "3ceb32ba401d: Pushed\n",
      "e15faef6b0c6: Pushed\n",
      "17d3bfcc5741: Pushed\n",
      "739e5e43a72a: Pushed\n",
      "7e66bd8b5b8d: Pushed\n",
      "10535f2fe91d: Pushed\n",
      "38aaf0adbb42: Pushed\n",
      "9192d6b86332: Pushed\n",
      "2b55ab636d0a: Pushed\n",
      "f14c2d2eb39e: Pushed\n",
      "060f5a6593f6: Pushed\n",
      "74115b901496: Pushed\n",
      "35707bcced3f: Pushed\n",
      "7365f548d2be: Pushed\n",
      "8d14dba85309: Pushed\n",
      "fc09c92af34e: Pushed\n",
      "6e36bffef97d: Pushed\n",
      "60575184de11: Pushed\n",
      "2bd19354a27e: Pushed\n",
      "5f70bf18a086: Layer already exists\n",
      "b763c1c026b0: Pushed\n",
      "9f24c36cbd64: Pushed\n",
      "81ac346eb187: Pushed\n",
      "d812321bae2e: Pushed\n",
      "31a4ee0dfc1f: Pushed\n",
      "0c0255fe70c0: Pushed\n",
      "988e9d26d6ee: Pushed\n",
      "6aa81253de72: Pushed\n",
      "25e27b6ba1ab: Pushed\n",
      "2e699e937b48: Pushed\n",
      "3f09125d08e2: Pushed\n",
      "f4462d5b2da2: Pushed\n",
      "3f68638ae737: Pushed\n",
      "e39fe845e186: Pushed\n",
      "99799594069a: Pushed\n",
      "55a8be34ac87: Pushed\n",
      "latest: digest: sha256:4131efdc69e4c38bd785251293b073b8b4baa89a3292f1b5b51f963ee5b2701b size: 9553\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                  IMAGES                                                                                                                         STATUS\n",
      "cb3d5135-a728-4a5f-bb4e-8213f8e99259  2024-06-25T09:41:07+00:00  10M37S    gs://vertexai-service-project_cloudbuild/source/1719308467.317051-f8a6f615c0bc46478be1dd0c2aeb87ba.tgz  us-central1-docker.pkg.dev/vertexai-service-project/torchrun-imagenet-repo/pytorch-torchrun-imagenet-multi-node-gpu (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit \\\n",
    "   --region $LOCATION \\\n",
    "   --tag $custom_container_host_image_uri \\\n",
    "   --timeout \"2h\" \\\n",
    "   --machine-type=e2-highcpu-32 \\\n",
    "   trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4a87f56e869a"
   },
   "source": [
    "### Run training on Vertex AI using `torchrun` with ETCD on host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "63ca30b33176",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "BUCKET_NAME = BUCKET_URI.replace(\"gs://\", \"\")\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "PRIMARY_COMPUTE = \"n1-highmem-16\"\n",
    "TRAIN_COMPUTE = \"n1-highmem-16\"\n",
    "NUM_CPUS = 14  # Set to a few less than max CPUs per instance for paralle data loading\n",
    "TRAIN_GPU = \"NVIDIA_TESLA_T4\"\n",
    "TRAIN_NGPU = 1\n",
    "BATCH_SIZE = 512\n",
    "REPLICAS = 2\n",
    "EPOCHS = 5\n",
    "ARCH = \"resnet18\"\n",
    "BACKEND = \"nccl\"  # gloo for CPU only, nccl for GPUs\n",
    "TRAIN_DATA_LOCATION = (\n",
    "    \"/trainer/tiny-imagenet-200\"  # Data location of filed downloaded in Dockerfile\n",
    ")\n",
    "\n",
    "display_name = (\n",
    "    CONTAINER_NAME\n",
    "    + \"-LOCAL-ETCD-\"\n",
    "    + f\"{REPLICAS}workers-{TRAIN_NGPU}{TRAIN_GPU}-{BATCH_SIZE}batch-\"\n",
    "    + TIMESTAMP\n",
    ")\n",
    "gcs_output_uri_prefix = f\"{BUCKET_URI}/{display_name}\"\n",
    "\n",
    "RDZV_BACKEND = \"etcd-v2\"\n",
    "RDZV_BACKEND_STORE = f\"/gcs/{BUCKET_NAME}/sharedfile-{display_name}\"\n",
    "RDZV_ENDPOINT = \"localhost:2379\"\n",
    "\n",
    "# Use letters for each parameter to be processed in the shell script\n",
    "\"\"\"\n",
    "e)epochs=${OPTARG};;\n",
    "a)arch=${OPTARG};;\n",
    "b)batchsize=${OPTARG};;\n",
    "d)distbackend=${OPTARG};;\n",
    "t)data=${OPTARG};;\n",
    "w)workers=${OPTARG};;\n",
    "v)env=${OPTARG};;\n",
    "u)rdvzbackend=${OPTARG};;\n",
    "i)rdvzid=${OPTARG};;\n",
    "p)endpoint=${OPTARG};;\n",
    "n)nnodes=${OPTARG};;\n",
    "r)nprocpernode=${OPTARG};;\n",
    "c)ischief=${OPTARG};;\n",
    "\"\"\"\n",
    "\n",
    "CONTAINER_SPEC = {\n",
    "    \"image_uri\": custom_container_host_image_uri,\n",
    "    \"command\": [\n",
    "        \"/bin/bash\",\n",
    "        \"main.sh\",\n",
    "        f\"-e {EPOCHS}\",\n",
    "        f\"-a {ARCH}\",\n",
    "        f\"-b {BATCH_SIZE}\",\n",
    "        f\"-d {BACKEND}\",\n",
    "        f\"-t {TRAIN_DATA_LOCATION}\",\n",
    "        f\"-w {NUM_CPUS}\",\n",
    "        f\"-v {RDZV_BACKEND_STORE}\",\n",
    "        f\"-u {RDZV_BACKEND}\",\n",
    "        f\"-i {display_name}\",\n",
    "        f\"-p {RDZV_ENDPOINT}\",\n",
    "        f\"-n {REPLICAS+1}\",\n",
    "        f\"-r {TRAIN_NGPU}\",\n",
    "        \"-c y\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "CONTAINER_WORKER_SPEC = {\n",
    "    \"image_uri\": custom_container_host_image_uri,\n",
    "    \"command\": [\n",
    "        \"/bin/bash\",\n",
    "        \"main.sh\",\n",
    "        f\"-e {EPOCHS}\",\n",
    "        f\"-a {ARCH}\",\n",
    "        f\"-b {BATCH_SIZE}\",\n",
    "        f\"-d {BACKEND}\",\n",
    "        f\"-t {TRAIN_DATA_LOCATION}\",\n",
    "        f\"-w {NUM_CPUS}\",\n",
    "        f\"-v {RDZV_BACKEND_STORE}\",\n",
    "        f\"-u {RDZV_BACKEND}\",\n",
    "        f\"-i {display_name}\",\n",
    "        f\"-p {RDZV_ENDPOINT}\",\n",
    "        f\"-n {REPLICAS+1}\",\n",
    "        f\"-r {TRAIN_NGPU}\",\n",
    "        \"-c n\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "PRIMARY_WORKER_POOL = {\n",
    "    \"replica_count\": 1,\n",
    "    \"machine_spec\": {\n",
    "        \"machine_type\": PRIMARY_COMPUTE,\n",
    "        \"accelerator_count\": TRAIN_NGPU,\n",
    "        \"accelerator_type\": TRAIN_GPU,\n",
    "    },\n",
    "    \"container_spec\": CONTAINER_SPEC,\n",
    "}\n",
    "\n",
    "WORKER_POOL_SPECS = [PRIMARY_WORKER_POOL]\n",
    "\n",
    "TRAIN_WORKER_POOL = {\n",
    "    \"replica_count\": REPLICAS,\n",
    "    \"machine_spec\": {\n",
    "        \"machine_type\": TRAIN_COMPUTE,\n",
    "        \"accelerator_count\": TRAIN_NGPU,\n",
    "        \"accelerator_type\": TRAIN_GPU,\n",
    "    },\n",
    "    \"container_spec\": CONTAINER_WORKER_SPEC,\n",
    "}\n",
    "\n",
    "WORKER_POOL_SPECS.append(TRAIN_WORKER_POOL)\n",
    "\n",
    "job = aiplatform.CustomJob(\n",
    "    display_name=display_name,\n",
    "    base_output_dir=gcs_output_uri_prefix,\n",
    "    worker_pool_specs=WORKER_POOL_SPECS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "2b4684fde1a6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating CustomJob\n",
      "CustomJob created. Resource name: projects/314481442207/locations/us-central1/customJobs/6190879144511799296\n",
      "To use this CustomJob in another session:\n",
      "custom_job = aiplatform.CustomJob.get('projects/314481442207/locations/us-central1/customJobs/6190879144511799296')\n",
      "View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/6190879144511799296?project=314481442207\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_QUEUED\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 access the interactive shell terminals for the custom job:\n",
      "workerpool1-1:\n",
      "ca1179138ff55c3b-dot-us-central1.aiplatform-training.googleusercontent.com\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 access the interactive shell terminals for the custom job:\n",
      "workerpool1-0:\n",
      "6267d90e3e2dcf51-dot-us-central1.aiplatform-training.googleusercontent.com\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 access the interactive shell terminals for the custom job:\n",
      "workerpool0-0:\n",
      "91211ff367292699-dot-us-central1.aiplatform-training.googleusercontent.com\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/6190879144511799296 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "CustomJob run completed. Resource name: projects/314481442207/locations/us-central1/customJobs/6190879144511799296\n"
     ]
    }
   ],
   "source": [
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    job.run(\n",
    "        sync=True\n",
    "        # comment out the line below to turn off interactive debug\n",
    "        ,\n",
    "        enable_web_access=True,\n",
    "        service_account=SERVICE_ACCOUNT,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "796de99ae13a"
   },
   "source": [
    "### Run training on Vertex AI using `torchrun` with ETCD on host and reduction server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "6815ecb07ad9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "BUCKET_NAME = BUCKET_URI.replace(\"gs://\", \"\")\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "PRIMARY_COMPUTE = \"n1-highmem-16\"\n",
    "TRAIN_COMPUTE = \"n1-highmem-16\"\n",
    "REDUCTION_COMPUTE = \"n1-highcpu-16\"\n",
    "NUM_CPUS = 14  # Set to a few less than max CPUs per instance for paralle data loading\n",
    "TRAIN_GPU = \"NVIDIA_TESLA_T4\"\n",
    "TRAIN_NGPU = 1\n",
    "BATCH_SIZE = 512\n",
    "REPLICAS = 2\n",
    "EPOCHS = 5\n",
    "ARCH = \"resnet18\"\n",
    "BACKEND = \"nccl\"  # gloo for CPU only, nccl for GPUs\n",
    "TRAIN_DATA_LOCATION = (\n",
    "    \"/trainer/tiny-imagenet-200\"  # Data location of filed downloaded in Dockerfile\n",
    ")\n",
    "\n",
    "\n",
    "display_name = (\n",
    "    CONTAINER_NAME\n",
    "    + \"-LOCAL-ETCD-reduc-server-\"\n",
    "    + f\"{REPLICAS}workers-{TRAIN_NGPU}{TRAIN_GPU}-{BATCH_SIZE}batch-\"\n",
    "    + TIMESTAMP\n",
    ")\n",
    "gcs_output_uri_prefix = f\"{BUCKET_URI}/{display_name}\"\n",
    "\n",
    "RDZV_BACKEND = \"etcd-v2\"\n",
    "RDZV_BACKEND_STORE = f\"/gcs/{BUCKET_NAME}/sharedfile-{display_name}\"\n",
    "RDZV_ENDPOINT = \"localhost:2379\"\n",
    "\n",
    "\n",
    "# Use letters for each parameter to be processed in the shell script\n",
    "\"\"\"\n",
    "e)epochs=${OPTARG};;\n",
    "a)arch=${OPTARG};;\n",
    "b)batchsize=${OPTARG};;\n",
    "d)distbackend=${OPTARG};;\n",
    "t)data=${OPTARG};;\n",
    "w)workers=${OPTARG};;\n",
    "v)env=${OPTARG};;\n",
    "u)rdvzbackend=${OPTARG};;\n",
    "i)rdvzid=${OPTARG};;\n",
    "p)endpoint=${OPTARG};;\n",
    "n)nnodes=${OPTARG};;\n",
    "r)nprocpernode=${OPTARG};;\n",
    "c)ischief=${OPTARG};;\n",
    "\"\"\"\n",
    "\n",
    "CONTAINER_SPEC = {\n",
    "    \"image_uri\": custom_container_host_image_uri,\n",
    "    \"command\": [\n",
    "        \"/bin/bash\",\n",
    "        \"main.sh\",\n",
    "        f\"-e {EPOCHS}\",\n",
    "        f\"-a {ARCH}\",\n",
    "        f\"-b {BATCH_SIZE}\",\n",
    "        f\"-d {BACKEND}\",\n",
    "        f\"-t {TRAIN_DATA_LOCATION}\",\n",
    "        f\"-w {NUM_CPUS}\",\n",
    "        f\"-v {RDZV_BACKEND_STORE}\",\n",
    "        f\"-u {RDZV_BACKEND}\",\n",
    "        f\"-i {display_name}\",\n",
    "        f\"-p {RDZV_ENDPOINT}\",\n",
    "        f\"-n {REPLICAS+1}\",\n",
    "        f\"-r {TRAIN_NGPU}\",\n",
    "        \"-c y\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "CONTAINER_WORKER_SPEC = {\n",
    "    \"image_uri\": custom_container_host_image_uri,\n",
    "    \"command\": [\n",
    "        \"/bin/bash\",\n",
    "        \"main.sh\",\n",
    "        f\"-e {EPOCHS}\",\n",
    "        f\"-a {ARCH}\",\n",
    "        f\"-b {BATCH_SIZE}\",\n",
    "        f\"-d {BACKEND}\",\n",
    "        f\"-t {TRAIN_DATA_LOCATION}\",\n",
    "        f\"-w {NUM_CPUS}\",\n",
    "        f\"-v {RDZV_BACKEND_STORE}\",\n",
    "        f\"-u {RDZV_BACKEND}\",\n",
    "        f\"-i {display_name}\",\n",
    "        f\"-p {RDZV_ENDPOINT}\",\n",
    "        f\"-n {REPLICAS+1}\",\n",
    "        f\"-r {TRAIN_NGPU}\",\n",
    "        \"-c n\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "PRIMARY_WORKER_POOL = {\n",
    "    \"replica_count\": 1,\n",
    "    \"machine_spec\": {\n",
    "        \"machine_type\": PRIMARY_COMPUTE,\n",
    "        \"accelerator_count\": TRAIN_NGPU,\n",
    "        \"accelerator_type\": TRAIN_GPU,\n",
    "    },\n",
    "    \"container_spec\": CONTAINER_SPEC,\n",
    "}\n",
    "\n",
    "WORKER_POOL_SPECS = [PRIMARY_WORKER_POOL]\n",
    "\n",
    "TRAIN_WORKER_POOL = {\n",
    "    \"replica_count\": REPLICAS,\n",
    "    \"machine_spec\": {\n",
    "        \"machine_type\": TRAIN_COMPUTE,\n",
    "        \"accelerator_count\": TRAIN_NGPU,\n",
    "        \"accelerator_type\": TRAIN_GPU,\n",
    "    },\n",
    "    \"container_spec\": CONTAINER_WORKER_SPEC,\n",
    "}\n",
    "\n",
    "WORKER_POOL_SPECS.append(TRAIN_WORKER_POOL)\n",
    "\n",
    "# Add Reduction Server worker pool\n",
    "REDUCTION_SERVER_REPLICAS = 3\n",
    "REDUCTION_SERVER_IMAGE_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest\"\n",
    ")\n",
    "\n",
    "CONTAINER_REDUCTION_SPEC = {\"image_uri\": REDUCTION_SERVER_IMAGE_URI}\n",
    "\n",
    "REDUCTION_WORKER_POOL = {\n",
    "    \"replica_count\": REDUCTION_SERVER_REPLICAS,\n",
    "    \"machine_spec\": {\n",
    "        \"machine_type\": REDUCTION_COMPUTE,\n",
    "    },\n",
    "    \"container_spec\": CONTAINER_REDUCTION_SPEC,\n",
    "}\n",
    "\n",
    "WORKER_POOL_SPECS.append(REDUCTION_WORKER_POOL)\n",
    "\n",
    "job = aiplatform.CustomJob(\n",
    "    display_name=display_name,\n",
    "    base_output_dir=gcs_output_uri_prefix,\n",
    "    worker_pool_specs=WORKER_POOL_SPECS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "d4591d0a861a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating CustomJob\n",
      "CustomJob created. Resource name: projects/314481442207/locations/us-central1/customJobs/5772888804096475136\n",
      "To use this CustomJob in another session:\n",
      "custom_job = aiplatform.CustomJob.get('projects/314481442207/locations/us-central1/customJobs/5772888804096475136')\n",
      "View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/5772888804096475136?project=314481442207\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_QUEUED\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 access the interactive shell terminals for the custom job:\n",
      "workerpool2-0:\n",
      "7d7f1770ad67f5e3-dot-us-central1.aiplatform-training.googleusercontent.com\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 access the interactive shell terminals for the custom job:\n",
      "workerpool1-0:\n",
      "402b86e6a47abc6e-dot-us-central1.aiplatform-training.googleusercontent.com\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 access the interactive shell terminals for the custom job:\n",
      "workerpool0-0:\n",
      "db4a346330891156-dot-us-central1.aiplatform-training.googleusercontent.com\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/314481442207/locations/us-central1/customJobs/5772888804096475136 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "CustomJob run completed. Resource name: projects/314481442207/locations/us-central1/customJobs/5772888804096475136\n"
     ]
    }
   ],
   "source": [
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    job.run(\n",
    "        sync=True\n",
    "        # comment out the line below to turn off interactive debug\n",
    "        ,\n",
    "        enable_web_access=True,\n",
    "        service_account=SERVICE_ACCOUNT,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "sx_vKniMq9ZX",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting CustomJob : projects/314481442207/locations/us-central1/customJobs/5772888804096475136\n",
      "CustomJob deleted. . Resource name: projects/314481442207/locations/us-central1/customJobs/5772888804096475136\n",
      "Deleting CustomJob resource: projects/314481442207/locations/us-central1/customJobs/5772888804096475136\n",
      "Delete CustomJob backing LRO: projects/314481442207/locations/us-central1/operations/6719704105297641472\n",
      "CustomJob resource projects/314481442207/locations/us-central1/customJobs/5772888804096475136 deleted.\n",
      "Deleting image us-central1-docker.pkg.dev/vertexai-service-project/torchrun-imagenet-repo/pytorch-torchrun-imagenet-multi-node-gpu including all tags\n",
      "Delete request issued.\n",
      "Waiting for operation [projects/vertexai-service-project/locations/us-central1/\n",
      "operations/4969c651-23e8-4de2-ab62-b27377f13e13] to complete...done.           \n",
      "Removing gs://k-bucket-vertexai-service-project-torchrun-custom/sharedfile-pytorch-torchrun-imagenet-multi-node-gpu-LOCAL-ETCD-2workers-1NVIDIA_TESLA_T4-512batch-20240625095233#1719310269326021...\n",
      "Removing gs://k-bucket-vertexai-service-project-torchrun-custom/sharedfile-pytorch-torchrun-imagenet-multi-node-gpu-LOCAL-ETCD-reduc-server-1workers-1NVIDIA_TESLA_T4-512batch-20240625101633#1719311067260053...\n",
      "/ [2/2 objects] 100% Done                                                       \n",
      "Operation completed over 2 objects.                                              \n",
      "Removing gs://k-bucket-vertexai-service-project-torchrun-custom/...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    job.delete()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "gar_images = ! gcloud artifacts docker images list $LOCATION-docker.pkg.dev/$PROJECT_ID/$REPOSITORY \\\n",
    "      --filter=\"package~\"$(echo $CONTAINER_NAME | sed 's/:.*//') \\\n",
    "      --format=\"get(package)\"\n",
    "\n",
    "delete_image = True\n",
    "try:\n",
    "    if delete_image or os.getenv(\"IS_TESTING\"):\n",
    "        for image in gar_images:\n",
    "            # delete only if image name starts with valid region\n",
    "            if image.startswith(f'{LOCATION}-docker.pkg.dev'):\n",
    "                print(f\"Deleting image {image} including all tags\")\n",
    "                ! gcloud artifacts docker images delete $image --delete-tags --quiet\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Delete Cloud Storage objects that were created\n",
    "delete_bucket = False\n",
    "if delete_bucket:\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sdk_pytorch_torchrun_custom_container_training_imagenet.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

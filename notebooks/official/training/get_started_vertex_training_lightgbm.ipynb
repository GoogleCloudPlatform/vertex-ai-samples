{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "copyright"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Get started with Vertex AI Training for LightGBM\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/training/get_started_vertex_training_lightgbm.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Ftraining%2Fget_started_vertex_training_lightgbm.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>    \n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/training/get_started_vertex_training_lightgbm.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/training/get_started_vertex_training_lightgbm.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview:mlops"
      },
      "source": [
        "## Overview\n",
        "\n",
        "\n",
        "This tutorial demonstrates how to use Vertex AI Training for training a LightGBM model.\n",
        "\n",
        "Learn more about [Custom training](https://cloud.google.com/vertex-ai/docs/training/custom-training).\n",
        "\n",
        "\n",
        "__NOTE__: This notebook is a revised version of a [notebook](https://github.com/RajeshThallam/vertex-ai-labs/blob/main/07-vertex-train-deploy-lightgbm/vertex-train-deploy-lightgbm-model.ipynb) from the [**vertex-ai-labs** public repo](https://github.com/RajeshThallam/vertex-ai-labs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "objective:mlops,stage2,get_started_vertex_training_xgboost"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to train a LightGBM custom model using the custom container method for Vertex AI Training.\n",
        "\n",
        "This tutorial uses the following Vertex AI services:\n",
        "\n",
        "- Vertex AI Training\n",
        "- Vertex AI Model Registry\n",
        "- Vertex AI Batch predictions\n",
        "- Vertex AI Online prediction\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Training using a Python package.\n",
        "- Save the model artifacts to Cloud Storage using GCSFuse.\n",
        "- Construct a FastAPI prediction server.\n",
        "- Construct a Dockerfile deployment image for the server.\n",
        "- Test the deployment image locally.\n",
        "- Create a Vertex AI model resource.\n",
        "- Run a batch prediction job.\n",
        "- Deploy the model to an endpoint and send online prediction requests.\n",
        "- Clean up the created resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset:iris,lcn"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset used for this tutorial is the [Iris dataset](https://www.tensorflow.org/datasets/catalog/iris) from [TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/overview). This dataset doesn't require any feature engineering. The version of the dataset in this tutorial is stored in a public Cloud Storage bucket. The trained model predicts the type of Iris flower species from a class of three species: setosa, virginica, or versicolor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de76bb18c85b"
      },
      "source": [
        "### Costs \n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "* Artifact Registry\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), and [Artifact Registry pricing](https://cloud.google.com/artifact-registry/pricing) and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0316df526f8"
      },
      "source": [
        "## Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_aip:mbsdk"
      },
      "source": [
        "### Install Vertex AI SDK for Python and other required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "! pip3 install --upgrade -q google-cloud-aiplatform \\\n",
        "                            tensorflow==2.17.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58707a750154"
      },
      "source": [
        "### Restart runtime (Colab only)\n",
        "\n",
        "To use the newly installed packages, you must restart the runtime on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f200f10a1da3"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee775571c2b5"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92e68cfc3a90"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "Authenticate your environment on Google Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46604f70e831"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WReHDGG5g0XY"
      },
      "source": [
        "### Set Google Cloud project information\n",
        "\n",
        "To run this tutorial, you must have an existing Google Cloud project. Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM1iC_MfAts1"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgPO1eR3CYjk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "Create a storage bucket to store intermediate artifacts such as datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzGDU7TWdts_"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_bucket"
      },
      "source": [
        "**If your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIq7R4HZCfIc"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_vars"
      },
      "source": [
        "### Import the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import google.cloud.aiplatform as aiplatform\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "To get started using Vertex AI, you must [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Initialize the Vertex AI SDK for Python using your project and Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gar_enable_api"
      },
      "source": [
        "### Enable Artifact Registry API\n",
        "\n",
        "Learn more about [Enabling service](https://cloud.google.com/artifact-registry/docs/enable-service)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gar_enable_api"
      },
      "outputs": [],
      "source": [
        "! gcloud services enable artifactregistry.googleapis.com\n",
        "\n",
        "if os.getenv(\"IS_TESTING\"):\n",
        "    ! sudo apt-get update --yes && sudo apt-get --only-upgrade --yes install google-cloud-sdk-cloud-run-proxy google-cloud-sdk-harbourbridge google-cloud-sdk-cbt google-cloud-sdk-gke-gcloud-auth-plugin google-cloud-sdk-kpt google-cloud-sdk-local-extract google-cloud-sdk-minikube google-cloud-sdk-app-engine-java google-cloud-sdk-app-engine-go google-cloud-sdk-app-engine-python google-cloud-sdk-spanner-emulator google-cloud-sdk-bigtable-emulator google-cloud-sdk-nomos google-cloud-sdk-package-go-module google-cloud-sdk-firestore-emulator kubectl google-cloud-sdk-datastore-emulator google-cloud-sdk-app-engine-python-extras google-cloud-sdk-cloud-build-local google-cloud-sdk-kubectl-oidc google-cloud-sdk-anthos-auth google-cloud-sdk-app-engine-grpc google-cloud-sdk-pubsub-emulator google-cloud-sdk-datalab google-cloud-sdk-skaffold google-cloud-sdk google-cloud-sdk-terraform-tools google-cloud-sdk-config-connector\n",
        "    ! gcloud components update --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gar_create_repo"
      },
      "source": [
        "## Create a private Docker repository\n",
        "\n",
        "Your first step is to create your own Docker repository in the Artifact Registry.\n",
        "\n",
        "1. Run the `gcloud artifacts repositories create` command to create a new Docker repository with your region with the description \"docker repository\".\n",
        "\n",
        "2. Run the `gcloud artifacts repositories list` command to verify that your repository is created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gar_create_repo"
      },
      "outputs": [],
      "source": [
        "# Set a display name for the app to use later\n",
        "APP_NAME = \"iris-classification\"\n",
        "# Set the name for your private repo\n",
        "PRIVATE_REPO = f\"{APP_NAME}-repo-unique\"\n",
        "# Create the repo\n",
        "! gcloud artifacts repositories create {PRIVATE_REPO} --repository-format=docker --location={LOCATION} --description=\"Prediction repository\"\n",
        "# List the repos and check if your repo is created\n",
        "! gcloud artifacts repositories list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gar_auth"
      },
      "source": [
        "## Configure authentication to your private repo\n",
        "\n",
        "Before you push or pull container images, configure Docker to use the `gcloud` command-line tool to authenticate requests to `Artifact Registry` for your region."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gar_auth"
      },
      "outputs": [],
      "source": [
        "! gcloud auth configure-docker {LOCATION}-docker.pkg.dev --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "container:training,prediction,xgboost"
      },
      "source": [
        "## Set container image paths\n",
        "\n",
        "Set the prebuilt Docker container image for training and custom container for predictions.\n",
        "\n",
        "\n",
        "For the latest list, see [Prebuilt containers for training](https://cloud.google.com/ai-platform-unified/docs/training/pre-built-containers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "container:training,prediction,xgboost"
      },
      "outputs": [],
      "source": [
        "# Set prebuilt image for training\n",
        "TRAIN_VERSION = \"scikit-learn-cpu.0-23\"\n",
        "TRAIN_IMAGE = \"{}-docker.pkg.dev/vertex-ai/training/{}:latest\".format(\n",
        "    LOCATION.split(\"-\")[0], TRAIN_VERSION\n",
        ")\n",
        "\n",
        "# Set prebuilt image for serving\n",
        "DEPLOY_VERSION = \"lightgbm-cpu\"\n",
        "DEPLOY_IMAGE = \"{}-docker.pkg.dev/{}/{}/{}:latest\".format(\n",
        "    LOCATION, PROJECT_ID, PRIVATE_REPO, DEPLOY_VERSION\n",
        ")\n",
        "print(\"Deploy image:\", DEPLOY_IMAGE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "machine:training,prediction"
      },
      "source": [
        "## Set machine type\n",
        "\n",
        "Next, set the machine type to use for training and prediction.\n",
        "\n",
        "- Set the variables `TRAIN_COMPUTE` and `DEPLOY_COMPUTE` to configure the compute resources(VMs) needed for training and prediction.\n",
        " - `machine type`\n",
        "     - `n1-standard`: 3.75GB of memory per vCPU\n",
        "     - `n1-highmem`: 6.5GB of memory per vCPU\n",
        "     - `n1-highcpu`: 0.9 GB of memory per vCPU\n",
        " - `vCPUs`: number of \\[2, 4, 8, 16, 32, 64, 96 \\]\n",
        "\n",
        "**Note:** The following isn't supported for training:\n",
        "\n",
        " - `standard`: 2 vCPUs\n",
        " - `highcpu`: 2, 4 and 8 vCPUs\n",
        "\n",
        "**Note:** You may also use n2 and e2 machine types for training and deployment, but they don't support GPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "machine:training,prediction"
      },
      "outputs": [],
      "source": [
        "MACHINE_TYPE = \"n1-standard\"\n",
        "\n",
        "VCPU = \"4\"\n",
        "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
        "print(\"Train machine type\", TRAIN_COMPUTE)\n",
        "\n",
        "VCPU = \"4\"\n",
        "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
        "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "031247f96c41"
      },
      "source": [
        "## Create the training package"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "examine_training_package:xgboost"
      },
      "source": [
        "### Package layout\n",
        "\n",
        "Before you start the training, take a look at how a Python package is assembled for a custom training job. When unarchived, the package contains the following directory layout.\n",
        "\n",
        "- PKG-INFO\n",
        "- README.md\n",
        "- setup.cfg\n",
        "- setup.py\n",
        "- trainer\n",
        "  - \\_\\_init\\_\\_.py\n",
        "  - task.py\n",
        "\n",
        "The files `setup.cfg` and `setup.py` include the instructions for installing the package into the operating environment of the Docker image.\n",
        "\n",
        "The file `trainer/task.py` is the Python script for executing the custom training job. \n",
        "\n",
        "**Note**: When referred to the file in the worker pool specification, the file suffix(`.py`) is dropped and the directory slash is replaced with a dot(`trainer.task`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6348aba91413"
      },
      "source": [
        "### Package Assembly\n",
        "\n",
        "In the following cells, you assemble the training package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4wS4eISox9V"
      },
      "outputs": [],
      "source": [
        "# Make folder for Python training script\n",
        "! rm -rf custom\n",
        "! mkdir custom\n",
        "\n",
        "# Add package information\n",
        "! touch custom/README.md\n",
        "\n",
        "setup_cfg = \"[egg_info]\\n\\ntag_build =\\n\\ntag_date = 0\"\n",
        "! echo \"$setup_cfg\" > custom/setup.cfg\n",
        "\n",
        "setup_py = \"import setuptools\\n\\nsetuptools.setup(\\n\\n    install_requires=[\\n\\n'lightgbm'    ],\\n\\n    packages=setuptools.find_packages())\"\n",
        "! echo \"$setup_py\" > custom/setup.py\n",
        "\n",
        "pkg_info = \"Metadata-Version: 1.0\\n\\nName: Iris tabular classification\\n\\nVersion: 0.0.0\\n\\nSummary: Demostration training script\\n\\nHome-page: www.google.com\\n\\nAuthor: Google\\n\\nAuthor-email: aferlitsch@google.com\\n\\nLicense: Public\\n\\nDescription: Demo\\n\\nPlatform: Vertex\"\n",
        "! echo \"$pkg_info\" > custom/PKG-INFO\n",
        "\n",
        "# Make the training subfolder\n",
        "! mkdir custom/trainer\n",
        "! touch custom/trainer/__init__.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taskpy_contents:iris,xgboost"
      },
      "source": [
        "### Create *task.py* script\n",
        "\n",
        "Next, you create the *task.py* script for running the training package. Some noteable steps include:\n",
        "\n",
        "- **Parse command-line arguments**: \n",
        "    - `model-dir`: The location to save the trained model. When using Vertex AI custom training, the location is specified through the environment variable: `AIP_MODEL_DIR`\n",
        "    \n",
        "- **Data preprocessing** (`get_data()`): Download the dataset and split it into training and test sets.\n",
        "    \n",
        "- **Training** (`train_model()`): Train the model.\n",
        "    \n",
        "- **Saving the model artifacts**: Save the model artifacts and evaluation metrics to the Cloud Storage location specified in `model-dir`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taskpy_contents:xgboost,iris"
      },
      "outputs": [],
      "source": [
        "%%writefile custom/trainer/task.py\n",
        "# Single Instance Training for Iris\n",
        "\n",
        "import datetime\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "import argparse\n",
        "import logging\n",
        "\n",
        "logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "logging.info(\"Parsing arguments\")\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\n",
        "    '--model-dir', \n",
        "    dest='model_dir',        \n",
        "    default=os.getenv('AIP_MODEL_DIR'), \n",
        "    type=str, \n",
        "    help='Location to export GCS model')\n",
        "args = parser.parse_args()\n",
        "logging.info(args)\n",
        "\n",
        "def get_data():\n",
        "    # Download data\n",
        "    logging.info(\"Downloading data\")\n",
        "    iris = load_iris()\n",
        "    print(iris.data.shape)\n",
        "\n",
        "    # split data\n",
        "    print(\"Splitting data into test and train\")\n",
        "    x, y = iris.data, iris.target\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=123)\n",
        "\n",
        "    # create dataset for lightgbm\n",
        "    print(\"creating dataset for LightGBM\")\n",
        "    lgb_train = lgb.Dataset(x_train, y_train)\n",
        "    lgb_eval = lgb.Dataset(x_test, y_test, reference=lgb_train)\n",
        "    \n",
        "    return lgb_train, lgb_eval\n",
        "\n",
        "def train_model(lgb_train, lg_eval):\n",
        "    # specify your configurations as a dict\n",
        "    params = {\n",
        "        'boosting_type': 'gbdt',\n",
        "        'objective': 'multiclass',\n",
        "        'metric': {'multi_error'},\n",
        "        'num_leaves': 31,\n",
        "        'learning_rate': 0.05,\n",
        "        'feature_fraction': 0.9,\n",
        "        'bagging_fraction': 0.8,\n",
        "        'bagging_freq': 5,\n",
        "        'verbose': 0,\n",
        "        'num_class' : 3\n",
        "    }\n",
        "\n",
        "    # train lightgbm model\n",
        "    logging.info('Starting training...')\n",
        "    model = lgb.train(params,\n",
        "                    lgb_train,\n",
        "                    num_boost_round=20,\n",
        "                    valid_sets=lgb_eval)\n",
        "    \n",
        "    return model\n",
        "\n",
        "lgb_train, lgb_eval = get_data()\n",
        "model = train_model(lgb_train, lgb_eval)\n",
        "\n",
        "# GCSFuse conversion\n",
        "gs_prefix = 'gs://'\n",
        "gcsfuse_prefix = '/gcs/'\n",
        "if args.model_dir.startswith(gs_prefix):\n",
        "    args.model_dir = args.model_dir.replace(gs_prefix, gcsfuse_prefix)\n",
        "    dirpath = os.path.split(args.model_dir)[0]\n",
        "    if not os.path.isdir(dirpath):\n",
        "        os.makedirs(dirpath)\n",
        "        \n",
        "# save model to file\n",
        "logging.info('Saving model...')\n",
        "model_filename = 'model.txt'\n",
        "gcs_model_path = os.path.join(args.model_dir, model_filename)\n",
        "model.save_model(gcs_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tarball_training_script"
      },
      "source": [
        "### Store training package in Cloud Storage bucket\n",
        "\n",
        "Next, package the training folder into a compressed tar ball, and then store it in your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnmdycf6ox9X"
      },
      "outputs": [],
      "source": [
        "# Remove any existing tar and zip files\n",
        "! rm -f custom.tar custom.tar.gz\n",
        "# Create a tar file\n",
        "! tar cvf custom.tar custom\n",
        "# Create a zip file\n",
        "! gzip custom.tar\n",
        "# Copy the package to Cloud Storage bucket\n",
        "! gsutil cp custom.tar.gz $BUCKET_URI/trainer_iris.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_custom_pp_training_job:mbsdk"
      },
      "source": [
        "## Create custom training job\n",
        "\n",
        "In this step, you create a custom training job using the `CustomTrainingJob` class, with the following parameters:\n",
        "\n",
        "- `display_name`: The human readable name for the custom training job.\n",
        "- `container_uri`: The training container image.\n",
        "\n",
        "- `python_package_gcs_uri`: The Cloud Storage location of the Python training package.\n",
        "- `python_module_name`: The relative path to the training script in the Python package.\n",
        "\n",
        "**Note:** For specifying any dependencies, use `install_requires` parameter in the *setup.py* script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVEMz1xqox9X"
      },
      "outputs": [],
      "source": [
        "# Set display name for training job\n",
        "DISPLAY_NAME = f\"{APP_NAME}-training-unique\"\n",
        "\n",
        "# Define the training job\n",
        "job = aiplatform.CustomPythonPackageTrainingJob(\n",
        "    display_name=DISPLAY_NAME,\n",
        "    python_package_gcs_uri=f\"{BUCKET_URI}/trainer_iris.tar.gz\",\n",
        "    python_module_name=\"trainer.task\",\n",
        "    container_uri=TRAIN_IMAGE,\n",
        "    project=PROJECT_ID,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prepare_custom_cmdargs:iris,xgboost"
      },
      "source": [
        "## Prepare your training arguments\n",
        "\n",
        "Now, define the parameters to run your custom training container:\n",
        "\n",
        "- `--model-dir`: Command-line argument to specify where to store the model artifacts. You can use either of the following methods to specify the storage location for artifacts.\n",
        "    - **method-1**(set *DIRECT* to True): You pass the Cloud Storage location as a command line argument to your training script.\n",
        "    - **method-2**(set *DIRECT* to False): You pass the Cloud Storage location as the environment variable `AIP_MODEL_DIR` to your training script. In this case, you provide the model artifact location in the job specification itself as `base_output_dir`.\n",
        "    \n",
        "**Note**: Depending on how you pass your model artifact location to the training job, the training task must be configured to receive the value. In this tutorial, the training task parses the `model_dir` argument and if no value is found, then it looks for `AIP_MODEL_DIR` that is set using `base_output_dir`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoUfpBqVox9Y"
      },
      "outputs": [],
      "source": [
        "MODEL_DIR = f\"{BUCKET_URI}/model\"\n",
        "\n",
        "DIRECT = False\n",
        "if DIRECT:\n",
        "    CMDARGS = [\n",
        "        \"--model_dir=\" + MODEL_DIR,\n",
        "    ]\n",
        "else:\n",
        "    CMDARGS = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_custom_job:mbsdk"
      },
      "source": [
        "## Run the custom training job\n",
        "\n",
        "Next, run the custom job to start training by invoking the `run` method, with the following parameters:\n",
        "\n",
        "- `args`: The command-line arguments to pass to the training script.\n",
        "- `replica_count`: The number of compute instances for training (replica_count = 1 is single node training).\n",
        "- `machine_type`: The machine type for the compute instances.\n",
        "- `accelerator_type`: The hardware accelerator type.\n",
        "- `accelerator_count`: The number of accelerators to attach to a worker replica.\n",
        "- `base_output_dir`: The Cloud Storage location to save the model artifacts.\n",
        "- `sync`: Set **True** to wait until the completion of the job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCruQq1aox9Y"
      },
      "outputs": [],
      "source": [
        "# Run the job\n",
        "job.run(\n",
        "    args=CMDARGS,\n",
        "    replica_count=1,\n",
        "    machine_type=TRAIN_COMPUTE,\n",
        "    base_output_dir=MODEL_DIR,\n",
        "    sync=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "custom_job_wait:mbsdk"
      },
      "source": [
        "### Wait for completion of custom training job\n",
        "\n",
        "Next, wait for the custom training job to complete. Alternatively, you can set the parameter `sync` to `True` in the `run()` method to block the job until it is completed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHPMHbSyox9Z"
      },
      "outputs": [],
      "source": [
        "job.wait()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2478e67d679f"
      },
      "source": [
        "### Verify the model artifacts\n",
        "\n",
        "Next, verify that the training script has successfully saved the trained model to your Cloud Storage location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c644f0d3a201"
      },
      "outputs": [],
      "source": [
        "# Set the path where model is saved\n",
        "model_path_to_deploy = MODEL_DIR + \"/model\"\n",
        "print(f\"Model path with trained model artifacts {model_path_to_deploy}\")\n",
        "\n",
        "# List the contents of the model folder\n",
        "! gsutil ls $model_path_to_deploy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_model:migration,new"
      },
      "source": [
        "## Build a server app using FastAPI\n",
        "\n",
        "Next, you use FastAPI to implement HTTP server as a custom deployment container. The container must listen and respond to liveness checks, health checks, and prediction requests. The HTTP server must listen for requests on **0.0.0.0**.\n",
        "\n",
        "Learn more about [deployment container requirements](https://cloud.google.com/ai-platform-unified/docs/predictions/custom-container-requirements#image) and [FastAPI](https://fastapi.tiangolo.com/).\n",
        "\n",
        "### Create a folder to store resources for your app\n",
        "Below, you create a folder called `serve` to store your artifacts for serving. Then, you create a subfolder called `app` to store your serving script.\n",
        "\n",
        "Finally, you follow the below folder structure to build your app:\n",
        "\n",
        "```\n",
        "    - serve\n",
        "        - app\n",
        "            - main.py\n",
        "            - prestart.sh\n",
        "        - requirements.txt\n",
        "        - Dockerfile\n",
        "        - instances.json\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "b28b53f8b3f4"
      },
      "outputs": [],
      "source": [
        "# Remove if the folder exists already\n",
        "! rm -rf serve\n",
        "\n",
        "# Create a new folder\n",
        "! mkdir serve\n",
        "\n",
        "# Create a subfolder for storing the serving scripts\n",
        "! mkdir serve/app"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8baa8361ecb4"
      },
      "source": [
        "### Create requirements file for the serving container\n",
        "\n",
        "Next, create the `requirements.txt` file which lists the Python packages needed for the serving container."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2dd3e671130"
      },
      "outputs": [],
      "source": [
        "%%writefile serve/requirements.txt\n",
        "numpy~=1.20\n",
        "scikit-learn~=0.24\n",
        "lightgbm==4.5.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07c16e1003ef"
      },
      "source": [
        "### Write the FastAPI serving script\n",
        "\n",
        "Next, you write the serving script for the HTTP server using `FastAPI`.\n",
        "\n",
        "The serving script consists of the following steps:\n",
        "\n",
        "- Instantiate a `FastAPI` application class.\n",
        "- Load the trained model from the artifacts.\n",
        "- Load the class names from the dataset (Iris dataset from Sklearn datasets) used for training.\n",
        "- Define `health()` method to address your app's [health check requests](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#health).\n",
        "- Define `predict()` method to return responses to the [prediction requests](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#prediction)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97b0cd77339c"
      },
      "outputs": [],
      "source": [
        "%%writefile serve/app/main.py\n",
        "from fastapi.logger import logger\n",
        "from fastapi import FastAPI, Request\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.datasets import load_iris\n",
        "import lightgbm as lgb\n",
        "import logging\n",
        "\n",
        "# Set logging\n",
        "gunicorn_logger = logging.getLogger('gunicorn.error')\n",
        "logger.handlers = gunicorn_logger.handlers\n",
        "\n",
        "if __name__ != \"main\":\n",
        "    logger.setLevel(gunicorn_logger.level)\n",
        "else:\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "\n",
        "# Create the server app\n",
        "app = FastAPI()\n",
        "\n",
        "# Load the model\n",
        "logger.info(\"Loading the model\")\n",
        "model = lgb.Booster(model_file=\"model/model.txt\")\n",
        "    \n",
        "# Load the class names\n",
        "logger.info(\"Loading the target class labels\")\n",
        "class_names = load_iris().target_names\n",
        "\n",
        "@app.get(os.environ['AIP_HEALTH_ROUTE'], status_code=200)\n",
        "def health():\n",
        "    \"\"\" health check to ensure HTTP server is ready to handle \n",
        "        prediction requests\n",
        "    \"\"\"\n",
        "    return {\"status\": \"healthy\"}\n",
        "\n",
        "@app.post(os.environ['AIP_PREDICT_ROUTE'])\n",
        "async def predict(request: Request):\n",
        "    # Asynchronous wait for HTTP requests\n",
        "    body = await request.json()\n",
        "    # Get the content of the prediction request\n",
        "    instances = body[\"instances\"]\n",
        "    # Reformat prediction request as a numpy array\n",
        "    inputs = np.asarray(instances)\n",
        "    # Invoke the model to make predictions\n",
        "    outputs = model.predict(inputs)\n",
        "    # Return formatted predictions as response\n",
        "    logger.info(f\"Outputs {outputs}\")\n",
        "    return {\"predictions\": [class_names[class_num] for class_num in np.argmax(outputs, axis=1)]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea37d0597e2b"
      },
      "source": [
        "### Add a pre-start script\n",
        "\n",
        "FastAPI executes the pre-start script before starting the server. The environment variable `PORT` is set equal to `AIP_HTTP_PORT` in order to run FastAPI on the same port expected by Vertex AI. Vertex AI sends liveness checks, health checks, and prediction requests to `AIP_HTTP_PORT` on the container. Your container's HTTP server must listen for requests on this port."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e029a2d85935"
      },
      "outputs": [],
      "source": [
        "%%writefile serve/app/prestart.sh\n",
        "#!/bin/bash\n",
        "export PORT=$AIP_HTTP_PORT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2d7a9a2c563"
      },
      "source": [
        "Set the variable `PORT` to use while running the app inside a container."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee0c523b51f0"
      },
      "outputs": [],
      "source": [
        "# Set the port for serving the app\n",
        "PORT = 7080"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a402c1f4bcc"
      },
      "source": [
        "### Store test instances\n",
        "\n",
        "Next, you create some examples to subsequently test the FastAPI server and the hosted LightGBM model.\n",
        "\n",
        "Learn more about [JSON formatting of prediction requests for custom models](https://cloud.google.com/ai-platform-unified/docs/predictions/online-predictions-custom-models#request-body-details)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d45c868ccaed"
      },
      "outputs": [],
      "source": [
        "%%writefile serve/instances.json\n",
        "{\n",
        "    \"instances\": [\n",
        "        [6.7, 3.1, 4.7, 1.5],\n",
        "        [4.6, 3.1, 1.5, 0.2]\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a7e749f188a"
      },
      "source": [
        "## Build the custom container image for serving\n",
        "\n",
        "In this section, you containerize your serving app and create an image for it. You use the image later while uploading your model to Vertex AI Model Registry."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ae1b8aead6c"
      },
      "source": [
        "### Create the Dockerfile\n",
        "\n",
        "Write the Dockerfile, using `tiangolo/uvicorn-gunicorn-fastapi:python3.9` as base image. This automatically runs FastAPI for you using Gunicorn and Uvicorn. \n",
        "\n",
        "Learn more about [Deploying FastAPI with Docker](https://fastapi.tiangolo.com/deployment/docker/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "289dcdf1c4f4"
      },
      "outputs": [],
      "source": [
        "%%bash -s $MODEL_DIR $PORT\n",
        "\n",
        "MODEL_DIR=$1\n",
        "PORT=$2\n",
        "mkdir -p ./serve/model/\n",
        "gsutil cp $MODEL_DIR/model/* ./serve/model/\n",
        "\n",
        "cat > ./serve/Dockerfile <<EOF\n",
        "# Load the base image\n",
        "FROM tiangolo/uvicorn-gunicorn-fastapi:python3.9\n",
        "# Set the workdir\n",
        "WORKDIR /app\n",
        "# Copy the required files and folders to the workdir\n",
        "COPY ./app /app\n",
        "COPY ./model /app/model\n",
        "COPY requirements.txt requirements.txt\n",
        "# Install the requirements\n",
        "RUN pip3 install -r requirements.txt\n",
        "# Expose the required port for running the server\n",
        "EXPOSE $PORT\n",
        "# Start the server\n",
        "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"$PORT\"]\n",
        "EOF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f3bca44ead4"
      },
      "source": [
        "### Build the container image locally\n",
        "\n",
        "Next, build your custom container image. While building the image, pass the tag as `DEPLOY_IMAGE` which is later interpreted as the repo path when pushed to Artifact Registry.\n",
        "\n",
        "**Note**: This step doesn't work in Colab. For building and pushing the image from a Colab notebook, skip to the __[Build and Push the image(for Colab users)](#colab-section)__ section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64261b86085e"
      },
      "outputs": [],
      "source": [
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "if not IS_COLAB:\n",
        "    ! docker build --tag={DEPLOY_IMAGE} ./serve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c746c507aba6"
      },
      "source": [
        "### Run and test the container locally (optional)\n",
        "\n",
        "Before you push the container image to Artifact Registry, you can run it as a container in your local environment to verify that the server works as expected.\n",
        "\n",
        "In this step, you run the container locally in detached mode(`-d`) and provide the environment variables needed for serving predictions. Then, you test the `/health` and `/predict` routes to make sure the container works as intended."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3246b3a01d92"
      },
      "outputs": [],
      "source": [
        "if not IS_COLAB:\n",
        "    # stop and remove if there's any pre-existing container\n",
        "    ! sudo docker stop local-iris\n",
        "    ! sudo docker rm local-iris\n",
        "    # create and run the container\n",
        "    ! docker run -t -d --rm -p {PORT}:{PORT} \\\n",
        "        --name=local-iris \\\n",
        "        -e AIP_HTTP_PORT={PORT} \\\n",
        "        -e AIP_HEALTH_ROUTE=/health \\\n",
        "        -e AIP_PREDICT_ROUTE=/predict \\\n",
        "        -e AIP_STORAGE_URI={MODEL_DIR} \\\n",
        "        {DEPLOY_IMAGE}\n",
        "    # list the containers and verify\n",
        "    ! docker container ls\n",
        "    # wait a few seconds to let the server ready\n",
        "    ! sleep 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bdfb565a253"
      },
      "source": [
        "#### Health check\n",
        "\n",
        "Send a health check request to container. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdc1f41f8dcc"
      },
      "outputs": [],
      "source": [
        "if not IS_COLAB:\n",
        "    ! curl http://localhost:{PORT}/health"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c92e4992b2f4"
      },
      "source": [
        "If successful, the server returns the following response:\n",
        "\n",
        "```\n",
        "{\n",
        "  \"status\": \"healthy\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5433a0c62a99"
      },
      "source": [
        "#### Prediction check\n",
        "\n",
        "Send a prediction request to the container."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "458de32ee979"
      },
      "outputs": [],
      "source": [
        "! curl -X POST \\\n",
        "  -d @serve/instances.json \\\n",
        "  -H \"Content-Type: application/json; charset=utf-8\" \\\n",
        "  http://localhost:{PORT}/predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db78dc1f2313"
      },
      "source": [
        "If successful, the server returns predictions in the below format:\n",
        "\n",
        "```\n",
        "{\"predictions\":[\"versicolor\",\"setosa\"]}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc16b5fa1b6d"
      },
      "source": [
        "#### Stop the container\n",
        "\n",
        "Finally, stop the local container."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8127f3e5b74"
      },
      "outputs": [],
      "source": [
        "if not IS_COLAB:\n",
        "    ! docker stop local-iris"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c57c65c5d25"
      },
      "source": [
        "#### Push the container image to Artifact Registry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f1fc336bf79"
      },
      "outputs": [],
      "source": [
        "if not IS_COLAB:\n",
        "    ! docker push $DEPLOY_IMAGE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f50e9c553fb7"
      },
      "source": [
        "<a id=\"colab-section\"></a>\n",
        "## Build and push the image (for Colab users)\n",
        "\n",
        "You may skip this section if you aren't running this tutorial in Colab.\n",
        "\n",
        "Run the below cell to install docker."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "867f8224340d"
      },
      "outputs": [],
      "source": [
        "if IS_COLAB:\n",
        "    # install docker daemon in Colab\n",
        "    ! apt-get -qq install docker.io"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8d262d4723b"
      },
      "source": [
        "Use the below script to run `docker build` and `docker push` commands in Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7e8c98f1e56"
      },
      "outputs": [],
      "source": [
        "%%bash -s $IS_COLAB $DEPLOY_IMAGE\n",
        "if [ $1 == \"False\" ]; then\n",
        "  exit 0\n",
        "fi\n",
        "set -x\n",
        "dockerd -b none --iptables=0 -l warn &\n",
        "for i in $(seq 5); do [ ! -S \"/var/run/docker.sock\" ] && sleep 2 || break; done\n",
        "docker build --tag=$2 ./serve\n",
        "docker push $2\n",
        "kill $(jobs -p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upload_model:mbsdk"
      },
      "source": [
        "## Upload the model to Vertex AI Model Registry\n",
        "\n",
        "Next, upload your model to Vertex AI Model Registry using `Model.upload()` method, with the following parameters:\n",
        "\n",
        "- `display_name`: The human readable name for the model resource.\n",
        "- `description`: A description of the model(optional).\n",
        "- `artifact_uri`: The path to the directory containing the Model artifact and any of its supporting files.\n",
        "- `serving_container_image_uri`: The serving container image path in Artifact Registry.\n",
        "- `serving_container_predict_route`:  HTTP path to send prediction requests to the container.\n",
        "- `serving_container_health_route`: HTTP path to send health check requests to the container.\n",
        "- `serving_container_ports`: The ports exposed by the container to listen to requests.\n",
        "- `sync`: Set **False** to run the job asynchronously.\n",
        "\n",
        "If the `upload()` method is run asynchronously, you can subsequently block the cell until completion using the `wait()` method.\n",
        "\n",
        "**Note**: In this tutorial, the saved model(`model.txt`) is directly loaded from the custom container image itself. However, this method isn't recommended if your model size is large. For large models, you can access the model from the Cloud Storage bucket directly inside the container using the `artifact_uri` parameter. Learn more about [model upload to Vertex AI](https://cloud.google.com/vertex-ai/docs/samples/aiplatform-upload-model-sample)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upload_model:mbsdk"
      },
      "outputs": [],
      "source": [
        "# Set the model display name\n",
        "MODEL_DISPLAY_NAME = f\"{APP_NAME}-model-unique\"\n",
        "# Set the model description(optional)\n",
        "MODEL_DESCRIPTION = \"LightGBM based iris flower classifier with custom container\"\n",
        "# Set the health route\n",
        "HEALTH_ROUTE = \"/health\"\n",
        "# Set the predict route\n",
        "PREDICT_ROUTE = \"/predict\"\n",
        "# Set the ports used for serving\n",
        "SERVING_PORTS = [PORT]\n",
        "\n",
        "# Upload the model\n",
        "model = aiplatform.Model.upload(\n",
        "    display_name=MODEL_DISPLAY_NAME,\n",
        "    description=MODEL_DESCRIPTION,\n",
        "    serving_container_image_uri=DEPLOY_IMAGE,\n",
        "    serving_container_predict_route=PREDICT_ROUTE,\n",
        "    serving_container_health_route=HEALTH_ROUTE,\n",
        "    serving_container_ports=SERVING_PORTS,\n",
        ")\n",
        "# Print the model display name\n",
        "print(model.display_name)\n",
        "# Print the model resource name\n",
        "print(model.resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "make_batch_predictions:migration"
      },
      "source": [
        "## Make batch predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "batchpredictionjobs_create:migration,new,mbsdk"
      },
      "source": [
        "In this section, you create batch prediction requests for your uploaded model using random examples.\n",
        "\n",
        "Learn more about [prediction requests in Vertex AI](https://cloud.google.com/vertex-ai/docs/predictions/batch-predictions)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "make_test_items:xgboost,tabular,iris"
      },
      "source": [
        "### Create test samples\n",
        "\n",
        "Create random examples to use as input instances for batch prediction job. \n",
        "\n",
        "__Note__: Random examples are used as test samples only to demonstrate how to make a prediction request."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "make_test_items:xgboost,tabular,iris"
      },
      "outputs": [],
      "source": [
        "# Define the test samples\n",
        "INSTANCES = [[6.7, 3.1, 4.7, 1.5], [4.6, 3.1, 1.5, 0.2]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "make_batch_file:custom,tabular,list"
      },
      "source": [
        "### Make batch input file\n",
        "\n",
        "Now make a batch input file, which is further stored in your Cloud Storage bucket. Each instance in the instances list is again a list. For this example, you use `.jsonl` format as below:\n",
        "\n",
        "                        [instance_1]\n",
        "                        [instance_2]\n",
        "                            .\n",
        "                            .\n",
        "\n",
        "**Note**: In this example, only two instances are stored in the input file for demonstration purpose. In general, online predictions are more suitable for short payloads and low latencies. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "make_batch_file:custom,tabular,list"
      },
      "outputs": [],
      "source": [
        "# Set a Cloud Storage path to save the test input file\n",
        "BATCH_INPUT_URI = f\"{BUCKET_URI}/{APP_NAME}/test/batch_input/test.jsonl\"\n",
        "\n",
        "# Write the instances to the file in Cloud Storage bucket\n",
        "with tf.io.gfile.GFile(BATCH_INPUT_URI, \"w\") as f:\n",
        "    for i in INSTANCES:\n",
        "        f.write(str(i) + \"\\n\")\n",
        "\n",
        "# Show the file contents\n",
        "! gsutil cat $BATCH_INPUT_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "batch_request:mbsdk,jsonl,custom,cpu"
      },
      "source": [
        "### Make batch prediction request\n",
        "\n",
        "You can create a batch prediction job by invoking the `batch_predict()` method, with the following parameters:\n",
        "\n",
        "- `job_display_name`: The human readable name for the batch prediction job.\n",
        "- `gcs_source`: A list of one or more batch request input files.\n",
        "- `gcs_destination_prefix`: The Cloud Storage location for storing the batch prediction resuls.\n",
        "- `instances_format`: The format for the input instances, either 'csv' or 'jsonl'. Defaults to 'jsonl'.\n",
        "- `predictions_format`: The format for the output predictions, either 'csv' or 'jsonl'. Defaults to 'jsonl'.\n",
        "- `machine_type`: The type of machine to use for training.\n",
        "- `sync`: If set to **True**, the call blocks while waiting for the batch job to complete execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "batch_request:mbsdk,jsonl,custom,cpu"
      },
      "outputs": [],
      "source": [
        "# Set the batch job's display name\n",
        "BATCH_JOB_DISPLAY_NAME = f\"{APP_NAME}-batch-unique\"\n",
        "\n",
        "# Create a batch prediction job\n",
        "batch_predict_job = model.batch_predict(\n",
        "    job_display_name=BATCH_JOB_DISPLAY_NAME,\n",
        "    gcs_source=BATCH_INPUT_URI,\n",
        "    gcs_destination_prefix=f\"{BUCKET_URI}/{APP_NAME}/test/batch_output/\",\n",
        "    instances_format=\"jsonl\",\n",
        "    predictions_format=\"jsonl\",\n",
        "    model_parameters=None,\n",
        "    machine_type=DEPLOY_COMPUTE,\n",
        "    starting_replica_count=1,\n",
        "    max_replica_count=1,\n",
        "    sync=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "batch_request_wait:mbsdk"
      },
      "source": [
        "### Wait for completion of the batch prediction job\n",
        "\n",
        "If you set `sync` to **False** earlier, you can wait for the batch prediction job to complete execution using the `wait()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "batch_request_wait:mbsdk"
      },
      "outputs": [],
      "source": [
        "batch_predict_job.wait()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "get_batch_prediction:mbsdk,custom,lcn"
      },
      "source": [
        "### Get batch prediction results\n",
        "\n",
        "Next, get the results from the completed batch prediction job.The results are written to the Cloud Storage output bucket you specified in the batch prediction request.\n",
        "\n",
        "Call the method `iter_outputs()` to get a list of each Cloud Storage file generated in the results. Each file contains one or more prediction responses in a JSON format with following keys:\n",
        "\n",
        "- `instance`: The input from the prediction request.\n",
        "- `prediction`: The prediction response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "get_batch_prediction:mbsdk,custom,lcn"
      },
      "outputs": [],
      "source": [
        "# Parse the output results\n",
        "bp_iter_outputs = batch_predict_job.iter_outputs()\n",
        "\n",
        "# Extract the predictions from the parsed output\n",
        "prediction_results = list()\n",
        "for blob in bp_iter_outputs:\n",
        "    if blob.name.split(\"/\")[-1].startswith(\"prediction\"):\n",
        "        prediction_results.append(blob.name)\n",
        "\n",
        "tags = list()\n",
        "for prediction_result in prediction_results:\n",
        "    gfile_name = f\"gs://{bp_iter_outputs.bucket.name}/{prediction_result}\"\n",
        "    with tf.io.gfile.GFile(name=gfile_name, mode=\"r\") as gfile:\n",
        "        for line in gfile.readlines():\n",
        "            line = json.loads(line)\n",
        "            print(line)\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deploy_model:mbsdk,cpu"
      },
      "source": [
        "## Deploy the model to an endpoint\n",
        "\n",
        "Next, deploy your model for online predictions. To deploy the model, you invoke the `deploy()` method, with the following parameters:\n",
        "\n",
        "- `deployed_model_display_name`: A human readable name for the deployed model.\n",
        "- `traffic_split`: Percent of traffic at the endpoint that goes to the deployed model, which is specified as a dictionary of one or more key/value pairs.\n",
        "If only one model, then specify as { \"0\": 100 }, where \"0\" refers to this model being uploaded and 100 means 100% of the traffic.\n",
        "If there are existing models on the endpoint, for which the traffic needs to be split, then use *model_id* to specify as { \"0\": percent, model_id: percent, ... }, where *model_id* is the id of an existing model deployed to the endpoint. The percentages must add up to 100.\n",
        "- `machine_type`: The type of machine to use for serving.\n",
        "- `starting_replica_count`: The number of compute instances to provision initially.\n",
        "- `max_replica_count`: The maximum number of compute instances to scale up. In this tutorial, only one instance is provisioned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deploy_model:mbsdk,cpu"
      },
      "outputs": [],
      "source": [
        "# Set the display name\n",
        "DEPLOYED_MODEL_DISPLAY_NAME = f\"{APP_NAME}-deployed-model-unique\"\n",
        "# Set traffic split for the endpoint\n",
        "TRAFFIC_SPLIT = {\"0\": 100}\n",
        "# Set min. no. of nodes\n",
        "MIN_NODES = 1\n",
        "# Set max. no. of nodes\n",
        "MAX_NODES = 1\n",
        "\n",
        "# Deploy the model to an endpoint\n",
        "endpoint = model.deploy(\n",
        "    deployed_model_display_name=DEPLOYED_MODEL_DISPLAY_NAME,\n",
        "    traffic_split=TRAFFIC_SPLIT,\n",
        "    machine_type=DEPLOY_COMPUTE,\n",
        "    min_replica_count=MIN_NODES,\n",
        "    max_replica_count=MAX_NODES,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "predict_request:mbsdk,custom,lcn"
      },
      "source": [
        "## Send online prediction requests\n",
        "\n",
        "After your model is successfully deployed to an endpoint, you can send online prediction requests to your model.\n",
        "\n",
        "### Request\n",
        "\n",
        "The format of each instance is:\n",
        "\n",
        "    [feature_list]\n",
        "\n",
        "You can send more than one instance in a prediction request to your model. However, there's a limit of 1.5 MB on the request payload size.\n",
        "\n",
        "Learn more about [sending online prediction requests](https://cloud.google.com/vertex-ai/docs/predictions/get-online-predictions).\n",
        "\n",
        "### Response\n",
        "\n",
        "The response from the `predict()` call is a Python dictionary with the following entries:\n",
        "\n",
        "- `ids`: The internal assigned unique identifiers for each prediction request.\n",
        "- `predictions`: The predicted confidence, between 0 and 1, per class label.\n",
        "- `deployed_model_id`: The Vertex AI identifier for the deployed model resource.\n",
        "\n",
        "Learn more about [getting predictions on Vertex AI](https://cloud.google.com/vertex-ai/docs/predictions/overview)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "predict_request:mbsdk,custom,lcn"
      },
      "outputs": [],
      "source": [
        "# Send prediction requests to the endpoint\n",
        "prediction_response = endpoint.predict(INSTANCES)\n",
        "\n",
        "# Print the prediction response\n",
        "print(prediction_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cleanup:mbsdk"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup:mbsdk"
      },
      "outputs": [],
      "source": [
        "# Undeploy the model from endpoint\n",
        "endpoint.undeploy_all()\n",
        "\n",
        "# Delete the endpoint\n",
        "endpoint.delete()\n",
        "\n",
        "# Delete the model\n",
        "model.delete()\n",
        "\n",
        "# Delete the training job\n",
        "job.delete()\n",
        "\n",
        "# Delete the batch prediction job\n",
        "batch_predict_job.delete()\n",
        "\n",
        "# Delete the repo in the Artifact Registry\n",
        "! gcloud artifacts repositories delete {PRIVATE_REPO} --location={LOCATION} --quiet\n",
        "\n",
        "# Delete the Cloud Storage bucket\n",
        "delete_bucket = True\n",
        "if delete_bucket:\n",
        "    ! gsutil -m rm -r $BUCKET_URI\n",
        "\n",
        "# Delete the locally generated files and folders\n",
        "! rm -rf custom serve\n",
        "\n",
        "# Delete the local docker container image\n",
        "if not IS_COLAB:\n",
        "    ! docker image rm $DEPLOY_IMAGE -f"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "get_started_vertex_training_lightgbm.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

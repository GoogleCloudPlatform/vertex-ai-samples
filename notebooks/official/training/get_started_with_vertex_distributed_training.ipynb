{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "copyright"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title:generic,gcp"
      },
      "source": [
        "# Get started with Vertex AI distributed training\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/training/get_started_with_vertex_distributed_training.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Ftraining%2Fget_started_with_vertex_distributed_training.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>    \n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/training/get_started_with_vertex_distributed_training.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/training/get_started_with_vertex_distributed_training.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview:mlops"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial demonstrates how to use the Vertex AI Python client library to do distrbuted training of a TensorFlow model.\n",
        "\n",
        "**Note:** There are incompatibilities between Colab and Docker and the Docker section may not work until resolved by the platform.\n",
        "\n",
        "Learn more about [Vertex AI distributed training](https://cloud.google.com/vertex-ai/docs/training/distributed-training)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "objective:mlops,stage2,get_started_vertex_distributed_training"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to use Vertex AI distributed training when training with `Vertex AI`.\n",
        "\n",
        "This tutorial uses the following Vertex AI services:\n",
        "\n",
        "- Vertex AI distributed training\n",
        "- Vertex AI Reduction Server\n",
        "\n",
        "This tutorial covers the following distributed training techniques:\n",
        "\n",
        "- `MirroredStrategy`: Train on a single VM with multiple GPUs.\n",
        "- `MultiWorkerMirroredStrategy`(automatic): Train on multiple VMs with automatic setup of replicas.\n",
        "- `MultiWorkerMirroredStrategy`: Train on multiple VMs with fine grain control of replicas.\n",
        "- `ReductionServer`: Train on multiple VMs and sync updates across the VMs with **Vertex AI Reduction Server**.\n",
        "- `TPUTraining`: Train with multiple Cloud TPUs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "recommendation:mlops,stage2,vertex,distributed_training"
      },
      "source": [
        "### Recommendations\n",
        "\n",
        "While performing E2E MLOps on Google Cloud, the following are the best practices to use with Vertex AI distributed training:\n",
        "\n",
        "**Single VM / Single Device (OneDeviceStrategy)**\n",
        "\n",
        "You are experimenting and the total training data and number of model parameters is small.\n",
        "\n",
        "If the number of model parameters is very small, you may not get much benefit from a GPU and may consider using the VM's CPU.\n",
        "\n",
        "**Single VM / Multiple Compute Devices (MirroredStrategy)**\n",
        "\n",
        "The number of model parameters is very large, but the total training data is small.\n",
        "\n",
        "**Multiple VM / Multiple Compute Devices (MultiWorkerMirroredStrategy)**\n",
        "\n",
        "The number of model parameters is very large and the total training data is very large.\n",
        "\n",
        "**ReductionServer**\n",
        "\n",
        "While training across a large number of VMs and the model parameter updates to sync are very large."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset:custom,boston,lrg"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset used for this tutorial is the [Boston Housing Prices dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html). The version of the dataset you use in this tutorial is built into TensorFlow. The trained model predicts the median price of a house in units of 1K USD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d10166df7141"
      },
      "source": [
        "### Costs\n",
        " \n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "* Artifact Registry\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and [Artifact Registry pricing](https://cloud.google.com/artifact-registry/pricing) use the [Pricing Calculator](https://cloud.google.com/products/calculator/),\n",
        "        to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1ea81ac77f0"
      },
      "source": [
        "## Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkYpRvOQyVYb"
      },
      "source": [
        "### Install Vertex AI SDK for Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xs_Kt8RcyXTC"
      },
      "outputs": [],
      "source": [
        "! pip3 install --upgrade --quiet google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16220914acc5"
      },
      "source": [
        "### Restart runtime (Colab only)\n",
        "\n",
        "To use the newly installed packages, you must restart the runtime on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "157953ab28f0"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c87a2a5d7e35"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dccb1c8feb6"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "Authenticate your environment on Google Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc7251520a07"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF1j6f9HApxa"
      },
      "source": [
        "### Set Google Cloud project information\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM1iC_MfAts1"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10f695aad5fb"
      },
      "source": [
        "### Enable Artifact Registry API\n",
        "\n",
        "First, you must enable the Artifact Registry API service for your project.\n",
        "\n",
        "Learn more about [Enabling service](https://cloud.google.com/artifact-registry/docs/enable-service)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c3147ca0f49"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "! gcloud services enable artifactregistry.googleapis.com\n",
        "\n",
        "if os.getenv(\"IS_TESTING\"):\n",
        "    ! sudo apt-get update --yes && sudo apt-get --only-upgrade --yes install google-cloud-sdk-cloud-run-proxy google-cloud-sdk-harbourbridge google-cloud-sdk-cbt google-cloud-sdk-gke-gcloud-auth-plugin google-cloud-sdk-kpt google-cloud-sdk-local-extract google-cloud-sdk-minikube google-cloud-sdk-app-engine-java google-cloud-sdk-app-engine-go google-cloud-sdk-app-engine-python google-cloud-sdk-spanner-emulator google-cloud-sdk-bigtable-emulator google-cloud-sdk-nomos google-cloud-sdk-package-go-module google-cloud-sdk-firestore-emulator kubectl google-cloud-sdk-datastore-emulator google-cloud-sdk-app-engine-python-extras google-cloud-sdk-cloud-build-local google-cloud-sdk-kubectl-oidc google-cloud-sdk-anthos-auth google-cloud-sdk-app-engine-grpc google-cloud-sdk-pubsub-emulator google-cloud-sdk-datalab google-cloud-sdk-skaffold google-cloud-sdk google-cloud-sdk-terraform-tools google-cloud-sdk-config-connector\n",
        "    ! gcloud components update --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61efa7b08cee"
      },
      "source": [
        "### Create a repository in Artifact Registry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7479d575df98"
      },
      "outputs": [],
      "source": [
        "# Specify the name of your repo\n",
        "REPO_NAME = \"vertex-distributed-unique\"\n",
        "\n",
        "# Create the repo\n",
        "! gcloud artifacts repositories create {REPO_NAME} --repository-format=docker --location={LOCATION} --description=\"Docker repository\"\n",
        "\n",
        "# List all the repos and verify that your repo is created\n",
        "! gcloud artifacts repositories list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgPO1eR3CYjk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "Create a storage bucket to store intermediate artifacts such as datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzGDU7TWdts_"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_bucket"
      },
      "source": [
        "**If your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIq7R4HZCfIc"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $LOCATION -p $PROJECT_ID $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbvYPSTDulvS"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "accelerators:training,prediction,ngpu,mbsdk"
      },
      "source": [
        "#### Set hardware accelerators\n",
        "\n",
        "You can set hardware accelerators for training and prediction.\n",
        "\n",
        "Set the variables `TRAIN_GPU/TRAIN_NGPU` and `DEPLOY_GPU/DEPLOY_NGPU` to use a container image supporting a GPU and the number of GPUs allocated to the virtual machine (VM) instance. For example, to use a GPU container image with 4 Nvidia Telsa T4 GPUs allocated to each VM, you would specify:\n",
        "\n",
        "    (aiplatform.AcceleratorType.NVIDIA_TESLA_T4, 4)\n",
        "\n",
        "\n",
        "Otherwise specify `(None, None)` to use a container image to run on a CPU.\n",
        "\n",
        "Learn more about [hardware accelerator support for your region](https://cloud.google.com/vertex-ai/docs/general/locations#accelerators).\n",
        "\n",
        "**Note:** TF releases before version 2.3 for GPU support are known to fail while loading the custom model in this tutorial. The issue is fixed in TF versions 2.3 and above. This is caused by static graph ops that are generated in the serving function. If you encounter this issue on your own custom models, use a container image for TF 2.3 with GPU support."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PryARdnoulvT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if os.getenv(\"IS_TESTING_TRAIN_GPU\"):\n",
        "    TRAIN_GPU, TRAIN_NGPU = (\n",
        "        aiplatform.gapic.AcceleratorType.NVIDIA_TESLA_T4,\n",
        "        int(os.getenv(\"IS_TESTING_TRAIN_GPU\")),\n",
        "    )\n",
        "else:\n",
        "    TRAIN_GPU, TRAIN_NGPU = (aiplatform.gapic.AcceleratorType.NVIDIA_TESLA_T4, 1)\n",
        "\n",
        "if os.getenv(\"IS_TESTING_DEPLOY_GPU\"):\n",
        "    DEPLOY_GPU, DEPLOY_NGPU = (\n",
        "        aiplatform.gapic.AcceleratorType.NVIDIA_TESLA_T4,\n",
        "        int(os.getenv(\"IS_TESTING_DEPLOY_GPU\")),\n",
        "    )\n",
        "else:\n",
        "    DEPLOY_GPU, DEPLOY_NGPU = (None, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "container:training,prediction"
      },
      "source": [
        "#### Set pre-built containers\n",
        "\n",
        "Set the pre-built Docker container image for training and prediction.\n",
        "\n",
        "\n",
        "For the latest list, see [Pre-built containers for training](https://cloud.google.com/ai-platform-unified/docs/training/pre-built-containers).\n",
        "\n",
        "\n",
        "For the latest list, see [Pre-built containers for prediction](https://cloud.google.com/ai-platform-unified/docs/predictions/pre-built-containers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhhUFw2nulvT"
      },
      "outputs": [],
      "source": [
        "if os.getenv(\"IS_TESTING_TF\"):\n",
        "    TF = os.getenv(\"IS_TESTING_TF\")\n",
        "else:\n",
        "    TF = \"2.8\".replace(\".\", \"-\")\n",
        "\n",
        "if TF[0] == \"2\":\n",
        "    if TRAIN_GPU:\n",
        "        TRAIN_VERSION = \"tf-gpu.{}\".format(TF)\n",
        "    else:\n",
        "        TRAIN_VERSION = \"tf-cpu.{}\".format(TF)\n",
        "    if DEPLOY_GPU:\n",
        "        DEPLOY_VERSION = \"tf2-gpu.{}\".format(TF)\n",
        "    else:\n",
        "        DEPLOY_VERSION = \"tf2-cpu.{}\".format(TF)\n",
        "else:\n",
        "    if TRAIN_GPU:\n",
        "        TRAIN_VERSION = \"tf-gpu.{}\".format(TF)\n",
        "    else:\n",
        "        TRAIN_VERSION = \"tf-cpu.{}\".format(TF)\n",
        "    if DEPLOY_GPU:\n",
        "        DEPLOY_VERSION = \"tf-gpu.{}\".format(TF)\n",
        "    else:\n",
        "        DEPLOY_VERSION = \"tf-cpu.{}\".format(TF)\n",
        "\n",
        "TRAIN_IMAGE = \"{}-docker.pkg.dev/vertex-ai/training/{}:latest\".format(\n",
        "    LOCATION.split(\"-\")[0], TRAIN_VERSION\n",
        ")\n",
        "DEPLOY_IMAGE = \"{}-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(\n",
        "    LOCATION.split(\"-\")[0], DEPLOY_VERSION\n",
        ")\n",
        "\n",
        "print(\"Training:\", TRAIN_IMAGE, TRAIN_GPU, TRAIN_NGPU)\n",
        "print(\"Deployment:\", DEPLOY_IMAGE, DEPLOY_GPU, DEPLOY_NGPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "machine:training"
      },
      "source": [
        "#### Set machine configuration\n",
        "\n",
        "Next, set the machine configuration to use for training.\n",
        "\n",
        "- Set the variable `TRAIN_COMPUTE` to configure  the compute resources for the VMs used for training.\n",
        " - `machine type`\n",
        "     - `n1-standard`: 3.75GB of memory per vCPU\n",
        "     - `n1-highmem`: 6.5GB of memory per vCPU\n",
        "     - `n1-highcpu`: 0.9 GB of memory per vCPU\n",
        " - `vCPUs`: number of \\[2, 4, 8, 16, 32, 64, 96 \\]\n",
        "\n",
        "**Note:** The following is not supported for training:\n",
        "\n",
        " - `standard`: 2 vCPUs\n",
        " - `highcpu`: 2, 4 and 8 vCPUs\n",
        "\n",
        "**Note:** You may also use n2 and e2 machine types for training and deployment, but they don't support GPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vytMaukeulvT"
      },
      "outputs": [],
      "source": [
        "if os.getenv(\"IS_TESTING_TRAIN_MACHINE\"):\n",
        "    MACHINE_TYPE = os.getenv(\"IS_TESTING_TRAIN_MACHINE\")\n",
        "else:\n",
        "    MACHINE_TYPE = \"n1-standard\"\n",
        "\n",
        "VCPU = \"4\"\n",
        "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
        "print(\"Train machine type\", TRAIN_COMPUTE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mirrored_intro"
      },
      "source": [
        "## Mirrored Strategy\n",
        "\n",
        "When training on a single VM, you can either train with a single compute device or with multiple compute devices on the same VM. With Vertex AI distributed training you can specify both the number of compute devices for the VM instance and type of compute devices: CPU, GPU.\n",
        "\n",
        "Vertex AI distributed training supports *tf.distribute.MirroredStrategy* for TensorFlow models. To enable training across multiple compute devices on the same VM, you do the following additional steps in your Python training script:\n",
        "\n",
        "1. Set the *tf.distribute.MirrorStrategy*.\n",
        "2. Compile the model within the scope of *tf.distribute.MirrorStrategy*. This tells MirroredStrategy which variables to mirror across your compute devices.\n",
        "3. Increase the batch size for each compute device to *num_devices* x *batch size*.\n",
        "\n",
        "During transitions, the distribution of batches are synchronized as well as the updates to the model parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_custom_pp_training_job:mbsdk"
      },
      "source": [
        "### Create and run custom training job\n",
        "\n",
        "\n",
        "To train a custom model, you perform two steps: 1) create a custom training job, and 2) run the job.\n",
        "\n",
        "#### Create custom training job\n",
        "\n",
        "A custom training job is created with the `CustomTrainingJob` class, with the following parameters:\n",
        "\n",
        "- `display_name`: The human readable name for the custom training job.\n",
        "- `container_uri`: The training container image.\n",
        "\n",
        "- `python_package_gcs_uri`: The location of the Python training package as a tarball.\n",
        "- `python_module_name`: The relative path to the training script in the Python package.\n",
        "- `model_serving_container_uri`: The container image for deploying the model.\n",
        "\n",
        "**Note:** There is no requirements parameter. You specify any requirements in the `setup.py` script in your Python package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhw34XoOulvU"
      },
      "outputs": [],
      "source": [
        "DISPLAY_NAME = \"boston-unique\"\n",
        "\n",
        "job = aiplatform.CustomPythonPackageTrainingJob(\n",
        "    display_name=DISPLAY_NAME,\n",
        "    python_package_gcs_uri=f\"{BUCKET_URI}/trainer_boston.tar.gz\",\n",
        "    python_module_name=\"trainer.task\",\n",
        "    container_uri=TRAIN_IMAGE,\n",
        "    model_serving_container_image_uri=DEPLOY_IMAGE,\n",
        "    project=PROJECT_ID,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "examine_training_package"
      },
      "source": [
        "### Examine the training package\n",
        "\n",
        "#### Package layout\n",
        "\n",
        "Before you start the training, look at how a Python package is assembled for a custom training job. When unarchived, the package contains the following directory/file layout.\n",
        "\n",
        "- PKG-INFO\n",
        "- README.md\n",
        "- setup.cfg\n",
        "- setup.py\n",
        "- trainer\n",
        "  - \\_\\_init\\_\\_.py\n",
        "  - task.py\n",
        "\n",
        "The files `setup.cfg` and `setup.py` are the instructions for installing the package into the operating environment of the Docker image.\n",
        "\n",
        "The file `trainer/task.py` is the Python script for executing the custom training job. \n",
        "\n",
        "**Note**: When referring to the worker pool specification, the directory slash is replaced with a dot (`trainer.task`) and the file suffix (`.py`) is dropped.\n",
        "\n",
        "#### Package Assembly\n",
        "\n",
        "In the following cells, you assemble the training package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAaZpZyyulvU"
      },
      "outputs": [],
      "source": [
        "# Make folder for Python training script\n",
        "! rm -rf custom\n",
        "! mkdir custom\n",
        "\n",
        "# Add package information\n",
        "! touch custom/README.md\n",
        "\n",
        "setup_cfg = \"[egg_info]\\n\\ntag_build =\\n\\ntag_date = 0\"\n",
        "! echo \"$setup_cfg\" > custom/setup.cfg\n",
        "\n",
        "setup_py = \"import setuptools\\n\\nsetuptools.setup(\\n\\n    install_requires=[\\n\\n        'tensorflow==2.8.0',\\n\\n        'tensorflow_datasets==1.3.0',\\n\\n    ],\\n\\n    packages=setuptools.find_packages())\"\n",
        "! echo \"$setup_py\" > custom/setup.py\n",
        "\n",
        "pkg_info = \"Metadata-Version: 1.0\\n\\nName: Boston Housing cloud\\n\\nVersion: 0.0.0\\n\\nSummary: Demostration training script\\n\\nHome-page: www.google.com\\n\\nAuthor: Google\\n\\nAuthor-email: aferlitsch@google.com\\n\\nLicense: Public\\n\\nDescription: Demo\\n\\nPlatform: Vertex\"\n",
        "! echo \"$pkg_info\" > custom/PKG-INFO\n",
        "\n",
        "# Make the training subfolder\n",
        "! mkdir custom/trainer\n",
        "! touch custom/trainer/__init__.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taskpy_contents:mirrored,boston"
      },
      "source": [
        "#### Task.py contents\n",
        "\n",
        "In the next cell, write the contents of the training script task.py.\n",
        "\n",
        "In summary, task.py script does the following:\n",
        "\n",
        "- Get the directory where to save the model artifacts from the command line (`--model_dir`), and if not specified, then from the environment variable `AIP_MODEL_DIR`.\n",
        "- Loads Boston Housing dataset from TF.Keras builtin datasets.\n",
        "- Builds a simple deep neural network model using TF.Keras model API.\n",
        "- Compiles the model (`compile()`).\n",
        "- Sets a training distribution strategy according to the argument `args.distribute`.\n",
        "- Trains the model (`fit()`) with epochs specified by `args.epochs`.\n",
        "- Saves the trained model (`save(args.model_dir)`) to the specified model directory.\n",
        "- Saves the maximum value for each feature `f.write(str(params))` to the specified parameters file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKzddzl6ulvV"
      },
      "outputs": [],
      "source": [
        "%%writefile custom/trainer/task.py\n",
        "# Single, Mirrored and MultiWorker Distributed Training\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.client import device_lib\n",
        "import numpy as np\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--model-dir', dest='model_dir',\n",
        "                    default=os.getenv('AIP_MODEL_DIR'), type=str, help='Model dir.')\n",
        "parser.add_argument('--lr', dest='lr',\n",
        "                    default=0.001, type=float,\n",
        "                    help='Learning rate.')\n",
        "parser.add_argument('--epochs', dest='epochs',\n",
        "                    default=10, type=int,\n",
        "                    help='Number of epochs.')\n",
        "parser.add_argument('--steps', dest='steps',\n",
        "                    default=100, type=int,\n",
        "                    help='Number of steps per epoch.')\n",
        "parser.add_argument('--batch_size', dest='batch_size',\n",
        "                    default=16, type=int,\n",
        "                    help='Size of a batch.')\n",
        "parser.add_argument('--distribute', dest='distribute', type=str, default='single',\n",
        "                    help='distributed training strategy')\n",
        "parser.add_argument('--param-file', dest='param_file',\n",
        "                    default='/tmp/param.txt', type=str,\n",
        "                    help='Output file for parameters')\n",
        "args = parser.parse_args()\n",
        "\n",
        "logging.info('DEVICES'  + str(device_lib.list_local_devices()))\n",
        "\n",
        "# Single Machine, single compute device\n",
        "if args.distribute == 'single':\n",
        "    if tf.test.is_gpu_available():\n",
        "        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
        "    else:\n",
        "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
        "    logging.info(\"Single device training\")\n",
        "# Single Machine, multiple compute device\n",
        "elif args.distribute == 'mirrored':\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "    logging.info(\"Mirrored Strategy distributed training\")\n",
        "# Multi Machine, multiple compute device\n",
        "elif args.distribute == 'multiworker':\n",
        "    strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
        "    logging.info(\"Multi-worker Strategy distributed training\")\n",
        "    logging.info('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', 'Not found')))\n",
        "    # Single Machine, multiple TPU devices\n",
        "elif args.distribute == 'tpu':\n",
        "    cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=\"local\")\n",
        "    tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
        "    tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
        "    strategy = tf.distribute.TPUStrategy(cluster_resolver)\n",
        "    print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "\n",
        "logging.info('num_replicas_in_sync = {}'.format(strategy.num_replicas_in_sync))\n",
        "\n",
        "def _is_chief(task_type, task_id):\n",
        "    ''' Check for primary if multiworker training\n",
        "    '''\n",
        "    return (task_type == 'chief') or (task_type == 'worker' and task_id == 0) or task_type is None\n",
        "\n",
        "\n",
        "def get_data():\n",
        "    # Scaling Boston Housing data features\n",
        "    def scale(feature):\n",
        "        max = np.max(feature)\n",
        "        feature = (feature / max).astype(float)\n",
        "        return feature, max\n",
        "\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.boston_housing.load_data(\n",
        "        path=\"boston_housing.npz\", test_split=0.2, seed=113\n",
        "    )\n",
        "\n",
        "    params = []\n",
        "    for _ in range(13):\n",
        "        x_train[_], max = scale(x_train[_])\n",
        "        x_test[_], _ = scale(x_test[_])\n",
        "    params.append(max)\n",
        "\n",
        "    # store the normalization (max) value for each feature\n",
        "    with tf.io.gfile.GFile(args.param_file, 'w') as f:\n",
        "        f.write(str(params))\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "def get_model():\n",
        "    model = tf.keras.Sequential([\n",
        "          tf.keras.layers.Dense(128, activation='relu', input_shape=(13,)),\n",
        "          tf.keras.layers.Dense(128, activation='relu'),\n",
        "          tf.keras.layers.Dense(1, activation='linear')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "          loss='mse',\n",
        "          optimizer=tf.keras.optimizers.RMSprop(learning_rate=args.lr)\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def train(model, x_train, y_train):\n",
        "    NUM_WORKERS = strategy.num_replicas_in_sync\n",
        "    # Here the batch size scales up by number of workers since\n",
        "    # `tf.data.Dataset.batch` expects the global batch size.\n",
        "    GLOBAL_BATCH_SIZE = args.batch_size * NUM_WORKERS\n",
        "\n",
        "    model.fit(x_train, y_train, epochs=args.epochs, batch_size=GLOBAL_BATCH_SIZE)\n",
        "\n",
        "    if args.distribute == 'multiworker':\n",
        "        task_type, task_id = (strategy.cluster_resolver.task_type,\n",
        "                              strategy.cluster_resolver.task_id)\n",
        "    else:\n",
        "        task_type, task_id = None, None\n",
        "\n",
        "    if args.distribute==\"tpu\":\n",
        "        save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
        "        model.save(args.model_dir, options=save_locally)\n",
        "    # single, mirrored or primary for multiworker\n",
        "    elif _is_chief(task_type, task_id):\n",
        "        model.save(args.model_dir)\n",
        "    # non-primary workers for multi-workers\n",
        "    else:\n",
        "        # each worker saves their model instance to a unique temp location\n",
        "        worker_dir = args.model_dir + '/workertemp_' + str(task_id)\n",
        "        tf.io.gfile.makedirs(worker_dir)\n",
        "        model.save(worker_dir)\n",
        "\n",
        "with strategy.scope():\n",
        "    # Creation of dataset, and model building/compiling need to be within\n",
        "    # `strategy.scope()`.\n",
        "    model = get_model()\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = get_data()\n",
        "\n",
        "train(model, x_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tarball_training_script"
      },
      "source": [
        "#### Store training script on your Cloud Storage bucket\n",
        "\n",
        "Next, package the training folder into a compressed tar ball, and then store it in your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFUHioqTulvV"
      },
      "outputs": [],
      "source": [
        "! rm -f custom.tar custom.tar.gz\n",
        "! tar cvf custom.tar custom\n",
        "! gzip custom.tar\n",
        "! gsutil cp custom.tar.gz $BUCKET_URI/trainer_boston.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_custom_pp_training_job:mirrored"
      },
      "source": [
        "#### Run the custom Python package training job\n",
        "\n",
        "Next, run the custom job to start the training job by invoking the `run()` method. The parameters are the same as when running a CustomTrainingJob."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnUX0UkvulvV"
      },
      "outputs": [],
      "source": [
        "MODEL_DIR = BUCKET_URI\n",
        "MODEL_DISPLAY_NAME = \"boston-unique\"\n",
        "CMDARGS = [\"--epochs=5\", \"--batch_size=16\", \"--distribute=mirrored\"]\n",
        "\n",
        "model = job.run(\n",
        "    model_display_name=MODEL_DISPLAY_NAME,\n",
        "    args=CMDARGS,\n",
        "    replica_count=1,\n",
        "    machine_type=TRAIN_COMPUTE,\n",
        "    accelerator_type=TRAIN_GPU.name,\n",
        "    accelerator_count=TRAIN_NGPU,\n",
        "    base_output_dir=MODEL_DIR,\n",
        "    sync=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "delete_job"
      },
      "source": [
        "### Delete custom training job\n",
        "\n",
        "After a training job is completed, you can delete the training job with the `delete()` method.  Prior to completion, a training job can be cancelled with the `cancel()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUWHFpPoulvW"
      },
      "outputs": [],
      "source": [
        "job.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_delete:mbsdk"
      },
      "source": [
        "### Delete model\n",
        "\n",
        "Similarly, `delete()` method deletes the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0gqCUTEulvW"
      },
      "outputs": [],
      "source": [
        "model.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "multiworker_intro"
      },
      "source": [
        "## Multi-Worker Mirrored Strategy\n",
        "\n",
        "With Vertex AI distributed training you can train with multiple VM instances.\n",
        "\n",
        "Vertex AI distributed training supports *tf.distribute.MultiWorkerMirroredStrategy* for TensorFlow and PyTorch models. To enable training across multiple VMs, you do the following additional steps in your Python training script:\n",
        "\n",
        "1. All the steps followed for MirroredStrategy, except that MultiWorkerMirroredStrategy is set in place of MirroredStrategy.\n",
        "2. Set up the worker pools.\n",
        "3. Alter the saving of the model so that the non-primary workers save their model instance to a unique temporary directory each.\n",
        "\n",
        "**Note:** You don't need to construct the TF_CONFIG environment variable. It's automatically constructed by Vertex AI distributed training.\n",
        "\n",
        "Learn more about [distributed training](https://cloud.google.com/vertex-ai/docs/training/distributed-training)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "worker_pools"
      },
      "source": [
        "### Worker pools\n",
        "\n",
        "If you run a distributed training job with Vertex AI, you specify multiple machines (nodes) in a training cluster. The training service allocates the resources for the machine types you specify. Your running job on a given node is called a replica. A group of replicas with the same configuration is called a worker pool.\n",
        "\n",
        "Each replica in the training cluster is given a single role or task in distributed training. For example:\n",
        "\n",
        "- **Primary replica**: Only one replica is designated as the primary replica. This task manages the others and reports status for the job as a whole.\n",
        "\n",
        "- **Worker(s)**: One or more replicas may be designated as workers. These replicas do their portion of the work as you specify in your job configuration.\n",
        "\n",
        "- **Parameter server(s)**: If supported by your ML framework, one or more replicas may be designated as parameter servers. These replicas store model parameters and coordinate shared model state between the workers.\n",
        "\n",
        "- **Evaluator(s)**: If supported by your ML framework, one or more replicas may be designated as evaluators. These replicas can be used to evaluate your model. If you are using TensorFlow, note that TensorFlow generally expects that you use no more than one evaluator.\n",
        "\n",
        "To configure a distributed training job, define your list of worker pools (workerPoolSpecs[]), designating one WorkerPoolSpec for each type of task:\n",
        "\n",
        "**Note:** The worker pool is order dependent (0..3):\n",
        "\n",
        "**workerPoolSpecs[0]**: Primary, chief, scheduler, or \"master\"\n",
        "\n",
        "**workerPoolSpecs[1]**: Secondary, replicas, workers\n",
        "\n",
        "**workerPoolSpecs[2]**: Parameter servers, Reduction Server\n",
        "\n",
        "**workerPoolSpecs[2]**: Evaluators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "multiworker_methods"
      },
      "source": [
        "### Distributed training options for Multi-Worker Mirrored Strategy\n",
        "\n",
        "How you setup the worker pools is dependent on the Vertex AI method you use for training.\n",
        "\n",
        "**CustomTrainingJob** / **CustomContainerTrainingJob** / **CustomPythonPackageTrainingJob**\n",
        "\n",
        "The `replica_count` includes the primary and secondary (replica_count-1), and share the same machine type and accelerators.\n",
        "\n",
        "You can't specify a parameter server or evaluation node.\n",
        "\n",
        "**CustomJob**\n",
        "\n",
        "You specify a `worker_pool_spec`, where you can specify detailed settings for each of the four worker pools."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXvPN8P6ulvX"
      },
      "source": [
        "### Create and run custom training job\n",
        "\n",
        "\n",
        "To train a custom model, you perform two steps: 1) create a custom training job, and 2) run the job.\n",
        "\n",
        "#### Create custom training job\n",
        "\n",
        "A custom training job is created with the `CustomTrainingJob` class, with the following parameters:\n",
        "\n",
        "- `display_name`: The human readable name for the custom training job.\n",
        "- `container_uri`: The training container image.\n",
        "\n",
        "- `python_package_gcs_uri`: The location of the Python training package as a tarball.\n",
        "- `python_module_name`: The relative path to the training script in the Python package.\n",
        "- `model_serving_container_uri`: The container image for deploying the model.\n",
        "\n",
        "**Note:** There is no requirements parameter. You specify any requirements in the `setup.py` script in your Python package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYcFsVSEulvX"
      },
      "outputs": [],
      "source": [
        "DISPLAY_NAME = \"boston-unique\"\n",
        "\n",
        "job = aiplatform.CustomPythonPackageTrainingJob(\n",
        "    display_name=DISPLAY_NAME,\n",
        "    python_package_gcs_uri=f\"{BUCKET_URI}/trainer_boston.tar.gz\",\n",
        "    python_module_name=\"trainer.task\",\n",
        "    container_uri=TRAIN_IMAGE,\n",
        "    model_serving_container_image_uri=DEPLOY_IMAGE,\n",
        "    project=PROJECT_ID,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_custom_pp_training_job:multiworker"
      },
      "source": [
        "#### Run the custom Python package training job\n",
        "\n",
        "Next, run the custom job to start the training job by invoking the `run()` method. The parameters are the same as when running a CustomTrainingJob."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHRxPU32ulvX"
      },
      "outputs": [],
      "source": [
        "MODEL_DIR = BUCKET_URI\n",
        "\n",
        "CMDARGS = [\"--epochs=5\", \"--batch_size=16\", \"--distribute=multiworker\"]\n",
        "\n",
        "try:\n",
        "    model = job.run(\n",
        "        model_display_name=MODEL_DISPLAY_NAME,\n",
        "        args=CMDARGS,\n",
        "        replica_count=4,\n",
        "        machine_type=TRAIN_COMPUTE,\n",
        "        accelerator_type=TRAIN_GPU.name,\n",
        "        accelerator_count=TRAIN_NGPU,\n",
        "        base_output_dir=MODEL_DIR,\n",
        "        sync=True,\n",
        "    )\n",
        "except Exception as e:\n",
        "    # may fail duing model.save() -- seems to be some issue when merging checkpoints from the workers\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92D_hbuVulvX"
      },
      "source": [
        "### Delete custom training job\n",
        "\n",
        "After a training job is completed, you can delete the training job with the `delete()` method. Prior to completion, a training job can be canceled with the `cancel()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqrfWkB3ulvX"
      },
      "outputs": [],
      "source": [
        "job.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "customjob_intro:multiworker"
      },
      "source": [
        "### Multiworker distributed training with CustomJob\n",
        "\n",
        "Multiworker distributed training with `CustomJob` has the advantages of fine-grained control of the primary replica and optionally specifying worker pools for parameter server and evaluators. Creating a `CustomJob` includes the following steps:\n",
        "\n",
        "\n",
        "1. Specify individual details for each worker pool.\n",
        "2. Embed training package into Docker image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_docker_container:training"
      },
      "source": [
        "### Build a custom container for training\n",
        "\n",
        "To use your own custom training container, you build a Docker file and embed your training scripts into the container."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "write_docker_file:training,multiworker"
      },
      "source": [
        "#### Write the Docker file contents\n",
        "\n",
        "Your first step in containerizing your code is to create a Docker file. In the Docker file, include all the commands needed to run your container image. Docker installs all the libraries you specify and set up the entry point for your training code.\n",
        "\n",
        "Steps in your Docker file include:\n",
        "\n",
        "1. Install a pre-defined container image from TensorFlow repository for deep learning images.\n",
        "2. Copy the Python training code to the image, to be shown subsequently.\n",
        "3. Set the entry point for the Python training script as `trainer/task.py`. Note that the `.py` is dropped in the ENTRYPOINT command, as it's implied."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGI2viDAulvY"
      },
      "outputs": [],
      "source": [
        "%%writefile custom/Dockerfile\n",
        "\n",
        "FROM gcr.io/deeplearning-platform-release/tf2-gpu.2-8\n",
        "\n",
        "WORKDIR /\n",
        "\n",
        "# Copies the trainer code to the docker image.\n",
        "COPY trainer /trainer\n",
        "\n",
        "# Sets up the entry point to invoke the trainer.\n",
        "ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea91554a10a8"
      },
      "source": [
        "#### Configure authentication to your repository\n",
        "\n",
        "Before you push or pull container images, configure Docker to use the gcloud command-line tool to authenticate requests to Artifact Registry for your region."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eae17cce6bd"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "if not IS_COLAB:\n",
        "    ! gcloud auth configure-docker {LOCATION}-docker.pkg.dev --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "name_container:training"
      },
      "source": [
        "#### Build the container image locally\n",
        "\n",
        "Use `docker build` command to build your container image locally.\n",
        "\n",
        "Provide repository name as a tag using the `-t` flag. Learn more about adding images to [Artifact Registry](https://cloud.google.com/artifact-registry/docs/docker/store-docker-container-images#add-image)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cc28b1a4d16"
      },
      "outputs": [],
      "source": [
        "# Specify the repository name where the image needs to be uploaded\n",
        "TRAIN_IMAGE = f\"{LOCATION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/boston:v1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmw5cakNulvY"
      },
      "outputs": [],
      "source": [
        "if not IS_COLAB:\n",
        "    # build the image locally with the repository name as a tag\n",
        "    ! docker build custom -t $TRAIN_IMAGE\n",
        "else:\n",
        "    # install docker daemon\n",
        "    ! apt-get -qq install docker.io"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_container:training"
      },
      "source": [
        "#### Test the container locally\n",
        "\n",
        "Run the container within your notebook instance for 5 epochs to ensure it’s working correctly.\n",
        "\n",
        "**Note**: If you're running this tutorial in a notebook environment, for example Vertex AI workbench, proceed with the below steps. Instead, if you're running on Colab, skip to the \"[Executes in Colab](#Executes-in-Colab)\" section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJGLjU-TulvZ"
      },
      "outputs": [],
      "source": [
        "if not IS_COLAB:\n",
        "    ! docker run $TRAIN_IMAGE --epochs=5 --model-dir=./"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "register_container:training"
      },
      "source": [
        "#### Register the custom container\n",
        "\n",
        "When you’ve finished running the container locally, push it to the Artifact Registry.\n",
        "\n",
        "After you have configured authentication and tagged the local image, you can push the image to the repository that you created.\n",
        "\n",
        "To push the Docker image, run the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAXGjae7ulvZ"
      },
      "outputs": [],
      "source": [
        "if not IS_COLAB:\n",
        "    ! docker push $TRAIN_IMAGE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f50e9c553fb7"
      },
      "source": [
        "#### Executes in Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7e8c98f1e56"
      },
      "outputs": [],
      "source": [
        "%%bash -s $IS_COLAB $TRAIN_IMAGE\n",
        "if [ $1 == \"False\" ]; then\n",
        "  exit 0\n",
        "fi\n",
        "set -x\n",
        "dockerd -b none --iptables=0 -l warn &\n",
        "for i in $(seq 5); do [ ! -S \"/var/run/docker.sock\" ] && sleep 2 || break; done\n",
        "docker build custom -t $2\n",
        "docker run $2 --epochs=5 --model-dir=./\n",
        "docker push $2\n",
        "kill $(jobs -p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "worker_pool_primary"
      },
      "source": [
        "#### Primary worker pool\n",
        "\n",
        "The primary worker pool (index 0) coordinates the work done by all the other replicas. Set the `replica_count` to 1. \n",
        "\n",
        "Since the worker is coordinating and not training, use a general purpose CPU, instead of a GPU.\n",
        "\n",
        "Learn more about [machine types for training](https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEAnXBzCulvZ"
      },
      "outputs": [],
      "source": [
        "PRIMARY_COMPUTE = \"n2-highcpu-64\"\n",
        "\n",
        "MODEL_DIR = BUCKET_URI\n",
        "\n",
        "CMDARGS = [\n",
        "    \"--model-dir=\" + MODEL_DIR,\n",
        "    \"--epochs=5\",\n",
        "    \"--batch_size=16\",\n",
        "    \"--distribute=multiworker\",\n",
        "]\n",
        "\n",
        "CONTAINER_SPEC = {\"image_uri\": TRAIN_IMAGE, \"command\": \"trainer.task\", \"args\": CMDARGS}\n",
        "\n",
        "PRIMARY_WORKER_POOL = {\n",
        "    \"replica_count\": 1,\n",
        "    \"machine_spec\": {\"machine_type\": PRIMARY_COMPUTE, \"accelerator_count\": 0},\n",
        "    \"container_spec\": CONTAINER_SPEC,\n",
        "}\n",
        "\n",
        "WORKER_POOL_SPECS = [PRIMARY_WORKER_POOL]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "worker_pool_training"
      },
      "source": [
        "#### Training worker pool\n",
        "\n",
        "The secondary worker pool (index 1) performs model training. Each of the replicas has an instance of your training package installed on it.\n",
        "\n",
        "Each replica may have one (single device training) or multiple (mirrored) compute devices for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dchPSfNulvZ"
      },
      "outputs": [],
      "source": [
        "TRAIN_WORKER_POOL = {\n",
        "    \"replica_count\": 4,\n",
        "    \"machine_spec\": {\n",
        "        \"machine_type\": TRAIN_COMPUTE,\n",
        "        \"accelerator_count\": TRAIN_NGPU,\n",
        "        \"accelerator_type\": TRAIN_GPU,\n",
        "    },\n",
        "    \"container_spec\": CONTAINER_SPEC,\n",
        "}\n",
        "\n",
        "WORKER_POOL_SPECS.append(TRAIN_WORKER_POOL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "custom_job:worker_pool"
      },
      "source": [
        "### Create CustomJob with worker pool specifications\n",
        "\n",
        "Next, create a `CustomJob` for the multi-worker distributed training job.\n",
        "\n",
        "Specify the following parameters:\n",
        "\n",
        "- `display_name`: The display name for the custom job.\n",
        "\n",
        "- `worker_pool_specs`: The detailed specifications for each worker pool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2VgmqEOulva"
      },
      "outputs": [],
      "source": [
        "DISPLAY_NAME = \"boston-unique\"\n",
        "\n",
        "job = aiplatform.CustomJob(\n",
        "    display_name=DISPLAY_NAME, worker_pool_specs=WORKER_POOL_SPECS\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_custom_job:multiworker"
      },
      "source": [
        "### Run the CustomJob\n",
        "\n",
        "Next, run the custom job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hg8vnI_Wulva"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    job.run(sync=True)\n",
        "except Exception as e:\n",
        "    # may fail in multi-worker to find startup script\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WT76Sc-culva"
      },
      "source": [
        "### Delete custom training job\n",
        "\n",
        "After a training job is completed, you can delete the training job with the `delete()` method.  Prior to completion, a training job can be cancelled with the `cancel()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_IxVfuDulva"
      },
      "outputs": [],
      "source": [
        "job.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reduction_server_intro"
      },
      "source": [
        "## Reduction Server\n",
        "\n",
        "To speed up training of large models, many engineering teams are adopting distributed training using scale-out clusters of ML accelerators. However, distributed training at scale brings its own set of challenges. Specifically, limited network bandwidth between nodes makes optimizing performance of distributed training inherently difficult, particularly for large cluster configurations.\n",
        "\n",
        "Vertex AI Reduction Server optimizes bandwidth and latency of multi-node distributed training on NVIDIA GPUs for synchronous data parallel algorithms. Synchronous data parallelism is the foundation of many widely adopted distributed training frameworks, including TensorFlow’s MultiWorkerMirroredStrategy, Horovod, and PyTorch Distributed. By optimizing bandwidth usage and latency of the all-reduce collective operation used by these frameworks, Reduction Server can decrease both the time and cost of large training jobs.\n",
        "\n",
        "Learn more about [Optimizing training performance using Reduction Server on Vertex AI](https://cloud.google.com/blog/topics/developers-practitioners/optimize-training-performance-reduction-server-vertex-ai)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "worker_pool_reduction_server"
      },
      "outputs": [],
      "source": [
        "reduction_server_count = 1\n",
        "reduction_server_machine_type = \"n1-highcpu-16\"\n",
        "reduction_server_image_uri = (\n",
        "    \"us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest\"\n",
        ")\n",
        "\n",
        "PARAMETER_POOL = {\n",
        "    \"replica_count\": reduction_server_count,\n",
        "    \"machine_spec\": {\n",
        "        \"machine_type\": reduction_server_machine_type,\n",
        "    },\n",
        "    \"container_spec\": {\"image_uri\": reduction_server_image_uri},\n",
        "}\n",
        "WORKER_POOL_SPECS.append(PARAMETER_POOL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8Av8ATVulvb"
      },
      "source": [
        "### Create CustomJob with worker pool specifications\n",
        "\n",
        "Next, create a `CustomJob` for the multi-worker distributed training job.\n",
        "\n",
        "Specify the following parameters:\n",
        "\n",
        "- `display_name`: The display name for the custom job.\n",
        "\n",
        "- `worker_pool_specs`: The detailed specifications for each worker pool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUWEP1Lmulvb"
      },
      "outputs": [],
      "source": [
        "DISPLAY_NAME = \"boston-unique\"\n",
        "\n",
        "job = aiplatform.CustomJob(\n",
        "    display_name=DISPLAY_NAME, worker_pool_specs=WORKER_POOL_SPECS\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_95FH8jeulvb"
      },
      "source": [
        "### Run the CustomJob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEbrY05Gulvb"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    job.run(sync=True)\n",
        "except Exception as e:\n",
        "    # may fail in multi-worker to find startup script\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R2Bnmwmulvb"
      },
      "source": [
        "### Delete a custom training job\n",
        "\n",
        "After a training job is completed, you can delete the training job with the `delete()` method.  Prior to completion, a training job can be cancelled with the `cancel()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1geVE3Lulvb"
      },
      "outputs": [],
      "source": [
        "job.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpu_intro"
      },
      "source": [
        "## Cloud TPU Training\n",
        "\n",
        "To further speed up trainig, your organization can utilize Google's Cloud Tensor Processing Units (TPU) pods.\n",
        "\n",
        "Cloud TPU is the custom-designed machine learning ASIC that powers Google products like Translate, Photos, Search, Assistant, and Gmail. Cloud TPU is designed to run cutting-edge machine learning models with AI services on Google Cloud. And its custom high-speed network offers over 100 petaflops of performance in a single pod.\n",
        "\n",
        "Learn more about [Cloud TPU](https://cloud.google.com/tpu).\n",
        "\n",
        "**Note**: TPU VM Training is currently an opt-in feature. Your GCP project must first be added to the feature allowlist. Please email your project information(project id/number) to vertex-ai-tpu-vm-training-support@google.com for the allowlist. You receive an email as soon as your project is ready."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "docker_write:tpu"
      },
      "source": [
        "### Write Docker file for TPU training\n",
        "\n",
        "Currently, there is no pre-built Vertex AI Docker image for training with TPUs. However, you can make your own, using the following steps:\n",
        "\n",
        "1. Create a vanilla Python 3 image (e.g., `python3:8`).\n",
        "2. Get and install the TPU library (`libtpu.so`).\n",
        "3. Copy trainer code from your training package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQVPtknpulvb"
      },
      "outputs": [],
      "source": [
        "%%writefile custom/Dockerfile\n",
        "FROM python:3.8\n",
        "\n",
        "WORKDIR /\n",
        "\n",
        "# Copies the trainer code to the docker image.\n",
        "COPY trainer /trainer\n",
        "\n",
        "RUN pip3 install tensorflow-datasets\n",
        "\n",
        "# Install TPU Tensorflow and dependencies.\n",
        "# libtpu.so must be under the '/lib' directory.\n",
        "RUN wget https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/libtpu/20210525/libtpu.so -O /lib/libtpu.so\n",
        "RUN chmod 777 /lib/libtpu.so\n",
        "\n",
        "RUN wget https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/tensorflow/20210525/tf_nightly-2.6.0-cp38-cp38-linux_x86_64.whl\n",
        "RUN pip3 install tf_nightly-2.6.0-cp38-cp38-linux_x86_64.whl\n",
        "RUN rm tf_nightly-2.6.0-cp38-cp38-linux_x86_64.whl\n",
        "# Sets up the entry point to invoke the trainer.\n",
        "ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "docker_push:tpu"
      },
      "source": [
        "### Build and push the Docker image to the Artifact Registry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_d_zEXUulvc"
      },
      "outputs": [],
      "source": [
        "# Specify the repository name where the image needs to be uploaded\n",
        "TPU_TRAIN_IMAGE = f\"{LOCATION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/tpu-train:latest\"\n",
        "\n",
        "if not IS_COLAB:\n",
        "    # build the image locally with the repository name as a tag\n",
        "    ! docker build --quiet -t $TPU_TRAIN_IMAGE custom\n",
        "    # push the image to artifact registry\n",
        "    ! docker push $TPU_TRAIN_IMAGE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1662bb56b144"
      },
      "source": [
        "If you are using Colab, run the below cell to build and push the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8099b51c559c"
      },
      "outputs": [],
      "source": [
        "%%bash -s $IS_COLAB $TPU_TRAIN_IMAGE\n",
        "if [ $1 == \"False\" ]; then\n",
        "  exit 0\n",
        "fi\n",
        "set -x\n",
        "dockerd -b none --iptables=0 -l warn &\n",
        "for i in $(seq 5); do [ ! -S \"/var/run/docker.sock\" ] && sleep 2 || break; done\n",
        "docker build --quiet custom -t $2\n",
        "docker push $2\n",
        "kill $(jobs -p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "worker_pool_tpu"
      },
      "source": [
        "### TPU worker specification pool\n",
        "\n",
        "Next, create the worker specification pool. \n",
        "\n",
        "For TPUs:\n",
        "\n",
        "- Create only one worker pool (Primary).\n",
        "- Set the machine type to `cloud-tpu`.\n",
        "- Set the accelerator type to `TPU`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d514eU7lulvc"
      },
      "outputs": [],
      "source": [
        "# Use TPU Accelerators. Temporarily using numeric codes, until types are added to the SDK\n",
        "#   6 = TPU_V2\n",
        "#   7 = TPU_V3\n",
        "TRAIN_TPU, TRAIN_NTPU = (7, 8)\n",
        "TRAIN_COMPUTE = \"cloud-tpu\"\n",
        "\n",
        "\n",
        "if not TRAIN_NTPU or TRAIN_NTPU < 2:\n",
        "    TRAIN_STRATEGY = \"single\"\n",
        "else:\n",
        "    TRAIN_STRATEGY = \"tpu\"\n",
        "print(TRAIN_STRATEGY)\n",
        "\n",
        "EPOCHS = 20\n",
        "STEPS = 10000\n",
        "\n",
        "TRAINER_ARGS = [\n",
        "    \"--epochs=\" + str(EPOCHS),\n",
        "    \"--steps=\" + str(STEPS),\n",
        "    \"--distribute=\" + TRAIN_STRATEGY,\n",
        "]\n",
        "\n",
        "\n",
        "WORKER_POOL_SPECS = [\n",
        "    {\n",
        "        \"container_spec\": {\n",
        "            \"args\": TRAINER_ARGS,\n",
        "            \"image_uri\": TPU_TRAIN_IMAGE,\n",
        "        },\n",
        "        \"replica_count\": 1,\n",
        "        \"machine_spec\": {\n",
        "            \"machine_type\": TRAIN_COMPUTE,\n",
        "            \"accelerator_type\": TRAIN_TPU,\n",
        "            \"accelerator_count\": TRAIN_NTPU,\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "print(WORKER_POOL_SPECS[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RruSqNfrulvc"
      },
      "source": [
        "### Create CustomJob with worker pool specifications\n",
        "\n",
        "Next, create a `CustomJob` for the multi-worker distributed training job:\n",
        "\n",
        "- `display_name`: The display name for the custom job.\n",
        "\n",
        "- `worker_pool_specs`: The detailed specifications for each worker pool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QvSqbbHulvc"
      },
      "outputs": [],
      "source": [
        "DISPLAY_NAME = \"boston-unique\"\n",
        "\n",
        "job = aiplatform.CustomJob(\n",
        "    display_name=DISPLAY_NAME, worker_pool_specs=WORKER_POOL_SPECS\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw4L3UIfulvd"
      },
      "source": [
        "### Run the CustomJob\n",
        "\n",
        "Next, run the custom job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmqCNS78ulvd"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    job.run(sync=True)\n",
        "except Exception as e:\n",
        "    # may fail in multi-worker to find startup script\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWZoH9QKulvd"
      },
      "source": [
        "### Delete custom training job\n",
        "\n",
        "After a training job is completed, you can delete the training job with the `delete()` method.  Prior to completion, a training job can be cancelled with the `cancel()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lt8BJ4iBulvd"
      },
      "outputs": [],
      "source": [
        "job.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cleanup"
      },
      "source": [
        "# Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:\n",
        "\n",
        "\n",
        "- Cloud Storage bucket\n",
        "- Artifact Registry repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U98Wzc01ulvd"
      },
      "outputs": [],
      "source": [
        "# Set this to true only if you'd like to delete your bucket\n",
        "delete_bucket = False\n",
        "\n",
        "if delete_bucket:\n",
        "    ! gsutil rm -r $BUCKET_URI\n",
        "\n",
        "# Delete the repo in Artifact Registry\n",
        "!gcloud artifacts repositories delete --location=$LOCATION $REPO_NAME --quiet\n",
        "\n",
        "# Delete the locally generated files\n",
        "!rm -rf custom/\n",
        "!rm custom.tar.gz"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "get_started_with_vertex_distributed_training.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

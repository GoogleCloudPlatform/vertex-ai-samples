{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Vertex AI - Gemma distributed tuning with LoRA on TPUv5e, serving on L4 GPU\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/training/tpuv5e_gemma_peft_finetuning_and_serving.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab (Will require the higher memory Colab pro)\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/training/tpuv5e_gemma_peft_finetuning_and_serving.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/training/tpuv5e_gemma_peft_finetuning_and_serving.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "Open in Vertex AI Workbench\n",
        "    </a> (An e2-standard-8 CPU w/ 250GB disk is recommended)\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook is based on the [LoRA tuning example on ai.google.dev](https://ai.google.dev/gemma/docs/distributed_tuning). It follows an existing [Model Garden example written for fine-tuning on GPUs](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma_finetuning_on_vertex.ipynb), and has been modified to use the latest TPUv5e chips for training. It demonstrates fine-tuning and deploying Gemma models with [Vertex AI Custom Training Job](https://cloud.google.com/vertex-ai/docs/training/create-custom-job). A Vertex AI Custom Training Job allows for a higher level of customization and control over the fine-tuning job. All of the examples in this notebook use parameter efficient fine-tuning methods [PEFT](https://github.com/huggingface/peft) to reduce training and storage costs.\n",
        "\n",
        "This notebook deploys the model with a [vLLM](https://github.com/vllm-project/vllm) docker\n",
        "\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Fine-tune and deploy Gemma models with a Vertex AI Custom Training Job.\n",
        "- Send prediction requests to your fine-tuned Gemma model.\n",
        "\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08d289fa873f"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "In this example, you will use the IMDB reviews dataset from TensorFlow datasets to finetune the model. Details of the dataset can be found here: https://www.tensorflow.org/datasets/catalog/imdb_reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aed92deeb4a0"
      },
      "source": [
        "### Costs \n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "Vertex AI (Training, TPUv5e, L4 GPU), Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), [Cloud NL API pricing](https://cloud.google.com/natural-language/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7EUnXsZhAGF"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Install the following packages required to execute this notebook. \n",
        "\n",
        "Run this to install the latest google cloud platform library that supports TPUv5e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b4ef9b72d43"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# (optional) update gcloud if needed\n",
        "if os.getenv(\"IS_TESTING\"):\n",
        "    ! gcloud components update --quiet\n",
        "\n",
        "! pip3 install --upgrade --quiet google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58707a750154"
      },
      "source": [
        "### Colab only: Uncomment the following cell to restart the kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f200f10a1da3"
      },
      "outputs": [],
      "source": [
        "# import IPython\n",
        "\n",
        "# app = IPython.Application.instance()\n",
        "# app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF1j6f9HApxa"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "3. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "4. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
        "\n",
        "1. [Select or create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs.\n",
        "\n",
        "1. [Create a service account](https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console) with `Vertex AI User` and `Storage Object Admin` roles for deploying fine tuned model to Vertex AI endpoint."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "909aa61d1f13"
      },
      "source": [
        "### Kaggle credentials\n",
        "Gemma models are hosted by Kaggle. To use Gemma, request access on Kaggle:\n",
        "\n",
        "* Sign in or register at [kaggle.com](https://www.kaggle.com)\n",
        "* Open the [Gemma model card](https://www.kaggle.com/models/google/gemma) and select \"Request Access\"\n",
        "* Complete the consent form and accept the terms and conditions\n",
        "\n",
        "Then, to use the Kaggle API, create an API token:\n",
        "\n",
        "* Open [Kaggle settings](https://www.kaggle.com/settings)\n",
        "* Select \"Create New Token\"\n",
        "* A kaggle.json file is downloaded. It contains your Kaggle credentials. Note the username and key as you will populate this later"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WReHDGG5g0XY"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, try the following:\n",
        "* Run `gcloud config list`.\n",
        "* Run `gcloud projects list`.\n",
        "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM1iC_MfAts1"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations).\n",
        "\n",
        "TPUv5e is available in the [following regions listed here](https://cloud.google.com/tpu/pricing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "region"
      },
      "outputs": [],
      "source": [
        "REGION = \"us-west1\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBCra4QMA2wR"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74ccc9e52986"
      },
      "source": [
        "**1. Vertex AI Workbench**\n",
        "* Do nothing as you are already authenticated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de775a3773ba"
      },
      "source": [
        "**2. Local JupyterLab instance, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "254614fa0c46"
      },
      "outputs": [],
      "source": [
        "# ! gcloud auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef21552ccea8"
      },
      "source": [
        "**3. Colab, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "603adbbf0532"
      },
      "outputs": [],
      "source": [
        "# from google.colab import auth\n",
        "# auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6b2ccc891ed"
      },
      "source": [
        "**4. Service account or other**\n",
        "* See how to grant Cloud Storage permissions to your service account at https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "960505627ddf"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "from google.cloud import aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgPO1eR3CYjk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "Create a storage bucket to store intermediate artifacts such as datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzGDU7TWdts_"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "648eb19b83dd"
      },
      "source": [
        "#### Set folder paths for staging, environment, and model artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "855d6b96f291"
      },
      "outputs": [],
      "source": [
        "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
        "MODEL_BUCKET = os.path.join(BUCKET_URI, \"model\")\n",
        "\n",
        "# The service account looks like:\n",
        "# '@.iam.gserviceaccount.com'\n",
        "# Please go to https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console\n",
        "# and create service account with `Vertex AI User` and `Storage Object Admin` roles.\n",
        "# The service account for deploying fine tuned model.\n",
        "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EcIXiGsCePi"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIq7R4HZCfIc"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "outputs": [],
      "source": [
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c13158fa7be"
      },
      "source": [
        "### Select the Gemma base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d698e6e0c126"
      },
      "outputs": [],
      "source": [
        "# The Gemma base model.\n",
        "base_model = \"google/gemma-2b\"  # @param [\"google/gemma-2b\", \"google/gemma-2b-it\", \"google/gemma-7b\", \"google/gemma-7b-it\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "832bbd8cb9ef"
      },
      "source": [
        "### Create the artifact registry repository and set the custom docker image uri"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18fd49b88127"
      },
      "outputs": [],
      "source": [
        "REPOSITORY = \"tpuv5e-training-repository-unique\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2242d87effa"
      },
      "outputs": [],
      "source": [
        "image_name_train = \"gemma-lora-tuning-tpuv5e\"\n",
        "hostname = f\"{REGION}-docker.pkg.dev\"\n",
        "tag = \"latest\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dd94acb26e9"
      },
      "outputs": [],
      "source": [
        "# Register gcloud as a Docker credential helper\n",
        "!gcloud auth configure-docker $REGION-docker.pkg.dev --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c8e8364a097"
      },
      "outputs": [],
      "source": [
        "# One time or use an existing repository\n",
        "!gcloud artifacts repositories create $REPOSITORY --repository-format=docker \\\n",
        "--location=$REGION --description=\"Vertex TPUv5e training repository\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16488f1fc702"
      },
      "outputs": [],
      "source": [
        "# Define container image name\n",
        "KERAS_TRAIN_DOCKER_URI = (\n",
        "    f\"{hostname}/{PROJECT_ID}/{REPOSITORY}/{image_name_train}:{tag}\"\n",
        ")\n",
        "\n",
        "# Set the docker image uri for the vLLM serving container\n",
        "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240220_0936_RC01\"\n",
        "\n",
        "# Set the docker image uri for the model conversion container that converts the fine-tuned model to HF format\n",
        "KERAS_MODEL_CONVERSION_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/jax-keras-model-conversion:20240220_0936_RC01\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c250872074f"
      },
      "source": [
        "### Define common functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "354da31189dc"
      },
      "outputs": [],
      "source": [
        "def get_job_name_with_datetime(prefix: str) -> str:\n",
        "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
        "    jobs in Vertex AI.\n",
        "    \"\"\"\n",
        "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
        "\n",
        "\n",
        "def deploy_model_vllm(\n",
        "    model_name: str,\n",
        "    model_uri: str,\n",
        "    service_account: str,\n",
        "    machine_type: str = \"g2-standard-8\",\n",
        "    accelerator_type: str = \"NVIDIA_L4\",\n",
        "    accelerator_count: int = 1,\n",
        "    max_model_len: int = 8192,\n",
        "    dtype: str = \"bfloat16\",\n",
        ") -> tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    # Upload the model to \"Model Registry\"\n",
        "    job_name = get_job_name_with_datetime(model_name)\n",
        "    vllm_args = [\n",
        "        \"--host=0.0.0.0\",\n",
        "        \"--port=7080\",\n",
        "        f\"--tensor-parallel-size={accelerator_count}\",\n",
        "        \"--swap-space=16\",\n",
        "        \"--gpu-memory-utilization=0.95\",\n",
        "        f\"--max-model-len={max_model_len}\",\n",
        "        f\"--dtype={dtype}\",\n",
        "        \"--disable-log-stats\",\n",
        "    ]\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=job_name,\n",
        "        artifact_uri=model_uri,\n",
        "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
        "        serving_container_command=[\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
        "        serving_container_args=vllm_args,\n",
        "        serving_container_ports=[7080],\n",
        "        serving_container_predict_route=\"/generate\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "    )\n",
        "\n",
        "    # Deploy the model to an endpoint to serve \"Online predictions\"\n",
        "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=service_account,\n",
        "        sync=False,\n",
        "    )\n",
        "\n",
        "    return model, endpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e11e1f8a6c4c"
      },
      "source": [
        "### Build the Docker container files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19d584f9fe62"
      },
      "source": [
        "#### Create the trainer directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "909e93e7fd43"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists(\"trainer\"):\n",
        "    os.makedirs(\"trainer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c725bbcabe27"
      },
      "source": [
        "#### Kaggle credentials are required for KerasNLP training and Hex-LLM deployment with TPUs.\n",
        "Set the KAGGLE_USERNAME AND KAGGLE_KEY to pass in as an environment variable for Vertex Training to use\n",
        "Fenerate the Kaggle username and key by following [these instructions](https://github.com/Kaggle/kaggle-api?tab=readme-ov-file#api-credentials).\n",
        "You will need to review and accept the model license as mentioned earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0480a14d88e3"
      },
      "outputs": [],
      "source": [
        "KAGGLE_USERNAME = \"your-kaggle-username\"  # @param {type:\"string\", isTemplate:true}\n",
        "KAGGLE_KEY = \"your-kaggle-key\"  # @param {type:\"string\", isTemplate:true}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a91ddf0cbcb"
      },
      "source": [
        "#### Create the Dockerfile for the custom container. This will install JAX[TPU], Keras, and TensorFlow datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "756577886992"
      },
      "outputs": [],
      "source": [
        "%%writefile trainer/Dockerfile\n",
        "# This Dockerfile fine tunes the Gemma model using LoRA with JAX\n",
        "\n",
        "FROM python:3.10\n",
        "\n",
        "ENV DEBIAN_FRONTEND=noninteractive\n",
        "\n",
        "# Install basic libs\n",
        "RUN apt-get update && apt-get -y upgrade && apt-get install -y --no-install-recommends \\\n",
        "        cmake \\\n",
        "        curl \\\n",
        "        wget \\\n",
        "        sudo \\\n",
        "        gnupg \\\n",
        "        libsm6 \\\n",
        "        libxext6 \\\n",
        "        libxrender-dev \\\n",
        "        lsb-release \\\n",
        "        ca-certificates \\\n",
        "        build-essential \\\n",
        "        git \\\n",
        "        libgl1\n",
        "\n",
        "# Copy Apache license.\n",
        "RUN wget https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/LICENSE\n",
        "\n",
        "# Install required libs\n",
        "RUN pip install --upgrade pip\n",
        "RUN pip install jax[tpu]==0.4.25 -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "RUN pip install tensorflow==2.15.0.post1\n",
        "RUN pip install tensorflow-datasets==4.9.4\n",
        "RUN pip install -q -U keras-nlp==0.8.2\n",
        "RUN pip install keras==3.0.5\n",
        "\n",
        "# Copy other licenses.\n",
        "RUN wget -O MIT_LICENSE https://github.com/pytest-dev/pytest/blob/main/LICENSE\n",
        "RUN wget -O BSD_LICENSE https://github.com/pytorch/xla/blob/master/LICENSE\n",
        "RUN wget -O BSD-3_LICENSE https://github.com/pytorch/pytorch/blob/main/LICENSE\n",
        "\n",
        "ENV KERAS_BACKEND=jax\n",
        "ENV XLA_PYTHON_CLIENT_MEM_FRACTION=0.9\n",
        "ENV TPU_LIBRARY_PATH=/lib/libtpu.so\n",
        "\n",
        "# Copy install libtpu to PATH above\n",
        "RUN find ./usr/local/lib -name 'libtpu.so' -exec cp {} /lib \\;\n",
        "\n",
        "WORKDIR /\n",
        "COPY train.py train.py\n",
        "ENV PYTHONPATH ./\n",
        "\n",
        "ENTRYPOINT [\"python\", \"train.py\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ee7ac9469ce"
      },
      "source": [
        "#### Add the __init__.py file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0b57ecd02d8"
      },
      "outputs": [],
      "source": [
        "!touch trainer/__init__.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c648296e1057"
      },
      "source": [
        "#### Add the train.py file\n",
        "This code is from the LoRA distributed fine-tuning code from this example: https://ai.google.dev/gemma/docs/distributed_tuning\n",
        "\n",
        "The IMDB TensorFlow dataset is used to fine-tune the Gemma model. Additional logic is added to handle the TPU topology setting required by TPUv5e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b6a47a6089c"
      },
      "outputs": [],
      "source": [
        "%%writefile trainer/train.py\n",
        "import os\n",
        "import argparse\n",
        "import shutil\n",
        "import locale\n",
        "\n",
        "# Model saving variables\n",
        "_ENCODING_FOR_MODEL_SAVING = \"UTF-8\"\n",
        "_VOCABULARY_FILENAME = \"vocabulary.spm\"\n",
        "_TOKENIZER_FILENAME = \"tokenizer.model\"\n",
        "\n",
        "import keras\n",
        "import keras_nlp\n",
        "import tensorflow\n",
        "import tensorflow_datasets as tfds\n",
        "print (keras.__version__)\n",
        "print (tensorflow.__version__)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\n",
        "    \"--tpu_topology\",\n",
        "    help=\"Topology to use for the TPUv5e (1x1, 1x4, 2x2, etc.)\",\n",
        "    type=str\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--model_name\",\n",
        "    help=\"Kaggle model name (gemma_2b_en, gemma_7b_en)\",\n",
        "    type=str\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--output_folder\",\n",
        "    type=str,\n",
        "    required=True,\n",
        "    help=\"Path to the output folder.\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--checkpoint_filename\",\n",
        "    type=str,\n",
        "    default=\"fine_tuned.weights.h5\",\n",
        "    help=\"Checkpoint filename.\",\n",
        ")\n",
        "args = parser.parse_args()\n",
        "\n",
        "def main():\n",
        "    x = args.tpu_topology.split(\"x\")\n",
        "    tpu_topology_x = int(x[0])\n",
        "    tpu_topology_y = int(x[1])\n",
        "    print (f'TPU topology is ({tpu_topology_x}, {tpu_topology_y})')\n",
        "    print (f'Model name is {args.model_name}')\n",
        "\n",
        "    device_mesh = keras.distribution.DeviceMesh(\n",
        "        (tpu_topology_x, tpu_topology_y),\n",
        "        [\"batch\", \"model\"],\n",
        "        devices=keras.distribution.list_devices())\n",
        "\n",
        "    model_dim = \"model\"\n",
        "\n",
        "    layout_map = keras.distribution.LayoutMap(device_mesh)\n",
        "    # Weights that match 'token_embedding/embeddings' will be sharded on 8 TPUs\n",
        "    layout_map[\"token_embedding/embeddings\"] = (None, model_dim)\n",
        "    # Regex to match against the query, key and value matrices in the decoder\n",
        "    # attention layers\n",
        "    layout_map[\"decoder_block.*attention.*(query|key|value).*kernel\"] = (\n",
        "        None, model_dim, None)\n",
        "    layout_map[\"decoder_block.*attention_output.*kernel\"] = (\n",
        "        None, None, model_dim)\n",
        "    layout_map[\"decoder_block.*ffw_gating.*kernel\"] = (model_dim, None)\n",
        "    layout_map[\"decoder_block.*ffw_linear.*kernel\"] = (None, model_dim)\n",
        "    model_parallel = keras.distribution.ModelParallel(device_mesh, layout_map,\n",
        "                                                    batch_dim_name=\"batch\")\n",
        "    keras.distribution.set_distribution(model_parallel)\n",
        "    model_name = args.model_name\n",
        "    gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(args.model_name)\n",
        "    print (f'Running inference on the base {args.model_name} model')\n",
        "    lm_output = gemma_lm.generate(\"Prompt: Return 3 things I ask for in this format. \\\n",
        "        Response: 1) item 1 2) item 2 3) item 3. \\\n",
        "        Prompt: List the 3 best comedy movies in the 90s Response: \", max_length=100)\n",
        "    print (lm_output)\n",
        "\n",
        "    # Start training\n",
        "    imdb_train = tfds.load(\n",
        "        \"imdb_reviews\",\n",
        "        split=\"train\",\n",
        "        as_supervised=True,\n",
        "        batch_size=2,\n",
        "    )\n",
        "    # Drop labels.\n",
        "    imdb_train = imdb_train.map(lambda x, y: x)\n",
        "\n",
        "    imdb_train.unbatch().take(1).get_single_element().numpy()\n",
        "\n",
        "    gemma_lm.backbone.enable_lora(rank=4)\n",
        "\n",
        "    # Fine-tune on the IMDb movie reviews dataset.\n",
        "\n",
        "    # Limit the input sequence length to 128 to control memory usage.\n",
        "    gemma_lm.preprocessor.sequence_length = 128\n",
        "    # Use AdamW (a common optimizer for transformer models).\n",
        "    optimizer = keras.optimizers.AdamW(learning_rate=5e-5,weight_decay=0.01,)\n",
        "\n",
        "    # Exclude layernorm and bias terms from decay.\n",
        "    optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
        "\n",
        "    gemma_lm.compile(\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        optimizer=optimizer,\n",
        "        weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    )\n",
        "    gemma_lm.summary()\n",
        "    gemma_lm.fit(imdb_train, epochs=1)\n",
        "\n",
        "    print (f'Running inference on the fine-tuned {args.model_name} model')\n",
        "    lm_output = gemma_lm.generate(\"Prompt: Return 3 things I ask for in this format. \\\n",
        "        Response: 1) item 1 2) item 2 3) item 3. \\\n",
        "        Prompt: List the 3 best comedy movies in the 90s Response: \", max_length=100)\n",
        "    print (lm_output) \n",
        "\n",
        "    # Save checkpoint and tokenizer.\n",
        "    print(\"Saving checkpoint and tokenizer.\")\n",
        "    if not os.path.exists(args.output_folder):\n",
        "        os.makedirs(args.output_folder)\n",
        "    locale.getpreferredencoding = lambda: _ENCODING_FOR_MODEL_SAVING\n",
        "    gemma_lm.save_weights(\n",
        "        os.path.join(args.output_folder, args.checkpoint_filename)\n",
        "    )\n",
        "    gemma_lm.preprocessor.tokenizer.save_assets(args.output_folder)\n",
        "\n",
        "    # Copy and rename the tokenizer file.\n",
        "    print(\"Copying tokenizer file.\")\n",
        "    shutil.copy(\n",
        "        os.path.join(args.output_folder, _VOCABULARY_FILENAME),\n",
        "        os.path.join(args.output_folder, _TOKENIZER_FILENAME),\n",
        "    )\n",
        "    print ('Exiting job')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq4iF00YG_4T"
      },
      "source": [
        "## Fine-tune with Vertex AI Custom Training Jobs\n",
        "\n",
        "This section demonstrates how to fine-tune and deploy Gemma models with PEFT LoRA on Vertex AI Custom Training Jobs. LoRA (Low-Rank Adaptation) is one approach of PEFT (Parameter Efficient Fine-tuning), where pretrained model weights are frozen and rank decomposition matrices representing the change in model weights are trained during fine-tuning. Read more about LoRA in the following publication: [Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L. and Chen, W., 2021. Lora: Low-rank adaptation of large language models. *arXiv preprint arXiv:2106.09685*](https://arxiv.org/abs/2106.09685)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1b90914fc81"
      },
      "source": [
        "#### Enable docker to run as a regular user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "232a259d3edc"
      },
      "outputs": [],
      "source": [
        "!sudo usermod -a -G docker ${USER}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e028d107fbe5"
      },
      "source": [
        "#### Change to the trainer directory to build the docker container"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3a9390f87b66"
      },
      "outputs": [],
      "source": [
        "%cd trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81eb3c13afa9"
      },
      "source": [
        "#### Build the custom docker container and push to artifact registry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee497559677f"
      },
      "outputs": [],
      "source": [
        "!docker build -t $KERAS_TRAIN_DOCKER_URI -f Dockerfile ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0715b34162b4"
      },
      "outputs": [],
      "source": [
        "!docker push $KERAS_TRAIN_DOCKER_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "586a13cb67b4"
      },
      "source": [
        "#### Change back to your home directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "937b7269c93b"
      },
      "outputs": [],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08bf867e8f4c"
      },
      "source": [
        "#### Set GCS folder locations and job configurations settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17392b36e9c0"
      },
      "outputs": [],
      "source": [
        "# Create a GCS folder to store the merged model with the base model and the\n",
        "# fine-tuned LORA adapter.\n",
        "merged_model_dir = get_job_name_with_datetime(\"gemma-lora-model-tpuv5\")\n",
        "merged_model_output_dir = os.path.join(MODEL_BUCKET, merged_model_dir)\n",
        "merged_model_output_dir_gcsfuse = merged_model_output_dir.replace(\"gs://\", \"/gcs/\")\n",
        "\n",
        "# Set the checkpoint output filename\n",
        "checkpoint_filename = \"fine_tuned.weights.h5\"\n",
        "\n",
        "DISPLAY_NAME_PREFIX = \"gemma-lora-train\"  # @param {type:\"string\"}\n",
        "tpuv5e_gemma_peft_job = {\n",
        "    \"display_name\": get_job_name_with_datetime(DISPLAY_NAME_PREFIX),\n",
        "    \"job_spec\": {\n",
        "        \"worker_pool_specs\": [\n",
        "            {\n",
        "                \"machine_spec\": {\n",
        "                    \"machine_type\": \"ct5lp-hightpu-1t\",\n",
        "                    \"tpu_topology\": \"1x1\",\n",
        "                },\n",
        "                \"replica_count\": 1,\n",
        "                \"container_spec\": {\n",
        "                    \"image_uri\": KERAS_TRAIN_DOCKER_URI,\n",
        "                    \"args\": [\n",
        "                        \"--tpu_topology=1x1\",\n",
        "                        \"--model_name=gemma_2b_en\",\n",
        "                        f\"--output_folder={merged_model_output_dir_gcsfuse}\",\n",
        "                        f\"--checkpoint_filename={checkpoint_filename}\",\n",
        "                    ],\n",
        "                    \"env\": [\n",
        "                        {\"name\": \"KAGGLE_USERNAME\", \"value\": KAGGLE_USERNAME},\n",
        "                        {\"name\": \"KAGGLE_KEY\", \"value\": KAGGLE_KEY},\n",
        "                    ],\n",
        "                },\n",
        "            },\n",
        "        ],\n",
        "    },\n",
        "}\n",
        "\n",
        "tpuv5e_gemma_peft_job"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c2bc267c4a5"
      },
      "source": [
        "#### Create job client and run job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc9b403c8515"
      },
      "outputs": [],
      "source": [
        "job_client = aiplatform.gapic.JobServiceClient(\n",
        "    client_options=dict(api_endpoint=f\"{REGION}-aiplatform.googleapis.com\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4da63b786392"
      },
      "outputs": [],
      "source": [
        "create_tpuv5e_gemma_peft_job_response = job_client.create_custom_job(\n",
        "    parent=\"projects/{project}/locations/{location}\".format(\n",
        "        project=PROJECT_ID, location=REGION\n",
        "    ),\n",
        "    custom_job=tpuv5e_gemma_peft_job,\n",
        ")\n",
        "print(create_tpuv5e_gemma_peft_job_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb8dfcc23789"
      },
      "source": [
        "#### Check on job progress\n",
        "This may take 20-60 minutes or more depending on the model size. Run this cell multiple times to check progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f402309d9dbb"
      },
      "outputs": [],
      "source": [
        "get_tpuv5e_gemma_peft_job_response = job_client.get_custom_job(\n",
        "    name=create_tpuv5e_gemma_peft_job_response.name\n",
        ")\n",
        "get_tpuv5e_gemma_peft_job_response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb7f4f1ac160"
      },
      "source": [
        "#### Click on the console log url output from this cell to see your logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "babf40cf7821"
      },
      "outputs": [],
      "source": [
        "job_id = create_tpuv5e_gemma_peft_job_response.name[\n",
        "    create_tpuv5e_gemma_peft_job_response.name.rfind(\"/\") + 1 :\n",
        "]\n",
        "startdate = datetime.today() - timedelta(days=1)\n",
        "startdate = startdate.strftime(\"%Y-%m-%d\")\n",
        "print(\n",
        "    f\"https://console.cloud.google.com/logs/query;query=resource.labels.job_id=%22{job_id}%22%20timestamp%3E={startdate}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0a1e0dbdf24"
      },
      "source": [
        "### Convert the fine-tuned Keras checkpoint to HF format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae10c82ba26e"
      },
      "source": [
        "#### Download the conversion script from KerasNLP tools\n",
        "The GitHub repo is https://github.com/keras-team/keras-nlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b02a8f0a54ab"
      },
      "outputs": [],
      "source": [
        "!wget -nv -nc https://raw.githubusercontent.com/keras-team/keras-nlp/master/tools/gemma/export_gemma_to_hf.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c77580797b26"
      },
      "source": [
        "#### Download the fine-tuned checkpoint files locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba0e4080beba"
      },
      "outputs": [],
      "source": [
        "!gcloud storage cp -r $merged_model_output_dir ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52c738bc53fc"
      },
      "source": [
        "#### Install libraries for model conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc8e6fc5327e"
      },
      "outputs": [],
      "source": [
        "!pip install torch==2.1\n",
        "!pip install --upgrade keras-nlp\n",
        "!pip install --upgrade keras>=3\n",
        "!pip install --upgrade accelerate sentencepiece transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c27b3cdd841b"
      },
      "source": [
        "#### Run the model conversion script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70e543b66fd1"
      },
      "outputs": [],
      "source": [
        "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
        "os.environ[\"KAGGLE_USERNAME\"] = KAGGLE_USERNAME\n",
        "os.environ[\"KAGGLE_KEY\"] = KAGGLE_KEY\n",
        "MODEL_SIZE=\"2b\"\n",
        "!KERAS_BACKEND=torch python export_gemma_to_hf.py \\\n",
        "  --weights_file ./$merged_model_dir/fine_tuned.weights.h5 \\\n",
        "  --size $MODEL_SIZE \\\n",
        "  --vocab_path ./$merged_model_dir/vocabulary.spm \\\n",
        "  --output_dir ./$merged_model_dir/fine_tuned_gg_hf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e81e9115533"
      },
      "source": [
        "#### Copy converted HF files to GCS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2f307e01787"
      },
      "outputs": [],
      "source": [
        "HUGGINGFACE_MODEL_DIR = os.path.join(\"./\", merged_model_dir, \"fine_tuned_gg_hf\")\n",
        "HUGGINGFACE_MODEL_DIR_GCS = os.path.join(merged_model_output_dir, \"fine_tuned_gg_hf\")\n",
        "HUGGINGFACE_MODEL_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3016583388ff"
      },
      "outputs": [],
      "source": [
        "!gcloud storage cp $HUGGINGFACE_MODEL_DIR/* $HUGGINGFACE_MODEL_DIR_GCS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LMBN_gDG_4U"
      },
      "source": [
        "### Deploy fine tuned models\n",
        "This section uploads the model to Model Registry and deploys it on the Endpoint using [vLLM](https://github.com/vllm-project/vllm)\n",
        "\n",
        "The model deployment step will take 15 minutes to 1 hour to complete, depending on the model sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e251550b01df"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME_VLLM = get_job_name_with_datetime(prefix=\"gemma-vllm-serve\")\n",
        "\n",
        "# Start with a G2 Series cost-effective configuration\n",
        "if MODEL_SIZE == \"2b\":\n",
        "    machine_type = \"g2-standard-8\"\n",
        "    accelerator_type = \"NVIDIA_L4\"\n",
        "    accelerator_count = 1\n",
        "elif MODEL_SIZE == \"7b\":\n",
        "    machine_type = \"g2-standard-12\"\n",
        "    accelerator_type = \"NVIDIA_L4\"\n",
        "    accelerator_count = 1\n",
        "else:\n",
        "    assert MODEL_SIZE in (\"2b\", \"7b\")\n",
        "\n",
        "# See supported machine/GPU configurations in chosen region:\n",
        "# https://cloud.google.com/vertex-ai/docs/predictions/configure-compute\n",
        "\n",
        "# For even more performance, consider V100 and A100 GPUs\n",
        "# > Nvidia Tesla V100\n",
        "# machine_type = \"n1-standard-8\"\n",
        "# accelerator_type = \"NVIDIA_TESLA_V100\"\n",
        "# > Nvidia Tesla A100\n",
        "# machine_type = \"a2-highgpu-1g\"\n",
        "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
        "\n",
        "# Larger `max_model_len` values will require more GPU memory\n",
        "max_model_len = 2048\n",
        "\n",
        "model, endpoint = deploy_model_vllm(\n",
        "    MODEL_NAME_VLLM,\n",
        "    HUGGINGFACE_MODEL_DIR_GCS,\n",
        "    SERVICE_ACCOUNT,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    max_model_len=max_model_len,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c95e08ad63e3"
      },
      "source": [
        "#### Click on the console log url output from this cell to see your logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6277300c5531"
      },
      "outputs": [],
      "source": [
        "startdate = datetime.today() - timedelta(days=1)\n",
        "startdate = startdate.strftime(\"%Y-%m-%d\")\n",
        "log_link = \"https://console.cloud.google.com/logs/query;query=resource.type=%22aiplatform.googleapis.com%2FEndpoint%22\"\n",
        "log_link += f\"%20resource.labels.endpoint_id=%22{endpoint.name}%22\"\n",
        "log_link += f\"%20resource.labels.location={REGION}\"\n",
        "log_link += f\"%20timestamp%3E={startdate}\"\n",
        "print(log_link)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GI363hMzG_4U"
      },
      "source": [
        "NOTE: The overall deployment can take 30-40 minutes or more. After the deployment succeeds (15-20 minutes or so), the fine-tuned model will be downloaded from the GCS bucket used in training above. Thus, an additional ~15-20 minutes (depending on the model sizes) of waiting time is needed **after** the model deployment step above succeeds and before you run the next step below. Otherwise you might see a `ServiceUnavailable: 503 502:Bad Gateway` error when you send requests to the endpoint.\n",
        "\n",
        "### Send a prediction request\n",
        "\n",
        "Once deployment succeeds, you can send requests to the endpoint with text prompts. Use the same example used earlier in the notebook\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "Prompt: Return 3 things I ask for in this format and do not repeat my prompt. Response: 1) item 1 2) item 2 3) item 3. List the 3 best comedy movies in the 90s Response:\n",
        "Response:  1) The Cable Guy 2) Scooby-Doo 3) Beethoven Requirements\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UYUNn60G_4U"
      },
      "outputs": [],
      "source": [
        "PROMPT = \"Prompt: Return 3 things I ask for in this format and do not repeat my prompt. \\\n",
        "Response: 1) item 1 2) item 2 3) item 3. \\\n",
        "Prompt: List the 3 best comedy movies in the 90s Response: \"\n",
        "\n",
        "instances = [\n",
        "    {\"prompt\": PROMPT},\n",
        "    {\"max_tokens\": 500},\n",
        "    {\"temperature\": 1.0},\n",
        "    {\"top_p\": 1.0},\n",
        "    {\"top_k\": 1.0},\n",
        "]\n",
        "\n",
        "response = endpoint.predict(instances=instances)\n",
        "\n",
        "for prediction in response.predictions:\n",
        "    print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af21a3cff1e0"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "911406c1561e"
      },
      "outputs": [],
      "source": [
        "# Delete the train job.\n",
        "job_client.delete_custom_job(name=create_tpuv5e_gemma_peft_job_response.name)\n",
        "\n",
        "# Undeploy model and delete endpoint.\n",
        "endpoint.delete(force=True)\n",
        "\n",
        "# Delete models.\n",
        "model.delete()\n",
        "\n",
        "import os\n",
        "\n",
        "# Delete Cloud Storage objects that were created\n",
        "delete_bucket = False\n",
        "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
        "    ! gsutil -m rm -r $BUCKET_URI"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "tpuv5e_gemma_peft_finetuning_and_serving.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

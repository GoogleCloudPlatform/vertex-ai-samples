{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Vertex AI - Llama2 fine-tuning with LoRA and serving on TPUv5e\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/training/tpuv5e_llama2_pytorch_finetuning_and_serving.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Ftraining%2Ftpuv5e_llama2_pytorch_finetuning_and_serving.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/training/tpuv5e_llama2_pytorch_finetuning_and_serving.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br>\n",
    "Open in Vertex AI Workbench\n",
    "    </a> \n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/training/tpuv5e_llama2_pytorch_finetuning_and_serving.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br>\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9e4695f5e9dc"
   },
   "source": [
    "## Quota - Make sure this is complete before you start!\n",
    "\n",
    "In order to run this example, you will need the following TPUv5e quota approved. You can make requests in the console via IAM & Admin > Quotas or by reaching out to your Google account team:\n",
    "\n",
    "aiplatform.googleapis.com/custom_model_serving_tpu_v5e (4-8 chips. 4 chips minimum for Llama2 7B)\n",
    "aiplatform.googleapis.com/custom_model_training_tpu_v5e (16 chips minimum)\n",
    "\n",
    "Check the [TPU pricing page](https://cloud.google.com/tpu/pricing) for the region availability and pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates fine-tuning of a Llama2 7B model with [LoRA](https://huggingface.co/docs/peft/v0.9.0/en/package_reference/lora#peft.LoraConfig), and uses a TPUv5e for both fine-tuning and serving. The fine-tuning is based on a [Hugging Face example](https://huggingface.co/google/gemma-7b/blob/main/examples/example_fsdp.py) that uses [fully sharded data parallel with PyTorch XLA](https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/), and [SPMD](https://pytorch.org/blog/pytorch-xla-spmd/). Follow the links to learn more.\n",
    "\n",
    "\n",
    "Fine-tuning occurs with a [Vertex AI Custom Training Job](https://cloud.google.com/vertex-ai/docs/training/create-custom-job). A Vertex AI Custom Training Job allows for a higher level of customization and control over the fine-tuning job. All of the examples in this notebook use parameter Low-Rank Adaption [LoRA](https://huggingface.co/docs/peft/en/package_reference/lora) to reduce training and storage costs.\n",
    "\n",
    "This notebook deploys the model using Hex-LLM, a High-Efficiency Large Language Model serving solution built with XLA that is being developed by Google Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "- Fine-tune and deploy Llama2 models with a Vertex AI Custom Training Job and Vertex Prediction endpoint.\n",
    "- Send prediction requests to your fine-tuned Llama2 model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08d289fa873f"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "In this example, you will use an english_quotes dataset from Hugging Face to fine-tune the model. Details of the dataset can be found here: https://huggingface.co/datasets/Abirate/english_quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aed92deeb4a0"
   },
   "source": [
    "### Costs \n",
    "\n",
    "This tutorial uses the following billable components of Google Cloud:\n",
    "\n",
    "- Vertex AI (Training, Prediction, TPUv5e)\n",
    "- Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), [Cloud NL API pricing](https://cloud.google.com/natural-language/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Vertex AI SDK for Python and other required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "! pip3 install --upgrade --quiet google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58707a750154"
   },
   "source": [
    "### Restart runtime (Colab only)\n",
    "\n",
    "To use the newly installed packages, you must restart the runtime on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "f200f10a1da3"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "Authenticate your environment on Google Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Google Cloud project information and initialize Vertex AI SDK for Python\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "PROJECT_ID = \"vertexai-service-project\"  # @param {type:\"string\"} ##TODO: DELETE LINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "### Location\n",
    "\n",
    "You can also change the `LOCATION` variable used by Vertex AI. Learn more about [Vertex AI locations](https://cloud.google.com/vertex-ai/docs/general/locations).\n",
    "\n",
    "TPUv5e is available in the [following locations listed here](https://cloud.google.com/tpu/pricing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "LOCATION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "Create a storage bucket to store intermediate artifacts such as datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://k-bucket-{PROJECT_ID}-tpuv5ellama\"  # @param {type:\"string\"} ##TODO: Delete line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "648eb19b83dd"
   },
   "source": [
    "#### Set folder paths for staging, environment, and model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "855d6b96f291"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://k-bucket-vertexai-service-project-tpuv5ellama/...\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI SDK for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up service account\n",
    "The service account looks like:\n",
    "`*@.iam.gserviceaccount.com`\n",
    "Please go to https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console\n",
    "and create service account with `Vertex AI User` and `Storage Object Admin` roles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The service account for deploying fine tuned model.\n",
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}\n",
    "SERVICE_ACCOUNT = \"tupv5e-notebook@vertexai-service-project.iam.gserviceaccount.com\"  # @param {type:\"string\"} ##TODO: DELETE LINE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2931d2e5a94"
   },
   "source": [
    "### Access Llama 2 pretrained and finetuned models\n",
    "The original models from Meta are converted into the Hugging Face format for finetuning and serving in Vertex AI.\n",
    "\n",
    "Accept the model agreement to access the models:\n",
    "\n",
    "1. Navigate to the Vertex AI > Model Garden page in the Google Cloud console\n",
    "2. Search for Llama 2\n",
    "3. Review the agreement that pops up on the model card page\n",
    "4. Accept the agreement of Llama 2\n",
    "5. On the documentation tab, a Cloud Storage bucket containing Llama 2 pretrained and finetuned models will be shared\n",
    "5. Paste the Cloud Storage bucket link below and assign it to VERTEX_AI_MODEL_GARDEN_LLAMA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "47330c9cae9f"
   },
   "outputs": [],
   "source": [
    "VERTEX_AI_MODEL_GARDEN_LLAMA2 = \"<Bucket path from documentation tab of Llama 2 in Vertex Model Garden>\"  # This will be shared once click the agreement of LLaMA2 in Vertex AI Model Garden.\n",
    "VERTEX_AI_MODEL_GARDEN_LLAMA2 = \"gs://vertex-model-garden-public-us-central1/llama2\" ##TODO: DELETE LINE\n",
    "VERTEX_MODEL_ID = \"llama2-7b-hf\"\n",
    "HF_MODEL_ID = \"meta-llama/Llama-2-7b-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "890b49a97355"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy Llama 2 model artifacts from gs://vertex-model-garden-public-us-central1/llama2 to  gs://k-bucket-vertexai-service-project-tpuv5ellama/meta-llama/Llama-2-7b-hf\n",
      "Copying gs://vertex-model-garden-public-us-central1/llama2/llama2-7b-hf/LICENSE to gs://k-bucket-vertexai-service-project-tpuv5ellama/meta-llama/Llama-2-7b-hf/LICENSE\n",
      "Copying gs://vertex-model-garden-public-us-central1/llama2/llama2-7b-hf/LLaMA V2 Model Preview User Guide.pdf to gs://k-bucket-vertexai-service-project-tpuv5ellama/meta-llama/Llama-2-7b-hf/LLaMA V2 Model Preview User Guide.pdf\n",
      "Copying gs://vertex-model-garden-public-us-central1/llama2/llama2-7b-hf/MODEL_CARD.md to gs://k-bucket-vertexai-service-project-tpuv5ellama/meta-llama/Llama-2-7b-hf/MODEL_CARD.md\n",
      "Copying gs://vertex-model-garden-public-us-central1/llama2/llama2-7b-hf/Notice-File.docx to gs://k-bucket-vertexai-service-project-tpuv5ellama/meta-llama/Llama-2-7b-hf/Notice-File.docx\n",
      "Copying gs://vertex-model-garden-public-us-central1/llama2/llama2-7b-hf/Responsible-Use-Guide.pdf to gs://k-bucket-vertexai-service-project-tpuv5ellama/meta-llama/Llama-2-7b-hf/Responsible-Use-Guide.pdf\n",
      "Copying gs://vertex-model-garden-public-us-central1/llama2/llama2-7b-hf/USE_POLICY.md to gs://k-bucket-vertexai-service-project-tpuv5ellama/meta-llama/Llama-2-7b-hf/USE_POLICY.md\n",
      "Copying gs://vertex-model-garden-public-us-central1/llama2/llama2-7b-hf/config.json to gs://k-bucket-vertexai-service-project-tpuv5ellama/meta-llama/Llama-2-7b-hf/config.json\n",
      "Copying gs://vertex-model-garden-public-us-central1/llama2/llama2-7b-hf/config.json.bak to gs://k-bucket-vertexai-service-project-tpuv5ellama/meta-llama/Llama-2-7b-hf/config.json.bak\n",
      "Copying gs://vertex-model-garden-public-us-central1/llama2/llama2-7b-hf/generation_config.json to gs://k-bucket-vertexai-service-project-tpuv5ellama/meta-llama/Llama-2-7b-hf/generation_config.json\n",
      "Copying gs://vertex-model-garden-public-us-central1/llama2/llama2-7b-hf/pytorch_model-00001-of-00002.bin to gs://k-bucket-vertexai-service-project-tpuv5ellama/meta-llama/Llama-2-7b-hf/pytorch_model-00001-of-00002.bin\n",
      "Copying gs://vertex-model-garden-public-us-central1/llama2/llama2-7b-hf/pytorch_model-00002-of-00002.bin to gs://k-bucket-vertexai-service-project-tpuv5ellama/meta-llama/Llama-2-7b-hf/pytorch_model-00002-of-00002.bin\n",
      "Copying gs://vertex-model-garden-public-us-central1/llama2/llama2-7b-hf/pytorch_model.bin.index.json to gs://k-bucket-vertexai-service-project-tpuv5ellama/meta-llama/Llama-2-7b-hf/pytorch_model.bin.index.json\n",
      "Copying gs://vertex-model-garden-public-us-central1/llama2/llama2-7b-hf/special_tokens_map.json to gs://k-bucket-vertexai-service-project-tpuv5ellama/meta-llama/Llama-2-7b-hf/special_tokens_map.json\n",
      "Copying gs://vertex-model-garden-public-us-central1/llama2/llama2-7b-hf/tokenizer.json to gs://k-bucket-vertexai-service-project-tpuv5ellama/meta-llama/Llama-2-7b-hf/tokenizer.json\n",
      "Copying gs://vertex-model-garden-public-us-central1/llama2/llama2-7b-hf/tokenizer.model to gs://k-bucket-vertexai-service-project-tpuv5ellama/meta-llama/Llama-2-7b-hf/tokenizer.model\n",
      "Copying gs://vertex-model-garden-public-us-central1/llama2/llama2-7b-hf/tokenizer_config.json to gs://k-bucket-vertexai-service-project-tpuv5ellama/meta-llama/Llama-2-7b-hf/tokenizer_config.json\n",
      "  Completed files 16/16 | 12.6GiB/12.6GiB                                      \n",
      "\n",
      "Average throughput: 86.6GiB/s\n"
     ]
    }
   ],
   "source": [
    "assert (\n",
    "    VERTEX_AI_MODEL_GARDEN_LLAMA2\n",
    "), \"Please click the agreement of Llama 2 in Vertex AI Model Garden, and get the GCS path of Llama 2 model artifacts.\"\n",
    "print(\n",
    "    \"Copy Llama 2 model artifacts from\",\n",
    "    VERTEX_AI_MODEL_GARDEN_LLAMA2,\n",
    "    \"to \",\n",
    "    f\"{BUCKET_URI}/{HF_MODEL_ID}\",\n",
    ")\n",
    "\n",
    "# Copy model files to your bucket\n",
    "! gcloud storage cp -R $VERTEX_AI_MODEL_GARDEN_LLAMA2/$VERTEX_MODEL_ID/* $BUCKET_URI/$HF_MODEL_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "832bbd8cb9ef"
   },
   "source": [
    "### Create the Artifact Registry repository and set the custom docker image uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "18fd49b88127"
   },
   "outputs": [],
   "source": [
    "REPOSITORY = \"tpuv5e-training-repository-unique\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "c2242d87effa"
   },
   "outputs": [],
   "source": [
    "image_name_train = \"llama2-7b-hf-lora-tuning-tpuv5e\"\n",
    "hostname = f\"{LOCATION}-docker.pkg.dev\"\n",
    "tag = \"latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "9dd94acb26e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/home/jupyter/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"gcr.io\": \"gcloud\",\n",
      "    \"us.gcr.io\": \"gcloud\",\n",
      "    \"eu.gcr.io\": \"gcloud\",\n",
      "    \"asia.gcr.io\": \"gcloud\",\n",
      "    \"staging-k8s.gcr.io\": \"gcloud\",\n",
      "    \"marketplace.gcr.io\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: us-central1-docker.pkg.dev\n",
      "Docker configuration file updated.\n"
     ]
    }
   ],
   "source": [
    "# Register gcloud as a Docker credential helper\n",
    "!gcloud auth configure-docker $LOCATION-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If your repository doesn't already exist**: Run the following cell to create your Artifact Registry repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5c8e8364a097"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create request issued for: [tpuv5e-training-repository-unique]\n",
      "Waiting for operation [projects/vertexai-service-project/locations/us-central1/\n",
      "operations/b7f908ad-b999-4e54-a79d-13613e968190] to complete...done.           \n",
      "Created repository [tpuv5e-training-repository-unique].\n"
     ]
    }
   ],
   "source": [
    "!gcloud artifacts repositories create $REPOSITORY --repository-format=docker \\\n",
    "--location=$LOCATION --description=\"Vertex TPUv5e training repository\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "16488f1fc702"
   },
   "outputs": [],
   "source": [
    "# Define container image name\n",
    "PYTORCH_TRAIN_DOCKER_URI = (\n",
    "    f\"{hostname}/{PROJECT_ID}/{REPOSITORY}/{image_name_train}:{tag}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c250872074f"
   },
   "source": [
    "### Define common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "354da31189dc"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_job_name_with_datetime(prefix: str) -> str:\n",
    "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
    "    jobs in Vertex AI.\n",
    "    \"\"\"\n",
    "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e11e1f8a6c4c"
   },
   "source": [
    "### Build the Docker container files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19d584f9fe62"
   },
   "source": [
    "#### Create the trainer directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "909e93e7fd43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"trainer\"):\n",
    "    os.makedirs(\"trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8a91ddf0cbcb"
   },
   "source": [
    "#### Create the Dockerfile for the custom container. This will install Hugging Face transformers, datasets, trl, and peft for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "756577886992"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trainer/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/Dockerfile\n",
    "# This Dockerfile fine tunes the Llamas2 model using LoRA with PyTorch XLA\n",
    "# Nightly TPU VM docker image\n",
    "FROM us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla:nightly_3.10_tpuvm_20240324\n",
    "\n",
    "ENV DEBIAN_FRONTEND=noninteractive\n",
    "\n",
    "# Install basic libs\n",
    "RUN apt-get update && apt-get -y upgrade && apt-get install -y --no-install-recommends \\\n",
    "        cmake \\\n",
    "        curl \\\n",
    "        wget \\\n",
    "        sudo \\\n",
    "        gnupg \\\n",
    "        libsm6 \\\n",
    "        libxext6 \\\n",
    "        libxrender-dev \\\n",
    "        lsb-release \\\n",
    "        ca-certificates \\\n",
    "        build-essential \\\n",
    "        git \\\n",
    "        libgl1\n",
    "\n",
    "# Copy Apache license.\n",
    "RUN wget https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/LICENSE\n",
    "\n",
    "# Install required libs\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip install transformers==4.38.2 -U\n",
    "RUN pip install datasets==2.18.0\n",
    "RUN pip install trl==0.8.1 peft==0.10.0\n",
    "RUN pip install accelerate==0.28.0\n",
    "RUN pip install --upgrade google-cloud-storage\n",
    "\n",
    "# Copy other licenses.\n",
    "RUN wget -O MIT_LICENSE https://github.com/pytest-dev/pytest/blob/main/LICENSE\n",
    "RUN wget -O BSD_LICENSE https://github.com/pytorch/xla/blob/master/LICENSE\n",
    "RUN wget -O BSD-3_LICENSE https://github.com/pytorch/pytorch/blob/main/LICENSE\n",
    "\n",
    "# Copy install libtpu to PATH above\n",
    "RUN find ./usr/local/lib -name 'libtpu.so' -exec cp {} /lib \\;\n",
    "\n",
    "WORKDIR /\n",
    "COPY train.py train.py\n",
    "ENV PYTHONPATH ./\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ee7ac9469ce"
   },
   "source": [
    "#### Add the __init__.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "b0b57ecd02d8"
   },
   "outputs": [],
   "source": [
    "!touch trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c648296e1057"
   },
   "source": [
    "#### Add the train.py file\n",
    "This code is from the LoRA distributed fine-tuning code from this example: https://ai.google.dev/gemma/docs/distributed_tuning\n",
    "\n",
    "The IMDB TensorFlow dataset is used to fine-tune the Gemma model. Additional logic is added to handle the TPU topology setting required by TPUv5e: https://cloud.google.com/tpu/docs/v5e#tpu-v5e-config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "1b6a47a6089c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/train.py\n",
    "import os, sys\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "# use spmd\n",
    "import torch_xla.runtime as xr\n",
    "xr.use_spmd()\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--tpu_topology\",\n",
    "    help=\"Topology to use for the TPUv5e (2x2, 2x4, 4x4)\",\n",
    "    default=\"4x4\",\n",
    "    type=str\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--model_name\",\n",
    "    help=\"Llama2 model name (meta-llama/Llama-2-7b-hf, meta-llama/Llama-2-13b-hf)\",\n",
    "    default=\"meta-llama/Llama-2-7b-hf\",\n",
    "    type=str\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--bucket_name\",\n",
    "    help=\"The name of the bucket you copied the Llama2 model files to\",\n",
    "    required=True,\n",
    "    type=str\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--output_folder\",\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"Output folder name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--checkpoint_directory\",\n",
    "    type=str,\n",
    "    default=\"output_ckpt\",\n",
    "    help=\"Checkpoint Directory name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--epochs\",\n",
    "    type=int,\n",
    "    default=10,\n",
    "    help=\"Number of epochs to train\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--merged_model_folder\",\n",
    "    type=str,\n",
    "    default=\"llama2-7b-hf/modelfiles\",\n",
    "    help=\"Checkpoint Directory name\",\n",
    ")\n",
    "args = parser.parse_args()\n",
    "\n",
    "GCS_PREFIX = \"gs://\"\n",
    "\n",
    "def is_gcs_path(input_path: str) -> bool:\n",
    "    return input_path.startswith(GCS_PREFIX)\n",
    "\n",
    "def download_gcs_dir(gcs_dir: str, local_dir: str):\n",
    "    \"\"\"Download files in a GCS directory to a local directory.\n",
    "\n",
    "    For example:\n",
    "    download_gcs_dir(gs://bucket/foo, /tmp/bar)\n",
    "    gs://bucket/foo/a -> /tmp/bar/a\n",
    "    gs://bucket/foo/b/c -> /tmp/bar/b/c\n",
    "\n",
    "    Arguments:\n",
    "    gcs_dir: A string of directory path on GCS.\n",
    "    local_dir: A string of local directory path.\n",
    "    \"\"\"\n",
    "    if not is_gcs_path(gcs_dir):\n",
    "        raise ValueError(f\"{gcs_dir} is not a GCS path starting with gs://.\")\n",
    "\n",
    "    bucket_name = gcs_dir.split(\"/\")[2]\n",
    "    prefix = gcs_dir[len(GCS_PREFIX + bucket_name) :].strip(\"/\")\n",
    "    client = storage.Client()\n",
    "    blobs = client.list_blobs(bucket_name, prefix=prefix)\n",
    "    for blob in blobs:\n",
    "        if blob.name[-1] == \"/\":\n",
    "            continue\n",
    "        file_path = blob.name[len(prefix) :].strip(\"/\")\n",
    "        local_file_path = os.path.join(local_dir, file_path)\n",
    "        os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "        blob.download_to_filename(local_file_path)\n",
    "        print (f'download of {local_file_path} complete')\n",
    "    print (f'Show all files in directory {os.listdir(local_dir)}')\n",
    "\n",
    "def upload_directory_with_transfer_manager(bucket_name, source_directory, blob_name_prefix, workers=8):\n",
    "    \"\"\"Upload every file in a directory, including all files in subdirectories.\n",
    "\n",
    "    Each blob name is derived from the filename, not including the `directory`\n",
    "    parameter itself. For complete control of the blob name for each file (and\n",
    "    other aspects of individual blob metadata), use\n",
    "    transfer_manager.upload_many() instead.\n",
    "    \"\"\"\n",
    "\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "\n",
    "    # The directory on your computer to upload. Files in the directory and its\n",
    "    # subdirectories will be uploaded. An empty string means \"the current\n",
    "    # working directory\".\n",
    "    # source_directory=\"\"\n",
    "\n",
    "    # blob_name_prefix = prefix for the files being uploaded to GCS\n",
    "    # example: file1 and file2 in a folder uploaded to my-bucket with blob_name_prefix=my-folder/a/\n",
    "    # will be uploaded to gs://my-bucket/my-folder/a/file1 and gs://my-bucket/my-folder/a/file2\n",
    "    \n",
    "    # The maximum number of processes to use for the operation. The performance\n",
    "    # impact of this value depends on the use case, but smaller files usually\n",
    "    # benefit from a higher number of processes. Each additional process occupies\n",
    "    # some CPU and memory resources until finished. Threads can be used instead\n",
    "    # of processes by passing `worker_type=transfer_manager.THREAD`.\n",
    "    # workers=8\n",
    "\n",
    "    from pathlib import Path\n",
    "\n",
    "    from google.cloud.storage import Client, transfer_manager\n",
    "\n",
    "    storage_client = Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # Generate a list of paths (in string form) relative to the `directory`.\n",
    "    # This can be done in a single list comprehension, but is expanded into\n",
    "    # multiple lines here for clarity.\n",
    "\n",
    "    # First, recursively get all files in `directory` as Path objects.\n",
    "    directory_as_path_obj = Path(source_directory)\n",
    "    paths = directory_as_path_obj.rglob(\"*\")\n",
    "\n",
    "    # Filter so the list only includes files, not directories themselves.\n",
    "    file_paths = [path for path in paths if path.is_file()]\n",
    "\n",
    "    # These paths are relative to the current working directory. Next, make them\n",
    "    # relative to `directory`\n",
    "    relative_paths = [path.relative_to(source_directory) for path in file_paths]\n",
    "\n",
    "    # Finally, convert them all to strings.\n",
    "    string_paths = [str(path) for path in relative_paths]\n",
    "\n",
    "    print(\"Found {} files.\".format(len(string_paths)))\n",
    "\n",
    "    # Start the upload.\n",
    "    print (f\"source directory {source_directory}\")\n",
    "    results = transfer_manager.upload_many_from_filenames(\n",
    "        bucket, string_paths, blob_name_prefix=blob_name_prefix, source_directory=source_directory, max_workers=workers\n",
    "    )\n",
    "\n",
    "    for name, result in zip(string_paths, results):\n",
    "        # The results list is either `None` or an exception for each filename in\n",
    "        # the input list, in order.\n",
    "\n",
    "        if isinstance(result, Exception):\n",
    "            print(\"Failed to upload {} due to exception: {}\".format(name, result))\n",
    "        else:\n",
    "            print(\"Uploaded {} to {}/{}.\".format(name, bucket.name, blob_name_prefix))\n",
    "    \n",
    "def main():\n",
    "    x = args.tpu_topology.split(\"x\")\n",
    "    tpu_topology_x = int(x[0])\n",
    "    tpu_topology_y = int(x[1])\n",
    "    print (f'TPU topology is ({tpu_topology_x}, {tpu_topology_y})')\n",
    "    print (f'Model name is {args.model_name}')\n",
    "    \n",
    "    # Set batch size to 8 for each chip\n",
    "    BATCH_SIZE = 8 * tpu_topology_x * tpu_topology_y\n",
    "    # For anything larger than an 8 chip instance, set the BATCH_SIZE to 128, since we run out of samples\n",
    "    if (tpu_topology_x * tpu_topology_y) >=16:\n",
    "        BATCH_SIZE = 128\n",
    "    \n",
    "    # Set download directory to a tempory folder\n",
    "    DL_DIR=\"/tmp/modelfiles\"\n",
    "    if not os.path.exists(DL_DIR):\n",
    "        os.makedirs(DL_DIR)\n",
    "\n",
    "    print ('Downloading data to temporary folder')\n",
    "    download_gcs_dir (f\"gs://{args.bucket_name}/{args.model_name}\", DL_DIR)\n",
    "    \n",
    "    # Create output folders\n",
    "    if not os.path.exists(f\"/tmp/{args.output_folder}\"):\n",
    "        os.makedirs(f\"/tmp/{args.output_folder}\")\n",
    "    if not os.path.exists(f\"/tmp/{args.checkpoint_directory}\"):\n",
    "        os.makedirs(f\"/tmp/{args.checkpoint_directory}\")\n",
    "\n",
    "    device = xm.xla_device()\n",
    "    \n",
    "    # Set tokenizer parallelism to false to avoid warnings\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(DL_DIR)\n",
    "    print ('Loaded tokenizer')\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(DL_DIR, torch_dtype=torch.bfloat16)\n",
    "    print ('Loaded base model')\n",
    "\n",
    "    # Set LoRA configuration\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"k_proj\", \"v_proj\"],\n",
    "    )\n",
    "    \n",
    "    # Required when using Llama2, as the tokenizer has no padding\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Load the dataset and format it for training.\n",
    "    data = load_dataset(\"Abirate/english_quotes\", split=\"train\")\n",
    "    max_seq_length = 512\n",
    "    print ('Loaded dataset')\n",
    "\n",
    "    # Set up the FSDP config. To enable FSDP via SPMD, set xla_fsdp_v2 to True.\n",
    "    fsdp_config = {\"fsdp_transformer_layer_cls_to_wrap\": [\n",
    "            \"LlamaDecoderLayer\"\n",
    "        ],\n",
    "        \"xla\": True,\n",
    "        \"xla_fsdp_v2\": True,\n",
    "        \"xla_fsdp_grad_ckpt\": True}\n",
    "\n",
    "    OUTPUT_DIR=f\"/tmp/{args.output_folder}\"\n",
    "    CHECKPOINT_DIR=f\"/tmp/{args.checkpoint_directory}\"\n",
    "\n",
    "    # Finally, set up the trainer and train the model.\n",
    "    trainer = SFTTrainer(\n",
    "        model=base_model,\n",
    "        train_dataset=data,\n",
    "        args=TrainingArguments(\n",
    "            per_device_train_batch_size=BATCH_SIZE,  # This is actually the global batch size for SPMD.\n",
    "            num_train_epochs=args.epochs,\n",
    "            max_steps=-1,\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            optim=\"adafactor\",\n",
    "            logging_steps=1,\n",
    "            dataloader_drop_last = True,  # Required for SPMD.\n",
    "            fsdp=\"full_shard\",\n",
    "            fsdp_config=fsdp_config,\n",
    "        ),\n",
    "        peft_config=lora_config,\n",
    "        dataset_text_field=\"quote\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        packing=True,\n",
    "    )\n",
    "\n",
    "    # train\n",
    "    trainer.train()\n",
    "    \n",
    "    adapter_model_id = \"adapter_model\"\n",
    "    adapter_path = f\"{CHECKPOINT_DIR}/{adapter_model_id}\"\n",
    "    merged_model_id = \"merged_model\"\n",
    "    merged_model_path = f\"{CHECKPOINT_DIR}/{merged_model_id}\"\n",
    "    \n",
    "    trainer.model.to('cpu').save_pretrained(adapter_path)\n",
    "    \n",
    "    # Save the adapter, merged model, and tokenizer\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(DL_DIR, torch_dtype=torch.bfloat16)\n",
    "    peft_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    merged_model = peft_model.merge_and_unload()\n",
    "    merged_model.save_pretrained(merged_model_path,safe_serialization=False)\n",
    "    tokenizer.save_pretrained(merged_model_path)\n",
    "    \n",
    "    # Copy merged files to GCS folder\n",
    "    OUTPUT_PREFIX=f\"{args.merged_model_folder}/{merged_model_id}/{xr.process_index()}/\"\n",
    "    upload_directory_with_transfer_manager(bucket_name=args.bucket_name,source_directory=merged_model_path,\n",
    "                                       blob_name_prefix=OUTPUT_PREFIX)\n",
    "    print ('Uploaded merged model files')\n",
    "\n",
    "    # copy adapter files to GCS folder\n",
    "    OUTPUT_PREFIX=f\"{args.merged_model_folder}/{adapter_model_id}/{xr.process_index()}/\"\n",
    "    upload_directory_with_transfer_manager(bucket_name=args.bucket_name,source_directory=adapter_path,\n",
    "                                       blob_name_prefix=OUTPUT_PREFIX)\n",
    "    print ('Uploaded adapter model files')\n",
    "\n",
    "    print ('Exiting job')\n",
    "    sys.exit(0)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pq4iF00YG_4T"
   },
   "source": [
    "## Fine-tune with Vertex AI Custom Training Jobs\n",
    "\n",
    "This section demonstrates how to fine-tune and deploy Llama2 models with PEFT LoRA on Vertex AI Custom Training Jobs. LoRA (Low-Rank Adaptation) is one approach of PEFT (Parameter Efficient Fine-tuning), where pretrained model weights are frozen and rank decomposition matrices representing the change in model weights are trained during fine-tuning. Read more about LoRA in the following publication: [Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L. and Chen, W., 2021. Lora: Low-rank adaptation of large language models. *arXiv preprint arXiv:2106.09685*](https://arxiv.org/abs/2106.09685)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1b90914fc81"
   },
   "source": [
    "#### Enable docker to run as a regular user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "232a259d3edc"
   },
   "outputs": [],
   "source": [
    "!sudo usermod -a -G docker ${USER}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e028d107fbe5"
   },
   "source": [
    "#### Change to the trainer directory to build the docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "3a9390f87b66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/vertex-ai-samples/notebooks/official/training/trainer\n"
     ]
    }
   ],
   "source": [
    "%cd trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81eb3c13afa9"
   },
   "source": [
    "#### Build the custom docker container and push to artifact registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ee497559677f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  14.34kB\n",
      "Step 1/19 : FROM us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla:nightly_3.10_tpuvm_20240324\n",
      "nightly_3.10_tpuvm_20240324: Pulling from tpu-pytorch-releases/docker/xla\n",
      "\n",
      "\u001b[1B5f17d0c7: Pulling fs layer \n",
      "\u001b[1B675e1918: Pulling fs layer \n",
      "\u001b[1Bb1746a83: Pulling fs layer \n",
      "\u001b[1B39aa9b63: Pulling fs layer \n",
      "\u001b[1B5db05c76: Pulling fs layer \n",
      "\u001b[1Bd409a431: Pulling fs layer \n",
      "\u001b[1Be92977b7: Pulling fs layer \n",
      "\u001b[1Bdbc1dd53: Pulling fs layer \n",
      "\u001b[1B8b771932: Pulling fs layer \n",
      "\u001b[1B4f7e71c8: Pulling fs layer \n",
      "\u001b[1Bef234518: Pulling fs layer \n",
      "\u001b[1B8a3d6129: Pulling fs layer \n",
      "\u001b[1B772c88a6: Pulling fs layer \n",
      "\u001b[1B3386ec0d: Pulling fs layer \n",
      "\u001b[1B76f7c809: Pulling fs layer \n",
      "\u001b[1Ba259ce43: Pulling fs layer \n",
      "\u001b[1Bd6b58ba1: Pulling fs layer \n",
      "\u001b[1Ba0bd010a: Pull complete 3.3MB/653.3MBB\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[16A\u001b[2K\u001b[18A\u001b[2K\u001b[17A\u001b[2K\u001b[16A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[16A\u001b[2K\u001b[15A\u001b[2K\u001b[18A\u001b[2K\u001b[15A\u001b[2K\u001b[14A\u001b[2K\u001b[15A\u001b[2K\u001b[14A\u001b[2K\u001b[18A\u001b[2K\u001b[16A\u001b[2K\u001b[18A\u001b[2K\u001b[13A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[18A\u001b[2K\u001b[13A\u001b[2K\u001b[12A\u001b[2K\u001b[18A\u001b[2K\u001b[15A\u001b[2K\u001b[11A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[18A\u001b[2K\u001b[15A\u001b[2K\u001b[18A\u001b[2K\u001b[15A\u001b[2K\u001b[18A\u001b[2K\u001b[18A\u001b[2K\u001b[9A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[15A\u001b[2K\u001b[7A\u001b[2K\u001b[15A\u001b[2K\u001b[7A\u001b[2K\u001b[15A\u001b[2K\u001b[7A\u001b[2K\u001b[15A\u001b[2K\u001b[7A\u001b[2K\u001b[15A\u001b[2K\u001b[7A\u001b[2K\u001b[15A\u001b[2K\u001b[7A\u001b[2K\u001b[15A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[17A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[15A\u001b[2K\u001b[7A\u001b[2K\u001b[15A\u001b[2K\u001b[7A\u001b[2K\u001b[17A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[7A\u001b[2K\u001b[4A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2KDownloading  38.77MB/196.6MB\u001b[16A\u001b[2K\u001b[4A\u001b[2K\u001b[16A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[16A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[16A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[16A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[7A\u001b[2K\u001b[3A\u001b[2K\u001b[7A\u001b[2K\u001b[3A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[3A\u001b[2K\u001b[7A\u001b[2K\u001b[3A\u001b[2K\u001b[7A\u001b[2K\u001b[3A\u001b[2K\u001b[7A\u001b[2K\u001b[3A\u001b[2K\u001b[1A\u001b[2K\u001b[16A\u001b[2K\u001b[3A\u001b[2K\u001b[1A\u001b[2K\u001b[3A\u001b[2K\u001b[1A\u001b[2K\u001b[3A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[16A\u001b[2K\u001b[1A\u001b[2K\u001b[7A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[7A\u001b[2K\u001b[1A\u001b[2K\u001b[7A\u001b[2K\u001b[1A\u001b[2K\u001b[7A\u001b[2K\u001b[1A\u001b[2K\u001b[3A\u001b[2K\u001b[7A\u001b[2K\u001b[1A\u001b[2K\u001b[7A\u001b[2K\u001b[1A\u001b[2K\u001b[3A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[16A\u001b[2K\u001b[7A\u001b[2K\u001b[3A\u001b[2K\u001b[1A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[15A\u001b[2K\u001b[3A\u001b[2K\u001b[15A\u001b[2K\u001b[3A\u001b[2K\u001b[15A\u001b[2K\u001b[1A\u001b[2K\u001b[3A\u001b[2K\u001b[1A\u001b[2K\u001b[15A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[15A\u001b[2K\u001b[3A\u001b[2K\u001b[15A\u001b[2K\u001b[1A\u001b[2K\u001b[3A\u001b[2K\u001b[15A\u001b[2K\u001b[1A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[1A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[1A\u001b[2K\u001b[3A\u001b[2K\u001b[1A\u001b[2K\u001b[3A\u001b[2K\u001b[15A\u001b[2K\u001b[1A\u001b[2K\u001b[15A\u001b[2K\u001b[3A\u001b[2K\u001b[1A\u001b[2K\u001b[3A\u001b[2K\u001b[1A\u001b[2K\u001b[3A\u001b[2K\u001b[15A\u001b[2K\u001b[3A\u001b[2K\u001b[15A\u001b[2K\u001b[3A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[2K\u001b[15A\u001b[2K\u001b[1A\u001b[2K\u001b[15A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[1A\u001b[2K\u001b[15A\u001b[2K\u001b[1A\u001b[2K\u001b[15A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[15A\u001b[2K\u001b[1A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[3A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[15A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[15A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[15A\u001b[2K\u001b[1A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[12A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KDigest: sha256:24c573a100892a45f2987983724ba6663877a4f6c70e7f8b8c677962a6c825ce\n",
      "Status: Downloaded newer image for us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla:nightly_3.10_tpuvm_20240324\n",
      " ---> de2fc27f56f5\n",
      "Step 2/19 : ENV DEBIAN_FRONTEND=noninteractive\n",
      " ---> Running in 1ed4011969f4\n",
      "Removing intermediate container 1ed4011969f4\n",
      " ---> 12fc7a95bd1b\n",
      "Step 3/19 : RUN apt-get update && apt-get -y upgrade && apt-get install -y --no-install-recommends         cmake         curl         wget         sudo         gnupg         libsm6         libxext6         libxrender-dev         lsb-release         ca-certificates         build-essential         git         libgl1\n",
      " ---> Running in c623eb87f94f\n",
      "Hit:1 http://deb.debian.org/debian bullseye InRelease\n",
      "Get:2 http://deb.debian.org/debian-security bullseye-security InRelease [48.4 kB]\n",
      "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/debian11/x86_64  InRelease [1581 B]\n",
      "Get:4 http://deb.debian.org/debian bullseye-updates InRelease [44.1 kB]\n",
      "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/debian11/x86_64  Packages [1217 kB]\n",
      "Hit:5 https://apt.llvm.org/bullseye llvm-toolchain-bullseye-17 InRelease\n",
      "Get:7 http://deb.debian.org/debian-security bullseye-security/main amd64 Packages [275 kB]\n",
      "Fetched 1586 kB in 1s (1152 kB/s)\n",
      "Reading package lists...\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "Calculating upgrade...\n",
      "The following packages will be upgraded:\n",
      "  bsdutils libblkid-dev libblkid1 libc-bin libc-dev-bin libc6 libc6-dev\n",
      "  libdav1d4 libglib2.0-0 libglib2.0-bin libglib2.0-data libglib2.0-dev\n",
      "  libglib2.0-dev-bin libmount-dev libmount1 libsmartcols1 libuuid1\n",
      "  linux-libc-dev mount util-linux uuid-dev\n",
      "21 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n",
      "Need to get 15.3 MB of archives.\n",
      "After this operation, 57.3 kB of additional disk space will be used.\n",
      "Get:1 http://deb.debian.org/debian-security bullseye-security/main amd64 bsdutils amd64 1:2.36.1-8+deb11u2 [149 kB]\n",
      "Get:2 http://deb.debian.org/debian-security bullseye-security/main amd64 util-linux amd64 2.36.1-8+deb11u2 [1142 kB]\n",
      "Get:3 http://deb.debian.org/debian-security bullseye-security/main amd64 libc6-dev amd64 2.31-13+deb11u10 [2362 kB]\n",
      "Get:4 http://deb.debian.org/debian-security bullseye-security/main amd64 libc-dev-bin amd64 2.31-13+deb11u10 [276 kB]\n",
      "Get:5 http://deb.debian.org/debian-security bullseye-security/main amd64 linux-libc-dev amd64 5.10.218-1 [1728 kB]\n",
      "Get:6 http://deb.debian.org/debian-security bullseye-security/main amd64 libc6 amd64 2.31-13+deb11u10 [2825 kB]\n",
      "Get:7 http://deb.debian.org/debian-security bullseye-security/main amd64 libc-bin amd64 2.31-13+deb11u10 [827 kB]\n",
      "Get:8 http://deb.debian.org/debian-security bullseye-security/main amd64 mount amd64 2.36.1-8+deb11u2 [186 kB]\n",
      "Get:9 http://deb.debian.org/debian-security bullseye-security/main amd64 uuid-dev amd64 2.36.1-8+deb11u2 [99.4 kB]\n",
      "Get:10 http://deb.debian.org/debian-security bullseye-security/main amd64 libuuid1 amd64 2.36.1-8+deb11u2 [83.9 kB]\n",
      "Get:11 http://deb.debian.org/debian-security bullseye-security/main amd64 libblkid-dev amd64 2.36.1-8+deb11u2 [225 kB]\n",
      "Get:12 http://deb.debian.org/debian-security bullseye-security/main amd64 libblkid1 amd64 2.36.1-8+deb11u2 [194 kB]\n",
      "Get:13 http://deb.debian.org/debian-security bullseye-security/main amd64 libmount-dev amd64 2.36.1-8+deb11u2 [78.1 kB]\n",
      "Get:14 http://deb.debian.org/debian-security bullseye-security/main amd64 libmount1 amd64 2.36.1-8+deb11u2 [212 kB]\n",
      "Get:15 http://deb.debian.org/debian-security bullseye-security/main amd64 libsmartcols1 amd64 2.36.1-8+deb11u2 [158 kB]\n",
      "Get:16 http://deb.debian.org/debian-security bullseye-security/main amd64 libdav1d4 amd64 0.7.1-3+deb11u1 [331 kB]\n",
      "Get:17 http://deb.debian.org/debian-security bullseye-security/main amd64 libglib2.0-dev-bin amd64 2.66.8-1+deb11u3 [181 kB]\n",
      "Get:18 http://deb.debian.org/debian-security bullseye-security/main amd64 libglib2.0-dev amd64 2.66.8-1+deb11u3 [1584 kB]\n",
      "Get:19 http://deb.debian.org/debian-security bullseye-security/main amd64 libglib2.0-data all 2.66.8-1+deb11u3 [1177 kB]\n",
      "Get:20 http://deb.debian.org/debian-security bullseye-security/main amd64 libglib2.0-bin amd64 2.66.8-1+deb11u3 [142 kB]\n",
      "Get:21 http://deb.debian.org/debian-security bullseye-security/main amd64 libglib2.0-0 amd64 2.66.8-1+deb11u3 [1376 kB]\n",
      "\u001b[91mdebconf: delaying package configuration, since apt-utils is not installed\n",
      "\u001b[0mFetched 15.3 MB in 1s (25.1 MB/s)\n",
      "(Reading database ... 24793 files and directories currently installed.)\n",
      "Preparing to unpack .../bsdutils_1%3a2.36.1-8+deb11u2_amd64.deb ...\n",
      "Unpacking bsdutils (1:2.36.1-8+deb11u2) over (1:2.36.1-8+deb11u1) ...\n",
      "Setting up bsdutils (1:2.36.1-8+deb11u2) ...\n",
      "(Reading database ... 24793 files and directories currently installed.)\n",
      "Preparing to unpack .../util-linux_2.36.1-8+deb11u2_amd64.deb ...\n",
      "Unpacking util-linux (2.36.1-8+deb11u2) over (2.36.1-8+deb11u1) ...\n",
      "Setting up util-linux (2.36.1-8+deb11u2) ...\n",
      "(Reading database ... 24793 files and directories currently installed.)\n",
      "Preparing to unpack .../libc6-dev_2.31-13+deb11u10_amd64.deb ...\n",
      "Unpacking libc6-dev:amd64 (2.31-13+deb11u10) over (2.31-13+deb11u8) ...\n",
      "Preparing to unpack .../libc-dev-bin_2.31-13+deb11u10_amd64.deb ...\n",
      "Unpacking libc-dev-bin (2.31-13+deb11u10) over (2.31-13+deb11u8) ...\n",
      "Preparing to unpack .../linux-libc-dev_5.10.218-1_amd64.deb ...\n",
      "Unpacking linux-libc-dev:amd64 (5.10.218-1) over (5.10.209-2) ...\n",
      "Preparing to unpack .../libc6_2.31-13+deb11u10_amd64.deb ...\n",
      "Unpacking libc6:amd64 (2.31-13+deb11u10) over (2.31-13+deb11u8) ...\n",
      "Setting up libc6:amd64 (2.31-13+deb11u10) ...\n",
      "(Reading database ... 24793 files and directories currently installed.)\n",
      "Preparing to unpack .../libc-bin_2.31-13+deb11u10_amd64.deb ...\n",
      "Unpacking libc-bin (2.31-13+deb11u10) over (2.31-13+deb11u8) ...\n",
      "Setting up libc-bin (2.31-13+deb11u10) ...\n",
      "ldconfig: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "ldconfig: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "ldconfig: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "ldconfig: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "ldconfig: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "ldconfig: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "(Reading database ... 24793 files and directories currently installed.)\n",
      "Preparing to unpack .../mount_2.36.1-8+deb11u2_amd64.deb ...\n",
      "Unpacking mount (2.36.1-8+deb11u2) over (2.36.1-8+deb11u1) ...\n",
      "Preparing to unpack .../uuid-dev_2.36.1-8+deb11u2_amd64.deb ...\n",
      "Unpacking uuid-dev:amd64 (2.36.1-8+deb11u2) over (2.36.1-8+deb11u1) ...\n",
      "Preparing to unpack .../libuuid1_2.36.1-8+deb11u2_amd64.deb ...\n",
      "Unpacking libuuid1:amd64 (2.36.1-8+deb11u2) over (2.36.1-8+deb11u1) ...\n",
      "Setting up libuuid1:amd64 (2.36.1-8+deb11u2) ...\n",
      "(Reading database ... 24793 files and directories currently installed.)\n",
      "Preparing to unpack .../libblkid-dev_2.36.1-8+deb11u2_amd64.deb ...\n",
      "Unpacking libblkid-dev:amd64 (2.36.1-8+deb11u2) over (2.36.1-8+deb11u1) ...\n",
      "Preparing to unpack .../libblkid1_2.36.1-8+deb11u2_amd64.deb ...\n",
      "Unpacking libblkid1:amd64 (2.36.1-8+deb11u2) over (2.36.1-8+deb11u1) ...\n",
      "Setting up libblkid1:amd64 (2.36.1-8+deb11u2) ...\n",
      "(Reading database ... 24793 files and directories currently installed.)\n",
      "Preparing to unpack .../libmount-dev_2.36.1-8+deb11u2_amd64.deb ...\n",
      "Unpacking libmount-dev:amd64 (2.36.1-8+deb11u2) over (2.36.1-8+deb11u1) ...\n",
      "Preparing to unpack .../libmount1_2.36.1-8+deb11u2_amd64.deb ...\n",
      "Unpacking libmount1:amd64 (2.36.1-8+deb11u2) over (2.36.1-8+deb11u1) ...\n",
      "Setting up libmount1:amd64 (2.36.1-8+deb11u2) ...\n",
      "(Reading database ... 24793 files and directories currently installed.)\n",
      "Preparing to unpack .../libsmartcols1_2.36.1-8+deb11u2_amd64.deb ...\n",
      "Unpacking libsmartcols1:amd64 (2.36.1-8+deb11u2) over (2.36.1-8+deb11u1) ...\n",
      "Setting up libsmartcols1:amd64 (2.36.1-8+deb11u2) ...\n",
      "(Reading database ... 24793 files and directories currently installed.)\n",
      "Preparing to unpack .../0-libdav1d4_0.7.1-3+deb11u1_amd64.deb ...\n",
      "Unpacking libdav1d4:amd64 (0.7.1-3+deb11u1) over (0.7.1-3) ...\n",
      "Preparing to unpack .../1-libglib2.0-dev-bin_2.66.8-1+deb11u3_amd64.deb ...\n",
      "Unpacking libglib2.0-dev-bin (2.66.8-1+deb11u3) over (2.66.8-1+deb11u1) ...\n",
      "Preparing to unpack .../2-libglib2.0-dev_2.66.8-1+deb11u3_amd64.deb ...\n",
      "Unpacking libglib2.0-dev:amd64 (2.66.8-1+deb11u3) over (2.66.8-1+deb11u1) ...\n",
      "Preparing to unpack .../3-libglib2.0-data_2.66.8-1+deb11u3_all.deb ...\n",
      "Unpacking libglib2.0-data (2.66.8-1+deb11u3) over (2.66.8-1+deb11u1) ...\n",
      "Preparing to unpack .../4-libglib2.0-bin_2.66.8-1+deb11u3_amd64.deb ...\n",
      "Unpacking libglib2.0-bin (2.66.8-1+deb11u3) over (2.66.8-1+deb11u1) ...\n",
      "Preparing to unpack .../5-libglib2.0-0_2.66.8-1+deb11u3_amd64.deb ...\n",
      "Unpacking libglib2.0-0:amd64 (2.66.8-1+deb11u3) over (2.66.8-1+deb11u1) ...\n",
      "Setting up libglib2.0-0:amd64 (2.66.8-1+deb11u3) ...\n",
      "No schema files found: doing nothing.\n",
      "Setting up linux-libc-dev:amd64 (5.10.218-1) ...\n",
      "Setting up libglib2.0-data (2.66.8-1+deb11u3) ...\n",
      "Setting up mount (2.36.1-8+deb11u2) ...\n",
      "Setting up libdav1d4:amd64 (0.7.1-3+deb11u1) ...\n",
      "Setting up libc-dev-bin (2.31-13+deb11u10) ...\n",
      "Setting up libglib2.0-dev-bin (2.66.8-1+deb11u3) ...\n",
      "Setting up libglib2.0-bin (2.66.8-1+deb11u3) ...\n",
      "Setting up libc6-dev:amd64 (2.31-13+deb11u10) ...\n",
      "Setting up uuid-dev:amd64 (2.36.1-8+deb11u2) ...\n",
      "Setting up libblkid-dev:amd64 (2.36.1-8+deb11u2) ...\n",
      "Setting up libmount-dev:amd64 (2.36.1-8+deb11u2) ...\n",
      "Setting up libglib2.0-dev:amd64 (2.66.8-1+deb11u3) ...\n",
      "Processing triggers for libc-bin (2.31-13+deb11u10) ...\n",
      "ldconfig: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "ldconfig: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "ldconfig: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "ldconfig: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "ldconfig: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "ldconfig: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "ca-certificates is already the newest version (20210119).\n",
      "curl is already the newest version (7.74.0-1.3+deb11u11).\n",
      "git is already the newest version (1:2.30.2-1+deb11u2).\n",
      "gnupg is already the newest version (2.2.27-2+deb11u2).\n",
      "libsm6 is already the newest version (2:1.2.3-1).\n",
      "libsm6 set to manually installed.\n",
      "libxext6 is already the newest version (2:1.3.3-1.1).\n",
      "libxext6 set to manually installed.\n",
      "libxrender-dev is already the newest version (1:0.9.10-1).\n",
      "libxrender-dev set to manually installed.\n",
      "lsb-release is already the newest version (11.1.0).\n",
      "lsb-release set to manually installed.\n",
      "wget is already the newest version (1.21-1+deb11u1).\n",
      "The following additional packages will be installed:\n",
      "  cmake-data libarchive13 libdrm-amdgpu1 libdrm-common libdrm-intel1\n",
      "  libdrm-nouveau2 libdrm-radeon1 libdrm2 libgl1-mesa-dri libglapi-mesa\n",
      "  libglvnd0 libglx-mesa0 libglx0 libjsoncpp24 libllvm11 libpciaccess0\n",
      "  librhash0 libsensors-config libsensors5 libuv1 libvulkan1 libx11-xcb1\n",
      "  libxcb-dri2-0 libxcb-dri3-0 libxcb-glx0 libxcb-present0 libxcb-sync1\n",
      "  libxcb-xfixes0 libxdamage1 libxfixes3 libxshmfence1 libxxf86vm1 libz3-4\n",
      "Suggested packages:\n",
      "  cmake-doc ninja-build lrzip pciutils lm-sensors\n",
      "Recommended packages:\n",
      "  mesa-vulkan-drivers | vulkan-icd\n",
      "The following NEW packages will be installed:\n",
      "  build-essential cmake cmake-data libarchive13 libdrm-amdgpu1 libdrm-common\n",
      "  libdrm-intel1 libdrm-nouveau2 libdrm-radeon1 libdrm2 libgl1 libgl1-mesa-dri\n",
      "  libglapi-mesa libglvnd0 libglx-mesa0 libglx0 libjsoncpp24 libllvm11\n",
      "  libpciaccess0 librhash0 libsensors-config libsensors5 libuv1 libvulkan1\n",
      "  libx11-xcb1 libxcb-dri2-0 libxcb-dri3-0 libxcb-glx0 libxcb-present0\n",
      "  libxcb-sync1 libxcb-xfixes0 libxdamage1 libxfixes3 libxshmfence1 libxxf86vm1\n",
      "  libz3-4 sudo\n",
      "0 upgraded, 37 newly installed, 0 to remove and 0 not upgraded.\n",
      "Need to get 45.3 MB of archives.\n",
      "After this operation, 187 MB of additional disk space will be used.\n",
      "Get:1 http://deb.debian.org/debian bullseye/main amd64 build-essential amd64 12.9 [7704 B]\n",
      "Get:2 http://deb.debian.org/debian bullseye/main amd64 cmake-data all 3.18.4-2+deb11u1 [1725 kB]\n",
      "Get:3 http://deb.debian.org/debian bullseye/main amd64 libarchive13 amd64 3.4.3-2+deb11u1 [343 kB]\n",
      "Get:4 http://deb.debian.org/debian bullseye/main amd64 libjsoncpp24 amd64 1.9.4-4 [78.9 kB]\n",
      "Get:5 http://deb.debian.org/debian bullseye/main amd64 librhash0 amd64 1.4.1-2 [129 kB]\n",
      "Get:6 http://deb.debian.org/debian-security bullseye-security/main amd64 libuv1 amd64 1.40.0-2+deb11u1 [132 kB]\n",
      "Get:7 http://deb.debian.org/debian bullseye/main amd64 cmake amd64 3.18.4-2+deb11u1 [5593 kB]\n",
      "Get:8 http://deb.debian.org/debian bullseye/main amd64 libdrm-common all 2.4.104-1 [14.9 kB]\n",
      "Get:9 http://deb.debian.org/debian bullseye/main amd64 libdrm2 amd64 2.4.104-1 [41.5 kB]\n",
      "Get:10 http://deb.debian.org/debian bullseye/main amd64 libdrm-amdgpu1 amd64 2.4.104-1 [28.5 kB]\n",
      "Get:11 http://deb.debian.org/debian bullseye/main amd64 libpciaccess0 amd64 0.16-1 [53.6 kB]\n",
      "Get:12 http://deb.debian.org/debian bullseye/main amd64 libdrm-intel1 amd64 2.4.104-1 [71.8 kB]\n",
      "Get:13 http://deb.debian.org/debian bullseye/main amd64 libdrm-nouveau2 amd64 2.4.104-1 [26.8 kB]\n",
      "Get:14 http://deb.debian.org/debian bullseye/main amd64 libdrm-radeon1 amd64 2.4.104-1 [30.2 kB]\n",
      "Get:15 http://deb.debian.org/debian bullseye/main amd64 libglapi-mesa amd64 20.3.5-1 [71.7 kB]\n",
      "Get:16 http://deb.debian.org/debian bullseye/main amd64 libz3-4 amd64 4.8.10-1 [6949 kB]\n",
      "Get:17 http://deb.debian.org/debian bullseye/main amd64 libllvm11 amd64 1:11.0.1-2 [17.9 MB]\n",
      "Get:18 http://deb.debian.org/debian bullseye/main amd64 libsensors-config all 1:3.6.0-7 [32.3 kB]\n",
      "Get:19 http://deb.debian.org/debian bullseye/main amd64 libsensors5 amd64 1:3.6.0-7 [52.3 kB]\n",
      "Get:20 http://deb.debian.org/debian bullseye/main amd64 libvulkan1 amd64 1.2.162.0-1 [103 kB]\n",
      "Get:21 http://deb.debian.org/debian bullseye/main amd64 libgl1-mesa-dri amd64 20.3.5-1 [9633 kB]\n",
      "Get:22 http://deb.debian.org/debian bullseye/main amd64 libglvnd0 amd64 1.3.2-1 [53.6 kB]\n",
      "Get:23 http://deb.debian.org/debian bullseye/main amd64 libx11-xcb1 amd64 2:1.7.2-1+deb11u2 [204 kB]\n",
      "Get:24 http://deb.debian.org/debian bullseye/main amd64 libxcb-dri2-0 amd64 1.14-3 [103 kB]\n",
      "Get:25 http://deb.debian.org/debian bullseye/main amd64 libxcb-dri3-0 amd64 1.14-3 [102 kB]\n",
      "Get:26 http://deb.debian.org/debian bullseye/main amd64 libxcb-glx0 amd64 1.14-3 [118 kB]\n",
      "Get:27 http://deb.debian.org/debian bullseye/main amd64 libxcb-present0 amd64 1.14-3 [101 kB]\n",
      "Get:28 http://deb.debian.org/debian bullseye/main amd64 libxcb-sync1 amd64 1.14-3 [105 kB]\n",
      "Get:29 http://deb.debian.org/debian bullseye/main amd64 libxcb-xfixes0 amd64 1.14-3 [105 kB]\n",
      "Get:30 http://deb.debian.org/debian bullseye/main amd64 libxdamage1 amd64 1:1.1.5-2 [15.7 kB]\n",
      "Get:31 http://deb.debian.org/debian bullseye/main amd64 libxfixes3 amd64 1:5.0.3-2 [22.1 kB]\n",
      "Get:32 http://deb.debian.org/debian bullseye/main amd64 libxshmfence1 amd64 1.3-1 [8820 B]\n",
      "Get:33 http://deb.debian.org/debian bullseye/main amd64 libxxf86vm1 amd64 1:1.1.4-1+b2 [20.8 kB]\n",
      "Get:34 http://deb.debian.org/debian bullseye/main amd64 libglx-mesa0 amd64 20.3.5-1 [186 kB]\n",
      "Get:35 http://deb.debian.org/debian bullseye/main amd64 sudo amd64 1.9.5p2-3+deb11u1 [1061 kB]\n",
      "Get:36 http://deb.debian.org/debian bullseye/main amd64 libglx0 amd64 1.3.2-1 [35.7 kB]\n",
      "Get:37 http://deb.debian.org/debian bullseye/main amd64 libgl1 amd64 1.3.2-1 [89.5 kB]\n",
      "\u001b[91mdebconf: delaying package configuration, since apt-utils is not installed\n",
      "\u001b[0mFetched 45.3 MB in 1s (46.2 MB/s)\n",
      "Selecting previously unselected package build-essential.\n",
      "(Reading database ... 24793 files and directories currently installed.)\n",
      "Preparing to unpack .../00-build-essential_12.9_amd64.deb ...\n",
      "Unpacking build-essential (12.9) ...\n",
      "Selecting previously unselected package cmake-data.\n",
      "Preparing to unpack .../01-cmake-data_3.18.4-2+deb11u1_all.deb ...\n",
      "Unpacking cmake-data (3.18.4-2+deb11u1) ...\n",
      "Selecting previously unselected package libarchive13:amd64.\n",
      "Preparing to unpack .../02-libarchive13_3.4.3-2+deb11u1_amd64.deb ...\n",
      "Unpacking libarchive13:amd64 (3.4.3-2+deb11u1) ...\n",
      "Selecting previously unselected package libjsoncpp24:amd64.\n",
      "Preparing to unpack .../03-libjsoncpp24_1.9.4-4_amd64.deb ...\n",
      "Unpacking libjsoncpp24:amd64 (1.9.4-4) ...\n",
      "Selecting previously unselected package librhash0:amd64.\n",
      "Preparing to unpack .../04-librhash0_1.4.1-2_amd64.deb ...\n",
      "Unpacking librhash0:amd64 (1.4.1-2) ...\n",
      "Selecting previously unselected package libuv1:amd64.\n",
      "Preparing to unpack .../05-libuv1_1.40.0-2+deb11u1_amd64.deb ...\n",
      "Unpacking libuv1:amd64 (1.40.0-2+deb11u1) ...\n",
      "Selecting previously unselected package cmake.\n",
      "Preparing to unpack .../06-cmake_3.18.4-2+deb11u1_amd64.deb ...\n",
      "Unpacking cmake (3.18.4-2+deb11u1) ...\n",
      "Selecting previously unselected package libdrm-common.\n",
      "Preparing to unpack .../07-libdrm-common_2.4.104-1_all.deb ...\n",
      "Unpacking libdrm-common (2.4.104-1) ...\n",
      "Selecting previously unselected package libdrm2:amd64.\n",
      "Preparing to unpack .../08-libdrm2_2.4.104-1_amd64.deb ...\n",
      "Unpacking libdrm2:amd64 (2.4.104-1) ...\n",
      "Selecting previously unselected package libdrm-amdgpu1:amd64.\n",
      "Preparing to unpack .../09-libdrm-amdgpu1_2.4.104-1_amd64.deb ...\n",
      "Unpacking libdrm-amdgpu1:amd64 (2.4.104-1) ...\n",
      "Selecting previously unselected package libpciaccess0:amd64.\n",
      "Preparing to unpack .../10-libpciaccess0_0.16-1_amd64.deb ...\n",
      "Unpacking libpciaccess0:amd64 (0.16-1) ...\n",
      "Selecting previously unselected package libdrm-intel1:amd64.\n",
      "Preparing to unpack .../11-libdrm-intel1_2.4.104-1_amd64.deb ...\n",
      "Unpacking libdrm-intel1:amd64 (2.4.104-1) ...\n",
      "Selecting previously unselected package libdrm-nouveau2:amd64.\n",
      "Preparing to unpack .../12-libdrm-nouveau2_2.4.104-1_amd64.deb ...\n",
      "Unpacking libdrm-nouveau2:amd64 (2.4.104-1) ...\n",
      "Selecting previously unselected package libdrm-radeon1:amd64.\n",
      "Preparing to unpack .../13-libdrm-radeon1_2.4.104-1_amd64.deb ...\n",
      "Unpacking libdrm-radeon1:amd64 (2.4.104-1) ...\n",
      "Selecting previously unselected package libglapi-mesa:amd64.\n",
      "Preparing to unpack .../14-libglapi-mesa_20.3.5-1_amd64.deb ...\n",
      "Unpacking libglapi-mesa:amd64 (20.3.5-1) ...\n",
      "Selecting previously unselected package libz3-4:amd64.\n",
      "Preparing to unpack .../15-libz3-4_4.8.10-1_amd64.deb ...\n",
      "Unpacking libz3-4:amd64 (4.8.10-1) ...\n",
      "Selecting previously unselected package libllvm11:amd64.\n",
      "Preparing to unpack .../16-libllvm11_1%3a11.0.1-2_amd64.deb ...\n",
      "Unpacking libllvm11:amd64 (1:11.0.1-2) ...\n",
      "Selecting previously unselected package libsensors-config.\n",
      "Preparing to unpack .../17-libsensors-config_1%3a3.6.0-7_all.deb ...\n",
      "Unpacking libsensors-config (1:3.6.0-7) ...\n",
      "Selecting previously unselected package libsensors5:amd64.\n",
      "Preparing to unpack .../18-libsensors5_1%3a3.6.0-7_amd64.deb ...\n",
      "Unpacking libsensors5:amd64 (1:3.6.0-7) ...\n",
      "Selecting previously unselected package libvulkan1:amd64.\n",
      "Preparing to unpack .../19-libvulkan1_1.2.162.0-1_amd64.deb ...\n",
      "Unpacking libvulkan1:amd64 (1.2.162.0-1) ...\n",
      "Selecting previously unselected package libgl1-mesa-dri:amd64.\n",
      "Preparing to unpack .../20-libgl1-mesa-dri_20.3.5-1_amd64.deb ...\n",
      "Unpacking libgl1-mesa-dri:amd64 (20.3.5-1) ...\n",
      "Selecting previously unselected package libglvnd0:amd64.\n",
      "Preparing to unpack .../21-libglvnd0_1.3.2-1_amd64.deb ...\n",
      "Unpacking libglvnd0:amd64 (1.3.2-1) ...\n",
      "Selecting previously unselected package libx11-xcb1:amd64.\n",
      "Preparing to unpack .../22-libx11-xcb1_2%3a1.7.2-1+deb11u2_amd64.deb ...\n",
      "Unpacking libx11-xcb1:amd64 (2:1.7.2-1+deb11u2) ...\n",
      "Selecting previously unselected package libxcb-dri2-0:amd64.\n",
      "Preparing to unpack .../23-libxcb-dri2-0_1.14-3_amd64.deb ...\n",
      "Unpacking libxcb-dri2-0:amd64 (1.14-3) ...\n",
      "Selecting previously unselected package libxcb-dri3-0:amd64.\n",
      "Preparing to unpack .../24-libxcb-dri3-0_1.14-3_amd64.deb ...\n",
      "Unpacking libxcb-dri3-0:amd64 (1.14-3) ...\n",
      "Selecting previously unselected package libxcb-glx0:amd64.\n",
      "Preparing to unpack .../25-libxcb-glx0_1.14-3_amd64.deb ...\n",
      "Unpacking libxcb-glx0:amd64 (1.14-3) ...\n",
      "Selecting previously unselected package libxcb-present0:amd64.\n",
      "Preparing to unpack .../26-libxcb-present0_1.14-3_amd64.deb ...\n",
      "Unpacking libxcb-present0:amd64 (1.14-3) ...\n",
      "Selecting previously unselected package libxcb-sync1:amd64.\n",
      "Preparing to unpack .../27-libxcb-sync1_1.14-3_amd64.deb ...\n",
      "Unpacking libxcb-sync1:amd64 (1.14-3) ...\n",
      "Selecting previously unselected package libxcb-xfixes0:amd64.\n",
      "Preparing to unpack .../28-libxcb-xfixes0_1.14-3_amd64.deb ...\n",
      "Unpacking libxcb-xfixes0:amd64 (1.14-3) ...\n",
      "Selecting previously unselected package libxdamage1:amd64.\n",
      "Preparing to unpack .../29-libxdamage1_1%3a1.1.5-2_amd64.deb ...\n",
      "Unpacking libxdamage1:amd64 (1:1.1.5-2) ...\n",
      "Selecting previously unselected package libxfixes3:amd64.\n",
      "Preparing to unpack .../30-libxfixes3_1%3a5.0.3-2_amd64.deb ...\n",
      "Unpacking libxfixes3:amd64 (1:5.0.3-2) ...\n",
      "Selecting previously unselected package libxshmfence1:amd64.\n",
      "Preparing to unpack .../31-libxshmfence1_1.3-1_amd64.deb ...\n",
      "Unpacking libxshmfence1:amd64 (1.3-1) ...\n",
      "Selecting previously unselected package libxxf86vm1:amd64.\n",
      "Preparing to unpack .../32-libxxf86vm1_1%3a1.1.4-1+b2_amd64.deb ...\n",
      "Unpacking libxxf86vm1:amd64 (1:1.1.4-1+b2) ...\n",
      "Selecting previously unselected package libglx-mesa0:amd64.\n",
      "Preparing to unpack .../33-libglx-mesa0_20.3.5-1_amd64.deb ...\n",
      "Unpacking libglx-mesa0:amd64 (20.3.5-1) ...\n",
      "Selecting previously unselected package sudo.\n",
      "Preparing to unpack .../34-sudo_1.9.5p2-3+deb11u1_amd64.deb ...\n",
      "Unpacking sudo (1.9.5p2-3+deb11u1) ...\n",
      "Selecting previously unselected package libglx0:amd64.\n",
      "Preparing to unpack .../35-libglx0_1.3.2-1_amd64.deb ...\n",
      "Unpacking libglx0:amd64 (1.3.2-1) ...\n",
      "Selecting previously unselected package libgl1:amd64.\n",
      "Preparing to unpack .../36-libgl1_1.3.2-1_amd64.deb ...\n",
      "Unpacking libgl1:amd64 (1.3.2-1) ...\n",
      "Setting up libxcb-dri3-0:amd64 (1.14-3) ...\n",
      "Setting up libx11-xcb1:amd64 (2:1.7.2-1+deb11u2) ...\n",
      "Setting up libpciaccess0:amd64 (0.16-1) ...\n",
      "Setting up libxdamage1:amd64 (1:1.1.5-2) ...\n",
      "Setting up libxcb-xfixes0:amd64 (1.14-3) ...\n",
      "Setting up libarchive13:amd64 (3.4.3-2+deb11u1) ...\n",
      "Setting up libglvnd0:amd64 (1.3.2-1) ...\n",
      "Setting up libxcb-glx0:amd64 (1.14-3) ...\n",
      "Setting up libsensors-config (1:3.6.0-7) ...\n",
      "Setting up libxxf86vm1:amd64 (1:1.1.4-1+b2) ...\n",
      "Setting up libxcb-present0:amd64 (1.14-3) ...\n",
      "Setting up libz3-4:amd64 (4.8.10-1) ...\n",
      "Setting up libuv1:amd64 (1.40.0-2+deb11u1) ...\n",
      "Setting up libllvm11:amd64 (1:11.0.1-2) ...\n",
      "Setting up libxfixes3:amd64 (1:5.0.3-2) ...\n",
      "Setting up libxcb-sync1:amd64 (1.14-3) ...\n",
      "Setting up sudo (1.9.5p2-3+deb11u1) ...\n",
      "invoke-rc.d: could not determine current runlevel\n",
      "invoke-rc.d: policy-rc.d denied execution of start.\n",
      "Setting up libsensors5:amd64 (1:3.6.0-7) ...\n",
      "Setting up libglapi-mesa:amd64 (20.3.5-1) ...\n",
      "Setting up libvulkan1:amd64 (1.2.162.0-1) ...\n",
      "Setting up libjsoncpp24:amd64 (1.9.4-4) ...\n",
      "Setting up libxcb-dri2-0:amd64 (1.14-3) ...\n",
      "Setting up libxshmfence1:amd64 (1.3-1) ...\n",
      "Setting up librhash0:amd64 (1.4.1-2) ...\n",
      "Setting up build-essential (12.9) ...\n",
      "Setting up cmake-data (3.18.4-2+deb11u1) ...\n",
      "Setting up libdrm-common (2.4.104-1) ...\n",
      "Setting up libdrm2:amd64 (2.4.104-1) ...\n",
      "Setting up cmake (3.18.4-2+deb11u1) ...\n",
      "Setting up libdrm-amdgpu1:amd64 (2.4.104-1) ...\n",
      "Setting up libdrm-nouveau2:amd64 (2.4.104-1) ...\n",
      "Setting up libdrm-radeon1:amd64 (2.4.104-1) ...\n",
      "Setting up libdrm-intel1:amd64 (2.4.104-1) ...\n",
      "Setting up libgl1-mesa-dri:amd64 (20.3.5-1) ...\n",
      "Setting up libglx-mesa0:amd64 (20.3.5-1) ...\n",
      "Setting up libglx0:amd64 (1.3.2-1) ...\n",
      "Setting up libgl1:amd64 (1.3.2-1) ...\n",
      "Processing triggers for libc-bin (2.31-13+deb11u10) ...\n",
      "ldconfig: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "ldconfig: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "ldconfig: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "ldconfig: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "ldconfig: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "ldconfig: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "Removing intermediate container c623eb87f94f\n",
      " ---> 34d9284fbcb3\n",
      "Step 4/19 : RUN wget https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/LICENSE\n",
      " ---> Running in 0e514aabf17c\n",
      "\u001b[91m--2024-06-11 09:00:57--  https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/LICENSE\n",
      "\u001b[0m\u001b[91mResolving raw.githubusercontent.com (raw.githubusercontent.com)... \u001b[0m\u001b[91m185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... \u001b[0m\u001b[91mconnected.\n",
      "\u001b[0m\u001b[91mHTTP request sent, awaiting response... \u001b[0m\u001b[91m200 OK\n",
      "Length: 11358 (11K) [text/plain]\n",
      "\u001b[0m\u001b[91mSaving to: ‘LICENSE’\n",
      "\u001b[0m\u001b[91m\n",
      "     0K ......\u001b[0m\u001b[91m.... .                                          100% 17.2M=0.001s\n",
      "\n",
      "2024-06-11 09:00:57 (17.2 MB/s) - ‘LICENSE’ saved [11358/11358]\n",
      "\n",
      "\u001b[0mRemoving intermediate container 0e514aabf17c\n",
      " ---> 17d7d8e8d545\n",
      "Step 5/19 : RUN pip install --upgrade pip\n",
      " ---> Running in 54ad132773f7\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/site-packages (23.0.1)\n",
      "Collecting pip\n",
      "  Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 11.1 MB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.0.1\n",
      "    Uninstalling pip-23.0.1:\n",
      "      Successfully uninstalled pip-23.0.1\n",
      "Successfully installed pip-24.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 54ad132773f7\n",
      " ---> eff75ea3f462\n",
      "Step 6/19 : RUN pip install --upgrade pip\n",
      " ---> Running in de7e993c3558\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/site-packages (24.0)\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container de7e993c3558\n",
      " ---> ce1c1d0bc30e\n",
      "Step 7/19 : RUN pip install transformers==4.38.2 -U\n",
      " ---> Running in e503286ac7ea\n",
      "Collecting transformers==4.38.2\n",
      "  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 130.7/130.7 kB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers==4.38.2) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers==4.38.2)\n",
      "  Downloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from transformers==4.38.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from transformers==4.38.2) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers==4.38.2) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.38.2)\n",
      "  Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 3.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers==4.38.2) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.38.2)\n",
      "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.38.2)\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers==4.38.2)\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.6/57.6 kB 4.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.38.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.38.2) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.38.2) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers==4.38.2) (2024.2.2)\n",
      "Downloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.5/8.5 MB 37.1 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.23.3-py3-none-any.whl (401 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 401.7/401.7 kB 30.3 MB/s eta 0:00:00\n",
      "Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 775.1/775.1 kB 42.3 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 59.4 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 67.4 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.3/78.3 kB 8.2 MB/s eta 0:00:00\n",
      "Installing collected packages: tqdm, safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.23.3 regex-2024.5.15 safetensors-0.4.3 tokenizers-0.15.2 tqdm-4.66.4 transformers-4.38.2\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container e503286ac7ea\n",
      " ---> d8a0084acda9\n",
      "Step 8/19 : RUN pip install datasets==2.18.0\n",
      " ---> Running in 8e2b4c3b33e4\n",
      "Collecting datasets==2.18.0\n",
      "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from datasets==2.18.0) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from datasets==2.18.0) (1.26.4)\n",
      "Collecting pyarrow>=12.0.0 (from datasets==2.18.0)\n",
      "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets==2.18.0)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets==2.18.0)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets==2.18.0)\n",
      "  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/site-packages (from datasets==2.18.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/site-packages (from datasets==2.18.0) (4.66.4)\n",
      "Collecting xxhash (from datasets==2.18.0)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets==2.18.0)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.2.0,>=2023.1.0 (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets==2.18.0)\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets==2.18.0)\n",
      "  Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/site-packages (from datasets==2.18.0) (0.23.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from datasets==2.18.0) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from datasets==2.18.0) (6.0.1)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets==2.18.0)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets==2.18.0)\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==2.18.0)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.18.0)\n",
      "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets==2.18.0)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets==2.18.0)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets==2.18.0) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.18.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.18.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.18.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.18.0) (2024.2.2)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas->datasets==2.18.0)\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets==2.18.0)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets==2.18.0)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.18.0) (1.16.0)\n",
      "Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 510.5/510.5 kB 4.6 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 116.3/116.3 kB 11.1 MB/s eta 0:00:00\n",
      "Downloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 170.9/170.9 kB 15.9 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 21.4 MB/s eta 0:00:00\n",
      "Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.8/40.8 MB 26.9 MB/s eta 0:00:00\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 13.7 MB/s eta 0:00:00\n",
      "Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.0/13.0 MB 59.4 MB/s eta 0:00:00\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 18.7 MB/s eta 0:00:00\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.8/60.8 kB 6.0 MB/s eta 0:00:00\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 239.5/239.5 kB 20.7 MB/s eta 0:00:00\n",
      "Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.3/124.3 kB 12.1 MB/s eta 0:00:00\n",
      "Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 229.9/229.9 kB 22.1 MB/s eta 0:00:00\n",
      "Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 505.5/505.5 kB 29.7 MB/s eta 0:00:00\n",
      "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 345.4/345.4 kB 21.5 MB/s eta 0:00:00\n",
      "Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.6/301.6 kB 20.5 MB/s eta 0:00:00\n",
      "Installing collected packages: pytz, xxhash, tzdata, python-dateutil, pyarrow-hotfix, pyarrow, multidict, fsspec, frozenlist, dill, attrs, async-timeout, yarl, pandas, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.3.1\n",
      "    Uninstalling fsspec-2024.3.1:\n",
      "      Successfully uninstalled fsspec-2024.3.1\n",
      "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 attrs-23.2.0 datasets-2.18.0 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.2.0 multidict-6.0.5 multiprocess-0.70.16 pandas-2.2.2 pyarrow-16.1.0 pyarrow-hotfix-0.6 python-dateutil-2.9.0.post0 pytz-2024.1 tzdata-2024.1 xxhash-3.4.1 yarl-1.9.4\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 8e2b4c3b33e4\n",
      " ---> bd927ef218bd\n",
      "Step 9/19 : RUN pip install trl==0.8.1 peft==0.10.0\n",
      " ---> Running in 96ad945a4ad5\n",
      "Collecting trl==0.8.1\n",
      "  Downloading trl-0.8.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting peft==0.10.0\n",
      "  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/site-packages (from trl==0.8.1) (2.3.0)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/site-packages (from trl==0.8.1) (4.38.2)\n",
      "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.10/site-packages (from trl==0.8.1) (1.26.4)\n",
      "Collecting accelerate (from trl==0.8.1)\n",
      "  Downloading accelerate-0.31.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/site-packages (from trl==0.8.1) (2.18.0)\n",
      "Collecting tyro>=0.5.11 (from trl==0.8.1)\n",
      "  Downloading tyro-0.8.4-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from peft==0.10.0) (24.0)\n",
      "Collecting psutil (from peft==0.10.0)\n",
      "  Downloading psutil-5.9.8-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/site-packages (from peft==0.10.0) (6.0.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from peft==0.10.0) (4.66.4)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/site-packages (from peft==0.10.0) (0.4.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/site-packages (from peft==0.10.0) (0.23.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (2024.2.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (4.10.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.1) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.1) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.1) (3.1.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.1) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.1) (0.15.2)\n",
      "Collecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl==0.8.1)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting rich>=11.1.0 (from tyro>=0.5.11->trl==0.8.1)\n",
      "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.8.1)\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/site-packages (from datasets->trl==0.8.1) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/site-packages (from datasets->trl==0.8.1) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/site-packages (from datasets->trl==0.8.1) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from datasets->trl==0.8.1) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/site-packages (from datasets->trl==0.8.1) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/site-packages (from datasets->trl==0.8.1) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/site-packages (from datasets->trl==0.8.1) (3.9.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.1) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.1) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.1) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.1) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.1) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.1) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (2024.2.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.1)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting pygments<3.0.0,>=2.13.0 (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.1)\n",
      "  Downloading pygments-2.18.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl==0.8.1) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas->datasets->trl==0.8.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets->trl==0.8.1) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas->datasets->trl==0.8.1) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl==0.8.1) (1.3.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.8.1)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.8.1) (1.16.0)\n",
      "Downloading trl-0.8.1-py3-none-any.whl (225 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 225.0/225.0 kB 2.3 MB/s eta 0:00:00\n",
      "Downloading peft-0.10.0-py3-none-any.whl (199 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199.1/199.1 kB 9.8 MB/s eta 0:00:00\n",
      "Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 309.4/309.4 kB 15.0 MB/s eta 0:00:00\n",
      "Downloading tyro-0.8.4-py3-none-any.whl (102 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.4/102.4 kB 10.4 MB/s eta 0:00:00\n",
      "Downloading psutil-5.9.8-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 288.2/288.2 kB 19.5 MB/s eta 0:00:00\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 240.7/240.7 kB 16.5 MB/s eta 0:00:00\n",
      "Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.5/87.5 kB 8.4 MB/s eta 0:00:00\n",
      "Downloading pygments-2.18.0-py3-none-any.whl (1.2 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 19.2 MB/s eta 0:00:00\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: shtab, pygments, psutil, mdurl, docstring-parser, markdown-it-py, rich, accelerate, tyro, trl, peft\n",
      "Successfully installed accelerate-0.31.0 docstring-parser-0.16 markdown-it-py-3.0.0 mdurl-0.1.2 peft-0.10.0 psutil-5.9.8 pygments-2.18.0 rich-13.7.1 shtab-1.7.1 trl-0.8.1 tyro-0.8.4\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 96ad945a4ad5\n",
      " ---> f82c30851526\n",
      "Step 10/19 : RUN pip install accelerate==0.28.0\n",
      " ---> Running in 1c2384716b47\n",
      "Collecting accelerate==0.28.0\n",
      "  Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from accelerate==0.28.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from accelerate==0.28.0) (24.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/site-packages (from accelerate==0.28.0) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/site-packages (from accelerate==0.28.0) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/site-packages (from accelerate==0.28.0) (2.3.0)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/site-packages (from accelerate==0.28.0) (0.23.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/site-packages (from accelerate==0.28.0) (0.4.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.28.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.28.0) (4.10.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.28.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.28.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.28.0) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.28.0) (2024.2.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.28.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.28.0) (4.66.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.28.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.28.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.28.0) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.28.0) (1.3.0)\n",
      "Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 290.1/290.1 kB 2.7 MB/s eta 0:00:00\n",
      "Installing collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.31.0\n",
      "    Uninstalling accelerate-0.31.0:\n",
      "      Successfully uninstalled accelerate-0.31.0\n",
      "Successfully installed accelerate-0.28.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 1c2384716b47\n",
      " ---> a91a8f46eeec\n",
      "Step 11/19 : RUN pip install --upgrade google-cloud-storage\n",
      " ---> Running in 35803c4b75fe\n",
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-2.17.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=2.26.1 in /usr/local/lib/python3.10/site-packages (from google-cloud-storage) (2.29.0)\n",
      "Collecting google-api-core<3.0.0dev,>=2.15.0 (from google-cloud-storage)\n",
      "  Downloading google_api_core-2.19.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0 (from google-cloud-storage)\n",
      "  Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-resumable-media>=2.6.0 (from google-cloud-storage)\n",
      "  Downloading google_resumable_media-2.7.1-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/site-packages (from google-cloud-storage) (2.31.0)\n",
      "Collecting google-crc32c<2.0dev,>=1.0 (from google-cloud-storage)\n",
      "  Downloading google_crc32c-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.63.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (3.20.3)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage)\n",
      "  Downloading proto_plus-1.23.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (4.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2024.2.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.5.1)\n",
      "Downloading google_cloud_storage-2.17.0-py2.py3-none-any.whl (126 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 1.6 MB/s eta 0:00:00\n",
      "Downloading google_api_core-2.19.0-py3-none-any.whl (139 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 139.0/139.0 kB 6.4 MB/s eta 0:00:00\n",
      "Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl (29 kB)\n",
      "Downloading google_crc32c-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Downloading google_resumable_media-2.7.1-py2.py3-none-any.whl (81 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.2/81.2 kB 7.2 MB/s eta 0:00:00\n",
      "Downloading proto_plus-1.23.0-py3-none-any.whl (48 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.8/48.8 kB 5.2 MB/s eta 0:00:00\n",
      "Installing collected packages: proto-plus, google-crc32c, google-resumable-media, google-api-core, google-cloud-core, google-cloud-storage\n",
      "  Attempting uninstall: google-api-core\n",
      "    Found existing installation: google-api-core 1.34.1\n",
      "    Uninstalling google-api-core-1.34.1:\n",
      "      Successfully uninstalled google-api-core-1.34.1\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-api-python-client 1.8.0 requires google-api-core<2dev,>=1.13.0, but you have google-api-core 2.19.0 which is incompatible.\n",
      "\u001b[0mSuccessfully installed google-api-core-2.19.0 google-cloud-core-2.4.1 google-cloud-storage-2.17.0 google-crc32c-1.5.0 google-resumable-media-2.7.1 proto-plus-1.23.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 35803c4b75fe\n",
      " ---> 8fe39f5ad268\n",
      "Step 12/19 : RUN wget -O MIT_LICENSE https://github.com/pytest-dev/pytest/blob/main/LICENSE\n",
      " ---> Running in 140bdeef737b\n",
      "\u001b[91m--2024-06-11 09:02:06--  https://github.com/pytest-dev/pytest/blob/main/LICENSE\n",
      "\u001b[0m\u001b[91mResolving github.com (github.com)... \u001b[0m\u001b[91m140.82.112.4\n",
      "Connecting to github.com (github.com)|140.82.112.4|:443... \u001b[0m\u001b[91mconnected.\n",
      "\u001b[0m\u001b[91mHTTP request sent, awaiting response... \u001b[0m\u001b[91m200 OK\n",
      "\u001b[0m\u001b[91mLength: unspecified [text/html]\n",
      "Saving to: ‘MIT_LICENSE’\n",
      "\u001b[0m\u001b[91m\n",
      "     0K .....\u001b[0m\u001b[91m...\u001b[0m\u001b[91m.. .....\u001b[0m\u001b[91m....\u001b[0m\u001b[91m.\u001b[0m\u001b[91m ..........\u001b[0m\u001b[91m ......\u001b[0m\u001b[91m..\u001b[0m\u001b[91m.. ..........  992K\n",
      "    50K ........\u001b[0m\u001b[91m.. ......\u001b[0m\u001b[91m..\u001b[0m\u001b[91m..\u001b[0m\u001b[91m ..\u001b[0m\u001b[91m........ ..........\u001b[0m\u001b[91m ......\u001b[0m\u001b[91m.... 1.94M\n",
      "   100K ....\u001b[0m\u001b[91m......\u001b[0m\u001b[91m .......... ..\u001b[0m\u001b[91m.\u001b[0m\u001b[91m....... ..\u001b[0m\u001b[91m....\u001b[0m\u001b[91m.... .....\u001b[0m\u001b[91m..... 55.8M\u001b[0m\u001b[91m\n",
      "   150K ........\u001b[0m\u001b[91m..                                             57.9M=0.08s\n",
      "\n",
      "\u001b[0m\u001b[91m2024-06-11 09:02:07 (2.04 MB/s) - ‘MIT_LICENSE’ saved [163866]\n",
      "\n",
      "\u001b[0mRemoving intermediate container 140bdeef737b\n",
      " ---> 856cc3b1c1b5\n",
      "Step 13/19 : RUN wget -O BSD_LICENSE https://github.com/pytorch/xla/blob/master/LICENSE\n",
      " ---> Running in e877ad2b5eee\n",
      "\u001b[91m--2024-06-11 09:02:07--  https://github.com/pytorch/xla/blob/master/LICENSE\n",
      "\u001b[0m\u001b[91mResolving github.com (github.com)... \u001b[0m\u001b[91m140.82.112.4\n",
      "Connecting to github.com (github.com)|140.82.112.4|:443... \u001b[0m\u001b[91mconnected.\n",
      "\u001b[0m\u001b[91mHTTP request sent, awaiting response... \u001b[0m\u001b[91m200 OK\n",
      "\u001b[0m\u001b[91mLength: unspecified [text/html]\n",
      "\u001b[0m\u001b[91mSaving to: ‘BSD_LICENSE’\n",
      "\u001b[0m\u001b[91m\n",
      "     0K ...\u001b[0m\u001b[91m....\u001b[0m\u001b[91m..\u001b[0m\u001b[91m. .......\u001b[0m\u001b[91m... .\u001b[0m\u001b[91m.\u001b[0m\u001b[91m.....\u001b[0m\u001b[91m..\u001b[0m\u001b[91m.\u001b[0m\u001b[91m ..\u001b[0m\u001b[91m..\u001b[0m\u001b[91m..\u001b[0m\u001b[91m.... ..\u001b[0m\u001b[91m........ 1002K\u001b[0m\u001b[91m\n",
      "    50K ...\u001b[0m\u001b[91m.\u001b[0m\u001b[91m.....\u001b[0m\u001b[91m. ....\u001b[0m\u001b[91m..\u001b[0m\u001b[91m....\u001b[0m\u001b[91m .\u001b[0m\u001b[91m.....\u001b[0m\u001b[91m.... .........\u001b[0m\u001b[91m. ....\u001b[0m\u001b[91m..\u001b[0m\u001b[91m.... 1.95M\u001b[0m\u001b[91m\n",
      "   100K ....\u001b[0m\u001b[91m..\u001b[0m\u001b[91m..\u001b[0m\u001b[91m.. ..\u001b[0m\u001b[91m..\u001b[0m\u001b[91m......\u001b[0m\u001b[91m ..\u001b[0m\u001b[91m.......\u001b[0m\u001b[91m. ...\u001b[0m\u001b[91m.....\u001b[0m\u001b[91m.. .......... 1.99M\u001b[0m\u001b[91m\n",
      "   150K ...\u001b[0m\u001b[91m..                             \u001b[0m\u001b[91m                     40.1M=0.1s\n",
      "\n",
      "\u001b[0m\u001b[91m2024-06-11 09:02:08 (1.52 MB/s) - ‘BSD_LICENSE’ saved [159268]\n",
      "\n",
      "\u001b[0mRemoving intermediate container e877ad2b5eee\n",
      " ---> 8483a93cdd73\n",
      "Step 14/19 : RUN wget -O BSD-3_LICENSE https://github.com/pytorch/pytorch/blob/main/LICENSE\n",
      " ---> Running in 4da7a3700608\n",
      "\u001b[91m--2024-06-11 09:02:08--  https://github.com/pytorch/pytorch/blob/main/LICENSE\n",
      "\u001b[0m\u001b[91mResolving github.com (github.com)... \u001b[0m\u001b[91m140.82.113.3\n",
      "Connecting to github.com (github.com)|140.82.113.3|:443... \u001b[0m\u001b[91mconnected.\n",
      "\u001b[0m\u001b[91mHTTP request sent, awaiting response... \u001b[0m\u001b[91m200 OK\n",
      "\u001b[0m\u001b[91mLength: unspecified [text/html]\n",
      "\u001b[0m\u001b[91mSaving to: ‘BSD-3_LICENSE’\n",
      "\u001b[0m\u001b[91m\n",
      "     0K .\u001b[0m\u001b[91m..\u001b[0m\u001b[91m...\u001b[0m\u001b[91m.\u001b[0m\u001b[91m..\u001b[0m\u001b[91m. .......... .........\u001b[0m\u001b[91m. ......\u001b[0m\u001b[91m.... .\u001b[0m\u001b[91m...\u001b[0m\u001b[91m...... 1.00M\n",
      "    50K ....\u001b[0m\u001b[91m.....\u001b[0m\u001b[91m. ...\u001b[0m\u001b[91m...\u001b[0m\u001b[91m....\u001b[0m\u001b[91m ........\u001b[0m\u001b[91m.\u001b[0m\u001b[91m. ......\u001b[0m\u001b[91m...\u001b[0m\u001b[91m.\u001b[0m\u001b[91m .\u001b[0m\u001b[91m....\u001b[0m\u001b[91m..... 2.00M\n",
      "   100K ..\u001b[0m\u001b[91m......\u001b[0m\u001b[91m.\u001b[0m\u001b[91m.\u001b[0m\u001b[91m ....\u001b[0m\u001b[91m...... ..\u001b[0m\u001b[91m.\u001b[0m\u001b[91m....... .\u001b[0m\u001b[91m....\u001b[0m\u001b[91m.....\u001b[0m\u001b[91m ........\u001b[0m\u001b[91m.. 2.02M\u001b[0m\u001b[91m\n",
      "   150K .....\u001b[0m\u001b[91m..... ....\u001b[0m\u001b[91m...\u001b[0m\u001b[91m.\u001b[0m\u001b[91m.. .\u001b[0m\u001b[91m...\u001b[0m\u001b[91m.\u001b[0m\u001b[91m....\u001b[0m\u001b[91m. .......... .\u001b[0m\u001b[91m.......\u001b[0m\u001b[91m.. 34.7M\u001b[0m\u001b[91m\n",
      "   200K ......\u001b[0m\u001b[91m.... .\u001b[0m\u001b[91m...\u001b[0m\u001b[91m...... ......\u001b[0m\u001b[91m..\u001b[0m\u001b[91m.. ...\u001b[0m\u001b[91m.....\u001b[0m\u001b[91m.. ....\u001b[0m\u001b[91m..\u001b[0m\u001b[91m.... 53.7M\n",
      "   250K ..\u001b[0m\u001b[91m....\u001b[0m\u001b[91m.... ..\u001b[0m\u001b[91m...\u001b[0m\u001b[91m.\u001b[0m\u001b[91m.... ....\u001b[0m\u001b[91m....\u001b[0m\u001b[91m..\u001b[0m\u001b[91m .....\u001b[0m\u001b[91m.....\u001b[0m\u001b[91m ....\u001b[0m\u001b[91m...... 2.20M\n",
      "   300K ....\u001b[0m\u001b[91m..\u001b[0m\u001b[91m.... .......                                    \u001b[0m\u001b[91m 58.5M=0.1s\n",
      "\n",
      "\u001b[0m\u001b[91m2024-06-11 09:02:09 (2.54 MB/s) - ‘BSD-3_LICENSE’ saved [325152]\n",
      "\n",
      "\u001b[0mRemoving intermediate container 4da7a3700608\n",
      " ---> 7b29ad5903dc\n",
      "Step 15/19 : RUN find ./usr/local/lib -name 'libtpu.so' -exec cp {} /lib \\;\n",
      " ---> Running in ed459d3cf6d3\n",
      "Removing intermediate container ed459d3cf6d3\n",
      " ---> e0b9e1cdfa19\n",
      "Step 16/19 : WORKDIR /\n",
      " ---> Running in 268c37326438\n",
      "Removing intermediate container 268c37326438\n",
      " ---> b13088883439\n",
      "Step 17/19 : COPY train.py train.py\n",
      " ---> 3f58d58385ba\n",
      "Step 18/19 : ENV PYTHONPATH ./\n",
      " ---> Running in f6bd82a502b4\n",
      "Removing intermediate container f6bd82a502b4\n",
      " ---> 81af6f088178\n",
      "Step 19/19 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 349f27d1a5f2\n",
      "Removing intermediate container 349f27d1a5f2\n",
      " ---> f22c15380157\n",
      "Successfully built f22c15380157\n",
      "Successfully tagged us-central1-docker.pkg.dev/vertexai-service-project/tpuv5e-training-repository-unique/llama2-7b-hf-lora-tuning-tpuv5e:latest\n",
      "The push refers to repository [us-central1-docker.pkg.dev/vertexai-service-project/tpuv5e-training-repository-unique/llama2-7b-hf-lora-tuning-tpuv5e]\n",
      "\n",
      "\u001b[1Bc7aedb91: Preparing \n",
      "\u001b[1B2305458e: Preparing \n",
      "\u001b[1B58e57c0d: Preparing \n",
      "\u001b[1Be7d6e148: Preparing \n",
      "\u001b[1Bbbaac4b7: Preparing \n",
      "\u001b[1Bfdb3ccda: Preparing \n",
      "\u001b[1B6a1f59d2: Preparing \n",
      "\u001b[1Bbc5d97ed: Preparing \n",
      "\u001b[1B5fa451e7: Preparing \n",
      "\u001b[1B6898bd18: Preparing \n",
      "\u001b[1B9915511e: Preparing \n",
      "\u001b[1B79718539: Preparing \n",
      "\u001b[1Bc828fb26: Preparing \n",
      "\u001b[1Be015b988: Preparing \n",
      "\u001b[1B8db23091: Preparing \n",
      "\u001b[1B6141a482: Preparing \n",
      "\u001b[1Bf79174ac: Preparing \n",
      "\u001b[1Bb892285d: Preparing \n",
      "\u001b[1B53ba77a8: Preparing \n",
      "\u001b[1B784a2757: Preparing \n",
      "\u001b[1B1c241054: Preparing \n",
      "\u001b[1Bcb157760: Preparing \n",
      "\u001b[1B81fcdce1: Preparing \n",
      "\u001b[1B07f0d13f: Preparing \n",
      "\u001b[1B6173ac2b: Preparing \n",
      "\u001b[1B4565ceed: Preparing \n",
      "\u001b[1B95e3aea4: Preparing \n",
      "\u001b[1B36fe7bf5: Preparing \n",
      "\u001b[1B68b110ff: Preparing \n",
      "\u001b[1Bebd30172: Preparing \n",
      "\u001b[1B91ef6d1f: Preparing \n",
      "\u001b[19B015b988: Pushed   265.7MB/260.6MB-releases/docker/xla 8A\u001b[2K\u001b[32A\u001b[2K\u001b[26A\u001b[2K\u001b[29A\u001b[2K\u001b[26A\u001b[2K\u001b[25A\u001b[2K\u001b[31A\u001b[2K\u001b[25A\u001b[2K\u001b[26A\u001b[2K\u001b[25A\u001b[2K\u001b[27A\u001b[2K\u001b[24A\u001b[2K\u001b[25A\u001b[2K\u001b[24A\u001b[2K\u001b[25A\u001b[2K\u001b[31A\u001b[2K\u001b[25A\u001b[2K\u001b[31A\u001b[2K\u001b[25A\u001b[2K\u001b[24A\u001b[2K\u001b[25A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[25A\u001b[2K\u001b[23A\u001b[2K\u001b[25A\u001b[2K\u001b[24A\u001b[2K\u001b[21A\u001b[2K\u001b[25A\u001b[2K\u001b[23A\u001b[2K\u001b[31A\u001b[2K\u001b[23A\u001b[2K\u001b[24A\u001b[2K\u001b[25A\u001b[2K\u001b[21A\u001b[2K\u001b[23A\u001b[2K\u001b[21A\u001b[2K\u001b[24A\u001b[2K\u001b[25A\u001b[2K\u001b[21A\u001b[2K\u001b[25A\u001b[2K\u001b[21A\u001b[2K\u001b[25A\u001b[2K\u001b[23A\u001b[2K\u001b[25A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[21A\u001b[2K\u001b[25A\u001b[2K\u001b[24A\u001b[2K\u001b[23A\u001b[2K\u001b[24A\u001b[2K\u001b[23A\u001b[2K\u001b[21A\u001b[2K\u001b[23A\u001b[2K\u001b[21A\u001b[2K\u001b[31A\u001b[2K\u001b[21A\u001b[2K\u001b[24A\u001b[2K\u001b[25A\u001b[2K\u001b[24A\u001b[2K\u001b[23A\u001b[2K\u001b[23A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[23A\u001b[2K\u001b[31A\u001b[2K\u001b[23A\u001b[2K\u001b[31A\u001b[2K\u001b[23A\u001b[2K\u001b[31A\u001b[2K\u001b[23A\u001b[2K\u001b[31A\u001b[2K\u001b[23A\u001b[2K\u001b[31A\u001b[2K\u001b[23A\u001b[2K\u001b[24A\u001b[2K\u001b[23A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[23A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[23A\u001b[2K\u001b[19A\u001b[2K\u001b[23A\u001b[2K\u001b[24A\u001b[2K\u001b[23A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[23A\u001b[2K\u001b[19A\u001b[2K\u001b[23A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[23A\u001b[2K\u001b[19A\u001b[2K\u001b[23A\u001b[2K\u001b[24A\u001b[2K\u001b[16A\u001b[2K\u001b[23A\u001b[2K\u001b[24A\u001b[2K\u001b[23A\u001b[2K\u001b[24A\u001b[2K\u001b[23A\u001b[2K\u001b[19A\u001b[2K\u001b[23A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[23A\u001b[2K\u001b[24A\u001b[2K\u001b[23A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[15A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[23A\u001b[2K\u001b[24A\u001b[2K\u001b[23A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[23A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[23A\u001b[2K\u001b[19A\u001b[2K\u001b[23A\u001b[2K\u001b[24A\u001b[2K\u001b[23A\u001b[2K\u001b[13A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[23A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[11A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[9A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[6A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[2A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[1A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[31A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[24A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[24A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[24A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2Klatest: digest: sha256:9fd34caa99f0463e17e019445435cf2b7a2d594bd6ea4f98753b1ed83f6445b4 size: 7060\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "    \n",
    "if not IS_COLAB:\n",
    "    !docker build -t $PYTORCH_TRAIN_DOCKER_URI -f Dockerfile .\n",
    "    !docker push $PYTORCH_TRAIN_DOCKER_URI\n",
    "\n",
    "else:\n",
    "   ! gcloud builds submit --region={LOCATION} --tag={IMAGE_URI}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "586a13cb67b4"
   },
   "source": [
    "#### Change back to your home directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "937b7269c93b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/vertex-ai-samples/notebooks/official/training\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08bf867e8f4c"
   },
   "source": [
    "#### Set GCS folder locations and job configurations settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "17392b36e9c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'display_name': 'llama2-7b-lora-train-4x4_20240611_090307',\n",
       " 'job_spec': {'worker_pool_specs': [{'machine_spec': {'machine_type': 'ct5lp-hightpu-4t',\n",
       "     'tpu_topology': '4x4'},\n",
       "    'replica_count': 1,\n",
       "    'container_spec': {'image_uri': 'us-central1-docker.pkg.dev/vertexai-service-project/tpuv5e-training-repository-unique/llama2-7b-hf-lora-tuning-tpuv5e:latest',\n",
       "     'args': ['--tpu_topology=4x4',\n",
       "      '--model_name=meta-llama/Llama-2-7b-hf',\n",
       "      '--bucket_name=k-bucket-vertexai-service-project-tpuv5ellama',\n",
       "      '--output_folder=output',\n",
       "      '--checkpoint_directory=output_chk',\n",
       "      '--epochs=1',\n",
       "      '--merged_model_folder=llama2-7b-hf/modelfiles']}}]}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a GCS folder to store the merged model with the base model and the\n",
    "# fine-tuned LORA adapter.\n",
    "BUCKET_NAME = BUCKET_URI.replace(\"gs://\", \"\")\n",
    "OUTPUT_DIR_NAME = \"output\"\n",
    "CHECKPOINT_DIR_NAME = \"output_chk\"\n",
    "NUM_EPOCHS = 200\n",
    "NUM_EPOCHS = 1##TODO: DELETE LINE\n",
    "MERGED_MODEL_FOLDER = \"llama2-7b-hf/modelfiles\"\n",
    "\n",
    "# See machines type to match chips being used\n",
    "# Topologies of 2x2, 2x4, 4x4 = 4, 8, 16 chip settings and use quota from aiplatform.googleapis.com/custom_model_training_tpu_v5e\n",
    "MACHINE_TYPE = \"ct5lp-hightpu-4t\"\n",
    "TPU_TOPOLOGY = \"4x4\"\n",
    "\n",
    "DISPLAY_NAME_PREFIX = f\"llama2-7b-lora-train-{TPU_TOPOLOGY}\"\n",
    "tpuv5e_llama2_peft_job = {\n",
    "    \"display_name\": get_job_name_with_datetime(DISPLAY_NAME_PREFIX),\n",
    "    \"job_spec\": {\n",
    "        \"worker_pool_specs\": [\n",
    "            {\n",
    "                \"machine_spec\": {\n",
    "                    \"machine_type\": MACHINE_TYPE,\n",
    "                    \"tpu_topology\": TPU_TOPOLOGY,\n",
    "                },\n",
    "                \"replica_count\": 1,\n",
    "                \"container_spec\": {\n",
    "                    \"image_uri\": PYTORCH_TRAIN_DOCKER_URI,\n",
    "                    \"args\": [\n",
    "                        f\"--tpu_topology={TPU_TOPOLOGY}\",\n",
    "                        f\"--model_name={HF_MODEL_ID}\",\n",
    "                        f\"--bucket_name={BUCKET_NAME}\",\n",
    "                        f\"--output_folder={OUTPUT_DIR_NAME}\",\n",
    "                        f\"--checkpoint_directory={CHECKPOINT_DIR_NAME}\",\n",
    "                        f\"--epochs={NUM_EPOCHS}\",\n",
    "                        f\"--merged_model_folder={MERGED_MODEL_FOLDER}\",\n",
    "                    ],\n",
    "                },\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "tpuv5e_llama2_peft_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c2bc267c4a5"
   },
   "source": [
    "#### Create job client and run job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bc9b403c8515"
   },
   "outputs": [],
   "source": [
    "job_client = aiplatform.gapic.JobServiceClient(\n",
    "    client_options=dict(api_endpoint=f\"{LOCATION}-aiplatform.googleapis.com\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4da63b786392"
   },
   "outputs": [],
   "source": [
    "create_tpuv5e_llama2_peft_job_response = job_client.create_custom_job(\n",
    "    parent=\"projects/{project}/locations/{location}\".format(\n",
    "        project=PROJECT_ID, location=LOCATION\n",
    "    ),\n",
    "    custom_job=tpuv5e_llama2_peft_job,\n",
    ")\n",
    "print(create_tpuv5e_llama2_peft_job_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb8dfcc23789"
   },
   "source": [
    "#### Check on job progress\n",
    "This may take 20-60 minutes or more depending on the model size. Run this cell multiple times to check progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f402309d9dbb"
   },
   "outputs": [],
   "source": [
    "get_tpuv5e_llama2_peft_job_response = job_client.get_custom_job(\n",
    "    name=create_tpuv5e_llama2_peft_job_response.name\n",
    ")\n",
    "get_tpuv5e_llama2_peft_job_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eb7f4f1ac160"
   },
   "source": [
    "#### Click on the console log url output from this cell to see your logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "babf40cf7821"
   },
   "outputs": [],
   "source": [
    "job_id = create_tpuv5e_llama2_peft_job_response.name[\n",
    "    create_tpuv5e_llama2_peft_job_response.name.rfind(\"/\") + 1 :\n",
    "]\n",
    "STARTDATE = datetime.today() - timedelta(days=1)\n",
    "STARTDATE = STARTDATE.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "ENDDATE = datetime.today() + timedelta(days=0.1)\n",
    "ENDDATE = ENDDATE.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "print(\n",
    "    f\"https://console.cloud.google.com/logs/query;query=resource.labels.job_id=%22{job_id}%22;cursorTimestamp={ENDDATE}Z;startTime={STARTDATE}Z;endTime={ENDDATE}Z?project={PROJECT_ID}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0f97d1b4fb05"
   },
   "source": [
    "#### Wait until the training job is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6ecd909fea8"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "\n",
    "while True:\n",
    "    response = job_client.get_custom_job(\n",
    "        name=create_tpuv5e_llama2_peft_job_response.name\n",
    "    )\n",
    "    if response.state != aip.JobState.JOB_STATE_SUCCEEDED:\n",
    "        print(f\"Training is not complete and is in state {response.state.name}\")\n",
    "        if response.state == aip.JobState.JOB_STATE_FAILED:\n",
    "            raise Exception(\"Training Job Failed\")\n",
    "    else:\n",
    "        print(\"Training has completed\")\n",
    "        break\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d52a6bb72aa6"
   },
   "source": [
    "### Deploy fine tuned models\n",
    "This section uploads the model to Model Registry and deploys the model using Hex-LLM, a High-Efficiency Large Language Model serving solution built with XLA that is being developed by Google Cloud\n",
    "\n",
    "The model deployment step will take 15-20 minutes to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e1936eee4a7b"
   },
   "outputs": [],
   "source": [
    "HEXLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai-restricted/vertex-vision-model-garden-dockers/hex-llm-serve:20240328_RC01\"\n",
    "\n",
    "# GCS folder path where the merged model files were saved in you bucket\n",
    "# MERGED_MODEL_FOLDER=\"llama2-7b-hf/modelfiles\" set during fine-tuning\n",
    "MERGED_MODEL_PATH = f\"{MERGED_MODEL_FOLDER}/merged_model/0\"\n",
    "GCS_MODEL_PATH = f\"{BUCKET_URI}/{MERGED_MODEL_PATH}\"\n",
    "\n",
    "DISPLAY_NAME_PREFIX = \"llama2-7b-lora-deploy\"  # @param {type:\"string\"}\n",
    "JOB_NAME = get_job_name_with_datetime(DISPLAY_NAME_PREFIX)\n",
    "GCS_MODEL_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c963a9dcf4ac"
   },
   "source": [
    "#### Check the model files in your GCS directory\n",
    "\n",
    "Your output should show a list of files like this\n",
    "```\n",
    "gs://<YOUR-BUCKET>/modelfiles/merged_model/config.json\n",
    "gs://<YOUR-BUCKET>/modelfiles/merged_model/generation_config.json\n",
    "gs://<YOUR-BUCKET>/modelfiles/merged_model/pytorch_model-00001-of-00003.bin\n",
    "gs://<YOUR-BUCKET>/modelfiles/merged_model/pytorch_model-00002-of-00003.bin\n",
    "gs://<YOUR-BUCKET>/modelfiles/merged_model/pytorch_model-00003-of-00003.bin\n",
    "gs://<YOUR-BUCKET>/modelfiles/merged_model/pytorch_model.bin.index.json\n",
    "gs://<YOUR-BUCKET>/modelfiles/merged_model/special_tokens_map.json\n",
    "gs://<YOUR-BUCKET>/modelfiles/merged_model/tokenizer.json\n",
    "gs://<YOUR-BUCKET>/modelfiles/merged_model/tokenizer_config.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9159d64417a0"
   },
   "outputs": [],
   "source": [
    "!gsutil ls $GCS_MODEL_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0188453d4c9f"
   },
   "source": [
    "#### Define function for deploying model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0931f14c09cb"
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def deploy_model_hexllm(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    service_account: str,\n",
    "    machine_type: str = \"ct5lp-hightpu-4t\",\n",
    "    max_num_batched_tokens: int = 11264,  # 11264\n",
    "    tokens_pad_multiple: int = 1024,\n",
    "    seqs_pad_multiple: int = 32,\n",
    "    sync: bool = True,\n",
    ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys models with Hex-LLM on TPU in Vertex AI.\"\"\"\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
    "\n",
    "    num_tpu_chips = int(machine_type[-2])\n",
    "    hexllm_args = [\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        \"--log_level=INFO\",\n",
    "        f\"--model={model_id}\",\n",
    "        \"--load_format=pt\",  # Note: Using Pytorch bin format for weights\n",
    "        f\"--tensor_parallel_size={num_tpu_chips}\",\n",
    "        \"--num_nodes=1\",\n",
    "        \"--use_ray\",\n",
    "        \"--batch_mode=continuous\",\n",
    "        f\"--max_num_batched_tokens={max_num_batched_tokens}\",\n",
    "        f\"--tokens_pad_multiple={tokens_pad_multiple}\",\n",
    "        f\"--seqs_pad_multiple={seqs_pad_multiple}\",\n",
    "    ]\n",
    "\n",
    "    env_vars = {\n",
    "        \"PJRT_DEVICE\": \"TPU\",\n",
    "        \"RAY_DEDUP_LOGS\": \"0\",\n",
    "        \"RAY_USAGE_STATS_ENABLED\": \"0\",\n",
    "    }\n",
    "\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=HEXLLM_DOCKER_URI,\n",
    "        serving_container_command=[\"python\", \"-m\", \"hex_llm.entrypoints.api_server\"],\n",
    "        serving_container_args=hexllm_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "        serving_container_environment_variables=env_vars,\n",
    "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
    "        serving_container_deployment_timeout=7200,\n",
    "    )\n",
    "\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "        sync=sync,\n",
    "    )\n",
    "    return model, endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8945e2e6d6ed"
   },
   "source": [
    "#### Deploy model to Vertex\n",
    "The `deploy_model_hexllm` function will return a reference to the model added to the Vertex AI Model Registry as well as a new endpoint where the model will be deployed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ecd851116c14"
   },
   "outputs": [],
   "source": [
    "print(\"Using model from: \", GCS_MODEL_PATH)\n",
    "model, endpoint = deploy_model_hexllm(\n",
    "    model_name=JOB_NAME,\n",
    "    model_id=GCS_MODEL_PATH,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    sync=False,\n",
    ")\n",
    "print(\"endpoint_name:\", endpoint.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81650c5e3444"
   },
   "source": [
    "#### Review the logs after the model has been deployed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7e31488fc924"
   },
   "outputs": [],
   "source": [
    "ENDPOINT_ID = endpoint.name[endpoint.name.rfind(\"/\") + 1 :]\n",
    "STARTDATE = datetime.today() - timedelta(days=1)\n",
    "STARTDATE = STARTDATE.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "ENDDATE = datetime.today() + timedelta(days=0.1)\n",
    "ENDDATE = ENDDATE.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "print(\n",
    "    f\"https://console.cloud.google.com/logs/query;query=resource.type%3D%22aiplatform.googleapis.com%2FEndpoint%22%20resource.labels.endpoint_id%3D%22{ENDPOINT_ID}%22%20resource.labels.location%3D%22{LOCATION}%22;startTime={STARTDATE}Z;endTime={ENDDATE}Z?project={PROJECT_ID}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adb2ed90a241"
   },
   "source": [
    "#### Wait until endpoint is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4c92f6cdd08d"
   },
   "outputs": [],
   "source": [
    "endpoint.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3d894b10257a"
   },
   "outputs": [],
   "source": [
    "# (optional) Wait 15 minutes while the model is downloaded and setup\n",
    "if os.getenv(\"IS_TESTING\"):\n",
    "    time.sleep(900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "221a5b47cd7d"
   },
   "source": [
    "NOTE: The overall deployment can take 30-40 minutes or more. After the deployment succeeds (15-20 minutes or so), the fine-tuned model will be downloaded from the GCS bucket used in training above. Thus, an additional ~15-20 minutes (depending on the model sizes) of waiting time is needed **after** the model deployment step above succeeds and before you run the next step below. Otherwise you might see a `ServiceUnavailable: 503 502:Bad Gateway` error when you send requests to the endpoint.\n",
    "\n",
    "### Once deployment is ready, send a prediction request\n",
    "\n",
    "Once deployment succeeds, you can send requests to the endpoint with text prompts. The first request will take a minute or two while model warmup occurs\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "Prompt: Provide a list of the 3 best comedy movies in the 90s in 50 characters or less\n",
    "Response:  1) The Cable Guy 2) Scooby-Doo 3) Beethoven Requirements\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0a85ec25e11f"
   },
   "outputs": [],
   "source": [
    "PROMPT = (\n",
    "    \"Provide a list of the 3 best comedy movies in the 90s in 50 characters or less\"\n",
    ")\n",
    "\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": PROMPT,\n",
    "        \"max_tokens\": 80,\n",
    "        \"temperature\": 1.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"top_k\": 1.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "response = endpoint.predict(instances=instances)\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af21a3cff1e0"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "911406c1561e"
   },
   "outputs": [],
   "source": [
    "# Delete the train job.\n",
    "job_client.delete_custom_job(name=create_tpuv5e_llama2_peft_job_response.name)\n",
    "\n",
    "# Undeploy model and delete endpoint.\n",
    "endpoint.delete(force=True)\n",
    "\n",
    "# Delete models.\n",
    "model.delete()\n",
    "\n",
    "import os\n",
    "\n",
    "# Delete Cloud Storage objects that were created\n",
    "delete_bucket = False\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tpuv5e_llama2_pytorch_finetuning_and_serving.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

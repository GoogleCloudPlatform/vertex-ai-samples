{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Vertex AI - Llama2 fine-tuning with LoRA and serving on TPUv5e\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/training/tpuv5e_llama2_pytorch_finetuning_and_serving.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/training/tpuv5e_llama2_pytorch_finetuning_and_serving.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/training/tpuv5e_llama2_pytorch_finetuning_and_serving.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "Open in Vertex AI Workbench\n",
        "    </a> \n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e4695f5e9dc"
      },
      "source": [
        "# Quota - Make sure this is complete before you start!\n",
        "\n",
        "In order to run this example, you will need the following TPUv5e quota approved. You can make requests in the console via IAM & Admin > Quotas or by reaching out to your Google account team:\n",
        "\n",
        "aiplatform.googleapis.com/custom_model_serving_tpu_v5e (4-8 chips. 4 chips minimum for Llama2 7B)\n",
        "aiplatform.googleapis.com/custom_model_training_tpu_v5e (16 chips minimum)\n",
        "\n",
        "Check the [TPU pricing page](https://cloud.google.com/tpu/pricing) for the region availability and pricing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates fine-tuning of a Llama2 7B model with [LoRA](https://huggingface.co/docs/peft/v0.9.0/en/package_reference/lora#peft.LoraConfig), and uses a TPUv5e for both fine-tuning and serving. The fine-tuning is based on a [Hugging Face example](https://huggingface.co/google/gemma-7b/blob/main/examples/example_fsdp.py) that uses [fully sharded data parallel with PyTorch XLA](https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/), and [SPMD](https://pytorch.org/blog/pytorch-xla-spmd/). Follow the links to learn more.\n",
        "\n",
        "\n",
        "Fine-tuning occurs with a [Vertex AI Custom Training Job](https://cloud.google.com/vertex-ai/docs/training/create-custom-job). A Vertex AI Custom Training Job allows for a higher level of customization and control over the fine-tuning job. All of the examples in this notebook use parameter Low-Rank Adaption [LoRA](https://huggingface.co/docs/peft/en/package_reference/lora) to reduce training and storage costs.\n",
        "\n",
        "This notebook deploys the model using Hex-LLM, a High-Efficiency Large Language Model serving solution built with XLA that is being developed by Google Cloud\n",
        "\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Fine-tune and deploy Llama2 models with a Vertex AI Custom Training Job and Vertex Prediction endpoint.\n",
        "- Send prediction requests to your fine-tuned Llama2 model.\n",
        "\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08d289fa873f"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "In this example, you will use an english_quotes dataset from Hugging Face to fine-tune the model. Details of the dataset can be found here: https://huggingface.co/datasets/Abirate/english_quotes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aed92deeb4a0"
      },
      "source": [
        "### Costs \n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "Vertex AI (Training, Prediction, TPUv5e), Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), [Cloud NL API pricing](https://cloud.google.com/natural-language/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7EUnXsZhAGF"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Install the following packages required to execute this notebook. \n",
        "\n",
        "Run this to install the latest google cloud platform library that supports TPUv5e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b4ef9b72d43"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# (optional) update gcloud if needed\n",
        "if os.getenv(\"IS_TESTING\"):\n",
        "    ! gcloud components update --quiet\n",
        "\n",
        "! pip3 install --upgrade --quiet google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58707a750154"
      },
      "source": [
        "### Colab only: Uncomment the following cell to restart the kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f200f10a1da3"
      },
      "outputs": [],
      "source": [
        "# import IPython\n",
        "\n",
        "# app = IPython.Application.instance()\n",
        "# app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF1j6f9HApxa"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "3. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "4. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
        "\n",
        "1. [Select or create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs.\n",
        "\n",
        "1. [Create a service account](https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console) with `Vertex AI User` and `Storage Object Admin` roles for deploying fine tuned model to Vertex AI endpoint."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WReHDGG5g0XY"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, try the following:\n",
        "* Run `gcloud config list`.\n",
        "* Run `gcloud projects list`.\n",
        "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM1iC_MfAts1"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations).\n",
        "\n",
        "TPUv5e is available in the [following regions listed here](https://cloud.google.com/tpu/pricing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "region"
      },
      "outputs": [],
      "source": [
        "REGION = \"us-west1\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBCra4QMA2wR"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74ccc9e52986"
      },
      "source": [
        "**1. Vertex AI Workbench**\n",
        "* Do nothing as you are already authenticated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de775a3773ba"
      },
      "source": [
        "**2. Local JupyterLab instance, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "254614fa0c46"
      },
      "outputs": [],
      "source": [
        "# ! gcloud auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef21552ccea8"
      },
      "source": [
        "**3. Colab, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "603adbbf0532"
      },
      "outputs": [],
      "source": [
        "# from google.colab import auth\n",
        "# auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6b2ccc891ed"
      },
      "source": [
        "**4. Service account or other**\n",
        "* See how to grant Cloud Storage permissions to your service account at https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "960505627ddf"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "from google.cloud import aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgPO1eR3CYjk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "Create a storage bucket to store intermediate artifacts such as datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzGDU7TWdts_"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "648eb19b83dd"
      },
      "source": [
        "#### Set folder paths for staging, environment, and model artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "855d6b96f291"
      },
      "outputs": [],
      "source": [
        "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
        "\n",
        "# The service account looks like:\n",
        "# '@.iam.gserviceaccount.com'\n",
        "# Please go to https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console\n",
        "# and create service account with `Vertex AI User` and `Storage Object Admin` roles.\n",
        "# The service account for deploying fine tuned model.\n",
        "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EcIXiGsCePi"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIq7R4HZCfIc"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2931d2e5a94"
      },
      "source": [
        "### Access Llama 2 pretrained and finetuned models\n",
        "The original models from Meta are converted into the Hugging Face format for finetuning and serving in Vertex AI.\n",
        "\n",
        "Accept the model agreement to access the models:\n",
        "\n",
        "1. Navigate to the Vertex AI > Model Garden page in the Google Cloud console\n",
        "2. Search for Llama 2\n",
        "3. Review the agreement that pops up on the model card page\n",
        "4. Accept the agreement of Llama 2\n",
        "5. On the documentation tab, a Cloud Storage bucket containing Llama 2 pretrained and finetuned models will be shared\n",
        "5. Paste the Cloud Storage bucket link below and assign it to VERTEX_AI_MODEL_GARDEN_LLAMA2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47330c9cae9f"
      },
      "outputs": [],
      "source": [
        "VERTEX_AI_MODEL_GARDEN_LLAMA2 = \"<Bucket path from documentation tab of Llama 2 in Vertex Model Garden>\"  # This will be shared once click the agreement of LLaMA2 in Vertex AI Model Garden.\n",
        "VERTEX_MODEL_ID = \"llama2-7b-hf\"\n",
        "HF_MODEL_ID = \"meta-llama/Llama-2-7b-hf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "890b49a97355"
      },
      "outputs": [],
      "source": [
        "assert (\n",
        "    VERTEX_AI_MODEL_GARDEN_LLAMA2\n",
        "), \"Please click the agreement of Llama 2 in Vertex AI Model Garden, and get the GCS path of Llama 2 model artifacts.\"\n",
        "print(\n",
        "    \"Copy Llama 2 model artifacts from\",\n",
        "    VERTEX_AI_MODEL_GARDEN_LLAMA2,\n",
        "    \"to \",\n",
        "    f\"{BUCKET_URI}/{HF_MODEL_ID}\",\n",
        ")\n",
        "\n",
        "# Copy model files to your bucket\n",
        "! gcloud storage cp -R $VERTEX_AI_MODEL_GARDEN_LLAMA2/$VERTEX_MODEL_ID/* $BUCKET_URI/$HF_MODEL_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "outputs": [],
      "source": [
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "832bbd8cb9ef"
      },
      "source": [
        "### Create the artifact registry repository and set the custom docker image uri"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18fd49b88127"
      },
      "outputs": [],
      "source": [
        "REPOSITORY = \"tpuv5e-training-repository-unique\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2242d87effa"
      },
      "outputs": [],
      "source": [
        "image_name_train = \"llama2-7b-hf-lora-tuning-tpuv5e\"\n",
        "hostname = f\"{REGION}-docker.pkg.dev\"\n",
        "tag = \"latest\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dd94acb26e9"
      },
      "outputs": [],
      "source": [
        "# Register gcloud as a Docker credential helper\n",
        "!gcloud auth configure-docker $REGION-docker.pkg.dev --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c8e8364a097"
      },
      "outputs": [],
      "source": [
        "# One time or use an existing repository\n",
        "!gcloud artifacts repositories create $REPOSITORY --repository-format=docker \\\n",
        "--location=$REGION --description=\"Vertex TPUv5e training repository\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16488f1fc702"
      },
      "outputs": [],
      "source": [
        "# Define container image name\n",
        "PYTORCH_TRAIN_DOCKER_URI = (\n",
        "    f\"{hostname}/{PROJECT_ID}/{REPOSITORY}/{image_name_train}:{tag}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c250872074f"
      },
      "source": [
        "### Define common functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "354da31189dc"
      },
      "outputs": [],
      "source": [
        "def get_job_name_with_datetime(prefix: str) -> str:\n",
        "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
        "    jobs in Vertex AI.\n",
        "    \"\"\"\n",
        "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e11e1f8a6c4c"
      },
      "source": [
        "### Build the Docker container files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19d584f9fe62"
      },
      "source": [
        "#### Create the trainer directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "909e93e7fd43"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists(\"trainer\"):\n",
        "    os.makedirs(\"trainer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a91ddf0cbcb"
      },
      "source": [
        "#### Create the Dockerfile for the custom container. This will install Hugging Face transformers, datasets, trl, and peft for fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "756577886992"
      },
      "outputs": [],
      "source": [
        "%%writefile trainer/Dockerfile\n",
        "# This Dockerfile fine tunes the Llamas2 model using LoRA with PyTorch XLA\n",
        "# Nightly TPU VM docker image\n",
        "FROM us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla:nightly_3.10_tpuvm_20240324\n",
        "\n",
        "ENV DEBIAN_FRONTEND=noninteractive\n",
        "\n",
        "# Install basic libs\n",
        "RUN apt-get update && apt-get -y upgrade && apt-get install -y --no-install-recommends \\\n",
        "        cmake \\\n",
        "        curl \\\n",
        "        wget \\\n",
        "        sudo \\\n",
        "        gnupg \\\n",
        "        libsm6 \\\n",
        "        libxext6 \\\n",
        "        libxrender-dev \\\n",
        "        lsb-release \\\n",
        "        ca-certificates \\\n",
        "        build-essential \\\n",
        "        git \\\n",
        "        libgl1\n",
        "\n",
        "# Copy Apache license.\n",
        "RUN wget https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/LICENSE\n",
        "\n",
        "# Install required libs\n",
        "RUN pip install --upgrade pip\n",
        "RUN pip install --upgrade pip\n",
        "RUN pip install transformers==4.38.2 -U\n",
        "RUN pip install datasets==2.18.0\n",
        "RUN pip install trl==0.8.1 peft==0.10.0\n",
        "RUN pip install accelerate==0.28.0\n",
        "RUN pip install --upgrade google-cloud-storage\n",
        "\n",
        "# Copy other licenses.\n",
        "RUN wget -O MIT_LICENSE https://github.com/pytest-dev/pytest/blob/main/LICENSE\n",
        "RUN wget -O BSD_LICENSE https://github.com/pytorch/xla/blob/master/LICENSE\n",
        "RUN wget -O BSD-3_LICENSE https://github.com/pytorch/pytorch/blob/main/LICENSE\n",
        "\n",
        "# Copy install libtpu to PATH above\n",
        "RUN find ./usr/local/lib -name 'libtpu.so' -exec cp {} /lib \\;\n",
        "\n",
        "WORKDIR /\n",
        "COPY train.py train.py\n",
        "ENV PYTHONPATH ./\n",
        "\n",
        "ENTRYPOINT [\"python\", \"train.py\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ee7ac9469ce"
      },
      "source": [
        "#### Add the __init__.py file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0b57ecd02d8"
      },
      "outputs": [],
      "source": [
        "!touch trainer/__init__.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c648296e1057"
      },
      "source": [
        "#### Add the train.py file\n",
        "This code is from the LoRA distributed fine-tuning code from this example: https://ai.google.dev/gemma/docs/distributed_tuning\n",
        "\n",
        "The IMDB TensorFlow dataset is used to fine-tune the Gemma model. Additional logic is added to handle the TPU topology setting required by TPUv5e: https://cloud.google.com/tpu/docs/v5e#tpu-v5e-config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b6a47a6089c"
      },
      "outputs": [],
      "source": [
        "%%writefile trainer/train.py\n",
        "import os, sys\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "from google.cloud import storage\n",
        "\n",
        "# use spmd\n",
        "import torch_xla.runtime as xr\n",
        "xr.use_spmd()\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\n",
        "    \"--tpu_topology\",\n",
        "    help=\"Topology to use for the TPUv5e (2x2, 2x4, 4x4)\",\n",
        "    default=\"4x4\",\n",
        "    type=str\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--model_name\",\n",
        "    help=\"Llama2 model name (meta-llama/Llama-2-7b-hf, meta-llama/Llama-2-13b-hf)\",\n",
        "    default=\"meta-llama/Llama-2-7b-hf\",\n",
        "    type=str\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--bucket_name\",\n",
        "    help=\"The name of the bucket you copied the Llama2 model files to\",\n",
        "    required=True,\n",
        "    type=str\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--output_folder\",\n",
        "    type=str,\n",
        "    required=True,\n",
        "    help=\"Output folder name\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--checkpoint_directory\",\n",
        "    type=str,\n",
        "    default=\"output_ckpt\",\n",
        "    help=\"Checkpoint Directory name\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--epochs\",\n",
        "    type=int,\n",
        "    default=10,\n",
        "    help=\"Number of epochs to train\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--merged_model_folder\",\n",
        "    type=str,\n",
        "    default=\"llama2-7b-hf/modelfiles\",\n",
        "    help=\"Checkpoint Directory name\",\n",
        ")\n",
        "args = parser.parse_args()\n",
        "\n",
        "GCS_PREFIX = \"gs://\"\n",
        "\n",
        "def is_gcs_path(input_path: str) -> bool:\n",
        "    return input_path.startswith(GCS_PREFIX)\n",
        "\n",
        "def download_gcs_dir(gcs_dir: str, local_dir: str):\n",
        "    \"\"\"Download files in a GCS directory to a local directory.\n",
        "\n",
        "    For example:\n",
        "    download_gcs_dir(gs://bucket/foo, /tmp/bar)\n",
        "    gs://bucket/foo/a -> /tmp/bar/a\n",
        "    gs://bucket/foo/b/c -> /tmp/bar/b/c\n",
        "\n",
        "    Arguments:\n",
        "    gcs_dir: A string of directory path on GCS.\n",
        "    local_dir: A string of local directory path.\n",
        "    \"\"\"\n",
        "    if not is_gcs_path(gcs_dir):\n",
        "        raise ValueError(f\"{gcs_dir} is not a GCS path starting with gs://.\")\n",
        "\n",
        "    bucket_name = gcs_dir.split(\"/\")[2]\n",
        "    prefix = gcs_dir[len(GCS_PREFIX + bucket_name) :].strip(\"/\")\n",
        "    client = storage.Client()\n",
        "    blobs = client.list_blobs(bucket_name, prefix=prefix)\n",
        "    for blob in blobs:\n",
        "        if blob.name[-1] == \"/\":\n",
        "            continue\n",
        "        file_path = blob.name[len(prefix) :].strip(\"/\")\n",
        "        local_file_path = os.path.join(local_dir, file_path)\n",
        "        os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
        "        blob.download_to_filename(local_file_path)\n",
        "        print (f'download of {local_file_path} complete')\n",
        "    print (f'Show all files in directory {os.listdir(local_dir)}')\n",
        "\n",
        "def upload_directory_with_transfer_manager(bucket_name, source_directory, blob_name_prefix, workers=8):\n",
        "    \"\"\"Upload every file in a directory, including all files in subdirectories.\n",
        "\n",
        "    Each blob name is derived from the filename, not including the `directory`\n",
        "    parameter itself. For complete control of the blob name for each file (and\n",
        "    other aspects of individual blob metadata), use\n",
        "    transfer_manager.upload_many() instead.\n",
        "    \"\"\"\n",
        "\n",
        "    # bucket_name = \"your-bucket-name\"\n",
        "\n",
        "    # The directory on your computer to upload. Files in the directory and its\n",
        "    # subdirectories will be uploaded. An empty string means \"the current\n",
        "    # working directory\".\n",
        "    # source_directory=\"\"\n",
        "\n",
        "    # blob_name_prefix = prefix for the files being uploaded to GCS\n",
        "    # example: file1 and file2 in a folder uploaded to my-bucket with blob_name_prefix=my-folder/a/\n",
        "    # will be uploaded to gs://my-bucket/my-folder/a/file1 and gs://my-bucket/my-folder/a/file2\n",
        "    \n",
        "    # The maximum number of processes to use for the operation. The performance\n",
        "    # impact of this value depends on the use case, but smaller files usually\n",
        "    # benefit from a higher number of processes. Each additional process occupies\n",
        "    # some CPU and memory resources until finished. Threads can be used instead\n",
        "    # of processes by passing `worker_type=transfer_manager.THREAD`.\n",
        "    # workers=8\n",
        "\n",
        "    from pathlib import Path\n",
        "\n",
        "    from google.cloud.storage import Client, transfer_manager\n",
        "\n",
        "    storage_client = Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    # Generate a list of paths (in string form) relative to the `directory`.\n",
        "    # This can be done in a single list comprehension, but is expanded into\n",
        "    # multiple lines here for clarity.\n",
        "\n",
        "    # First, recursively get all files in `directory` as Path objects.\n",
        "    directory_as_path_obj = Path(source_directory)\n",
        "    paths = directory_as_path_obj.rglob(\"*\")\n",
        "\n",
        "    # Filter so the list only includes files, not directories themselves.\n",
        "    file_paths = [path for path in paths if path.is_file()]\n",
        "\n",
        "    # These paths are relative to the current working directory. Next, make them\n",
        "    # relative to `directory`\n",
        "    relative_paths = [path.relative_to(source_directory) for path in file_paths]\n",
        "\n",
        "    # Finally, convert them all to strings.\n",
        "    string_paths = [str(path) for path in relative_paths]\n",
        "\n",
        "    print(\"Found {} files.\".format(len(string_paths)))\n",
        "\n",
        "    # Start the upload.\n",
        "    print (f\"source directory {source_directory}\")\n",
        "    results = transfer_manager.upload_many_from_filenames(\n",
        "        bucket, string_paths, blob_name_prefix=blob_name_prefix, source_directory=source_directory, max_workers=workers\n",
        "    )\n",
        "\n",
        "    for name, result in zip(string_paths, results):\n",
        "        # The results list is either `None` or an exception for each filename in\n",
        "        # the input list, in order.\n",
        "\n",
        "        if isinstance(result, Exception):\n",
        "            print(\"Failed to upload {} due to exception: {}\".format(name, result))\n",
        "        else:\n",
        "            print(\"Uploaded {} to {}/{}.\".format(name, bucket.name, blob_name_prefix))\n",
        "    \n",
        "def main():\n",
        "    x = args.tpu_topology.split(\"x\")\n",
        "    tpu_topology_x = int(x[0])\n",
        "    tpu_topology_y = int(x[1])\n",
        "    print (f'TPU topology is ({tpu_topology_x}, {tpu_topology_y})')\n",
        "    print (f'Model name is {args.model_name}')\n",
        "    \n",
        "    # Set batch size to 8 for each chip\n",
        "    BATCH_SIZE = 8 * tpu_topology_x * tpu_topology_y\n",
        "    # For anything larger than an 8 chip instance, set the BATCH_SIZE to 128, since we run out of samples\n",
        "    if (tpu_topology_x * tpu_topology_y) >=16:\n",
        "        BATCH_SIZE = 128\n",
        "    \n",
        "    # Set download directory to a tempory folder\n",
        "    DL_DIR=\"/tmp/modelfiles\"\n",
        "    if not os.path.exists(DL_DIR):\n",
        "        os.makedirs(DL_DIR)\n",
        "\n",
        "    print ('Downloading data to temporary folder')\n",
        "    download_gcs_dir (f\"gs://{args.bucket_name}/{args.model_name}\", DL_DIR)\n",
        "    \n",
        "    # Create output folders\n",
        "    if not os.path.exists(f\"/tmp/{args.output_folder}\"):\n",
        "        os.makedirs(f\"/tmp/{args.output_folder}\")\n",
        "    if not os.path.exists(f\"/tmp/{args.checkpoint_directory}\"):\n",
        "        os.makedirs(f\"/tmp/{args.checkpoint_directory}\")\n",
        "\n",
        "    device = xm.xla_device()\n",
        "    \n",
        "    # Set tokenizer parallelism to false to avoid warnings\n",
        "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(DL_DIR)\n",
        "    print ('Loaded tokenizer')\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(DL_DIR, torch_dtype=torch.bfloat16)\n",
        "    print ('Loaded base model')\n",
        "\n",
        "    # Set LoRA configuration\n",
        "    lora_config = LoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.1,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=[\"k_proj\", \"v_proj\"],\n",
        "    )\n",
        "    \n",
        "    # Required when using Llama2, as the tokenizer has no padding\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Load the dataset and format it for training.\n",
        "    data = load_dataset(\"Abirate/english_quotes\", split=\"train\")\n",
        "    max_seq_length = 512\n",
        "    print ('Loaded dataset')\n",
        "\n",
        "    # Set up the FSDP config. To enable FSDP via SPMD, set xla_fsdp_v2 to True.\n",
        "    fsdp_config = {\"fsdp_transformer_layer_cls_to_wrap\": [\n",
        "            \"LlamaDecoderLayer\"\n",
        "        ],\n",
        "        \"xla\": True,\n",
        "        \"xla_fsdp_v2\": True,\n",
        "        \"xla_fsdp_grad_ckpt\": True}\n",
        "\n",
        "    OUTPUT_DIR=f\"/tmp/{args.output_folder}\"\n",
        "    CHECKPOINT_DIR=f\"/tmp/{args.checkpoint_directory}\"\n",
        "\n",
        "    # Finally, set up the trainer and train the model.\n",
        "    trainer = SFTTrainer(\n",
        "        model=base_model,\n",
        "        train_dataset=data,\n",
        "        args=TrainingArguments(\n",
        "            per_device_train_batch_size=BATCH_SIZE,  # This is actually the global batch size for SPMD.\n",
        "            num_train_epochs=args.epochs,\n",
        "            max_steps=-1,\n",
        "            output_dir=OUTPUT_DIR,\n",
        "            optim=\"adafactor\",\n",
        "            logging_steps=1,\n",
        "            dataloader_drop_last = True,  # Required for SPMD.\n",
        "            fsdp=\"full_shard\",\n",
        "            fsdp_config=fsdp_config,\n",
        "        ),\n",
        "        peft_config=lora_config,\n",
        "        dataset_text_field=\"quote\",\n",
        "        max_seq_length=max_seq_length,\n",
        "        packing=True,\n",
        "    )\n",
        "\n",
        "    # train\n",
        "    trainer.train()\n",
        "    \n",
        "    adapter_model_id = \"adapter_model\"\n",
        "    adapter_path = f\"{CHECKPOINT_DIR}/{adapter_model_id}\"\n",
        "    merged_model_id = \"merged_model\"\n",
        "    merged_model_path = f\"{CHECKPOINT_DIR}/{merged_model_id}\"\n",
        "    \n",
        "    trainer.model.to('cpu').save_pretrained(adapter_path)\n",
        "    \n",
        "    # Save the adapter, merged model, and tokenizer\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(DL_DIR, torch_dtype=torch.bfloat16)\n",
        "    peft_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "    merged_model = peft_model.merge_and_unload()\n",
        "    merged_model.save_pretrained(merged_model_path,safe_serialization=False)\n",
        "    tokenizer.save_pretrained(merged_model_path)\n",
        "    \n",
        "    # Copy merged files to GCS folder\n",
        "    OUTPUT_PREFIX=f\"{args.merged_model_folder}/{merged_model_id}/{xr.process_index()}/\"\n",
        "    upload_directory_with_transfer_manager(bucket_name=args.bucket_name,source_directory=merged_model_path,\n",
        "                                       blob_name_prefix=OUTPUT_PREFIX)\n",
        "    print ('Uploaded merged model files')\n",
        "\n",
        "    # copy adapter files to GCS folder\n",
        "    OUTPUT_PREFIX=f\"{args.merged_model_folder}/{adapter_model_id}/{xr.process_index()}/\"\n",
        "    upload_directory_with_transfer_manager(bucket_name=args.bucket_name,source_directory=adapter_path,\n",
        "                                       blob_name_prefix=OUTPUT_PREFIX)\n",
        "    print ('Uploaded adapter model files')\n",
        "\n",
        "    print ('Exiting job')\n",
        "    sys.exit(0)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq4iF00YG_4T"
      },
      "source": [
        "## Fine-tune with Vertex AI Custom Training Jobs\n",
        "\n",
        "This section demonstrates how to fine-tune and deploy Llama2 models with PEFT LoRA on Vertex AI Custom Training Jobs. LoRA (Low-Rank Adaptation) is one approach of PEFT (Parameter Efficient Fine-tuning), where pretrained model weights are frozen and rank decomposition matrices representing the change in model weights are trained during fine-tuning. Read more about LoRA in the following publication: [Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L. and Chen, W., 2021. Lora: Low-rank adaptation of large language models. *arXiv preprint arXiv:2106.09685*](https://arxiv.org/abs/2106.09685)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1b90914fc81"
      },
      "source": [
        "#### Enable docker to run as a regular user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "232a259d3edc"
      },
      "outputs": [],
      "source": [
        "!sudo usermod -a -G docker ${USER}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e028d107fbe5"
      },
      "source": [
        "#### Change to the trainer directory to build the docker container"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3a9390f87b66"
      },
      "outputs": [],
      "source": [
        "%cd trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81eb3c13afa9"
      },
      "source": [
        "#### Build the custom docker container and push to artifact registry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee497559677f"
      },
      "outputs": [],
      "source": [
        "!docker build -t $PYTORCH_TRAIN_DOCKER_URI -f Dockerfile ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0715b34162b4"
      },
      "outputs": [],
      "source": [
        "!docker push $PYTORCH_TRAIN_DOCKER_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "586a13cb67b4"
      },
      "source": [
        "#### Change back to your home directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "937b7269c93b"
      },
      "outputs": [],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08bf867e8f4c"
      },
      "source": [
        "#### Set GCS folder locations and job configurations settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17392b36e9c0"
      },
      "outputs": [],
      "source": [
        "# Create a GCS folder to store the merged model with the base model and the\n",
        "# fine-tuned LORA adapter.\n",
        "BUCKET_NAME = BUCKET_URI.replace(\"gs://\", \"\")\n",
        "OUTPUT_DIR_NAME = \"output\"\n",
        "CHECKPOINT_DIR_NAME = \"output_chk\"\n",
        "NUM_EPOCHS = 200\n",
        "MERGED_MODEL_FOLDER = \"llama2-7b-hf/modelfiles\"\n",
        "\n",
        "# See machines type to match chips being used\n",
        "# Topologies of 2x2, 2x4, 4x4 = 4, 8, 16 chip settings and use quota from aiplatform.googleapis.com/custom_model_training_tpu_v5e\n",
        "MACHINE_TYPE = \"ct5lp-hightpu-4t\"\n",
        "TPU_TOPOLOGY = \"4x4\"\n",
        "\n",
        "DISPLAY_NAME_PREFIX = f\"llama2-7b-lora-train-{TPU_TOPOLOGY}\"\n",
        "tpuv5e_llama2_peft_job = {\n",
        "    \"display_name\": get_job_name_with_datetime(DISPLAY_NAME_PREFIX),\n",
        "    \"job_spec\": {\n",
        "        \"worker_pool_specs\": [\n",
        "            {\n",
        "                \"machine_spec\": {\n",
        "                    \"machine_type\": MACHINE_TYPE,\n",
        "                    \"tpu_topology\": TPU_TOPOLOGY,\n",
        "                },\n",
        "                \"replica_count\": 1,\n",
        "                \"container_spec\": {\n",
        "                    \"image_uri\": PYTORCH_TRAIN_DOCKER_URI,\n",
        "                    \"args\": [\n",
        "                        f\"--tpu_topology={TPU_TOPOLOGY}\",\n",
        "                        f\"--model_name={HF_MODEL_ID}\",\n",
        "                        f\"--bucket_name={BUCKET_NAME}\",\n",
        "                        f\"--output_folder={OUTPUT_DIR_NAME}\",\n",
        "                        f\"--checkpoint_directory={CHECKPOINT_DIR_NAME}\",\n",
        "                        f\"--epochs={NUM_EPOCHS}\",\n",
        "                        f\"--merged_model_folder={MERGED_MODEL_FOLDER}\",\n",
        "                    ],\n",
        "                },\n",
        "            },\n",
        "        ],\n",
        "    },\n",
        "}\n",
        "\n",
        "tpuv5e_llama2_peft_job"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c2bc267c4a5"
      },
      "source": [
        "#### Create job client and run job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc9b403c8515"
      },
      "outputs": [],
      "source": [
        "job_client = aiplatform.gapic.JobServiceClient(\n",
        "    client_options=dict(api_endpoint=f\"{REGION}-aiplatform.googleapis.com\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4da63b786392"
      },
      "outputs": [],
      "source": [
        "create_tpuv5e_llama2_peft_job_response = job_client.create_custom_job(\n",
        "    parent=\"projects/{project}/locations/{location}\".format(\n",
        "        project=PROJECT_ID, location=REGION\n",
        "    ),\n",
        "    custom_job=tpuv5e_llama2_peft_job,\n",
        ")\n",
        "print(create_tpuv5e_llama2_peft_job_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb8dfcc23789"
      },
      "source": [
        "#### Check on job progress\n",
        "This may take 20-60 minutes or more depending on the model size. Run this cell multiple times to check progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f402309d9dbb"
      },
      "outputs": [],
      "source": [
        "get_tpuv5e_llama2_peft_job_response = job_client.get_custom_job(\n",
        "    name=create_tpuv5e_llama2_peft_job_response.name\n",
        ")\n",
        "get_tpuv5e_llama2_peft_job_response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb7f4f1ac160"
      },
      "source": [
        "#### Click on the console log url output from this cell to see your logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "babf40cf7821"
      },
      "outputs": [],
      "source": [
        "job_id = create_tpuv5e_llama2_peft_job_response.name[\n",
        "    create_tpuv5e_llama2_peft_job_response.name.rfind(\"/\") + 1 :\n",
        "]\n",
        "STARTDATE = datetime.today() - timedelta(days=1)\n",
        "STARTDATE = STARTDATE.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
        "ENDDATE = datetime.today() + timedelta(days=0.1)\n",
        "ENDDATE = ENDDATE.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
        "print(\n",
        "    f\"https://console.cloud.google.com/logs/query;query=resource.labels.job_id=%22{job_id}%22;cursorTimestamp={ENDDATE}Z;startTime={STARTDATE}Z;endTime={ENDDATE}Z?project={PROJECT_ID}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f97d1b4fb05"
      },
      "source": [
        "#### Wait until the training job is complete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6ecd909fea8"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "from google.cloud.aiplatform import gapic as aip\n",
        "\n",
        "while True:\n",
        "    response = job_client.get_custom_job(\n",
        "        name=create_tpuv5e_llama2_peft_job_response.name\n",
        "    )\n",
        "    if response.state != aip.JobState.JOB_STATE_SUCCEEDED:\n",
        "        print(f\"Training is not complete and is in state {response.state.name}\")\n",
        "        if response.state == aip.JobState.JOB_STATE_FAILED:\n",
        "            raise Exception(\"Training Job Failed\")\n",
        "    else:\n",
        "        print(\"Training has completed\")\n",
        "        break\n",
        "    time.sleep(60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d52a6bb72aa6"
      },
      "source": [
        "### Deploy fine tuned models\n",
        "This section uploads the model to Model Registry and deploys the model using Hex-LLM, a High-Efficiency Large Language Model serving solution built with XLA that is being developed by Google Cloud\n",
        "\n",
        "The model deployment step will take 15-20 minutes to complete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1936eee4a7b"
      },
      "outputs": [],
      "source": [
        "HEXLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai-restricted/vertex-vision-model-garden-dockers/hex-llm-serve:20240328_RC01\"\n",
        "\n",
        "# GCS folder path where the merged model files were saved in you bucket\n",
        "# MERGED_MODEL_FOLDER=\"llama2-7b-hf/modelfiles\" set during fine-tuning\n",
        "MERGED_MODEL_PATH = f\"{MERGED_MODEL_FOLDER}/merged_model/0\"\n",
        "GCS_MODEL_PATH = f\"{BUCKET_URI}/{MERGED_MODEL_PATH}\"\n",
        "\n",
        "DISPLAY_NAME_PREFIX = \"llama2-7b-lora-deploy\"  # @param {type:\"string\"}\n",
        "JOB_NAME = get_job_name_with_datetime(DISPLAY_NAME_PREFIX)\n",
        "GCS_MODEL_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c963a9dcf4ac"
      },
      "source": [
        "#### Check the model files in your GCS directory\n",
        "\n",
        "Your output should show a list of files like this\n",
        "```\n",
        "gs://<YOUR-BUCKET>/modelfiles/merged_model/config.json\n",
        "gs://<YOUR-BUCKET>/modelfiles/merged_model/generation_config.json\n",
        "gs://<YOUR-BUCKET>/modelfiles/merged_model/pytorch_model-00001-of-00003.bin\n",
        "gs://<YOUR-BUCKET>/modelfiles/merged_model/pytorch_model-00002-of-00003.bin\n",
        "gs://<YOUR-BUCKET>/modelfiles/merged_model/pytorch_model-00003-of-00003.bin\n",
        "gs://<YOUR-BUCKET>/modelfiles/merged_model/pytorch_model.bin.index.json\n",
        "gs://<YOUR-BUCKET>/modelfiles/merged_model/special_tokens_map.json\n",
        "gs://<YOUR-BUCKET>/modelfiles/merged_model/tokenizer.json\n",
        "gs://<YOUR-BUCKET>/modelfiles/merged_model/tokenizer_config.json\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9159d64417a0"
      },
      "outputs": [],
      "source": [
        "!gsutil ls $GCS_MODEL_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0188453d4c9f"
      },
      "source": [
        "#### Define function for deploying model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0931f14c09cb"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "\n",
        "def deploy_model_hexllm(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    service_account: str,\n",
        "    machine_type: str = \"ct5lp-hightpu-4t\",\n",
        "    max_num_batched_tokens: int = 11264,  # 11264\n",
        "    tokens_pad_multiple: int = 1024,\n",
        "    seqs_pad_multiple: int = 32,\n",
        "    sync: bool = True,\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys models with Hex-LLM on TPU in Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
        "\n",
        "    num_tpu_chips = int(machine_type[-2])\n",
        "    hexllm_args = [\n",
        "        \"--host=0.0.0.0\",\n",
        "        \"--port=7080\",\n",
        "        \"--log_level=INFO\",\n",
        "        f\"--model={model_id}\",\n",
        "        \"--load_format=pt\",  # Note: Using Pytorch bin format for weights\n",
        "        f\"--tensor_parallel_size={num_tpu_chips}\",\n",
        "        \"--num_nodes=1\",\n",
        "        \"--use_ray\",\n",
        "        \"--batch_mode=continuous\",\n",
        "        f\"--max_num_batched_tokens={max_num_batched_tokens}\",\n",
        "        f\"--tokens_pad_multiple={tokens_pad_multiple}\",\n",
        "        f\"--seqs_pad_multiple={seqs_pad_multiple}\",\n",
        "    ]\n",
        "\n",
        "    env_vars = {\n",
        "        \"PJRT_DEVICE\": \"TPU\",\n",
        "        \"RAY_DEDUP_LOGS\": \"0\",\n",
        "        \"RAY_USAGE_STATS_ENABLED\": \"0\",\n",
        "    }\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=HEXLLM_DOCKER_URI,\n",
        "        serving_container_command=[\"python\", \"-m\", \"hex_llm.entrypoints.api_server\"],\n",
        "        serving_container_args=hexllm_args,\n",
        "        serving_container_ports=[7080],\n",
        "        serving_container_predict_route=\"/generate\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "        serving_container_environment_variables=env_vars,\n",
        "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
        "        serving_container_deployment_timeout=7200,\n",
        "    )\n",
        "\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=service_account,\n",
        "        sync=sync,\n",
        "    )\n",
        "    return model, endpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8945e2e6d6ed"
      },
      "source": [
        "#### Deploy model to Vertex\n",
        "The `deploy_model_hexllm` function will return a reference to the model added to the Vertex AI Model Registry as well as a new endpoint where the model will be deployed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecd851116c14"
      },
      "outputs": [],
      "source": [
        "print(\"Using model from: \", GCS_MODEL_PATH)\n",
        "model, endpoint = deploy_model_hexllm(\n",
        "    model_name=JOB_NAME,\n",
        "    model_id=GCS_MODEL_PATH,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    sync=False,\n",
        ")\n",
        "print(\"endpoint_name:\", endpoint.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81650c5e3444"
      },
      "source": [
        "#### Review the logs after the model has been deployed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e31488fc924"
      },
      "outputs": [],
      "source": [
        "ENDPOINT_ID = endpoint.name[endpoint.name.rfind(\"/\") + 1 :]\n",
        "STARTDATE = datetime.today() - timedelta(days=1)\n",
        "STARTDATE = STARTDATE.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
        "ENDDATE = datetime.today() + timedelta(days=0.1)\n",
        "ENDDATE = ENDDATE.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
        "print(\n",
        "    f\"https://console.cloud.google.com/logs/query;query=resource.type%3D%22aiplatform.googleapis.com%2FEndpoint%22%20resource.labels.endpoint_id%3D%22{ENDPOINT_ID}%22%20resource.labels.location%3D%22{REGION}%22;startTime={STARTDATE}Z;endTime={ENDDATE}Z?project={PROJECT_ID}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adb2ed90a241"
      },
      "source": [
        "#### Wait until endpoint is complete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c92f6cdd08d"
      },
      "outputs": [],
      "source": [
        "endpoint.wait()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d894b10257a"
      },
      "outputs": [],
      "source": [
        "# (optional) Wait 15 minutes while the model is downloaded and setup\n",
        "if os.getenv(\"IS_TESTING\"):\n",
        "    time.sleep(900)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "221a5b47cd7d"
      },
      "source": [
        "NOTE: The overall deployment can take 30-40 minutes or more. After the deployment succeeds (15-20 minutes or so), the fine-tuned model will be downloaded from the GCS bucket used in training above. Thus, an additional ~15-20 minutes (depending on the model sizes) of waiting time is needed **after** the model deployment step above succeeds and before you run the next step below. Otherwise you might see a `ServiceUnavailable: 503 502:Bad Gateway` error when you send requests to the endpoint.\n",
        "\n",
        "### Once deployment is ready, send a prediction request\n",
        "\n",
        "Once deployment succeeds, you can send requests to the endpoint with text prompts. The first request will take a minute or two while model warmup occurs\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "Prompt: Provide a list of the 3 best comedy movies in the 90s in 50 characters or less\n",
        "Response:  1) The Cable Guy 2) Scooby-Doo 3) Beethoven Requirements\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a85ec25e11f"
      },
      "outputs": [],
      "source": [
        "PROMPT = (\n",
        "    \"Provide a list of the 3 best comedy movies in the 90s in 50 characters or less\"\n",
        ")\n",
        "\n",
        "instances = [\n",
        "    {\n",
        "        \"prompt\": PROMPT,\n",
        "        \"max_tokens\": 80,\n",
        "        \"temperature\": 1.0,\n",
        "        \"top_p\": 1.0,\n",
        "        \"top_k\": 1.0,\n",
        "    },\n",
        "]\n",
        "\n",
        "response = endpoint.predict(instances=instances)\n",
        "\n",
        "for prediction in response.predictions:\n",
        "    print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af21a3cff1e0"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "911406c1561e"
      },
      "outputs": [],
      "source": [
        "# Delete the train job.\n",
        "job_client.delete_custom_job(name=create_tpuv5e_llama2_peft_job_response.name)\n",
        "\n",
        "# Undeploy model and delete endpoint.\n",
        "endpoint.delete(force=True)\n",
        "\n",
        "# Delete models.\n",
        "model.delete()\n",
        "\n",
        "import os\n",
        "\n",
        "# Delete Cloud Storage objects that were created\n",
        "delete_bucket = False\n",
        "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
        "    ! gsutil -m rm -r $BUCKET_URI"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "tpuv5e_llama2_pytorch_finetuning_and_serving.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

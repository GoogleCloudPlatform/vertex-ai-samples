{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fPc-KWUi2Xd"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoXf8TfQoVth"
      },
      "source": [
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/training/xgboost_data_parallel_training_on_cpu_using_dask.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/training/xgboost_data_parallel_training_on_cpu_using_dask.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/training/xgboost_data_parallel_training_on_cpu_using_dask.ipynb\">\n",
        "        <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AksIKBzZ-nre"
      },
      "source": [
        "# Create a distributed custom training job\n",
        "## Overview\n",
        "\n",
        "This tutorial shows you how to create a distributed custom training job on Vertex AI that can handle large amounts of training data. \n",
        "\n",
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to create a distributed training job using Vertex AI SDK for Python. You build a custom docker container with simple Dask configuration to run a custom training job.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services:\n",
        "\n",
        "- `Vertex AI SDK`\n",
        "- `CustomContainerTrainingJob`\n",
        "- `Artifact Registry`\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Configure the `PROJECT_ID` and `REGION` variables for your Google Cloud project.\n",
        "- Create a Cloud Storage bucket to store your model artifacts.\n",
        "- Build a custom Docker container that hosts your training code and push the container image to Artifact Registry.\n",
        "- Run a Vertex AI SDK CustomContainerTrainingJob\n",
        "\n",
        "\n",
        "### Data \n",
        "\n",
        "This tutorial uses the <a href=\"https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html\">IRIS dataset</a>, which consists of different types of irises. \n",
        "\n",
        "### Costs\n",
        " \n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "\n",
        "* Cloud Storage\n",
        "\n",
        "* Artifact Registry\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing) and [Artifact Registry](https://cloud.google.com/artifact-registry/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/),\n",
        "        to generate a cost estimate based on your projected usage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMHz63rPbq6P"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Install the packages required for executing this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6f3dc43494b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Vertex AI Workbench Notebook product has specific requirements\n",
        "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
        "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
        "    \"/opt/deeplearning/metadata/env_version\"\n",
        ")\n",
        "\n",
        "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_WORKBENCH_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\"\n",
        "\n",
        "! pip3 install --upgrade google-cloud-aiplatform {USER_FLAG} -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64d24b4fab2c"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8AIwN0abq6U"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Restart the kernel after pip installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "before_you_begin:nogpu"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "2. [Make sure that billing is enabled for your project.](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
        "\n",
        "3. [Enable the Vertex AI APIs, Compute Engine APIs, and Cloud Storage.](https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,compute_component,storage-component.googleapis.com)\n",
        "\n",
        "4. [The Google Cloud SDK](https://cloud.google.com/sdk) is already installed in Google Cloud Notebook.\n",
        "\n",
        "5. Enter your project ID in the cell below. Then run the  cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_project_id"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_project_id"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID:\", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_gcloud_project_id"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
        "\n",
        "- Americas: `us-central1`\n",
        "- Europe: `europe-west4`\n",
        "- Asia Pacific: `asia-east1`\n",
        "\n",
        "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
        "\n",
        "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04933ed28eef"
      },
      "outputs": [],
      "source": [
        "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
        "\n",
        "if REGION == \"[your-region]\":\n",
        "    REGION = \"us-central1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtMai39NpNtI"
      },
      "source": [
        "#### UUID\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a uuid for each instance session, and append it onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OX0_fa3TpOog"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "\n",
        "# Generate a uuid of a specifed length(default=8)\n",
        "def generate_uuid(length: int = 8) -> str:\n",
        "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
        "\n",
        "\n",
        "UUID = generate_uuid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0SMyUsC-mzi"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using Vertex AI Workbench Notebook**, your environment is already authenticated. Skip this step.\n",
        "\n",
        "**If you are using Colab**, run the cell below and follow the instructions\n",
        "when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "1. In the Cloud Console, go to the [**Create service account key**\n",
        "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
        "\n",
        "2. Click **Create service account**.\n",
        "\n",
        "3. In the **Service account name** field, enter a name, and\n",
        "   click **Create**.\n",
        "\n",
        "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
        "into the filter box, and select\n",
        "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "\n",
        "5. Click *Create*. A JSON file that contains your key downloads to your\n",
        "local environment.\n",
        "\n",
        "6. Enter the path to your service account key as the\n",
        "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTQY9g4mRo6r"
      },
      "outputs": [],
      "source": [
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# If on Vertex AI Workbench, then don't execute this code\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
        "    \"DL_ANACONDA_HOME\"\n",
        "):\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0McDhrTpt-h"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "When you initialize the Vertex AI SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqdXNlFQqr-u"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
        "\n",
        "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
        "    BUCKET_NAME = PROJECT_ID + \"aip-\" + UUID\n",
        "    BUCKET_URI = f\"gs://{BUCKET_NAME}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxJaSKy7qymT"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8khtdIkVq0Ra"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PKfdM-Yq1vT"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StOSeEKaq4-7"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dax2zrpTi2Xy"
      },
      "source": [
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjeuNztBrLmr"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx_z9JQlrNwG"
      },
      "source": [
        "# Create a custom training Python package \n",
        "\n",
        "Before you can perform local training, you must a create a training script file and a docker file.\n",
        "\n",
        "Create a `trainer` directory for all of your training code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iYddwztr3g7"
      },
      "outputs": [],
      "source": [
        "PYTHON_PACKAGE_APPLICATION_DIR = \"trainer\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjeHKqHwr4rV"
      },
      "outputs": [],
      "source": [
        "!mkdir -p $PYTHON_PACKAGE_APPLICATION_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECBb7Lbqr9Hs"
      },
      "source": [
        "### Write the Training Script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EStvMiC9tdS"
      },
      "source": [
        "The `train.py` file checks whether the current node is the chief node or a worker node and runs `dask-scheduler` for the chief node and `dask-worker` for worker nodes. Worker nodes connect to the chief node through the IP address and port number specified in `CLUSTER_SPEC`.\n",
        "\n",
        "After the Dask scheduler is set up and connected to worker nodes, call `xgb.dask.train` to train a model through Dask. Once model training is complete, the model is uploaded to `AIP_MODEL_DIR`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thNtAY2Gsx2h"
      },
      "outputs": [],
      "source": [
        "%%writefile trainer/train.py\n",
        "from dask.distributed import Client, wait\n",
        "from xgboost.dask import DaskDMatrix\n",
        "from google.cloud import storage\n",
        "import xgboost as xgb\n",
        "import dask.dataframe as dd\n",
        "import sys\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "import json\n",
        "\n",
        "IRIS_DATA_FILENAME = 'gs://cloud-samples-data/ai-platform/iris/iris_data.csv'\n",
        "IRIS_TARGET_FILENAME = 'gs://cloud-samples-data/ai-platform/iris/iris_target.csv'\n",
        "MODEL_FILE = 'model.bst'\n",
        "MODEL_DIR = os.getenv(\"AIP_MODEL_DIR\")\n",
        "XGB_PARAMS = {\n",
        "    'verbosity': 2,\n",
        "    'learning_rate': 0.1,\n",
        "    'max_depth': 8,\n",
        "    'objective': 'reg:squarederror',\n",
        "    'subsample': 0.6,\n",
        "    'gamma': 1,\n",
        "    'verbose_eval': True,\n",
        "    'tree_method': 'hist',\n",
        "    'nthread': 1\n",
        "}\n",
        "\n",
        "\n",
        "def square(x):\n",
        "    return x ** 2\n",
        "\n",
        "def neg(x):\n",
        "    return -x\n",
        "\n",
        "def launch(cmd):\n",
        "    \"\"\" launch dask workers\n",
        "    \"\"\"\n",
        "    return subprocess.check_call(cmd, stdout=sys.stdout, stderr=sys.stderr, shell=True)\n",
        "\n",
        "def get_chief_ip(cluster_config_dict):\n",
        "    ip_address = cluster_config_dict['cluster']['workerpool0'][0].split(\":\")[0]\n",
        "    print('The ip address of workerpool 0 is : {}'.format(ip_address))\n",
        "    return ip_address\n",
        "\n",
        "def get_chief_port(cluster_config_dict):\n",
        "    print(\"The open port is: {}\".format(cluster_config_dict['open_ports'][0]))\n",
        "    return cluster_config_dict['open_ports'][0]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    cluster_config_str = os.environ.get('CLUSTER_SPEC')\n",
        "    cluster_config_dict  = json.loads(cluster_config_str)\n",
        "    print(json.dumps(cluster_config_dict, indent=2))\n",
        "    print('The workerpool type is:', flush=True)\n",
        "    print(cluster_config_dict['task']['type'], flush=True)\n",
        "    workerpool_type = cluster_config_dict['task']['type']\n",
        "    chief_ip = get_chief_ip(cluster_config_dict)\n",
        "    chief_port = get_chief_port(cluster_config_dict)\n",
        "    chief_address = \"{}:{}\".format(chief_ip, chief_port)\n",
        "\n",
        "    if workerpool_type == \"workerpool0\":\n",
        "      print('Running the dask scheduler.', flush=True)\n",
        "      proc_scheduler = launch('dask-scheduler --dashboard --dashboard-address 8888 --port {} &'.format(chief_port))\n",
        "      print('Done the dask scheduler.', flush=True)\n",
        "\n",
        "      client = Client(chief_address)\n",
        "      print('Waiting the scheduler to be connected.', flush=True)\n",
        "      client.wait_for_workers(1)\n",
        "\n",
        "      X = dd.read_csv(IRIS_DATA_FILENAME, header=None)\n",
        "      y = dd.read_csv(IRIS_TARGET_FILENAME, header=None)\n",
        "      X.persist()\n",
        "      y.persist()\n",
        "      wait(X)\n",
        "      wait(y)\n",
        "      dtrain = DaskDMatrix(client, X, y)\n",
        "      \n",
        "      output = xgb.dask.train(client, XGB_PARAMS, dtrain,  num_boost_round=100, evals=[(dtrain, 'train')])\n",
        "      print(\"Output: {}\".format(output), flush=True)\n",
        "      print(\"Saving file to: {}\".format(MODEL_FILE), flush=True)\n",
        "      output['booster'].save_model(MODEL_FILE)\n",
        "      bucket_name = MODEL_DIR.replace(\"gs://\", \"\").split(\"/\", 1)[0]\n",
        "      folder = MODEL_DIR.replace(\"gs://\", \"\").split(\"/\", 1)[1]\n",
        "      bucket = storage.Client().bucket(bucket_name)\n",
        "      print(\"Uploading file to: {}/{}{}\".format(bucket_name, folder, MODEL_FILE), flush=True)\n",
        "      blob = bucket.blob('{}{}'.format(folder, MODEL_FILE))\n",
        "      blob.upload_from_filename(MODEL_FILE)\n",
        "      print(\"Saved file to: {}/{}\".format(MODEL_DIR, MODEL_FILE), flush=True)\n",
        "\n",
        "      client.shutdown()\n",
        "\n",
        "    else:\n",
        "      print('Running the dask worker.', flush=True)\n",
        "      client = Client(chief_address, timeout=1200)\n",
        "      print('client: {}.'.format(client), flush=True)\n",
        "      launch('dask-worker {}'.format(chief_address))\n",
        "      print('Done with the dask worker.', flush=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxsT4Vaos2W5"
      },
      "source": [
        "### Write the docker file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xD60d6Q0i2X0"
      },
      "outputs": [],
      "source": [
        "%%writefile Dockerfile\n",
        "FROM us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-9:latest\n",
        "WORKDIR /root\n",
        "\n",
        "RUN apt-get update\n",
        "RUN apt-get install -y telnet netcat iputils-ping  net-tools\n",
        "RUN python3.8 -m pip install dask==2022.7.1 distributed==2022.7.1 bokeh==2.1.1 dask-cuda  --upgrade\n",
        "RUN python3.8 -m pip install 'xgboost>=1.4.2' 'dask-ml[complete]==2022.5.27'  #'dask[complete]==2022.7,1' --upgrade\n",
        "RUN python3.8 -m pip install gcsfs --upgrade\n",
        "\n",
        "\n",
        "## Make sure gsutil will use the default service account\n",
        "RUN echo '[GoogleCompute]\\nservice_account = default' > /etc/boto.cfg\n",
        "\n",
        "# Copies the trainer code\n",
        "RUN mkdir /root/trainer\n",
        "COPY trainer/train.py /root/trainer/train.py\n",
        "\n",
        "# Sets up the entry point to invoke the trainer.\n",
        "ENTRYPOINT [\"python3.8\", \"trainer/train.py\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6Yj8pZWAD7c"
      },
      "source": [
        "## Create a custom training job\n",
        "\n",
        "### Build a custom training container\n",
        "\n",
        "#### Enable Artifact Registry API\n",
        "You must enable the Artifact Registry API for your project. You will store your custom training container in Artifact Registry.\n",
        "\n",
        "<a href=\"https://cloud.google.com/artifact-registry/docs/enable-service\">Learn more about Enabling service</a>.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hd1j9BHeA81h"
      },
      "outputs": [],
      "source": [
        "! gcloud services enable artifactregistry.googleapis.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMUUSWBgA_Mb"
      },
      "source": [
        "### Create a private Docker repository\n",
        "Your first step is to create a Docker repository in Artifact Registry.\n",
        "\n",
        "1 - Run the `gcloud artifacts repositories create` command to create a new Docker repository with your region with the description `docker repository`.\n",
        "\n",
        "2 - Run the `gcloud artifacts repositories list` command to verify that your repository was created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_csN1pAH95F"
      },
      "outputs": [],
      "source": [
        "PRIVATE_REPO = \"my-docker-repo\"\n",
        "\n",
        "! gcloud artifacts repositories create {PRIVATE_REPO} --repository-format=docker --location={REGION} --description=\"Docker repository\"\n",
        "\n",
        "! gcloud artifacts repositories list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPpGuKi-BOAD"
      },
      "outputs": [],
      "source": [
        "DEPLOY_IMAGE = (\n",
        "    f\"{REGION}-docker.pkg.dev/\" + PROJECT_ID + f\"/{PRIVATE_REPO}\" + \"/dask_support\"\n",
        ")\n",
        "print(\"Deployment:\", DEPLOY_IMAGE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RgHDDL8BWgz"
      },
      "source": [
        "## Authenticate Docker to your repository\n",
        "### Configure authentication to your private repo\n",
        "Before you can push or pull container images to or from your Artifact Registry repository, you must configure Docker to use the gcloud command-line tool to authenticate requests to Artifact Registry for your region. On Colab, you'll have to use Cloud Build as the docker command is not available,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRLMyQwdKiLr"
      },
      "outputs": [],
      "source": [
        "if not IS_COLAB:\n",
        "    ! gcloud auth configure-docker {REGION}-docker.pkg.dev --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XW4EecX1Bj8j"
      },
      "source": [
        "### Set the custom Docker container image\n",
        "Set the custom Docker container image.\n",
        "\n",
        "1. Pull the corresponding CPU or GPU Docker image from Docker Hub.\n",
        "2. Create a tag for registering the image with Artifact Registry\n",
        "3. Register the image with Artifact Registry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2XTaEDHB9HK"
      },
      "outputs": [],
      "source": [
        "if not IS_COLAB:\n",
        "    ! docker build -t $DEPLOY_IMAGE -f Dockerfile .\n",
        "    ! docker push $DEPLOY_IMAGE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rzI1ZXQCA30"
      },
      "source": [
        "## Build and push the custom docker container image by using Cloud Build\n",
        "\n",
        "Build and push a Docker image with Cloud Build"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJMhP0kYCEgY"
      },
      "outputs": [],
      "source": [
        "if IS_COLAB:\n",
        "    !  gcloud builds submit --timeout=1800s --region={REGION} --tag $DEPLOY_IMAGE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtUycdZhCJvQ"
      },
      "source": [
        "### Initialize Vertex AI SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKONAbwtCMgJ"
      },
      "outputs": [],
      "source": [
        "aiplatform.init(\n",
        "    project=PROJECT_ID,\n",
        "    staging_bucket=BUCKET_URI,\n",
        "    location=REGION,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GB2j39BCXiy"
      },
      "source": [
        "### Run a Vertex AI SDK CustomContainerTrainingJob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoEkXaaDepfo"
      },
      "outputs": [],
      "source": [
        "gcs_output_uri_prefix = f\"{BUCKET_URI}/output\"\n",
        "replica_count = 2\n",
        "machine_type = \"n1-standard-4\"\n",
        "display_name = \"test_display_name\"\n",
        "\n",
        "custom_container_training_job = aiplatform.CustomContainerTrainingJob(\n",
        "    display_name=display_name,\n",
        "    model_serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest\",\n",
        "    container_uri=DEPLOY_IMAGE,\n",
        ")\n",
        "\n",
        "custom_container_training_job.run(\n",
        "    base_output_dir=gcs_output_uri_prefix,\n",
        "    replica_count=replica_count,\n",
        "    machine_type=machine_type,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpLMkJAzDTgx"
      },
      "outputs": [],
      "source": [
        "print(f\"Custom Training Job Name: {custom_container_training_job.resource_name}\")\n",
        "print(f\"GCS Output URI Prefix: {gcs_output_uri_prefix}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlsCCIFgDcGy"
      },
      "source": [
        "### View training output artifact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjshJ2dcDdkQ"
      },
      "outputs": [],
      "source": [
        "! gsutil ls $gcs_output_uri_prefix/model/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c26XO4bZDnDH"
      },
      "source": [
        "# Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:\n",
        "\n",
        "- Cloud Storage Bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ue3SfrMODunu"
      },
      "outputs": [],
      "source": [
        "# Set this to true only if you'd like to delete your bucket\n",
        "delete_bucket = False\n",
        "\n",
        "! gsutil rm -rf $gcs_output_uri_prefix\n",
        "\n",
        "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
        "    ! gsutil rm -r $BUCKET_URI"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "xgboost_data_parallel_training_on_cpu_using_dask.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fPc-KWUi2Xd"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoXf8TfQoVth"
      },
      "source": [
        "# Distributed XGBoost training with Dask\n",
        "\n",
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/training/xgboost_data_parallel_training_on_cpu_using_dask.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/training/xgboost_data_parallel_training_on_cpu_using_dask.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/training/xgboost_data_parallel_training_on_cpu_using_dask.ipynb\">\n",
        "        <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54c20a90a87c"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial shows you how to create a distributed custom training job using XGBoost with Dask on Vertex AI that can handle large amounts of training data.\n",
        "\n",
        "Learn more about [Custom training](https://cloud.google.com/vertex-ai/docs/training/custom-training)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AksIKBzZ-nre"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to create a distributed training job using XGBoost with Dask. You build a custom docker container with simple Dask configuration to run a custom training job. When your training job is running, you can access the Dask dashboard to monitor the real-time status of your cluster, resources, and computations.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services:\n",
        "\n",
        "- `Vertex AI Training`\n",
        "- `Artifact Registry`\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Configure the `PROJECT_ID` and `REGION` variables for your Google Cloud project.\n",
        "- Create a Cloud Storage bucket to store your model artifacts.\n",
        "- Build a custom Docker container that hosts your training code and push the container image to Artifact Registry.\n",
        "- Run a Vertex AI SDK CustomContainerTrainingJob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6fca200d52f"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "This tutorial uses the <a href=\"https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html\">IRIS dataset</a>, which predicts the iris species.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a110a9c9923b"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "\n",
        "* Cloud Storage\n",
        "\n",
        "* Artifact Registry\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing) and [Artifact Registry](https://cloud.google.com/artifact-registry/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/),\n",
        "        to generate a cost estimate based on your projected usage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMHz63rPbq6P"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Install the packages required for executing this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6f3dc43494b"
      },
      "outputs": [],
      "source": [
        "! pip3 install --upgrade --quiet google-cloud-aiplatform "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58707a750154"
      },
      "source": [
        "### Colab only: Uncomment the following cell to restart the kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f200f10a1da3"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "# import IPython\n",
        "\n",
        "# app = IPython.Application.instance()\n",
        "# app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WReHDGG5g0XY"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, try the following:\n",
        "* Run `gcloud config list`.\n",
        "* Run `gcloud projects list`.\n",
        "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM1iC_MfAts1"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Set the region\n",
        "\n",
        "**Optional**: Update the 'REGION' variable to specify the region that you want to use. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsN5NJKSu-GU"
      },
      "outputs": [],
      "source": [
        "REGION = \"us-central1\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtMai39NpNtI"
      },
      "source": [
        "#### UUID\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a uuid for each instance session, and append it onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OX0_fa3TpOog"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "\n",
        "# Generate a uuid of a specifed length(default=8)\n",
        "def generate_uuid(length: int = 8) -> str:\n",
        "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
        "\n",
        "\n",
        "UUID = generate_uuid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBCra4QMA2wR"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "To authenticate your Google Cloud account, follow the instructions for your Jupyter environment:\n",
        "\n",
        "**1. Vertex AI Workbench**\n",
        "<br>You are already authenticated.\n",
        "\n",
        "**2. Local JupyterLab instance**\n",
        "<br>Uncomment and run the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "254614fa0c46"
      },
      "outputs": [],
      "source": [
        "# ! gcloud auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef21552ccea8"
      },
      "source": [
        "**3. Colab**\n",
        "<br>Uncomment and run the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "603adbbf0532"
      },
      "outputs": [],
      "source": [
        "# from google.colab import auth\n",
        "\n",
        "# auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c13224697bfb"
      },
      "source": [
        "**4. Service account or other**\n",
        "* See how to grant Cloud Storage permissions to your service account at https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgPO1eR3CYjk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "Create a storage bucket to store intermediate artifacts such as datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzGDU7TWdts_"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_bucket"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIq7R4HZCfIc"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dax2zrpTi2Xy"
      },
      "source": [
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjeuNztBrLmr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from google.cloud import aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx_z9JQlrNwG"
      },
      "source": [
        "# Create a custom training Python package\n",
        "\n",
        "Before you can perform local training, you must a create a training script file and a docker file.\n",
        "\n",
        "Create a `trainer` directory for all of your training code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iYddwztr3g7"
      },
      "outputs": [],
      "source": [
        "PYTHON_PACKAGE_APPLICATION_DIR = \"trainer\"\n",
        "!mkdir -p $PYTHON_PACKAGE_APPLICATION_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECBb7Lbqr9Hs"
      },
      "source": [
        "### Write the Training Script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EStvMiC9tdS"
      },
      "source": [
        "The `train.py` file checks whether the current node is the chief node or a worker node and runs `dask-scheduler` for the chief node and `dask-worker` for worker nodes. Worker nodes connect to the chief node through the IP address and port number specified in `CLUSTER_SPEC`.\n",
        "\n",
        "After the Dask scheduler is set up and connected to worker nodes, call `xgb.dask.train` to train a model through Dask. Once model training is complete, the model is uploaded to `AIP_MODEL_DIR`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thNtAY2Gsx2h"
      },
      "outputs": [],
      "source": [
        "%%writefile trainer/train.py\n",
        "from dask.distributed import Client, wait\n",
        "from xgboost.dask import DaskDMatrix\n",
        "from google.cloud import storage\n",
        "import xgboost as xgb\n",
        "import dask.dataframe as dd\n",
        "import sys\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "import json\n",
        "\n",
        "IRIS_DATA_FILENAME = 'gs://cloud-samples-data/ai-platform/iris/iris_data.csv'\n",
        "IRIS_TARGET_FILENAME = 'gs://cloud-samples-data/ai-platform/iris/iris_target.csv'\n",
        "MODEL_FILE = 'model.bst'\n",
        "MODEL_DIR = os.getenv(\"AIP_MODEL_DIR\")\n",
        "XGB_PARAMS = {\n",
        "    'verbosity': 2,\n",
        "    'learning_rate': 0.1,\n",
        "    'max_depth': 8,\n",
        "    'objective': 'reg:squarederror',\n",
        "    'subsample': 0.6,\n",
        "    'gamma': 1,\n",
        "    'verbose_eval': True,\n",
        "    'tree_method': 'hist',\n",
        "    'nthread': 1\n",
        "}\n",
        "\n",
        "\n",
        "def square(x):\n",
        "    return x ** 2\n",
        "\n",
        "def neg(x):\n",
        "    return -x\n",
        "\n",
        "def launch(cmd):\n",
        "    \"\"\" launch dask workers\n",
        "    \"\"\"\n",
        "    return subprocess.check_call(cmd, stdout=sys.stdout, stderr=sys.stderr, shell=True)\n",
        "\n",
        "\n",
        "def get_chief_ip(cluster_config_dict):\n",
        "    if 'workerpool0' in cluster_config_dict['cluster']:\n",
        "      ip_address = cluster_config_dict['cluster']['workerpool0'][0].split(\":\")[0]\n",
        "    else:\n",
        "      # if the job is not distributed, 'chief' will be populated instead of\n",
        "      # workerpool0.\n",
        "      ip_address = cluster_config_dict['cluster']['chief'][0].split(\":\")[0]\n",
        "\n",
        "    print('The ip address of workerpool 0 is : {}'.format(ip_address))\n",
        "    return ip_address\n",
        "\n",
        "def get_chief_port(cluster_config_dict):\n",
        "\n",
        "    if \"open_ports\" in cluster_config_dict:\n",
        "      port = cluster_config_dict['open_ports'][0]\n",
        "    else:\n",
        "      # Use any port for the non-distributed job.\n",
        "      port = 7777\n",
        "    print(\"The open port is: {}\".format(port))\n",
        "\n",
        "    return port\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    cluster_config_str = os.environ.get('CLUSTER_SPEC')\n",
        "    cluster_config_dict  = json.loads(cluster_config_str)\n",
        "    print(json.dumps(cluster_config_dict, indent=2))\n",
        "    print('The workerpool type is:', flush=True)\n",
        "    print(cluster_config_dict['task']['type'], flush=True)\n",
        "    workerpool_type = cluster_config_dict['task']['type']\n",
        "    chief_ip = get_chief_ip(cluster_config_dict)\n",
        "    chief_port = get_chief_port(cluster_config_dict)\n",
        "    chief_address = \"{}:{}\".format(chief_ip, chief_port)\n",
        "\n",
        "    if workerpool_type == \"workerpool0\":\n",
        "      print('Running the dask scheduler.', flush=True)\n",
        "      proc_scheduler = launch('dask-scheduler --dashboard --dashboard-address 8888 --port {} &'.format(chief_port))\n",
        "      print('Done the dask scheduler.', flush=True)\n",
        "\n",
        "      client = Client(chief_address, timeout=1200)\n",
        "      print('Waiting the scheduler to be connected.', flush=True)\n",
        "      client.wait_for_workers(1)\n",
        "\n",
        "      X = dd.read_csv(IRIS_DATA_FILENAME, header=None)\n",
        "      y = dd.read_csv(IRIS_TARGET_FILENAME, header=None)\n",
        "      X.persist()\n",
        "      y.persist()\n",
        "      wait(X)\n",
        "      wait(y)\n",
        "      dtrain = DaskDMatrix(client, X, y)\n",
        "\n",
        "      output = xgb.dask.train(client, XGB_PARAMS, dtrain,  num_boost_round=100, evals=[(dtrain, 'train')])\n",
        "      print(\"Output: {}\".format(output), flush=True)\n",
        "      print(\"Saving file to: {}\".format(MODEL_FILE), flush=True)\n",
        "      output['booster'].save_model(MODEL_FILE)\n",
        "      bucket_name = MODEL_DIR.replace(\"gs://\", \"\").split(\"/\", 1)[0]\n",
        "      folder = MODEL_DIR.replace(\"gs://\", \"\").split(\"/\", 1)[1]\n",
        "      bucket = storage.Client().bucket(bucket_name)\n",
        "      print(\"Uploading file to: {}/{}{}\".format(bucket_name, folder, MODEL_FILE), flush=True)\n",
        "      blob = bucket.blob('{}{}'.format(folder, MODEL_FILE))\n",
        "      blob.upload_from_filename(MODEL_FILE)\n",
        "      print(\"Saved file to: {}/{}\".format(MODEL_DIR, MODEL_FILE), flush=True)\n",
        "\n",
        "      # Waiting 10 mins to connect the Dask dashboard\n",
        "      time.sleep(60 * 10)\n",
        "      client.shutdown()\n",
        "\n",
        "    else:\n",
        "      print('Running the dask worker.', flush=True)\n",
        "      client = Client(chief_address, timeout=1200)\n",
        "      print('client: {}.'.format(client), flush=True)\n",
        "      launch('dask-worker {}'.format(chief_address))\n",
        "      print('Done with the dask worker.', flush=True)\n",
        "\n",
        "      # Waiting 10 mins to connect the Dask dashboard\n",
        "      time.sleep(60 * 10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxsT4Vaos2W5"
      },
      "source": [
        "### Write the docker file\n",
        "The docker file is used to build the custom training container and passed to the Vertex Training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xD60d6Q0i2X0"
      },
      "outputs": [],
      "source": [
        "%%writefile Dockerfile\n",
        "FROM us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-9:latest\n",
        "WORKDIR /root\n",
        "\n",
        "# Update the keyring in order to run apt-get update.\n",
        "RUN rm -rf /usr/share/keyrings/cloud.google.gpg\n",
        "RUN rm -rf /etc/apt/sources.list.d/google-cloud-sdk.list\n",
        "RUN curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "RUN echo \"deb https://packages.cloud.google.com/apt cloud-sdk main\" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list\n",
        "\n",
        "RUN apt-get update\n",
        "RUN apt-get install -y telnet netcat iputils-ping  net-tools\n",
        "RUN python3.8 -m pip install 'xgboost>=1.4.2' 'dask-ml[complete]==2022.5.27' 'dask[complete]==2022.7.1' --upgrade\n",
        "RUN python3.8 -m pip install dask==2022.7.1 distributed==2022.7.1 bokeh==2.4.3 dask-cuda==22.8.0  --upgrade\n",
        "RUN python3.8 -m pip install gcsfs --upgrade\n",
        "\n",
        "\n",
        "# Make sure gsutil will use the default service account\n",
        "RUN echo '[GoogleCompute]\\nservice_account = default' > /etc/boto.cfg\n",
        "\n",
        "# Copies the trainer code\n",
        "RUN mkdir /root/trainer\n",
        "COPY trainer/train.py /root/trainer/train.py\n",
        "\n",
        "# Sets up the entry point to invoke the trainer.\n",
        "ENTRYPOINT [\"python3.8\", \"trainer/train.py\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6Yj8pZWAD7c"
      },
      "source": [
        "## Create a custom training job\n",
        "\n",
        "### Build a custom training container\n",
        "\n",
        "#### Enable Artifact Registry API\n",
        "You must enable the Artifact Registry API for your project. You will store your custom training container in Artifact Registry.\n",
        "\n",
        "<a href=\"https://cloud.google.com/artifact-registry/docs/enable-service\">Learn more about Enabling service</a>.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hd1j9BHeA81h"
      },
      "outputs": [],
      "source": [
        "! gcloud services enable artifactregistry.googleapis.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMUUSWBgA_Mb"
      },
      "source": [
        "### Create a private Docker repository\n",
        "Your first step is to create a Docker repository in Artifact Registry.\n",
        "\n",
        "1 - Run the `gcloud artifacts repositories create` command to create a new Docker repository with your region with the description `docker repository`.\n",
        "\n",
        "2 - Run the `gcloud artifacts repositories list` command to verify that your repository was created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_csN1pAH95F"
      },
      "outputs": [],
      "source": [
        "PRIVATE_REPO = \"my-docker-repo\"\n",
        "\n",
        "if os.getenv(\"IS_TESTING\"):\n",
        "    ! sudo apt-get update --yes && sudo apt-get --only-upgrade --yes install google-cloud-sdk-cloud-run-proxy google-cloud-sdk-harbourbridge google-cloud-sdk-cbt google-cloud-sdk-gke-gcloud-auth-plugin google-cloud-sdk-kpt google-cloud-sdk-local-extract google-cloud-sdk-minikube google-cloud-sdk-app-engine-java google-cloud-sdk-app-engine-go google-cloud-sdk-app-engine-python google-cloud-sdk-spanner-emulator google-cloud-sdk-bigtable-emulator google-cloud-sdk-nomos google-cloud-sdk-package-go-module google-cloud-sdk-firestore-emulator kubectl google-cloud-sdk-datastore-emulator google-cloud-sdk-app-engine-python-extras google-cloud-sdk-cloud-build-local google-cloud-sdk-kubectl-oidc google-cloud-sdk-anthos-auth google-cloud-sdk-app-engine-grpc google-cloud-sdk-pubsub-emulator google-cloud-sdk-datalab google-cloud-sdk-skaffold google-cloud-sdk google-cloud-sdk-terraform-tools google-cloud-sdk-config-connector\n",
        "    ! gcloud components update --quiet\n",
        "\n",
        "! gcloud artifacts repositories create {PRIVATE_REPO} --repository-format=docker --location={REGION} --description=\"Docker repository\"\n",
        "\n",
        "! gcloud artifacts repositories list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPpGuKi-BOAD"
      },
      "outputs": [],
      "source": [
        "TRAIN_IMAGE = (\n",
        "    f\"{REGION}-docker.pkg.dev/\" + PROJECT_ID + f\"/{PRIVATE_REPO}\" + \"/dask_support\"\n",
        ")\n",
        "print(\"Deployment:\", TRAIN_IMAGE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RgHDDL8BWgz"
      },
      "source": [
        "## Authenticate Docker to your repository\n",
        "### Configure authentication to your private repo\n",
        "Before you can push or pull container images to or from your Artifact Registry repository, you must configure Docker to use the gcloud command-line tool to authenticate requests to Artifact Registry for your region. On Colab, you'll have to use Cloud Build as the docker command is not available,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRLMyQwdKiLr"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "if not IS_COLAB:\n",
        "    ! gcloud auth configure-docker {REGION}-docker.pkg.dev --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XW4EecX1Bj8j"
      },
      "source": [
        "### Set the custom Docker container image\n",
        "Set the custom Docker container image.\n",
        "\n",
        "1. Pull the corresponding CPU or GPU Docker image from Docker Hub.\n",
        "2. Create a tag for registering the image with Artifact Registry\n",
        "3. Register the image with Artifact Registry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2XTaEDHB9HK"
      },
      "outputs": [],
      "source": [
        "if not IS_COLAB:\n",
        "    ! docker build -t $TRAIN_IMAGE -f Dockerfile .\n",
        "    ! docker push $TRAIN_IMAGE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rzI1ZXQCA30"
      },
      "source": [
        "## Build and push the custom docker container image by using Cloud Build\n",
        "\n",
        "Build and push a Docker image with Cloud Build"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJMhP0kYCEgY"
      },
      "outputs": [],
      "source": [
        "if IS_COLAB:\n",
        "    !  gcloud builds submit --timeout=1800s --region={REGION} --tag $TRAIN_IMAGE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41ZgDYPNfEvt"
      },
      "source": [
        "## Run training job with SDK (Option 1) or with gcloud (Option 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtUycdZhCJvQ"
      },
      "source": [
        "### 1.1 Initialize Vertex AI SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKONAbwtCMgJ"
      },
      "outputs": [],
      "source": [
        "aiplatform.init(\n",
        "    project=PROJECT_ID,\n",
        "    staging_bucket=BUCKET_URI,\n",
        "    location=REGION,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GB2j39BCXiy"
      },
      "source": [
        "### 1.2 Run a Vertex AI SDK CustomContainerTrainingJob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7udaU3jxfKs8"
      },
      "source": [
        "You can specify the fields enable_web_access and enable_dashboard_access. The enable_web_access enables the interactive shell for the job and enable_dashboard_access allows the dask dashboard to be accessed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoEkXaaDepfo"
      },
      "outputs": [],
      "source": [
        "gcs_output_uri_prefix = f\"{BUCKET_URI}/output\"\n",
        "replica_count = 2\n",
        "machine_type = \"n1-standard-4\"\n",
        "display_name = \"test_display_name\"\n",
        "DEPLOY_IMAGE = \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest\"\n",
        "\n",
        "custom_container_training_job = aiplatform.CustomContainerTrainingJob(\n",
        "    display_name=display_name,\n",
        "    model_serving_container_image_uri=DEPLOY_IMAGE,\n",
        "    container_uri=TRAIN_IMAGE,\n",
        ")\n",
        "\n",
        "custom_container_training_job.run(\n",
        "    base_output_dir=gcs_output_uri_prefix,\n",
        "    replica_count=replica_count,\n",
        "    machine_type=machine_type,\n",
        "    enable_dashboard_access=True,\n",
        "    enable_web_access=True,\n",
        "    sync=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROqdvgBKFGKL"
      },
      "source": [
        "Wait for a few minutes for the Custom Job to start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBYbvim_FJz0"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "time.sleep(60 * 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpLMkJAzDTgx"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    print(f\"Custom Training Job Name: {custom_container_training_job.resource_name}\")\n",
        "    print(f\"GCS Output URI Prefix: {gcs_output_uri_prefix}\")\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0EJdcjPyjDK"
      },
      "source": [
        "You can access the link to the Custom Job in the Cloud Console UI here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMZPrcqsyj0e"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    print(\n",
        "        f\"Custom Training Job URI: {custom_container_training_job._custom_job_console_uri()}\"\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7McPKJGxymLU"
      },
      "source": [
        "Once the job is in the state \"RUNNING\", you can access the web access and dashboard access URIs here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiHdwJs1yoE0"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    print(\n",
        "        f\"Web Access and Dashboard URIs: {custom_container_training_job.web_access_uris}\"\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RDh5CzNywIA"
      },
      "source": [
        "The interactive shell has the key with the format \"workerpool0-0\", while the dashboard uri has the key with the format \"workerpool0-0:\" + port number (workerpool0-0:8888 in this example). On the page for your Custom Job in the Cloud Console UI, you can also \"Launch web terminal\" for \"workerpool0-0\" for web access, or click \"Launch web terminal\" for \"workerpool0-0:\" + port number for dashboard access.\n",
        "\n",
        "Note that you can only access an interactive shell and dashboard while the job is running. If you don't see Launch web terminal in the UI or the URIs in the output of the Web Access and Dashboard URIs command, this might be because Vertex AI hasn't started running your job yet, or because the job has already finished or failed. If the job's Status is Queued or Pending, wait a minute; then try refreshing the page, or trying the command again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVktIbToRpmR"
      },
      "source": [
        "### 2. Run a CustomContainerTraining Job with gcloud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVvxLj8GRsM6"
      },
      "source": [
        "You can also create a training job with the gcloud command.  With the gcloud command, you can specify the field enableWebAccess and enableDashboardAccess. The enableWebAccess enables the interactive shell for the job and enableDashboardAccess allows the dask dashboard to be accessed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkOQtyDsRwsS"
      },
      "outputs": [],
      "source": [
        "%%bash -s \"$BUCKET_URI/output\" \"$TRAIN_IMAGE\"\n",
        "\n",
        "cat <<EOF >config.yaml\n",
        "enableDashboardAccess: true\n",
        "enableWebAccess: true\n",
        "# Creates two worker pool. The first worker pool is a chief and the second is\n",
        "# a worker.\n",
        "workerPoolSpecs:\n",
        "  - machineSpec:\n",
        "      machineType: n1-standard-8\n",
        "    replicaCount: 1\n",
        "    containerSpec:\n",
        "      imageUri: $2\n",
        "  - machineSpec:\n",
        "      machineType: n1-standard-8\n",
        "    replicaCount: 1\n",
        "    containerSpec:\n",
        "      imageUri: $2\n",
        "baseOutputDirectory:\n",
        "  outputUriPrefix: $1\n",
        "EOF\n",
        "cat config.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5FLoTWzSNw7"
      },
      "source": [
        "The following command creates a training job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MPj-NnpSQ1U"
      },
      "outputs": [],
      "source": [
        "! gcloud ai custom-jobs create --region=us-central1 --config=config.yaml --display-name={display_name}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAYihHT9ff9q"
      },
      "source": [
        "#### Access the dashboard and interactive shell for a gcloud custom job\n",
        "\n",
        "Once the job is created, you can access the web access URI and dashboard access URI by using the `gcloud ai custom-jobs describe` command to print the field webAccessUris. The interactive shell has the key with the format \"workerpool0-0\", while the dashboard uri has the key with the format \"workerpool0-0:\" + port number (workerpool0-0:8888 in this example).\n",
        "\n",
        "You also can find the links in the Cloud Console UI. In the Cloud Console UI, in the Vertex AI section, go to Training and then Custom Jobs. Click on the name of your custom training job. On the page for your job, click \"Launch web terminal\" for \"workerpool0-0\" for web access, or click \"Launch web terminal\" for \"workerpool0-0:\" + port number for dashboard access.\n",
        "\n",
        "Note that you can only access an interactive shell and dashboard while the job is running. If you don't see Launch web terminal in the UI or the URIs in the output of the gcloud command, this might be because Vertex AI hasn't started running your job yet, or because the job has already finished or failed. If the job's Status is Queued or Pending, wait a minute; then try refreshing the page, or trying the gcloud command again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqFwDvWCSYFX"
      },
      "source": [
        "#### Troubleshooting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjbPElukSiZt"
      },
      "source": [
        "The [interactive shell](https://cloud.google.com/vertex-ai/docs/training/monitor-debug-interactive-shell) can be used to debugging the access of the dask dashboard. You can get the dashboard point by the following command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9eNOtYUTzJW"
      },
      "outputs": [],
      "source": [
        "# Note the following command should run inside the interactive shell.\n",
        "# printenv | grep AIP_DASHBOARD_PORT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SCRLCkpUDNM"
      },
      "source": [
        "Then you can check if there are dashboard monitoring the port."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fK74qU78ULPS"
      },
      "outputs": [],
      "source": [
        "# Note the following command should run inside the interactive shell.\n",
        "# netstat -ntlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ucKHMGFUUF4"
      },
      "source": [
        "You can manually turn up the dashboard instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJrjiePvUip0"
      },
      "outputs": [],
      "source": [
        "# Note the following command should run inside the interactive shell.\n",
        "# dask-scheduler --dashboard-address :port_number"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlsCCIFgDcGy"
      },
      "source": [
        "### View training output artifact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjshJ2dcDdkQ"
      },
      "outputs": [],
      "source": [
        "! gsutil ls $gcs_output_uri_prefix/model/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c26XO4bZDnDH"
      },
      "source": [
        "# Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:\n",
        "\n",
        "- Cloud Storage Bucket\n",
        "- Cloud Vertex Training Job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ue3SfrMODunu"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import traceback\n",
        "\n",
        "# Set this to true only if you'd like to delete your bucket\n",
        "delete_bucket = False\n",
        "\n",
        "! gsutil rm -rf $gcs_output_uri_prefix\n",
        "\n",
        "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
        "    ! gsutil rm -r $BUCKET_URI\n",
        "\n",
        "try:\n",
        "    custom_container_training_job.delete()\n",
        "except Exception as e:\n",
        "    logging.error(traceback.format_exc())\n",
        "    print(e)\n",
        "\n",
        "! gcloud artifacts repositories delete {PRIVATE_REPO} --location={REGION} --quiet"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "xgboost_data_parallel_training_on_cpu_using_dask.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

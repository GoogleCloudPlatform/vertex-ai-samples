{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title:generic,gcp"
   },
   "source": [
    "# E2E ML on GCP: MLOps stage 1 : data management\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/ml_ops/stage1/mlops_data_management.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/ml_ops/stage1/mlops_data_management.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:mlops"
   },
   "source": [
    "## Overview\n",
    "\n",
    "\n",
    "This tutorial demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. This tutorial covers stage 1 : data management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:bq,chicago,lbn"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset used for this tutorial is the [Chicago Taxi](https://www.kaggle.com/chicago/chicago-taxi-trips-bq). The version of the dataset you will use in this tutorial is stored in a public BigQuery table. The trained model predicts whether someone would leave a tip for a taxi fare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:mlops,stage1,tabular"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you create a MLOps stage 1: data management process.\n",
    "\n",
    "This tutorial uses the following Vertex AI and Data Analytics services:\n",
    "\n",
    "- `Vertex AI Datasets`\n",
    "- `BigQuery`\n",
    "- `Dataflow`\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Explore and visualize the data.\n",
    "- Create a Vertex AI `Dataset` resource from `BigQuery` table -- for AutoML training.\n",
    "- Extract a copy of the dataset to a CSV file in Cloud Storage.\n",
    "- Create a Vertex AI `Dataset` resource from CSV files -- alternative for AutoML training.\n",
    "- Read a sample of the `BigQuery` dataset into a dataframe.\n",
    "- Generate statistics and data schema using TensorFlow Data Validation from the samples in the dataframe.\n",
    "- Generate a TFRecord feature specification using TensorFlow Data Validation from the data schema.\n",
    "- Preprocess a portion of the BigQuery data using `Dataflow` -- for custom training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costs \n",
    "\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "* Dataflow\n",
    "* BigQuery\n",
    "\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "recommendation:mlops,stage1,tabular"
   },
   "source": [
    "### Recommendations\n",
    "\n",
    "When doing E2E MLOps on Google Cloud for data management, the following best practices with structured (tabular) data are recommended:\n",
    "\n",
    " - For large amounts of data, use BigQuery table. Otherwise, use a CSV file stored in Cloud Storage.\n",
    " - When storing a large amount of data in CSV file, shard the data at 10,000 rows per shard.\n",
    " - Create a managed dataset with Vertex AI `TabularDataset`.\n",
    " - Preprocess the data with `Dataflow`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_mlops"
   },
   "source": [
    "## Installations\n",
    "\n",
    "Install *one time* the packages for executing the MLOps notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "install_mlops",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting tensorflow==2.5\n",
      "  Using cached tensorflow-2.5.0-cp37-cp37m-manylinux2010_x86_64.whl (454.3 MB)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.5) (3.19.4)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.5) (1.34.1)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow==2.5) (1.6.3)\n",
      "Requirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.5) (0.4.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow==2.5) (1.1.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.5) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.5) (2.5.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.5) (0.37.1)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.5) (1.19.5)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.5) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.5) (3.7.4.3)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow==2.5) (1.1.2)\n",
      "Requirement already satisfied: tensorboard~=2.5 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow==2.5) (2.8.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow==2.5) (0.2.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.5) (1.12.1)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow==2.5) (3.3.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.5) (1.12)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.5) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.5) (0.12.0)\n",
      "Requirement already satisfied: cached-property in /home/jupyter/.local/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow==2.5) (1.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow==2.5) (2.27.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow==2.5) (2.0.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow==2.5) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow==2.5) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow==2.5) (1.35.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow==2.5) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow==2.5) (0.6.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow==2.5) (59.8.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5) (4.11.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5) (2.0.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5) (3.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5) (3.2.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: tensorflow\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.5.3\n",
      "    Uninstalling tensorflow-2.5.3:\n",
      "      Successfully uninstalled tensorflow-2.5.3\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-data-validation 1.2.0 requires google-cloud-bigquery<2.21,>=1.28.0, but you have google-cloud-bigquery 2.34.0 which is incompatible.\n",
      "tensorflow-data-validation 1.2.0 requires pyarrow<3,>=1, but you have pyarrow 7.0.0 which is incompatible.\n",
      "tfx-bsl 1.2.0 requires google-cloud-bigquery<2.21,>=1.28.0, but you have google-cloud-bigquery 2.34.0 which is incompatible.\n",
      "tfx-bsl 1.2.0 requires pyarrow<3,>=1, but you have pyarrow 7.0.0 which is incompatible.\n",
      "tensorflow-transform 1.2.0 requires google-cloud-bigquery<2.21,>=1.28.0, but you have google-cloud-bigquery 2.34.0 which is incompatible.\n",
      "tensorflow-transform 1.2.0 requires pyarrow<3,>=1, but you have pyarrow 7.0.0 which is incompatible.\n",
      "tensorflow-serving-api 2.5.4 requires tensorflow<3,>=2.5.3, but you have tensorflow 2.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tensorflow-2.5.3\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tensorflow-data-validation==1.2 in /home/jupyter/.local/lib/python3.7/site-packages (1.2.0)\n",
      "Requirement already satisfied: apache-beam[gcp]<3,>=2.31 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.2) (2.36.0)\n",
      "Requirement already satisfied: six<2,>=1.12 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.2) (1.15.0)\n",
      "Collecting pyarrow<3,>=1\n",
      "  Using cached pyarrow-2.0.0-cp37-cp37m-manylinux2014_x86_64.whl (17.7 MB)\n",
      "Requirement already satisfied: tensorflow-metadata<1.3,>=1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.2) (1.2.0)\n",
      "Requirement already satisfied: tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.2) (2.5.3)\n",
      "Requirement already satisfied: tfx-bsl<1.3,>=1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.2) (1.2.0)\n",
      "Requirement already satisfied: pandas<2,>=1.0 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow-data-validation==1.2) (1.0.5)\n",
      "Requirement already satisfied: numpy<1.20,>=1.16 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.2) (1.19.5)\n",
      "Collecting google-cloud-bigquery<2.21,>=1.28.0\n",
      "  Using cached google_cloud_bigquery-2.20.0-py2.py3-none-any.whl (189 kB)\n",
      "Requirement already satisfied: absl-py<0.13,>=0.9 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.2) (0.12.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.13 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.2) (3.19.4)\n",
      "Requirement already satisfied: joblib<0.15,>=0.12 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.2) (0.14.1)\n",
      "Requirement already satisfied: grpcio<2,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (1.34.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (2.27.1)\n",
      "Requirement already satisfied: fastavro<2,>=0.21.4 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (1.4.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (3.7.4.3)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (1.4.2)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (1.7)\n",
      "Requirement already satisfied: httplib2<0.20.0,>=0.8 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (0.19.1)\n",
      "Requirement already satisfied: cloudpickle<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (2.0.0)\n",
      "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (3.12.3)\n",
      "Requirement already satisfied: pytz>=2018.3 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (2021.3)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (2.8.2)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (2.6.0)\n",
      "Requirement already satisfied: proto-plus<2,>=1.7.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (1.19.9)\n",
      "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (0.3.1.1)\n",
      "Requirement already satisfied: orjson<4.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (3.6.7)\n",
      "Requirement already satisfied: oauth2client<5,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (4.1.3)\n",
      "Requirement already satisfied: google-cloud-language<2,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (1.3.0)\n",
      "Requirement already satisfied: google-cloud-bigtable<2,>=0.31.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (1.7.0)\n",
      "Requirement already satisfied: google-cloud-core<2,>=0.28.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (1.7.2)\n",
      "Requirement already satisfied: google-cloud-videointelligence<2,>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (1.16.1)\n",
      "Requirement already satisfied: google-cloud-dlp<4,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (3.6.0)\n",
      "Requirement already satisfied: grpcio-gcp<1,>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (0.2.2)\n",
      "Requirement already satisfied: google-cloud-spanner<2,>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (1.19.1)\n",
      "Requirement already satisfied: google-cloud-pubsublite<2,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (1.4.0)\n",
      "Requirement already satisfied: google-cloud-datastore<2,>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (1.15.3)\n",
      "Requirement already satisfied: google-cloud-recommendations-ai<=0.2.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (0.2.0)\n",
      "Requirement already satisfied: google-cloud-vision<2,>=0.38.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (1.0.0)\n",
      "Requirement already satisfied: google-apitools<0.5.32,>=0.5.31 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (0.5.31)\n",
      "Requirement already satisfied: cachetools<5,>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (4.2.4)\n",
      "Requirement already satisfied: google-cloud-pubsub<3,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (2.6.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.18.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (1.35.0)\n",
      "Requirement already satisfied: google-cloud-bigquery-storage>=2.6.3 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (2.11.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<2.21,>=1.28.0->tensorflow-data-validation==1.2) (21.3)\n",
      "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<2.21,>=1.28.0->tensorflow-data-validation==1.2) (1.31.5)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<2.21,>=1.28.0->tensorflow-data-validation==1.2) (1.3.3)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation==1.2) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation==1.2) (3.3.0)\n",
      "Requirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation==1.2) (0.4.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation==1.2) (1.12.1)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation==1.2) (0.37.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation==1.2) (2.5.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation==1.2) (1.1.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation==1.2) (1.6.3)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation==1.2) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation==1.2) (0.2.0)\n",
      "Requirement already satisfied: tensorboard~=2.5 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation==1.2) (2.8.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation==1.2) (1.12)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation==1.2) (3.1.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-metadata<1.3,>=1.2->tensorflow-data-validation==1.2) (1.54.0)\n",
      "Requirement already satisfied: tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15 in /opt/conda/lib/python3.7/site-packages (from tfx-bsl<1.3,>=1.2->tensorflow-data-validation==1.2) (2.5.4)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.11 in /opt/conda/lib/python3.7/site-packages (from tfx-bsl<1.3,>=1.2->tensorflow-data-validation==1.2) (1.12.10)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.29.0->google-cloud-bigquery<2.21,>=1.28.0->tensorflow-data-validation==1.2) (59.8.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.11->tfx-bsl<1.3,>=1.2->tensorflow-data-validation==1.2) (0.1.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.11->tfx-bsl<1.3,>=1.2->tensorflow-data-validation==1.2) (3.0.1)\n",
      "Requirement already satisfied: fasteners>=0.14 in /opt/conda/lib/python3.7/site-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (0.17.3)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (0.2.7)\n",
      "Requirement already satisfied: libcst>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery-storage>=2.6.3->apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (0.4.1)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (0.12.3)\n",
      "Requirement already satisfied: grpcio-status>=1.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (1.34.1)\n",
      "Requirement already satisfied: overrides<7.0.0,>=6.0.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (6.1.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<2.21,>=1.28.0->tensorflow-data-validation==1.2) (1.1.2)\n",
      "Requirement already satisfied: cached-property in /home/jupyter/.local/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation==1.2) (1.5.2)\n",
      "Requirement already satisfied: docopt in /opt/conda/lib/python3.7/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (0.6.2)\n",
      "Requirement already satisfied: pyparsing<3,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<0.20.0,>=0.8->apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (2.4.7)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from oauth2client<5,>=2.0.1->apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (2.0.11)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (2021.10.8)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation==1.2) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation==1.2) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation==1.2) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation==1.2) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation==1.2) (2.0.3)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation==1.2) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<2.21,>=1.28.0->tensorflow-data-validation==1.2) (1.15.0)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from libcst>=0.2.5->google-cloud-bigquery-storage>=2.6.3->apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (0.7.1)\n",
      "Requirement already satisfied: pyyaml>=5.2 in /opt/conda/lib/python3.7/site-packages (from libcst>=0.2.5->google-cloud-bigquery-storage>=2.6.3->apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (5.4.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation==1.2) (4.11.1)\n",
      "Requirement already satisfied: typing-utils>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from overrides<7.0.0,>=6.0.1->google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (0.1.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<2.21,>=1.28.0->tensorflow-data-validation==1.2) (2.21)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation==1.2) (3.7.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15.2->tensorflow-data-validation==1.2) (3.2.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from typing-inspect>=0.4.0->libcst>=0.2.5->google-cloud-bigquery-storage>=2.6.3->apache-beam[gcp]<3,>=2.31->tensorflow-data-validation==1.2) (0.4.3)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: pyarrow, google-cloud-bigquery\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 7.0.0\n",
      "    Uninstalling pyarrow-7.0.0:\n",
      "      Successfully uninstalled pyarrow-7.0.0\n",
      "  Attempting uninstall: google-cloud-bigquery\n",
      "    Found existing installation: google-cloud-bigquery 2.34.0\n",
      "    Uninstalling google-cloud-bigquery-2.34.0:\n",
      "      Successfully uninstalled google-cloud-bigquery-2.34.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed google-cloud-bigquery-2.34.0 pyarrow-7.0.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tensorflow-transform==1.2 in /opt/conda/lib/python3.7/site-packages (1.2.0)\n",
      "Requirement already satisfied: tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow-transform==1.2) (2.5.3)\n",
      "Requirement already satisfied: pydot<2,>=1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow-transform==1.2) (1.4.2)\n",
      "Collecting google-cloud-bigquery<2.21,>=1.28.0\n",
      "  Using cached google_cloud_bigquery-2.20.0-py2.py3-none-any.whl (189 kB)\n",
      "Requirement already satisfied: numpy<1.20,>=1.16 in /opt/conda/lib/python3.7/site-packages (from tensorflow-transform==1.2) (1.19.5)\n",
      "Requirement already satisfied: protobuf<4,>=3.13 in /opt/conda/lib/python3.7/site-packages (from tensorflow-transform==1.2) (3.19.4)\n",
      "Collecting pyarrow<3,>=1\n",
      "  Using cached pyarrow-2.0.0-cp37-cp37m-manylinux2014_x86_64.whl (17.7 MB)\n",
      "Requirement already satisfied: tfx-bsl<1.3.0,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-transform==1.2) (1.2.0)\n",
      "Requirement already satisfied: apache-beam[gcp]<3,>=2.31 in /opt/conda/lib/python3.7/site-packages (from tensorflow-transform==1.2) (2.36.0)\n",
      "Requirement already satisfied: absl-py<0.13,>=0.9 in /opt/conda/lib/python3.7/site-packages (from tensorflow-transform==1.2) (0.12.0)\n",
      "Requirement already satisfied: tensorflow-metadata<1.3.0,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-transform==1.2) (1.2.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py<0.13,>=0.9->tensorflow-transform==1.2) (1.15.0)\n",
      "Requirement already satisfied: oauth2client<5,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (4.1.3)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (2.8.2)\n",
      "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (3.12.3)\n",
      "Requirement already satisfied: cloudpickle<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (2.0.0)\n",
      "Requirement already satisfied: grpcio<2,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (1.34.1)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (2.6.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (2.27.1)\n",
      "Requirement already satisfied: httplib2<0.20.0,>=0.8 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (0.19.1)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (1.7)\n",
      "Requirement already satisfied: orjson<4.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (3.6.7)\n",
      "Requirement already satisfied: proto-plus<2,>=1.7.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (1.19.9)\n",
      "Requirement already satisfied: fastavro<2,>=0.21.4 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (1.4.9)\n",
      "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (0.3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (3.7.4.3)\n",
      "Requirement already satisfied: pytz>=2018.3 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (2021.3)\n",
      "Requirement already satisfied: google-cloud-recommendations-ai<=0.2.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (0.2.0)\n",
      "Requirement already satisfied: google-cloud-videointelligence<2,>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (1.16.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.18.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (1.35.0)\n",
      "Requirement already satisfied: google-cloud-core<2,>=0.28.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (1.7.2)\n",
      "Requirement already satisfied: google-cloud-vision<2,>=0.38.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (1.0.0)\n",
      "Requirement already satisfied: google-cloud-pubsublite<2,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (1.4.0)\n",
      "Requirement already satisfied: google-cloud-dlp<4,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (3.6.0)\n",
      "Requirement already satisfied: google-cloud-datastore<2,>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (1.15.3)\n",
      "Requirement already satisfied: google-cloud-bigtable<2,>=0.31.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (1.7.0)\n",
      "Requirement already satisfied: cachetools<5,>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (4.2.4)\n",
      "Requirement already satisfied: google-cloud-spanner<2,>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (1.19.1)\n",
      "Requirement already satisfied: grpcio-gcp<1,>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (0.2.2)\n",
      "Requirement already satisfied: google-cloud-bigquery-storage>=2.6.3 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (2.11.0)\n",
      "Requirement already satisfied: google-apitools<0.5.32,>=0.5.31 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (0.5.31)\n",
      "Requirement already satisfied: google-cloud-language<2,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (1.3.0)\n",
      "Requirement already satisfied: google-cloud-pubsub<3,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (2.6.0)\n",
      "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<2.21,>=1.28.0->tensorflow-transform==1.2) (1.31.5)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<2.21,>=1.28.0->tensorflow-transform==1.2) (21.3)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<2.21,>=1.28.0->tensorflow-transform==1.2) (1.3.3)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /opt/conda/lib/python3.7/site-packages (from pydot<2,>=1.2->tensorflow-transform==1.2) (2.4.7)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow-transform==1.2) (2.5.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow-transform==1.2) (1.12.1)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow-transform==1.2) (0.2.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow-transform==1.2) (1.12)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow-transform==1.2) (3.1.0)\n",
      "Requirement already satisfied: tensorboard~=2.5 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow-transform==1.2) (2.8.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow-transform==1.2) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow-transform==1.2) (3.3.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow-transform==1.2) (1.1.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow-transform==1.2) (0.37.1)\n",
      "Requirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow-transform==1.2) (0.4.0)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow-transform==1.2) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow-transform==1.2) (1.1.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-metadata<1.3.0,>=1.2.0->tensorflow-transform==1.2) (1.54.0)\n",
      "Requirement already satisfied: pandas<2,>=1.0 in /home/jupyter/.local/lib/python3.7/site-packages (from tfx-bsl<1.3.0,>=1.2.0->tensorflow-transform==1.2) (1.0.5)\n",
      "Requirement already satisfied: tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15 in /opt/conda/lib/python3.7/site-packages (from tfx-bsl<1.3.0,>=1.2.0->tensorflow-transform==1.2) (2.5.4)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.11 in /opt/conda/lib/python3.7/site-packages (from tfx-bsl<1.3.0,>=1.2.0->tensorflow-transform==1.2) (1.12.10)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.29.0->google-cloud-bigquery<2.21,>=1.28.0->tensorflow-transform==1.2) (59.8.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.11->tfx-bsl<1.3.0,>=1.2.0->tensorflow-transform==1.2) (0.1.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.11->tfx-bsl<1.3.0,>=1.2.0->tensorflow-transform==1.2) (3.0.1)\n",
      "Requirement already satisfied: fasteners>=0.14 in /opt/conda/lib/python3.7/site-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (0.17.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (4.8)\n",
      "Requirement already satisfied: libcst>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery-storage>=2.6.3->apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (0.4.1)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (0.12.3)\n",
      "Requirement already satisfied: grpcio-status>=1.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (1.34.1)\n",
      "Requirement already satisfied: overrides<7.0.0,>=6.0.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (6.1.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<2.21,>=1.28.0->tensorflow-transform==1.2) (1.1.2)\n",
      "Requirement already satisfied: cached-property in /home/jupyter/.local/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow-transform==1.2) (1.5.2)\n",
      "Requirement already satisfied: docopt in /opt/conda/lib/python3.7/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (0.6.2)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from oauth2client<5,>=2.0.1->apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (2.0.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (1.26.8)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow-transform==1.2) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow-transform==1.2) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow-transform==1.2) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow-transform==1.2) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow-transform==1.2) (2.0.3)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow-transform==1.2) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<2.21,>=1.28.0->tensorflow-transform==1.2) (1.15.0)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from libcst>=0.2.5->google-cloud-bigquery-storage>=2.6.3->apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (0.7.1)\n",
      "Requirement already satisfied: pyyaml>=5.2 in /opt/conda/lib/python3.7/site-packages (from libcst>=0.2.5->google-cloud-bigquery-storage>=2.6.3->apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (5.4.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow-transform==1.2) (4.11.1)\n",
      "Requirement already satisfied: typing-utils>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from overrides<7.0.0,>=6.0.1->google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (0.1.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<2.21,>=1.28.0->tensorflow-transform==1.2) (2.21)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow-transform==1.2) (3.7.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow-transform==1.2) (3.2.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from typing-inspect>=0.4.0->libcst>=0.2.5->google-cloud-bigquery-storage>=2.6.3->apache-beam[gcp]<3,>=2.31->tensorflow-transform==1.2) (0.4.3)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: pyarrow, google-cloud-bigquery\n",
      "  Attempting uninstall: pyarrow\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: pyarrow 7.0.0\n",
      "    Uninstalling pyarrow-7.0.0:\n",
      "      Successfully uninstalled pyarrow-7.0.0\n",
      "  Attempting uninstall: google-cloud-bigquery\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: google-cloud-bigquery 2.34.0\n",
      "    Uninstalling google-cloud-bigquery-2.34.0:\n",
      "      Successfully uninstalled google-cloud-bigquery-2.34.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed google-cloud-bigquery-2.20.0 pyarrow-2.0.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tensorflow-io==0.18 in /opt/conda/lib/python3.7/site-packages (0.18.0)\n",
      "Requirement already satisfied: tensorflow<2.6.0,>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-io==0.18) (2.5.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.18.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-io==0.18) (0.18.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (3.7.4.3)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (1.1.2)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (0.37.1)\n",
      "Requirement already satisfied: tensorboard~=2.5 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (2.8.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (0.12.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (1.15.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (3.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (2.5.0)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (3.19.4)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (1.12.1)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (1.19.5)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (1.1.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (0.2.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (1.12)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (1.34.1)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (1.6.3)\n",
      "Requirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (0.4.0)\n",
      "Requirement already satisfied: cached-property in /home/jupyter/.local/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (1.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (2.27.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (0.4.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (59.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (2.0.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (1.35.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (4.11.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (2.0.11)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow-io==0.18) (3.2.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: google-cloud-aiplatform[tensorboard] in /home/jupyter/.local/lib/python3.7/site-packages (1.10.0)\n",
      "Requirement already satisfied: proto-plus>=1.10.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform[tensorboard]) (1.19.9)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform[tensorboard]) (21.3)\n",
      "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.26.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform[tensorboard]) (1.31.5)\n",
      "Requirement already satisfied: google-cloud-storage<2.0.0dev,>=1.32.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-aiplatform[tensorboard]) (1.44.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform[tensorboard]) (2.20.0)\n",
      "Requirement already satisfied: tensorflow<=2.7.0,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform[tensorboard]) (2.5.3)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform[tensorboard]) (2021.3)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform[tensorboard]) (1.35.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform[tensorboard]) (59.8.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform[tensorboard]) (2.27.1)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform[tensorboard]) (3.19.4)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform[tensorboard]) (1.15.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform[tensorboard]) (1.54.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform[tensorboard]) (1.34.1)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform[tensorboard]) (1.3.3)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform[tensorboard]) (1.7.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-cloud-aiplatform[tensorboard]) (2.4.7)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (2.5.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (0.37.1)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (1.12)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (3.1.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (1.1.2)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (1.19.5)\n",
      "Requirement already satisfied: tensorboard~=2.5 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (2.8.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (0.12.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (3.3.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (1.12.1)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (3.7.4.3)\n",
      "Requirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (0.4.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (1.6.3)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (0.2.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform[tensorboard]) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform[tensorboard]) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform[tensorboard]) (4.8)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform[tensorboard]) (1.1.2)\n",
      "Requirement already satisfied: cached-property in /home/jupyter/.local/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (1.5.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform[tensorboard]) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform[tensorboard]) (2.0.11)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform[tensorboard]) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform[tensorboard]) (2021.10.8)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (2.0.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (0.4.6)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform[tensorboard]) (1.15.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (4.11.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform[tensorboard]) (0.4.8)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform[tensorboard]) (2.21)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (3.7.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (3.2.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: google-cloud-pipeline-components in /opt/conda/lib/python3.7/site-packages (0.3.1)\n",
      "Collecting google-cloud-pipeline-components\n",
      "  Downloading google_cloud_pipeline_components-1.0.0-py3-none-any.whl (347 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.5/347.5 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-api-core<2dev,>=1.26.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-pipeline-components) (1.31.5)\n",
      "Requirement already satisfied: kfp<2.0.0,>=1.8.9 in /opt/conda/lib/python3.7/site-packages (from google-cloud-pipeline-components) (1.8.11)\n",
      "Requirement already satisfied: google-cloud-aiplatform>=1.4.3 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-pipeline-components) (1.10.0)\n",
      "Requirement already satisfied: google-cloud-notebooks>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-pipeline-components) (1.1.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components) (1.54.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components) (59.8.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components) (2021.3)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components) (2.27.1)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components) (1.15.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components) (21.3)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components) (3.19.4)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components) (1.35.0)\n",
      "Requirement already satisfied: proto-plus>=1.10.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.4.3->google-cloud-pipeline-components) (1.19.9)\n",
      "Requirement already satisfied: google-cloud-storage<2.0.0dev,>=1.32.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.4.3->google-cloud-pipeline-components) (1.44.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.4.3->google-cloud-pipeline-components) (2.20.0)\n",
      "Requirement already satisfied: pydantic<2,>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (1.9.0)\n",
      "Requirement already satisfied: kubernetes<19,>=8.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (18.20.0)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /opt/conda/lib/python3.7/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (8.0.3)\n",
      "Requirement already satisfied: fire<1,>=0.3.1 in /opt/conda/lib/python3.7/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (0.4.0)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (0.9.1)\n",
      "Requirement already satisfied: absl-py<2,>=0.9 in /opt/conda/lib/python3.7/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (0.12.0)\n",
      "Requirement already satisfied: cloudpickle<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (2.0.0)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.13 in /opt/conda/lib/python3.7/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (0.1.13)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (1.8.0)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.7/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (0.8.9)\n",
      "Requirement already satisfied: typing-extensions<4,>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (3.7.4.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.3.2 in /opt/conda/lib/python3.7/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (0.4.0)\n",
      "Requirement already satisfied: jsonschema<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (3.2.0)\n",
      "Requirement already satisfied: Deprecated<2,>=1.2.7 in /opt/conda/lib/python3.7/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (1.2.13)\n",
      "Requirement already satisfied: PyYAML<6,>=5.3 in /opt/conda/lib/python3.7/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (5.4.1)\n",
      "Requirement already satisfied: strip-hints<1,>=0.1.8 in /opt/conda/lib/python3.7/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (0.1.10)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /opt/conda/lib/python3.7/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (0.13)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /opt/conda/lib/python3.7/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (1.12.10)\n",
      "Requirement already satisfied: uritemplate<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (3.0.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<9,>=7.1.2->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (4.11.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated<2,>=1.2.7->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (1.12.1)\n",
      "Requirement already satisfied: termcolor in /home/jupyter/.local/lib/python3.7/site-packages (from fire<1,>=0.3.1->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (1.1.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components) (1.34.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (0.1.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (0.19.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components) (4.8)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.4.3->google-cloud-pipeline-components) (1.3.3)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.4.3->google-cloud-pipeline-components) (1.7.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (21.4.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (0.18.1)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (2.8.2)\n",
      "Requirement already satisfied: urllib3>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (1.26.8)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (2021.10.8)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (1.3.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (1.2.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components) (2.4.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components) (2.0.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components) (3.3)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (0.37.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.4.3->google-cloud-pipeline-components) (1.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.25.0->google-api-core<2dev,>=1.26.0->google-cloud-pipeline-components) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click<9,>=7.1.2->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (3.7.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.4.3->google-cloud-pipeline-components) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.4.3->google-cloud-pipeline-components) (2.21)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: google-cloud-pipeline-components\n",
      "  Attempting uninstall: google-cloud-pipeline-components\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: google-cloud-pipeline-components 0.3.1\n",
      "    Uninstalling google-cloud-pipeline-components-0.3.1:\n",
      "      Successfully uninstalled google-cloud-pipeline-components-0.3.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed google-cloud-pipeline-components-1.0.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: google-cloud-bigquery in /opt/conda/lib/python3.7/site-packages (2.20.0)\n",
      "Collecting google-cloud-bigquery\n",
      "  Downloading google_cloud_bigquery-2.34.1-py2.py3-none-any.whl (206 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.1/206.1 KB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery) (1.3.3)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery) (2.8.2)\n",
      "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery) (1.31.5)\n",
      "Requirement already satisfied: proto-plus>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery) (1.19.9)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery) (3.19.4)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery) (1.7.2)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery) (21.3)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery) (2.27.1)\n",
      "Collecting grpcio<2.0dev,>=1.38.1\n",
      "  Using cached grpcio-1.44.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery) (1.54.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery) (2021.3)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery) (1.35.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery) (59.8.0)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery) (1.15.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery) (1.1.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-cloud-bigquery) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery) (2.0.11)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery) (4.8)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery) (2.21)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery) (0.4.8)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: grpcio, google-cloud-bigquery\n",
      "  Attempting uninstall: grpcio\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: grpcio 1.34.1\n",
      "    Uninstalling grpcio-1.34.1:\n",
      "      Successfully uninstalled grpcio-1.34.1\n",
      "  Attempting uninstall: google-cloud-bigquery\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: google-cloud-bigquery 2.20.0\n",
      "    Uninstalling google-cloud-bigquery-2.20.0:\n",
      "      Successfully uninstalled google-cloud-bigquery-2.20.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-data-validation 1.2.0 requires google-cloud-bigquery<2.21,>=1.28.0, but you have google-cloud-bigquery 2.34.1 which is incompatible.\n",
      "tfx-bsl 1.2.0 requires google-cloud-bigquery<2.21,>=1.28.0, but you have google-cloud-bigquery 2.34.1 which is incompatible.\n",
      "tensorflow 2.5.3 requires grpcio~=1.34.0, but you have grpcio 1.44.0 which is incompatible.\n",
      "tensorflow-transform 1.2.0 requires google-cloud-bigquery<2.21,>=1.28.0, but you have google-cloud-bigquery 2.34.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed google-cloud-bigquery-2.34.1 grpcio-1.44.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: google-cloud-logging in /opt/conda/lib/python3.7/site-packages (3.0.0)\n",
      "Requirement already satisfied: google-cloud-appengine-logging<2.0.0dev,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-logging) (1.1.0)\n",
      "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.26.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-logging) (1.31.5)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-logging) (1.7.2)\n",
      "Requirement already satisfied: google-cloud-audit-log<1.0.0dev,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-logging) (0.2.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-logging) (0.12.3)\n",
      "Requirement already satisfied: proto-plus>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-logging) (1.19.9)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-logging) (1.54.0)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-logging) (1.35.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-logging) (21.3)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-logging) (59.8.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-logging) (3.19.4)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-logging) (1.15.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-logging) (2021.3)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-logging) (2.27.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-logging) (1.44.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-logging) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-logging) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-logging) (4.8)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-logging) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-logging) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-logging) (2.0.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-logging) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-logging) (1.26.8)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-logging) (0.4.8)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: apache-beam[gcp] in /opt/conda/lib/python3.7/site-packages (2.36.0)\n",
      "Requirement already satisfied: oauth2client<5,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (4.1.3)\n",
      "Requirement already satisfied: httplib2<0.20.0,>=0.8 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (0.19.1)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (2.6.0)\n",
      "Requirement already satisfied: cloudpickle<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (2.0.0)\n",
      "Requirement already satisfied: pyarrow<7.0.0,>=0.15.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (2.0.0)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (1.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (2.27.1)\n",
      "Requirement already satisfied: grpcio<2,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (1.44.0)\n",
      "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (3.12.3)\n",
      "Requirement already satisfied: numpy<1.22.0,>=1.14.3 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (1.19.5)\n",
      "Requirement already satisfied: pytz>=2018.3 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (2021.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (3.7.4.3)\n",
      "Requirement already satisfied: proto-plus<2,>=1.7.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (1.19.9)\n",
      "Requirement already satisfied: protobuf<4,>=3.12.2 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (3.19.4)\n",
      "Requirement already satisfied: fastavro<2,>=0.21.4 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (1.4.9)\n",
      "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (0.3.1.1)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (1.4.2)\n",
      "Requirement already satisfied: orjson<4.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (3.6.7)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (2.8.2)\n",
      "Requirement already satisfied: google-cloud-bigquery<3,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (2.34.1)\n",
      "Requirement already satisfied: google-cloud-dlp<4,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (3.6.0)\n",
      "Requirement already satisfied: google-cloud-bigtable<2,>=0.31.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (1.7.0)\n",
      "Requirement already satisfied: google-cloud-recommendations-ai<=0.2.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (0.2.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.18.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (1.35.0)\n",
      "Requirement already satisfied: google-apitools<0.5.32,>=0.5.31 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (0.5.31)\n",
      "Requirement already satisfied: google-cloud-vision<2,>=0.38.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (1.0.0)\n",
      "Requirement already satisfied: google-cloud-core<2,>=0.28.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (1.7.2)\n",
      "Requirement already satisfied: cachetools<5,>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (4.2.4)\n",
      "Requirement already satisfied: google-cloud-bigquery-storage>=2.6.3 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (2.11.0)\n",
      "Requirement already satisfied: google-cloud-language<2,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (1.3.0)\n",
      "Requirement already satisfied: google-cloud-spanner<2,>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (1.19.1)\n",
      "Requirement already satisfied: grpcio-gcp<1,>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (0.2.2)\n",
      "Requirement already satisfied: google-cloud-videointelligence<2,>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (1.16.1)\n",
      "Requirement already satisfied: google-cloud-datastore<2,>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (1.15.3)\n",
      "Requirement already satisfied: google-cloud-pubsublite<2,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (1.4.0)\n",
      "Requirement already satisfied: google-cloud-pubsub<3,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (2.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]) (1.15.0)\n",
      "Requirement already satisfied: fasteners>=0.14 in /opt/conda/lib/python3.7/site-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]) (0.17.3)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]) (59.8.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]) (4.8)\n",
      "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3,>=1.6.0->apache-beam[gcp]) (1.31.5)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3,>=1.6.0->apache-beam[gcp]) (21.3)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3,>=1.6.0->apache-beam[gcp]) (1.3.3)\n",
      "Requirement already satisfied: libcst>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery-storage>=2.6.3->apache-beam[gcp]) (0.4.1)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]) (0.12.3)\n",
      "Requirement already satisfied: overrides<7.0.0,>=6.0.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]) (6.1.0)\n",
      "Requirement already satisfied: grpcio-status>=1.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]) (1.34.1)\n",
      "Requirement already satisfied: docopt in /opt/conda/lib/python3.7/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]) (0.6.2)\n",
      "Requirement already satisfied: pyparsing<3,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<0.20.0,>=0.8->apache-beam[gcp]) (2.4.7)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from oauth2client<5,>=2.0.1->apache-beam[gcp]) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]) (2.0.11)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery<3,>=1.6.0->apache-beam[gcp]) (1.54.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3,>=1.6.0->apache-beam[gcp]) (1.1.2)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from libcst>=0.2.5->google-cloud-bigquery-storage>=2.6.3->apache-beam[gcp]) (0.7.1)\n",
      "Requirement already satisfied: pyyaml>=5.2 in /opt/conda/lib/python3.7/site-packages (from libcst>=0.2.5->google-cloud-bigquery-storage>=2.6.3->apache-beam[gcp]) (5.4.1)\n",
      "Requirement already satisfied: typing-utils>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from overrides<7.0.0,>=6.0.1->google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]) (0.1.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3,>=1.6.0->apache-beam[gcp]) (1.15.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from typing-inspect>=0.4.0->libcst>=0.2.5->google-cloud-bigquery-storage>=2.6.3->apache-beam[gcp]) (0.4.3)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3,>=1.6.0->apache-beam[gcp]) (2.21)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pyarrow in /opt/conda/lib/python3.7/site-packages (2.0.0)\n",
      "Collecting pyarrow\n",
      "  Using cached pyarrow-7.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.7/site-packages (from pyarrow) (1.19.5)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: pyarrow\n",
      "  Attempting uninstall: pyarrow\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: pyarrow 2.0.0\n",
      "    Uninstalling pyarrow-2.0.0:\n",
      "      Successfully uninstalled pyarrow-2.0.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-data-validation 1.2.0 requires google-cloud-bigquery<2.21,>=1.28.0, but you have google-cloud-bigquery 2.34.1 which is incompatible.\n",
      "tensorflow-data-validation 1.2.0 requires pyarrow<3,>=1, but you have pyarrow 7.0.0 which is incompatible.\n",
      "tfx-bsl 1.2.0 requires google-cloud-bigquery<2.21,>=1.28.0, but you have google-cloud-bigquery 2.34.1 which is incompatible.\n",
      "tfx-bsl 1.2.0 requires pyarrow<3,>=1, but you have pyarrow 7.0.0 which is incompatible.\n",
      "tensorflow-transform 1.2.0 requires google-cloud-bigquery<2.21,>=1.28.0, but you have google-cloud-bigquery 2.34.1 which is incompatible.\n",
      "tensorflow-transform 1.2.0 requires pyarrow<3,>=1, but you have pyarrow 7.0.0 which is incompatible.\n",
      "apache-beam 2.36.0 requires pyarrow<7.0.0,>=0.15.1, but you have pyarrow 7.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pyarrow-7.0.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: cloudml-hypertune in /opt/conda/lib/python3.7/site-packages (0.1.0.dev6)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: kfp in /opt/conda/lib/python3.7/site-packages (1.8.11)\n",
      "Requirement already satisfied: cloudpickle<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp) (2.0.0)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from kfp) (1.8.0)\n",
      "Requirement already satisfied: Deprecated<2,>=1.2.7 in /opt/conda/lib/python3.7/site-packages (from kfp) (1.2.13)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.7/site-packages (from kfp) (0.8.9)\n",
      "Requirement already satisfied: strip-hints<1,>=0.1.8 in /opt/conda/lib/python3.7/site-packages (from kfp) (0.1.10)\n",
      "Requirement already satisfied: jsonschema<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp) (3.2.0)\n",
      "Requirement already satisfied: PyYAML<6,>=5.3 in /opt/conda/lib/python3.7/site-packages (from kfp) (5.4.1)\n",
      "Requirement already satisfied: fire<1,>=0.3.1 in /opt/conda/lib/python3.7/site-packages (from kfp) (0.4.0)\n",
      "Requirement already satisfied: kubernetes<19,>=8.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp) (18.20.0)\n",
      "Requirement already satisfied: uritemplate<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp) (3.0.1)\n",
      "Requirement already satisfied: google-cloud-storage<2,>=1.20.0 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (1.44.0)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /opt/conda/lib/python3.7/site-packages (from kfp) (1.12.10)\n",
      "Requirement already satisfied: typer<1.0,>=0.3.2 in /opt/conda/lib/python3.7/site-packages (from kfp) (0.4.0)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.13 in /opt/conda/lib/python3.7/site-packages (from kfp) (0.1.13)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp) (1.35.0)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /opt/conda/lib/python3.7/site-packages (from kfp) (8.0.3)\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp) (3.19.4)\n",
      "Requirement already satisfied: pydantic<2,>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from kfp) (1.9.0)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /opt/conda/lib/python3.7/site-packages (from kfp) (0.13)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from kfp) (0.9.1)\n",
      "Requirement already satisfied: typing-extensions<4,>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from kfp) (3.7.4.3)\n",
      "Requirement already satisfied: absl-py<2,>=0.9 in /opt/conda/lib/python3.7/site-packages (from kfp) (0.12.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py<2,>=0.9->kfp) (1.15.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<9,>=7.1.2->kfp) (4.11.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated<2,>=1.2.7->kfp) (1.12.1)\n",
      "Requirement already satisfied: termcolor in /home/jupyter/.local/lib/python3.7/site-packages (from fire<1,>=0.3.1->kfp) (1.1.0)\n",
      "Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp) (1.31.5)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp) (0.1.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp) (0.19.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp) (0.2.7)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp) (59.8.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp) (4.8)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2,>=1.20.0->kfp) (2.27.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2,>=1.20.0->kfp) (1.3.3)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2,>=1.20.0->kfp) (1.7.2)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp) (0.18.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp) (21.4.0)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2.8.2)\n",
      "Requirement already satisfied: urllib3>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (1.26.8)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2021.10.8)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp) (1.3.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp) (1.2.3)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp) (0.37.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp) (1.54.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp) (2021.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp) (21.3)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp) (1.1.2)\n",
      "Requirement already satisfied: pyparsing<3,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client<2,>=1.7.8->kfp) (2.4.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<2,>=1.20.0->kfp) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<2,>=1.20.0->kfp) (2.0.11)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click<9,>=7.1.2->kfp) (3.7.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp) (2.21)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "ONCE_ONLY = True\n",
    "if ONCE_ONLY:\n",
    "    ! pip3 install -U tensorflow==2.5 $USER_FLAG\n",
    "    ! pip3 install -U tensorflow-data-validation==1.2 $USER_FLAG\n",
    "    ! pip3 install -U tensorflow-transform==1.2 $USER_FLAG\n",
    "    ! pip3 install -U tensorflow-io==0.18 $USER_FLAG\n",
    "    ! pip3 install --upgrade google-cloud-aiplatform[tensorboard] $USER_FLAG\n",
    "    ! pip3 install --upgrade google-cloud-pipeline-components $USER_FLAG\n",
    "    ! pip3 install --upgrade google-cloud-bigquery $USER_FLAG\n",
    "    ! pip3 install --upgrade google-cloud-logging $USER_FLAG\n",
    "    ! pip3 install --upgrade apache-beam[gcp] $USER_FLAG\n",
    "    ! pip3 install --upgrade pyarrow $USER_FLAG\n",
    "    ! pip3 install --upgrade cloudml-hypertune $USER_FLAG\n",
    "    ! pip3 install --upgrade kfp $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "restart"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_id"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "autoset_project_id"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: vertex-ai-dev\n"
     ]
    }
   ],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
    "\n",
    "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type:\"string\"}\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "timestamp"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "timestamp"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:custom"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "When you submit a custom training job using the Vertex SDK, you upload a Python package\n",
    "containing your training code to a Cloud Storage bucket. Vertex AI runs\n",
    "the code from this package. In this tutorial, Vertex AI also saves the\n",
    "trained model that results from your job in the same bucket. You can then\n",
    "create an `Endpoint` resource based on this output in order to serve\n",
    "online predictions.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "autoset_bucket"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "create_bucket"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://vertex-ai-devaip-20220303121514/...\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validate_bucket"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "validate_bucket"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### Set up variables\n",
    "\n",
    "Next, set up some variables used throughout the tutorial.\n",
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_bq"
   },
   "source": [
    "#### Import BigQuery\n",
    "\n",
    "Import the BigQuery package into your Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "import_bq"
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_beam"
   },
   "source": [
    "#### Import Apache Beam\n",
    "\n",
    "Import the Apache Beam package into your Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "import_beam"
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_tf"
   },
   "source": [
    "#### Import TensorFlow\n",
    "\n",
    "Import the TensorFlow package into your Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "import_tf"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_tfdv"
   },
   "source": [
    "#### Import TensorFlow Data Validation\n",
    "\n",
    "Import the TensorFlow Data Validation (TFDV) package into your Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "import_tfdv"
   },
   "outputs": [],
   "source": [
    "import tensorflow_data_validation as tfdv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_tft"
   },
   "source": [
    "#### Import TensorFlow Transform\n",
    "\n",
    "Import the TensorFlow Transform (TFT) package into your Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "import_tft"
   },
   "outputs": [],
   "source": [
    "import tensorflow_transform as tft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_bq"
   },
   "source": [
    "### Create BigQuery client\n",
    "\n",
    "Create the BigQuery client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "init_bq"
   },
   "outputs": [],
   "source": [
    "bqclient = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "recommendation:dataset,tabular"
   },
   "source": [
    "## Datasets\n",
    "\n",
    "Next, you look at options for creating managed datasets:\n",
    "\n",
    "* `BigQuery`: Create a Vertex `TabularDataset` resource.\n",
    "* `CSV`: Create a Vertex `TabularDataset` resource.\n",
    "* `TFRecords`: Self-manage the dataset on Cloud Storage storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_file:u_dataset,bq"
   },
   "source": [
    "#### Location of BigQuery training data.\n",
    "\n",
    "Now set the variable `IMPORT_FILE` to the location of the data table in BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "import_file:chicago,bq,lbn"
   },
   "outputs": [],
   "source": [
    "IMPORT_FILE = \"bq://bigquery-public-data.chicago_taxi_trips.taxi_trips\"\n",
    "BQ_TABLE = \"bigquery-public-data.chicago_taxi_trips.taxi_trips\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "explore_bq:chicago"
   },
   "source": [
    "### Explore BigQuery dataset\n",
    "\n",
    "Explore the contents of the BigQuery table:\n",
    "\n",
    "- Get all examples from 2015\n",
    "- Sort by the day of the week\n",
    "- Count the number of examples for each day of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "explore_bq:chicago"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  trip_dayofweek trip_dayname  trip_count\n",
      "0              1       Sunday     4141154\n",
      "1              2       Monday     4105900\n",
      "2              3      Tuesday     4378805\n",
      "3              4    Wednesday     4542810\n",
      "4              5     Thursday     4918190\n",
      "5              6       Friday     5289830\n",
      "6              7     Saturday     5009186\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"SELECT\n",
    "    CAST(EXTRACT(DAYOFWEEK FROM trip_start_timestamp) AS string) AS trip_dayofweek,\n",
    "    FORMAT_DATE('%A',cast(trip_start_timestamp as date)) AS trip_dayname,\n",
    "    COUNT(*) as trip_count,\n",
    "FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "WHERE\n",
    "    EXTRACT(YEAR FROM trip_start_timestamp) = 2015\n",
    "GROUP BY\n",
    "    trip_dayofweek,\n",
    "    trip_dayname\n",
    "ORDER BY\n",
    "    trip_dayofweek\"\"\"\n",
    "\n",
    "_ = bqclient.query(query)\n",
    "rows = _.result()\n",
    "dataframe = rows.to_dataframe()\n",
    "print(dataframe.head(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "plot_bq:chicago"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='trip_dayname'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataframe.plot(kind=\"bar\", x=\"trip_dayname\", y=\"trip_count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a BigQuery dataset\n",
    "Next you create a dataset in your project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "BQ_DATASET = BQ_TABLE.split(\".\")[1]\n",
    "\n",
    "#create a dataset in BigQuery\n",
    "query=f\"\"\"\n",
    "CREATE SCHEMA {BQ_DATASET}\n",
    "OPTIONS(\n",
    "  location=\"us\"\n",
    "  )\n",
    "\"\"\"\n",
    "response = bqclient.query(query)\n",
    "_ = response.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq_copy:chicago"
   },
   "source": [
    "### Make a private copy of subset of BigQuery table\n",
    "\n",
    "Next, you make a private copy of the BigQuery table:\n",
    "- Select a subset of columns\n",
    "- Select a subset of rows (LIMIT)\n",
    "- Set conditions (WHERE)\n",
    "- Do feature engineering on geolocation coords.\n",
    "- Pre-split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "bq_copy:chicago"
   },
   "outputs": [],
   "source": [
    "BQ_TABLE_COPY = f\"{PROJECT_ID}.{BQ_DATASET}.taxi_trips\"\n",
    "LIMIT = 300000\n",
    "YEAR = 2020\n",
    "\n",
    "query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{BQ_TABLE_COPY}`\n",
    "AS (\n",
    "    WITH\n",
    "      taxitrips AS (\n",
    "      SELECT\n",
    "        trip_start_timestamp,\n",
    "        trip_seconds,\n",
    "        trip_miles,\n",
    "        payment_type,\n",
    "        pickup_longitude,\n",
    "        pickup_latitude,\n",
    "        dropoff_longitude,\n",
    "        dropoff_latitude,\n",
    "        tips,\n",
    "        fare\n",
    "      FROM\n",
    "        `{BQ_TABLE}`\n",
    "      WHERE pickup_longitude IS NOT NULL\n",
    "      AND pickup_latitude IS NOT NULL\n",
    "      AND dropoff_longitude IS NOT NULL\n",
    "      AND dropoff_latitude IS NOT NULL\n",
    "      AND trip_miles > 0\n",
    "      AND trip_seconds > 0\n",
    "      AND fare > 0\n",
    "      AND EXTRACT(YEAR FROM trip_start_timestamp) = {YEAR}\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "      EXTRACT(MONTH from trip_start_timestamp) as trip_month,\n",
    "      EXTRACT(DAY from trip_start_timestamp) as trip_day,\n",
    "      EXTRACT(DAYOFWEEK from trip_start_timestamp) as trip_day_of_week,\n",
    "      EXTRACT(HOUR from trip_start_timestamp) as trip_hour,\n",
    "      CAST(trip_seconds AS FLOAT64) as trip_seconds,\n",
    "      trip_miles,\n",
    "      payment_type,\n",
    "      ST_AsText(\n",
    "          ST_SnapToGrid(ST_GeogPoint(pickup_longitude, pickup_latitude), 0.1)\n",
    "      ) AS pickup_grid,\n",
    "      ST_AsText(\n",
    "          ST_SnapToGrid(ST_GeogPoint(dropoff_longitude, dropoff_latitude), 0.1)\n",
    "      ) AS dropoff_grid,\n",
    "      ST_Distance(\n",
    "          ST_GeogPoint(pickup_longitude, pickup_latitude),\n",
    "          ST_GeogPoint(dropoff_longitude, dropoff_latitude)\n",
    "      ) AS euclidean,\n",
    "      CONCAT(\n",
    "          ST_AsText(ST_SnapToGrid(ST_GeogPoint(pickup_longitude,\n",
    "              pickup_latitude), 0.1)),\n",
    "          ST_AsText(ST_SnapToGrid(ST_GeogPoint(dropoff_longitude,\n",
    "              dropoff_latitude), 0.1))\n",
    "      ) AS loc_cross,\n",
    "      IF((tips/fare >= 0.2), 1, 0) AS tip_bin,\n",
    "    FROM\n",
    "      taxitrips\n",
    "    LIMIT {LIMIT}\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "response = bqclient.query(query)\n",
    "_ = response.result()\n",
    "\n",
    "BQ_TABLE = BQ_TABLE_COPY\n",
    "IMPORT_FILE = f\"bq://{BQ_TABLE_COPY}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_dataset:tabular,bq,lbn"
   },
   "source": [
    "### Create the Dataset\n",
    "\n",
    "#### BigQuery input data\n",
    "\n",
    "Next, create the `Dataset` resource using the `create` method for the `TabularDataset` class, which takes the following parameters:\n",
    "\n",
    "- `display_name`: The human readable name for the `Dataset` resource.\n",
    "- `bq_source`: Import data items from a BigQuery table into the `Dataset` resource.\n",
    "- `labels`: User defined metadata. In this example, you store the location of the Cloud Storage bucket containing the user defined data.\n",
    "\n",
    "Learn more about [TabularDataset from BigQuery table](https://cloud.google.com/vertex-ai/docs/datasets/create-dataset-api#aiplatform_create_dataset_tabular_bigquery_sample-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "create_dataset:tabular,bq,lbn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.datasets.dataset:Creating TabularDataset\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:Create TabularDataset backing LRO: projects/931647533046/locations/us-central1/datasets/1591974089763848192/operations/2565385546448764928\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:TabularDataset created. Resource name: projects/931647533046/locations/us-central1/datasets/1591974089763848192\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:To use this TabularDataset in another session:\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:ds = aiplatform.TabularDataset('projects/931647533046/locations/us-central1/datasets/1591974089763848192')\n",
      "projects/931647533046/locations/us-central1/datasets/1591974089763848192\n"
     ]
    }
   ],
   "source": [
    "dataset = aip.TabularDataset.create(\n",
    "    display_name=\"Chicago Taxi\" + \"_\" + TIMESTAMP,\n",
    "    bq_source=[IMPORT_FILE],\n",
    "    labels={\"user_metadata\": BUCKET_NAME[5:]},\n",
    ")\n",
    "\n",
    "label_column = \"tip_bin\"\n",
    "\n",
    "print(dataset.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq_to_dataframe:all"
   },
   "source": [
    "### Read the BigQuery dataset into a pandas dataframe\n",
    "\n",
    "Next, you read a sample of the dataset into a pandas dataframe using BigQuery `list_rows()` and `to_dataframe()` method, as follows:\n",
    "\n",
    "- `list_rows()`: Performs a query on the specified table and returns a row iterator to the query results. Optionally specify:\n",
    " - `selected_fields`: Subset of fields (columns) to return.\n",
    " - `max_results`: The maximum number of rows to return. Same as SQL LIMIT command.\n",
    "\n",
    "\n",
    "- `rows.to_dataframe()`: Invokes the row iterator and reads in the data into a pandas dataframe.\n",
    "\n",
    "Learn more about [Loading BigQuery table into a dataframe](https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "bq_to_dataframe:all"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   trip_month  trip_day  trip_day_of_week  trip_hour  trip_seconds  \\\n",
      "0           1         1                 4          0        1723.0   \n",
      "1           5         1                 6         11         826.0   \n",
      "2           5         1                 6         12         255.0   \n",
      "3           5         1                 6         10         480.0   \n",
      "4           5         1                 6         12         141.0   \n",
      "\n",
      "   trip_miles payment_type        pickup_grid       dropoff_grid  euclidean  \\\n",
      "0        0.70         Cash  POINT(-87.6 41.9)  POINT(-87.6 41.9)        0.0   \n",
      "1        1.35         Cash  POINT(-87.6 41.9)  POINT(-87.6 41.9)        0.0   \n",
      "2        1.48  Credit Card  POINT(-87.7 41.9)  POINT(-87.7 41.9)        0.0   \n",
      "3        2.10         Cash  POINT(-87.6 41.8)  POINT(-87.6 41.8)        0.0   \n",
      "4        0.34         Cash    POINT(-87.7 42)    POINT(-87.7 42)        0.0   \n",
      "\n",
      "                            loc_cross  tip_bin  \n",
      "0  POINT(-87.6 41.9)POINT(-87.6 41.9)        0  \n",
      "1  POINT(-87.6 41.9)POINT(-87.6 41.9)        0  \n",
      "2  POINT(-87.7 41.9)POINT(-87.7 41.9)        1  \n",
      "3  POINT(-87.6 41.8)POINT(-87.6 41.8)        0  \n",
      "4      POINT(-87.7 42)POINT(-87.7 42)        0  \n"
     ]
    }
   ],
   "source": [
    "# Download a table.\n",
    "table = bigquery.TableReference.from_string(BQ_TABLE)\n",
    "\n",
    "rows = bqclient.list_rows(table, max_results=300000)\n",
    "\n",
    "dataframe = rows.to_dataframe()\n",
    "print(dataframe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfdv_stats:dataframe"
   },
   "source": [
    "###  Generate dataset statistics\n",
    "\n",
    "#### Dataframe input data\n",
    "\n",
    "Generate statistics on the dataset with the TensorFlow Data Validation (TFDV) package. Use the `generate_statistics_from_dataframe()` method, with the following parameters:\n",
    "\n",
    "- `dataframe`: The dataset in an in-memory pandas dataframe.\n",
    "- `stats_options`: The selected statistics options:\n",
    "  - `label_feature`: The column which is the label to predict.\n",
    "  - `sample_rate`: The sampling rate. If specified, statistics is computed over the sample.\n",
    "  - `num_top_values`: number of most frequent feature values to keep for string features.\n",
    "\n",
    "Learn about [TensorFlow Data Validation (TFDV)](https://www.tensorflow.org/tfx/data_validation/get_started)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "id": "tfdv_stats:dataframe",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets {\n",
      "  num_examples: 300000\n",
      "  features {\n",
      "    num_stats {\n",
      "      common_stats {\n",
      "        num_non_missing: 300000\n",
      "        min_num_values: 1\n",
      "        max_num_values: 1\n",
      "        avg_num_values: 1.0\n",
      "        num_values_histogram {\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          type: QUANTILES\n",
      "        }\n",
      "        tot_num_values: 300000\n",
      "      }\n",
      "      mean: 2.6197066666666666\n",
      "      std_dev: 1.5838172181859316\n",
      "      min: 1.0\n",
      "      median: 2.0\n",
      "      max: 12.0\n",
      "      histograms {\n",
      "        buckets {\n",
      "          low_value: 1.0\n",
      "          high_value: 2.1\n",
      "          sample_count: 253815.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 2.1\n",
      "          high_value: 3.2\n",
      "          sample_count: 165.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 3.2\n",
      "          high_value: 4.300000000000001\n",
      "          sample_count: 510.0000000000002\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 4.300000000000001\n",
      "          high_value: 5.4\n",
      "          sample_count: 16530.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 5.4\n",
      "          high_value: 6.5\n",
      "          sample_count: 23630.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 6.5\n",
      "          high_value: 7.6000000000000005\n",
      "          sample_count: 110.00000000000006\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 7.6000000000000005\n",
      "          high_value: 8.700000000000001\n",
      "          sample_count: 110.00000000000006\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 8.700000000000001\n",
      "          high_value: 9.8\n",
      "          sample_count: 870.0000000000001\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 9.8\n",
      "          high_value: 10.9\n",
      "          sample_count: 3795.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 10.9\n",
      "          high_value: 12.0\n",
      "          sample_count: 464.99999999999994\n",
      "        }\n",
      "      }\n",
      "      histograms {\n",
      "        buckets {\n",
      "          low_value: 1.0\n",
      "          high_value: 2.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 2.0\n",
      "          high_value: 2.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 2.0\n",
      "          high_value: 2.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 2.0\n",
      "          high_value: 2.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 2.0\n",
      "          high_value: 2.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 2.0\n",
      "          high_value: 2.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 2.0\n",
      "          high_value: 2.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 2.0\n",
      "          high_value: 2.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 2.0\n",
      "          high_value: 5.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 5.0\n",
      "          high_value: 12.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        type: QUANTILES\n",
      "      }\n",
      "    }\n",
      "    path {\n",
      "      step: \"trip_month\"\n",
      "    }\n",
      "  }\n",
      "  features {\n",
      "    num_stats {\n",
      "      common_stats {\n",
      "        num_non_missing: 300000\n",
      "        min_num_values: 1\n",
      "        max_num_values: 1\n",
      "        avg_num_values: 1.0\n",
      "        num_values_histogram {\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          type: QUANTILES\n",
      "        }\n",
      "        tot_num_values: 300000\n",
      "      }\n",
      "      mean: 15.49675\n",
      "      std_dev: 8.139057036137539\n",
      "      min: 1.0\n",
      "      median: 15.0\n",
      "      max: 31.0\n",
      "      histograms {\n",
      "        buckets {\n",
      "          low_value: 1.0\n",
      "          high_value: 4.0\n",
      "          sample_count: 21900.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 4.0\n",
      "          high_value: 7.0\n",
      "          sample_count: 35400.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 7.0\n",
      "          high_value: 10.0\n",
      "          sample_count: 24300.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 10.0\n",
      "          high_value: 13.0\n",
      "          sample_count: 33600.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 13.0\n",
      "          high_value: 16.0\n",
      "          sample_count: 35700.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 16.0\n",
      "          high_value: 19.0\n",
      "          sample_count: 27600.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 19.0\n",
      "          high_value: 22.0\n",
      "          sample_count: 41100.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 22.0\n",
      "          high_value: 25.0\n",
      "          sample_count: 24900.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 25.0\n",
      "          high_value: 28.0\n",
      "          sample_count: 36600.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 28.0\n",
      "          high_value: 31.0\n",
      "          sample_count: 18900.0\n",
      "        }\n",
      "      }\n",
      "      histograms {\n",
      "        buckets {\n",
      "          low_value: 1.0\n",
      "          high_value: 4.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 4.0\n",
      "          high_value: 7.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 7.0\n",
      "          high_value: 10.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 10.0\n",
      "          high_value: 13.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 13.0\n",
      "          high_value: 15.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 15.0\n",
      "          high_value: 19.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 19.0\n",
      "          high_value: 21.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 21.0\n",
      "          high_value: 24.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 24.0\n",
      "          high_value: 27.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 27.0\n",
      "          high_value: 31.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        type: QUANTILES\n",
      "      }\n",
      "    }\n",
      "    path {\n",
      "      step: \"trip_day\"\n",
      "    }\n",
      "  }\n",
      "  features {\n",
      "    num_stats {\n",
      "      common_stats {\n",
      "        num_non_missing: 300000\n",
      "        min_num_values: 1\n",
      "        max_num_values: 1\n",
      "        avg_num_values: 1.0\n",
      "        num_values_histogram {\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          type: QUANTILES\n",
      "        }\n",
      "        tot_num_values: 300000\n",
      "      }\n",
      "      mean: 4.188193333333333\n",
      "      std_dev: 1.7861793310365621\n",
      "      min: 1.0\n",
      "      median: 4.0\n",
      "      max: 7.0\n",
      "      histograms {\n",
      "        buckets {\n",
      "          low_value: 1.0\n",
      "          high_value: 1.6\n",
      "          sample_count: 24180.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 1.6\n",
      "          high_value: 2.2\n",
      "          sample_count: 40080.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 2.2\n",
      "          high_value: 2.8\n",
      "          sample_count: 179.9999999999999\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 2.8\n",
      "          high_value: 3.4\n",
      "          sample_count: 45480.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 3.4\n",
      "          high_value: 4.0\n",
      "          sample_count: 180.00000000000003\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 4.0\n",
      "          high_value: 4.6\n",
      "          sample_count: 49680.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 4.6\n",
      "          high_value: 5.2\n",
      "          sample_count: 55380.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 5.2\n",
      "          high_value: 5.8\n",
      "          sample_count: 179.9999999999999\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 5.8\n",
      "          high_value: 6.3999999999999995\n",
      "          sample_count: 54780.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 6.3999999999999995\n",
      "          high_value: 7.0\n",
      "          sample_count: 29880.0\n",
      "        }\n",
      "      }\n",
      "      histograms {\n",
      "        buckets {\n",
      "          low_value: 1.0\n",
      "          high_value: 2.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 2.0\n",
      "          high_value: 2.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 2.0\n",
      "          high_value: 3.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 3.0\n",
      "          high_value: 4.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 4.0\n",
      "          high_value: 4.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 4.0\n",
      "          high_value: 5.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 5.0\n",
      "          high_value: 5.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 5.0\n",
      "          high_value: 6.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 6.0\n",
      "          high_value: 6.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 6.0\n",
      "          high_value: 7.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        type: QUANTILES\n",
      "      }\n",
      "    }\n",
      "    path {\n",
      "      step: \"trip_day_of_week\"\n",
      "    }\n",
      "  }\n",
      "  features {\n",
      "    num_stats {\n",
      "      common_stats {\n",
      "        num_non_missing: 300000\n",
      "        min_num_values: 1\n",
      "        max_num_values: 1\n",
      "        avg_num_values: 1.0\n",
      "        num_values_histogram {\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          type: QUANTILES\n",
      "        }\n",
      "        tot_num_values: 300000\n",
      "      }\n",
      "      mean: 13.81846\n",
      "      std_dev: 5.336158096271137\n",
      "      num_zeros: 5454\n",
      "      median: 14.0\n",
      "      max: 23.0\n",
      "      histograms {\n",
      "        buckets {\n",
      "          high_value: 2.3\n",
      "          sample_count: 12990.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 2.3\n",
      "          high_value: 4.6\n",
      "          sample_count: 4590.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 4.6\n",
      "          high_value: 6.8999999999999995\n",
      "          sample_count: 6690.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 6.8999999999999995\n",
      "          high_value: 9.2\n",
      "          sample_count: 40290.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 9.2\n",
      "          high_value: 11.5\n",
      "          sample_count: 32490.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 11.5\n",
      "          high_value: 13.799999999999999\n",
      "          sample_count: 36990.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 13.799999999999999\n",
      "          high_value: 16.099999999999998\n",
      "          sample_count: 59490.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 16.099999999999998\n",
      "          high_value: 18.4\n",
      "          sample_count: 44790.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 18.4\n",
      "          high_value: 20.7\n",
      "          sample_count: 32790.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 20.7\n",
      "          high_value: 23.0\n",
      "          sample_count: 28890.0\n",
      "        }\n",
      "      }\n",
      "      histograms {\n",
      "        buckets {\n",
      "          high_value: 7.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 7.0\n",
      "          high_value: 9.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 9.0\n",
      "          high_value: 11.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 11.0\n",
      "          high_value: 13.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 13.0\n",
      "          high_value: 14.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 14.0\n",
      "          high_value: 16.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 16.0\n",
      "          high_value: 17.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 17.0\n",
      "          high_value: 19.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 19.0\n",
      "          high_value: 20.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 20.0\n",
      "          high_value: 23.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        type: QUANTILES\n",
      "      }\n",
      "    }\n",
      "    path {\n",
      "      step: \"trip_hour\"\n",
      "    }\n",
      "  }\n",
      "  features {\n",
      "    type: FLOAT\n",
      "    num_stats {\n",
      "      common_stats {\n",
      "        num_non_missing: 300000\n",
      "        min_num_values: 1\n",
      "        max_num_values: 1\n",
      "        avg_num_values: 1.0\n",
      "        num_values_histogram {\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          type: QUANTILES\n",
      "        }\n",
      "        tot_num_values: 300000\n",
      "      }\n",
      "      mean: 862.5700533333334\n",
      "      std_dev: 1459.7976913962646\n",
      "      min: 1.0\n",
      "      median: 600.0\n",
      "      max: 85428.0\n",
      "      histograms {\n",
      "        buckets {\n",
      "          low_value: 1.0\n",
      "          high_value: 8543.7\n",
      "          sample_count: 299598.35373469896\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 8543.7\n",
      "          high_value: 17086.4\n",
      "          sample_count: 130.19091099533622\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 17086.4\n",
      "          high_value: 25629.100000000002\n",
      "          sample_count: 33.93191928821099\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 25629.100000000002\n",
      "          high_value: 34171.8\n",
      "          sample_count: 33.93191928821099\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 34171.8\n",
      "          high_value: 42714.5\n",
      "          sample_count: 33.931919288210985\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 42714.5\n",
      "          high_value: 51257.200000000004\n",
      "          sample_count: 33.93191928821101\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 51257.200000000004\n",
      "          high_value: 59799.90000000001\n",
      "          sample_count: 33.93191928821101\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 59799.90000000001\n",
      "          high_value: 68342.6\n",
      "          sample_count: 33.931919288210985\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 68342.6\n",
      "          high_value: 76885.3\n",
      "          sample_count: 33.931919288210985\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 76885.3\n",
      "          high_value: 85428.0\n",
      "          sample_count: 33.931919288210985\n",
      "        }\n",
      "      }\n",
      "      histograms {\n",
      "        buckets {\n",
      "          low_value: 1.0\n",
      "          high_value: 240.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 240.0\n",
      "          high_value: 338.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 338.0\n",
      "          high_value: 420.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 420.0\n",
      "          high_value: 493.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 493.0\n",
      "          high_value: 600.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 600.0\n",
      "          high_value: 720.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 720.0\n",
      "          high_value: 900.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 900.0\n",
      "          high_value: 1204.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 1204.0\n",
      "          high_value: 1740.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 1740.0\n",
      "          high_value: 85428.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        type: QUANTILES\n",
      "      }\n",
      "    }\n",
      "    path {\n",
      "      step: \"trip_seconds\"\n",
      "    }\n",
      "  }\n",
      "  features {\n",
      "    type: FLOAT\n",
      "    num_stats {\n",
      "      common_stats {\n",
      "        num_non_missing: 300000\n",
      "        min_num_values: 1\n",
      "        max_num_values: 1\n",
      "        avg_num_values: 1.0\n",
      "        num_values_histogram {\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          type: QUANTILES\n",
      "        }\n",
      "        tot_num_values: 300000\n",
      "      }\n",
      "      mean: 3.7525645999999995\n",
      "      std_dev: 6.032812342528166\n",
      "      min: 0.01\n",
      "      median: 1.48\n",
      "      max: 993.6\n",
      "      histograms {\n",
      "        buckets {\n",
      "          low_value: 0.01\n",
      "          high_value: 99.36900000000001\n",
      "          sample_count: 299721.33655344346\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 99.36900000000001\n",
      "          high_value: 198.728\n",
      "          sample_count: 30.962605172951072\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 198.728\n",
      "          high_value: 298.087\n",
      "          sample_count: 30.96260517295107\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 298.087\n",
      "          high_value: 397.446\n",
      "          sample_count: 30.962605172951086\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 397.446\n",
      "          high_value: 496.80500000000006\n",
      "          sample_count: 30.962605172951086\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 496.80500000000006\n",
      "          high_value: 596.164\n",
      "          sample_count: 30.962605172951047\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 596.164\n",
      "          high_value: 695.523\n",
      "          sample_count: 30.962605172951086\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 695.523\n",
      "          high_value: 794.8820000000001\n",
      "          sample_count: 30.962605172951086\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 794.8820000000001\n",
      "          high_value: 894.2410000000001\n",
      "          sample_count: 30.962605172951086\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 894.2410000000001\n",
      "          high_value: 993.6\n",
      "          sample_count: 30.962605172951047\n",
      "        }\n",
      "      }\n",
      "      histograms {\n",
      "        buckets {\n",
      "          low_value: 0.01\n",
      "          high_value: 0.5\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 0.5\n",
      "          high_value: 0.7\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 0.7\n",
      "          high_value: 0.91\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 0.91\n",
      "          high_value: 1.15\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 1.15\n",
      "          high_value: 1.48\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 1.48\n",
      "          high_value: 1.91\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 1.91\n",
      "          high_value: 2.9\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 2.9\n",
      "          high_value: 5.4\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 5.4\n",
      "          high_value: 12.5\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 12.5\n",
      "          high_value: 993.6\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        type: QUANTILES\n",
      "      }\n",
      "    }\n",
      "    path {\n",
      "      step: \"trip_miles\"\n",
      "    }\n",
      "  }\n",
      "  features {\n",
      "    type: STRING\n",
      "    string_stats {\n",
      "      common_stats {\n",
      "        num_non_missing: 300000\n",
      "        min_num_values: 1\n",
      "        max_num_values: 1\n",
      "        avg_num_values: 1.0\n",
      "        num_values_histogram {\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          type: QUANTILES\n",
      "        }\n",
      "        tot_num_values: 300000\n",
      "      }\n",
      "      unique: 8\n",
      "      top_values {\n",
      "        value: \"Credit Card\"\n",
      "        frequency: 135461.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"Cash\"\n",
      "        frequency: 134457.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"Prcard\"\n",
      "        frequency: 14032.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"Unknown\"\n",
      "        frequency: 8552.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"Mobile\"\n",
      "        frequency: 7069.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"No Charge\"\n",
      "        frequency: 298.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"Dispute\"\n",
      "        frequency: 117.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"Prepaid\"\n",
      "        frequency: 14.0\n",
      "      }\n",
      "      avg_length: 7.393226623535156\n",
      "      rank_histogram {\n",
      "        buckets {\n",
      "          label: \"Credit Card\"\n",
      "          sample_count: 135461.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 1\n",
      "          high_rank: 1\n",
      "          label: \"Cash\"\n",
      "          sample_count: 134457.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 2\n",
      "          high_rank: 2\n",
      "          label: \"Prcard\"\n",
      "          sample_count: 14032.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 3\n",
      "          high_rank: 3\n",
      "          label: \"Unknown\"\n",
      "          sample_count: 8552.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 4\n",
      "          high_rank: 4\n",
      "          label: \"Mobile\"\n",
      "          sample_count: 7069.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 5\n",
      "          high_rank: 5\n",
      "          label: \"No Charge\"\n",
      "          sample_count: 298.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 6\n",
      "          high_rank: 6\n",
      "          label: \"Dispute\"\n",
      "          sample_count: 117.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 7\n",
      "          high_rank: 7\n",
      "          label: \"Prepaid\"\n",
      "          sample_count: 14.0\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    path {\n",
      "      step: \"payment_type\"\n",
      "    }\n",
      "  }\n",
      "  features {\n",
      "    type: STRING\n",
      "    string_stats {\n",
      "      common_stats {\n",
      "        num_non_missing: 300000\n",
      "        min_num_values: 1\n",
      "        max_num_values: 1\n",
      "        avg_num_values: 1.0\n",
      "        num_values_histogram {\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          type: QUANTILES\n",
      "        }\n",
      "        tot_num_values: 300000\n",
      "      }\n",
      "      unique: 13\n",
      "      top_values {\n",
      "        value: \"POINT(-87.6 41.9)\"\n",
      "        frequency: 204512.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.7 41.9)\"\n",
      "        frequency: 30564.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.9 42)\"\n",
      "        frequency: 23333.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.7 42)\"\n",
      "        frequency: 16633.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.6 41.8)\"\n",
      "        frequency: 10653.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.8 41.8)\"\n",
      "        frequency: 4874.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.6 41.7)\"\n",
      "        frequency: 3065.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.8 42)\"\n",
      "        frequency: 2333.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.7 41.8)\"\n",
      "        frequency: 1536.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.8 41.9)\"\n",
      "        frequency: 1112.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.7 41.7)\"\n",
      "        frequency: 1108.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.6 42)\"\n",
      "        frequency: 173.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.5 41.7)\"\n",
      "        frequency: 104.0\n",
      "      }\n",
      "      avg_length: 16.716854095458984\n",
      "      rank_histogram {\n",
      "        buckets {\n",
      "          label: \"POINT(-87.6 41.9)\"\n",
      "          sample_count: 204512.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 1\n",
      "          high_rank: 1\n",
      "          label: \"POINT(-87.7 41.9)\"\n",
      "          sample_count: 30564.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 2\n",
      "          high_rank: 2\n",
      "          label: \"POINT(-87.9 42)\"\n",
      "          sample_count: 23333.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 3\n",
      "          high_rank: 3\n",
      "          label: \"POINT(-87.7 42)\"\n",
      "          sample_count: 16633.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 4\n",
      "          high_rank: 4\n",
      "          label: \"POINT(-87.6 41.8)\"\n",
      "          sample_count: 10653.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 5\n",
      "          high_rank: 5\n",
      "          label: \"POINT(-87.8 41.8)\"\n",
      "          sample_count: 4874.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 6\n",
      "          high_rank: 6\n",
      "          label: \"POINT(-87.6 41.7)\"\n",
      "          sample_count: 3065.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 7\n",
      "          high_rank: 7\n",
      "          label: \"POINT(-87.8 42)\"\n",
      "          sample_count: 2333.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 8\n",
      "          high_rank: 8\n",
      "          label: \"POINT(-87.7 41.8)\"\n",
      "          sample_count: 1536.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 9\n",
      "          high_rank: 9\n",
      "          label: \"POINT(-87.8 41.9)\"\n",
      "          sample_count: 1112.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 10\n",
      "          high_rank: 10\n",
      "          label: \"POINT(-87.7 41.7)\"\n",
      "          sample_count: 1108.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 11\n",
      "          high_rank: 11\n",
      "          label: \"POINT(-87.6 42)\"\n",
      "          sample_count: 173.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 12\n",
      "          high_rank: 12\n",
      "          label: \"POINT(-87.5 41.7)\"\n",
      "          sample_count: 104.0\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    path {\n",
      "      step: \"pickup_grid\"\n",
      "    }\n",
      "  }\n",
      "  features {\n",
      "    type: STRING\n",
      "    string_stats {\n",
      "      common_stats {\n",
      "        num_non_missing: 300000\n",
      "        min_num_values: 1\n",
      "        max_num_values: 1\n",
      "        avg_num_values: 1.0\n",
      "        num_values_histogram {\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          type: QUANTILES\n",
      "        }\n",
      "        tot_num_values: 300000\n",
      "      }\n",
      "      unique: 13\n",
      "      top_values {\n",
      "        value: \"POINT(-87.6 41.9)\"\n",
      "        frequency: 192858.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.7 41.9)\"\n",
      "        frequency: 49484.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.7 42)\"\n",
      "        frequency: 21080.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.6 41.8)\"\n",
      "        frequency: 13477.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.9 42)\"\n",
      "        frequency: 8551.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.6 41.7)\"\n",
      "        frequency: 3449.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.8 42)\"\n",
      "        frequency: 3227.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.8 41.8)\"\n",
      "        frequency: 2405.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.7 41.8)\"\n",
      "        frequency: 2124.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.8 41.9)\"\n",
      "        frequency: 1732.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.7 41.7)\"\n",
      "        frequency: 1258.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.6 42)\"\n",
      "        frequency: 279.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.5 41.7)\"\n",
      "        frequency: 76.0\n",
      "      }\n",
      "      avg_length: 16.77908706665039\n",
      "      rank_histogram {\n",
      "        buckets {\n",
      "          label: \"POINT(-87.6 41.9)\"\n",
      "          sample_count: 192858.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 1\n",
      "          high_rank: 1\n",
      "          label: \"POINT(-87.7 41.9)\"\n",
      "          sample_count: 49484.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 2\n",
      "          high_rank: 2\n",
      "          label: \"POINT(-87.7 42)\"\n",
      "          sample_count: 21080.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 3\n",
      "          high_rank: 3\n",
      "          label: \"POINT(-87.6 41.8)\"\n",
      "          sample_count: 13477.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 4\n",
      "          high_rank: 4\n",
      "          label: \"POINT(-87.9 42)\"\n",
      "          sample_count: 8551.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 5\n",
      "          high_rank: 5\n",
      "          label: \"POINT(-87.6 41.7)\"\n",
      "          sample_count: 3449.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 6\n",
      "          high_rank: 6\n",
      "          label: \"POINT(-87.8 42)\"\n",
      "          sample_count: 3227.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 7\n",
      "          high_rank: 7\n",
      "          label: \"POINT(-87.8 41.8)\"\n",
      "          sample_count: 2405.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 8\n",
      "          high_rank: 8\n",
      "          label: \"POINT(-87.7 41.8)\"\n",
      "          sample_count: 2124.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 9\n",
      "          high_rank: 9\n",
      "          label: \"POINT(-87.8 41.9)\"\n",
      "          sample_count: 1732.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 10\n",
      "          high_rank: 10\n",
      "          label: \"POINT(-87.7 41.7)\"\n",
      "          sample_count: 1258.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 11\n",
      "          high_rank: 11\n",
      "          label: \"POINT(-87.6 42)\"\n",
      "          sample_count: 279.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 12\n",
      "          high_rank: 12\n",
      "          label: \"POINT(-87.5 41.7)\"\n",
      "          sample_count: 76.0\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    path {\n",
      "      step: \"dropoff_grid\"\n",
      "    }\n",
      "  }\n",
      "  features {\n",
      "    type: FLOAT\n",
      "    num_stats {\n",
      "      common_stats {\n",
      "        num_non_missing: 300000\n",
      "        min_num_values: 1\n",
      "        max_num_values: 1\n",
      "        avg_num_values: 1.0\n",
      "        num_values_histogram {\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          type: QUANTILES\n",
      "        }\n",
      "        tot_num_values: 300000\n",
      "      }\n",
      "      mean: 5184.993590519638\n",
      "      std_dev: 7022.8883501735\n",
      "      num_zeros: 39721\n",
      "      median: 2234.5609530669853\n",
      "      max: 46829.87587503158\n",
      "      histograms {\n",
      "        buckets {\n",
      "          high_value: 4682.9875875031585\n",
      "          sample_count: 212528.58146283822\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 4682.9875875031585\n",
      "          high_value: 9365.975175006317\n",
      "          sample_count: 35025.34009670891\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 9365.975175006317\n",
      "          high_value: 14048.962762509476\n",
      "          sample_count: 14246.205940595954\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 14048.962762509476\n",
      "          high_value: 18731.950350012634\n",
      "          sample_count: 10412.161808728388\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 18731.950350012634\n",
      "          high_value: 23414.93793751579\n",
      "          sample_count: 9371.146364299313\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 23414.93793751579\n",
      "          high_value: 28097.925525018953\n",
      "          sample_count: 17781.481795807773\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 28097.925525018953\n",
      "          high_value: 32780.91311252211\n",
      "          sample_count: 323.2162123677731\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 32780.91311252211\n",
      "          high_value: 37463.90070002527\n",
      "          sample_count: 109.15250640712766\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 37463.90070002527\n",
      "          high_value: 42146.888287528425\n",
      "          sample_count: 101.35690612327183\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 42146.888287528425\n",
      "          high_value: 46829.87587503158\n",
      "          sample_count: 101.35690612327183\n",
      "        }\n",
      "      }\n",
      "      histograms {\n",
      "        buckets {\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          high_value: 890.4416872336377\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 890.4416872336377\n",
      "          high_value: 1212.7148908059676\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 1212.7148908059676\n",
      "          high_value: 1700.4244612651905\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 1700.4244612651905\n",
      "          high_value: 2234.5609530669853\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 2234.5609530669853\n",
      "          high_value: 2895.076560337729\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 2895.076560337729\n",
      "          high_value: 4343.772678920504\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 4343.772678920504\n",
      "          high_value: 7701.91615150836\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 7701.91615150836\n",
      "          high_value: 16747.42171942631\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 16747.42171942631\n",
      "          high_value: 46829.87587503158\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        type: QUANTILES\n",
      "      }\n",
      "    }\n",
      "    path {\n",
      "      step: \"euclidean\"\n",
      "    }\n",
      "  }\n",
      "  features {\n",
      "    type: STRING\n",
      "    string_stats {\n",
      "      common_stats {\n",
      "        num_non_missing: 300000\n",
      "        min_num_values: 1\n",
      "        max_num_values: 1\n",
      "        avg_num_values: 1.0\n",
      "        num_values_histogram {\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          type: QUANTILES\n",
      "        }\n",
      "        tot_num_values: 300000\n",
      "      }\n",
      "      unique: 149\n",
      "      top_values {\n",
      "        value: \"POINT(-87.6 41.9)POINT(-87.6 41.9)\"\n",
      "        frequency: 160386.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.6 41.9)POINT(-87.7 41.9)\"\n",
      "        frequency: 25891.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.9 42)POINT(-87.6 41.9)\"\n",
      "        frequency: 13136.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.7 41.9)POINT(-87.7 41.9)\"\n",
      "        frequency: 12765.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.7 42)POINT(-87.7 42)\"\n",
      "        frequency: 10384.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.7 41.9)POINT(-87.6 41.9)\"\n",
      "        frequency: 9917.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.6 41.9)POINT(-87.9 42)\"\n",
      "        frequency: 5092.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.6 41.9)POINT(-87.6 41.8)\"\n",
      "        frequency: 4911.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.9 42)POINT(-87.7 41.9)\"\n",
      "        frequency: 4667.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.6 41.9)POINT(-87.7 42)\"\n",
      "        frequency: 4420.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.6 41.8)POINT(-87.6 41.8)\"\n",
      "        frequency: 4099.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.7 41.9)POINT(-87.7 42)\"\n",
      "        frequency: 3340.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.6 41.8)POINT(-87.6 41.9)\"\n",
      "        frequency: 3088.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.8 41.8)POINT(-87.6 41.9)\"\n",
      "        frequency: 2821.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.7 42)POINT(-87.7 41.9)\"\n",
      "        frequency: 2573.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.7 42)POINT(-87.6 41.9)\"\n",
      "        frequency: 2256.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.9 42)POINT(-87.9 42)\"\n",
      "        frequency: 1987.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.9 42)POINT(-87.7 42)\"\n",
      "        frequency: 1951.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.7 41.9)POINT(-87.6 41.8)\"\n",
      "        frequency: 1694.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.6 41.9)POINT(-87.8 41.8)\"\n",
      "        frequency: 1647.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.6 41.8)POINT(-87.7 41.9)\"\n",
      "        frequency: 1248.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.8 42)POINT(-87.8 42)\"\n",
      "        frequency: 1069.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.6 41.7)POINT(-87.6 41.7)\"\n",
      "        frequency: 977.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.6 41.7)POINT(-87.6 41.8)\"\n",
      "        frequency: 964.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.6 41.8)POINT(-87.6 41.7)\"\n",
      "        frequency: 954.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.8 41.8)POINT(-87.7 41.9)\"\n",
      "        frequency: 879.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.9 42)POINT(-87.8 42)\"\n",
      "        frequency: 791.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.7 41.9)POINT(-87.9 42)\"\n",
      "        frequency: 629.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.7 41.9)POINT(-87.8 41.9)\"\n",
      "        frequency: 563.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.6 41.9)POINT(-87.6 41.7)\"\n",
      "        frequency: 519.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.6 41.9)POINT(-87.7 41.8)\"\n",
      "        frequency: 508.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.7 41.9)POINT(-87.6 41.7)\"\n",
      "        frequency: 471.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.9 42)POINT(-87.6 41.8)\"\n",
      "        frequency: 446.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.7 42)POINT(-87.8 42)\"\n",
      "        frequency: 442.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.7 41.9)POINT(-87.7 41.8)\"\n",
      "        frequency: 433.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.8 42)POINT(-87.7 42)\"\n",
      "        frequency: 421.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.7 42)POINT(-87.9 42)\"\n",
      "        frequency: 397.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.6 41.9)POINT(-87.8 42)\"\n",
      "        frequency: 394.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.7 41.8)POINT(-87.7 41.8)\"\n",
      "        frequency: 375.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.7 41.9)POINT(-87.8 42)\"\n",
      "        frequency: 369.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.6 41.7)POINT(-87.7 41.9)\"\n",
      "        frequency: 366.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.8 41.8)POINT(-87.6 41.8)\"\n",
      "        frequency: 359.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.6 41.8)POINT(-87.7 41.8)\"\n",
      "        frequency: 358.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.6 41.7)POINT(-87.6 41.9)\"\n",
      "        frequency: 344.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.8 41.9)POINT(-87.7 41.9)\"\n",
      "        frequency: 337.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.7 41.8)POINT(-87.6 41.8)\"\n",
      "        frequency: 325.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.6 41.9)POINT(-87.8 41.9)\"\n",
      "        frequency: 322.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.8 42)POINT(-87.7 41.9)\"\n",
      "        frequency: 309.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.6 41.8)POINT(-87.7 41.7)\"\n",
      "        frequency: 303.0\n",
      "      }\n",
      "      top_values {\n",
      "        value: \"POINT(-87.7 41.7)POINT(-87.6 41.8)\"\n",
      "        frequency: 297.0\n",
      "      }\n",
      "      avg_length: 33.495941162109375\n",
      "      rank_histogram {\n",
      "        buckets {\n",
      "          label: \"POINT(-87.6 41.9)POINT(-87.6 41.9)\"\n",
      "          sample_count: 160386.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 1\n",
      "          high_rank: 1\n",
      "          label: \"POINT(-87.6 41.9)POINT(-87.7 41.9)\"\n",
      "          sample_count: 25891.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 2\n",
      "          high_rank: 2\n",
      "          label: \"POINT(-87.9 42)POINT(-87.6 41.9)\"\n",
      "          sample_count: 13136.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 3\n",
      "          high_rank: 3\n",
      "          label: \"POINT(-87.7 41.9)POINT(-87.7 41.9)\"\n",
      "          sample_count: 12765.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 4\n",
      "          high_rank: 4\n",
      "          label: \"POINT(-87.7 42)POINT(-87.7 42)\"\n",
      "          sample_count: 10384.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 5\n",
      "          high_rank: 5\n",
      "          label: \"POINT(-87.7 41.9)POINT(-87.6 41.9)\"\n",
      "          sample_count: 9917.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 6\n",
      "          high_rank: 6\n",
      "          label: \"POINT(-87.6 41.9)POINT(-87.9 42)\"\n",
      "          sample_count: 5092.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 7\n",
      "          high_rank: 7\n",
      "          label: \"POINT(-87.6 41.9)POINT(-87.6 41.8)\"\n",
      "          sample_count: 4911.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 8\n",
      "          high_rank: 8\n",
      "          label: \"POINT(-87.9 42)POINT(-87.7 41.9)\"\n",
      "          sample_count: 4667.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 9\n",
      "          high_rank: 9\n",
      "          label: \"POINT(-87.6 41.9)POINT(-87.7 42)\"\n",
      "          sample_count: 4420.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 10\n",
      "          high_rank: 10\n",
      "          label: \"POINT(-87.6 41.8)POINT(-87.6 41.8)\"\n",
      "          sample_count: 4099.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 11\n",
      "          high_rank: 11\n",
      "          label: \"POINT(-87.7 41.9)POINT(-87.7 42)\"\n",
      "          sample_count: 3340.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 12\n",
      "          high_rank: 12\n",
      "          label: \"POINT(-87.6 41.8)POINT(-87.6 41.9)\"\n",
      "          sample_count: 3088.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 13\n",
      "          high_rank: 13\n",
      "          label: \"POINT(-87.8 41.8)POINT(-87.6 41.9)\"\n",
      "          sample_count: 2821.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 14\n",
      "          high_rank: 14\n",
      "          label: \"POINT(-87.7 42)POINT(-87.7 41.9)\"\n",
      "          sample_count: 2573.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 15\n",
      "          high_rank: 15\n",
      "          label: \"POINT(-87.7 42)POINT(-87.6 41.9)\"\n",
      "          sample_count: 2256.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 16\n",
      "          high_rank: 16\n",
      "          label: \"POINT(-87.9 42)POINT(-87.9 42)\"\n",
      "          sample_count: 1987.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 17\n",
      "          high_rank: 17\n",
      "          label: \"POINT(-87.9 42)POINT(-87.7 42)\"\n",
      "          sample_count: 1951.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 18\n",
      "          high_rank: 18\n",
      "          label: \"POINT(-87.7 41.9)POINT(-87.6 41.8)\"\n",
      "          sample_count: 1694.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 19\n",
      "          high_rank: 19\n",
      "          label: \"POINT(-87.6 41.9)POINT(-87.8 41.8)\"\n",
      "          sample_count: 1647.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 20\n",
      "          high_rank: 20\n",
      "          label: \"POINT(-87.6 41.8)POINT(-87.7 41.9)\"\n",
      "          sample_count: 1248.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 21\n",
      "          high_rank: 21\n",
      "          label: \"POINT(-87.8 42)POINT(-87.8 42)\"\n",
      "          sample_count: 1069.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 22\n",
      "          high_rank: 22\n",
      "          label: \"POINT(-87.6 41.7)POINT(-87.6 41.7)\"\n",
      "          sample_count: 977.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 23\n",
      "          high_rank: 23\n",
      "          label: \"POINT(-87.6 41.7)POINT(-87.6 41.8)\"\n",
      "          sample_count: 964.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 24\n",
      "          high_rank: 24\n",
      "          label: \"POINT(-87.6 41.8)POINT(-87.6 41.7)\"\n",
      "          sample_count: 954.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 25\n",
      "          high_rank: 25\n",
      "          label: \"POINT(-87.8 41.8)POINT(-87.7 41.9)\"\n",
      "          sample_count: 879.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 26\n",
      "          high_rank: 26\n",
      "          label: \"POINT(-87.9 42)POINT(-87.8 42)\"\n",
      "          sample_count: 791.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 27\n",
      "          high_rank: 27\n",
      "          label: \"POINT(-87.7 41.9)POINT(-87.9 42)\"\n",
      "          sample_count: 629.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 28\n",
      "          high_rank: 28\n",
      "          label: \"POINT(-87.7 41.9)POINT(-87.8 41.9)\"\n",
      "          sample_count: 563.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 29\n",
      "          high_rank: 29\n",
      "          label: \"POINT(-87.6 41.9)POINT(-87.6 41.7)\"\n",
      "          sample_count: 519.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 30\n",
      "          high_rank: 30\n",
      "          label: \"POINT(-87.6 41.9)POINT(-87.7 41.8)\"\n",
      "          sample_count: 508.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 31\n",
      "          high_rank: 31\n",
      "          label: \"POINT(-87.7 41.9)POINT(-87.6 41.7)\"\n",
      "          sample_count: 471.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 32\n",
      "          high_rank: 32\n",
      "          label: \"POINT(-87.9 42)POINT(-87.6 41.8)\"\n",
      "          sample_count: 446.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 33\n",
      "          high_rank: 33\n",
      "          label: \"POINT(-87.7 42)POINT(-87.8 42)\"\n",
      "          sample_count: 442.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 34\n",
      "          high_rank: 34\n",
      "          label: \"POINT(-87.7 41.9)POINT(-87.7 41.8)\"\n",
      "          sample_count: 433.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 35\n",
      "          high_rank: 35\n",
      "          label: \"POINT(-87.8 42)POINT(-87.7 42)\"\n",
      "          sample_count: 421.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 36\n",
      "          high_rank: 36\n",
      "          label: \"POINT(-87.7 42)POINT(-87.9 42)\"\n",
      "          sample_count: 397.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 37\n",
      "          high_rank: 37\n",
      "          label: \"POINT(-87.6 41.9)POINT(-87.8 42)\"\n",
      "          sample_count: 394.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 38\n",
      "          high_rank: 38\n",
      "          label: \"POINT(-87.7 41.8)POINT(-87.7 41.8)\"\n",
      "          sample_count: 375.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 39\n",
      "          high_rank: 39\n",
      "          label: \"POINT(-87.7 41.9)POINT(-87.8 42)\"\n",
      "          sample_count: 369.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 40\n",
      "          high_rank: 40\n",
      "          label: \"POINT(-87.6 41.7)POINT(-87.7 41.9)\"\n",
      "          sample_count: 366.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 41\n",
      "          high_rank: 41\n",
      "          label: \"POINT(-87.8 41.8)POINT(-87.6 41.8)\"\n",
      "          sample_count: 359.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 42\n",
      "          high_rank: 42\n",
      "          label: \"POINT(-87.6 41.8)POINT(-87.7 41.8)\"\n",
      "          sample_count: 358.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 43\n",
      "          high_rank: 43\n",
      "          label: \"POINT(-87.6 41.7)POINT(-87.6 41.9)\"\n",
      "          sample_count: 344.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 44\n",
      "          high_rank: 44\n",
      "          label: \"POINT(-87.8 41.9)POINT(-87.7 41.9)\"\n",
      "          sample_count: 337.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 45\n",
      "          high_rank: 45\n",
      "          label: \"POINT(-87.7 41.8)POINT(-87.6 41.8)\"\n",
      "          sample_count: 325.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 46\n",
      "          high_rank: 46\n",
      "          label: \"POINT(-87.6 41.9)POINT(-87.8 41.9)\"\n",
      "          sample_count: 322.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 47\n",
      "          high_rank: 47\n",
      "          label: \"POINT(-87.8 42)POINT(-87.7 41.9)\"\n",
      "          sample_count: 309.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 48\n",
      "          high_rank: 48\n",
      "          label: \"POINT(-87.6 41.8)POINT(-87.7 41.7)\"\n",
      "          sample_count: 303.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 49\n",
      "          high_rank: 49\n",
      "          label: \"POINT(-87.7 41.7)POINT(-87.6 41.8)\"\n",
      "          sample_count: 297.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 50\n",
      "          high_rank: 50\n",
      "          label: \"POINT(-87.8 41.8)POINT(-87.8 41.8)\"\n",
      "          sample_count: 284.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 51\n",
      "          high_rank: 51\n",
      "          label: \"POINT(-87.7 41.8)POINT(-87.7 41.9)\"\n",
      "          sample_count: 253.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 52\n",
      "          high_rank: 52\n",
      "          label: \"POINT(-87.7 42)POINT(-87.6 41.8)\"\n",
      "          sample_count: 237.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 53\n",
      "          high_rank: 53\n",
      "          label: \"POINT(-87.8 41.9)POINT(-87.8 41.9)\"\n",
      "          sample_count: 225.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 54\n",
      "          high_rank: 54\n",
      "          label: \"POINT(-87.7 41.8)POINT(-87.6 41.9)\"\n",
      "          sample_count: 225.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 55\n",
      "          high_rank: 55\n",
      "          label: \"POINT(-87.6 41.9)POINT(-87.6 42)\"\n",
      "          sample_count: 211.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 56\n",
      "          high_rank: 56\n",
      "          label: \"POINT(-87.7 41.7)POINT(-87.7 41.7)\"\n",
      "          sample_count: 201.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 57\n",
      "          high_rank: 57\n",
      "          label: \"POINT(-87.6 41.9)POINT(-87.7 41.7)\"\n",
      "          sample_count: 200.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 58\n",
      "          high_rank: 58\n",
      "          label: \"POINT(-87.8 42)POINT(-87.6 41.9)\"\n",
      "          sample_count: 193.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 59\n",
      "          high_rank: 59\n",
      "          label: \"POINT(-87.7 41.9)POINT(-87.8 41.8)\"\n",
      "          sample_count: 188.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 60\n",
      "          high_rank: 60\n",
      "          label: \"POINT(-87.6 41.8)POINT(-87.7 42)\"\n",
      "          sample_count: 183.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 61\n",
      "          high_rank: 61\n",
      "          label: \"POINT(-87.8 41.9)POINT(-87.6 41.9)\"\n",
      "          sample_count: 182.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 62\n",
      "          high_rank: 62\n",
      "          label: \"POINT(-87.6 41.7)POINT(-87.7 41.7)\"\n",
      "          sample_count: 175.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 63\n",
      "          high_rank: 63\n",
      "          label: \"POINT(-87.7 42)POINT(-87.8 41.9)\"\n",
      "          sample_count: 174.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 64\n",
      "          high_rank: 64\n",
      "          label: \"POINT(-87.7 41.8)POINT(-87.6 41.7)\"\n",
      "          sample_count: 174.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 65\n",
      "          high_rank: 65\n",
      "          label: \"POINT(-87.7 41.7)POINT(-87.7 41.9)\"\n",
      "          sample_count: 174.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 66\n",
      "          high_rank: 66\n",
      "          label: \"POINT(-87.7 41.9)POINT(-87.7 41.7)\"\n",
      "          sample_count: 172.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 67\n",
      "          high_rank: 67\n",
      "          label: \"POINT(-87.7 41.7)POINT(-87.6 41.7)\"\n",
      "          sample_count: 171.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 68\n",
      "          high_rank: 68\n",
      "          label: \"POINT(-87.8 41.8)POINT(-87.7 42)\"\n",
      "          sample_count: 165.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 69\n",
      "          high_rank: 69\n",
      "          label: \"POINT(-87.7 41.7)POINT(-87.6 41.9)\"\n",
      "          sample_count: 165.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 70\n",
      "          high_rank: 70\n",
      "          label: \"POINT(-87.9 42)POINT(-87.8 41.9)\"\n",
      "          sample_count: 163.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 71\n",
      "          high_rank: 71\n",
      "          label: \"POINT(-87.6 41.8)POINT(-87.9 42)\"\n",
      "          sample_count: 161.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 72\n",
      "          high_rank: 72\n",
      "          label: \"POINT(-87.8 42)POINT(-87.8 41.9)\"\n",
      "          sample_count: 157.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 73\n",
      "          high_rank: 73\n",
      "          label: \"POINT(-87.8 42)POINT(-87.9 42)\"\n",
      "          sample_count: 147.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 74\n",
      "          high_rank: 74\n",
      "          label: \"POINT(-87.6 42)POINT(-87.6 41.9)\"\n",
      "          sample_count: 139.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 75\n",
      "          high_rank: 75\n",
      "          label: \"POINT(-87.6 41.7)POINT(-87.7 41.8)\"\n",
      "          sample_count: 138.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 76\n",
      "          high_rank: 76\n",
      "          label: \"POINT(-87.8 41.8)POINT(-87.7 41.8)\"\n",
      "          sample_count: 135.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 77\n",
      "          high_rank: 77\n",
      "          label: \"POINT(-87.6 41.8)POINT(-87.8 41.8)\"\n",
      "          sample_count: 128.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 78\n",
      "          high_rank: 78\n",
      "          label: \"POINT(-87.8 41.9)POINT(-87.7 42)\"\n",
      "          sample_count: 105.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 79\n",
      "          high_rank: 79\n",
      "          label: \"POINT(-87.8 41.9)POINT(-87.8 42)\"\n",
      "          sample_count: 92.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 80\n",
      "          high_rank: 80\n",
      "          label: \"POINT(-87.7 41.8)POINT(-87.7 41.7)\"\n",
      "          sample_count: 84.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 81\n",
      "          high_rank: 81\n",
      "          label: \"POINT(-87.8 41.9)POINT(-87.6 41.8)\"\n",
      "          sample_count: 78.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 82\n",
      "          high_rank: 82\n",
      "          label: \"POINT(-87.8 41.8)POINT(-87.7 41.7)\"\n",
      "          sample_count: 78.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 83\n",
      "          high_rank: 83\n",
      "          label: \"POINT(-87.9 42)POINT(-87.8 41.8)\"\n",
      "          sample_count: 71.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 84\n",
      "          high_rank: 84\n",
      "          label: \"POINT(-87.8 41.8)POINT(-87.9 42)\"\n",
      "          sample_count: 71.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 85\n",
      "          high_rank: 85\n",
      "          label: \"POINT(-87.6 41.8)POINT(-87.8 41.9)\"\n",
      "          sample_count: 69.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 86\n",
      "          high_rank: 86\n",
      "          label: \"POINT(-87.7 42)POINT(-87.6 41.7)\"\n",
      "          sample_count: 65.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 87\n",
      "          high_rank: 87\n",
      "          label: \"POINT(-87.7 41.7)POINT(-87.7 41.8)\"\n",
      "          sample_count: 63.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 88\n",
      "          high_rank: 88\n",
      "          label: \"POINT(-87.6 41.7)POINT(-87.7 42)\"\n",
      "          sample_count: 52.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 89\n",
      "          high_rank: 89\n",
      "          label: \"POINT(-87.9 42)POINT(-87.7 41.8)\"\n",
      "          sample_count: 45.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 90\n",
      "          high_rank: 90\n",
      "          label: \"POINT(-87.7 41.8)POINT(-87.7 42)\"\n",
      "          sample_count: 42.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 91\n",
      "          high_rank: 91\n",
      "          label: \"POINT(-87.6 41.8)POINT(-87.8 42)\"\n",
      "          sample_count: 42.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 92\n",
      "          high_rank: 92\n",
      "          label: \"POINT(-87.5 41.7)POINT(-87.6 41.8)\"\n",
      "          sample_count: 42.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 93\n",
      "          high_rank: 93\n",
      "          label: \"POINT(-87.7 42)POINT(-87.8 41.8)\"\n",
      "          sample_count: 41.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 94\n",
      "          high_rank: 94\n",
      "          label: \"POINT(-87.7 42)POINT(-87.7 41.8)\"\n",
      "          sample_count: 39.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 95\n",
      "          high_rank: 95\n",
      "          label: \"POINT(-87.8 41.8)POINT(-87.6 41.7)\"\n",
      "          sample_count: 34.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 96\n",
      "          high_rank: 96\n",
      "          label: \"POINT(-87.8 41.9)POINT(-87.9 42)\"\n",
      "          sample_count: 33.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 97\n",
      "          high_rank: 97\n",
      "          label: \"POINT(-87.8 41.9)POINT(-87.6 41.7)\"\n",
      "          sample_count: 31.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 98\n",
      "          high_rank: 98\n",
      "          label: \"POINT(-87.9 42)POINT(-87.6 42)\"\n",
      "          sample_count: 30.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 99\n",
      "          high_rank: 99\n",
      "          label: \"POINT(-87.5 41.7)POINT(-87.6 41.7)\"\n",
      "          sample_count: 30.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 100\n",
      "          high_rank: 100\n",
      "          label: \"POINT(-87.7 41.8)POINT(-87.8 41.8)\"\n",
      "          sample_count: 27.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 101\n",
      "          high_rank: 101\n",
      "          label: \"POINT(-87.9 42)POINT(-87.7 41.7)\"\n",
      "          sample_count: 24.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 102\n",
      "          high_rank: 102\n",
      "          label: \"POINT(-87.8 42)POINT(-87.6 41.8)\"\n",
      "          sample_count: 24.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 103\n",
      "          high_rank: 103\n",
      "          label: \"POINT(-87.8 41.8)POINT(-87.8 42)\"\n",
      "          sample_count: 23.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 104\n",
      "          high_rank: 104\n",
      "          label: \"POINT(-87.9 42)POINT(-87.6 41.7)\"\n",
      "          sample_count: 21.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 105\n",
      "          high_rank: 105\n",
      "          label: \"POINT(-87.8 41.9)POINT(-87.7 41.8)\"\n",
      "          sample_count: 20.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 106\n",
      "          high_rank: 106\n",
      "          label: \"POINT(-87.7 41.8)POINT(-87.8 41.9)\"\n",
      "          sample_count: 20.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 107\n",
      "          high_rank: 107\n",
      "          label: \"POINT(-87.6 41.8)POINT(-87.5 41.7)\"\n",
      "          sample_count: 19.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 108\n",
      "          high_rank: 108\n",
      "          label: \"POINT(-87.7 41.9)POINT(-87.5 41.7)\"\n",
      "          sample_count: 17.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 109\n",
      "          high_rank: 109\n",
      "          label: \"POINT(-87.6 41.7)POINT(-87.8 41.9)\"\n",
      "          sample_count: 16.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 110\n",
      "          high_rank: 110\n",
      "          label: \"POINT(-87.8 41.8)POINT(-87.8 41.9)\"\n",
      "          sample_count: 14.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 111\n",
      "          high_rank: 111\n",
      "          label: \"POINT(-87.7 42)POINT(-87.7 41.7)\"\n",
      "          sample_count: 13.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 112\n",
      "          high_rank: 112\n",
      "          label: \"POINT(-87.7 41.7)POINT(-87.7 42)\"\n",
      "          sample_count: 13.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 113\n",
      "          high_rank: 113\n",
      "          label: \"POINT(-87.6 42)POINT(-87.6 42)\"\n",
      "          sample_count: 13.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 114\n",
      "          high_rank: 114\n",
      "          label: \"POINT(-87.6 41.7)POINT(-87.5 41.7)\"\n",
      "          sample_count: 13.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 115\n",
      "          high_rank: 115\n",
      "          label: \"POINT(-87.5 41.7)POINT(-87.7 41.9)\"\n",
      "          sample_count: 13.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 116\n",
      "          high_rank: 116\n",
      "          label: \"POINT(-87.7 42)POINT(-87.6 42)\"\n",
      "          sample_count: 11.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 117\n",
      "          high_rank: 117\n",
      "          label: \"POINT(-87.6 41.9)POINT(-87.5 41.7)\"\n",
      "          sample_count: 11.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 118\n",
      "          high_rank: 118\n",
      "          label: \"POINT(-87.6 41.7)POINT(-87.9 42)\"\n",
      "          sample_count: 10.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 119\n",
      "          high_rank: 119\n",
      "          label: \"POINT(-87.7 41.7)POINT(-87.9 42)\"\n",
      "          sample_count: 9.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 120\n",
      "          high_rank: 120\n",
      "          label: \"POINT(-87.7 41.7)POINT(-87.8 41.9)\"\n",
      "          sample_count: 9.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 121\n",
      "          high_rank: 121\n",
      "          label: \"POINT(-87.6 42)POINT(-87.7 41.9)\"\n",
      "          sample_count: 9.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 122\n",
      "          high_rank: 122\n",
      "          label: \"POINT(-87.6 41.7)POINT(-87.8 41.8)\"\n",
      "          sample_count: 9.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 123\n",
      "          high_rank: 123\n",
      "          label: \"POINT(-87.8 42)POINT(-87.7 41.8)\"\n",
      "          sample_count: 8.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 124\n",
      "          high_rank: 124\n",
      "          label: \"POINT(-87.8 41.8)POINT(-87.6 42)\"\n",
      "          sample_count: 7.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 125\n",
      "          high_rank: 125\n",
      "          label: \"POINT(-87.7 41.8)POINT(-87.9 42)\"\n",
      "          sample_count: 7.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 126\n",
      "          high_rank: 126\n",
      "          label: \"POINT(-87.5 41.7)POINT(-87.5 41.7)\"\n",
      "          sample_count: 7.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 127\n",
      "          high_rank: 127\n",
      "          label: \"POINT(-87.8 41.9)POINT(-87.7 41.7)\"\n",
      "          sample_count: 6.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 128\n",
      "          high_rank: 128\n",
      "          label: \"POINT(-87.7 41.9)POINT(-87.6 42)\"\n",
      "          sample_count: 6.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 129\n",
      "          high_rank: 129\n",
      "          label: \"POINT(-87.6 42)POINT(-87.9 42)\"\n",
      "          sample_count: 6.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 130\n",
      "          high_rank: 130\n",
      "          label: \"POINT(-87.5 41.7)POINT(-87.6 41.9)\"\n",
      "          sample_count: 6.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 131\n",
      "          high_rank: 131\n",
      "          label: \"POINT(-87.8 41.8)POINT(-87.5 41.7)\"\n",
      "          sample_count: 4.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 132\n",
      "          high_rank: 132\n",
      "          label: \"POINT(-87.7 41.8)POINT(-87.8 42)\"\n",
      "          sample_count: 4.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 133\n",
      "          high_rank: 133\n",
      "          label: \"POINT(-87.7 41.7)POINT(-87.8 41.8)\"\n",
      "          sample_count: 4.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 134\n",
      "          high_rank: 134\n",
      "          label: \"POINT(-87.6 42)POINT(-87.7 42)\"\n",
      "          sample_count: 4.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 135\n",
      "          high_rank: 135\n",
      "          label: \"POINT(-87.8 41.9)POINT(-87.8 41.8)\"\n",
      "          sample_count: 3.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 136\n",
      "          high_rank: 136\n",
      "          label: \"POINT(-87.8 42)POINT(-87.8 41.8)\"\n",
      "          sample_count: 2.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 137\n",
      "          high_rank: 137\n",
      "          label: \"POINT(-87.8 42)POINT(-87.6 41.7)\"\n",
      "          sample_count: 2.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 138\n",
      "          high_rank: 138\n",
      "          label: \"POINT(-87.7 41.7)POINT(-87.5 41.7)\"\n",
      "          sample_count: 2.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 139\n",
      "          high_rank: 139\n",
      "          label: \"POINT(-87.5 41.7)POINT(-87.9 42)\"\n",
      "          sample_count: 2.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 140\n",
      "          high_rank: 140\n",
      "          label: \"POINT(-87.5 41.7)POINT(-87.7 41.8)\"\n",
      "          sample_count: 2.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 141\n",
      "          high_rank: 141\n",
      "          label: \"POINT(-87.5 41.7)POINT(-87.7 41.7)\"\n",
      "          sample_count: 2.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 142\n",
      "          high_rank: 142\n",
      "          label: \"POINT(-87.9 42)POINT(-87.5 41.7)\"\n",
      "          sample_count: 1.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 143\n",
      "          high_rank: 143\n",
      "          label: \"POINT(-87.8 42)POINT(-87.5 41.7)\"\n",
      "          sample_count: 1.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 144\n",
      "          high_rank: 144\n",
      "          label: \"POINT(-87.7 42)POINT(-87.5 41.7)\"\n",
      "          sample_count: 1.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 145\n",
      "          high_rank: 145\n",
      "          label: \"POINT(-87.6 42)POINT(-87.8 41.8)\"\n",
      "          sample_count: 1.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 146\n",
      "          high_rank: 146\n",
      "          label: \"POINT(-87.6 42)POINT(-87.6 41.8)\"\n",
      "          sample_count: 1.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 147\n",
      "          high_rank: 147\n",
      "          label: \"POINT(-87.6 41.8)POINT(-87.6 42)\"\n",
      "          sample_count: 1.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_rank: 148\n",
      "          high_rank: 148\n",
      "          label: \"POINT(-87.6 41.7)POINT(-87.8 42)\"\n",
      "          sample_count: 1.0\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    path {\n",
      "      step: \"loc_cross\"\n",
      "    }\n",
      "  }\n",
      "  features {\n",
      "    num_stats {\n",
      "      common_stats {\n",
      "        num_non_missing: 300000\n",
      "        min_num_values: 1\n",
      "        max_num_values: 1\n",
      "        avg_num_values: 1.0\n",
      "        num_values_histogram {\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          buckets {\n",
      "            low_value: 1.0\n",
      "            high_value: 1.0\n",
      "            sample_count: 30000.0\n",
      "          }\n",
      "          type: QUANTILES\n",
      "        }\n",
      "        tot_num_values: 300000\n",
      "      }\n",
      "      mean: 0.36504\n",
      "      std_dev: 0.48144137587041685\n",
      "      num_zeros: 190488\n",
      "      max: 1.0\n",
      "      histograms {\n",
      "        buckets {\n",
      "          high_value: 0.1\n",
      "          sample_count: 190230.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 0.1\n",
      "          high_value: 0.2\n",
      "          sample_count: 30.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 0.2\n",
      "          high_value: 0.30000000000000004\n",
      "          sample_count: 30.00000000000001\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 0.30000000000000004\n",
      "          high_value: 0.4\n",
      "          sample_count: 29.999999999999993\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 0.4\n",
      "          high_value: 0.5\n",
      "          sample_count: 29.999999999999993\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 0.5\n",
      "          high_value: 0.6000000000000001\n",
      "          sample_count: 30.00000000000003\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 0.6000000000000001\n",
      "          high_value: 0.7000000000000001\n",
      "          sample_count: 29.999999999999993\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 0.7000000000000001\n",
      "          high_value: 0.8\n",
      "          sample_count: 29.999999999999993\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 0.8\n",
      "          high_value: 0.9\n",
      "          sample_count: 29.999999999999993\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 0.9\n",
      "          high_value: 1.0\n",
      "          sample_count: 109530.0\n",
      "        }\n",
      "      }\n",
      "      histograms {\n",
      "        buckets {\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          high_value: 1.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 1.0\n",
      "          high_value: 1.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 1.0\n",
      "          high_value: 1.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        buckets {\n",
      "          low_value: 1.0\n",
      "          high_value: 1.0\n",
      "          sample_count: 30000.0\n",
      "        }\n",
      "        type: QUANTILES\n",
      "      }\n",
      "    }\n",
      "    path {\n",
      "      step: \"tip_bin\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stats = tfdv.generate_statistics_from_dataframe(\n",
    "    dataframe=dataframe,\n",
    "    stats_options=tfdv.StatsOptions(\n",
    "        label_feature=\"tip_bin\", sample_rate=1, num_top_values=50\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfdv_visualize_stats"
   },
   "source": [
    "### Visualize dataset statistics\n",
    "\n",
    "A visualization of the dataset statistics can be displayed using the TFDV `visualize_statistics()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "id": "tfdv_visualize_stats",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id='facets-iframe' width=\"100%\" height=\"500px\"></iframe>\n",
       "        <script>\n",
       "        facets_iframe = document.getElementById('facets-iframe');\n",
       "        facets_html = '<script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"><\\/script><link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/master/facets-dist/facets-jupyter.html\"><facets-overview proto-input=\"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\"></facets-overview>';\n",
       "        facets_iframe.srcdoc = facets_html;\n",
       "         facets_iframe.id = \"\";\n",
       "         setTimeout(() => {\n",
       "           facets_iframe.setAttribute('height', facets_iframe.contentWindow.document.body.offsetHeight + 'px')\n",
       "         }, 1500)\n",
       "         </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfdv.visualize_statistics(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stats_extract_features"
   },
   "source": [
    "### Extract feature grouping from statistics\n",
    "\n",
    "Next, you extract from the statistics the feature names and data types, from which you group features into:\n",
    "\n",
    "- numeric: float\n",
    "- categorical: string, int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "stats_extract_features"
   },
   "outputs": [],
   "source": [
    "NUMERIC_FEATURES = []\n",
    "CATEGORICAL_FEATURES = []\n",
    "for _ in range(len(stats.datasets[0].features)):\n",
    "    if stats.datasets[0].features[_].path.step[0] == label_column:\n",
    "        continue\n",
    "    if stats.datasets[0].features[_].type == 0:  # int\n",
    "        CATEGORICAL_FEATURES.append(stats.datasets[0].features[_].path.step[0])\n",
    "    elif stats.datasets[0].features[_].type == 1:  # float\n",
    "        NUMERIC_FEATURES.append(stats.datasets[0].features[_].path.step[0])\n",
    "    elif stats.datasets[0].features[_].type == 2:  # string\n",
    "        CATEGORICAL_FEATURES.append(stats.datasets[0].features[_].path.step[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "retain_feature_info:tabular"
   },
   "source": [
    "### Retain feature column information\n",
    "\n",
    "Next, you retain information on the feature columns in the dataset. In this example, you add this user-defined metadata as a JSON file to the Cloud Storage bucket you associated with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "retain_feature_info:tabular"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "metadata = {\n",
    "    \"label_column\": label_column,\n",
    "    \"numeric_features\": NUMERIC_FEATURES,\n",
    "    \"categorical_features\": CATEGORICAL_FEATURES,\n",
    "}\n",
    "with tf.io.gfile.GFile(\n",
    "    \"gs://\" + dataset.labels[\"user_metadata\"] + \"/metadata.jsonl\", \"w\"\n",
    ") as f:\n",
    "    json.dump(metadata, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfdv_stats:write"
   },
   "source": [
    "#### Retain statistics for the dataset\n",
    "\n",
    "Next, you write the statistics for the dataset to the dataset's Cloud Storage bucket, and retain the Cloud Storage location of the statistics file. In this example, you add it to the user-defined metadata for this dataset, which is stored in the dataset's Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "tfdv_stats:write"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"label_column\": \"tip_bin\", \"numeric_features\": [\"trip_seconds\", \"trip_miles\", \"euclidean\"], \"categorical_features\": [\"trip_month\", \"trip_day\", \"trip_day_of_week\", \"trip_hour\", \"payment_type\", \"pickup_grid\", \"dropoff_grid\", \"loc_cross\"], \"statistics\": \"gs://vertex-ai-devaip-20220303091526/statistics.jsonl\"}"
     ]
    }
   ],
   "source": [
    "STATISTICS_SCHEMA = BUCKET_NAME + \"/statistics.jsonl\"\n",
    "\n",
    "tfdv.write_stats_text(stats, BUCKET_NAME + \"/statistics.jsonl\")\n",
    "\n",
    "with tf.io.gfile.GFile(\n",
    "    \"gs://\" + dataset.labels[\"user_metadata\"] + \"/metadata.jsonl\", \"r\"\n",
    ") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "metadata[\"statistics\"] = STATISTICS_SCHEMA\n",
    "with tf.io.gfile.GFile(\n",
    "    \"gs://\" + dataset.labels[\"user_metadata\"] + \"/metadata.jsonl\", \"w\"\n",
    ") as f:\n",
    "    json.dump(metadata, f)\n",
    "\n",
    "!gsutil cat $BUCKET_NAME/metadata.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfdv_schema"
   },
   "source": [
    "###  Generate the raw data schema\n",
    "\n",
    "Generate the data schema on the dataset with the TensorFlow Data Validation (TFDV) package. Use the `infer_schema()` method, with the following parameters:\n",
    "\n",
    "- `statistics`: The statistics generated by TFDV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "id": "tfdv_schema",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature {\n",
      "  name: \"trip_month\"\n",
      "  type: INT\n",
      "  presence {\n",
      "    min_fraction: 1.0\n",
      "    min_count: 1\n",
      "  }\n",
      "  shape {\n",
      "    dim {\n",
      "      size: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"trip_day\"\n",
      "  type: INT\n",
      "  presence {\n",
      "    min_fraction: 1.0\n",
      "    min_count: 1\n",
      "  }\n",
      "  shape {\n",
      "    dim {\n",
      "      size: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"trip_day_of_week\"\n",
      "  type: INT\n",
      "  presence {\n",
      "    min_fraction: 1.0\n",
      "    min_count: 1\n",
      "  }\n",
      "  shape {\n",
      "    dim {\n",
      "      size: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"trip_hour\"\n",
      "  type: INT\n",
      "  presence {\n",
      "    min_fraction: 1.0\n",
      "    min_count: 1\n",
      "  }\n",
      "  shape {\n",
      "    dim {\n",
      "      size: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"trip_seconds\"\n",
      "  type: FLOAT\n",
      "  presence {\n",
      "    min_fraction: 1.0\n",
      "    min_count: 1\n",
      "  }\n",
      "  shape {\n",
      "    dim {\n",
      "      size: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"trip_miles\"\n",
      "  type: FLOAT\n",
      "  presence {\n",
      "    min_fraction: 1.0\n",
      "    min_count: 1\n",
      "  }\n",
      "  shape {\n",
      "    dim {\n",
      "      size: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"payment_type\"\n",
      "  type: BYTES\n",
      "  domain: \"payment_type\"\n",
      "  presence {\n",
      "    min_fraction: 1.0\n",
      "    min_count: 1\n",
      "  }\n",
      "  shape {\n",
      "    dim {\n",
      "      size: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"pickup_grid\"\n",
      "  type: BYTES\n",
      "  domain: \"pickup_grid\"\n",
      "  presence {\n",
      "    min_fraction: 1.0\n",
      "    min_count: 1\n",
      "  }\n",
      "  shape {\n",
      "    dim {\n",
      "      size: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"dropoff_grid\"\n",
      "  type: BYTES\n",
      "  domain: \"dropoff_grid\"\n",
      "  presence {\n",
      "    min_fraction: 1.0\n",
      "    min_count: 1\n",
      "  }\n",
      "  shape {\n",
      "    dim {\n",
      "      size: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"euclidean\"\n",
      "  type: FLOAT\n",
      "  presence {\n",
      "    min_fraction: 1.0\n",
      "    min_count: 1\n",
      "  }\n",
      "  shape {\n",
      "    dim {\n",
      "      size: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"loc_cross\"\n",
      "  type: BYTES\n",
      "  presence {\n",
      "    min_fraction: 1.0\n",
      "    min_count: 1\n",
      "  }\n",
      "  shape {\n",
      "    dim {\n",
      "      size: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"tip_bin\"\n",
      "  type: INT\n",
      "  bool_domain {\n",
      "  }\n",
      "  presence {\n",
      "    min_fraction: 1.0\n",
      "    min_count: 1\n",
      "  }\n",
      "  shape {\n",
      "    dim {\n",
      "      size: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "string_domain {\n",
      "  name: \"payment_type\"\n",
      "  value: \"Cash\"\n",
      "  value: \"Credit Card\"\n",
      "  value: \"Dispute\"\n",
      "  value: \"Mobile\"\n",
      "  value: \"No Charge\"\n",
      "  value: \"Prcard\"\n",
      "  value: \"Prepaid\"\n",
      "  value: \"Unknown\"\n",
      "}\n",
      "string_domain {\n",
      "  name: \"pickup_grid\"\n",
      "  value: \"POINT(-87.5 41.7)\"\n",
      "  value: \"POINT(-87.6 41.7)\"\n",
      "  value: \"POINT(-87.6 41.8)\"\n",
      "  value: \"POINT(-87.6 41.9)\"\n",
      "  value: \"POINT(-87.6 42)\"\n",
      "  value: \"POINT(-87.7 41.7)\"\n",
      "  value: \"POINT(-87.7 41.8)\"\n",
      "  value: \"POINT(-87.7 41.9)\"\n",
      "  value: \"POINT(-87.7 42)\"\n",
      "  value: \"POINT(-87.8 41.8)\"\n",
      "  value: \"POINT(-87.8 41.9)\"\n",
      "  value: \"POINT(-87.8 42)\"\n",
      "  value: \"POINT(-87.9 42)\"\n",
      "}\n",
      "string_domain {\n",
      "  name: \"dropoff_grid\"\n",
      "  value: \"POINT(-87.5 41.7)\"\n",
      "  value: \"POINT(-87.6 41.7)\"\n",
      "  value: \"POINT(-87.6 41.8)\"\n",
      "  value: \"POINT(-87.6 41.9)\"\n",
      "  value: \"POINT(-87.6 42)\"\n",
      "  value: \"POINT(-87.7 41.7)\"\n",
      "  value: \"POINT(-87.7 41.8)\"\n",
      "  value: \"POINT(-87.7 41.9)\"\n",
      "  value: \"POINT(-87.7 42)\"\n",
      "  value: \"POINT(-87.8 41.8)\"\n",
      "  value: \"POINT(-87.8 41.9)\"\n",
      "  value: \"POINT(-87.8 42)\"\n",
      "  value: \"POINT(-87.9 42)\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = tfdv.infer_schema(statistics=stats)\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfdv_schema:save"
   },
   "source": [
    "#### Save schema for the dataset to Cloud Storage\n",
    "\n",
    "Next, you write the schema for the dataset to the dataset's Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "tfdv_schema:save"
   },
   "outputs": [],
   "source": [
    "SCHEMA_LOCATION = BUCKET_NAME + \"/schema.txt\"\n",
    "\n",
    "# When running Apache Beam directly (file is directly accessed)\n",
    "tfdv.write_schema_text(output_path=SCHEMA_LOCATION, schema=schema)\n",
    "# When running with Dataflow (file is uploaded to worker pool)\n",
    "tfdv.write_schema_text(output_path=\"schema.txt\", schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfdv_schema:write"
   },
   "source": [
    "#### Retain schema for the dataset\n",
    "\n",
    "Next, you retain the Cloud Storage location of the schema file. In this example, you add it to the user-defined metadata for this dataset, which is stored in the dataset's Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "tfdv_schema:write"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"label_column\": \"tip_bin\", \"numeric_features\": [\"trip_seconds\", \"trip_miles\", \"euclidean\"], \"categorical_features\": [\"trip_month\", \"trip_day\", \"trip_day_of_week\", \"trip_hour\", \"payment_type\", \"pickup_grid\", \"dropoff_grid\", \"loc_cross\"], \"statistics\": \"gs://vertex-ai-devaip-20220303091526/statistics.jsonl\", \"schema\": \"gs://vertex-ai-devaip-20220303091526/schema.txt\"}"
     ]
    }
   ],
   "source": [
    "with tf.io.gfile.GFile(\n",
    "    \"gs://\" + dataset.labels[\"user_metadata\"] + \"/metadata.jsonl\", \"r\"\n",
    ") as f:\n",
    "    metadata = json.load(f)\n",
    "metadata[\"schema\"] = SCHEMA_LOCATION\n",
    "\n",
    "with tf.io.gfile.GFile(\n",
    "    \"gs://\" + dataset.labels[\"user_metadata\"] + \"/metadata.jsonl\", \"w\"\n",
    ") as f:\n",
    "    json.dump(metadata, f)\n",
    "\n",
    "!gsutil cat $BUCKET_NAME/metadata.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tft_feature_spec"
   },
   "source": [
    "### Generate the feature specification\n",
    "\n",
    "Generate the feature specification, compatible with TFRecords, on the dataset with the TensorFlow Transform (TFT) package. Use the `schema_as_feature_spec()` method, with the following parameters:\n",
    "\n",
    "- `schema`: The data schema generated by TFDV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "tft_feature_spec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'trip_month': FixedLenFeature(shape=[1], dtype=tf.int64, default_value=None), 'trip_day': FixedLenFeature(shape=[1], dtype=tf.int64, default_value=None), 'trip_day_of_week': FixedLenFeature(shape=[1], dtype=tf.int64, default_value=None), 'trip_hour': FixedLenFeature(shape=[1], dtype=tf.int64, default_value=None), 'trip_seconds': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None), 'trip_miles': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None), 'payment_type': FixedLenFeature(shape=[1], dtype=tf.string, default_value=None), 'pickup_grid': FixedLenFeature(shape=[1], dtype=tf.string, default_value=None), 'dropoff_grid': FixedLenFeature(shape=[1], dtype=tf.string, default_value=None), 'euclidean': FixedLenFeature(shape=[1], dtype=tf.float32, default_value=None), 'loc_cross': FixedLenFeature(shape=[1], dtype=tf.string, default_value=None), 'tip_bin': FixedLenFeature(shape=[1], dtype=tf.int64, default_value=None)}\n"
     ]
    }
   ],
   "source": [
    "feature_spec = tft.tf_metadata.schema_utils.schema_as_feature_spec(schema).feature_spec\n",
    "\n",
    "print(feature_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataflow_setup:transform"
   },
   "source": [
    "#### Prepare package requirements for Dataflow job.\n",
    "\n",
    "Before you can run a Dataflow job, you need to specify the package requirements for the worker pool that will execute the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "dataflow_setup:transform"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile setup.py\n",
    "import setuptools\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    \"google-cloud-aiplatform==1.4.2\",\n",
    "    \"tensorflow-transform==1.2.0\",\n",
    "    \"tensorflow-data-validation==1.2.0\",\n",
    "]\n",
    "\n",
    "setuptools.setup(\n",
    "    name=\"executor\",\n",
    "    version=\"0.0.1\",\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=setuptools.find_packages(),\n",
    "    include_package_data=True,\n",
    "    package_data={\"./\": [\"schema.txt\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataflow:preprocess,chicago"
   },
   "source": [
    "#### Create preprocessing function\n",
    "\n",
    "Next, you create a preprocessing function specific to your dataset. In this example, you write the preprocessing function to a separate python module and add a __init__.py to make it appear as a package. Why? When you run the Apache beam pipeline in Dataflow, your scripts are ran across one or more workers. The preprocessing function runs in a separate worker than the pipeline, and thus does not contain the run-time of the pipeline, like values of global variables. To resolve this, you hard-code all the dependencies and values into the preprocesing package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "dataflow:preprocess,chicago"
   },
   "outputs": [],
   "source": [
    "! rm -rf src\n",
    "! mkdir src\n",
    "! touch src/__init__.py\n",
    "\n",
    "with open(\"src/features.py\", \"w\") as f:\n",
    "    f.write(\"import tensorflow as tf\\n\")\n",
    "    f.write(\"import tensorflow_transform as tft\\n\")\n",
    "\n",
    "    f.write(\"def preprocessing_fn(inputs):\\n\")\n",
    "    f.write(\"\toutputs = {}\\n\")\n",
    "    f.write(\"\tfor key in inputs.keys():\\n\")\n",
    "    f.write(f\"\t\tif key in {NUMERIC_FEATURES}:\\n\")\n",
    "    f.write(\"\t\t\toutputs[key] = tft.scale_to_z_score(inputs[key])\\n\")\n",
    "    f.write(f\"\t\telif key in {CATEGORICAL_FEATURES}:\\n\")\n",
    "    f.write(\"\t\t\toutputs[key] = tft.compute_and_apply_vocabulary(\\n\")\n",
    "    f.write(\"\t\t\t\tinputs[key],\\n\")\n",
    "    f.write(\"\t\t\t\tnum_oov_buckets=1,\\n\")\n",
    "    f.write(\"\t\t\t\tvocab_filename=key,\\n\")\n",
    "    f.write(\"\t\t\t)\\n\")\n",
    "    f.write(\"\t\telse:\\n\")\n",
    "    f.write(\"\t\t\toutputs[key] = inputs[key]\\n\")\n",
    "    f.write(\"\t\toutputs[key] = tf.squeeze(outputs[key], -1)\\n\")\n",
    "    f.write(\"\treturn outputs\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataflow:transform,bq,chicago"
   },
   "source": [
    "### Preprocess data with Dataflow\n",
    "\n",
    "#### Data Preprocessing\n",
    "\n",
    "Next, you preprocess the data using Dataflow. In this example, you query the BigQuery table and split the examples into training, validation and test (eval) datasets and preprocess feature columns:\n",
    "\n",
    "- `Numeric`: Rescale the values with `tft.scale_to_z_score`.\n",
    "- `Categorical`: Encode as a categorical column with `tft.compute_and_apply_vocabulary`.\n",
    "\n",
    "In addition to the preprocessed (transformed) data, raw versions of the test data are generated in both tf.Example and JSONL format. The tranform artifacts are stored as well to be used by subsequent serving function for transforming raw data into transformed data.\n",
    "\n",
    "In summary, the outputs produced are:\n",
    "\n",
    "- transformed training data\n",
    "- transformed validation data\n",
    "- tranformed test dats\n",
    "- raw test data as JSONL\n",
    "- raw test data as tf.Example\n",
    "- transform function artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "id": "dataflow:transform,bq,chicago",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing started...\n",
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:2437: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  temp_location = pcoll.pipeline.options.view_as(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n",
      "INFO:absl:Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_transform/tf_utils.py:261: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-03 09:17:21.864384: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-03-03 09:17:21.864430: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-03-03 09:17:21.864459: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (vertex-ai-sdk-tests): /proc/driver/nvidia/version does not exist\n",
      "2022-03-03 09:17:21.864731: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n",
      "INFO:absl:Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-03 09:17:44.125390: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-03-03 09:17:44.126153: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2199995000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n",
      "INFO:absl:Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n",
      "INFO:absl:Feature dropoff_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature euclidean has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loc_cross has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature payment_type has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature pickup_grid has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature tip_bin has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_day_of_week has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_hour has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_miles has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_month has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature trip_seconds has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/notebook_testing_sudarshan/myenv45/bin/python', 'setup.py', 'sdist', '--dist-dir', '/tmp/tmpdr7vm2n7']\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/notebook_testing_sudarshan/myenv45/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpdr7vm2n7', 'apache-beam==2.36.0', '--no-deps', '--no-binary', ':all:']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: check: missing required meta-data: url\n",
      "\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/notebook_testing_sudarshan/myenv45/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpdr7vm2n7', 'apache-beam==2.36.0', '--no-deps', '--only-binary', ':all:', '--python-version', '37', '--implementation', 'cp', '--abi', 'cp37m', '--platform', 'manylinux1_x86_64']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/home/jupyter/notebook_testing_sudarshan/myenv45/bin/python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI: apache_beam-2.36.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.36.0\n",
      "INFO:root:Using provided Python SDK container image: gcr.io/cloud-dataflow/v1beta3/python37:2.36.0\n",
      "INFO:root:Python SDK container image set to \"gcr.io/cloud-dataflow/v1beta3/python37:2.36.0\" for Docker environment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/home/jupyter/notebook_testing_sudarshan/myenv45/bin/python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fd03a0bc4d0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fd03a0bccb0> ====================\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Defaulting to the temp_location as staging_location: gs://vertex-ai-devaip-20220303091526/temp\n",
      "INFO:apache_beam.internal.gcp.auth:Setting socket default timeout to 60 seconds.\n",
      "INFO:apache_beam.internal.gcp.auth:socket default timeout is 60.0 seconds.\n",
      "INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Started BigQuery job: <JobReference\n",
      " location: 'US'\n",
      " projectId: 'vertex-ai-dev'>\n",
      " bq show -j --format=prettyjson --project_id=vertex-ai-dev None\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://vertex-ai-devaip-20220303091526/temp/beamapp-jupyter-0303091808-976492.1646299088.977573/workflow.tar.gz...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://vertex-ai-devaip-20220303091526/temp/beamapp-jupyter-0303091808-976492.1646299088.977573/workflow.tar.gz in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://vertex-ai-devaip-20220303091526/temp/beamapp-jupyter-0303091808-976492.1646299088.977573/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://vertex-ai-devaip-20220303091526/temp/beamapp-jupyter-0303091808-976492.1646299088.977573/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://vertex-ai-devaip-20220303091526/temp/beamapp-jupyter-0303091808-976492.1646299088.977573/apache_beam-2.36.0-cp37-cp37m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://vertex-ai-devaip-20220303091526/temp/beamapp-jupyter-0303091808-976492.1646299088.977573/apache_beam-2.36.0-cp37-cp37m-manylinux1_x86_64.whl in 1 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://vertex-ai-devaip-20220303091526/temp/beamapp-jupyter-0303091808-976492.1646299088.977573/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://vertex-ai-devaip-20220303091526/temp/beamapp-jupyter-0303091808-976492.1646299088.977573/pipeline.pb in 0 seconds.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'raw_data_query': 'SELECT * FROM vertex-ai-dev.chicago_taxi_trips.taxi_trips LIMIT 300000', 'label': 'tip_bin', 'transformed_data_prefix': 'gs://vertex-ai-devaip-20220303091526/transformed_data', 'transform_artifact_dir': 'gs://vertex-ai-devaip-20220303091526/transformed_artifacts', 'exported_jsonl_prefix': 'gs://vertex-ai-devaip-20220303091526/exported_data/jsonl', 'exported_tfrec_prefix': 'gs://vertex-ai-devaip-20220303091526/exported_data/tfrec'}\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding invalid overrides: {'raw_data_query': 'SELECT * FROM vertex-ai-dev.chicago_taxi_trips.taxi_trips LIMIT 300000', 'label': 'tip_bin', 'transformed_data_prefix': 'gs://vertex-ai-devaip-20220303091526/transformed_data', 'transform_artifact_dir': 'gs://vertex-ai-devaip-20220303091526/transformed_artifacts', 'exported_jsonl_prefix': 'gs://vertex-ai-devaip-20220303091526/exported_data/jsonl', 'exported_tfrec_prefix': 'gs://vertex-ai-devaip-20220303091526/exported_data/tfrec'}\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " clientRequestId: '20220303091808978925-7814'\n",
      " createTime: '2022-03-03T09:18:20.543140Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2022-03-03_01_18_20-2421235367684640617'\n",
      " location: 'us-central1'\n",
      " name: 'beamapp-jupyter-0303091808-976492'\n",
      " projectId: 'vertex-ai-dev'\n",
      " stageStates: []\n",
      " startTime: '2022-03-03T09:18:20.543140Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2022-03-03_01_18_20-2421235367684640617]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2022-03-03_01_18_20-2421235367684640617\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/us-central1/2022-03-03_01_18_20-2421235367684640617?project=vertex-ai-dev\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2022-03-03_01_18_20-2421235367684640617 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:21.319Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2022-03-03_01_18_20-2421235367684640617. The number of workers will be between 1 and 1000.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:21.379Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2022-03-03_01_18_20-2421235367684640617.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:23.834Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in us-central1-b.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:24.695Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:24.744Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write JSONL Test Data/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:24.777Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write TF Test Data/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:24.804Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write Transformed Test Data/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:24.838Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write Transformed Validation Data/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:24.909Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write Transformed Train Data/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:24.981Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:25.180Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:25.282Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:25.414Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:25.543Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:25.661Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:25.782Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:25.930Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:26.075Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:26.121Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:27.449Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:27.585Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:27.652Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/TensorSource[compute_and_apply_vocabulary_1#vocabulary]/ExtractKeys into Analyze & Transform/AnalyzeDataset/ApplySavedModel[Phase0]/ConvertToNumpy\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:27.689Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/TensorSource[compute_and_apply_vocabulary_2#vocabulary]/ExtractKeys into Analyze & Transform/AnalyzeDataset/ApplySavedModel[Phase0]/ConvertToNumpy\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:27.722Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/TensorSource[compute_and_apply_vocabulary_3#vocabulary]/ExtractKeys into Analyze & Transform/AnalyzeDataset/ApplySavedModel[Phase0]/ConvertToNumpy\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:27.756Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/TensorSource[compute_and_apply_vocabulary_4#vocabulary]/ExtractKeys into Analyze & Transform/AnalyzeDataset/ApplySavedModel[Phase0]/ConvertToNumpy\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:27.793Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/KeyWithVoid into Analyze & Transform/AnalyzeDataset/ApplySavedModel[Phase0]/ConvertToNumpy\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:27.814Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/TensorSource[compute_and_apply_vocabulary_5#vocabulary]/ExtractKeys into Analyze & Transform/AnalyzeDataset/ApplySavedModel[Phase0]/ConvertToNumpy\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:27.841Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/TensorSource[compute_and_apply_vocabulary_6#vocabulary]/ExtractKeys into Analyze & Transform/AnalyzeDataset/ApplySavedModel[Phase0]/ConvertToNumpy\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:27.905Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/TensorSource[compute_and_apply_vocabulary_7#vocabulary]/ExtractKeys into Analyze & Transform/AnalyzeDataset/ApplySavedModel[Phase0]/ConvertToNumpy\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:27.933Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/TensorSource[compute_and_apply_vocabulary#vocabulary]/ExtractKeys into Analyze & Transform/AnalyzeDataset/ApplySavedModel[Phase0]/ConvertToNumpy\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:27.954Z: JOB_MESSAGE_DETAILED: Unzipping flatten s33 for input s27.None\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:27.978Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/Combine/Partial, through flatten Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/Flatten, into producer Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/ParDo(SplitHotCold)/ParDo(SplitHotCold)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:28.010Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:28.033Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/Combine into Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:28.067Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/Combine/Extract into Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/Combine\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:28.107Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/UnKey into Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/Combine/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:28.135Z: JOB_MESSAGE_DETAILED: Unzipping flatten s33-u607 for input partial-s34-out19-c605\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:28.160Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/GroupByKey/Reify, through flatten Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/Flatten/Unzipped-1, into producer Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/Combine/Partial\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:28.213Z: JOB_MESSAGE_DETAILED: Fusing consumer Transform Test Data/InstanceDictToRecordBatch/EncodeInstanceDictsAsTfExample into Split/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:28.251Z: JOB_MESSAGE_DETAILED: Fusing consumer Write TF Test Data/Write/WriteImpl/WindowInto(WindowIntoFn) into Split/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:28.280Z: JOB_MESSAGE_DETAILED: Fusing consumer Convert Batch Test Data into Split/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:28.312Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/InstanceDictToRecordBatch/EncodeInstanceDictsAsTfExample into Split/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:28.346Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/TransformDataset/InstanceDictToRecordBatch/EncodeInstanceDictsAsTfExample into Split/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:28.368Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/SwapTokensAndCounts/KvSwap\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:28.398Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary#vocabulary]/KeepOnlyValidStrings into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/SwapTokensAndCounts/KvSwap\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:28.434Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/SwapTokensAndCounts/KvSwap\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:28.480Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_1#vocabulary]/KeepOnlyValidStrings into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/SwapTokensAndCounts/KvSwap\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:28.515Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/SwapTokensAndCounts/KvSwap\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:28.549Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_2#vocabulary]/KeepOnlyValidStrings into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/SwapTokensAndCounts/KvSwap\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:28.582Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/SwapTokensAndCounts/KvSwap\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:28.610Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_3#vocabulary]/KeepOnlyValidStrings into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/SwapTokensAndCounts/KvSwap\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:28.638Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/SwapTokensAndCounts/KvSwap\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:28.674Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_4#vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/SwapTokensAndCounts/KvSwap\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:28.699Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/SwapTokensAndCounts/KvSwap\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:28.732Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_5#vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/SwapTokensAndCounts/KvSwap\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:28.766Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/SwapTokensAndCounts/KvSwap\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:28.801Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_6#vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/SwapTokensAndCounts/KvSwap\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:28.833Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/SwapTokensAndCounts/KvSwap\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:28.882Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_7#vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/SwapTokensAndCounts/KvSwap\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:28.907Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/TransformDataset/MakeCheapBarrier into Analyze & Transform/TransformDataset/ConvertAndUnbatchToInstanceDicts\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:28.929Z: JOB_MESSAGE_DETAILED: Fusing consumer Write Transformed Train Data/Write/WriteImpl/WindowInto(WindowIntoFn) into Analyze & Transform/TransformDataset/ConvertAndUnbatchToInstanceDicts\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:28.956Z: JOB_MESSAGE_DETAILED: Fusing consumer Transform Validation Data/MakeCheapBarrier into Transform Validation Data/ConvertAndUnbatchToInstanceDicts\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:28.984Z: JOB_MESSAGE_DETAILED: Fusing consumer Write Transformed Validation Data/Write/WriteImpl/WindowInto(WindowIntoFn) into Transform Validation Data/ConvertAndUnbatchToInstanceDicts\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:29.020Z: JOB_MESSAGE_DETAILED: Fusing consumer Transform Test Data/MakeCheapBarrier into Transform Test Data/ConvertAndUnbatchToInstanceDicts\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:29.054Z: JOB_MESSAGE_DETAILED: Fusing consumer Write Transformed Test Data/Write/WriteImpl/WindowInto(WindowIntoFn) into Transform Test Data/ConvertAndUnbatchToInstanceDicts\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:29.089Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/Combine/Partial into Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/WindowIntoOriginal\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:29.114Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/Combine/Partial\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:29.140Z: JOB_MESSAGE_DETAILED: Fusing consumer Read Raw Data/_PassThroughThenCleanup/ParDo(PassThrough)/ParDo(PassThrough) into Read Raw Data/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:29.167Z: JOB_MESSAGE_DETAILED: Fusing consumer Parse Data into Read Raw Data/_PassThroughThenCleanup/ParDo(PassThrough)/ParDo(PassThrough)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:29.206Z: JOB_MESSAGE_DETAILED: Fusing consumer Split/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn) into Parse Data\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:29.246Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordBeamSource/CollectRawRecordTelemetry/ProfileRawRecords into Analyze & Transform/AnalyzeDataset/InstanceDictToRecordBatch/EncodeInstanceDictsAsTfExample\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:29.272Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/RawRecordToRecordBatch/Batch/ParDo(_GlobalWindowsBatchingDoFn) into Analyze & Transform/AnalyzeDataset/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordBeamSource/CollectRawRecordTelemetry/ProfileRawRecords\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:29.308Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/RawRecordToRecordBatch/Decode into Analyze & Transform/AnalyzeDataset/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/RawRecordToRecordBatch/Batch/ParDo(_GlobalWindowsBatchingDoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:29.330Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/CollectRecordBatchTelemetry/ProfileRecordBatches into Analyze & Transform/AnalyzeDataset/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/RawRecordToRecordBatch/Decode\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:29.364Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/ExtractInputForSavedModel[FlattenedDataset]/Identity into Analyze & Transform/AnalyzeDataset/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/CollectRecordBatchTelemetry/ProfileRecordBatches\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:29.396Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/ApplySavedModel[Phase0]/ApplySavedModel/ApplySavedModel into Analyze & Transform/AnalyzeDataset/ExtractInputForSavedModel[FlattenedDataset]/Identity\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:29.432Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/ApplySavedModel[Phase0]/ConvertToNumpy into Analyze & Transform/AnalyzeDataset/ApplySavedModel[Phase0]/ApplySavedModel/ApplySavedModel\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:29.464Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/FlattenTokensAndMaybeWeightsLabels into Analyze & Transform/AnalyzeDataset/TensorSource[compute_and_apply_vocabulary#vocabulary]/ExtractKeys\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:29.499Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CountPerToken:PairWithVoid into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/FlattenTokensAndMaybeWeightsLabels\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:29.528Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Partial into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CountPerToken:PairWithVoid\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:29.560Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Partial\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:29.589Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:29.627Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:29.675Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Extract into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:29.707Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/Combine/Partial into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:29.741Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/Combine/Partial\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:29.772Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:29.796Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/Combine into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:29.838Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/Combine/Extract into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/Combine\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:29.916Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/SwapTokensAndCounts/KvSwap into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/Combine/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:29.950Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary#vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric into Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary#vocabulary]/KeepOnlyValidStrings\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:29.974Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/BatchAndPreSort/BatchVocabulary/ParDo(_GlobalWindowsBatchingDoFn) into Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary#vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:29.999Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/BatchAndPreSort/SortBatches into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/BatchAndPreSort/BatchVocabulary/ParDo(_GlobalWindowsBatchingDoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:30.048Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/FlattenTokensAndMaybeWeightsLabels into Analyze & Transform/AnalyzeDataset/TensorSource[compute_and_apply_vocabulary_1#vocabulary]/ExtractKeys\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:30.082Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CountPerToken:PairWithVoid into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/FlattenTokensAndMaybeWeightsLabels\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:30.113Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Partial into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CountPerToken:PairWithVoid\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:30.146Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Partial\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:30.183Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:30.217Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:30.247Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Extract into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:30.275Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/Combine/Partial into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:30.305Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/Combine/Partial\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:30.340Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:30.371Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/Combine into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:30.406Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/Combine/Extract into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/Combine\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:30.441Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/SwapTokensAndCounts/KvSwap into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/Combine/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:30.474Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Partial into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:30.498Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Partial\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:30.531Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:30.577Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:30.603Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Extract into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:30.628Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:30.654Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_1#vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric into Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_1#vocabulary]/KeepOnlyValidStrings\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:30.676Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/BatchAndPreSort/BatchVocabulary/ParDo(_GlobalWindowsBatchingDoFn) into Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_1#vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:30.707Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/BatchAndPreSort/SortBatches into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/BatchAndPreSort/BatchVocabulary/ParDo(_GlobalWindowsBatchingDoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:30.745Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/FlattenTokensAndMaybeWeightsLabels into Analyze & Transform/AnalyzeDataset/TensorSource[compute_and_apply_vocabulary_2#vocabulary]/ExtractKeys\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:30.779Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CountPerToken:PairWithVoid into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/FlattenTokensAndMaybeWeightsLabels\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:30.813Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Partial into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CountPerToken:PairWithVoid\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:30.848Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Partial\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:30.899Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:30.935Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:30.970Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Extract into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:31.005Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/Combine/Partial into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:31.037Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/Combine/Partial\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:31.058Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:31.085Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/Combine into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:31.116Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/Combine/Extract into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/Combine\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:31.138Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/SwapTokensAndCounts/KvSwap into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/Combine/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:31.162Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Partial into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:31.208Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Partial\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:31.240Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:31.275Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:31.311Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Extract into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:31.346Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:31.383Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_2#vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric into Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_2#vocabulary]/KeepOnlyValidStrings\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:31.416Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/BatchAndPreSort/BatchVocabulary/ParDo(_GlobalWindowsBatchingDoFn) into Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_2#vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:31.454Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/BatchAndPreSort/SortBatches into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/BatchAndPreSort/BatchVocabulary/ParDo(_GlobalWindowsBatchingDoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:31.481Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/FlattenTokensAndMaybeWeightsLabels into Analyze & Transform/AnalyzeDataset/TensorSource[compute_and_apply_vocabulary_3#vocabulary]/ExtractKeys\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:31.517Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CountPerToken:PairWithVoid into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/FlattenTokensAndMaybeWeightsLabels\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:31.554Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Partial into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CountPerToken:PairWithVoid\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:31.589Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Partial\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:31.622Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:31.655Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:31.709Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Extract into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:31.766Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/Combine/Partial into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:31.812Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/Combine/Partial\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:31.886Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:31.947Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/Combine into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:31.998Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/Combine/Extract into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/Combine\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:32.044Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/SwapTokensAndCounts/KvSwap into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/Combine/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:32.072Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Partial into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:32.107Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Partial\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:32.139Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:32.165Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:32.205Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Extract into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:32.231Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:32.269Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_3#vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric into Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_3#vocabulary]/KeepOnlyValidStrings\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:32.306Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/BatchAndPreSort/BatchVocabulary/ParDo(_GlobalWindowsBatchingDoFn) into Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_3#vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:32.329Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/BatchAndPreSort/SortBatches into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/BatchAndPreSort/BatchVocabulary/ParDo(_GlobalWindowsBatchingDoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:32.355Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/FlattenTokensAndMaybeWeightsLabels into Analyze & Transform/AnalyzeDataset/TensorSource[compute_and_apply_vocabulary_4#vocabulary]/ExtractKeys\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:32.381Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CountPerToken:PairWithVoid into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/FlattenTokensAndMaybeWeightsLabels\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:32.412Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Partial into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CountPerToken:PairWithVoid\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:32.444Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Partial\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:32.480Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:32.516Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:32.548Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Extract into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:32.573Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/Combine/Partial into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:32.604Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/Combine/Partial\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:32.627Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:32.660Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/Combine into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:32.682Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/Combine/Extract into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/Combine\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:32.719Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/SwapTokensAndCounts/KvSwap into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/Combine/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:32.741Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Partial into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:32.777Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Partial\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:32.810Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:32.841Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:32.911Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Extract into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:32.936Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:32.969Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_4#vocabulary]/ApplyThresholdsAndTopK/EncodeNumericalTerms into Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_4#vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:32.990Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/BatchAndPreSort/BatchVocabulary/ParDo(_GlobalWindowsBatchingDoFn) into Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_4#vocabulary]/ApplyThresholdsAndTopK/EncodeNumericalTerms\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:33.033Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/BatchAndPreSort/SortBatches into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/BatchAndPreSort/BatchVocabulary/ParDo(_GlobalWindowsBatchingDoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:33.069Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/FlattenTokensAndMaybeWeightsLabels into Analyze & Transform/AnalyzeDataset/TensorSource[compute_and_apply_vocabulary_5#vocabulary]/ExtractKeys\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:33.102Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CountPerToken:PairWithVoid into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/FlattenTokensAndMaybeWeightsLabels\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:33.124Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Partial into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CountPerToken:PairWithVoid\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:33.160Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Partial\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:33.193Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:33.229Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:33.266Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Extract into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:33.290Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/Combine/Partial into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:33.309Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/Combine/Partial\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:33.331Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:33.367Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/Combine into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:33.399Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/Combine/Extract into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/Combine\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:33.432Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/SwapTokensAndCounts/KvSwap into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/Combine/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:33.478Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Partial into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:33.516Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Partial\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:33.542Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:33.574Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:33.603Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Extract into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:33.635Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:33.660Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_5#vocabulary]/ApplyThresholdsAndTopK/EncodeNumericalTerms into Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_5#vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:33.683Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/BatchAndPreSort/BatchVocabulary/ParDo(_GlobalWindowsBatchingDoFn) into Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_5#vocabulary]/ApplyThresholdsAndTopK/EncodeNumericalTerms\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:33.719Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/BatchAndPreSort/SortBatches into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/BatchAndPreSort/BatchVocabulary/ParDo(_GlobalWindowsBatchingDoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:33.753Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/FlattenTokensAndMaybeWeightsLabels into Analyze & Transform/AnalyzeDataset/TensorSource[compute_and_apply_vocabulary_6#vocabulary]/ExtractKeys\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:33.786Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CountPerToken:PairWithVoid into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/FlattenTokensAndMaybeWeightsLabels\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:33.819Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Partial into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CountPerToken:PairWithVoid\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:33.845Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Partial\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:33.896Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:33.931Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:33.965Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Extract into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:33.986Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/Combine/Partial into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:34.009Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/Combine/Partial\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:34.051Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:34.084Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/Combine into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:34.118Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/Combine/Extract into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/Combine\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:34.151Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/SwapTokensAndCounts/KvSwap into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/Combine/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:34.179Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Partial into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:34.216Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Partial\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:34.250Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:34.283Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:34.311Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Extract into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:34.332Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:34.354Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_6#vocabulary]/ApplyThresholdsAndTopK/EncodeNumericalTerms into Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_6#vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:34.386Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/BatchAndPreSort/BatchVocabulary/ParDo(_GlobalWindowsBatchingDoFn) into Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_6#vocabulary]/ApplyThresholdsAndTopK/EncodeNumericalTerms\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:34.419Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/BatchAndPreSort/SortBatches into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/BatchAndPreSort/BatchVocabulary/ParDo(_GlobalWindowsBatchingDoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:34.452Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/FlattenTokensAndMaybeWeightsLabels into Analyze & Transform/AnalyzeDataset/TensorSource[compute_and_apply_vocabulary_7#vocabulary]/ExtractKeys\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:34.484Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CountPerToken:PairWithVoid into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/FlattenTokensAndMaybeWeightsLabels\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:34.518Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Partial into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CountPerToken:PairWithVoid\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:34.544Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Partial\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:34.574Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:34.609Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:34.632Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Extract into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:34.656Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/Combine/Partial into Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:34.688Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/Combine/Partial\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:34.711Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:34.736Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/Combine into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:34.756Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/Combine/Extract into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/Combine\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:34.782Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/SwapTokensAndCounts/KvSwap into Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/Combine/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:34.804Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Partial into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:34.824Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Partial\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:34.853Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:34.899Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:34.933Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Extract into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:34.971Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:34.999Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_7#vocabulary]/ApplyThresholdsAndTopK/EncodeNumericalTerms into Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_7#vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:35.040Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/BatchAndPreSort/BatchVocabulary/ParDo(_GlobalWindowsBatchingDoFn) into Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_7#vocabulary]/ApplyThresholdsAndTopK/EncodeNumericalTerms\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:35.069Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/BatchAndPreSort/SortBatches into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/BatchAndPreSort/BatchVocabulary/ParDo(_GlobalWindowsBatchingDoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:35.100Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/ParDo(SplitHotCold)/ParDo(SplitHotCold) into Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/KeyWithVoid\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:35.134Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/WindowIntoDiscarding into Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/ParDo(SplitHotCold)/ParDo(SplitHotCold)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:35.162Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/Combine/Partial into Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/WindowIntoDiscarding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:35.209Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/Combine/Partial\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:35.244Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:35.288Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/Combine into Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:35.325Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/Combine/Extract into Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/Combine\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:35.360Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/Map(StripNonce) into Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/Combine/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:35.389Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/WindowIntoOriginal into Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/Map(StripNonce)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:35.427Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Partial into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:35.478Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Partial\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:35.499Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:35.522Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:35.557Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Extract into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:35.589Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:35.621Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/TransformDataset/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordBeamSource/CollectRawRecordTelemetry/ProfileRawRecords into Analyze & Transform/TransformDataset/InstanceDictToRecordBatch/EncodeInstanceDictsAsTfExample\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:35.649Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/TransformDataset/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/RawRecordToRecordBatch/Batch/ParDo(_GlobalWindowsBatchingDoFn) into Analyze & Transform/TransformDataset/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordBeamSource/CollectRawRecordTelemetry/ProfileRawRecords\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:35.684Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/TransformDataset/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/RawRecordToRecordBatch/Decode into Analyze & Transform/TransformDataset/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/RawRecordToRecordBatch/Batch/ParDo(_GlobalWindowsBatchingDoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:35.706Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/TransformDataset/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/CollectRecordBatchTelemetry/ProfileRecordBatches into Analyze & Transform/TransformDataset/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/RawRecordToRecordBatch/Decode\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:35.727Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/TransformDataset/ConvertAndUnbatchToInstanceDicts into Analyze & Transform/TransformDataset/Transform/Transform\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:35.750Z: JOB_MESSAGE_DETAILED: Fusing consumer Write Transformed Train Data/Write/WriteImpl/WriteBundles/WriteBundles into Write Transformed Train Data/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:35.777Z: JOB_MESSAGE_DETAILED: Fusing consumer Write Transformed Train Data/Write/WriteImpl/Pair into Write Transformed Train Data/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:35.807Z: JOB_MESSAGE_DETAILED: Fusing consumer Write Transformed Train Data/Write/WriteImpl/GroupByKey/Reify into Write Transformed Train Data/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:35.828Z: JOB_MESSAGE_DETAILED: Fusing consumer Write Transformed Train Data/Write/WriteImpl/GroupByKey/Write into Write Transformed Train Data/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:35.851Z: JOB_MESSAGE_DETAILED: Fusing consumer Write Transformed Train Data/Write/WriteImpl/GroupByKey/GroupByWindow into Write Transformed Train Data/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:35.900Z: JOB_MESSAGE_DETAILED: Fusing consumer Write Transformed Train Data/Write/WriteImpl/Extract into Write Transformed Train Data/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:35.930Z: JOB_MESSAGE_DETAILED: Fusing consumer Transform Validation Data/InstanceDictToRecordBatch/EncodeInstanceDictsAsTfExample into Split/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:35.965Z: JOB_MESSAGE_DETAILED: Fusing consumer Transform Validation Data/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordBeamSource/CollectRawRecordTelemetry/ProfileRawRecords into Transform Validation Data/InstanceDictToRecordBatch/EncodeInstanceDictsAsTfExample\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:36.001Z: JOB_MESSAGE_DETAILED: Fusing consumer Transform Validation Data/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/RawRecordToRecordBatch/Batch/ParDo(_GlobalWindowsBatchingDoFn) into Transform Validation Data/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordBeamSource/CollectRawRecordTelemetry/ProfileRawRecords\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:36.037Z: JOB_MESSAGE_DETAILED: Fusing consumer Transform Validation Data/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/RawRecordToRecordBatch/Decode into Transform Validation Data/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/RawRecordToRecordBatch/Batch/ParDo(_GlobalWindowsBatchingDoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:36.069Z: JOB_MESSAGE_DETAILED: Fusing consumer Transform Validation Data/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/CollectRecordBatchTelemetry/ProfileRecordBatches into Transform Validation Data/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/RawRecordToRecordBatch/Decode\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:36.093Z: JOB_MESSAGE_DETAILED: Fusing consumer Transform Validation Data/ConvertAndUnbatchToInstanceDicts into Transform Validation Data/Transform/Transform\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:36.128Z: JOB_MESSAGE_DETAILED: Fusing consumer Write Transformed Validation Data/Write/WriteImpl/WriteBundles/WriteBundles into Write Transformed Validation Data/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:36.159Z: JOB_MESSAGE_DETAILED: Fusing consumer Write Transformed Validation Data/Write/WriteImpl/Pair into Write Transformed Validation Data/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:36.205Z: JOB_MESSAGE_DETAILED: Fusing consumer Write Transformed Validation Data/Write/WriteImpl/GroupByKey/Reify into Write Transformed Validation Data/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:36.239Z: JOB_MESSAGE_DETAILED: Fusing consumer Write Transformed Validation Data/Write/WriteImpl/GroupByKey/Write into Write Transformed Validation Data/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:36.274Z: JOB_MESSAGE_DETAILED: Fusing consumer Write Transformed Validation Data/Write/WriteImpl/GroupByKey/GroupByWindow into Write Transformed Validation Data/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:36.315Z: JOB_MESSAGE_DETAILED: Fusing consumer Write Transformed Validation Data/Write/WriteImpl/Extract into Write Transformed Validation Data/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:36.349Z: JOB_MESSAGE_DETAILED: Fusing consumer Transform Test Data/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordBeamSource/CollectRawRecordTelemetry/ProfileRawRecords into Transform Test Data/InstanceDictToRecordBatch/EncodeInstanceDictsAsTfExample\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:36.371Z: JOB_MESSAGE_DETAILED: Fusing consumer Transform Test Data/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/RawRecordToRecordBatch/Batch/ParDo(_GlobalWindowsBatchingDoFn) into Transform Test Data/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordBeamSource/CollectRawRecordTelemetry/ProfileRawRecords\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:36.406Z: JOB_MESSAGE_DETAILED: Fusing consumer Transform Test Data/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/RawRecordToRecordBatch/Decode into Transform Test Data/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/RawRecordToRecordBatch/Batch/ParDo(_GlobalWindowsBatchingDoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:36.439Z: JOB_MESSAGE_DETAILED: Fusing consumer Transform Test Data/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/CollectRecordBatchTelemetry/ProfileRecordBatches into Transform Test Data/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/RawRecordToRecordBatch/Decode\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:36.483Z: JOB_MESSAGE_DETAILED: Fusing consumer Transform Test Data/ConvertAndUnbatchToInstanceDicts into Transform Test Data/Transform/Transform\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:36.515Z: JOB_MESSAGE_DETAILED: Fusing consumer Write Transformed Test Data/Write/WriteImpl/WriteBundles/WriteBundles into Write Transformed Test Data/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:36.548Z: JOB_MESSAGE_DETAILED: Fusing consumer Write Transformed Test Data/Write/WriteImpl/Pair into Write Transformed Test Data/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:36.585Z: JOB_MESSAGE_DETAILED: Fusing consumer Write Transformed Test Data/Write/WriteImpl/GroupByKey/Reify into Write Transformed Test Data/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:36.615Z: JOB_MESSAGE_DETAILED: Fusing consumer Write Transformed Test Data/Write/WriteImpl/GroupByKey/Write into Write Transformed Test Data/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:36.649Z: JOB_MESSAGE_DETAILED: Fusing consumer Write Transformed Test Data/Write/WriteImpl/GroupByKey/GroupByWindow into Write Transformed Test Data/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:36.683Z: JOB_MESSAGE_DETAILED: Fusing consumer Write Transformed Test Data/Write/WriteImpl/Extract into Write Transformed Test Data/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:36.719Z: JOB_MESSAGE_DETAILED: Fusing consumer Write TF Test Data/Write/WriteImpl/WriteBundles/WriteBundles into Write TF Test Data/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:36.751Z: JOB_MESSAGE_DETAILED: Fusing consumer Write TF Test Data/Write/WriteImpl/Pair into Write TF Test Data/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:36.787Z: JOB_MESSAGE_DETAILED: Fusing consumer Write TF Test Data/Write/WriteImpl/GroupByKey/Reify into Write TF Test Data/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:36.822Z: JOB_MESSAGE_DETAILED: Fusing consumer Write TF Test Data/Write/WriteImpl/GroupByKey/Write into Write TF Test Data/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:36.843Z: JOB_MESSAGE_DETAILED: Fusing consumer Write TF Test Data/Write/WriteImpl/GroupByKey/GroupByWindow into Write TF Test Data/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:36.911Z: JOB_MESSAGE_DETAILED: Fusing consumer Write TF Test Data/Write/WriteImpl/Extract into Write TF Test Data/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:36.944Z: JOB_MESSAGE_DETAILED: Fusing consumer Write JSONL Test Data/Write/WriteImpl/WindowInto(WindowIntoFn) into Convert Batch Test Data\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:36.967Z: JOB_MESSAGE_DETAILED: Fusing consumer Write JSONL Test Data/Write/WriteImpl/WriteBundles/WriteBundles into Write JSONL Test Data/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:36.994Z: JOB_MESSAGE_DETAILED: Fusing consumer Write JSONL Test Data/Write/WriteImpl/Pair into Write JSONL Test Data/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:37.026Z: JOB_MESSAGE_DETAILED: Fusing consumer Write JSONL Test Data/Write/WriteImpl/GroupByKey/Reify into Write JSONL Test Data/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:37.062Z: JOB_MESSAGE_DETAILED: Fusing consumer Write JSONL Test Data/Write/WriteImpl/GroupByKey/Write into Write JSONL Test Data/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:37.087Z: JOB_MESSAGE_DETAILED: Fusing consumer Write JSONL Test Data/Write/WriteImpl/GroupByKey/GroupByWindow into Write JSONL Test Data/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:37.112Z: JOB_MESSAGE_DETAILED: Fusing consumer Write JSONL Test Data/Write/WriteImpl/Extract into Write JSONL Test Data/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:37.147Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WaitForVocabularyFile/WaitForVocabularyFile into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/CreatePath/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:37.182Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_5#vocabulary#temporary_analyzer_output_1#Const]/ToTensorBinding into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WaitForVocabularyFile/WaitForVocabularyFile\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:37.212Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WaitForVocabularyFile/WaitForVocabularyFile into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/CreatePath/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:37.249Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary#vocabulary#temporary_analyzer_output_1#Const]/ToTensorBinding into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WaitForVocabularyFile/WaitForVocabularyFile\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:37.281Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WaitForVocabularyFile/WaitForVocabularyFile into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/CreatePath/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:37.317Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_2#vocabulary#temporary_analyzer_output_1#Const]/ToTensorBinding into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WaitForVocabularyFile/WaitForVocabularyFile\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:37.349Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WaitForVocabularyFile/WaitForVocabularyFile into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/CreatePath/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:37.381Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_3#vocabulary#temporary_analyzer_output_1#Const]/ToTensorBinding into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WaitForVocabularyFile/WaitForVocabularyFile\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:37.416Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WaitForVocabularyFile/WaitForVocabularyFile into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/CreatePath/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:37.440Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_7#vocabulary#temporary_analyzer_output_1#Const]/ToTensorBinding into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WaitForVocabularyFile/WaitForVocabularyFile\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:37.483Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WaitForVocabularyFile/WaitForVocabularyFile into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/CreatePath/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:37.505Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_1#vocabulary#temporary_analyzer_output_1#Const]/ToTensorBinding into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WaitForVocabularyFile/WaitForVocabularyFile\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:37.527Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WaitForVocabularyFile/WaitForVocabularyFile into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/CreatePath/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:37.560Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_6#vocabulary#temporary_analyzer_output_1#Const]/ToTensorBinding into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WaitForVocabularyFile/WaitForVocabularyFile\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:37.593Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WaitForVocabularyFile/WaitForVocabularyFile into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/CreatePath/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:37.618Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_4#vocabulary#temporary_analyzer_output_1#Const]/ToTensorBinding into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WaitForVocabularyFile/WaitForVocabularyFile\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:37.650Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/ComputeDeferredMetadata[compat_v1=False] into Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/CreateSavedModel\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:37.680Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/MakeCheapBarrier into Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/CreateSavedModel\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:37.712Z: JOB_MESSAGE_DETAILED: Fusing consumer Write Transform Artifacts/WriteTransformFnToTemp into Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/CreateSavedModel\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:37.748Z: JOB_MESSAGE_DETAILED: Unzipping flatten s48 for input s43.None\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:37.772Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/KeyWithVoid, through flatten Analyze & Transform/AnalyzeDataset/FlattenInputForPackedCombineMerge[3]/Flatten, into producer Analyze & Transform/AnalyzeDataset/AddKey[CacheableCombineMerge[scale_to_z_score#mean_and_var]]/AddKey\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:37.806Z: JOB_MESSAGE_DETAILED: Unzipping flatten s48 for input s45.None\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:37.836Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/KeyWithVoid, through flatten Analyze & Transform/AnalyzeDataset/FlattenInputForPackedCombineMerge[3]/Flatten, into producer Analyze & Transform/AnalyzeDataset/AddKey[CacheableCombineMerge[scale_to_z_score_1#mean_and_var]]/AddKey\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:37.905Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/Combine/Partial\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:37.930Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:37.965Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/Combine into Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:37.986Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/Combine/Extract into Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/Combine\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:38.017Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/UnKey into Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/Combine/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:38.051Z: JOB_MESSAGE_DETAILED: Unzipping flatten s48-u616 for input s49.None-c614\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:38.083Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/Combine/Partial, through flatten Analyze & Transform/AnalyzeDataset/FlattenInputForPackedCombineMerge[3]/Flatten/Unzipped-1, into producer Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/KeyWithVoid\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:38.118Z: JOB_MESSAGE_DETAILED: Unzipping flatten s48-u616 for input s49.None-c618\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:38.153Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/Combine/Partial, through flatten Analyze & Transform/AnalyzeDataset/FlattenInputForPackedCombineMerge[3]/Flatten/Unzipped-1, into producer Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/KeyWithVoid\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:38.184Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:38.209Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:38.243Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:38.275Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:38.309Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:38.342Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:38.366Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:38.389Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:38.411Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score#mean_and_var]/ExtractKeys into Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/InjectDefault/InjectDefault\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:38.437Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1#mean_and_var]/ExtractKeys into Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/InjectDefault/InjectDefault\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:38.481Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2#mean_and_var]/ExtractKeys into Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/InjectDefault/InjectDefault\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:38.515Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/ExtractFromDict[CacheableCombineMerge[scale_to_z_score#mean_and_var]]/ExtractKeys into Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/InjectDefault/InjectDefault\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:38.547Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/ExtractFromDict[CacheableCombineMerge[scale_to_z_score_1#mean_and_var]]/ExtractKeys into Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/InjectDefault/InjectDefault\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:38.581Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/ExtractFromDict[CacheableCombineMerge[scale_to_z_score_2#mean_and_var]]/ExtractKeys into Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/InjectDefault/InjectDefault\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:38.613Z: JOB_MESSAGE_DETAILED: Fusing consumer Write Transformed Train Data/Write/WriteImpl/InitializeWrite into Write Transformed Train Data/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:38.648Z: JOB_MESSAGE_DETAILED: Fusing consumer Write Transformed Validation Data/Write/WriteImpl/InitializeWrite into Write Transformed Validation Data/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:38.680Z: JOB_MESSAGE_DETAILED: Fusing consumer Write Transformed Test Data/Write/WriteImpl/InitializeWrite into Write Transformed Test Data/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:38.714Z: JOB_MESSAGE_DETAILED: Fusing consumer Write TF Test Data/Write/WriteImpl/InitializeWrite into Write TF Test Data/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:38.746Z: JOB_MESSAGE_DETAILED: Fusing consumer Write JSONL Test Data/Write/WriteImpl/InitializeWrite into Write JSONL Test Data/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:38.780Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/KeyWithVoid into Analyze & Transform/AnalyzeDataset/AddKey[CacheableCombineMerge[scale_to_z_score_2#mean_and_var]]/AddKey\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:38.814Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/Combine/Partial into Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/KeyWithVoid\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:38.849Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/CreateSavedModelForAnalyzerInputs[Phase0][tf_v2_only]/CreateSavedModel into Analyze & Transform/AnalyzeDataset/CreateSavedModelForAnalyzerInputs[Phase0][tf_v2_only]/CreateSole/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:38.906Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/OrderElements/OrderElements into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/Prepare/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:38.929Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1130>) into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/OrderElements/OrderElements\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:38.958Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn) into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1130>)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:38.992Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:39.024Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:39.059Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:39.091Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/WriteBundles into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:39.125Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/OrderElements/OrderElements into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/Prepare/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:39.168Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1130>) into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/OrderElements/OrderElements\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:39.208Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn) into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1130>)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:39.231Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:39.254Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:39.287Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:39.312Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/WriteBundles into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:39.347Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/OrderElements/OrderElements into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/Prepare/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:39.382Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1130>) into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/OrderElements/OrderElements\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:39.442Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn) into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1130>)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:39.494Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:39.551Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:39.590Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:39.659Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/WriteBundles into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:39.726Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/OrderElements/OrderElements into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/Prepare/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:39.797Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1130>) into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/OrderElements/OrderElements\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:39.837Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn) into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1130>)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:39.905Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:39.937Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:39.960Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:39.994Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/WriteBundles into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:40.016Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/OrderElements/OrderElements into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/Prepare/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:40.041Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1130>) into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/OrderElements/OrderElements\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:40.063Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn) into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1130>)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:40.096Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:40.117Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:40.150Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:40.174Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/WriteBundles into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:40.210Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/OrderElements/OrderElements into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/Prepare/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:40.268Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1130>) into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/OrderElements/OrderElements\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:40.367Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn) into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1130>)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:40.450Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:40.505Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:40.548Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:40.591Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/WriteBundles into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:40.623Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/OrderElements/OrderElements into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/Prepare/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:40.655Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1130>) into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/OrderElements/OrderElements\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:40.695Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn) into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1130>)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:40.726Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:40.760Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:40.790Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:40.814Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/WriteBundles into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:40.836Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/OrderElements/OrderElements into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/Prepare/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:40.859Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1130>) into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/OrderElements/OrderElements\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:40.904Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn) into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1130>)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:40.936Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Reify into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:40.970Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:41.008Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:41.039Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/WriteBundles into Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:41.075Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/InjectDefault/InjectDefault into Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:41.107Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/AddKey[CacheableCombineMerge[scale_to_z_score#mean_and_var]]/AddKey into Analyze & Transform/AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score#mean_and_var]/ExtractKeys\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:41.138Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/AddKey[CacheableCombineMerge[scale_to_z_score_1#mean_and_var]]/AddKey into Analyze & Transform/AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1#mean_and_var]/ExtractKeys\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:41.172Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/AddKey[CacheableCombineMerge[scale_to_z_score_2#mean_and_var]]/AddKey into Analyze & Transform/AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2#mean_and_var]/ExtractKeys\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:41.216Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/InjectDefault/InjectDefault into Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:41.252Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/ExtractPackedCombineMergeOutputs[CacheableCombineMerge[scale_to_z_score_2#mean_and_var]]/ExtractOutputs/FlatMap(extract_outputs) into Analyze & Transform/AnalyzeDataset/ExtractFromDict[CacheableCombineMerge[scale_to_z_score_2#mean_and_var]]/ExtractKeys\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:41.287Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/CreateTensorBinding[scale_to_z_score_2#mean_and_var#temporary_analyzer_output#PlaceholderWithDefault]/ToTensorBinding into Analyze & Transform/AnalyzeDataset/ExtractPackedCombineMergeOutputs[CacheableCombineMerge[scale_to_z_score_2#mean_and_var]]/ExtractOutputs/FlatMap(extract_outputs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:41.320Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/CreateTensorBinding[scale_to_z_score_2#mean_and_var#temporary_analyzer_output_1#PlaceholderWithDefault]/ToTensorBinding into Analyze & Transform/AnalyzeDataset/ExtractPackedCombineMergeOutputs[CacheableCombineMerge[scale_to_z_score_2#mean_and_var]]/ExtractOutputs/FlatMap(extract_outputs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:41.342Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/InjectDefault into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:41.363Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/ToInt64 into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/InjectDefault\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:41.396Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary#vocabulary#temporary_analyzer_output#dropoff_grid_unpruned_vocab_size]/ToTensorBinding into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/ToInt64\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:41.428Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/InjectDefault into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:41.461Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/ToInt64 into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/InjectDefault\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:41.493Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_1#vocabulary#temporary_analyzer_output#loc_cross_unpruned_vocab_size]/ToTensorBinding into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/ToInt64\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:41.525Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/InjectDefault into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:41.559Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/ToInt64 into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/InjectDefault\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:41.582Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_2#vocabulary#temporary_analyzer_output#payment_type_unpruned_vocab_size]/ToTensorBinding into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/ToInt64\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:41.613Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/InjectDefault into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:41.647Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/ToInt64 into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/InjectDefault\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:41.680Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_3#vocabulary#temporary_analyzer_output#pickup_grid_unpruned_vocab_size]/ToTensorBinding into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/ToInt64\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:41.715Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/InjectDefault into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:41.749Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/ToInt64 into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/InjectDefault\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:41.782Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_4#vocabulary#temporary_analyzer_output#trip_day_unpruned_vocab_size]/ToTensorBinding into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/ToInt64\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:41.804Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/InjectDefault into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:41.837Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/ToInt64 into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/InjectDefault\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:41.859Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_5#vocabulary#temporary_analyzer_output#trip_day_of_week_unpruned_vocab_size]/ToTensorBinding into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/ToInt64\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:41.909Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/InjectDefault into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:41.941Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/ToInt64 into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/InjectDefault\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:41.988Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_6#vocabulary#temporary_analyzer_output#trip_hour_unpruned_vocab_size]/ToTensorBinding into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/ToInt64\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:42.044Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/InjectDefault into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:42.067Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/ToInt64 into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/InjectDefault\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:42.101Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_7#vocabulary#temporary_analyzer_output#trip_month_unpruned_vocab_size]/ToTensorBinding into Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/ToInt64\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:42.134Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/ExtractPackedCombineMergeOutputs[CacheableCombineMerge[scale_to_z_score#mean_and_var]]/ExtractOutputs/FlatMap(extract_outputs) into Analyze & Transform/AnalyzeDataset/ExtractFromDict[CacheableCombineMerge[scale_to_z_score#mean_and_var]]/ExtractKeys\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:42.170Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/CreateTensorBinding[scale_to_z_score#mean_and_var#temporary_analyzer_output#PlaceholderWithDefault]/ToTensorBinding into Analyze & Transform/AnalyzeDataset/ExtractPackedCombineMergeOutputs[CacheableCombineMerge[scale_to_z_score#mean_and_var]]/ExtractOutputs/FlatMap(extract_outputs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:42.220Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/CreateTensorBinding[scale_to_z_score#mean_and_var#temporary_analyzer_output_1#PlaceholderWithDefault]/ToTensorBinding into Analyze & Transform/AnalyzeDataset/ExtractPackedCombineMergeOutputs[CacheableCombineMerge[scale_to_z_score#mean_and_var]]/ExtractOutputs/FlatMap(extract_outputs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:42.242Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/ExtractPackedCombineMergeOutputs[CacheableCombineMerge[scale_to_z_score_1#mean_and_var]]/ExtractOutputs/FlatMap(extract_outputs) into Analyze & Transform/AnalyzeDataset/ExtractFromDict[CacheableCombineMerge[scale_to_z_score_1#mean_and_var]]/ExtractKeys\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:42.264Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/CreateTensorBinding[scale_to_z_score_1#mean_and_var#temporary_analyzer_output#PlaceholderWithDefault]/ToTensorBinding into Analyze & Transform/AnalyzeDataset/ExtractPackedCombineMergeOutputs[CacheableCombineMerge[scale_to_z_score_1#mean_and_var]]/ExtractOutputs/FlatMap(extract_outputs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:42.297Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/CreateTensorBinding[scale_to_z_score_1#mean_and_var#temporary_analyzer_output_1#PlaceholderWithDefault]/ToTensorBinding into Analyze & Transform/AnalyzeDataset/ExtractPackedCombineMergeOutputs[CacheableCombineMerge[scale_to_z_score_1#mean_and_var]]/ExtractOutputs/FlatMap(extract_outputs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:42.329Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/ReplaceWithConstants into Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/CreateSole/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:42.351Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/CreateSavedModel into Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/ReplaceWithConstants\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:42.372Z: JOB_MESSAGE_DETAILED: Fusing consumer Write Transform Artifacts/WriteMetadataToTemp/WriteMetadata into Analyze & Transform/AnalyzeDataset/ComputeDeferredMetadata[compat_v1=False]\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:42.399Z: JOB_MESSAGE_DETAILED: Fusing consumer Read Raw Data/MapFilesToRemove into Read Raw Data/FilesToRemoveImpulse/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:42.435Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/InstrumentAPI/CountAPIUse into Analyze & Transform/AnalyzeDataset/InstrumentAPI/CreateSoleAPIUse/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:42.477Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/CreateSavedModelForAnalyzerInputs[Phase0][tf_v2_only]/Count/Count into Analyze & Transform/AnalyzeDataset/CreateSavedModelForAnalyzerInputs[Phase0][tf_v2_only]/Count/CreateSole/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:42.510Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/Count/Count into Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/Count/CreateSole/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:42.535Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/Count/Count into Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/Count/CreateSole/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:42.566Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/WaitAndClearSharedKeepAlives/WaitAndClearSharedKeepAlives into Analyze & Transform/AnalyzeDataset/PrepareToClearSharedKeepAlives/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:42.599Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/TransformDataset/WaitAndClearSharedKeepAlives/WaitAndClearSharedKeepAlives into Analyze & Transform/TransformDataset/PrepareToClearSharedKeepAlives/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:42.631Z: JOB_MESSAGE_DETAILED: Fusing consumer Transform Validation Data/WaitAndClearSharedKeepAlives/WaitAndClearSharedKeepAlives into Transform Validation Data/PrepareToClearSharedKeepAlives/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:42.666Z: JOB_MESSAGE_DETAILED: Fusing consumer Read Raw Data/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/ParDo(RemoveExtractedFiles) into Read Raw Data/_PassThroughThenCleanup/Create/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:42.698Z: JOB_MESSAGE_DETAILED: Fusing consumer Transform Test Data/WaitAndClearSharedKeepAlives/WaitAndClearSharedKeepAlives into Transform Test Data/PrepareToClearSharedKeepAlives/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:42.734Z: JOB_MESSAGE_DETAILED: Fusing consumer Write Transform Artifacts/PublishMetadataAndTransformFn/PublishMetadataAndTransformFn into Write Transform Artifacts/CreateSole/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:42.769Z: JOB_MESSAGE_DETAILED: Fusing consumer Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/Count/Count into Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/Count/CreateSole/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:42.897Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:42.928Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:42.963Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:43.011Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:43.593Z: JOB_MESSAGE_DEBUG: Executing wait step start758\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:43.663Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/Count/CreateSole/Read+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/Count/Count\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:43.692Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/Count/CreateSole/Read+Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/Count/Count\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:43.705Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:43.713Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/Count/CreateSole/Read+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/Count/Count\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:43.734Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1-b...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:43.752Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/CreateSavedModelForAnalyzerInputs[Phase0][tf_v2_only]/Count/CreateSole/Read+Analyze & Transform/AnalyzeDataset/CreateSavedModelForAnalyzerInputs[Phase0][tf_v2_only]/Count/Count\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:43.788Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/InstrumentAPI/CreateSoleAPIUse/Read+Analyze & Transform/AnalyzeDataset/InstrumentAPI/CountAPIUse\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:43.821Z: JOB_MESSAGE_BASIC: Executing operation Read Raw Data/FilesToRemoveImpulse/Read+Read Raw Data/MapFilesToRemove\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:43.852Z: JOB_MESSAGE_BASIC: Executing operation Write JSONL Test Data/Write/WriteImpl/DoOnce/Read+Write JSONL Test Data/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:43.905Z: JOB_MESSAGE_BASIC: Executing operation Write TF Test Data/Write/WriteImpl/DoOnce/Read+Write TF Test Data/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:43.939Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Test Data/Write/WriteImpl/DoOnce/Read+Write Transformed Test Data/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:43.981Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Validation Data/Write/WriteImpl/DoOnce/Read+Write Transformed Validation Data/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:44.015Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Train Data/Write/WriteImpl/DoOnce/Read+Write Transformed Train Data/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:44.047Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:44.073Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:44.105Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:44.137Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:44.171Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:44.202Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:44.238Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:44.282Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/CreateSavedModelForAnalyzerInputs[Phase0][tf_v2_only]/CreateSole/Read+Analyze & Transform/AnalyzeDataset/CreateSavedModelForAnalyzerInputs[Phase0][tf_v2_only]/CreateSavedModel\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:18:44.320Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2022-03-03_01_18_20-2421235367684640617 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:19:24.324Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running stage(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:19:47.512Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:19:47.547Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:15.871Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:15.937Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:15.961Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:16.028Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:16.076Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:16.103Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:16.139Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:16.152Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:16.174Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:16.187Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:16.213Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:16.251Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:19.347Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:19.392Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:19.443Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:19.502Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:19.536Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:19.548Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:19.562Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:19.588Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:19.608Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:19.608Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:19.644Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:19.679Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:22.846Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:22.898Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:22.935Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:22.997Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:23.030Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:23.049Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:23.055Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:23.082Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:23.130Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:23.131Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:23.155Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:23.190Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:26.357Z: JOB_MESSAGE_BASIC: Finished operation Read Raw Data/FilesToRemoveImpulse/Read+Read Raw Data/MapFilesToRemove\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:26.428Z: JOB_MESSAGE_DEBUG: Value \"Read Raw Data/MapFilesToRemove.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:26.484Z: JOB_MESSAGE_BASIC: Executing operation Read Raw Data/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(MapFilesToRemove.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:26.528Z: JOB_MESSAGE_BASIC: Finished operation Read Raw Data/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(MapFilesToRemove.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:26.605Z: JOB_MESSAGE_DEBUG: Value \"Read Raw Data/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(MapFilesToRemove.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:36.696Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/CreateSavedModelForAnalyzerInputs[Phase0][tf_v2_only]/Count/CreateSole/Read+Analyze & Transform/AnalyzeDataset/CreateSavedModelForAnalyzerInputs[Phase0][tf_v2_only]/Count/Count\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:46.380Z: JOB_MESSAGE_BASIC: Finished operation Write JSONL Test Data/Write/WriteImpl/DoOnce/Read+Write JSONL Test Data/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:46.443Z: JOB_MESSAGE_DEBUG: Value \"Write JSONL Test Data/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:46.477Z: JOB_MESSAGE_DEBUG: Value \"Write JSONL Test Data/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:46.540Z: JOB_MESSAGE_BASIC: Executing operation Write JSONL Test Data/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:46.580Z: JOB_MESSAGE_BASIC: Executing operation Write JSONL Test Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:46.597Z: JOB_MESSAGE_BASIC: Finished operation Write JSONL Test Data/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:46.611Z: JOB_MESSAGE_BASIC: Executing operation Write JSONL Test Data/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:46.636Z: JOB_MESSAGE_BASIC: Finished operation Write JSONL Test Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:46.657Z: JOB_MESSAGE_DEBUG: Value \"Write JSONL Test Data/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:46.662Z: JOB_MESSAGE_BASIC: Finished operation Write JSONL Test Data/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:46.690Z: JOB_MESSAGE_DEBUG: Value \"Write JSONL Test Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:46.730Z: JOB_MESSAGE_DEBUG: Value \"Write JSONL Test Data/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:48.346Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:48.397Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:48.433Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:48.493Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:48.526Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:48.539Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:48.560Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:48.584Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:48.597Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:48.606Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:48.655Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:48.690Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:50.924Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Validation Data/Write/WriteImpl/DoOnce/Read+Write Transformed Validation Data/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:50.981Z: JOB_MESSAGE_DEBUG: Value \"Write Transformed Validation Data/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:51.013Z: JOB_MESSAGE_DEBUG: Value \"Write Transformed Validation Data/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:51.106Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Validation Data/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:51.135Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Validation Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:51.158Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Validation Data/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:51.163Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Validation Data/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:51.189Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Validation Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:51.210Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Validation Data/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:51.213Z: JOB_MESSAGE_DEBUG: Value \"Write Transformed Validation Data/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:51.253Z: JOB_MESSAGE_DEBUG: Value \"Write Transformed Validation Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:51.287Z: JOB_MESSAGE_DEBUG: Value \"Write Transformed Validation Data/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:52.670Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/Count/CreateSole/Read+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/Count/Count\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:55.574Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:55.626Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:55.659Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:55.719Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:55.751Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:55.758Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:55.784Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:55.791Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:55.819Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:55.828Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:55.852Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:55.905Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:56.198Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/Count/CreateSole/Read+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/Count/Count\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:59.169Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:59.225Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:59.258Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:59.316Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:59.351Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:59.369Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:59.373Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:59.394Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:59.417Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:59.431Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:59.453Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:26:59.475Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:02.546Z: JOB_MESSAGE_BASIC: Finished operation Write TF Test Data/Write/WriteImpl/DoOnce/Read+Write TF Test Data/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:02.602Z: JOB_MESSAGE_DEBUG: Value \"Write TF Test Data/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:02.635Z: JOB_MESSAGE_DEBUG: Value \"Write TF Test Data/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:02.692Z: JOB_MESSAGE_BASIC: Executing operation Write TF Test Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:02.728Z: JOB_MESSAGE_BASIC: Executing operation Write TF Test Data/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:02.736Z: JOB_MESSAGE_BASIC: Finished operation Write TF Test Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:02.760Z: JOB_MESSAGE_BASIC: Executing operation Write TF Test Data/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:02.770Z: JOB_MESSAGE_BASIC: Finished operation Write TF Test Data/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:02.792Z: JOB_MESSAGE_DEBUG: Value \"Write TF Test Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:02.803Z: JOB_MESSAGE_BASIC: Finished operation Write TF Test Data/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:02.839Z: JOB_MESSAGE_DEBUG: Value \"Write TF Test Data/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:02.875Z: JOB_MESSAGE_DEBUG: Value \"Write TF Test Data/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:21.535Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/CreateSavedModelForAnalyzerInputs[Phase0][tf_v2_only]/CreateSole/Read+Analyze & Transform/AnalyzeDataset/CreateSavedModelForAnalyzerInputs[Phase0][tf_v2_only]/CreateSavedModel\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:21.596Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateSavedModelForAnalyzerInputs[Phase0][tf_v2_only]/CreateSavedModel.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:21.664Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/ApplySavedModel[Phase0]/ApplySavedModel/_UnpickledSideInput(CreateSavedModel.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:21.713Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/ApplySavedModel[Phase0]/ApplySavedModel/_UnpickledSideInput(CreateSavedModel.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:21.779Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/ApplySavedModel[Phase0]/ApplySavedModel/_UnpickledSideInput(CreateSavedModel.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:21.835Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:21.867Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:21.903Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:21.940Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:21.966Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:22.002Z: JOB_MESSAGE_BASIC: Executing operation Write TF Test Data/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:22.038Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:22.063Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:22.088Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:22.135Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:22.135Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:22.144Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:22.196Z: JOB_MESSAGE_BASIC: Executing operation Write JSONL Test Data/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:22.214Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:22.245Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:22.264Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:22.296Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:22.339Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:22.381Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:22.410Z: JOB_MESSAGE_BASIC: Finished operation Write TF Test Data/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:22.432Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:22.435Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:22.461Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:22.464Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:22.471Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:22.472Z: JOB_MESSAGE_BASIC: Finished operation Write JSONL Test Data/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:22.503Z: JOB_MESSAGE_DEBUG: Value \"Write TF Test Data/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:22.519Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:22.539Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:22.571Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:22.607Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:22.627Z: JOB_MESSAGE_DEBUG: Value \"Write JSONL Test Data/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:22.664Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:22.698Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:22.879Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:22.948Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:23.023Z: JOB_MESSAGE_BASIC: Executing operation Read Raw Data/Read+Read Raw Data/_PassThroughThenCleanup/ParDo(PassThrough)/ParDo(PassThrough)+Parse Data+Split/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)+Transform Test Data/InstanceDictToRecordBatch/EncodeInstanceDictsAsTfExample+Write TF Test Data/Write/WriteImpl/WindowInto(WindowIntoFn)+Convert Batch Test Data+Analyze & Transform/AnalyzeDataset/InstanceDictToRecordBatch/EncodeInstanceDictsAsTfExample+Analyze & Transform/TransformDataset/InstanceDictToRecordBatch/EncodeInstanceDictsAsTfExample+Analyze & Transform/AnalyzeDataset/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordBeamSource/CollectRawRecordTelemetry/ProfileRawRecords+Analyze & Transform/AnalyzeDataset/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/RawRecordToRecordBatch/Batch/ParDo(_GlobalWindowsBatchingDoFn)+Analyze & Transform/AnalyzeDataset/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/RawRecordToRecordBatch/Decode+Analyze & Transform/AnalyzeDataset/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/CollectRecordBatchTelemetry/ProfileRecordBatches+Analyze & Transform/AnalyzeDataset/ExtractInputForSavedModel[FlattenedDataset]/Identity+Analyze & Transform/AnalyzeDataset/ApplySavedModel[Phase0]/ApplySavedModel/ApplySavedModel+Analyze & Transform/AnalyzeDataset/ApplySavedModel[Phase0]/ConvertToNumpy+Analyze & Transform/AnalyzeDataset/TensorSource[compute_and_apply_vocabulary_1#vocabulary]/ExtractKeys+Analyze & Transform/AnalyzeDataset/TensorSource[compute_and_apply_vocabulary_2#vocabulary]/ExtractKeys+Analyze & Transform/AnalyzeDataset/TensorSource[compute_and_apply_vocabulary_3#vocabulary]/ExtractKeys+Analyze & Transform/AnalyzeDataset/TensorSource[compute_and_apply_vocabulary_4#vocabulary]/ExtractKeys+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/KeyWithVoid+Analyze & Transform/AnalyzeDataset/TensorSource[compute_and_apply_vocabulary_5#vocabulary]/ExtractKeys+Analyze & Transform/AnalyzeDataset/TensorSource[compute_and_apply_vocabulary_6#vocabulary]/ExtractKeys+Analyze & Transform/AnalyzeDataset/TensorSource[compute_and_apply_vocabulary_7#vocabulary]/ExtractKeys+Analyze & Transform/AnalyzeDataset/TensorSource[compute_and_apply_vocabulary#vocabulary]/ExtractKeys+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/FlattenTokensAndMaybeWeightsLabels+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CountPerToken:PairWithVoid+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Write+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/FlattenTokensAndMaybeWeightsLabels+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CountPerToken:PairWithVoid+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Write+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/FlattenTokensAndMaybeWeightsLabels+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CountPerToken:PairWithVoid+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Write+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/FlattenTokensAndMaybeWeightsLabels+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CountPerToken:PairWithVoid+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Write+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/FlattenTokensAndMaybeWeightsLabels+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CountPerToken:PairWithVoid+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Write+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/FlattenTokensAndMaybeWeightsLabels+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CountPerToken:PairWithVoid+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Write+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/FlattenTokensAndMaybeWeightsLabels+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CountPerToken:PairWithVoid+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Write+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/FlattenTokensAndMaybeWeightsLabels+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CountPerToken:PairWithVoid+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Write+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/ParDo(SplitHotCold)/ParDo(SplitHotCold)+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/Combine/Partial+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/GroupByKey/Write+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/WindowIntoDiscarding+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/Combine/Partial+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/GroupByKey/Write+Analyze & Transform/TransformDataset/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordBeamSource/CollectRawRecordTelemetry/ProfileRawRecords+Analyze & Transform/TransformDataset/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/RawRecordToRecordBatch/Batch/ParDo(_GlobalWindowsBatchingDoFn)+Analyze & Transform/TransformDataset/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/RawRecordToRecordBatch/Decode+Analyze & Transform/TransformDataset/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/CollectRecordBatchTelemetry/ProfileRecordBatches+Transform Validation Data/InstanceDictToRecordBatch/EncodeInstanceDictsAsTfExample+Transform Validation Data/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordBeamSource/CollectRawRecordTelemetry/ProfileRawRecords+Transform Validation Data/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/RawRecordToRecordBatch/Batch/ParDo(_GlobalWindowsBatchingDoFn)+Transform Validation Data/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/RawRecordToRecordBatch/Decode+Transform Validation Data/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/CollectRecordBatchTelemetry/ProfileRecordBatches+Transform Test Data/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordBeamSource/CollectRawRecordTelemetry/ProfileRawRecords+Transform Test Data/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/RawRecordToRecordBatch/Batch/ParDo(_GlobalWindowsBatchingDoFn)+Transform Test Data/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/RawRecordToRecordBatch/Decode+Transform Test Data/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/CollectRecordBatchTelemetry/ProfileRecordBatches+Write TF Test Data/Write/WriteImpl/WriteBundles/WriteBundles+Write TF Test Data/Write/WriteImpl/Pair+Write TF Test Data/Write/WriteImpl/GroupByKey/Reify+Write TF Test Data/Write/WriteImpl/GroupByKey/Write+Write JSONL Test Data/Write/WriteImpl/WindowInto(WindowIntoFn)+Write JSONL Test Data/Write/WriteImpl/WriteBundles/WriteBundles+Write JSONL Test Data/Write/WriteImpl/Pair+Write JSONL Test Data/Write/WriteImpl/GroupByKey/Reify+Write JSONL Test Data/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:24.140Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Test Data/Write/WriteImpl/DoOnce/Read+Write Transformed Test Data/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:24.194Z: JOB_MESSAGE_DEBUG: Value \"Write Transformed Test Data/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:24.215Z: JOB_MESSAGE_DEBUG: Value \"Write Transformed Test Data/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:24.284Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Test Data/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:24.317Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Test Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:24.342Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Test Data/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:24.348Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Test Data/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:24.358Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Test Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:24.386Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Test Data/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:24.413Z: JOB_MESSAGE_DEBUG: Value \"Write Transformed Test Data/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:24.446Z: JOB_MESSAGE_DEBUG: Value \"Write Transformed Test Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:24.470Z: JOB_MESSAGE_DEBUG: Value \"Write Transformed Test Data/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:24.586Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:24.653Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:24.687Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:24.756Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:24.782Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:24.802Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:24.804Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:24.842Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:24.847Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:24.866Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:24.911Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:27:24.943Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:26.854Z: JOB_MESSAGE_BASIC: Finished operation Read Raw Data/Read+Read Raw Data/_PassThroughThenCleanup/ParDo(PassThrough)/ParDo(PassThrough)+Parse Data+Split/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)+Transform Test Data/InstanceDictToRecordBatch/EncodeInstanceDictsAsTfExample+Write TF Test Data/Write/WriteImpl/WindowInto(WindowIntoFn)+Convert Batch Test Data+Analyze & Transform/AnalyzeDataset/InstanceDictToRecordBatch/EncodeInstanceDictsAsTfExample+Analyze & Transform/TransformDataset/InstanceDictToRecordBatch/EncodeInstanceDictsAsTfExample+Analyze & Transform/AnalyzeDataset/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordBeamSource/CollectRawRecordTelemetry/ProfileRawRecords+Analyze & Transform/AnalyzeDataset/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/RawRecordToRecordBatch/Batch/ParDo(_GlobalWindowsBatchingDoFn)+Analyze & Transform/AnalyzeDataset/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/RawRecordToRecordBatch/Decode+Analyze & Transform/AnalyzeDataset/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/CollectRecordBatchTelemetry/ProfileRecordBatches+Analyze & Transform/AnalyzeDataset/ExtractInputForSavedModel[FlattenedDataset]/Identity+Analyze & Transform/AnalyzeDataset/ApplySavedModel[Phase0]/ApplySavedModel/ApplySavedModel+Analyze & Transform/AnalyzeDataset/ApplySavedModel[Phase0]/ConvertToNumpy+Analyze & Transform/AnalyzeDataset/TensorSource[compute_and_apply_vocabulary_1#vocabulary]/ExtractKeys+Analyze & Transform/AnalyzeDataset/TensorSource[compute_and_apply_vocabulary_2#vocabulary]/ExtractKeys+Analyze & Transform/AnalyzeDataset/TensorSource[compute_and_apply_vocabulary_3#vocabulary]/ExtractKeys+Analyze & Transform/AnalyzeDataset/TensorSource[compute_and_apply_vocabulary_4#vocabulary]/ExtractKeys+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/KeyWithVoid+Analyze & Transform/AnalyzeDataset/TensorSource[compute_and_apply_vocabulary_5#vocabulary]/ExtractKeys+Analyze & Transform/AnalyzeDataset/TensorSource[compute_and_apply_vocabulary_6#vocabulary]/ExtractKeys+Analyze & Transform/AnalyzeDataset/TensorSource[compute_and_apply_vocabulary_7#vocabulary]/ExtractKeys+Analyze & Transform/AnalyzeDataset/TensorSource[compute_and_apply_vocabulary#vocabulary]/ExtractKeys+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/FlattenTokensAndMaybeWeightsLabels+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CountPerToken:PairWithVoid+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Write+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/FlattenTokensAndMaybeWeightsLabels+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CountPerToken:PairWithVoid+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Write+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/FlattenTokensAndMaybeWeightsLabels+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CountPerToken:PairWithVoid+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Write+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/FlattenTokensAndMaybeWeightsLabels+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CountPerToken:PairWithVoid+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Write+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/FlattenTokensAndMaybeWeightsLabels+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CountPerToken:PairWithVoid+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Write+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/FlattenTokensAndMaybeWeightsLabels+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CountPerToken:PairWithVoid+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Write+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/FlattenTokensAndMaybeWeightsLabels+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CountPerToken:PairWithVoid+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Write+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/FlattenTokensAndMaybeWeightsLabels+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CountPerToken:PairWithVoid+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Write+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/ParDo(SplitHotCold)/ParDo(SplitHotCold)+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/Combine/Partial+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/GroupByKey/Write+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/WindowIntoDiscarding+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/Combine/Partial+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/GroupByKey/Write+Analyze & Transform/TransformDataset/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordBeamSource/CollectRawRecordTelemetry/ProfileRawRecords+Analyze & Transform/TransformDataset/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/RawRecordToRecordBatch/Batch/ParDo(_GlobalWindowsBatchingDoFn)+Analyze & Transform/TransformDataset/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/RawRecordToRecordBatch/Decode+Analyze & Transform/TransformDataset/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/CollectRecordBatchTelemetry/ProfileRecordBatches+Transform Validation Data/InstanceDictToRecordBatch/EncodeInstanceDictsAsTfExample+Transform Validation Data/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordBeamSource/CollectRawRecordTelemetry/ProfileRawRecords+Transform Validation Data/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/RawRecordToRecordBatch/Batch/ParDo(_GlobalWindowsBatchingDoFn)+Transform Validation Data/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/RawRecordToRecordBatch/Decode+Transform Validation Data/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/CollectRecordBatchTelemetry/ProfileRecordBatches+Transform Test Data/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordBeamSource/CollectRawRecordTelemetry/ProfileRawRecords+Transform Test Data/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/RawRecordToRecordBatch/Batch/ParDo(_GlobalWindowsBatchingDoFn)+Transform Test Data/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/RawRecordToRecordBatch/Decode+Transform Test Data/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/CollectRecordBatchTelemetry/ProfileRecordBatches+Write TF Test Data/Write/WriteImpl/WriteBundles/WriteBundles+Write TF Test Data/Write/WriteImpl/Pair+Write TF Test Data/Write/WriteImpl/GroupByKey/Reify+Write TF Test Data/Write/WriteImpl/GroupByKey/Write+Write JSONL Test Data/Write/WriteImpl/WindowInto(WindowIntoFn)+Write JSONL Test Data/Write/WriteImpl/WriteBundles/WriteBundles+Write JSONL Test Data/Write/WriteImpl/Pair+Write JSONL Test Data/Write/WriteImpl/GroupByKey/Reify+Write JSONL Test Data/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:26.925Z: JOB_MESSAGE_DEBUG: Value \"Read Raw Data/_PassThroughThenCleanup/ParDo(PassThrough)/ParDo(PassThrough).cleanup_signal\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:26.958Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/TransformDataset/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/CollectRecordBatchTelemetry/ProfileRecordBatches.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.004Z: JOB_MESSAGE_DEBUG: Value \"Transform Validation Data/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/CollectRecordBatchTelemetry/ProfileRecordBatches.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.027Z: JOB_MESSAGE_DEBUG: Value \"Transform Test Data/InstanceDictToRecordBatch/TfExampleToRecordBatch/RawRecordToRecordBatch/CollectRecordBatchTelemetry/ProfileRecordBatches.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.050Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.073Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.099Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.107Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.121Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.133Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.157Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.187Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.198Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.220Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.233Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.244Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.268Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.276Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.295Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.302Z: JOB_MESSAGE_BASIC: Executing operation Write TF Test Data/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.327Z: JOB_MESSAGE_BASIC: Executing operation Write JSONL Test Data/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.328Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.351Z: JOB_MESSAGE_BASIC: Finished operation Write TF Test Data/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.363Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.379Z: JOB_MESSAGE_BASIC: Finished operation Write JSONL Test Data/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.396Z: JOB_MESSAGE_BASIC: Executing operation Read Raw Data/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(ParDo(PassThrough).cleanup_signal.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.411Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.430Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.439Z: JOB_MESSAGE_BASIC: Finished operation Read Raw Data/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(ParDo(PassThrough).cleanup_signal.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.464Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.499Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.533Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.564Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.601Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.613Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.635Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.650Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.669Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/Combine+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/Combine/Extract+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/Map(StripNonce)+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/WindowIntoOriginal+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/Combine/Partial+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.694Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.702Z: JOB_MESSAGE_BASIC: Executing operation Write TF Test Data/Write/WriteImpl/GroupByKey/Read+Write TF Test Data/Write/WriteImpl/GroupByKey/GroupByWindow+Write TF Test Data/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.726Z: JOB_MESSAGE_BASIC: Executing operation Write JSONL Test Data/Write/WriteImpl/GroupByKey/Read+Write JSONL Test Data/Write/WriteImpl/GroupByKey/GroupByWindow+Write JSONL Test Data/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.738Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.752Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.762Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.772Z: JOB_MESSAGE_DEBUG: Value \"Read Raw Data/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/_UnpickledSideInput(ParDo(PassThrough).cleanup_signal.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.777Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.796Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.812Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.818Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.848Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.872Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.904Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.930Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.941Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:27.955Z: JOB_MESSAGE_BASIC: Executing operation Read Raw Data/_PassThroughThenCleanup/Create/Read+Read Raw Data/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/ParDo(RemoveExtractedFiles)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:28.014Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:28.051Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:28.086Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:28.120Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:28.187Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:28.224Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:28.249Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:28.275Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:28.308Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:28.344Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:29:28.371Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/Count/CreateSole/Read+Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/Count/Count\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:09.153Z: JOB_MESSAGE_BASIC: Finished operation Read Raw Data/_PassThroughThenCleanup/Create/Read+Read Raw Data/_PassThroughThenCleanup/ParDo(RemoveExtractedFiles)/ParDo(RemoveExtractedFiles)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:11.269Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/InstrumentAPI/CreateSoleAPIUse/Read+Analyze & Transform/AnalyzeDataset/InstrumentAPI/CountAPIUse\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:14.237Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Train Data/Write/WriteImpl/DoOnce/Read+Write Transformed Train Data/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:14.292Z: JOB_MESSAGE_DEBUG: Value \"Write Transformed Train Data/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:14.324Z: JOB_MESSAGE_DEBUG: Value \"Write Transformed Train Data/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:14.400Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Train Data/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:14.422Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Train Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:14.443Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Train Data/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:14.455Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Train Data/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:14.465Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Train Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:14.496Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Train Data/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:14.509Z: JOB_MESSAGE_DEBUG: Value \"Write Transformed Train Data/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:14.542Z: JOB_MESSAGE_DEBUG: Value \"Write Transformed Train Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:14.641Z: JOB_MESSAGE_DEBUG: Value \"Write Transformed Train Data/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:15.952Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:16.021Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:16.057Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:16.122Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:16.146Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:16.182Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:16.218Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:16.240Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:16.266Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:16.268Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:16.304Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:16.335Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:17.794Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_7#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:17.856Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:17.906Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:17.980Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:18.125Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:18.221Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:18.274Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/Combine+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/SwapTokensAndCounts/KvSwap+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid+Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_7#vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Write+Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_7#vocabulary]/ApplyThresholdsAndTopK/EncodeNumericalTerms+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/BatchAndPreSort/BatchVocabulary/ParDo(_GlobalWindowsBatchingDoFn)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/BatchAndPreSort/SortBatches\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:20.016Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_5#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:20.081Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:20.131Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:20.233Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:20.392Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:20.450Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:20.521Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/Combine+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/SwapTokensAndCounts/KvSwap+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid+Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_5#vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Write+Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_5#vocabulary]/ApplyThresholdsAndTopK/EncodeNumericalTerms+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/BatchAndPreSort/BatchVocabulary/ParDo(_GlobalWindowsBatchingDoFn)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/BatchAndPreSort/SortBatches\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:28.942Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/Combine+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/MergeCountPerToken/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_7#vocabulary]/SwapTokensAndCounts/KvSwap+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid+Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_7#vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Write+Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_7#vocabulary]/ApplyThresholdsAndTopK/EncodeNumericalTerms+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/BatchAndPreSort/BatchVocabulary/ParDo(_GlobalWindowsBatchingDoFn)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/BatchAndPreSort/SortBatches\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:29.017Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/BatchAndPreSort/SortBatches.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:29.052Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:29.087Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/OrderElements/_UnpickledSideInput(SortBatches.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:29.101Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:29.142Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/OrderElements/_UnpickledSideInput(SortBatches.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:29.166Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:29.220Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/OrderElements/_UnpickledSideInput(SortBatches.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:29.268Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:29.472Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:29.527Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:29.595Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/Prepare/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/OrderElements/OrderElements+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1130>)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:30.323Z: JOB_MESSAGE_BASIC: Finished operation Write TF Test Data/Write/WriteImpl/GroupByKey/Read+Write TF Test Data/Write/WriteImpl/GroupByKey/GroupByWindow+Write TF Test Data/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:30.389Z: JOB_MESSAGE_DEBUG: Value \"Write TF Test Data/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:30.448Z: JOB_MESSAGE_BASIC: Executing operation Write TF Test Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:30.484Z: JOB_MESSAGE_BASIC: Executing operation Write TF Test Data/Write/WriteImpl/PreFinalize/_UnpickledSideInput(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:30.485Z: JOB_MESSAGE_BASIC: Finished operation Write TF Test Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:30.533Z: JOB_MESSAGE_BASIC: Finished operation Write TF Test Data/Write/WriteImpl/PreFinalize/_UnpickledSideInput(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:30.566Z: JOB_MESSAGE_DEBUG: Value \"Write TF Test Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:30.598Z: JOB_MESSAGE_DEBUG: Value \"Write TF Test Data/Write/WriteImpl/PreFinalize/_UnpickledSideInput(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:30.657Z: JOB_MESSAGE_BASIC: Executing operation Write TF Test Data/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:31.986Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/Prepare/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/OrderElements/OrderElements+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1130>)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:32.048Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:32.096Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:32.153Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:36.525Z: JOB_MESSAGE_BASIC: Finished operation Write TF Test Data/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:36.593Z: JOB_MESSAGE_DEBUG: Value \"Write TF Test Data/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:36.649Z: JOB_MESSAGE_BASIC: Executing operation Write TF Test Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:36.694Z: JOB_MESSAGE_BASIC: Finished operation Write TF Test Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:36.735Z: JOB_MESSAGE_DEBUG: Value \"Write TF Test Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:36.780Z: JOB_MESSAGE_BASIC: Executing operation Write TF Test Data/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:41.886Z: JOB_MESSAGE_BASIC: Finished operation Write TF Test Data/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:42.714Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:42.773Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:42.827Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/_UnpickledSideInput(UnKey.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:42.877Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/_UnpickledSideInput(UnKey.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:42.943Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/_UnpickledSideInput(UnKey.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:42.994Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/Read+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/InjectDefault+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/ToInt64+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_7#vocabulary#temporary_analyzer_output#trip_month_unpruned_vocab_size]/ToTensorBinding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:43.344Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/Combine+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/MergeCountPerToken/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_5#vocabulary]/SwapTokensAndCounts/KvSwap+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid+Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_5#vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Write+Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_5#vocabulary]/ApplyThresholdsAndTopK/EncodeNumericalTerms+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/BatchAndPreSort/BatchVocabulary/ParDo(_GlobalWindowsBatchingDoFn)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/BatchAndPreSort/SortBatches\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:43.407Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/BatchAndPreSort/SortBatches.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:43.441Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:43.472Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/OrderElements/_UnpickledSideInput(SortBatches.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:43.513Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/OrderElements/_UnpickledSideInput(SortBatches.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:43.573Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/OrderElements/_UnpickledSideInput(SortBatches.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:43.574Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_4#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:43.629Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:43.634Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:43.659Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:43.682Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:43.724Z: JOB_MESSAGE_BASIC: Finished operation Write JSONL Test Data/Write/WriteImpl/GroupByKey/Read+Write JSONL Test Data/Write/WriteImpl/GroupByKey/GroupByWindow+Write JSONL Test Data/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:43.783Z: JOB_MESSAGE_DEBUG: Value \"Write JSONL Test Data/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:43.840Z: JOB_MESSAGE_BASIC: Executing operation Write JSONL Test Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:43.871Z: JOB_MESSAGE_BASIC: Executing operation Write JSONL Test Data/Write/WriteImpl/PreFinalize/_UnpickledSideInput(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:43.884Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:43.889Z: JOB_MESSAGE_BASIC: Finished operation Write JSONL Test Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:43.909Z: JOB_MESSAGE_BASIC: Finished operation Write JSONL Test Data/Write/WriteImpl/PreFinalize/_UnpickledSideInput(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:43.948Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:43.974Z: JOB_MESSAGE_DEBUG: Value \"Write JSONL Test Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:44.024Z: JOB_MESSAGE_DEBUG: Value \"Write JSONL Test Data/Write/WriteImpl/PreFinalize/_UnpickledSideInput(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:44.061Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/Prepare/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/OrderElements/OrderElements+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1130>)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:44.093Z: JOB_MESSAGE_BASIC: Executing operation Write JSONL Test Data/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:44.296Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:44.353Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:44.505Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:44.567Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:44.623Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/Combine+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/SwapTokensAndCounts/KvSwap+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid+Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_4#vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Write+Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_4#vocabulary]/ApplyThresholdsAndTopK/EncodeNumericalTerms+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/BatchAndPreSort/BatchVocabulary/ParDo(_GlobalWindowsBatchingDoFn)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/BatchAndPreSort/SortBatches\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:44.954Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_1#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:45.020Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:45.081Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:45.148Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:45.303Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:45.363Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:45.420Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/Combine+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/SwapTokensAndCounts/KvSwap+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid+Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_1#vocabulary]/KeepOnlyValidStrings+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Write+Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_1#vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/BatchAndPreSort/BatchVocabulary/ParDo(_GlobalWindowsBatchingDoFn)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/BatchAndPreSort/SortBatches\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:48.505Z: JOB_MESSAGE_BASIC: Finished operation Write JSONL Test Data/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:48.583Z: JOB_MESSAGE_DEBUG: Value \"Write JSONL Test Data/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:48.642Z: JOB_MESSAGE_BASIC: Executing operation Write JSONL Test Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:48.686Z: JOB_MESSAGE_BASIC: Finished operation Write JSONL Test Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:48.750Z: JOB_MESSAGE_DEBUG: Value \"Write JSONL Test Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:48.818Z: JOB_MESSAGE_BASIC: Executing operation Write JSONL Test Data/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:49Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/Read+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/InjectDefault+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_7#vocabulary]/ToInt64+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_7#vocabulary#temporary_analyzer_output#trip_month_unpruned_vocab_size]/ToTensorBinding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:49.077Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_7#vocabulary#temporary_analyzer_output#trip_month_unpruned_vocab_size]/ToTensorBinding.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:49.342Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.16)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:49.411Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.16)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:49.474Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.16).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:49.956Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/Prepare/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/OrderElements/OrderElements+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1130>)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:50.025Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:51.014Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:51.125Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:51.222Z: JOB_MESSAGE_BASIC: Finished operation Write JSONL Test Data/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:57.595Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:57.652Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:57.730Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/_UnpickledSideInput(UnKey.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:57.782Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/_UnpickledSideInput(UnKey.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:57.852Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/_UnpickledSideInput(UnKey.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:57.952Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/Read+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/InjectDefault+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/ToInt64+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_5#vocabulary#temporary_analyzer_output#trip_day_of_week_unpruned_vocab_size]/ToTensorBinding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:58.031Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:58.108Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:58.179Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(WriteBundles.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:58.229Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(WriteBundles.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:58.245Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(WriteBundles.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:58.280Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(WriteBundles.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:58.329Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(WriteBundles.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:58.359Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(WriteBundles.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:58.438Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:58.834Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/Combine+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/MergeCountPerToken/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_1#vocabulary]/SwapTokensAndCounts/KvSwap+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid+Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_1#vocabulary]/KeepOnlyValidStrings+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Write+Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_1#vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/BatchAndPreSort/BatchVocabulary/ParDo(_GlobalWindowsBatchingDoFn)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/BatchAndPreSort/SortBatches\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:59.412Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/BatchAndPreSort/SortBatches.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:59.568Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:59.849Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/OrderElements/_UnpickledSideInput(SortBatches.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:59.913Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/OrderElements/_UnpickledSideInput(SortBatches.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:30:59.930Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:00.203Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/OrderElements/_UnpickledSideInput(SortBatches.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:00.244Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:00.387Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:00.676Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:00.772Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:00.904Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/Prepare/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/OrderElements/OrderElements+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1130>)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:01.405Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/Read+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/InjectDefault+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_5#vocabulary]/ToInt64+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_5#vocabulary#temporary_analyzer_output#trip_day_of_week_unpruned_vocab_size]/ToTensorBinding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:01.481Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_5#vocabulary#temporary_analyzer_output#trip_day_of_week_unpruned_vocab_size]/ToTensorBinding.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:01.527Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.12)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:01.563Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.12)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:01.617Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.12).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:02.305Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:02.364Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:02.421Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:02.442Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/Prepare/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/OrderElements/OrderElements+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1130>)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:02.461Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:02.508Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:02.540Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:02.562Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:02.611Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:02.644Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:04.153Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:04.237Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:04.369Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/_UnpickledSideInput(UnKey.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:04.437Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/_UnpickledSideInput(UnKey.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:04.569Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/_UnpickledSideInput(UnKey.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:04.820Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/Read+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/InjectDefault+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/ToInt64+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_1#vocabulary#temporary_analyzer_output#loc_cross_unpruned_vocab_size]/ToTensorBinding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:05.822Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:05.875Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:05.932Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WaitForVocabularyFile/_UnpickledSideInput(FinalizeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:05.979Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WaitForVocabularyFile/_UnpickledSideInput(FinalizeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:06.034Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WaitForVocabularyFile/_UnpickledSideInput(FinalizeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:06.096Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/CreatePath/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WaitForVocabularyFile/WaitForVocabularyFile+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_5#vocabulary#temporary_analyzer_output_1#Const]/ToTensorBinding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:08.431Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/Read+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/InjectDefault+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_1#vocabulary]/ToInt64+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_1#vocabulary#temporary_analyzer_output#loc_cross_unpruned_vocab_size]/ToTensorBinding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:08.493Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_1#vocabulary#temporary_analyzer_output#loc_cross_unpruned_vocab_size]/ToTensorBinding.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:08.569Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.4)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:08.603Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.4)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:08.625Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/CreatePath/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_5#vocabulary]/WaitForVocabularyFile/WaitForVocabularyFile+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_5#vocabulary#temporary_analyzer_output_1#Const]/ToTensorBinding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:08.658Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.4).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:08.692Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_5#vocabulary#temporary_analyzer_output_1#Const]/ToTensorBinding.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:08.776Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.13)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:08.981Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.13)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:09.093Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.13).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:09.096Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:09.114Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:09.154Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:09.177Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:09.231Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_3#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:09.244Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(WriteBundles.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:09.267Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(WriteBundles.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:09.281Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(WriteBundles.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:09.291Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:09.308Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(WriteBundles.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:09.336Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(WriteBundles.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:09.371Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(WriteBundles.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:09.426Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:09.457Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_2#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:09.528Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:09.702Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/Combine+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/Combine/Extract+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/Map(StripNonce)+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/WindowIntoOriginal+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/GroupByKey+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/Combine/Partial+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:09.761Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:10.341Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine+Analyze & Transform/AnalyzeDataset/VocabularyAccumulate[compute_and_apply_vocabulary_6#vocabulary]/CountPerToken/CombinePerKey(CountCombineFn)/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:10.373Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/Combine+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/MergeCountPerToken/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_4#vocabulary]/SwapTokensAndCounts/KvSwap+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid+Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_4#vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Write+Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_4#vocabulary]/ApplyThresholdsAndTopK/EncodeNumericalTerms+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/BatchAndPreSort/BatchVocabulary/ParDo(_GlobalWindowsBatchingDoFn)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/BatchAndPreSort/SortBatches\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:10.402Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:10.404Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:10.426Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/BatchAndPreSort/SortBatches.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:10.458Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:10.490Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:10.513Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/OrderElements/_UnpickledSideInput(SortBatches.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:10.515Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:10.567Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/OrderElements/_UnpickledSideInput(SortBatches.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:10.570Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:10.573Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:10.678Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/OrderElements/_UnpickledSideInput(SortBatches.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:10.692Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:10.719Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:10.847Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:10.865Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:10.883Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:10.958Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:11.156Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:11.333Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:11.405Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:11.414Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:11.518Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/Combine+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/SwapTokensAndCounts/KvSwap+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid+Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary#vocabulary]/KeepOnlyValidStrings+Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary#vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/BatchAndPreSort/BatchVocabulary/ParDo(_GlobalWindowsBatchingDoFn)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/BatchAndPreSort/SortBatches+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:11.552Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:11.627Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/Combine+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/SwapTokensAndCounts/KvSwap+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid+Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_2#vocabulary]/KeepOnlyValidStrings+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Write+Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_2#vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/BatchAndPreSort/BatchVocabulary/ParDo(_GlobalWindowsBatchingDoFn)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/BatchAndPreSort/SortBatches\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:11.775Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/Combine+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/Combine/Extract+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/UnKey\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:11.850Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:11.915Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:11.957Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:11.958Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:12.034Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:12.091Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/Combine+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/SwapTokensAndCounts/KvSwap+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid+Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_3#vocabulary]/KeepOnlyValidStrings+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Write+Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_3#vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/BatchAndPreSort/BatchVocabulary/ParDo(_GlobalWindowsBatchingDoFn)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/BatchAndPreSort/SortBatches\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:12.131Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/Prepare/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/OrderElements/OrderElements+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1130>)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:12.165Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:12.191Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:12.257Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:12.305Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:12.311Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:12.348Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:12.381Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:12.405Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/Combine+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/SwapTokensAndCounts/KvSwap+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid+Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_6#vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Write+Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_6#vocabulary]/ApplyThresholdsAndTopK/EncodeNumericalTerms+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/BatchAndPreSort/BatchVocabulary/ParDo(_GlobalWindowsBatchingDoFn)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/BatchAndPreSort/SortBatches\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:12.698Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:12.737Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:12.794Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(WriteBundles.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:12.827Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(WriteBundles.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:12.834Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(WriteBundles.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:12.871Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(WriteBundles.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:12.873Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(WriteBundles.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:12.931Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(WriteBundles.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:12.989Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:14.549Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/Prepare/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/OrderElements/OrderElements+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1130>)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:14.606Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:15.163Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:15.239Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:20.551Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:20.611Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:20.657Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WaitForVocabularyFile/_UnpickledSideInput(FinalizeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:20.708Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WaitForVocabularyFile/_UnpickledSideInput(FinalizeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:20.767Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WaitForVocabularyFile/_UnpickledSideInput(FinalizeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:20.826Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/CreatePath/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WaitForVocabularyFile/WaitForVocabularyFile+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_1#vocabulary#temporary_analyzer_output_1#Const]/ToTensorBinding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:30.600Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:30.656Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:30.723Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:30.771Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:30.823Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:30.880Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:30.905Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/CreatePath/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1#vocabulary]/WaitForVocabularyFile/WaitForVocabularyFile+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_1#vocabulary#temporary_analyzer_output_1#Const]/ToTensorBinding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:30.961Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_1#vocabulary#temporary_analyzer_output_1#Const]/ToTensorBinding.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:31.037Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.5)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:31.077Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.5)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:31.126Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.5).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:31.230Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:31.290Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:31.393Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/_UnpickledSideInput(UnKey.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:31.526Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/_UnpickledSideInput(UnKey.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:31.669Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/_UnpickledSideInput(UnKey.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:31.774Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/Read+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/InjectDefault+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/ToInt64+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_4#vocabulary#temporary_analyzer_output#trip_day_unpruned_vocab_size]/ToTensorBinding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:31.822Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/Combine+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/MergeCountPerToken/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_6#vocabulary]/SwapTokensAndCounts/KvSwap+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid+Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_6#vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Write+Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_6#vocabulary]/ApplyThresholdsAndTopK/EncodeNumericalTerms+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/BatchAndPreSort/BatchVocabulary/ParDo(_GlobalWindowsBatchingDoFn)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/BatchAndPreSort/SortBatches\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:31.877Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/BatchAndPreSort/SortBatches.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:31.901Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:31.922Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/OrderElements/_UnpickledSideInput(SortBatches.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:31.952Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:31.959Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/OrderElements/_UnpickledSideInput(SortBatches.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:32.041Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:32.069Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/OrderElements/_UnpickledSideInput(SortBatches.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:32.128Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:32.210Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/Combine+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/MergeCountPerToken/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary#vocabulary]/SwapTokensAndCounts/KvSwap+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid+Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary#vocabulary]/KeepOnlyValidStrings+Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary#vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/BatchAndPreSort/BatchVocabulary/ParDo(_GlobalWindowsBatchingDoFn)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/BatchAndPreSort/SortBatches+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:32.302Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/BatchAndPreSort/SortBatches.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:32.312Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:32.337Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:32.367Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/OrderElements/_UnpickledSideInput(SortBatches.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:32.383Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:32.403Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:32.405Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/OrderElements/_UnpickledSideInput(SortBatches.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:32.443Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:32.477Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/Prepare/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/OrderElements/OrderElements+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1130>)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:32.499Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/OrderElements/_UnpickledSideInput(SortBatches.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:32.556Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:32.559Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/Combine+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/Combine/Extract+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/UnKey\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:32.610Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/UnKey.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:32.666Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/InjectDefault/_UnpickledSideInput(UnKey.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:32.705Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/InjectDefault/_UnpickledSideInput(UnKey.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:32.729Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:32.754Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/InjectDefault/_UnpickledSideInput(UnKey.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:32.786Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:32.810Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:32.842Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/Prepare/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/OrderElements/OrderElements+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1130>)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:32.959Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:33.042Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:33.101Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/DoOnce/Read+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/InjectDefault/InjectDefault+Analyze & Transform/AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score#mean_and_var]/ExtractKeys+Analyze & Transform/AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1#mean_and_var]/ExtractKeys+Analyze & Transform/AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2#mean_and_var]/ExtractKeys+Analyze & Transform/AnalyzeDataset/AddKey[CacheableCombineMerge[scale_to_z_score#mean_and_var]]/AddKey+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/KeyWithVoid+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/Combine/Partial+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/AddKey[CacheableCombineMerge[scale_to_z_score_1#mean_and_var]]/AddKey+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/KeyWithVoid+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/Combine/Partial+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/AddKey[CacheableCombineMerge[scale_to_z_score_2#mean_and_var]]/AddKey+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/KeyWithVoid+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/Combine/Partial+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/GroupByKey/Session/Flatten+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:34.129Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:34.198Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:34.260Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WaitForVocabularyFile/_UnpickledSideInput(FinalizeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:34.303Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WaitForVocabularyFile/_UnpickledSideInput(FinalizeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:34.364Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WaitForVocabularyFile/_UnpickledSideInput(FinalizeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:34.418Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/CreatePath/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WaitForVocabularyFile/WaitForVocabularyFile+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_7#vocabulary#temporary_analyzer_output_1#Const]/ToTensorBinding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:36.135Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/Read+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/InjectDefault+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_4#vocabulary]/ToInt64+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_4#vocabulary#temporary_analyzer_output#trip_day_unpruned_vocab_size]/ToTensorBinding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:36.157Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/Prepare/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/OrderElements/OrderElements+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1130>)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:36.213Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_4#vocabulary#temporary_analyzer_output#trip_day_unpruned_vocab_size]/ToTensorBinding.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:36.267Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:36.294Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.10)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:36.341Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.10)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:36.411Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.10).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:36.610Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/Prepare/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/OrderElements/OrderElements+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1130>)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:36.662Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:37.084Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/CreatePath/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_7#vocabulary]/WaitForVocabularyFile/WaitForVocabularyFile+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_7#vocabulary#temporary_analyzer_output_1#Const]/ToTensorBinding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:37.137Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_7#vocabulary#temporary_analyzer_output_1#Const]/ToTensorBinding.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:37.187Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.17)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:37.231Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.17)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:37.319Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.17).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:37.349Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:37.414Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:37.428Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/DoOnce/Read+Analyze & Transform/AnalyzeDataset/PackedCombineAccumulate[ApplySavedModel[Phase0]]/InitialPackedCombineGlobally/InjectDefault/InjectDefault+Analyze & Transform/AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score#mean_and_var]/ExtractKeys+Analyze & Transform/AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_1#mean_and_var]/ExtractKeys+Analyze & Transform/AnalyzeDataset/CacheableCombineAccumulate[scale_to_z_score_2#mean_and_var]/ExtractKeys+Analyze & Transform/AnalyzeDataset/AddKey[CacheableCombineMerge[scale_to_z_score#mean_and_var]]/AddKey+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/KeyWithVoid+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/Combine/Partial+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/AddKey[CacheableCombineMerge[scale_to_z_score_1#mean_and_var]]/AddKey+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/KeyWithVoid+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/Combine/Partial+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/AddKey[CacheableCombineMerge[scale_to_z_score_2#mean_and_var]]/AddKey+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/KeyWithVoid+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/Combine/Partial+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/GroupByKey/Session/Flatten+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:37.487Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:37.621Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:37.689Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:37.794Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:37.845Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/Combine+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/Combine/Extract+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/UnKey\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:38.104Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:38.157Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:38.212Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/_UnpickledSideInput(UnKey.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:38.276Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/_UnpickledSideInput(UnKey.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:38.335Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/_UnpickledSideInput(UnKey.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:38.402Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/Read+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/InjectDefault+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/ToInt64+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary#vocabulary#temporary_analyzer_output#dropoff_grid_unpruned_vocab_size]/ToTensorBinding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:38.406Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:38.477Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:38.535Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/_UnpickledSideInput(UnKey.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:38.601Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/_UnpickledSideInput(UnKey.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:38.666Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/_UnpickledSideInput(UnKey.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:38.722Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/Read+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/InjectDefault+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/ToInt64+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_6#vocabulary#temporary_analyzer_output#trip_hour_unpruned_vocab_size]/ToTensorBinding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:41.484Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/Read+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/InjectDefault+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary#vocabulary]/ToInt64+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary#vocabulary#temporary_analyzer_output#dropoff_grid_unpruned_vocab_size]/ToTensorBinding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:41.557Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary#vocabulary#temporary_analyzer_output#dropoff_grid_unpruned_vocab_size]/ToTensorBinding.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:41.624Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.2)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:41.672Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.2)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:41.736Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.2).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:41.906Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/Read+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/InjectDefault+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_6#vocabulary]/ToInt64+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_6#vocabulary#temporary_analyzer_output#trip_hour_unpruned_vocab_size]/ToTensorBinding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:41.949Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_6#vocabulary#temporary_analyzer_output#trip_hour_unpruned_vocab_size]/ToTensorBinding.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:42.030Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.14)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:42.071Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.14)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:42.132Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.14).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:42.774Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/Combine+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/CombinePerKey/Combine/Extract+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/UnKey\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:42.827Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/UnKey.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:42.894Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/InjectDefault/_UnpickledSideInput(UnKey.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:42.942Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/InjectDefault/_UnpickledSideInput(UnKey.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:43.006Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/InjectDefault/_UnpickledSideInput(UnKey.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:43.072Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/DoOnce/Read+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/InjectDefault/InjectDefault+Analyze & Transform/AnalyzeDataset/ExtractFromDict[CacheableCombineMerge[scale_to_z_score#mean_and_var]]/ExtractKeys+Analyze & Transform/AnalyzeDataset/ExtractFromDict[CacheableCombineMerge[scale_to_z_score_1#mean_and_var]]/ExtractKeys+Analyze & Transform/AnalyzeDataset/ExtractFromDict[CacheableCombineMerge[scale_to_z_score_2#mean_and_var]]/ExtractKeys+Analyze & Transform/AnalyzeDataset/ExtractPackedCombineMergeOutputs[CacheableCombineMerge[scale_to_z_score_2#mean_and_var]]/ExtractOutputs/FlatMap(extract_outputs)+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[scale_to_z_score_2#mean_and_var#temporary_analyzer_output#PlaceholderWithDefault]/ToTensorBinding+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[scale_to_z_score_2#mean_and_var#temporary_analyzer_output_1#PlaceholderWithDefault]/ToTensorBinding+Analyze & Transform/AnalyzeDataset/ExtractPackedCombineMergeOutputs[CacheableCombineMerge[scale_to_z_score#mean_and_var]]/ExtractOutputs/FlatMap(extract_outputs)+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[scale_to_z_score#mean_and_var#temporary_analyzer_output#PlaceholderWithDefault]/ToTensorBinding+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[scale_to_z_score#mean_and_var#temporary_analyzer_output_1#PlaceholderWithDefault]/ToTensorBinding+Analyze & Transform/AnalyzeDataset/ExtractPackedCombineMergeOutputs[CacheableCombineMerge[scale_to_z_score_1#mean_and_var]]/ExtractOutputs/FlatMap(extract_outputs)+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[scale_to_z_score_1#mean_and_var#temporary_analyzer_output#PlaceholderWithDefault]/ToTensorBinding+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[scale_to_z_score_1#mean_and_var#temporary_analyzer_output_1#PlaceholderWithDefault]/ToTensorBinding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:43.188Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/Combine+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/MergeCountPerToken/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_2#vocabulary]/SwapTokensAndCounts/KvSwap+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid+Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_2#vocabulary]/KeepOnlyValidStrings+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Write+Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_2#vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/BatchAndPreSort/BatchVocabulary/ParDo(_GlobalWindowsBatchingDoFn)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/BatchAndPreSort/SortBatches\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:43.253Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/BatchAndPreSort/SortBatches.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:43.282Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:43.313Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/OrderElements/_UnpickledSideInput(SortBatches.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:43.334Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:43.358Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/OrderElements/_UnpickledSideInput(SortBatches.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:43.410Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:43.432Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/OrderElements/_UnpickledSideInput(SortBatches.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:43.490Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:43.642Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:43.719Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:43.776Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/Prepare/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/OrderElements/OrderElements+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1130>)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:43.841Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:43.908Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:43.965Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(WriteBundles.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:43.999Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(WriteBundles.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:44.006Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(WriteBundles.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:44.036Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(WriteBundles.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:44.065Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(WriteBundles.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:44.099Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(WriteBundles.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:44.168Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:44.199Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/Combine+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/MergeCountPerToken/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyMerge[compute_and_apply_vocabulary_3#vocabulary]/SwapTokensAndCounts/KvSwap+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid+Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_3#vocabulary]/KeepOnlyValidStrings+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Partial+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Write+Analyze & Transform/AnalyzeDataset/VocabularyPrune[compute_and_apply_vocabulary_3#vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/BatchAndPreSort/BatchVocabulary/ParDo(_GlobalWindowsBatchingDoFn)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/BatchAndPreSort/SortBatches\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:44.284Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/BatchAndPreSort/SortBatches.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:44.306Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:44.346Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/Prepare/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/OrderElements/OrderElements+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1130>)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:44.352Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/OrderElements/_UnpickledSideInput(SortBatches.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:44.406Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/OrderElements/_UnpickledSideInput(SortBatches.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:44.414Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:44.458Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/OrderElements/_UnpickledSideInput(SortBatches.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:44.464Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:44.514Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:44.541Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:44.650Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:44.719Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:44.784Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/Prepare/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/OrderElements/OrderElements+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1130>)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:44.843Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:44.896Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:46.224Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/DoOnce/Read+Analyze & Transform/AnalyzeDataset/PackedCombineMerge[3]/MergePackedCombinesGlobally/InjectDefault/InjectDefault+Analyze & Transform/AnalyzeDataset/ExtractFromDict[CacheableCombineMerge[scale_to_z_score#mean_and_var]]/ExtractKeys+Analyze & Transform/AnalyzeDataset/ExtractFromDict[CacheableCombineMerge[scale_to_z_score_1#mean_and_var]]/ExtractKeys+Analyze & Transform/AnalyzeDataset/ExtractFromDict[CacheableCombineMerge[scale_to_z_score_2#mean_and_var]]/ExtractKeys+Analyze & Transform/AnalyzeDataset/ExtractPackedCombineMergeOutputs[CacheableCombineMerge[scale_to_z_score_2#mean_and_var]]/ExtractOutputs/FlatMap(extract_outputs)+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[scale_to_z_score_2#mean_and_var#temporary_analyzer_output#PlaceholderWithDefault]/ToTensorBinding+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[scale_to_z_score_2#mean_and_var#temporary_analyzer_output_1#PlaceholderWithDefault]/ToTensorBinding+Analyze & Transform/AnalyzeDataset/ExtractPackedCombineMergeOutputs[CacheableCombineMerge[scale_to_z_score#mean_and_var]]/ExtractOutputs/FlatMap(extract_outputs)+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[scale_to_z_score#mean_and_var#temporary_analyzer_output#PlaceholderWithDefault]/ToTensorBinding+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[scale_to_z_score#mean_and_var#temporary_analyzer_output_1#PlaceholderWithDefault]/ToTensorBinding+Analyze & Transform/AnalyzeDataset/ExtractPackedCombineMergeOutputs[CacheableCombineMerge[scale_to_z_score_1#mean_and_var]]/ExtractOutputs/FlatMap(extract_outputs)+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[scale_to_z_score_1#mean_and_var#temporary_analyzer_output#PlaceholderWithDefault]/ToTensorBinding+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[scale_to_z_score_1#mean_and_var#temporary_analyzer_output_1#PlaceholderWithDefault]/ToTensorBinding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:46.320Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateTensorBinding[scale_to_z_score_2#mean_and_var#temporary_analyzer_output#PlaceholderWithDefault]/ToTensorBinding.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:46.343Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateTensorBinding[scale_to_z_score_2#mean_and_var#temporary_analyzer_output_1#PlaceholderWithDefault]/ToTensorBinding.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:46.365Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateTensorBinding[scale_to_z_score#mean_and_var#temporary_analyzer_output#PlaceholderWithDefault]/ToTensorBinding.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:46.399Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateTensorBinding[scale_to_z_score#mean_and_var#temporary_analyzer_output_1#PlaceholderWithDefault]/ToTensorBinding.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:46.420Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateTensorBinding[scale_to_z_score_1#mean_and_var#temporary_analyzer_output#PlaceholderWithDefault]/ToTensorBinding.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:46.454Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateTensorBinding[scale_to_z_score_1#mean_and_var#temporary_analyzer_output_1#PlaceholderWithDefault]/ToTensorBinding.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:46.487Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:46.518Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.1)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:46.537Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:46.540Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.18)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:46.554Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.1)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:46.567Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.19)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:46.586Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.18)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:46.588Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.20)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:46.600Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.19)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:46.609Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.21)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:46.636Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.1).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:46.652Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.21)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:46.663Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:46.663Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.20)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:46.697Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.19).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:46.733Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.18).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:46.757Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.21).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:46.786Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.20).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:47.002Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:47.069Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:47.130Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:47.197Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:47.281Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:47.341Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:47.737Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/Prepare/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/OrderElements/OrderElements+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1130>)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn)+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Reify+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:47.807Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:48.301Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:48.350Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:50.092Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:50.151Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:50.200Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WaitForVocabularyFile/_UnpickledSideInput(FinalizeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:50.258Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WaitForVocabularyFile/_UnpickledSideInput(FinalizeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:50.321Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WaitForVocabularyFile/_UnpickledSideInput(FinalizeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:50.367Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/CreatePath/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WaitForVocabularyFile/WaitForVocabularyFile+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_4#vocabulary#temporary_analyzer_output_1#Const]/ToTensorBinding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:51.812Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:51.874Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:51.928Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/_UnpickledSideInput(UnKey.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:51.967Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/_UnpickledSideInput(UnKey.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:52.047Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/_UnpickledSideInput(UnKey.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:52.103Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/Read+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/InjectDefault+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/ToInt64+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_3#vocabulary#temporary_analyzer_output#pickup_grid_unpruned_vocab_size]/ToTensorBinding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:52.205Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:52.296Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:52.342Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(WriteBundles.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:52.378Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(WriteBundles.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:52.387Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(WriteBundles.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:52.446Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(WriteBundles.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:52.562Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(WriteBundles.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:52.614Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(WriteBundles.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:52.672Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:52.708Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:52.769Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:52.830Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(WriteBundles.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:52.862Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(WriteBundles.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:52.871Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(WriteBundles.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:52.902Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(WriteBundles.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:52.928Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(WriteBundles.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:52.947Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(WriteBundles.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:53.007Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:53.510Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:53.569Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:53.629Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(WriteBundles.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:53.664Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(WriteBundles.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:53.674Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(WriteBundles.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:53.705Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(WriteBundles.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:53.733Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(WriteBundles.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:53.769Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(WriteBundles.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:53.825Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:54.032Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/Read+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/InjectDefault+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_3#vocabulary]/ToInt64+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_3#vocabulary#temporary_analyzer_output#pickup_grid_unpruned_vocab_size]/ToTensorBinding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:54.092Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_3#vocabulary#temporary_analyzer_output#pickup_grid_unpruned_vocab_size]/ToTensorBinding.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:54.149Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.8)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:54.198Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.8)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:54.288Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.8).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:55.011Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:55.071Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:55.126Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:55.166Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:55.214Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:55.297Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:56.321Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:56.373Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:56.436Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:56.473Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:56.529Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:56.586Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:57.003Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:57.058Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:57.117Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:57.190Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:57.261Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:57.327Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:58.432Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:58.491Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:58.548Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WaitForVocabularyFile/_UnpickledSideInput(FinalizeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:58.587Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WaitForVocabularyFile/_UnpickledSideInput(FinalizeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:58.625Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WaitForVocabularyFile/_UnpickledSideInput(FinalizeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:58.696Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/CreatePath/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WaitForVocabularyFile/WaitForVocabularyFile+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_3#vocabulary#temporary_analyzer_output_1#Const]/ToTensorBinding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:59.690Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:59.761Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:59.821Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WaitForVocabularyFile/_UnpickledSideInput(FinalizeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:59.855Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WaitForVocabularyFile/_UnpickledSideInput(FinalizeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:59.912Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WaitForVocabularyFile/_UnpickledSideInput(FinalizeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:31:59.972Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/CreatePath/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WaitForVocabularyFile/WaitForVocabularyFile+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_2#vocabulary#temporary_analyzer_output_1#Const]/ToTensorBinding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:01.145Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:01.266Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:01.330Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WaitForVocabularyFile/_UnpickledSideInput(FinalizeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:01.367Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WaitForVocabularyFile/_UnpickledSideInput(FinalizeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:01.424Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WaitForVocabularyFile/_UnpickledSideInput(FinalizeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:01.482Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/CreatePath/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WaitForVocabularyFile/WaitForVocabularyFile+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_6#vocabulary#temporary_analyzer_output_1#Const]/ToTensorBinding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:01.510Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/CreatePath/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2#vocabulary]/WaitForVocabularyFile/WaitForVocabularyFile+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_2#vocabulary#temporary_analyzer_output_1#Const]/ToTensorBinding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:01.589Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_2#vocabulary#temporary_analyzer_output_1#Const]/ToTensorBinding.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:01.653Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.7)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:01.691Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.7)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:01.746Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.7).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:01.796Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/CreatePath/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4#vocabulary]/WaitForVocabularyFile/WaitForVocabularyFile+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_4#vocabulary#temporary_analyzer_output_1#Const]/ToTensorBinding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:01.868Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_4#vocabulary#temporary_analyzer_output_1#Const]/ToTensorBinding.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:01.935Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.11)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:01.974Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.11)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:02Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/CreatePath/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3#vocabulary]/WaitForVocabularyFile/WaitForVocabularyFile+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_3#vocabulary#temporary_analyzer_output_1#Const]/ToTensorBinding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:02.048Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.11).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:02.083Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_3#vocabulary#temporary_analyzer_output_1#Const]/ToTensorBinding.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:02.134Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.9)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:02.173Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.9)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:02.251Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/GroupByWindow+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:02.265Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.9).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:02.308Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/WriteBundles.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:02.364Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(WriteBundles.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:02.396Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(WriteBundles.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:02.400Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(WriteBundles.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:02.453Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(WriteBundles.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:02.462Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(WriteBundles.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:02.506Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/_UnpickledSideInput(WriteBundles.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:02.562Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:02.590Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/GroupByKey/Read+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Combine/Extract+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:02.651Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:02.705Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/_UnpickledSideInput(UnKey.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:02.752Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/_UnpickledSideInput(UnKey.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:02.810Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/_UnpickledSideInput(UnKey.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:02.858Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/Read+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/InjectDefault+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/ToInt64+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_2#vocabulary#temporary_analyzer_output#payment_type_unpruned_vocab_size]/ToTensorBinding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:02.893Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/CreatePath/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary_6#vocabulary]/WaitForVocabularyFile/WaitForVocabularyFile+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_6#vocabulary#temporary_analyzer_output_1#Const]/ToTensorBinding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:02.952Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_6#vocabulary#temporary_analyzer_output_1#Const]/ToTensorBinding.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:03.031Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.15)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:03.069Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.15)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:03.127Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.15).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:04.502Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:04.554Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:04.597Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:04.637Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:04.699Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:04.755Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:05.096Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/Read+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault/InjectDefault+Analyze & Transform/AnalyzeDataset/VocabularyCount[compute_and_apply_vocabulary_2#vocabulary]/ToInt64+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_2#vocabulary#temporary_analyzer_output#payment_type_unpruned_vocab_size]/ToTensorBinding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:05.152Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary_2#vocabulary#temporary_analyzer_output#payment_type_unpruned_vocab_size]/ToTensorBinding.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:05.222Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.6)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:05.288Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.6)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:05.346Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.6).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:07.771Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:08.009Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:08.095Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WaitForVocabularyFile/_UnpickledSideInput(FinalizeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:08.132Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WaitForVocabularyFile/_UnpickledSideInput(FinalizeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:08.189Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WaitForVocabularyFile/_UnpickledSideInput(FinalizeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:08.267Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/CreatePath/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WaitForVocabularyFile/WaitForVocabularyFile+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary#vocabulary#temporary_analyzer_output_1#Const]/ToTensorBinding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:09.508Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/CreatePath/Read+Analyze & Transform/AnalyzeDataset/VocabularyOrderAndWrite[compute_and_apply_vocabulary#vocabulary]/WaitForVocabularyFile/WaitForVocabularyFile+Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary#vocabulary#temporary_analyzer_output_1#Const]/ToTensorBinding\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:09.573Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateTensorBinding[compute_and_apply_vocabulary#vocabulary#temporary_analyzer_output_1#Const]/ToTensorBinding.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:09.636Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.3)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:09.673Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.3)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:09.735Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/_UnpickledSideInput(ToTensorBinding.out.3).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:09.792Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/CreateSole/Read+Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/ReplaceWithConstants+Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/CreateSavedModel+Analyze & Transform/AnalyzeDataset/ComputeDeferredMetadata[compat_v1=False]+Analyze & Transform/AnalyzeDataset/MakeCheapBarrier+Write Transform Artifacts/WriteTransformFnToTemp+Write Transform Artifacts/WriteMetadataToTemp/WriteMetadata\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:50.519Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/CreateSole/Read+Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/ReplaceWithConstants/ReplaceWithConstants+Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/CreateSavedModel+Analyze & Transform/AnalyzeDataset/ComputeDeferredMetadata[compat_v1=False]+Analyze & Transform/AnalyzeDataset/MakeCheapBarrier+Write Transform Artifacts/WriteTransformFnToTemp+Write Transform Artifacts/WriteMetadataToTemp/WriteMetadata\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:50.572Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/CreateSavedModel[tf_v2_only]/CreateSavedModel.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:50.650Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/MakeCheapBarrier.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:50.682Z: JOB_MESSAGE_DEBUG: Value \"Write Transform Artifacts/WriteTransformFnToTemp.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:50.717Z: JOB_MESSAGE_DEBUG: Value \"Write Transform Artifacts/WriteMetadataToTemp/WriteMetadata.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:50.750Z: JOB_MESSAGE_BASIC: Executing operation Transform Test Data/Transform/_UnpickledSideInput(CreateSavedModel.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:50.783Z: JOB_MESSAGE_BASIC: Executing operation Transform Validation Data/Transform/_UnpickledSideInput(CreateSavedModel.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:50.797Z: JOB_MESSAGE_BASIC: Finished operation Transform Test Data/Transform/_UnpickledSideInput(CreateSavedModel.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:50.820Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/TransformDataset/Transform/_UnpickledSideInput(CreateSavedModel.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:50.824Z: JOB_MESSAGE_BASIC: Finished operation Transform Validation Data/Transform/_UnpickledSideInput(CreateSavedModel.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:50.854Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/WaitAndClearSharedKeepAlives/_UnpickledSideInput(MakeCheapBarrier.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:50.863Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/TransformDataset/Transform/_UnpickledSideInput(CreateSavedModel.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:50.886Z: JOB_MESSAGE_BASIC: Executing operation Write Transform Artifacts/PublishMetadataAndTransformFn/_UnpickledSideInput(WriteTransformFnToTemp.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:50.890Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/WaitAndClearSharedKeepAlives/_UnpickledSideInput(MakeCheapBarrier.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:50.913Z: JOB_MESSAGE_BASIC: Executing operation Write Transform Artifacts/PublishMetadataAndTransformFn/_UnpickledSideInput(WriteMetadata.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:50.927Z: JOB_MESSAGE_BASIC: Finished operation Write Transform Artifacts/PublishMetadataAndTransformFn/_UnpickledSideInput(WriteTransformFnToTemp.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:50.935Z: JOB_MESSAGE_DEBUG: Value \"Transform Test Data/Transform/_UnpickledSideInput(CreateSavedModel.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:50.949Z: JOB_MESSAGE_BASIC: Finished operation Write Transform Artifacts/PublishMetadataAndTransformFn/_UnpickledSideInput(WriteMetadata.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:50.961Z: JOB_MESSAGE_DEBUG: Value \"Transform Validation Data/Transform/_UnpickledSideInput(CreateSavedModel.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:50.987Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/TransformDataset/Transform/_UnpickledSideInput(CreateSavedModel.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:51.020Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/AnalyzeDataset/WaitAndClearSharedKeepAlives/_UnpickledSideInput(MakeCheapBarrier.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:51.047Z: JOB_MESSAGE_DEBUG: Value \"Write Transform Artifacts/PublishMetadataAndTransformFn/_UnpickledSideInput(WriteTransformFnToTemp.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:51.073Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Test Data/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:51.106Z: JOB_MESSAGE_DEBUG: Value \"Write Transform Artifacts/PublishMetadataAndTransformFn/_UnpickledSideInput(WriteMetadata.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:51.141Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Validation Data/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:51.176Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Train Data/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:51.213Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/AnalyzeDataset/PrepareToClearSharedKeepAlives/Read+Analyze & Transform/AnalyzeDataset/WaitAndClearSharedKeepAlives/WaitAndClearSharedKeepAlives\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:51.249Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Test Data/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:51.285Z: JOB_MESSAGE_BASIC: Executing operation Write Transform Artifacts/CreateSole/Read+Write Transform Artifacts/PublishMetadataAndTransformFn/PublishMetadataAndTransformFn\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:51.321Z: JOB_MESSAGE_DEBUG: Value \"Write Transformed Test Data/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:51.323Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Validation Data/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:51.363Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Train Data/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:51.378Z: JOB_MESSAGE_BASIC: Executing operation Transform Test Data/Transform/Transform+Transform Test Data/ConvertAndUnbatchToInstanceDicts+Transform Test Data/MakeCheapBarrier+Write Transformed Test Data/Write/WriteImpl/WindowInto(WindowIntoFn)+Write Transformed Test Data/Write/WriteImpl/WriteBundles/WriteBundles+Write Transformed Test Data/Write/WriteImpl/Pair+Write Transformed Test Data/Write/WriteImpl/GroupByKey/Reify+Write Transformed Test Data/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:51.413Z: JOB_MESSAGE_DEBUG: Value \"Write Transformed Validation Data/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:51.446Z: JOB_MESSAGE_DEBUG: Value \"Write Transformed Train Data/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:51.479Z: JOB_MESSAGE_BASIC: Executing operation Transform Validation Data/Transform/Transform+Transform Validation Data/ConvertAndUnbatchToInstanceDicts+Transform Validation Data/MakeCheapBarrier+Write Transformed Validation Data/Write/WriteImpl/WindowInto(WindowIntoFn)+Write Transformed Validation Data/Write/WriteImpl/WriteBundles/WriteBundles+Write Transformed Validation Data/Write/WriteImpl/Pair+Write Transformed Validation Data/Write/WriteImpl/GroupByKey/Reify+Write Transformed Validation Data/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:51.515Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/TransformDataset/Transform/Transform+Analyze & Transform/TransformDataset/ConvertAndUnbatchToInstanceDicts+Analyze & Transform/TransformDataset/MakeCheapBarrier+Write Transformed Train Data/Write/WriteImpl/WindowInto(WindowIntoFn)+Write Transformed Train Data/Write/WriteImpl/WriteBundles/WriteBundles+Write Transformed Train Data/Write/WriteImpl/Pair+Write Transformed Train Data/Write/WriteImpl/GroupByKey/Reify+Write Transformed Train Data/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:51.596Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/AnalyzeDataset/PrepareToClearSharedKeepAlives/Read+Analyze & Transform/AnalyzeDataset/WaitAndClearSharedKeepAlives/WaitAndClearSharedKeepAlives\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:32:54.894Z: JOB_MESSAGE_BASIC: Finished operation Write Transform Artifacts/CreateSole/Read+Write Transform Artifacts/PublishMetadataAndTransformFn/PublishMetadataAndTransformFn\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:33:32.945Z: JOB_MESSAGE_BASIC: Finished operation Transform Test Data/Transform/Transform+Transform Test Data/ConvertAndUnbatchToInstanceDicts+Transform Test Data/MakeCheapBarrier+Write Transformed Test Data/Write/WriteImpl/WindowInto(WindowIntoFn)+Write Transformed Test Data/Write/WriteImpl/WriteBundles/WriteBundles+Write Transformed Test Data/Write/WriteImpl/Pair+Write Transformed Test Data/Write/WriteImpl/GroupByKey/Reify+Write Transformed Test Data/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:33:33Z: JOB_MESSAGE_DEBUG: Value \"Transform Test Data/MakeCheapBarrier.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:33:33.035Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Test Data/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:33:33.070Z: JOB_MESSAGE_BASIC: Executing operation Transform Test Data/WaitAndClearSharedKeepAlives/_UnpickledSideInput(MakeCheapBarrier.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:33:33.085Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Test Data/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:33:33.117Z: JOB_MESSAGE_BASIC: Finished operation Transform Test Data/WaitAndClearSharedKeepAlives/_UnpickledSideInput(MakeCheapBarrier.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:33:33.138Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Test Data/Write/WriteImpl/GroupByKey/Read+Write Transformed Test Data/Write/WriteImpl/GroupByKey/GroupByWindow+Write Transformed Test Data/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:33:33.173Z: JOB_MESSAGE_DEBUG: Value \"Transform Test Data/WaitAndClearSharedKeepAlives/_UnpickledSideInput(MakeCheapBarrier.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:33:33.233Z: JOB_MESSAGE_BASIC: Executing operation Transform Test Data/PrepareToClearSharedKeepAlives/Read+Transform Test Data/WaitAndClearSharedKeepAlives/WaitAndClearSharedKeepAlives\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:33:48.735Z: JOB_MESSAGE_BASIC: Finished operation Transform Test Data/PrepareToClearSharedKeepAlives/Read+Transform Test Data/WaitAndClearSharedKeepAlives/WaitAndClearSharedKeepAlives\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:33:49.425Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Test Data/Write/WriteImpl/GroupByKey/Read+Write Transformed Test Data/Write/WriteImpl/GroupByKey/GroupByWindow+Write Transformed Test Data/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:33:49.492Z: JOB_MESSAGE_DEBUG: Value \"Write Transformed Test Data/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:33:49.583Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Test Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:33:49.638Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Test Data/Write/WriteImpl/PreFinalize/_UnpickledSideInput(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:33:49.746Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Test Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:33:49.755Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Test Data/Write/WriteImpl/PreFinalize/_UnpickledSideInput(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:33:49.813Z: JOB_MESSAGE_DEBUG: Value \"Write Transformed Test Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:33:49.858Z: JOB_MESSAGE_DEBUG: Value \"Write Transformed Test Data/Write/WriteImpl/PreFinalize/_UnpickledSideInput(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:33:49.923Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Test Data/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:33:58.330Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Test Data/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:33:58.402Z: JOB_MESSAGE_DEBUG: Value \"Write Transformed Test Data/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:33:58.460Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Test Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:33:58.508Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Test Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:33:58.560Z: JOB_MESSAGE_DEBUG: Value \"Write Transformed Test Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:33:58.628Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Test Data/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:01.379Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Test Data/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:03.519Z: JOB_MESSAGE_BASIC: Finished operation Transform Validation Data/Transform/Transform+Transform Validation Data/ConvertAndUnbatchToInstanceDicts+Transform Validation Data/MakeCheapBarrier+Write Transformed Validation Data/Write/WriteImpl/WindowInto(WindowIntoFn)+Write Transformed Validation Data/Write/WriteImpl/WriteBundles/WriteBundles+Write Transformed Validation Data/Write/WriteImpl/Pair+Write Transformed Validation Data/Write/WriteImpl/GroupByKey/Reify+Write Transformed Validation Data/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:03.571Z: JOB_MESSAGE_DEBUG: Value \"Transform Validation Data/MakeCheapBarrier.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:03.604Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Validation Data/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:03.628Z: JOB_MESSAGE_BASIC: Executing operation Transform Validation Data/WaitAndClearSharedKeepAlives/_UnpickledSideInput(MakeCheapBarrier.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:03.652Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Validation Data/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:03.693Z: JOB_MESSAGE_BASIC: Finished operation Transform Validation Data/WaitAndClearSharedKeepAlives/_UnpickledSideInput(MakeCheapBarrier.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:03.710Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Validation Data/Write/WriteImpl/GroupByKey/Read+Write Transformed Validation Data/Write/WriteImpl/GroupByKey/GroupByWindow+Write Transformed Validation Data/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:03.763Z: JOB_MESSAGE_DEBUG: Value \"Transform Validation Data/WaitAndClearSharedKeepAlives/_UnpickledSideInput(MakeCheapBarrier.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:03.821Z: JOB_MESSAGE_BASIC: Executing operation Transform Validation Data/PrepareToClearSharedKeepAlives/Read+Transform Validation Data/WaitAndClearSharedKeepAlives/WaitAndClearSharedKeepAlives\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:17.444Z: JOB_MESSAGE_BASIC: Finished operation Transform Validation Data/PrepareToClearSharedKeepAlives/Read+Transform Validation Data/WaitAndClearSharedKeepAlives/WaitAndClearSharedKeepAlives\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:32.191Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/TransformDataset/Transform/Transform+Analyze & Transform/TransformDataset/ConvertAndUnbatchToInstanceDicts+Analyze & Transform/TransformDataset/MakeCheapBarrier+Write Transformed Train Data/Write/WriteImpl/WindowInto(WindowIntoFn)+Write Transformed Train Data/Write/WriteImpl/WriteBundles/WriteBundles+Write Transformed Train Data/Write/WriteImpl/Pair+Write Transformed Train Data/Write/WriteImpl/GroupByKey/Reify+Write Transformed Train Data/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:32.267Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/TransformDataset/MakeCheapBarrier.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:32.320Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Train Data/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:32.352Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/TransformDataset/WaitAndClearSharedKeepAlives/_UnpickledSideInput(MakeCheapBarrier.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:32.374Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Train Data/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:32.395Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/TransformDataset/WaitAndClearSharedKeepAlives/_UnpickledSideInput(MakeCheapBarrier.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:32.426Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Validation Data/Write/WriteImpl/GroupByKey/Read+Write Transformed Validation Data/Write/WriteImpl/GroupByKey/GroupByWindow+Write Transformed Validation Data/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:32.434Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Train Data/Write/WriteImpl/GroupByKey/Read+Write Transformed Train Data/Write/WriteImpl/GroupByKey/GroupByWindow+Write Transformed Train Data/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:32.468Z: JOB_MESSAGE_DEBUG: Value \"Analyze & Transform/TransformDataset/WaitAndClearSharedKeepAlives/_UnpickledSideInput(MakeCheapBarrier.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:32.493Z: JOB_MESSAGE_DEBUG: Value \"Write Transformed Validation Data/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:32.525Z: JOB_MESSAGE_BASIC: Executing operation Analyze & Transform/TransformDataset/PrepareToClearSharedKeepAlives/Read+Analyze & Transform/TransformDataset/WaitAndClearSharedKeepAlives/WaitAndClearSharedKeepAlives\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:32.559Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Validation Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:32.582Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Validation Data/Write/WriteImpl/PreFinalize/_UnpickledSideInput(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:32.602Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Validation Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:32.628Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Validation Data/Write/WriteImpl/PreFinalize/_UnpickledSideInput(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:32.668Z: JOB_MESSAGE_DEBUG: Value \"Write Transformed Validation Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:32.701Z: JOB_MESSAGE_DEBUG: Value \"Write Transformed Validation Data/Write/WriteImpl/PreFinalize/_UnpickledSideInput(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:32.770Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Validation Data/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:33.003Z: JOB_MESSAGE_BASIC: Finished operation Analyze & Transform/TransformDataset/PrepareToClearSharedKeepAlives/Read+Analyze & Transform/TransformDataset/WaitAndClearSharedKeepAlives/WaitAndClearSharedKeepAlives\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:34.858Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Validation Data/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:34.927Z: JOB_MESSAGE_DEBUG: Value \"Write Transformed Validation Data/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:35.002Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Validation Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:35.045Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Validation Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:35.101Z: JOB_MESSAGE_DEBUG: Value \"Write Transformed Validation Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:35.160Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Validation Data/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:35.311Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Train Data/Write/WriteImpl/GroupByKey/Read+Write Transformed Train Data/Write/WriteImpl/GroupByKey/GroupByWindow+Write Transformed Train Data/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:35.368Z: JOB_MESSAGE_DEBUG: Value \"Write Transformed Train Data/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:35.418Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Train Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:35.440Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Train Data/Write/WriteImpl/PreFinalize/_UnpickledSideInput(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:35.459Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Train Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:35.482Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Train Data/Write/WriteImpl/PreFinalize/_UnpickledSideInput(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:35.514Z: JOB_MESSAGE_DEBUG: Value \"Write Transformed Train Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:35.539Z: JOB_MESSAGE_DEBUG: Value \"Write Transformed Train Data/Write/WriteImpl/PreFinalize/_UnpickledSideInput(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:35.600Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Train Data/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:37.062Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Validation Data/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:38.244Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Train Data/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:38.318Z: JOB_MESSAGE_DEBUG: Value \"Write Transformed Train Data/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:38.376Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Train Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:38.417Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Train Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:38.485Z: JOB_MESSAGE_DEBUG: Value \"Write Transformed Train Data/Write/WriteImpl/FinalizeWrite/_UnpickledSideInput(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:38.543Z: JOB_MESSAGE_BASIC: Executing operation Write Transformed Train Data/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:40.113Z: JOB_MESSAGE_BASIC: Finished operation Write Transformed Train Data/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:40.172Z: JOB_MESSAGE_DEBUG: Executing success step success756\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:40.474Z: JOB_MESSAGE_DETAILED: Cleaning up.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:40.544Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:34:40.575Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:36:55.188Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:36:55.265Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-03-03T09:36:55.293Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2022-03-03_01_18_20-2421235367684640617 is in state JOB_STATE_DONE\n",
      "Data preprocessing completed.\n",
      "gs://vertex-ai-devaip-20220303091526/transformed_data/train/data-00000-of-00006.gz\n",
      "gs://vertex-ai-devaip-20220303091526/transformed_data/train/data-00001-of-00006.gz\n",
      "gs://vertex-ai-devaip-20220303091526/transformed_data/train/data-00002-of-00006.gz\n",
      "gs://vertex-ai-devaip-20220303091526/transformed_data/train/data-00003-of-00006.gz\n",
      "gs://vertex-ai-devaip-20220303091526/transformed_data/train/data-00004-of-00006.gz\n",
      "gs://vertex-ai-devaip-20220303091526/transformed_data/train/data-00005-of-00006.gz\n",
      "gs://vertex-ai-devaip-20220303091526/transformed_data/val/data-00000-of-00005.gz\n",
      "gs://vertex-ai-devaip-20220303091526/transformed_data/val/data-00001-of-00005.gz\n",
      "gs://vertex-ai-devaip-20220303091526/transformed_data/val/data-00002-of-00005.gz\n",
      "gs://vertex-ai-devaip-20220303091526/transformed_data/val/data-00003-of-00005.gz\n",
      "gs://vertex-ai-devaip-20220303091526/transformed_data/val/data-00004-of-00005.gz\n",
      "gs://vertex-ai-devaip-20220303091526/transformed_data/test/data-00000-of-00005.gz\n",
      "gs://vertex-ai-devaip-20220303091526/transformed_data/test/data-00001-of-00005.gz\n",
      "gs://vertex-ai-devaip-20220303091526/transformed_data/test/data-00002-of-00005.gz\n",
      "gs://vertex-ai-devaip-20220303091526/transformed_data/test/data-00003-of-00005.gz\n",
      "gs://vertex-ai-devaip-20220303091526/transformed_data/test/data-00004-of-00005.gz\n",
      "gs://vertex-ai-devaip-20220303091526/transformed_artifacts/\n",
      "gs://vertex-ai-devaip-20220303091526/transformed_artifacts/transform_fn/\n",
      "gs://vertex-ai-devaip-20220303091526/transformed_artifacts/transformed_metadata/\n",
      "gs://vertex-ai-devaip-20220303091526/exported_data/jsonl-00000-of-00004.jsonl\n",
      "gs://vertex-ai-devaip-20220303091526/exported_data/jsonl-00001-of-00004.jsonl\n",
      "gs://vertex-ai-devaip-20220303091526/exported_data/jsonl-00002-of-00004.jsonl\n",
      "gs://vertex-ai-devaip-20220303091526/exported_data/jsonl-00003-of-00004.jsonl\n",
      "gs://vertex-ai-devaip-20220303091526/exported_data/tfrec/data-00000-of-00004.tfrecord\n",
      "gs://vertex-ai-devaip-20220303091526/exported_data/tfrec/data-00001-of-00004.tfrecord\n",
      "gs://vertex-ai-devaip-20220303091526/exported_data/tfrec/data-00002-of-00004.tfrecord\n",
      "gs://vertex-ai-devaip-20220303091526/exported_data/tfrec/data-00003-of-00004.tfrecord\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow_transform.beam as tft_beam\n",
    "from src import features\n",
    "\n",
    "RUNNER = \"DataflowRunner\"  # DirectRunner for local running w/o Dataflow\n",
    "\n",
    "\n",
    "def parse_bq_record(bq_record):\n",
    "    \"\"\"Parses a bq_record to a dictionary.\"\"\"\n",
    "    output = {}\n",
    "    for key in bq_record:\n",
    "        output[key] = [bq_record[key]]\n",
    "    return output\n",
    "\n",
    "\n",
    "def split_dataset(bq_row, num_partitions, ratio):\n",
    "    \"\"\"Returns a partition number for a given bq_row.\"\"\"\n",
    "    import json\n",
    "\n",
    "    assert num_partitions == len(ratio)\n",
    "    bucket = sum(map(ord, json.dumps(bq_row))) % sum(ratio)\n",
    "    total = 0\n",
    "    for i, part in enumerate(ratio):\n",
    "        total += part\n",
    "        if bucket < total:\n",
    "            return i\n",
    "    return len(ratio) - 1\n",
    "\n",
    "\n",
    "def convert_to_jsonl(data, label=None):\n",
    "    \"\"\"Converts a parsed record to JSON\"\"\"\n",
    "    import json\n",
    "\n",
    "    if label:\n",
    "        del data[label]\n",
    "    return json.dumps(data)\n",
    "\n",
    "\n",
    "def run_pipeline(args):\n",
    "    \"\"\"Runs a Beam pipeline to split the dataset\"\"\"\n",
    "\n",
    "    pipeline_options = beam.pipeline.PipelineOptions(flags=[], **args)\n",
    "\n",
    "    raw_data_query = args[\"raw_data_query\"]\n",
    "    label = args[\"label\"]\n",
    "    transformed_data_prefix = args[\"transformed_data_prefix\"]\n",
    "    transform_artifact_dir = args[\"transform_artifact_dir\"]\n",
    "    exported_jsonl_prefix = args[\"exported_jsonl_prefix\"]\n",
    "    exported_tfrec_prefix = args[\"exported_tfrec_prefix\"]\n",
    "    temp_location = args[\"temp_location\"]\n",
    "    project = args[\"project\"]\n",
    "\n",
    "    schema = tfdv.load_schema_text(SCHEMA_LOCATION)\n",
    "    feature_spec = tft.tf_metadata.schema_utils.schema_as_feature_spec(\n",
    "        schema\n",
    "    ).feature_spec\n",
    "\n",
    "    raw_metadata = tft.tf_metadata.dataset_metadata.DatasetMetadata(\n",
    "        tft.tf_metadata.schema_utils.schema_from_feature_spec(feature_spec)\n",
    "    )\n",
    "\n",
    "    with beam.Pipeline(options=pipeline_options) as pipeline:\n",
    "        with tft_beam.Context(temp_location):\n",
    "\n",
    "            # Read raw BigQuery data.\n",
    "            raw_train_data, raw_val_data, raw_test_data = (\n",
    "                pipeline\n",
    "                | \"Read Raw Data\"\n",
    "                >> beam.io.ReadFromBigQuery(\n",
    "                    query=raw_data_query,\n",
    "                    project=project,\n",
    "                    use_standard_sql=True,\n",
    "                )\n",
    "                | \"Parse Data\" >> beam.Map(parse_bq_record)\n",
    "                | \"Split\" >> beam.Partition(split_dataset, 3, ratio=[8, 1, 1])\n",
    "            )\n",
    "\n",
    "            # Create a train_dataset from the data and schema.\n",
    "            raw_train_dataset = (raw_train_data, raw_metadata)\n",
    "\n",
    "            # Analyze and transform raw_train_dataset to produced transformed_train_dataset and transform_fn.\n",
    "            transformed_train_dataset, transform_fn = (\n",
    "                raw_train_dataset\n",
    "                | \"Analyze & Transform\"\n",
    "                >> tft_beam.AnalyzeAndTransformDataset(features.preprocessing_fn)\n",
    "            )\n",
    "\n",
    "            # Get data and schema separately from the transformed_dataset.\n",
    "            transformed_train_data, transformed_metadata = transformed_train_dataset\n",
    "\n",
    "            # write transformed train data.\n",
    "            _ = (\n",
    "                transformed_train_data\n",
    "                | \"Write Transformed Train Data\"\n",
    "                >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                    file_path_prefix=os.path.join(\n",
    "                        transformed_data_prefix, \"train/data\"\n",
    "                    ),\n",
    "                    file_name_suffix=\".gz\",\n",
    "                    coder=tft.coders.ExampleProtoCoder(transformed_metadata.schema),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Create a val_dataset from the data and schema.\n",
    "            raw_val_dataset = (raw_val_data, raw_metadata)\n",
    "\n",
    "            # Transform raw_val_dataset to produced transformed_val_dataset using transform_fn.\n",
    "            transformed_val_dataset = (\n",
    "                raw_val_dataset,\n",
    "                transform_fn,\n",
    "            ) | \"Transform Validation Data\" >> tft_beam.TransformDataset()\n",
    "\n",
    "            # Get data from the transformed_val_dataset.\n",
    "            transformed_val_data, _ = transformed_val_dataset\n",
    "\n",
    "            # write transformed val data.\n",
    "            _ = (\n",
    "                transformed_val_data\n",
    "                | \"Write Transformed Validation Data\"\n",
    "                >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                    file_path_prefix=os.path.join(transformed_data_prefix, \"val/data\"),\n",
    "                    file_name_suffix=\".gz\",\n",
    "                    coder=tft.coders.ExampleProtoCoder(transformed_metadata.schema),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Create a test_dataset from the data and schema.\n",
    "            raw_test_dataset = (raw_test_data, raw_metadata)\n",
    "\n",
    "            # Transform raw_test_dataset to produced transformed_test_dataset using transform_fn.\n",
    "            transformed_test_dataset = (\n",
    "                raw_test_dataset,\n",
    "                transform_fn,\n",
    "            ) | \"Transform Test Data\" >> tft_beam.TransformDataset()\n",
    "\n",
    "            # Get data from the transformed_test_dataset.\n",
    "            transformed_test_data, _ = transformed_test_dataset\n",
    "\n",
    "            # write transformed test data.\n",
    "            _ = (\n",
    "                transformed_test_data\n",
    "                | \"Write Transformed Test Data\"\n",
    "                >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                    file_path_prefix=os.path.join(transformed_data_prefix, \"test/data\"),\n",
    "                    file_name_suffix=\".gz\",\n",
    "                    coder=tft.coders.ExampleProtoCoder(transformed_metadata.schema),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Write transform_fn.\n",
    "            _ = transform_fn | \"Write Transform Artifacts\" >> tft_beam.WriteTransformFn(\n",
    "                transform_artifact_dir\n",
    "            )\n",
    "\n",
    "            # Write raw test data to GCS as TF Records\n",
    "            _ = (\n",
    "                raw_test_data\n",
    "                | \"Write TF Test Data\"\n",
    "                >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                    file_path_prefix=os.path.join(exported_tfrec_prefix, \"data\"),\n",
    "                    file_name_suffix=\".tfrecord\",\n",
    "                    coder=tft.coders.ExampleProtoCoder(raw_metadata.schema),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Convert raw test data to JSON (for batch prediction)\n",
    "            json_test_data = (raw_test_data) | \"Convert Batch Test Data\" >> beam.Map(\n",
    "                convert_to_jsonl, label=label\n",
    "            )\n",
    "\n",
    "            # Write raw test data to GCS as JSONL files.\n",
    "            _ = json_test_data | \"Write JSONL Test Data\" >> beam.io.WriteToText(\n",
    "                file_path_prefix=exported_jsonl_prefix, file_name_suffix=\".jsonl\"\n",
    "            )\n",
    "\n",
    "\n",
    "EXPORTED_JSONL_PREFIX = os.path.join(BUCKET_NAME, \"exported_data/jsonl\")\n",
    "EXPORTED_TFREC_PREFIX = os.path.join(BUCKET_NAME, \"exported_data/tfrec\")\n",
    "TRANSFORMED_DATA_PREFIX = os.path.join(BUCKET_NAME, \"transformed_data\")\n",
    "TRANSFORM_ARTIFACTS_DIR = os.path.join(BUCKET_NAME, \"transformed_artifacts\")\n",
    "\n",
    "QUERY_STRING = \"SELECT * FROM {} LIMIT 300000\".format(BQ_TABLE)\n",
    "JOB_NAME = \"chicago\" + TIMESTAMP\n",
    "\n",
    "args = {\n",
    "    \"runner\": RUNNER,\n",
    "    \"raw_data_query\": QUERY_STRING,\n",
    "    \"label\": label_column,\n",
    "    \"transformed_data_prefix\": TRANSFORMED_DATA_PREFIX,\n",
    "    \"transform_artifact_dir\": TRANSFORM_ARTIFACTS_DIR,\n",
    "    \"exported_jsonl_prefix\": EXPORTED_JSONL_PREFIX,\n",
    "    \"exported_tfrec_prefix\": EXPORTED_TFREC_PREFIX,\n",
    "    \"temp_location\": os.path.join(BUCKET_NAME, \"temp\"),\n",
    "    \"project\": PROJECT_ID,\n",
    "    \"region\": REGION,\n",
    "    \"setup_file\": \"./setup.py\",\n",
    "}\n",
    "\n",
    "print(\"Data preprocessing started...\")\n",
    "run_pipeline(args)\n",
    "print(\"Data preprocessing completed.\")\n",
    "\n",
    "! gsutil ls $TRANSFORMED_DATA_PREFIX/train\n",
    "! gsutil ls $TRANSFORMED_DATA_PREFIX/val\n",
    "! gsutil ls $TRANSFORMED_DATA_PREFIX/test\n",
    "! gsutil ls $TRANSFORM_ARTIFACTS_DIR\n",
    "! gsutil ls {EXPORTED_JSONL_PREFIX}*\n",
    "! gsutil ls $EXPORTED_TFREC_PREFIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataflow:transform,retain"
   },
   "source": [
    "#### Retain transformed dataset references\n",
    "\n",
    "Next, you retain the Cloud Storage location of the transformed data with the dataset. In this example, you add it to the user-defined metadata for this dataset, which is stored in the dataset's Cloud Storage bucket.\n",
    "\n",
    "During the transformation of the data, the transformation function calculated the number of unique occurrences per feature. Some of the categorical values (string, int) may have a large number of unique values. In this case, its better to reduce their dimensionality by moving them from being categorical to embedding feature.\n",
    "\n",
    "The code uses a rule-of-thumb that the embedding size should be the sqrt() of the number of unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "dataflow:transform,retain"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert to embedding trip_day 31\n",
      "Convert to embedding trip_hour 24\n",
      "Convert to embedding pickup_grid 13\n",
      "Convert to embedding dropoff_grid 13\n",
      "Convert to embedding loc_cross 147\n",
      "{\"label_column\": \"tip_bin\", \"numeric_features\": [\"trip_seconds\", \"trip_miles\", \"euclidean\"], \"categorical_features\": [\"trip_month\", \"trip_day_of_week\", \"payment_type\"], \"statistics\": \"gs://vertex-ai-devaip-20220303091526/statistics.jsonl\", \"schema\": \"gs://vertex-ai-devaip-20220303091526/schema.txt\", \"embedding_features\": [\"trip_day\", \"trip_hour\", \"pickup_grid\", \"dropoff_grid\", \"loc_cross\"], \"transformed_data_prefix\": \"gs://vertex-ai-devaip-20220303091526/transformed_data\", \"transform_artifacts_dir\": \"gs://vertex-ai-devaip-20220303091526/transformed_artifacts\", \"exported_jsonl_prefix\": \"gs://vertex-ai-devaip-20220303091526/exported_data/jsonl\", \"exported_tfrec_prefix\": \"gs://vertex-ai-devaip-20220303091526/exported_data/tfrec\"}"
     ]
    }
   ],
   "source": [
    "with tf.io.gfile.GFile(\n",
    "    \"gs://\" + dataset.labels[\"user_metadata\"] + \"/metadata.jsonl\", \"r\"\n",
    ") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "tft_output = tft.TFTransformOutput(TRANSFORM_ARTIFACTS_DIR)\n",
    "\n",
    "CATEGORICAL_FEATURES = []\n",
    "EMBEDDING_FEATURES = []\n",
    "categorical_features = metadata[\"categorical_features\"]\n",
    "for feature in categorical_features:\n",
    "    unique = tft_output.vocabulary_size_by_name(feature)\n",
    "    if unique > 10:\n",
    "        EMBEDDING_FEATURES.append(feature)\n",
    "        print(\"Convert to embedding\", feature, unique)\n",
    "    else:\n",
    "        CATEGORICAL_FEATURES.append(feature)\n",
    "\n",
    "metadata[\"categorical_features\"] = CATEGORICAL_FEATURES\n",
    "metadata[\"embedding_features\"] = EMBEDDING_FEATURES\n",
    "\n",
    "metadata[\"transformed_data_prefix\"] = TRANSFORMED_DATA_PREFIX\n",
    "metadata[\"transform_artifacts_dir\"] = TRANSFORM_ARTIFACTS_DIR\n",
    "metadata[\"exported_jsonl_prefix\"] = EXPORTED_JSONL_PREFIX\n",
    "metadata[\"exported_tfrec_prefix\"] = EXPORTED_TFREC_PREFIX\n",
    "with tf.io.gfile.GFile(\n",
    "    \"gs://\" + dataset.labels[\"user_metadata\"] + \"/metadata.jsonl\", \"w\"\n",
    ") as f:\n",
    "    json.dump(metadata, f)\n",
    "\n",
    "!gsutil cat $BUCKET_NAME/metadata.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "source": [
    "# Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n",
    "- Dataset\n",
    "- Cloud Storage Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "id": "cleanup:stage1",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.base:Deleting TabularDataset : projects/931647533046/locations/us-central1/datasets/1591974089763848192\n",
      "INFO:google.cloud.aiplatform.base:Delete TabularDataset  backing LRO: projects/931647533046/locations/us-central1/operations/6273818364611657728\n",
      "INFO:google.cloud.aiplatform.base:TabularDataset deleted. . Resource name: projects/931647533046/locations/us-central1/datasets/1591974089763848192\n",
      "Removing gs://vertex-ai-devaip-20220303091526/metadata.jsonl#1646300299751086...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/schema.txt#1646299035626428...    \n",
      "Removing gs://vertex-ai-devaip-20220303091526/statistics.jsonl#1646299033342102...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/exported_data/jsonl-00000-of-00004.jsonl#1646299850842686...\n",
      "/ [4 objects]                                                                   \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m rm ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Removing gs://vertex-ai-devaip-20220303091526/exported_data/jsonl-00001-of-00004.jsonl#1646299850846490...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/exported_data/jsonl-00002-of-00004.jsonl#1646299850828192...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/exported_data/jsonl-00003-of-00004.jsonl#1646299850861908...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/exported_data/tfrec/data-00000-of-00004.tfrecord#1646299838144297...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/exported_data/tfrec/data-00001-of-00004.tfrecord#1646299838126920...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/exported_data/tfrec/data-00002-of-00004.tfrecord#1646299838126409...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/exported_data/tfrec/data-00003-of-00004.tfrecord#1646299838125253...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/#1646299040381098...         \n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/beamapp-jupyter-0303091808-976492.1646299088.977573/apache_beam-2.36.0-cp37-cp37m-manylinux1_x86_64.whl#1646299094432669...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/beamapp-jupyter-0303091808-976492.1646299088.977573/dataflow_python_sdk.tar#1646299093404232...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/beamapp-jupyter-0303091808-976492.1646299088.977573/pipeline.pb#1646299094705134...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/beamapp-jupyter-0303091808-976492.1646299088.977573/workflow.tar.gz#1646299093093455...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/#1646299040677926...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/#1646299041383008...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/0afc7cd2e8de4cb5b1b4fc54ccd2921b#1646299058639203...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/100b6534be154cbab158a87821da8382#1646299048763831...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/1375e13fa8704c298d5b85fd34d5d8bb#1646299046476750...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/2419f7e4a04844d0a529f05d803c0057#1646299938545423...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/2be7bbf08e054a58b60d93192acc3c03#1646299628235261...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/33de584599ff476f86500abd888b95b3#1646299955043867...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/4e393c604bf94290850269a1b6078603#1646299936280159...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/51262a4fa9ce4eb587baf497355654cd#1646299957266201...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/54879b49cf484d8181d739de8c3e4ba8#1646299623941493...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/5880f6ceb6d94b5a9d46e8da3c9b0e3f#1646299627324425...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/5f22c9dc812949bf80a73a27abff06f0#1646299047671283...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/5f848abfd1b64c32902d4a6c9a81953b#1646299045333233...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/611eb786bf2840af8de5008e0bf8e080#1646299940839598...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/690b71ddb9904327a3fefc231c8648cd#1646299063141671...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/7027d42908234bb08845dab1ff21237a#1646299954405212...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/7361aac9f5194984b4130bdbed15c00b#1646299940023843...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/7662bd1b69634890b01ab5a5877b0c56#1646299937039938...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/7c8ccb3bba2948edb20582321142123c#1646299953193667...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/7e23c3cd0b0e4041804a6e6fd15d64ee#1646299056616915...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/8702bbb5620f48679b3f7f3e910aeece#1646299937858675...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/88d5ed3520714579863569d3e1a82392#1646299050081728...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/88d8a182c9b446818132ef3edf7baa69#1646299955652690...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/92838bafa0354c0f98bf55425f965979#1646299044243445...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/98ab028be880413c83d5bd923b1949f3#1646299935351174...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/99f19461bd4c426299286932820c954b#1646299043142809...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/9d45a6e02e1d423bb8860e0e9d9e9dd0#1646299041834419...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/a49e28ae7afa4104a325fa0b316ada3e#1646299060841885...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/a745d81f07274d25840d78dd7fab25a1#1646299061873334...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/ab61d71292ed470da714ba967ce19946#1646299952300872...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/affd194516474fa1abd0bd59aeb309f6#1646299059740506...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/b6f4493161914ca18352be33f84bbfe8#1646299626564778...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/b9f485b92ebe4a5f924cabf7c9f06166#1646299057625874...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/ba5e3f0c0bb848c0bc53c1a3f6a0cf39#1646299622745411...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/bfe157f76f2e4d928e9ae66f8ff0e98b#1646299055258539...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/c5b5457912f14efc929c115912e84c3f#1646299956336160...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/d0ee182f35974902a8be7809e992b9b6#1646299625672708...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/dba3a4d9914b4c8ea012ed7f253750ad#1646299629279309...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/e12fd34879a742f3b3362c2357eed36d#1646299624753488...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/f0c38badbd984fffae8220ca427e1e05#1646299953855994...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/analyzer_temporary_assets/f5bf601e1f5d48b1a87bcd6d469a1f76#1646299939277309...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/b78384e439b34e1f90020f5003daca43/#1646299943134718...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/b78384e439b34e1f90020f5003daca43/.tft_metadata/#1646299958424550...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/b78384e439b34e1f90020f5003daca43/.tft_metadata/schema.pbtxt#1646299958709394...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/b78384e439b34e1f90020f5003daca43/assets/#1646299945735219...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/b78384e439b34e1f90020f5003daca43/assets/dropoff_grid#1646299946400945...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/b78384e439b34e1f90020f5003daca43/assets/loc_cross#1646299947103815...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/b78384e439b34e1f90020f5003daca43/assets/payment_type#1646299947831572...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/b78384e439b34e1f90020f5003daca43/assets/pickup_grid#1646299948490080...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/b78384e439b34e1f90020f5003daca43/assets/trip_day#1646299949244624...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/b78384e439b34e1f90020f5003daca43/assets/trip_day_of_week#1646299950011888...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/b78384e439b34e1f90020f5003daca43/assets/trip_hour#1646299950726319...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/b78384e439b34e1f90020f5003daca43/assets/trip_month#1646299951363888...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/b78384e439b34e1f90020f5003daca43/saved_model.pb#1646299951716825...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/b78384e439b34e1f90020f5003daca43/variables/#1646299943431982...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/b78384e439b34e1f90020f5003daca43/variables/variables.data-00000-of-00001#1646299944871239...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/b78384e439b34e1f90020f5003daca43/variables/variables.index#1646299945070429...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/ba88451d7b6c4455b7a53502979a2266/#1646299631717250...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/ba88451d7b6c4455b7a53502979a2266/assets/#1646299634845592...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/ba88451d7b6c4455b7a53502979a2266/assets/2be7bbf08e054a58b60d93192acc3c03#1646299639748730...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/ba88451d7b6c4455b7a53502979a2266/assets/54879b49cf484d8181d739de8c3e4ba8#1646299636397555...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/ba88451d7b6c4455b7a53502979a2266/assets/5880f6ceb6d94b5a9d46e8da3c9b0e3f#1646299639089362...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/ba88451d7b6c4455b7a53502979a2266/assets/b6f4493161914ca18352be33f84bbfe8#1646299638431353...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/ba88451d7b6c4455b7a53502979a2266/assets/ba5e3f0c0bb848c0bc53c1a3f6a0cf39#1646299635554328...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/ba88451d7b6c4455b7a53502979a2266/assets/d0ee182f35974902a8be7809e992b9b6#1646299637754477...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/ba88451d7b6c4455b7a53502979a2266/assets/dba3a4d9914b4c8ea012ed7f253750ad#1646299640354877...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/ba88451d7b6c4455b7a53502979a2266/assets/e12fd34879a742f3b3362c2357eed36d#1646299637068352...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/ba88451d7b6c4455b7a53502979a2266/saved_model.pb#1646299640697767...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/ba88451d7b6c4455b7a53502979a2266/variables/#1646299631909742...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/ba88451d7b6c4455b7a53502979a2266/variables/variables.data-00000-of-00001#1646299633844168...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/ba88451d7b6c4455b7a53502979a2266/variables/variables.index#1646299634048199...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/dropoff_grid#1646299926904641...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/loc_cross#1646299879683138...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/payment_type#1646299918867685...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/pickup_grid#1646299917582493...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/trip_day#1646299909224113...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/trip_day_of_week#1646299864689502...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/trip_hour#1646299920305614...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/temp/tftransform_tmp/trip_month#1646299893308186...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/transformed_artifacts/#1646299959609521...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/transformed_artifacts/transform_fn/#1646299972723984...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/transformed_artifacts/transform_fn/assets/#1646299972852713...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/transformed_artifacts/transform_fn/assets/dropoff_grid#1646299972966090...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/transformed_artifacts/transform_fn/assets/loc_cross#1646299973097454...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/transformed_artifacts/transform_fn/assets/payment_type#1646299973229601...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/transformed_artifacts/transform_fn/assets/pickup_grid#1646299973364840...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/transformed_artifacts/transform_fn/assets/trip_day#1646299973479650...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/transformed_artifacts/transform_fn/assets/trip_day_of_week#1646299973598776...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/transformed_artifacts/transform_fn/assets/trip_hour#1646299973722481...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/transformed_artifacts/transform_fn/assets/trip_month#1646299973852068...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/transformed_artifacts/transform_fn/saved_model.pb#1646299974033092...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/transformed_artifacts/transform_fn/variables/#1646299974179469...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/transformed_artifacts/transform_fn/variables/variables.data-00000-of-00001#1646299974379581...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/transformed_artifacts/transform_fn/variables/variables.index#1646299974530083...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/transformed_artifacts/transformed_metadata/#1646299972212936...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/transformed_artifacts/transformed_metadata/asset_map#1646299972338115...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/transformed_artifacts/transformed_metadata/schema.pbtxt#1646299972468692...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/transformed_data/test/data-00000-of-00005.gz#1646300040899917...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/transformed_data/test/data-00001-of-00005.gz#1646300040892487...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/transformed_data/test/data-00002-of-00005.gz#1646300040905720...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/transformed_data/test/data-00003-of-00005.gz#1646300040920918...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/transformed_data/test/data-00004-of-00005.gz#1646300040893182...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/transformed_data/train/data-00000-of-00006.gz#1646300079646994...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/transformed_data/train/data-00001-of-00006.gz#1646300079652882...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/transformed_data/train/data-00002-of-00006.gz#1646300079647944...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/transformed_data/train/data-00003-of-00006.gz#1646300079654029...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/transformed_data/train/data-00004-of-00006.gz#1646300079644744...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/transformed_data/train/data-00005-of-00006.gz#1646300079654345...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/transformed_data/val/data-00000-of-00005.gz#1646300076542976...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/transformed_data/val/data-00001-of-00005.gz#1646300076559583...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/transformed_data/val/data-00002-of-00005.gz#1646300076553327...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/transformed_data/val/data-00003-of-00005.gz#1646300076546551...\n",
      "Removing gs://vertex-ai-devaip-20220303091526/transformed_data/val/data-00004-of-00005.gz#1646300076543573...\n",
      "/ [130 objects]  11.50 objects/s                                                \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m rm ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "\n",
      "Operation completed over 130 objects.                                            \n",
      "Removing gs://vertex-ai-devaip-20220303091526/...\n"
     ]
    }
   ],
   "source": [
    "# Delete the dataset using the Vertex dataset object\n",
    "dataset.delete()\n",
    "\n",
    "if os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil rm -r $BUCKET_NAME"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "mlops_data_management.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "myenv45",
   "name": "common-cpu.m89",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m89"
  },
  "kernelspec": {
   "display_name": "myenv45",
   "language": "python",
   "name": "myenv45"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

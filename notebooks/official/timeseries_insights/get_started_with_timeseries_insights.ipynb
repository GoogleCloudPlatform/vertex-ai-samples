{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# [TODO] Timeseries Insights API Demonstration\n",
        "\n",
        "---\n",
        "\n",
        "{TODO: Update the links below.}\n",
        "\n",
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/notebook_template.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>                                                                                               \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24743cf4a1e1"
      },
      "source": [
        "**_NOTE_**: This notebook has been tested using Python version 3.9.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "The [Timeseries Insights](https://cloud.google.com/timeseries-insights) is an\n",
        "API designed for gathering insights in real time from your time series\n",
        "datasets.\n",
        "\n",
        "This tutorial provides a detailed, step-by-step guide on how to use the API to set up a real-world system for continuously detecting spiking n-grams from the live news data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d975e698c9a4"
      },
      "source": [
        "### Objective\n",
        "\n",
        "You will learn how to use the [Timeseries Insights](https://cloud.google.com/timeseries-insights) API through the utilization of publicly-available real-time data from the [GDELT](https://www.gdeltproject.org/data.html) project. The process involves creating an initial dataset from historical data and establishing a pipeline for the continuous ingestion of real-time information. Furthermore, you'll configure a query to detect anomalies from the news transcript timeseries.\n",
        "\n",
        "This tutorial uses the following Google Cloud services and resources:\n",
        "\n",
        "* GCP Dataflow Pipeline - Used for recurring data ingestion and anomaly\n",
        "  detection requests.\n",
        "* GCP Storage - The final detected anomalies are stored in the GCP\n",
        "  storage as a text file.\n",
        "\n",
        "The steps involved in this tutorial:\n",
        "\n",
        "1. Create a dataset.\n",
        "2. Setup a recurring append request.\n",
        "3. Set up a recurring query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08d289fa873f"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "This demo uses public real-time data from the\n",
        "[GDELT](https://www.gdeltproject.org/data.html) project to feed the system.\n",
        "\n",
        "GDELT data is available in the Bigquery project `gdelt-bq`, and we use unigrams and bigrams from the [Television News N-grams](https://blog.gdeltproject.org/announcing-the-television-news-ngram-2-0-dataset) data stored in `gdeltv2.iatv_1gramsv2` and `gdeltv2.iatv_2gramsv2` tables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhvOqYjo1hMo"
      },
      "source": [
        "### Setup Colab Instance\n",
        "If you are going to run the Colab instance on GCP, then see [GCP Deployment Manager](https://pantheon.corp.google.com/dm/deployments) to create Colab instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aed92deeb4a0"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Dataflow\n",
        "* Bigquery\n",
        "* Cloud Storage\n",
        "* Timeseries Insights API\n",
        "\n",
        "Learn about [Dataflow pricing](https://cloud.google.com/dataflow/pricing),\n",
        "[Bigquery pricing](https://cloud.google.com/bigquery#pricing-module)\n",
        "and [Storage pricing](https://cloud.google.com/storage/pricing),\n",
        "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7EUnXsZhAGF"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Install the following packages required to execute this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b4ef9b72d43"
      },
      "outputs": [],
      "source": [
        "# Install the packages\n",
        "\n",
        "! pip3 install --upgrade apache-beam[gcp] \\\n",
        "                         apache-beam[interactive] \\\n",
        "                         google-cloud-bigquery[pandas]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58707a750154"
      },
      "source": [
        "### Colab only: Uncomment the following cell to restart the kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f200f10a1da3"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "# import IPython\n",
        "\n",
        "# app = IPython.Application.instance()\n",
        "# app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF1j6f9HApxa"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get $300 free credit towards your compute and storage costs.\n",
        "\n",
        "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "3. Enable the necessary APIs for your project.\n",
        "\n",
        "4. If you are running this notebook locally, you must install the [Cloud SDK](https://cloud.google.com/sdk)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WReHDGG5g0XY"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "If you don't know your project ID, try the following:\n",
        "* Run `gcloud config list`.\n",
        "* Run `gcloud projects list`.\n",
        "* See the support page, [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM1iC_MfAts1"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = '[your-project-id]'  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable used by Timeseries Insights. Learn more about [Timeseries Insights API Regions](https://cloud.google.com/timeseries-insights/docs/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dNb8Px9oVdyg"
      },
      "outputs": [],
      "source": [
        "REGION = 'us-central1'  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBCra4QMA2wR"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "Depending on your Jupyter environment, you might have to authenticate differently. Follow the instructions below.\n",
        "\n",
        "Note: If you don't have an existing Jupyter environment, visit the [GCP Deployment Manager](https://pantheon.corp.google.com/dm/deployments) to create a Colab instance on GCP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74ccc9e52986"
      },
      "source": [
        "**1. Dataflow Workbench**\n",
        "* Because your account is authenticated, there is nothing for you to do."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de775a3773ba"
      },
      "source": [
        "**2. If you are using local JupyterLab instance, uncomment and run.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "254614fa0c46"
      },
      "outputs": [],
      "source": [
        "# ! gcloud auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef21552ccea8"
      },
      "source": [
        "**3. If you are using a Colab instance, uncomment and run.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "603adbbf0532"
      },
      "outputs": [],
      "source": [
        "# from google.colab import auth\n",
        "# auth.authenticate_user(project_id=PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2x9bvEwS5JMZ"
      },
      "source": [
        "**4. Enable the following APIs for your project:**\n",
        "   * [Dataflow\n",
        "     API](https://pantheon.corp.google.com/apis/api/dataflow.googleapis.com)\n",
        "   * [Compute Engine\n",
        "     API](https://pantheon.corp.google.com/apis/api/compute.googleapis.com)\n",
        "   * [BigQuery\n",
        "     API](https://pantheon.corp.google.com/apis/api/bigquery.googleapis.com)\n",
        "   * [Timeseries Insights\n",
        "     API](https://pantheon.corp.google.com/apis/api/timeseriesinsights.googleapis.com)\n",
        "   * [Data pipelines\n",
        "     API](https://pantheon.corp.google.com/apis/api/datapipelines.googleapis.com)\n",
        "   * [Cloud Logging\n",
        "     API](https://pantheon.corp.google.com/apis/api/logging.googleapis.com)\n",
        "   * [Cloud Notebook\n",
        "     API](https://pantheon.corp.google.com/apis/api/notebooks.googleapis.com)\n",
        "   * [BigQuery Storage\n",
        "     API](https://pantheon.corp.google.com/apis/api/bigquerystorage.googleapis.com)\n",
        "   * [Cloud Scheduler\n",
        "     API](https://pantheon.corp.google.com/apis/api/cloudscheduler.googleapis.com)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_2UVv3AaRMmS"
      },
      "outputs": [],
      "source": [
        "! gcloud services enable bigquery.googleapis.com\n",
        "! gcloud services enable bigquerystorage.googleapis.com\n",
        "! gcloud services enable cloudscheduler.googleapis.com\n",
        "! gcloud services enable dataflow.googleapis.com\n",
        "! gcloud services enable datapipelines.googleapis.com\n",
        "! gcloud services enable logging.googleapis.com\n",
        "! gcloud services enable notebooks.googleapis.com\n",
        "! gcloud services enable timeseriesinsights.googleapis.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6b2ccc891ed"
      },
      "source": [
        "**5. Allow permissions to the Service account**\n",
        "\n",
        "Set the service account to be used by this demo. If you are using [GCP Compute Engine](https://cloud.google.com/compute), you can use the [Compute Engine Default Service Account](https://cloud.google.com/iam/docs/service-account-types#default). Choose an existing account service from [Credentials](https://pantheon.corp.google.com/apis/credentials), or create a new one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "ezxL8R81X1zZ"
      },
      "outputs": [],
      "source": [
        "SERVICE_ACCOUNT = '[your-service-account]'  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TsJjfoeNv4V"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "IS_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if (\n",
        "    SERVICE_ACCOUNT == ''\n",
        "    or SERVICE_ACCOUNT is None\n",
        "    or SERVICE_ACCOUNT == '[your-service-account]'\n",
        "):\n",
        "    # Get your service account from gcloud\n",
        "    if not IS_COLAB:\n",
        "        shell_output = !gcloud auth list 2>/dev/null\n",
        "        SERVICE_ACCOUNT = shell_output[2].replace('*', '').strip()\n",
        "\n",
        "    if IS_COLAB:\n",
        "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
        "        project_number = shell_output[-1].split(':')[1].strip().replace(\"'\", '')\n",
        "        SERVICE_ACCOUNT = f'{project_number}-compute@developer.gserviceaccount.com'\n",
        "\n",
        "    print('Service Account:', SERVICE_ACCOUNT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQjSL7ZkJ2va"
      },
      "source": [
        "Grant the following role permissions to your service account.\n",
        "\n",
        "* BigQuery Data Editor\n",
        "* BigQuery Job User\n",
        "* BigQuery Read Session User\n",
        "* Dataflow Developer\n",
        "* Dataflow Worker\n",
        "* GCS Storage Bucket Owner\n",
        "* Storage Object Creator\n",
        "* Storage Object Viewer\n",
        "* Storage Object User\n",
        "* Timeseries Insights DataSet Editor\n",
        "* Data pipelines Invoker\n",
        "\n",
        "Simply run the following cell to grant roles or go to the GCP console, open the [IAM & Admin](https://pantheon.corp.google.com/iam-admin) menu, and select the IAM roles from the list.\n",
        "\n",
        "See the role details from the following documents:\n",
        "\n",
        "* https://cloud.google.com/storage/docs/access-control/iam-roles\n",
        "* https://cloud.google.com/bigquery/docs/access-control\n",
        "* https://cloud.google.com/dataflow/docs/concepts/access-control\n",
        "* https://cloud.google.com/iam/docs/understanding-roles#timeseriesinsights.datasetsEditor\n",
        "* https://cloud.google.com/iam/docs/understanding-roles#datapipelines.invoker\n",
        "\n",
        "See https://cloud.google.com/sdk/gcloud/reference/projects/add-iam-policy-binding for command-line usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzjl0G1lYObr"
      },
      "outputs": [],
      "source": [
        "! gcloud projects add-iam-policy-binding {PROJECT_ID} \\\n",
        "  --member=serviceAccount:{SERVICE_ACCOUNT} --role=roles/bigquery.dataEditor\n",
        "\n",
        "! gcloud projects add-iam-policy-binding {PROJECT_ID} \\\n",
        "  --member=serviceAccount:{SERVICE_ACCOUNT} --role=roles/bigquery.jobUser\n",
        "\n",
        "! gcloud projects add-iam-policy-binding {PROJECT_ID} \\\n",
        "  --member=serviceAccount:{SERVICE_ACCOUNT} \\\n",
        "  --role=roles/bigquery.readSessionUser\n",
        "\n",
        "! gcloud projects add-iam-policy-binding {PROJECT_ID} \\\n",
        "  --member=serviceAccount:{SERVICE_ACCOUNT} --role=roles/dataflow.developer\n",
        "\n",
        "! gcloud projects add-iam-policy-binding {PROJECT_ID} \\\n",
        "  --member=serviceAccount:{SERVICE_ACCOUNT} --role=roles/dataflow.worker\n",
        "\n",
        "! gcloud projects add-iam-policy-binding {PROJECT_ID} \\\n",
        "  --member=serviceAccount:{SERVICE_ACCOUNT} --role=roles/storage.objectCreator\n",
        "\n",
        "! gcloud projects add-iam-policy-binding {PROJECT_ID} \\\n",
        "  --member=serviceAccount:{SERVICE_ACCOUNT} --role=roles/storage.objectUser\n",
        "\n",
        "! gcloud projects add-iam-policy-binding {PROJECT_ID} \\\n",
        "  --member=serviceAccount:{SERVICE_ACCOUNT} --role=roles/storage.objectViewer\n",
        "\n",
        "! gcloud projects add-iam-policy-binding {PROJECT_ID} \\\n",
        "  --member=serviceAccount:{SERVICE_ACCOUNT} \\\n",
        "  --role=roles/timeseriesinsights.datasetsEditor\n",
        "\n",
        "! gcloud projects add-iam-policy-binding {PROJECT_ID} \\\n",
        "  --member=serviceAccount:{SERVICE_ACCOUNT} --role=roles/datapipelines.invoker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgPO1eR3CYjk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "Create a storage bucket to store intermediate artifacts such as datasets.\n",
        "\n",
        "- *{Note to the notebook author: For any user-provided strings that must be unique (like bucket names or model ID's), append \"-unique\" to the end of the strings for proper testing.}*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzGDU7TWdts_"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = 'gs://your-bucket-name-{PROJECT_ID}-unique'  # @param {type:\"string\"}\n",
        "\n",
        "print(BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EcIXiGsCePi"
      },
      "source": [
        "**If you don't have existing bucket, run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NIq7R4HZCfIc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb7bc45d-e482-476a-e853-ecd77800d6c1"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "960505627ddf"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "mY263-UbBv8Y"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime\n",
        "from typing import List\n",
        "\n",
        "import apache_beam as beam\n",
        "import google.auth\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "from apache_beam.options import pipeline_options\n",
        "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
        "from google.cloud import storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNRG15BswcIM"
      },
      "source": [
        "This is a library to build the [Event](https://cloud.google.com/timeseries-insights/docs/reference/rest/v1/projects.locations.datasets/appendEvents#event) from GDELT BigQuery dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Joa7P1lnbYxD"
      },
      "outputs": [],
      "source": [
        "class RowToPropertiesFn(beam.DoFn):\n",
        "    \"\"\"Converts BigQuery row to the properties.\"\"\"\n",
        "\n",
        "    def process(self, row):\n",
        "        \"\"\"Converts BigQuery row to properties tuple\"\"\"\n",
        "        group_id = hash((row['TIMESTAMP'], row['STATION'], row['SHOW']))\n",
        "\n",
        "        timestamp = row['TIMESTAMP']\n",
        "        properties = (\n",
        "            row['STATION'],\n",
        "            row['HOUR'],\n",
        "            row['SHOW'],\n",
        "        )\n",
        "        ngram = (row['NGRAM'].replace(\"'\", '').replace('\"', ''), row['COUNT'])\n",
        "\n",
        "        return [(group_id, timestamp, properties, ngram)]\n",
        "\n",
        "\n",
        "class ConvertPropertyToEventFn(beam.DoFn):\n",
        "    \"\"\"Converts single property to event.\"\"\"\n",
        "\n",
        "    def process(self, element):\n",
        "        \"\"\"Converts property to event\"\"\"\n",
        "        _, timestamp, properties, ngram = element\n",
        "        station, hour, show = properties\n",
        "        word, count = ngram\n",
        "\n",
        "        if show is None:\n",
        "            show = 'None'\n",
        "\n",
        "        event = {\n",
        "            'eventTime': timestamp.isoformat(),\n",
        "            'dimensions': [\n",
        "                {'name': 'station', 'stringVal': station},\n",
        "                {'name': 'hour', 'stringVal': str(hour)},\n",
        "                {'name': 'show', 'stringVal': show},\n",
        "                {'name': 'ngram', 'stringVal': word},\n",
        "                {'name': 'count', 'doubleVal': int(count)},\n",
        "            ],\n",
        "        }\n",
        "\n",
        "        return [event]\n",
        "\n",
        "\n",
        "class CombinePropertiesByGroupFn(beam.CombineFn):\n",
        "    \"\"\"Combines properties to single event by group id.\"\"\"\n",
        "\n",
        "    def create_accumulator(self):\n",
        "        \"\"\"Create a empty accumulator to track the event.\"\"\"\n",
        "\n",
        "        # Timestamp, event id, properties, ngrams.\n",
        "        return {'t': None, 'e': 0, 'p': (), 'n': []}\n",
        "\n",
        "    def add_input(self, mutable_accumulator, element):\n",
        "        \"\"\"Process the incoming value.\"\"\"\n",
        "\n",
        "        _, timestamp, properties, ngram = element\n",
        "        mutable_accumulator['t'] = timestamp\n",
        "        mutable_accumulator['p'] = properties\n",
        "        mutable_accumulator['n'].append(ngram)\n",
        "\n",
        "        return mutable_accumulator\n",
        "\n",
        "    def merge_accumulators(self, accumulators):\n",
        "        \"\"\"Merge several accumulators into a single one.\"\"\"\n",
        "        mutable_accumulator = self.create_accumulator()\n",
        "\n",
        "        for accumulator in accumulators:\n",
        "            if mutable_accumulator['t'] is None:\n",
        "                mutable_accumulator = accumulator\n",
        "            else:\n",
        "                mutable_accumulator['n'].extend(accumulator['n'])\n",
        "\n",
        "        return mutable_accumulator\n",
        "\n",
        "    def extract_output(self, accumulator):\n",
        "        \"\"\"Exports the json event.\"\"\"\n",
        "\n",
        "        timestamp = accumulator['t']\n",
        "        properties = accumulator['p']\n",
        "        ngrams = accumulator['n']\n",
        "        station, hour, show = properties\n",
        "\n",
        "        event = {\n",
        "            'eventTime': timestamp.isoformat(),\n",
        "            'dimensions': [\n",
        "                {'name': 'station', 'stringVal': station},\n",
        "                {'name': 'hour', 'stringVal': str(hour)},\n",
        "                {'name': 'show', 'stringVal': show},\n",
        "            ],\n",
        "        }\n",
        "\n",
        "        for ngram in ngrams:\n",
        "            word, _ = ngram\n",
        "            event['dimensions'].append({'name': 'ngram', 'stringVal': word})\n",
        "\n",
        "        return event\n",
        "\n",
        "\n",
        "class ConvertToJsonFn(beam.DoFn):\n",
        "    \"\"\"Converts events dictionary to json string.\"\"\"\n",
        "\n",
        "    def process(self, event):\n",
        "        \"\"\"Converts events dictionary to json string.\"\"\"\n",
        "        event_json = str(event).replace(\"'\", '\"')\n",
        "\n",
        "        return [event_json]\n",
        "\n",
        "\n",
        "class GdeltClient:\n",
        "    \"\"\"Gdelt client to build events from gdelt data in BigQuery.\"\"\"\n",
        "\n",
        "    def to_bq_request(read_config):\n",
        "        \"\"\"Converts read config to BQ query.\"\"\"\n",
        "\n",
        "        import apache_beam as beam\n",
        "\n",
        "        timestamp, duration, limit, table = read_config\n",
        "\n",
        "        # iatv_1gramsv2\n",
        "        query_tmpl = \"\"\"\n",
        "            SELECT\n",
        "                *\n",
        "            FROM\n",
        "                `gdelt-bq.gdeltv2.{table}`\n",
        "            WHERE\n",
        "                TIMESTAMP_SECONDS({timestamp} - {duration}) < TIMESTAMP AND\n",
        "                TIMESTAMP <= TIMESTAMP_SECONDS({timestamp})\n",
        "            \"\"\"\n",
        "\n",
        "        query = query_tmpl.format(timestamp=timestamp, duration=duration,\n",
        "                                  table=table)\n",
        "        if limit > 0:\n",
        "            query = '{query} LIMIT {limit}'.format(query=query, limit=limit)\n",
        "\n",
        "        return beam.io.ReadFromBigQueryRequest(query=query)\n",
        "\n",
        "    def read_ngrams_from_configs(\n",
        "        self, read_config: beam.PCollection\n",
        "    ) -> beam.PCollection:\n",
        "        \"\"\"Reads BigQuery using the given read config.\"\"\"\n",
        "\n",
        "        ngrams = (\n",
        "            read_config\n",
        "            | 'readConfigToBq' >> beam.Map(GdeltClient.to_bq_request)\n",
        "            | 'readFromBq' >> beam.io.ReadAllFromBigQuery()\n",
        "        )\n",
        "\n",
        "        return ngrams\n",
        "\n",
        "    def build_combined_events(\n",
        "        self, ngrams: beam.PCollection\n",
        "    ) -> beam.PCollection:\n",
        "        \"\"\"Builds combined events json from the given ngrams.\n",
        "\n",
        "        Args:\n",
        "          ngrams: Apache beam pcollection of ngrams.\n",
        "\n",
        "        Returns:\n",
        "          Returns Apache beam pcollection of events.\n",
        "        \"\"\"\n",
        "\n",
        "        events = (\n",
        "            ngrams\n",
        "            | 'convertToProperties' >> beam.ParDo(RowToPropertiesFn())\n",
        "            | 'groupByEventId' >> beam.Map(lambda x: (x[0], x))\n",
        "            | 'combineToEvent' >> beam.CombinePerKey(\n",
        "                CombinePropertiesByGroupFn())\n",
        "            | 'getValue' >> beam.MapTuple(lambda k, v: v)\n",
        "            | 'convertToJson' >> beam.ParDo(ConvertToJsonFn())\n",
        "        )\n",
        "\n",
        "        return events\n",
        "\n",
        "    def build_events(self, ngrams: beam.PCollection) -> beam.PCollection:\n",
        "        \"\"\"Builds events json from the given ngrams.\n",
        "\n",
        "        Args:\n",
        "          ngrams: Apache beam pcollection of ngrams.\n",
        "\n",
        "        Returns:\n",
        "          Returns Apache beam pcollection of events.\n",
        "        \"\"\"\n",
        "\n",
        "        events = (\n",
        "            ngrams\n",
        "            | 'convertToProperties' >> beam.ParDo(RowToPropertiesFn())\n",
        "            | 'propertyToEvent' >> beam.ParDo(ConvertPropertyToEventFn())\n",
        "            | 'convertToJson' >> beam.ParDo(ConvertToJsonFn())\n",
        "        )\n",
        "\n",
        "        return events\n",
        "\n",
        "    def write_events(\n",
        "        self, events: beam.PCollection, file_path_prefix: str,\n",
        "        num_shards: int\n",
        "    ):\n",
        "        \"\"\"Writes the events to the storage.\n",
        "\n",
        "        Args:\n",
        "          events: Apache beam pcollectino of events.\n",
        "          file_path_prefix: GCP storage file path prefix.\n",
        "          num_shards: Number of file shards.\n",
        "        \"\"\"\n",
        "\n",
        "        events | 'write' >> beam.io.WriteToText(\n",
        "            file_path_prefix=file_path_prefix, num_shards=num_shards\n",
        "        )\n",
        "\n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdk09cTfQs2G"
      },
      "source": [
        "This is a library for managing the timestamp for a recurring pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "99lIIcV6Qhg9"
      },
      "outputs": [],
      "source": [
        "class BuildReadConfigFn(beam.DoFn):\n",
        "    \"\"\"Builds read config from the timestamp file.\"\"\"\n",
        "\n",
        "    def process(\n",
        "        self,\n",
        "        element,\n",
        "        timestamp_filepath: str,\n",
        "        min_duration: int,\n",
        "        max_duration: int,\n",
        "        delay: int,\n",
        "        timestamp: int,\n",
        "        write: bool\n",
        "    ):\n",
        "        import re\n",
        "        from datetime import datetime\n",
        "\n",
        "        from google.cloud import storage\n",
        "\n",
        "        match = re.search(r'^gs://([a-zA-Z0-9-_]+)/(.*)', timestamp_filepath)\n",
        "        if not match:\n",
        "            return None\n",
        "\n",
        "        bucket_name = match.group(1)\n",
        "        blob_name = match.group(2)\n",
        "\n",
        "        storage_client = storage.Client()\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "        blob = bucket.blob(blob_name)\n",
        "\n",
        "        with blob.open('r') as f:\n",
        "            timestamp = int(f.read())\n",
        "\n",
        "        now = int(datetime.now().timestamp())\n",
        "        now = now - now % min_duration - delay\n",
        "        timestamp = timestamp - timestamp % min_duration\n",
        "        now = min(now, timestamp + max_duration)\n",
        "        duration = now - timestamp\n",
        "        if duration > 0:\n",
        "            duration = max(duration, min_duration)\n",
        "\n",
        "        if write:\n",
        "            with blob.open('w') as f:\n",
        "                f.write(str(now))\n",
        "\n",
        "        configs = [(now, duration, -1, 'iatv_1gramsv2'),\n",
        "                   (now, duration, -1, 'iatv_2gramsv2')]\n",
        "\n",
        "        return configs\n",
        "\n",
        "\n",
        "class TimestampOptions(pipeline_options.PipelineOptions):\n",
        "\n",
        "    @classmethod\n",
        "    def _add_argparse_args(cls, parser):\n",
        "        parser.add_argument(\n",
        "            '--min_duration',\n",
        "            default='3600',\n",
        "            help='Duration in second to build append events.',\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            '--max_duration',\n",
        "            default='86400',\n",
        "            help='Maximum duration in seconds to build append events.',\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            '--delay',\n",
        "            default='43200',\n",
        "            help='Duration seconds to wait until data arrives.',\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            '--timestamp_filepath',\n",
        "            default='gs://_/',\n",
        "            help='Timestamp metadata filepath.',\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            '--timestamp',\n",
        "            default='0',\n",
        "            help='Timestamp used for timeseries requests.',\n",
        "        )\n",
        "\n",
        "\n",
        "class TimestampManager:\n",
        "    \"\"\"Utility class to handle timestamp file.\"\"\"\n",
        "\n",
        "    def __init__(self, options: TimestampOptions):\n",
        "        self.timestamp_filepath = options.timestamp_filepath\n",
        "        self.min_duration = int(options.min_duration)\n",
        "        self.max_duration = int(options.max_duration)\n",
        "        self.delay = int(options.delay)\n",
        "        self.timestamp = int(options.timestamp)\n",
        "\n",
        "        match = re.search(r'^gs://([a-zA-Z0-9-_]+)/(.*)',\n",
        "                          self.timestamp_filepath)\n",
        "        if not match:\n",
        "            raise Exception(\n",
        "                'Bad timestamp filepath format: {}'.format(\n",
        "                    options.timestamp_filepath\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.bucket_name = match.group(1)\n",
        "        self.blob_name = match.group(2)\n",
        "\n",
        "    def write_timestamp(self, timestamp: int):\n",
        "        \"\"\"Writes the timestmap to the file.\"\"\"\n",
        "\n",
        "        storage_client = storage.Client()\n",
        "        bucket = storage_client.bucket(self.bucket_name)\n",
        "        blob = bucket.blob(self.blob_name)\n",
        "\n",
        "        with blob.open('w') as f:\n",
        "            f.write(str(timestamp))\n",
        "\n",
        "    def read_timestamp(self) -> int:\n",
        "        \"\"\"Returns the latest timestamp from the file.\"\"\"\n",
        "\n",
        "        if self.timestamp > 0:\n",
        "            return self.timestamp\n",
        "\n",
        "        try:\n",
        "            storage_client = storage.Client()\n",
        "            bucket = storage_client.bucket(self.bucket_name)\n",
        "            blob = bucket.blob(self.blob_name)\n",
        "\n",
        "            with blob.open('r') as f:\n",
        "                return int(f.read())\n",
        "        except:\n",
        "            return 0\n",
        "\n",
        "    def get_now_timestamp(self) -> int:\n",
        "        \"\"\"Returns timestamp for most recent events.\"\"\"\n",
        "\n",
        "        now = int(datetime.now().timestamp())\n",
        "        return now - now % self.min_duration - self.delay\n",
        "\n",
        "    def read_timerange_config(\n",
        "        self, init: beam.PCollection, write: bool\n",
        "    ) -> beam.PCollection:\n",
        "        \"\"\"Builds the tuple (timestamp, duration, limit, table) for beam.\"\"\"\n",
        "\n",
        "        timerange = init | 'buildTimerange' >> beam.ParDo(\n",
        "            BuildReadConfigFn(),\n",
        "            timestamp_filepath=self.timestamp_filepath,\n",
        "            min_duration=self.min_duration,\n",
        "            max_duration=self.max_duration,\n",
        "            delay=self.delay,\n",
        "            timestamp=self.timestamp,\n",
        "            write=write\n",
        "        )\n",
        "\n",
        "        return timerange"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwvbbMKuQ_CC"
      },
      "source": [
        "This is a library for printing and plotting the Timeseries API results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "HhcuGRFjQ9_Z"
      },
      "outputs": [],
      "source": [
        "class Plotter:\n",
        "    \"\"\"Utility class to print and plot the detected anomalies.\"\"\"\n",
        "\n",
        "    def __init__(self, dataset: str):\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def read_query_from_blob(self, blob):\n",
        "        \"\"\"Reads query content from the blob.\"\"\"\n",
        "\n",
        "        match = re.search(\n",
        "            r'.*/([0-9]{4})/([0-9]+)/([0-9]+)/([0-9]+).json', blob.name\n",
        "        )\n",
        "        if match:\n",
        "            content = blob.download_as_string().decode('utf8')\n",
        "            content = content.replace(\"'\", '\"')\n",
        "            query = json.loads(content)\n",
        "\n",
        "            if query and 'name' in query and query['name'].endswith(self.dataset):\n",
        "                return query\n",
        "\n",
        "    def print_slices(self, slices):\n",
        "        \"\"\"Builds datafram from the given slices.\"\"\"\n",
        "        dates = []\n",
        "        names = []\n",
        "        scores = []\n",
        "        values = []\n",
        "        forecasts = []\n",
        "        status = []\n",
        "\n",
        "        sorted_slices = sorted(\n",
        "            slices.items(), key=lambda x: x[1]['forecast']['point'][0]['time']\n",
        "        )\n",
        "\n",
        "        for ngram, slice in sorted_slices:\n",
        "            date_str = slice['forecast']['point'][0]['time']\n",
        "            date = datetime.strptime(date_str, '%Y-%m-%dT%H:%M:%SZ')\n",
        "            dates.append(date)\n",
        "            names.append(ngram)\n",
        "            scores.append(slice['anomalyScore'])\n",
        "            values.append(slice['detectionPointActual'])\n",
        "            forecasts.append(slice['forecast']['point'][0]['value'])\n",
        "            status.append(slice['status'])\n",
        "\n",
        "        df = pandas.DataFrame({\n",
        "            'Date (UTC)': dates,\n",
        "            'Name': names,\n",
        "            'Score': scores,\n",
        "            'Value': values,\n",
        "            'Forecast': forecasts,\n",
        "            'Status': status,\n",
        "        })\n",
        "\n",
        "        display(df)\n",
        "\n",
        "    def plot_slices(self, slices, ngrams):\n",
        "        \"\"\"Plots the timeseries of slices.\"\"\"\n",
        "\n",
        "        if not ngrams:\n",
        "            return\n",
        "\n",
        "        plots = list()\n",
        "        for ngram in filter(lambda x: x in slices.keys(), ngrams):\n",
        "            s = slices[ngram]\n",
        "            for r in ['history', 'forecast']:\n",
        "                p = dict()\n",
        "                for t in s[r]['point']:\n",
        "                    timestamp = int(\n",
        "                        datetime.strptime(t['time'],\n",
        "                                          '%Y-%m-%dT%H:%M:%SZ').timestamp()\n",
        "                    )\n",
        "                    p[timestamp] = t['value']\n",
        "                plots.append(p)\n",
        "\n",
        "        start = min(min(p.keys()) for p in plots)\n",
        "        end = max(max(p.keys()) for p in plots)\n",
        "        width_list = []\n",
        "        for p in plots:\n",
        "            tl = list(p.keys())\n",
        "            width_list.append(\n",
        "                min(\n",
        "                    tl[i + 1] - tl[i] for i in range(len(tl) - 1)\n",
        "                )\n",
        "            )\n",
        "        width = min(width_list)\n",
        "        size = int((end - start) / width) + 1\n",
        "\n",
        "        y = [[None] * len(plots) for i in range(size)]\n",
        "        for i in range(len(plots)):\n",
        "            for t in plots[i].keys():\n",
        "                y[int((t - start) / width)][i] = plots[i][t]\n",
        "\n",
        "        plt.figure(figsize=(30, 5))\n",
        "        plt.plot(y, linestyle='-')\n",
        "        plt.title('Anomalies')\n",
        "\n",
        "    def read_queries(self, output_prefix: str, num: int, min_score: float):\n",
        "        \"\"\"Reads query outputs.\"\"\"\n",
        "\n",
        "        match = re.search(r'^gs://([a-zA-Z0-9-_]+)/(.*)', output_prefix)\n",
        "\n",
        "        bucket_name = match.group(1)\n",
        "        dir = match.group(2)\n",
        "\n",
        "        storage_client = storage.Client()\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "        blobs = bucket.list_blobs(prefix=dir)\n",
        "        blob_list = list(blobs)\n",
        "\n",
        "        slices = dict()\n",
        "        for blob in blob_list[-num:]:\n",
        "            query = self.read_query_from_blob(blob)\n",
        "            if query:\n",
        "                for slice in query['slices']:\n",
        "                    if 'anomalyScore' in slice:\n",
        "                        ngram = slice['dimensions'][0]['stringVal']\n",
        "                        score = slice['anomalyScore']\n",
        "                        value = slice['detectionPointActual']\n",
        "                        forecast = slice['forecast']['point'][0]['value']\n",
        "                        if (\n",
        "                            ngram not in slices.keys()\n",
        "                            and score >= min_score\n",
        "                            and value > forecast\n",
        "                        ):\n",
        "                            slices[ngram] = slice\n",
        "\n",
        "        return slices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmiFOBRo21Vc"
      },
      "source": [
        "This is a library to call [Timeseries Insights API](https://cloud.google.com/timeseries-insights) for create, delete, list, get, and append requests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "lq4CDVGtP-Dc"
      },
      "outputs": [],
      "source": [
        "class BeamFn(beam.DoFn):\n",
        "    \"\"\"Base beam class for timeseries insights.\"\"\"\n",
        "\n",
        "    def __init__(self, domain: str, region: str, dataset: str):\n",
        "        self.domain = domain\n",
        "        self.region = region\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def setup(self):\n",
        "        import google.auth\n",
        "\n",
        "        credentials, self.project_id = google.auth.default(\n",
        "            'https://www.googleapis.com/auth/cloud-platform'\n",
        "        )\n",
        "\n",
        "        self.url = '{d}/projects/{p}/locations/{r}/datasets/{s}'.format(\n",
        "            d=self.domain, p=self.project_id, r=self.region, s=self.dataset\n",
        "        )\n",
        "\n",
        "        self.authed_session = google.auth.transport.requests.AuthorizedSession(\n",
        "            credentials\n",
        "        )\n",
        "\n",
        "\n",
        "class AppendEventsFn(BeamFn):\n",
        "    \"\"\"Append events to timeseries insights dataset.\"\"\"\n",
        "\n",
        "    def process(self, event_json):\n",
        "        \"\"\"Process the event\"\"\"\n",
        "        import json\n",
        "\n",
        "        url = self.url + ':appendEvents'\n",
        "\n",
        "        event = json.loads(event_json)\n",
        "        events = {'events': [event]}\n",
        "        response = self.authed_session.post(url=url, json=events)\n",
        "\n",
        "        return [str(response.json())]\n",
        "\n",
        "\n",
        "class QueryFn(BeamFn):\n",
        "    \"\"\"Anomaly detection query.\"\"\"\n",
        "\n",
        "    def process(self, element, bucket, post):\n",
        "        \"\"\"Sends the query.\"\"\"\n",
        "        from datetime import datetime\n",
        "\n",
        "        timestamp, _, _, _ = element\n",
        "        url = self.url + ':query'\n",
        "\n",
        "        t = datetime.fromtimestamp(timestamp - bucket).strftime(\n",
        "            '%Y-%m-%dT%H:%M:%SZ'\n",
        "        )\n",
        "\n",
        "        post['detectionTime'] = t\n",
        "        response = self.authed_session.post(url=url, json=post)\n",
        "\n",
        "        return [(timestamp, str(response.json()))]\n",
        "\n",
        "\n",
        "class WriteResponseFn(beam.DoFn):\n",
        "    \"\"\"Write the response to file.\"\"\"\n",
        "\n",
        "    def __init__(self, output_prefix: str):\n",
        "        self.output_prefix = output_prefix\n",
        "\n",
        "    def setup(self):\n",
        "        import re\n",
        "\n",
        "        match = re.search(r'^gs://([a-zA-Z0-9-_]+)/(.*)', self.output_prefix)\n",
        "        if not match:\n",
        "            raise Exception(\n",
        "                'Bad output filepath format: {}'.format(self.output_prefix)\n",
        "            )\n",
        "\n",
        "        self.bucket_name = match.group(1)\n",
        "        self.dir = match.group(2)\n",
        "\n",
        "    def process(self, element):\n",
        "        from datetime import datetime\n",
        "\n",
        "        from google.cloud import storage\n",
        "\n",
        "        timestamp, response = element\n",
        "        dt = datetime.utcfromtimestamp(timestamp)\n",
        "        storage_client = storage.Client()\n",
        "        bucket = storage_client.bucket(self.bucket_name)\n",
        "        filepath = '{b}/{y}/{m}/{d}/{h}.json'.format(\n",
        "            b=self.dir, y=dt.year, m=dt.month, d=dt.day, h=dt.hour\n",
        "        )\n",
        "        blob = bucket.blob(filepath)\n",
        "\n",
        "        with blob.open('w') as f:\n",
        "            f.write(response)\n",
        "\n",
        "        return ['gs://' + self.bucket_name + '/' + filepath]\n",
        "\n",
        "\n",
        "class TimeseriesClient:\n",
        "    \"\"\"Timeseries insights client.\"\"\"\n",
        "\n",
        "    def __init__(self, region: str, dataset: str):\n",
        "        self.domain = 'https://timeseriesinsights.googleapis.com/v1'\n",
        "        self.region = region\n",
        "        self.dataset = dataset\n",
        "\n",
        "        credentials, self.project_id = google.auth.default(\n",
        "            'https://www.googleapis.com/auth/cloud-platform'\n",
        "        )\n",
        "\n",
        "        self.authed_session = google.auth.transport.requests.AuthorizedSession(\n",
        "            credentials\n",
        "        )\n",
        "\n",
        "    def list_datasets(self):\n",
        "        \"\"\"Lists timeseries for the given region.\"\"\"\n",
        "\n",
        "        url = '{d}/projects/{p}/locations/{r}/datasets'.format(\n",
        "            d=self.domain, p=self.project_id, r=self.region\n",
        "        )\n",
        "        response = self.authed_session.get(url)\n",
        "        return response\n",
        "\n",
        "    def append_events(self, events):\n",
        "        \"\"\"Appends events using beam.\"\"\"\n",
        "\n",
        "        result = events | 'append' >> beam.ParDo(\n",
        "            AppendEventsFn(\n",
        "                domain=self.domain, region=self.region, dataset=self.dataset\n",
        "            )\n",
        "        )\n",
        "\n",
        "        return result\n",
        "\n",
        "    def create_dataset(self, events_filepath: str):\n",
        "        \"\"\"Creates timeseries insights dataset.\"\"\"\n",
        "\n",
        "        post = {\n",
        "            'name': self.dataset,\n",
        "            'dataSources': [{'uri': events_filepath}],\n",
        "            'ttl': '2592000s',\n",
        "        }\n",
        "        url = '{d}/projects/{p}/locations/{r}/datasets'.format(\n",
        "            d=self.domain, p=self.project_id, r=self.region\n",
        "        )\n",
        "\n",
        "        print(post)\n",
        "\n",
        "        return self.authed_session.post(url=url, json=post)\n",
        "\n",
        "    def get_dataset(self) -> str:\n",
        "        \"\"\"Gets the state of the given dataset.\"\"\"\n",
        "\n",
        "        datasets = self.list_datasets().json()\n",
        "\n",
        "        if 'datasets' in datasets:\n",
        "            name = '/{}'.format(self.dataset)\n",
        "            for dataset in datasets['datasets']:\n",
        "                if dataset['name'].endswith(name):\n",
        "                    return dataset\n",
        "\n",
        "        return {}\n",
        "\n",
        "    def delete_dataset(self) -> str:\n",
        "        \"\"\"Deletes the dataset.\"\"\"\n",
        "\n",
        "        url = '{d}/projects/{p}/locations/{r}/datasets/{s}'.format(\n",
        "            d=self.domain, p=self.project_id, r=self.region, s=self.dataset\n",
        "        )\n",
        "\n",
        "        response = self.authed_session.delete(url)\n",
        "        return response\n",
        "\n",
        "    def build_query(\n",
        "        self, metric: str, bucket: int, history: int = 999, forecast: int = 24\n",
        "    ):\n",
        "        \"\"\"Builds query from the given parameters.\"\"\"\n",
        "\n",
        "        post = {\n",
        "            'detectionTime': '',\n",
        "            'slicingParams': {\n",
        "                'dimensionNames': ['ngram'],\n",
        "            },\n",
        "            'timeseriesParams': {\n",
        "                'forecastHistory': '{}s'.format(bucket * history),\n",
        "                'granularity': '{}s'.format(bucket),\n",
        "                'metric': metric,\n",
        "            },\n",
        "            'forecastParams': {\n",
        "                'horizonDuration': '{}s'.format(bucket * forecast),\n",
        "                'noiseThreshold': 10,\n",
        "            },\n",
        "            'returnTimeseries': True,\n",
        "            'numReturnedSlices': 100,\n",
        "        }\n",
        "\n",
        "        return post\n",
        "\n",
        "    def query(self,\n",
        "              metric: str,\n",
        "              timestamp: int,\n",
        "              bucket: int,\n",
        "              history: int = 999,\n",
        "              forecast: int = 24):\n",
        "        \"\"\"Queries anomaly request.\"\"\"\n",
        "\n",
        "        url = '{d}/projects/{p}/locations/{r}/datasets/{s}:query'.format(\n",
        "            d=self.domain, p=self.project_id, r=self.region, s=self.dataset\n",
        "        )\n",
        "\n",
        "        t = datetime.fromtimestamp(timestamp - bucket).strftime(\n",
        "            '%Y-%m-%dT%H:%M:%SZ'\n",
        "        )\n",
        "\n",
        "        post = self.build_query(metric, bucket, history, forecast)\n",
        "        post['detectionTime'] = t\n",
        "\n",
        "        response = self.authed_session.post(url=url, json=post)\n",
        "        return response\n",
        "\n",
        "    def beam_query(self,\n",
        "                   timerange,\n",
        "                   metric: str,\n",
        "                   bucket: int,\n",
        "                   history: int = 999,\n",
        "                   forecast: int = 24):\n",
        "        \"\"\"Queries anomaly request using beam.\"\"\"\n",
        "\n",
        "        post = self.build_query(metric, bucket, history, forecast)\n",
        "\n",
        "        response = timerange | 'query' >> beam.ParDo(\n",
        "            QueryFn(domain=self.domain,\n",
        "                    region=self.region,\n",
        "                    dataset=self.dataset),\n",
        "            bucket,\n",
        "            post,\n",
        "        )\n",
        "\n",
        "        return response\n",
        "\n",
        "    def evaluate(self,\n",
        "                 slice: str,\n",
        "                 timestamp: int,\n",
        "                 bucket: int,\n",
        "                 metric: str,\n",
        "                 history: int = 180,\n",
        "                 forecast: int = 10):\n",
        "        \"\"\"Sends evaluate request.\"\"\"\n",
        "\n",
        "        dim = dict(slice.split('=') for x in str.split(','))\n",
        "        dim = list(map(lambda x: {'name': x, 'stringVal': dim[x]}, dim))\n",
        "\n",
        "        url = (\n",
        "            '{d}/projects/{p}/locations/{r}/datasets/{s}' + ':evaluateSlice'\n",
        "        ).format(d=self.domain,\n",
        "                 p=self.project_id,\n",
        "                 r=self.region,\n",
        "                 s=self.dataset)\n",
        "\n",
        "        t = datetime.fromtimestamp(timestamp - bucket).strftime(\n",
        "            '%Y-%m-%dT%H:%M:%SZ'\n",
        "        )\n",
        "\n",
        "        post = {\n",
        "            'detectionTime': t,\n",
        "            'pinnedDimensions': dim,\n",
        "            'timeseriesParams': {\n",
        "                'forecastHistory': '{}s'.format(bucket * history),\n",
        "                'granularity': '{}s'.format(bucket),\n",
        "                'metric': metric,\n",
        "            },\n",
        "            'forecastParams': {'horizonDuration': '{}s'.format(\n",
        "                bucket * forecast)},\n",
        "            'omitTimeseries': False,\n",
        "        }\n",
        "\n",
        "        print(post)\n",
        "\n",
        "        response = self.authed_session.post(url=url, json=post)\n",
        "        return response\n",
        "\n",
        "    def beam_write_response(self, response: beam.PCollection,\n",
        "                            output_prefix: str):\n",
        "        \"\"\"Writes the response to dated file.\"\"\"\n",
        "\n",
        "        filepath = response | 'write' >> beam.ParDo(\n",
        "            WriteResponseFn(output_prefix))\n",
        "\n",
        "        return filepath"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-ByPu4uQc9j"
      },
      "source": [
        "This is a library to write template metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "rGL8kH6fQNG5"
      },
      "outputs": [],
      "source": [
        "def write_metadata(filename: str):\n",
        "    \"\"\"Writes pipeline template metadata.\"\"\"\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(BUCKET_URI[5:])\n",
        "    blob = bucket.blob('templates/' + filename)\n",
        "\n",
        "    with blob.open('w') as f:\n",
        "        f.write('{}\\n')\n",
        "\n",
        "    print(\n",
        "        (\n",
        "            'Visit https://pantheon.corp.google.com/storage/browser/{b}' +\n",
        "            '/templates?&project={p} for checking the pipeline ' +\n",
        "            'template.'\n",
        "        ).format(b=BUCKET_URI[5:], p=PROJECT_ID)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Coz4yx9XZ18g"
      },
      "source": [
        "This is a library to create or delete a data pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "9524GjXgZu_b"
      },
      "outputs": [],
      "source": [
        "def create_pipeline_auth_session():\n",
        "    scope = 'https://www.googleapis.com/auth/cloud-platform'\n",
        "    domain = 'https://datapipelines.googleapis.com/v1'\n",
        "\n",
        "    path = 'projects/{p}/locations/{r}/pipelines'.format(p=PROJECT_ID, r=REGION)\n",
        "    url = '{d}/{p}'.format(d=domain, p=path)\n",
        "\n",
        "    credentials, _ = google.auth.default(scope)\n",
        "    authed_session = google.auth.transport.requests.AuthorizedSession(\n",
        "        credentials\n",
        "    )\n",
        "\n",
        "    return path, url, authed_session\n",
        "\n",
        "\n",
        "def create_pipeline(name: str, template_name: str, schedule: str):\n",
        "    path, url, authed_session = create_pipeline_auth_session()\n",
        "\n",
        "    pipeline = {\n",
        "        'name': path + '/' + name,\n",
        "        'displayName': name,\n",
        "        'type': 'PIPELINE_TYPE_BATCH',\n",
        "        'state': 'STATE_ACTIVE',\n",
        "        'workload': {\n",
        "            'dataflowLaunchTemplateRequest': {\n",
        "                'projectId': PROJECT_ID,\n",
        "                'gcsPath': BUCKET_URI + '/templates/' + template_name,\n",
        "                'launchParameters': {\n",
        "                    'jobName': name,\n",
        "                    'environment': {\n",
        "                        'tempLocation': BUCKET_URI + '/temp/',\n",
        "                    }\n",
        "                },\n",
        "                'location': REGION\n",
        "            }\n",
        "        },\n",
        "        'scheduleInfo': {\n",
        "            'schedule': schedule,\n",
        "            'timeZone': 'America/Los_Angeles',\n",
        "        },\n",
        "    }\n",
        "    response = authed_session.post(url=url, json=pipeline)\n",
        "    return response\n",
        "\n",
        "\n",
        "def delete_pipeline(name: str):\n",
        "    _, url, authed_session = create_pipeline_auth_session()\n",
        "    response = authed_session.delete(url=(url + '/' + name))\n",
        "    return response\n",
        "\n",
        "\n",
        "def list_pipelines():\n",
        "    _, url, authed_session = create_pipeline_auth_session()\n",
        "    response = authed_session.get(url=url)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g3h6IdmbNGe"
      },
      "source": [
        "This is a library to build an events JSON file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "k-4EJVNWbLqn"
      },
      "outputs": [],
      "source": [
        "class BuildEventsOptions(pipeline_options.PipelineOptions):\n",
        "    @classmethod\n",
        "    def _add_argparse_args(cls, parser):\n",
        "        parser.add_argument(\n",
        "            '--duration',\n",
        "            default='0',\n",
        "            help='Duration seconds of the events.'\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            '--output',\n",
        "            default='gs://',\n",
        "            help='Output filepath for events json files. Use @N for num shards'\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            '--combine',\n",
        "            default='false',\n",
        "            help='Set true to combine properties into single event.'\n",
        "        )\n",
        "\n",
        "\n",
        "def build_event_run(options: pipeline_options.PipelineOptions):\n",
        "    \"\"\"Pipeline to build timeseries dataset.\"\"\"\n",
        "\n",
        "    events_options = options.view_as(BuildEventsOptions)\n",
        "    timestamp_options = options.view_as(TimestampOptions)\n",
        "    timestamp_manager = TimestampManager(timestamp_options)\n",
        "\n",
        "    output = events_options.output\n",
        "    duration = int(events_options.duration)\n",
        "    combine = False\n",
        "    if events_options.combine == 'true':\n",
        "        combine = True\n",
        "    timestamp = timestamp_manager.get_now_timestamp()\n",
        "\n",
        "    pcs = output.split('@')\n",
        "    num_shards = 1\n",
        "    events_filepath = output\n",
        "    if len(pcs) > 1:\n",
        "        events_filepath = pcs[0]\n",
        "        num_shards = int(pcs[1])\n",
        "\n",
        "    pipeline = beam.Pipeline(options=options)\n",
        "\n",
        "    gdelt_client = GdeltClient()\n",
        "\n",
        "    now = int(datetime.now().timestamp())\n",
        "    configs = pipeline | 'init' >> beam.Create(\n",
        "        [(now, duration, -1, 'iatv_1gramsv2'),\n",
        "         (now, duration, -1, 'iatv_2gramsv2')])\n",
        "\n",
        "    ngrams = gdelt_client.read_ngrams_from_configs(configs)\n",
        "    if combine:\n",
        "        events = gdelt_client.build_combined_events(ngrams)\n",
        "    else:\n",
        "        events = gdelt_client.build_events(ngrams)\n",
        "    gdelt_client.write_events(events, events_filepath, num_shards)\n",
        "\n",
        "    result = pipeline.run()\n",
        "    result.wait_until_finish()\n",
        "\n",
        "    # Writes timestamp metadata.\n",
        "    timestamp_manager.write_timestamp(timestamp)\n",
        "\n",
        "\n",
        "def build_event_main(args: List[str]):\n",
        "    \"\"\"Main function to parse the arg and run the build event pipeline.\"\"\"\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "    print('\\n'.join(args))\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    _, beam_args = parser.parse_known_args(args)\n",
        "    options = pipeline_options.PipelineOptions(beam_args)\n",
        "\n",
        "    standard_options = options.view_as(pipeline_options.StandardOptions)\n",
        "    standard_options.runner = 'DataflowRunner'\n",
        "\n",
        "    build_event_run(options=options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODSQHbndbQ2e"
      },
      "source": [
        "This is a library to make Timeseries Insights API calls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "oR9tTgCbbm8l"
      },
      "outputs": [],
      "source": [
        "class TimeseriesOptions(pipeline_options.PipelineOptions):\n",
        "\n",
        "    @classmethod\n",
        "    def _add_argparse_args(cls, parser):\n",
        "        parser.add_argument(\n",
        "            '--input',\n",
        "            default='gs://',\n",
        "            help='Input gcp storage path for events input.',\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            '--dataset',\n",
        "            default='',\n",
        "            help='Dataset name used for timeseries insights api.',\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            '--command',\n",
        "            default='',\n",
        "            help='Dataset commands. get, list, create and delete.',\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            '--slice', default='ngram=today', help='Slice name to evaluate.'\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            '--metric', default='count', help='Metric to aggregate timeseries.'\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            '--bucket', default='3600', help='Bucket size to create timeseries.'\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            '--history', default='999', help='Number of history points to use.'\n",
        "        )\n",
        "\n",
        "\n",
        "def dataset_run(options: TimeseriesOptions):\n",
        "    \"\"\"Command line utility class to send timeseries requests.\"\"\"\n",
        "\n",
        "    cloud_options = options.view_as(GoogleCloudOptions)\n",
        "    region = cloud_options.region\n",
        "\n",
        "    timeseries_options = options.view_as(TimeseriesOptions)\n",
        "    timestamp_options = options.view_as(TimestampOptions)\n",
        "\n",
        "    input = timeseries_options.input\n",
        "    dataset = timeseries_options.dataset\n",
        "    command = timeseries_options.command\n",
        "    slice = timeseries_options.slice\n",
        "    metric = timeseries_options.metric\n",
        "    bucket = int(timeseries_options.bucket)\n",
        "    history = int(timeseries_options.history)\n",
        "\n",
        "    timestamp = TimestampManager(timestamp_options).read_timestamp()\n",
        "\n",
        "    timeseries_client = TimeseriesClient(region, dataset)\n",
        "\n",
        "    print('Command:', command, ' timestamp:', timestamp)\n",
        "\n",
        "    if command == 'create':\n",
        "        response = timeseries_client.create_dataset(input)\n",
        "        print(json.dumps(response.json(), indent=2))\n",
        "    elif command == 'delete':\n",
        "        response = timeseries_client.delete_dataset()\n",
        "        print(json.dumps(response.json(), indent=2))\n",
        "    elif command == 'list':\n",
        "        response = timeseries_client.list_datasets()\n",
        "        print(json.dumps(response.json(), indent=2))\n",
        "    elif command == 'query':\n",
        "        response = timeseries_client.query(\n",
        "            metric=metric, timestamp=timestamp, bucket=bucket, history=history\n",
        "        )\n",
        "        plotter = Plotter(DATASET)\n",
        "        slices = dict()\n",
        "        for slice in response.json()['slices']:\n",
        "            if 'anomalyScore' in slice:\n",
        "                ngram = slice['dimensions'][0]['stringVal']\n",
        "                slices[ngram] = slice\n",
        "        plotter.print_slices(slices)\n",
        "    elif command == 'evaluate':\n",
        "        response = timeseries_client.evaluate(\n",
        "            slice=slice,\n",
        "            metric=metric,\n",
        "            timestamp=timestamp,\n",
        "            bucket=bucket,\n",
        "            history=history,\n",
        "            forecast=10,\n",
        "        )\n",
        "        slice = response.json()\n",
        "        name = slice['dimensions'][0]['stringVal']\n",
        "        slices = {name: slice}\n",
        "\n",
        "        plotter = Plotter(dataset)\n",
        "        plotter.print_slices(slices)\n",
        "        plotter.plot_slices(slices, slices.keys())\n",
        "    elif command == 'wait':\n",
        "        finsihed = False\n",
        "        while not finsihed:\n",
        "            response = timeseries_client.get_dataset()\n",
        "            if 'state' not in response:\n",
        "                print('Dataset doesn\\'t exist.')\n",
        "                print(json.dumps(response, indent=2))\n",
        "                sys.exit(1)\n",
        "            state = response['state']\n",
        "            if state != 'LOADING' and state != 'PENDING':\n",
        "                finsihed = True\n",
        "                if state == 'LOADED':\n",
        "                    print('Dataset loaded.')\n",
        "                else:\n",
        "                    print(json.dumps(response, indent=2))\n",
        "                    sys.exit(1)\n",
        "            else:\n",
        "                time.sleep(10)\n",
        "    else:\n",
        "        response = timeseries_client.get_dataset()\n",
        "        print(json.dumps(response, indent=2))\n",
        "\n",
        "\n",
        "def dataset_main(args: List[str]):\n",
        "    \"\"\"Main function to parse the arg and send timeseries requests.\"\"\"\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "    print('\\n'.join(args))\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    _, beam_args = parser.parse_known_args(args)\n",
        "    options = pipeline_options.PipelineOptions(beam_args)\n",
        "\n",
        "    dataset_run(options=options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvVgcpF9dB7G"
      },
      "source": [
        "This is a library to run append events pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "F-ksCWCCallG"
      },
      "outputs": [],
      "source": [
        "class AppendEventsOptions(pipeline_options.PipelineOptions):\n",
        "    @classmethod\n",
        "    def _add_argparse_args(cls, parser):\n",
        "        parser.add_argument(\n",
        "            '--write', default='true', help='Updates the timestamp information.'\n",
        "        )\n",
        "\n",
        "\n",
        "def append_event_run(options: pipeline_options.PipelineOptions):\n",
        "    \"\"\"Pipeline to build timeseries dataset and append events.\"\"\"\n",
        "\n",
        "    append_options = options.view_as(AppendEventsOptions)\n",
        "    events_options = options.view_as(BuildEventsOptions)\n",
        "    timestamp_options = options.view_as(TimestampOptions)\n",
        "    timeseries_options = options.view_as(TimeseriesOptions)\n",
        "    timestamp_manager = TimestampManager(timestamp_options)\n",
        "\n",
        "    output = events_options.output\n",
        "    write = False\n",
        "    if append_options.write == 'true':\n",
        "        write = True\n",
        "    combine = False\n",
        "    if events_options.combine == 'true':\n",
        "        combine = True\n",
        "\n",
        "    dataset = timeseries_options.dataset\n",
        "\n",
        "    cloud_options = options.view_as(GoogleCloudOptions)\n",
        "    region = cloud_options.region\n",
        "\n",
        "    gdelt_client = GdeltClient()\n",
        "\n",
        "    p = beam.Pipeline(options=options)\n",
        "    init = p | 'init' >> beam.Create(['init'])\n",
        "    read_configs = timestamp_manager.read_timerange_config(init, write=write)\n",
        "    ngrams = gdelt_client.read_ngrams_from_configs(read_configs)\n",
        "    if combine:\n",
        "        events = gdelt_client.build_combined_events(ngrams)\n",
        "    else:\n",
        "        events = gdelt_client.build_events(ngrams)\n",
        "\n",
        "    timeseries_client = TimeseriesClient(region, dataset)\n",
        "    appends = timeseries_client.append_events(events)\n",
        "    appends | 'write' >> beam.io.WriteToText(\n",
        "        file_path_prefix=output, num_shards=10\n",
        "    )\n",
        "\n",
        "    result = p.run()\n",
        "    result.wait_until_finish()\n",
        "\n",
        "\n",
        "def append_event_main(args: List[str]):\n",
        "    \"\"\"Main function to parse the arg and run the append events pipeline.\"\"\"\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "    print('\\n'.join(args))\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    _, beam_args = parser.parse_known_args(args)\n",
        "    options = pipeline_options.PipelineOptions(beam_args)\n",
        "\n",
        "    standard_options = options.view_as(pipeline_options.StandardOptions)\n",
        "    standard_options.runner = 'DataflowRunner'\n",
        "\n",
        "    append_event_run(options=options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRw-PiZJeQRz"
      },
      "source": [
        "This is a library to run a Timeseries Insights query pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "aDn0R_KM2TUJ"
      },
      "outputs": [],
      "source": [
        "class QueryOptions(pipeline_options.PipelineOptions):\n",
        "\n",
        "    @classmethod\n",
        "    def _add_argparse_args(cls, parser):\n",
        "        parser.add_argument(\n",
        "            '--deleteme',\n",
        "            default='count',\n",
        "            help=(\n",
        "                'Use this metric to build timeseries. '\n",
        "                + 'Set empty to use 1 per group.'\n",
        "            ),\n",
        "        )\n",
        "\n",
        "\n",
        "def query_run(options: pipeline_options.PipelineOptions):\n",
        "    \"\"\"Pipeline to run anomaly detection query.\"\"\"\n",
        "\n",
        "    cloud_options = options.view_as(GoogleCloudOptions)\n",
        "    region = cloud_options.region\n",
        "\n",
        "    events_options = options.view_as(BuildEventsOptions)\n",
        "    timeseries_options = options.view_as(TimeseriesOptions)\n",
        "    timestamp_options = options.view_as(TimestampOptions)\n",
        "    timestamp_manager = TimestampManager(timestamp_options)\n",
        "\n",
        "    dataset = timeseries_options.dataset\n",
        "    output = events_options.output\n",
        "    metric = timeseries_options.metric\n",
        "    bucket = int(timeseries_options.bucket)\n",
        "    history = int(timeseries_options.history)\n",
        "\n",
        "    timeseries_client = TimeseriesClient(region, dataset)\n",
        "\n",
        "    p = beam.Pipeline(options=options)\n",
        "    init = p | 'init' >> beam.Create(['init'])\n",
        "    read_configs = timestamp_manager.read_timerange_config(init, write=False)\n",
        "    response = timeseries_client.beam_query(read_configs,\n",
        "                                            metric,\n",
        "                                            bucket,\n",
        "                                            history)\n",
        "    timeseries_client.beam_write_response(response, output)\n",
        "\n",
        "    result = p.run()\n",
        "    result.wait_until_finish()\n",
        "\n",
        "\n",
        "def query_main(args: List[str]):\n",
        "    \"\"\"Main function to parse the arg and run the pipeline.\"\"\"\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "    print('\\n'.join(args))\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    _, beam_args = parser.parse_known_args(args)\n",
        "    options = pipeline_options.PipelineOptions(beam_args)\n",
        "\n",
        "    standard_options = options.view_as(pipeline_options.StandardOptions)\n",
        "    standard_options.runner = 'DataflowRunner'\n",
        "\n",
        "    query_run(options=options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIVEU5_Tb6wG"
      },
      "source": [
        "###Set up dataflow package"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAmaK_llcAq6"
      },
      "source": [
        "Copy the `setup.py` file to the local directory for packaging the libraries for a dataflow pipeline setup. See [Specify dependencies in Python](https://cloud.google.com/functions/docs/writing/specifying-dependencies-python)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBC5gCNjb_rM"
      },
      "outputs": [],
      "source": [
        "! gsutil cp gs://timeseries-insights-samples/tsi-demo/v1/setup.py ./\n",
        "! cat setup.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMshsie_hNh_"
      },
      "source": [
        "##Create a dataset\n",
        "Builds event json files from the historical data and create Timeseries Insights Dataset from the built events file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kaf1y-2vRK4_"
      },
      "source": [
        "###Test Dataset\n",
        "Print out the unigram and bigram sample data from the GDELT dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhztEBeuxbbr"
      },
      "outputs": [],
      "source": [
        "import apache_beam.runners.interactive.interactive_beam as ib\n",
        "\n",
        "options = pipeline_options.PipelineOptions()\n",
        "standard_options = options.view_as(pipeline_options.StandardOptions)\n",
        "standard_options.runner = 'InteractiveRunner'\n",
        "\n",
        "cloud_options = options.view_as(pipeline_options.GoogleCloudOptions)\n",
        "cloud_options.project = PROJECT_ID\n",
        "\n",
        "cloud_options.region = REGION\n",
        "cloud_options.staging_location = '{}/staging'.format(BUCKET_URI)\n",
        "cloud_options.temp_location = '{}/temp'.format(BUCKET_URI)\n",
        "\n",
        "p = beam.Pipeline(options=options)\n",
        "\n",
        "gdelt_client = GdeltClient()\n",
        "\n",
        "now = int(datetime.now().timestamp())\n",
        "read_configs = p | beam.Create([\n",
        "    (int(now / 3600 - 12) * 3600, 3600, 10, 'iatv_1gramsv2'),\n",
        "    (int(now / 3600 - 12) * 3600, 3600, 10, 'iatv_2gramsv2')\n",
        "])\n",
        "\n",
        "ngrams = gdelt_client.read_ngrams_from_configs(read_configs)\n",
        "events = gdelt_client.build_events(ngrams)\n",
        "\n",
        "result = p.run()\n",
        "result.wait_until_finish()\n",
        "\n",
        "ib.show(events)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o61gCwrcwa-"
      },
      "source": [
        "###Build events"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7YDtbn1hTqw"
      },
      "source": [
        "Run the cell to create the events json files from six months of historical data. Then, check the [Dataflow Jobs](https://pantheon.corp.google.com/dataflow/jobs) to see the status of the jobs. The successful job flow looks like this.\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <img src=\"https://storage.googleapis.com/timeseries-insights-samples/tsi-demo/v1/images/build-events.png\" alt=\"Build events\" width=\"505px\" height=\"832px\">\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlAcJhPZhRhc"
      },
      "outputs": [],
      "source": [
        "build_event_main(args=[\n",
        "    '--project=' + PROJECT_ID,\n",
        "    '--setup_file=' + os.getcwd() + '/setup.py',\n",
        "    '--region=' + REGION,\n",
        "    '--job_name=' + DATASET + '-build-events',\n",
        "    '--staging_location=' + BUCKET_URI + '/staging',\n",
        "    '--temp_location=' + BUCKET_URI + '/temp',\n",
        "    '--delay=86400',\n",
        "    '--duration=15552000',\n",
        "    '--output=' + BUCKET_URI + '/output/' + DATASET + '-events@10',\n",
        "    ('--timestamp_filepath=' + BUCKET_URI + '/output/' + DATASET +\n",
        "     '-timestamp.txt')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsNr2Q-elGpP"
      },
      "outputs": [],
      "source": [
        "# Prints the link to the output storage.\n",
        "\n",
        "print(('Visit the bucket to check the output: ' +\n",
        "       'https://pantheon.corp.google.com/storage/browser/{b}/' +\n",
        "       'output?project={p}').format(\n",
        "          b=BUCKET_URI[5:],\n",
        "          p=PROJECT_ID))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-emfw4689-2P"
      },
      "source": [
        "Check the generated files in the GCP bucket. You can find the events json files and timestamp text file similar to the image. The file names could be different than the image.\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <img src=\"https://storage.googleapis.com/timeseries-insights-samples/tsi-demo/v1/images/events.png\" alt=\"Event json files\" width=\"561px\" height=\"439px\">\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk0wxR7R7Uw9"
      },
      "source": [
        "###Set dataset name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdF-GX-8cvNv"
      },
      "source": [
        "Set the dataset name to be used by the Timeseries Insights API."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List the existing datasets to check.\n",
        "\n",
        "dataset_main(args=[\n",
        "    '--region=' + REGION,\n",
        "    '--command=list'\n",
        "])"
      ],
      "metadata": {
        "id": "tC1BD-kAAhb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Y4SJt-oF6sCz"
      },
      "outputs": [],
      "source": [
        "DATASET = '[dataset_name]'  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "p0yTIfgcSXKj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9bc69a7-4571-483e-a691-24a30d35cd44"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "if (\n",
        "    DATASET == ''\n",
        "    or DATASET is None\n",
        "    or DATASET == '[dataset_name]'\n",
        "):\n",
        "    DATASET = 'gdelt-' + datetime.strftime(datetime.now(), '%Y-%m-%d-%H%M%S')\n",
        "\n",
        "print('Using dataset name: ' + DATASET)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kqop1Zuc00_"
      },
      "source": [
        "###Create a dataset\n",
        "Create a dataset from the generated events file in the GCP storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TTi3QCtAXlC"
      },
      "outputs": [],
      "source": [
        "dataset_main(args=[\n",
        "    '--input=' + BUCKET_URI + '/output/' + DATASET + '-events-*',\n",
        "    '--region=' + REGION,\n",
        "    (\n",
        "        '--timestamp_filepath=' + BUCKET_URI + '/output/' + DATASET +\n",
        "        '-timestamp.txt'\n",
        "    ),\n",
        "    '--dataset=' + DATASET,\n",
        "    '--command=create'\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUOLL-FOaEPs"
      },
      "source": [
        "Check the status of the created dataset. Wait until the state changes from 'LOADING' to 'LOADED'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIXYNPU8Z7A1"
      },
      "outputs": [],
      "source": [
        "dataset_main(args=[\n",
        "    '--region=' + REGION,\n",
        "    '--dataset=' + DATASET,\n",
        "    '--command=wait'\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HWh_rcOjxSy"
      },
      "source": [
        "###Evaluate loaded timeseries\n",
        "Send an evaluate timeseries request to the API to check the loaded dataset. You can change the bucket size, history and slice name to plot different timeseries. Also, you can add `--timestamp` flag to evaluate at the specific timestamp."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sl3EyMYjwgy"
      },
      "outputs": [],
      "source": [
        "dataset_main(args=[\n",
        "    '--region=' + REGION,\n",
        "    (\n",
        "        '--timestamp_filepath=' + BUCKET_URI + '/output/' + DATASET +\n",
        "        '-timestamp.txt'\n",
        "    ),\n",
        "    '--dataset=' + DATASET,\n",
        "    '--command=evaluate',\n",
        "    '--history=180',\n",
        "    '--bucket=' + str(3600 * 24),\n",
        "    '--slice=ngram=today',\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ585P8raZ4R"
      },
      "source": [
        "##Append real-time data\n",
        "Set up a pipeline to fetch data from the GDELT Bigquery, and stream to the dataset. This pipeline runs every hour."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jMWa4sVdpou"
      },
      "source": [
        "###Build the pipeline template\n",
        "Build the pipeline template to fetch the data, and stream to the dataset. The flag `--delay=43200` controls the data delay. We are using 12-hour delay considering the possible delay of the GDELT dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwWxOFK8dwue"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "append_event_main(args=[\n",
        "    '--project=' + PROJECT_ID,\n",
        "    '--setup_file=' + os.getcwd() + '/setup.py',\n",
        "    '--region=' + REGION,\n",
        "    '--job_name=' + DATASET + '-append-events',\n",
        "    '--staging_location=' + BUCKET_URI + '/staging',\n",
        "    (\n",
        "        '--template_location=' + BUCKET_URI + '/templates/' + DATASET +\n",
        "        '-append-events'\n",
        "    ),\n",
        "    '--temp_location=' + BUCKET_URI + '/temp',\n",
        "    '--output=' + BUCKET_URI + '/appends/responses',\n",
        "    (\n",
        "        '--timestamp_filepath=' + BUCKET_URI + '/output/' + DATASET +\n",
        "        '-timestamp.txt'\n",
        "    ),\n",
        "    '--min_duration=3600',\n",
        "    '--delay=43200',\n",
        "    '--dataset=' + DATASET,\n",
        "    '--autoscaling_algorithm=NONE',\n",
        "    '--num_workers=4',\n",
        "    '--number_of_worker_harness_threads=50',\n",
        "    '--write=true',\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytrQKrP4lxzE"
      },
      "source": [
        "###Set up a recurring append pipeline\n",
        "\n",
        "Set up a pipeline to stream a new events data every hour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjYtqZSMmDFK",
        "outputId": "48101919-2027-456e-fc45-4b3339f79e83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Visit https://pantheon.corp.google.com/storage/browser/tsi-demo-timeseries-insights-api-demo-unique/templates?&project=timeseries-insights-api-demo for checking the pipeline template.\n"
          ]
        }
      ],
      "source": [
        "# Build the pipeline metadata file.\n",
        "\n",
        "write_metadata(DATASET + '-append-events_metadata')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wyQjRNbmbQF"
      },
      "source": [
        "After running the cell, go to the Google Cloud Platform (GCP) bucket console to confirm whether the template file has been generated. Inside the folder `${BUCKET_URI}/templates`, you should see the template file.\n",
        "\n",
        "You can establish and set up the hourly append pipeline by either running the provided code cell or by following the step-by-step instructions in the gcp console.\n",
        "\n",
        "To use the console, go to GCP [Dataflow->Pipelines](https://pantheon.corp.google.com/dataflow/pipelines) menu and create a new pipeline with the following configurations:\n",
        "\n",
        "* Pipeline name: `append-event`\n",
        "* Dataflow template: Custom Template\n",
        "* Template path: `{BUCKET_URI}/templates/append-events`\n",
        "* Pipeline type: Batch\n",
        "* Temporary location: `{BUCKET_URI}/temp`\n",
        "* Repeat: Hourly (run every 0 minute)\n",
        "\n",
        "The pipeline is created and scheduled to run hourly.\n",
        "\n",
        "Note: The append-events pipeline reads the data from BigQuery. The timestamp of latest streamed data is tracked by the file `{BUCKET_URI}/output/{DATASET}-timestamp.txt`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVbK3PHDcEMd"
      },
      "outputs": [],
      "source": [
        "create_pipeline(name=DATASET + '-append-events',\n",
        "                template_name=DATASET + '-append-events',\n",
        "                schedule='0 * * * *')\n",
        "response = list_pipelines()\n",
        "print(json.dumps(response.json(), indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRp4BDMakDJ4"
      },
      "source": [
        "###Evaluate the real-time timeseries\n",
        "Send an evaluate timeseries request to check the real-time data. You can change the n-gram value of the `--slice` flag to evaluate a different timeseries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCSz3Et5j8Pt"
      },
      "outputs": [],
      "source": [
        "dataset_main(args=[\n",
        "    '--region=' + REGION,\n",
        "    (\n",
        "        '--timestamp_filepath=' + BUCKET_URI + '/output/' + DATASET +\n",
        "        '-timestamp.txt'\n",
        "    ),\n",
        "    '--dataset=' + DATASET,\n",
        "    '--bucket=3600',\n",
        "    '--history=1000',\n",
        "    '--metric=count',\n",
        "    '--command=evaluate',\n",
        "    '--slice=ngram=today',\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CX00Ol4N2RVY"
      },
      "source": [
        "##Query dataset\n",
        "Set up multiple query pipelines to detect anomalies with different bucket sizes. The anomaly results are stored in the GCP bucket."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXYCyoccimX3"
      },
      "source": [
        "Test the query API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNwo4EVSip1_"
      },
      "outputs": [],
      "source": [
        "dataset_main(args=[\n",
        "    '--region=' + REGION,\n",
        "    (\n",
        "        '--timestamp_filepath=' + BUCKET_URI + '/output/' + DATASET +\n",
        "        '-timestamp.txt'\n",
        "    ),\n",
        "    '--dataset=' + DATASET,\n",
        "    '--bucket=3600',\n",
        "    '--history=1000',\n",
        "    '--metric=count',\n",
        "    '--command=query'\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6qLGoV32imX"
      },
      "source": [
        "###Build a query with 1-hour time bucket\n",
        "Set up an hourly pipeline to send an anomaly detection request with 1-hour time bucket. The detection time is read from the `timestamp.txt` file which is updated while the new events append.\n",
        "The query results are stored in the GCP Bucket path, set by the flag `--output`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOGy4-pYP22U"
      },
      "outputs": [],
      "source": [
        "write_metadata(DATASET + '-query01_metadata')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMe2FHa46YIN"
      },
      "outputs": [],
      "source": [
        "query_main(args=[\n",
        "    '--project=' + PROJECT_ID,\n",
        "    '--setup_file=' + os.getcwd() + '/setup.py',\n",
        "    '--region=' + REGION,\n",
        "    '--job_name=' + DATASET + '-query01',\n",
        "    '--staging_location=' + BUCKET_URI + '/staging',\n",
        "    '--template_location=' + BUCKET_URI + '/templates/' + DATASET + '-query01',\n",
        "    '--temp_location=' + BUCKET_URI + '/temp',\n",
        "    '--output=' + BUCKET_URI + '/' + DATASET + '-query01',\n",
        "    (\n",
        "        '--timestamp_filepath=' + BUCKET_URI + '/output/' + DATASET +\n",
        "        '-timestamp.txt'\n",
        "    ),\n",
        "    '--dataset=' + DATASET,\n",
        "    '--bucket=3600',\n",
        "    '--history=1000',\n",
        "    '--metric=count',\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpFH40b_dKCS"
      },
      "outputs": [],
      "source": [
        "create_pipeline(name=DATASET + '-query01',\n",
        "                template_name=DATASET + '-query01',\n",
        "                schedule='10 * * * *')\n",
        "response = list_pipelines()\n",
        "print(json.dumps(response.json(), indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4iAXT4l2tbK"
      },
      "source": [
        "###Build a query with a 24-hour time bucket\n",
        "Set up same pipeline except the 24-hour time bucket size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XynIaC9o2e-d"
      },
      "outputs": [],
      "source": [
        "write_metadata(DATASET + '-query24_metadata')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qo1FyZmb2zY_"
      },
      "outputs": [],
      "source": [
        "query_main(args=[\n",
        "    '--project=' + PROJECT_ID,\n",
        "    '--setup_file=' + os.getcwd() + '/setup.py',\n",
        "    '--region=' + REGION,\n",
        "    '--job_name=' + DATASET + '-query24',\n",
        "    '--staging_location=' + BUCKET_URI + '/staging',\n",
        "    '--template_location=' + BUCKET_URI + '/templates/' + DATASET + '-query24',\n",
        "    '--temp_location=' + BUCKET_URI + '/temp',\n",
        "    '--output=' + BUCKET_URI + '/' + DATASET + '-query24',\n",
        "    (\n",
        "        '--timestamp_filepath=' + BUCKET_URI + '/output/' + DATASET +\n",
        "        '-timestamp.txt'\n",
        "    ),\n",
        "    '--dataset=' + DATASET,\n",
        "    '--bucket=86400',\n",
        "    '--history=180',\n",
        "    '--metric=count',\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2U7T8bupdapH"
      },
      "outputs": [],
      "source": [
        "create_pipeline(name=DATASET + '-query24',\n",
        "                template_name=DATASET + '-query24',\n",
        "                schedule='15 * * * *')\n",
        "response = list_pipelines()\n",
        "print(json.dumps(response.json(), indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOb7qFrwZvIW"
      },
      "source": [
        "##Display a query result\n",
        "Display the detected anomalies from the most recent 24 queries results. You might see the results after multiple successful runs of the query pipelines. You can check the pipeline runs from [Dataflow Jobs](https://pantheon.corp.google.com/dataflow/jobs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "8-ALVEWr3HHO"
      },
      "outputs": [],
      "source": [
        "plotter = Plotter(DATASET)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Bxw0rF6aDN7"
      },
      "outputs": [],
      "source": [
        "slices01 = plotter.read_queries(\n",
        "    output_prefix=BUCKET_URI + '/' + DATASET + '-query01', num=24,\n",
        "    min_score=1.0)\n",
        "\n",
        "plotter.print_slices(slices01)\n",
        "plotter.plot_slices(slices01, slices01.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrTtf6A22_Ve"
      },
      "outputs": [],
      "source": [
        "slices24 = plotter.read_queries(\n",
        "    output_prefix=BUCKET_URI + '/' + DATASET + '-query24', num=24,\n",
        "    min_score=1.0)\n",
        "\n",
        "plotter.print_slices(slices24)\n",
        "plotter.plot_slices(slices24, slices24.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "## Clean up\n",
        "\n",
        "To clean up all of the Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) that you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_8g4JGt3N9L"
      },
      "source": [
        "###Stop all running pipelines\n",
        "\n",
        "From the pipeline console (https://pantheon.corp.google.com/dataflow/pipelines) check the existing pipelines, and delete the created pipelines using the following commands:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9L3w-wOfdmEp"
      },
      "outputs": [],
      "source": [
        "# Delete the Cloud Pipelines that were created\n",
        "delete_pipeline(DATASET + '-query01')\n",
        "delete_pipeline(DATASET + '-query24')\n",
        "delete_pipeline(DATASET + '-append-events')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUk0dxGP3V4c"
      },
      "source": [
        "###Delete the GCS bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx_vKniMq9ZX"
      },
      "outputs": [],
      "source": [
        "# Delete Cloud Storage objects that were created\n",
        "! gsutil -m rm -r $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFBrP1OJ3n3T"
      },
      "source": [
        "###Delete the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OJgGNRedvsF"
      },
      "outputs": [],
      "source": [
        "# Delete the Timeseries Insights dataset that were created\n",
        "dataset_main(args=[\n",
        "    '--region=' + REGION,\n",
        "    '--dataset=' + DATASET,\n",
        "    '--command=delete'\n",
        "])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

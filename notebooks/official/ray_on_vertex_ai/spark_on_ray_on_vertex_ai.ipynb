{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Spark on Ray on Vertex AI\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/spark_on_ray_on_vertex_ai.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2spark_on_ray_on_vertex_ai.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>    \n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/spark_on_ray_on_vertex_ai.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/spark_on_ray_on_vertex_ai.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial demonstrate how Spark can be run on Ray on Vertex AI using [RayDP](https://github.com/oap-project/raydp).\n",
        "\n",
        "**_NOTE_**: This notebook has been tested in the following environment:\n",
        "\n",
        "* Python version = 3.10\n",
        "\n",
        "Learn more about [Ray on Vertex AI](https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/overview) and [Spark on Ray on Vertex AI](https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/run-spark-on-ray)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b55a6be7e27e"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to use RayDP to run Spark applications on a Ray cluster on Vertex AI.\n",
        "\n",
        "This tutorial uses the following Google Cloud services and resources:\n",
        "\n",
        "- Ray on Vertex AI\n",
        "- Artifact Registry\n",
        "- Cloud Storage\n",
        "\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Create custom Ray on Vertex AI container image\n",
        "- Create a Ray cluster on Vertex AI using custom container image\n",
        "- Run Spark interactively on the cluster using RayDP\n",
        "- Run Spark application on cluster via Ray Job API\n",
        "- Read files from Google Cloud Storage in Spark application\n",
        "- Pandas UDF in Spark application on Ray on Vertex AI\n",
        "- Delete the Ray cluster on Vertex AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afd827146603"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "This tutorial uses the [Guerry dataset](https://www.datavis.ca/gallery/guerry/guerrydat.html) which consists of 86 records in a CSV file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15db9d5b4a0e"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing),\n",
        "and [Cloud Storage pricing](https://cloud.google.com/storage/pricing),\n",
        "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Vertex AI SDK for Python and other required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFy3H3aPgx12"
      },
      "outputs": [],
      "source": [
        "! pip3 install --upgrade --quiet google-cloud-aiplatform[ray]==1.59.0\n",
        "! pip3 install --upgrade --quiet pyspark\n",
        "! pip3 install --upgrade --quiet ray[all]==2.9.3\n",
        "! pip3 install --upgrade --quiet raydp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "### Restart runtime (Colab only)\n",
        "\n",
        "To use the newly installed packages, you must restart the runtime on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRvKdaPDTznN"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmM4z7FOBpM"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "Authenticate your environment on Google Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI SDK for Python\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdvJRUWRNGHE"
      },
      "source": [
        "## Create custom container image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53100b3cbe10"
      },
      "source": [
        "[Ray on Vertex AI container images](https://cloud.google.com/vertex-ai/docs/supported-frameworks-list#ray) don't come with RayDP pre-installed, Create a custom Ray on Vertex AI container image to run Spark applications on Ray on Vertex AI. The following section explains how a custom container image for Ray on Vertex AI with RayDP can be built."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01a3fc372b9e"
      },
      "source": [
        "Create a directory to store the dockerfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2c25577425e3"
      },
      "outputs": [],
      "source": [
        "DOCKER_DIR = \"docker_dir\"\n",
        "! mkdir -p {DOCKER_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0a46bce3de8"
      },
      "source": [
        "Recommended using the latest Ray on Vertex AI prebuilt image for creating the custom container image. Install other Python packages that are expected to be used by the Spark applications. \n",
        "\n",
        "Note: pyarrow==14.0 is due to a dependency constraint of Ray 2.9.3 and it also fails to read csv with pandas version >= 2.2.0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3468ec9a1f3c"
      },
      "outputs": [],
      "source": [
        "%%writefile {DOCKER_DIR}/Dockerfile\n",
        "\n",
        "FROM us-docker.pkg.dev/vertex-ai/training/ray-cpu.2-9.py310:latest\n",
        "\n",
        "RUN apt-get update -y \\\n",
        "    && apt-get install openjdk-21-jdk -y \\\n",
        "    && pip install --no-cache-dir raydp pyarrow==14.0 pandas==2.1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a28c2e599e48"
      },
      "source": [
        "### Enable Artifact Registry API\n",
        "Enable the Artifact Registry API service for the Google cloud project. This tutorial requires [gcloud CLI](https://cloud.google.com/sdk/docs/install) installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26b48ef4ba29"
      },
      "outputs": [],
      "source": [
        "! gcloud components update --quiet && gcloud services enable artifactregistry.googleapis.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b37f718f7eae"
      },
      "source": [
        "### Create a private Docker repository\n",
        "Create a Docker repository in [Artifact Registry](https://cloud.google.com/artifact-registry/docs/overview)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a2f0e3b1cd5"
      },
      "outputs": [],
      "source": [
        "DOCKER_REPOSITORY = \"my-docker-repo\"\n",
        "IMAGE_NAME = \"raydp-rov-image\"\n",
        "! gcloud artifacts repositories create {DOCKER_REPOSITORY} --repository-format=docker --location={LOCATION} --description=\"Docker repository\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93ef2bf0de2d"
      },
      "source": [
        "### Build container image\n",
        "This tutorial requires that [Docker](https://docs.docker.com/engine/install/) is installed and available in the work environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "413836afe0e9"
      },
      "outputs": [],
      "source": [
        "! docker build {DOCKER_DIR} -t {LOCATION}-docker.pkg.dev/{PROJECT_ID}/{DOCKER_REPOSITORY}/{IMAGE_NAME}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8ddd31aa438"
      },
      "source": [
        "### Push container image\n",
        "Configure the [authentication for Google Artifact Registry's Docker repository](https://cloud.google.com/artifact-registry/docs/docker/pushing-and-pulling#auth) before pushing the container image to the repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5ee98c5514f"
      },
      "outputs": [],
      "source": [
        "! gcloud auth configure-docker {LOCATION}-docker.pkg.dev --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a31770a8dd0"
      },
      "source": [
        "#### Push the container image to docker repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce0ecd10af62"
      },
      "outputs": [],
      "source": [
        "!docker push {LOCATION}-docker.pkg.dev/{PROJECT_ID}/{DOCKER_REPOSITORY}/{IMAGE_NAME}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57ad7b911d1d"
      },
      "source": [
        "## Create Ray cluster on Vertex AI\n",
        "Use the custom container image to create a Ray cluster on Vertex AI using Vertex AI Python SDK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d48152709457"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timezone\n",
        "\n",
        "cluster_suffix = datetime.now(timezone.utc).strftime(\"%Y%m%d%H%M%S%f\")\n",
        "\n",
        "CLUSTER_NAME = f\"my-rov-cluster-{cluster_suffix}\"\n",
        "CUSTOM_CONTAINER_IMAGE_URI = (\n",
        "    f\"{LOCATION}-docker.pkg.dev/{PROJECT_ID}/{DOCKER_REPOSITORY}/{IMAGE_NAME}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11778fbceba4"
      },
      "outputs": [],
      "source": [
        "from google.cloud.aiplatform import vertex_ray\n",
        "from vertex_ray import Resources\n",
        "\n",
        "head_node_type = Resources(\n",
        "    machine_type=\"n1-standard-16\",\n",
        "    node_count=1,\n",
        "    custom_image=CUSTOM_CONTAINER_IMAGE_URI,\n",
        ")\n",
        "\n",
        "worker_node_types = [\n",
        "    Resources(\n",
        "        machine_type=\"n1-standard-8\",\n",
        "        node_count=2,\n",
        "        custom_image=CUSTOM_CONTAINER_IMAGE_URI,\n",
        "    )\n",
        "]\n",
        "\n",
        "ray_cluster_resource_name = vertex_ray.create_ray_cluster(\n",
        "    head_node_type=head_node_type,\n",
        "    worker_node_types=worker_node_types,\n",
        "    cluster_name=CLUSTER_NAME,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de35e5d0026b"
      },
      "source": [
        "## Spark on Ray on Vertex AI using Ray client\n",
        "Ray [Task](https://docs.ray.io/en/latest/ray-core/tasks.html#ray-remote-functions) or [Actor](https://docs.ray.io/en/latest/ray-core/actors.html), is required for creating a Spark session with [Ray client](https://docs.ray.io/en/latest/cluster/running-applications/job-submission/ray-client.html) on the Ray on Vertex AI.  The following code shows how a Ray Actor can be used for creating a Spark Session, running a Spark application, and stopping a Spark session on a Ray on Vertex AI using RayDP.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f59c07ce1d52"
      },
      "outputs": [],
      "source": [
        "import ray\n",
        "\n",
        "\n",
        "@ray.remote\n",
        "class SparkExecutor:\n",
        "    import pyspark\n",
        "\n",
        "    spark: pyspark.sql.SparkSession = None\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        import raydp\n",
        "\n",
        "        self.spark = raydp.init_spark(\n",
        "            app_name=\"RAYDP ACTOR EXAMPLE\",\n",
        "            num_executors=1,\n",
        "            executor_cores=1,\n",
        "            executor_memory=\"500M\",\n",
        "        )\n",
        "\n",
        "    def get_data(self):\n",
        "        df = self.spark.createDataFrame(\n",
        "            [\n",
        "                (\"sue\", 32),\n",
        "                (\"li\", 3),\n",
        "                (\"bob\", 75),\n",
        "                (\"heo\", 13),\n",
        "            ],\n",
        "            [\"first_name\", \"age\"],\n",
        "        )\n",
        "        return df.toJSON().collect()\n",
        "\n",
        "    def stop_spark(self):\n",
        "        import raydp\n",
        "\n",
        "        raydp.stop_spark()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "037540acbcca"
      },
      "source": [
        "### Connect to Ray cluster on Vertex AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5303ede32b8e"
      },
      "source": [
        "Ray server may take a while to accept connection, use a retry to avoid connection timeout error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd985c5eb008"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "RAY_ADDRESS = f\"vertex_ray://{ray_cluster_resource_name}\"\n",
        "\n",
        "\n",
        "def ray_init():\n",
        "    print(f\"creating connection with ray. address: {RAY_ADDRESS}\")\n",
        "    return ray.init(address=RAY_ADDRESS)\n",
        "\n",
        "\n",
        "def retry(func, max_tries=10):\n",
        "    for i in range(max_tries):\n",
        "        try:\n",
        "            print(\n",
        "                f\"Attempting to connect to Ray server {i+1}, sleeping for 30 seconds...\"\n",
        "            )\n",
        "            time.sleep(30)\n",
        "            func()\n",
        "            break\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "\n",
        "retry(ray_init)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e793c8720d4"
      },
      "source": [
        "### Call Ray Actor to get data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85cfe34cd2cf"
      },
      "outputs": [],
      "source": [
        "s = SparkExecutor.remote()\n",
        "data = ray.get(s.get_data.remote())\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03bae80cb559"
      },
      "source": [
        "### Stop Spark Session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7a429519bbf"
      },
      "outputs": [],
      "source": [
        "ray.get(s.stop_spark.remote())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "875351f4e1c9"
      },
      "source": [
        "### Disconnect from Ray cluster on Vertex AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e7b0bd24f79"
      },
      "outputs": [],
      "source": [
        "ray.shutdown()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7cd3ab8bd43"
      },
      "source": [
        "## Spark on Ray on Vertex AI using Ray Job API\n",
        "Ray client is useful for small experiments that require interactive connection with the Ray cluster, the Ray Job API is the recommended way to run long-running and production jobs on a Ray cluster. This also applies to running Spark applications on the Ray cluster on Vertex AI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f392371b16c1"
      },
      "source": [
        "Create a Python script that contains Spark application code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91addae5aebc"
      },
      "outputs": [],
      "source": [
        "SCRIPT_DIR = \"scripts\"\n",
        "! mkdir -p {SCRIPT_DIR}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0f3109326eca"
      },
      "outputs": [],
      "source": [
        "%%writefile {SCRIPT_DIR}/my_raydp_job.py\n",
        "\n",
        "import pyspark\n",
        "import raydp\n",
        "\n",
        "def get_data(spark: pyspark.sql.SparkSession):\n",
        "    df = spark.createDataFrame(\n",
        "        [\n",
        "            (\"sue\", 32),\n",
        "            (\"li\", 3),\n",
        "            (\"bob\", 75),\n",
        "            (\"heo\", 13),\n",
        "        ],\n",
        "        [\"first_name\", \"age\"],\n",
        "    )\n",
        "    return df.toJSON().collect()\n",
        "\n",
        "def stop_spark():\n",
        "    raydp.stop_spark()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    spark = raydp.init_spark(\n",
        "      app_name=\"RAYDP JOB EXAMPLE\",\n",
        "        num_executors=1,\n",
        "        executor_cores=1,\n",
        "        executor_memory=\"500M\",\n",
        "    )\n",
        "    print(get_data(spark))\n",
        "    stop_spark()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c31a0637934"
      },
      "source": [
        "### Submit the job using Ray Job API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9af8dc2da90d"
      },
      "source": [
        "#### Create Ray Job client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05259b0dbd03"
      },
      "outputs": [],
      "source": [
        "from ray.job_submission import JobSubmissionClient\n",
        "\n",
        "client = JobSubmissionClient(RAY_ADDRESS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e78ebf0f2996"
      },
      "source": [
        "#### Helper function for submitting Ray job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90a29e79c918"
      },
      "outputs": [],
      "source": [
        "def submit_ray_job(script_name: str):\n",
        "    job_id = client.submit_job(\n",
        "        # Entrypoint shell command to execute\n",
        "        entrypoint=f\"python {script_name}\",\n",
        "        # Path to the local directory that contains the python script file.\n",
        "        runtime_env={\n",
        "            \"working_dir\": SCRIPT_DIR,\n",
        "        },\n",
        "    )\n",
        "    return job_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "520e3cd4aa5a"
      },
      "source": [
        "#### Submit Ray job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9b85b23e0b0f"
      },
      "outputs": [],
      "source": [
        "job_id = submit_ray_job(\"my_raydp_job.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c8784686d72"
      },
      "source": [
        "### Monitor the job logs\n",
        "\n",
        "The job logs can also be viewed via Ray on Vertex AI OSS Dashboard in a web browser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08c360a961dc"
      },
      "outputs": [],
      "source": [
        "client.get_job_logs(job_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a97150123bdc"
      },
      "source": [
        "## Reading Cloud Storage files from Spark application\n",
        "\n",
        "The following section shows two different techniques for reading [Cloud Storage](https://cloud.google.com/storage/docs/buckets) files from Spark applications running on Ray on Vertex AI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "180671e0a120"
      },
      "source": [
        "### Create a Cloud Storage bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6aa1c2aabae"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b4cec7c939b"
      },
      "source": [
        "**If your bucket doesn't already exist**: Run the following cell to create your Google Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e837b03f84f3"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fe5968f0d8a"
      },
      "source": [
        "#### Copy [CSV file](https://www.datavis.ca/gallery/guerry/guerry.csv) to Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56143d1ce75b"
      },
      "outputs": [],
      "source": [
        "! curl https://www.datavis.ca/gallery/guerry/guerry.csv | gsutil cp - {BUCKET_URI}/guerry.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87467aad45e4"
      },
      "source": [
        "### Use the Google Cloud Storage Connector\n",
        "\n",
        "The [Google Cloud Connector for Hadoop](https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/master/gcs/README.md) can be used for reading files from a Google Cloud Storage bucket from a Spark application running on Ray on Vertex AI. This is done using a few configuration parameters when a Spark session is created using RayDP. The following code shows how a CSV file stored in a Google Cloud Storage bucket can be read from a Spark application.\n",
        "\n",
        "This tutorial assumes that the IAM Service Account used by the Ray cluster on Vertex AI has been granted required IAM permissions to read from the Google Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ca20d2a40cc"
      },
      "source": [
        "#### Create python script for Spark application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cc54e165e63"
      },
      "outputs": [],
      "source": [
        "%%writefile {SCRIPT_DIR}/spark_gcs_connector.py\n",
        "\n",
        "import raydp\n",
        "\n",
        "spark = raydp.init_spark(\n",
        "  app_name=\"RayDP GCS Example 1\",\n",
        "  configs={\n",
        "      \"spark.jars\": \"https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-2.2.22.jar\",\n",
        "      \"spark.hadoop.fs.AbstractFileSystem.gs.impl\": \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\",\n",
        "      \"spark.hadoop.fs.gs.impl\": \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\",\n",
        "  },\n",
        "  num_executors=2,\n",
        "  executor_cores=4,\n",
        "  executor_memory=\"500M\",\n",
        ")\n",
        "\n",
        "df = spark.read.csv(\"GCS_FILE_URI\", header = True, inferSchema = True)\n",
        "print(f\"CSV data is: {df.toJSON().collect()}\")\n",
        "raydp.stop_spark()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63dabc9d171d"
      },
      "source": [
        "#### Update CSV file path in the script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb78d24bdb7e"
      },
      "outputs": [],
      "source": [
        "! sed -i 's^GCS_FILE_URI^{BUCKET_URI}/guerry.csv^g' {SCRIPT_DIR}/spark_gcs_connector.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a7190d59885"
      },
      "source": [
        "#### Submit the job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "453705a1a29f"
      },
      "outputs": [],
      "source": [
        "submit_ray_job(\"spark_gcs_connector.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87d80b8a6a62"
      },
      "source": [
        "### Use Ray data\n",
        "\n",
        "[Ray data API](https://docs.ray.io/en/latest/data/api/api.html) provides very convenient methods to read files from Google Cloud Storage bucket and it also leverages Ray's distributed processing for reading data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89c77cc8498b"
      },
      "source": [
        "#### Create python script for Spark application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdeb7113557e"
      },
      "outputs": [],
      "source": [
        "%%writefile {SCRIPT_DIR}/spark_gcs_ray_data.py\n",
        "\n",
        "import raydp\n",
        "import ray\n",
        "\n",
        "spark = raydp.init_spark(\n",
        "  app_name=\"RayDP GCS Example 2\",\n",
        "  num_executors=2,\n",
        "  executor_cores=4,\n",
        "  executor_memory=\"500M\",\n",
        ")\n",
        "\n",
        "ray_dataset = ray.data.read_csv(\"GCS_FILE_URI\")\n",
        "df = ray_dataset.to_spark(spark)\n",
        "print(f\"CSV data is: {df.toJSON().collect()}\")\n",
        "raydp.stop_spark()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c98fa7878f7f"
      },
      "source": [
        "#### Update CSV file path in the script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed00b99f1af0"
      },
      "outputs": [],
      "source": [
        "! sed -i 's^GCS_FILE_URI^{BUCKET_URI}/guerry.csv^g' {SCRIPT_DIR}/spark_gcs_ray_data.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7b6499362cb"
      },
      "source": [
        "#### Submit the job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfe43c6ac366"
      },
      "outputs": [],
      "source": [
        "submit_ray_job(\"spark_gcs_ray_data.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "926bf8e334b2"
      },
      "source": [
        "## Pandas UDF on Ray on Vertex AI\n",
        "\n",
        "The [Pyspark Pandas UDF](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.pandas_udf.html) may sometimes require additional code when they're used in a Spark application running on a Ray cluster on Vertex AI. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2148885132f1"
      },
      "source": [
        "### Handle Python package dependencies\n",
        "\n",
        "The [Python dependencies](https://docs.ray.io/en/latest/cluster/running-applications/job-submission/sdk.html#dependency-management) of an application can be installed using Runtime Environment with Ray job API when the Ray job is submitted to the cluster, Ray installs those dependencies in the Python virtual environment that it creates for running the job. The Pandas UDF, however, do nt run in the same python virtual environment. It instead is run in the python System environment. If that dependency isn't available in the System environment, that dependency needs to be installed within Pandas UDF."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0404dcb11d0"
      },
      "source": [
        "#### Create Python script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a030297a651c"
      },
      "outputs": [],
      "source": [
        "%%writefile {SCRIPT_DIR}/pandas_udf_dependency.py\n",
        "\n",
        "import pandas as pd\n",
        "import pyspark\n",
        "import raydp\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "def test_udf(spark: pyspark.sql.SparkSession):\n",
        "    import pandas as pd\n",
        "    \n",
        "    df = spark.createDataFrame(pd.read_csv(\"https://www.datavis.ca/gallery/guerry/guerry.csv\"))\n",
        "    return df.select(func('Lottery','Literacy', 'Pop1831')).collect()\n",
        "\n",
        "\n",
        "@pandas_udf(StringType())\n",
        "def func(s1: pd.Series, s2: pd.Series, s3: pd.Series) -> str:\n",
        "    import numpy as np\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"statsmodels\"])\n",
        "    import statsmodels.api as sm\n",
        "    import statsmodels.formula.api as smf\n",
        "    \n",
        "    d = {'Lottery': s1, \n",
        "         'Literacy': s2,\n",
        "         'Pop1831': s3}\n",
        "    data = pd.DataFrame(d)\n",
        "\n",
        "    # Fit regression model (using the natural log of one of the regressors)\n",
        "    results = smf.ols('Lottery ~ Literacy + np.log(Pop1831)', data=data).fit()\n",
        "    return results.summary().as_csv()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    spark = raydp.init_spark(\n",
        "      app_name=\"RayDP UDF Example\",\n",
        "      num_executors=2,\n",
        "      executor_cores=4,\n",
        "      executor_memory=\"1500M\",\n",
        "    )\n",
        "    \n",
        "    print(test_udf(spark))\n",
        "    \n",
        "    raydp.stop_spark()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75dd63d9d5ff"
      },
      "source": [
        "#### Submit the job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71e33f7dd66a"
      },
      "outputs": [],
      "source": [
        "submit_ray_job(\"pandas_udf_dependency.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba50e964a36a"
      },
      "source": [
        "### Handle local Python dependencies\n",
        "\n",
        "The best practice for handling Python dependencies is via Python repository. Therefore, publish your own custom packages to your Python repository and install those packages using pip. In case you are using local python package dependency in Pandas UDF of your Spark application, additional code is required to add the local packages to PYTHONPATH of Python System environment of the Ray cluster on Vertex AI nodes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a3a20813c08"
      },
      "source": [
        "#### Create local python module\n",
        "\n",
        "This is a very simple python file that has one method. This method takes a string argument and prints it to the console."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89342ad3023c"
      },
      "outputs": [],
      "source": [
        "%%writefile {SCRIPT_DIR}/my_module.py\n",
        "\n",
        "def print_func(text: str):\n",
        "    print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5a66b5e6982"
      },
      "source": [
        "#### Create python script\n",
        "\n",
        "Use the python script of previous section to demonstrate the handling of local dependency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9bd291fca90"
      },
      "outputs": [],
      "source": [
        "%%writefile {SCRIPT_DIR}/pandas_udf_local_dependency.py\n",
        "\n",
        "import pandas as pd\n",
        "import pyspark\n",
        "import raydp\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "def test_udf(spark: pyspark.sql.SparkSession):\n",
        "    import pandas as pd\n",
        "    df = spark.createDataFrame(pd.read_csv(\"https://www.datavis.ca/gallery/guerry/guerry.csv\"))\n",
        "    import pathlib\n",
        "    module_path = str(pathlib.Path(__file__).parent.resolve())\n",
        "    return df.select(udf_wrapper_func('Lottery','Literacy', 'Pop1831', module_path)).collect()\n",
        "\n",
        "def udf_wrapper_func(s1: pd.Series, s2: pd.Series, s3: pd.Series, module_path: str) -> str:\n",
        "\n",
        "    @pandas_udf(StringType())\n",
        "    def func(s1: pd.Series, s2: pd.Series, s3: pd.Series) -> str:\n",
        "        import sys\n",
        "        sys.path.append(module_path)\n",
        "        \n",
        "        # import local module\n",
        "        import my_module\n",
        "        my_module.print_func(\"This is a UDF local dependency test.\")\n",
        "\n",
        "        import numpy as np\n",
        "        import subprocess\n",
        "        import sys\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"statsmodels\"])\n",
        "        import statsmodels.api as sm\n",
        "        import statsmodels.formula.api as smf\n",
        "\n",
        "        d = {'Lottery': s1, \n",
        "             'Literacy': s2,\n",
        "             'Pop1831': s3}\n",
        "        data = pd.DataFrame(d)\n",
        "\n",
        "    #     # Fit regression model (using the natural log of one of the regressors)\n",
        "        results = smf.ols('Lottery ~ Literacy + np.log(Pop1831)', data=data).fit()\n",
        "        return results.summary().as_csv()\n",
        "\n",
        "    return func(s1, s2, s3)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    spark = raydp.init_spark(\n",
        "      app_name=\"UDF_TEST\",\n",
        "      num_executors=2,\n",
        "      executor_cores=2,\n",
        "      executor_memory=\"500M\",\n",
        "    )\n",
        "    \n",
        "    print(test_udf(spark))\n",
        "    \n",
        "    raydp.stop_spark()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90cb909f991b"
      },
      "source": [
        "#### Submit the job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05dd644210e9"
      },
      "outputs": [],
      "source": [
        "submit_ray_job(\"pandas_udf_local_dependency.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0d970d9efe1"
      },
      "source": [
        "The Ray job should complete sucessfully and the job logs should have a line `This is a UDF local dependency test.`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a4e033321ad"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, delete the resources created in this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f881fa9e134a"
      },
      "source": [
        "### Delete Ray cluster on Vertex AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "633ee0562836"
      },
      "outputs": [],
      "source": [
        "# Delete the cluster\n",
        "vertex_ray.delete_ray_cluster(ray_cluster_resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71eb2168e323"
      },
      "source": [
        "### Delete Google Cloud Storage bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b29ca06aca5d"
      },
      "outputs": [],
      "source": [
        "! gcloud storage rm --recursive {BUCKET_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2791de554023"
      },
      "source": [
        "### Delete private docker repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0514a2e3bfd5"
      },
      "outputs": [],
      "source": [
        "! gcloud artifacts repositories delete {DOCKER_REPOSITORY} --location={LOCATION} --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c294baece1b"
      },
      "source": [
        "### Delete local directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "100b5c9a535b"
      },
      "outputs": [],
      "source": [
        "! rm -rf {DOCKER_DIR}\n",
        "! rm -rf {SCRIPT_DIR}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "spark_on_ray_on_vertex_ai.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Spark on Ray on Vertex AI\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/spark_on_ray_on_vertex_ai.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2spark_on_ray_on_vertex_ai.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>    \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/spark_on_ray_on_vertex_ai.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/spark_on_ray_on_vertex_ai.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_NOTE_**: This notebook has been tested in the following environment:\n",
    "\n",
    "* Python version = 3.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This tutorial demonstrate how Spark can be run on Ray on Vertex AI using [RayDP](https://github.com/oap-project/raydp).\n",
    "\n",
    "Learn more about [Ray on Vertex AI overview](https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you learn how to use RayDP to run Spark applications on Ray on Vertex AI cluster.\n",
    "\n",
    "This tutorial uses the following Google Cloud services and resources:\n",
    "\n",
    "- Ray on Vertex AI\n",
    "- Artifact Registry\n",
    "- Cloud Storage\n",
    "\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Create custom Ray on Vertex AI container image\n",
    "- Create a Ray on Vertex AI cluster using custom container image\n",
    "- Run Spark interactively on the cluster using RayDP\n",
    "- Run Spark application on cluster via Ray Job API\n",
    "- Read files from Google Cloud Storage in Spark application\n",
    "- Pandas UDF in Spark application on Ray on Vertex AI\n",
    "- Delete Ray on Vertex AI cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "This tutorial uses the [Guerry dataset](https://www.datavis.ca/gallery/guerry/guerrydat.html) which consists of 86 records in a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing),\n",
    "and [Cloud Storage pricing](https://cloud.google.com/storage/pricing),\n",
    "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install Vertex AI SDK for Python and other required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFy3H3aPgx12",
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip3 install --upgrade --quiet google-cloud-aiplatform\n",
    "! pip3 install --upgrade --quiet ray[all]==2.9.3\n",
    "! pip3 install --upgrade --quiet pyspark\n",
    "! pip3 install --upgrade --quiet raydp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5Xep4W9lq-Z",
    "tags": []
   },
   "source": [
    "### Restart runtime (Colab only)\n",
    "\n",
    "To use the newly installed packages, you must restart the runtime on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRvKdaPDTznN"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbmM4z7FOBpM"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmWOrTJ3gx13",
    "tags": []
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "Authenticate your environment on Google Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyKGtVQjgx13"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "### Set Google Cloud project information and initialize Vertex AI SDK for Python\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nqwi-5ufWp_B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[Your project id]\"  # @param {type:\"string\"}\n",
    "PROJECT_NUMBER = \"[Your project number]\" # @param {type:\"string\"}\n",
    "LOCATION = \"[GCP Region e.g. us-central1]\"  # @param {type:\"string\"}\n",
    "\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdvJRUWRNGHE"
   },
   "source": [
    "## Create custom container image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ray on Vertex AI container images](https://cloud.google.com/vertex-ai/docs/supported-frameworks-list#ray) do not come with RayDP pre-installed, Create a custom Ray on Vertex AI container image to run Spark applications on Ray on Vertex AI. The following section explains how a custom container image for Ray on Vertex AI with RayDP can be built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a directory to store the dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DOCKER_DIR = \"docker_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p {DOCKER_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommended to use the latest Ray on Vertex AI prebuilt image for creating the custom container image. Install other Python packages that are expected to be used by the Spark applications. \n",
    "\n",
    "Note: pyarrow==14.0 is due to a dependency constraint of Ray 2.9.3 and it also fails to read csv with pandas version >= 2.2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {DOCKER_DIR}/Dockerfile\n",
    "\n",
    "FROM us-docker.pkg.dev/vertex-ai/training/ray-cpu.2-9.py310:latest\n",
    "\n",
    "RUN apt-get update -y \\\n",
    "    && pip install --no-cache-dir raydp pyarrow==14.0 pandas==2.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable Artifact Registry API\n",
    "Enable the Artifact Registry API service for the Google cloud project. This tutorial assumes that [gcloud CLI](https://cloud.google.com/sdk/docs/install) is already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gcloud services enable artifactregistry.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a private Docker repository\n",
    "Create a Docker repository in Google Artifact Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DOCKER_REPOSITORY = \"my-rov-repo\"\n",
    "IMAGE_NAME = \"raydp-rov-image\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gcloud artifacts repositories create {DOCKER_REPOSITORY} --repository-format=docker --location={LOCATION} --description=\"Docker repository\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build container image\n",
    "This tutorial assumes that [Docker](https://docs.docker.com/engine/install/) is installed and available in the work environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker build {DOCKER_DIR} -t {LOCATION}-docker.pkg.dev/{PROJECT_ID}/{DOCKER_REPOSITORY}/{IMAGE_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push container image\n",
    "Configure the [authentication for Google Artifact Registry's Docker repository](https://cloud.google.com/artifact-registry/docs/docker/pushing-and-pulling#auth) before pushing the container image to the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker push {LOCATION}-docker.pkg.dev/{PROJECT_ID}/{DOCKER_REPOSITORY}/{IMAGE_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Ray cluster on Vertex AI\n",
    "Use the custom container image to create Ray cluster on Vertex AI using Vertex AI Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CLUSTER_NAME = \"my-rov-cluster\"\n",
    "CUSTOM_CONTAINER_IMAGE_URI = f\"{LOCATION}-docker.pkg.dev/{PROJECT_ID}/{DOCKER_REPOSITORY}/{IMAGE_NAME}\"\n",
    "CLUSTER_RESOURCE_NAME=f'projects/{PROJECT_NUMBER}/locations/{LOCATION}/persistentResources/{CLUSTER_NAME}'\n",
    "RAY_ADDRESS = f'vertex_ray://{CLUSTER_RESOURCE_NAME}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import vertex_ray\n",
    "from vertex_ray import Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "head_node_type = Resources(\n",
    "    machine_type=\"n1-standard-16\",\n",
    "    node_count=1,\n",
    "    custom_image=CUSTOM_CONTAINER_IMAGE_URI,\n",
    ")\n",
    "\n",
    "worker_node_types = [Resources(\n",
    "    machine_type=\"n1-standard-8\",\n",
    "    node_count=2,\n",
    "    custom_image=CUSTOM_CONTAINER_IMAGE_URI,\n",
    ")]\n",
    "\n",
    "ray_cluster_resource_name = vertex_ray.create_ray_cluster(\n",
    "    head_node_type=head_node_type,\n",
    "    worker_node_types=worker_node_types,\n",
    "    cluster_name=CLUSTER_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark on Ray on Vertex AI using Ray client\n",
    "Ray [Task](https://docs.ray.io/en/latest/ray-core/tasks.html#ray-remote-functions) or [Actor](https://docs.ray.io/en/latest/ray-core/actors.html), is required for creating a Spark session with [Ray client](https://docs.ray.io/en/latest/cluster/running-applications/job-submission/ray-client.html) on the Ray on Vertex AI.  The following code shows how a Ray Actor can be used for creating a Spark Session, running a Spark application and stopping a Spark session on a Ray on Vertex AI using RayDP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "@ray.remote\n",
    "class SparkExecutor:\n",
    "  import pyspark\n",
    "\n",
    "  spark: pyspark.sql.SparkSession = None\n",
    "\n",
    "  def __init__(self):\n",
    "\n",
    "    import ray\n",
    "    import raydp\n",
    "\n",
    "    self.spark = raydp.init_spark(\n",
    "      app_name=\"RAYDP ACTOR EXAMPLE\",\n",
    "      num_executors=1,\n",
    "      executor_cores=1,\n",
    "      executor_memory=\"500M\",\n",
    "    )\n",
    "\n",
    "  def get_data(self):\n",
    "    df = self.spark.createDataFrame(\n",
    "        [\n",
    "            (\"sue\", 32),\n",
    "            (\"li\", 3),\n",
    "            (\"bob\", 75),\n",
    "            (\"heo\", 13),\n",
    "        ],\n",
    "        [\"first_name\", \"age\"],\n",
    "    )\n",
    "    return df.toJSON().collect()\n",
    "\n",
    "  def stop_spark(self):\n",
    "    import raydp\n",
    "    raydp.stop_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Ray cluster on Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "def ray_init():\n",
    "    print(f\"creating connection with ray. address: {RAY_ADDRESS}\")\n",
    "    return ray.init(address=RAY_ADDRESS,\n",
    "              configure_logging=True,\n",
    "              logging_level=logging.DEBUG\n",
    "             )\n",
    "\n",
    "ray_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call Ray Actor to get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s = SparkExecutor.remote()\n",
    "data = ray.get(s.get_data.remote())\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.get(s.stop_spark.remote())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disconnect from Ray cluster on Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark on Ray on Vertex AI using Ray Job API\n",
    "Ray client is useful for small experiments that require interactive connection with the Ray cluster, the Ray Job API is the recommended way to run long running and production jobs on a Ray cluster. This also applies to running Spark applications on the Ray cluster on Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Python script that contains Spark application code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SCRIPT_DIR = \"scripts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! mkdir -p {SCRIPT_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {SCRIPT_DIR}/my_raydp_job.py\n",
    "\n",
    "import pyspark\n",
    "import raydp\n",
    "\n",
    "def get_data(spark: pyspark.sql.SparkSession):\n",
    "    df = spark.createDataFrame(\n",
    "        [\n",
    "            (\"sue\", 32),\n",
    "            (\"li\", 3),\n",
    "            (\"bob\", 75),\n",
    "            (\"heo\", 13),\n",
    "        ],\n",
    "        [\"first_name\", \"age\"],\n",
    "    )\n",
    "    return df.toJSON().collect()\n",
    "\n",
    "def stop_spark():\n",
    "    raydp.stop_spark()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    spark = raydp.init_spark(\n",
    "      app_name=\"RAYDP JOB EXAMPLE\",\n",
    "        num_executors=1,\n",
    "        executor_cores=1,\n",
    "        executor_memory=\"500M\",\n",
    "    )\n",
    "    print(get_data(spark))\n",
    "    stop_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the job using Ray Job API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ray.job_submission import JobSubmissionClient\n",
    "\n",
    "client = JobSubmissionClient(RAY_ADDRESS)\n",
    "\n",
    "job_id = client.submit_job(\n",
    "  # Entrypoint shell command to execute\n",
    "  entrypoint=\"python my_raydp_job.py\",\n",
    "  # Path to the local directory that contains the python script file.\n",
    "  runtime_env={\n",
    "    \"working_dir\": SCRIPT_DIR,\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor the job logs\n",
    "\n",
    "The job logs can also be viewed via Ray on Vertex AI OSS Dashboard in a web browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.get_job_logs(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Cloud Storage files from Spark application\n",
    "\n",
    "The following section shows two different techniques for reading Cloud Storage files from Spark applications running on Ray on Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Cloud Storage bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy [CSV file](https://www.datavis.ca/gallery/guerry/guerry.csv) to GCS bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! curl https://www.datavis.ca/gallery/guerry/guerry.csv | gsutil cp - {BUCKET_URI}/guerry.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Google Cloud Storage  Connector\n",
    "\n",
    "The [Google Cloud Connector for Hadoop](https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/master/gcs/README.md) can be used for reading files from a GCS bucket from a Spark application running on Ray on Vertex AI. This is done using a few configuration parameters when a Spark session is created using RayDP. The following code shows how a CSV file stored in a GCS bucket can be read from a Spark application.\n",
    "\n",
    "This tutorial assumes that the IAM Service Account used by Ray cluster on Vertex AI has been granted required IAM permissions to read from the GCS bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create python script for Spark application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {SCRIPT_DIR}/spark_gcs_connector.py\n",
    "\n",
    "import raydp\n",
    "\n",
    "spark = raydp.init_spark(\n",
    "  app_name=\"RayDP GCS Example 1\",\n",
    "  configs={\n",
    "      \"spark.jars\": \"https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-2.2.22.jar\",\n",
    "      \"spark.hadoop.fs.AbstractFileSystem.gs.impl\": \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\",\n",
    "      \"spark.hadoop.fs.gs.impl\": \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\",\n",
    "  },\n",
    "  num_executors=2,\n",
    "  executor_cores=4,\n",
    "  executor_memory=\"500M\",\n",
    ")\n",
    "\n",
    "df = spark.read.csv(\"GCS_FILE_URI\", header = True, inferSchema = True)\n",
    "print(f\"CSV data is: {df.toJSON().collect()}\")\n",
    "raydp.stop_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! sed -i 's^GCS_FILE_URI^{BUCKET_URI}/guerry.csv^g' {SCRIPT_DIR}/spark_gcs_connector.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ray.job_submission import JobSubmissionClient\n",
    "\n",
    "client = JobSubmissionClient(RAY_ADDRESS)\n",
    "\n",
    "job_id = client.submit_job(\n",
    "  # Entrypoint shell command to execute\n",
    "  entrypoint=\"python spark_gcs_connector.py\",\n",
    "  # Path to the local directory that contains the python script file.\n",
    "  runtime_env={\n",
    "    \"working_dir\": SCRIPT_DIR,\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Ray data\n",
    "\n",
    "[Ray data API](https://docs.ray.io/en/latest/data/api/api.html) provides very convenient methods to read files from GCS bucket and it also leverages Ray's distributed processing for reading data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create python script for Spark application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {SCRIPT_DIR}/spark_gcs_ray_data.py\n",
    "\n",
    "import raydp\n",
    "import ray\n",
    "\n",
    "spark = raydp.init_spark(\n",
    "  app_name=\"RayDP GCS Example 2\",\n",
    "  num_executors=2,\n",
    "  executor_cores=4,\n",
    "  executor_memory=\"500M\",\n",
    ")\n",
    "\n",
    "ray_dataset = ray.data.read_csv(\"GCS_FILE_URI\")\n",
    "df = ray_dataset.to_spark(spark)\n",
    "print(f\"CSV data is: {df.toJSON().collect()}\")\n",
    "raydp.stop_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! sed -i 's^GCS_FILE_URI^{BUCKET_URI}/guerry.csv^g' {SCRIPT_DIR}/spark_gcs_ray_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ray.job_submission import JobSubmissionClient\n",
    "\n",
    "client = JobSubmissionClient(RAY_ADDRESS)\n",
    "\n",
    "job_id = client.submit_job(\n",
    "  # Entrypoint shell command to execute\n",
    "  entrypoint=\"python spark_gcs_ray_data.py\",\n",
    "  # Path to the local directory that contains the python script file.\n",
    "  runtime_env={\n",
    "    \"working_dir\": SCRIPT_DIR,\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas UDF on Ray on Vertex AI\n",
    "\n",
    "The [Pyspark Pandas UDF](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.pandas_udf.html) may sometimes require additional code when they are used in a Spark application running on a Ray cluster on Vertex AI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Python package dependencies\n",
    "\n",
    "The [Python dependencies](https://docs.ray.io/en/latest/cluster/running-applications/job-submission/sdk.html#dependency-management) of an application can be installed using Runtime Environment with Ray job API when the Ray job is submitted to the cluster, Ray installs those dependencies in the Python virtual environment that it creates for running the job. The Pandas UDF, however, do nt run in the same python virtual environment. It instead is run in the python System environment. If that dependency is not available in the System environment, that dependency needs to be installed within Pandas UDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {SCRIPT_DIR}/pandas_udf_dependency.py\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import raydp\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def test_udf(spark: pyspark.sql.SparkSession):\n",
    "    import pandas as pd\n",
    "    \n",
    "    df = spark.createDataFrame(pd.read_csv(\"https://www.datavis.ca/gallery/guerry/guerry.csv\"))\n",
    "    return df.select(func('Lottery','Literacy', 'Pop1831')).collect()\n",
    "\n",
    "\n",
    "@pandas_udf(StringType())\n",
    "def func(s1: pd.Series, s2: pd.Series, s3: pd.Series) -> str:\n",
    "    import numpy as np\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"statsmodels\"])\n",
    "    import statsmodels.api as sm\n",
    "    import statsmodels.formula.api as smf\n",
    "    \n",
    "    d = {'Lottery': s1, \n",
    "         'Literacy': s2,\n",
    "         'Pop1831': s3}\n",
    "    data = pd.DataFrame(d)\n",
    "\n",
    "    # Fit regression model (using the natural log of one of the regressors)\n",
    "    results = smf.ols('Lottery ~ Literacy + np.log(Pop1831)', data=data).fit()\n",
    "    return results.summary().as_csv()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    spark = raydp.init_spark(\n",
    "      app_name=\"RayDP UDF Example\",\n",
    "      num_executors=2,\n",
    "      executor_cores=4,\n",
    "      executor_memory=\"1500M\",\n",
    "    )\n",
    "    \n",
    "    print(test_udf(spark))\n",
    "    \n",
    "    raydp.stop_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ray.job_submission import JobSubmissionClient\n",
    "\n",
    "client = JobSubmissionClient(RAY_ADDRESS)\n",
    "\n",
    "job_id = client.submit_job(\n",
    "  # Entrypoint shell command to execute\n",
    "  entrypoint=\"python pandas_udf_dependency.py\",\n",
    "  # Path to the local directory that contains the python script file.\n",
    "  runtime_env={\n",
    "    \"working_dir\": SCRIPT_DIR,\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle local Python dependencies\n",
    "\n",
    "The recommended best practice for handling Python dependencies is via Python repository. Therefore, publish your own custom packages to your Python repository and install those packages using pip. In case you are using local python package dependency in Pandas UDF of your Spark application, additional code is required to add the local packages to PYTHONPATH of Python System environment of Ray cluster on Vertex AI nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create local python module\n",
    "\n",
    "This is a very simple python file that has one method. This method takes a string argument and prints it to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {SCRIPT_DIR}/my_module.py\n",
    "\n",
    "def print_func(text: str):\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create python script\n",
    "\n",
    "Use the python script of previous section to demonstrate the handling of local dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {SCRIPT_DIR}/pandas_udf_local_dependency.py\n",
    "\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import raydp\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def test_udf(spark: pyspark.sql.SparkSession):\n",
    "    import pandas as pd\n",
    "    df = spark.createDataFrame(pd.read_csv(\"https://www.datavis.ca/gallery/guerry/guerry.csv\"))\n",
    "    import pathlib\n",
    "    module_path = str(pathlib.Path(__file__).parent.resolve())\n",
    "    return df.select(udf_wrapper_func('Lottery','Literacy', 'Pop1831', module_path)).collect()\n",
    "\n",
    "def udf_wrapper_func(s1: pd.Series, s2: pd.Series, s3: pd.Series, module_path: str) -> str:\n",
    "\n",
    "    @pandas_udf(StringType())\n",
    "    def func(s1: pd.Series, s2: pd.Series, s3: pd.Series) -> str:\n",
    "        import sys\n",
    "        sys.path.append(module_path)\n",
    "        \n",
    "        # import local module\n",
    "        import my_module\n",
    "        my_module.print_func(\"This is a UDF local dependency test.\")\n",
    "\n",
    "        import numpy as np\n",
    "        import subprocess\n",
    "        import sys\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"statsmodels\"])\n",
    "        import statsmodels.api as sm\n",
    "        import statsmodels.formula.api as smf\n",
    "\n",
    "        d = {'Lottery': s1, \n",
    "             'Literacy': s2,\n",
    "             'Pop1831': s3}\n",
    "        data = pd.DataFrame(d)\n",
    "\n",
    "    #     # Fit regression model (using the natural log of one of the regressors)\n",
    "        results = smf.ols('Lottery ~ Literacy + np.log(Pop1831)', data=data).fit()\n",
    "        return results.summary().as_csv()\n",
    "\n",
    "    return func(s1, s2, s3)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    spark = raydp.init_spark(\n",
    "      app_name=\"UDF_TEST\",\n",
    "      num_executors=2,\n",
    "      executor_cores=2,\n",
    "      executor_memory=\"500M\",\n",
    "    )\n",
    "    \n",
    "    print(test_udf(spark))\n",
    "    \n",
    "    raydp.stop_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ray.job_submission import JobSubmissionClient\n",
    "\n",
    "client = JobSubmissionClient(RAY_ADDRESS)\n",
    "\n",
    "job_id = client.submit_job(\n",
    "  # Entrypoint shell command to execute\n",
    "  entrypoint=\"python pandas_udf_local_dependency.py\",\n",
    "  # Path to the local directory that contains the python script file.\n",
    "  runtime_env={\n",
    "    \"working_dir\": SCRIPT_DIR,\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ray job should complete sucessfully and the job logs should have a line `This is a UDF local dependency test.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a4e033321ad"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the cluster you created in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete the cluster\n",
    "vertex_ray.delete_ray_cluster(CLUSTER_RESOURCE_NAME)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

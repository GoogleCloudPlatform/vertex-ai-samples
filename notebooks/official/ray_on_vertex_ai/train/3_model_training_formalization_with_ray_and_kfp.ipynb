{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Model Training formalization with Ray and KFP on Vertex AI\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/tree/main/notebooks/official/ray_on_vertex_ai/ray_train/model_training_formalization_with_ray_and_kfp.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Fray_on_vertex%2Fray_train%2model_training_formalization_with_ray_and_kfp.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>    \n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/tree/main/notebooks/official/ray_on_vertex_ai/ray_train/model_training_formalization_with_ray_and_kfp.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/tree/main/notebooks/official/ray_on_vertex_ai/ray_train/model_training_formalization_with_ray_and_kfp.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial demonstrates how to orchestrate a distributed training using Ray on Vertex AI and Vertex AI Pipelines for an XGBoost model.\n",
        "\n",
        "Learn more about [Ray on Vertex AI overview](https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/overview).\n",
        "\n",
        "### Objective\n",
        "\n",
        "In this notebook, you learn how to build a KFP pipeline component to preprocess, train and register a XGBoost model using Ray Train on Vertex AI.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services and resources:\n",
        "\n",
        "- Cloud storage\n",
        "- BigQuery\n",
        "- Ray on Vertex AI\n",
        "- Vertex AI Pipelines\n",
        "- Vertex AI Model Registry\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Build a `VertexRayJob` pipeline component\n",
        "- Preprocess data\n",
        "- Build features\n",
        "- Train model\n",
        "- Register model on Model Registry\n",
        "\n",
        "### Dataset\n",
        "\n",
        "The [Chicago Taxi Trips dataset](https://cloud.google.com/bigquery/public-data/) is one of public datasets hosted with BigQuery, which includes taxi trips from 2013 to the present, reported to the City of Chicago in its role as a regulatory agency. The taxi_trips table size is 70.72 GB and includes more than 195 million records. The dataset includes information about the trips, like pickup and dropoff datetime and location, passengers count, miles travelled, and trip toll.\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* BigQuery\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [BigQuery pricing](https://cloud.google.com/bigquery/pricing)\n",
        "and [Cloud Storage pricing](https://cloud.google.com/storage/pricing),\n",
        "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## Before to start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Vertex AI SDK for Python and other required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFy3H3aPgx12"
      },
      "outputs": [],
      "source": [
        "! pip3 install --upgrade --quiet google-cloud-bigquery google-cloud-aiplatform[ray] kfp google-cloud-pipeline-components\n",
        "! pip3 install --upgrade --quiet etils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "### Restart runtime (Colab only)\n",
        "\n",
        "To use the newly installed packages, you must restart the runtime on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRvKdaPDTznN"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmM4z7FOBpM"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "Authenticate your environment on Google Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Lj5fVWX1s2A"
      },
      "source": [
        "### Set Google Cloud project information\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WReHDGG5g0XY"
      },
      "source": [
        "Set your project ID.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM1iC_MfAts1"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmAeQqAhAK9p"
      },
      "source": [
        "Indicate the project number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZpm-sL8f1z_"
      },
      "outputs": [],
      "source": [
        "PROJECT_NUMBER = \"[your-project-number]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "Set the region used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6FmBV2_0fBP"
      },
      "outputs": [],
      "source": [
        "REGION = \"us-central1\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgPO1eR3CYjk"
      },
      "source": [
        "Create a storage bucket to store intermediate artifacts such as datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzGDU7TWdts_"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}\n",
        "\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIq7R4HZCfIc"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "set_service_account"
      },
      "source": [
        "Specify a service account with the following permissions:\n",
        "\n",
        "-   `Vertex AI User` to call Vertex LLM API\n",
        "-   `Storage Admin` to manage GCS bucket.\n",
        "-   `Storage Object Admin` to read and write to your GCS bucket.\n",
        "\n",
        "[Check out the documentation](https://cloud.google.com/iam/docs/manage-access-service-accounts#iam-view-access-sa-gcloud) to know how to grant those permissions to a single service account.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssUJJqXJJHgC"
      },
      "outputs": [],
      "source": [
        "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}\n",
        "\n",
        "SERVICE_ACCOUNT = f\"{PROJECT_NUMBER}-compute@developer.gserviceaccount.com\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqOHg5aid6HP"
      },
      "outputs": [],
      "source": [
        "! gcloud projects add-iam-policy-binding {PROJECT_ID} \\\n",
        "    --member=\"serviceAccount:{SERVICE_ACCOUNT}\" \\\n",
        "    --role=\"roles/storage.admin\"\n",
        "\n",
        "! gcloud projects add-iam-policy-binding {PROJECT_ID} \\\n",
        "    --member=\"serviceAccount:{SERVICE_ACCOUNT}\" \\\n",
        "    --role=\"roles/storage.objectAdmin\"\n",
        "\n",
        "! gcloud projects add-iam-policy-binding {PROJECT_ID} \\\n",
        "    --member=\"serviceAccount:{SERVICE_ACCOUNT}\" \\\n",
        "    --role=\"roles/aiplatform.admin\"\n",
        "\n",
        "! gcloud projects add-iam-policy-binding {PROJECT_ID} \\\n",
        "    --member=\"serviceAccount:{SERVICE_ACCOUNT}\" \\\n",
        "    --role=\"roles/iam.serviceAccountUser\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irpBaGlBX1Xm"
      },
      "source": [
        "### Set workspace\n",
        "\n",
        "Create a workspace to store pipelines deliverables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpMrFp1zX5Zm"
      },
      "outputs": [],
      "source": [
        "from etils import epath\n",
        "\n",
        "WORKSPACE_FOLDER_URI = epath.Path(BUCKET_URI) / \"chicago_taxitrips\"\n",
        "\n",
        "epath.Path(WORKSPACE_FOLDER_URI).mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmkCE0JpVg_2"
      },
      "source": [
        "### Prepare the BigQuery dataset\n",
        "\n",
        "You create a Bigquery dataset to extract, transform and load ML dataset for training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yx9zrNi7WAQ6"
      },
      "outputs": [],
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "\n",
        "def create_bq_dataset(dataset_id, project_id, location):\n",
        "    \"\"\"Create a BigQuery dataset.\"\"\"\n",
        "    bq_client = bigquery.Client(project=project_id, location=location)\n",
        "    dataset_uri = f\"{bq_client.project}.{dataset_id}\"\n",
        "    dataset = bigquery.Dataset(dataset_uri)\n",
        "    dataset = bq_client.create_dataset(dataset, exists_ok=True)\n",
        "    print(f\"Created dataset {dataset.dataset_id}!\")\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def run_bq_query(sql, project_id, location):\n",
        "    \"\"\"Run a BigQuery query.\"\"\"\n",
        "    bq_client = bigquery.Client(project=project_id, location=location)\n",
        "    job_config = bigquery.QueryJobConfig()\n",
        "    job = bq_client.query(sql, job_config=job_config)\n",
        "    job.result()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SUHQa4KWdI8"
      },
      "outputs": [],
      "source": [
        "LOCATION = REGION.split(\"-\")[0]\n",
        "DATASET_ID = \"rov_dataset\"\n",
        "TABLE_ID = \"chicago_taxitrips\"\n",
        "YEAR = 2023\n",
        "LIMIT = 100000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzarmmvJW8dn"
      },
      "outputs": [],
      "source": [
        "_ = create_bq_dataset(DATASET_ID, PROJECT_ID, LOCATION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFvTboYJBSRX"
      },
      "outputs": [],
      "source": [
        "SQL_QUERY = f\"\"\"\n",
        "CREATE OR REPLACE TABLE `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}` AS\n",
        "SELECT\n",
        "    trip_start_timestamp,\n",
        "    EXTRACT(MONTH FROM trip_start_timestamp) AS trip_month,\n",
        "    EXTRACT(DAY FROM trip_start_timestamp) AS trip_day,\n",
        "    EXTRACT(DAYOFWEEK FROM trip_start_timestamp) AS trip_day_of_week,\n",
        "    EXTRACT(HOUR FROM trip_start_timestamp) AS trip_hour,\n",
        "    trip_seconds,\n",
        "    trip_miles,\n",
        "    payment_type,\n",
        "    ST_ASTEXT(ST_SNAPTOGRID(ST_GEOGPOINT(pickup_longitude, pickup_latitude), 0.1)) AS pickup_grid,\n",
        "    ST_ASTEXT(ST_SNAPTOGRID(ST_GEOGPOINT(dropoff_longitude, dropoff_latitude), 0.1)) AS dropoff_grid,\n",
        "    ST_DISTANCE(ST_GEOGPOINT(pickup_longitude, pickup_latitude), ST_GEOGPOINT(dropoff_longitude, dropoff_latitude)) AS euclidean,\n",
        "    CONCAT(ST_ASTEXT(ST_SNAPTOGRID(ST_GEOGPOINT(pickup_longitude, pickup_latitude), 0.1)), ST_ASTEXT(ST_SNAPTOGRID(ST_GEOGPOINT(dropoff_longitude, dropoff_latitude), 0.1))) AS loc_cross,\n",
        "    IF((tips / fare >= 0.2), 1, 0) AS tip_bin,\n",
        "    IF(RAND() <= 0.8, 'UNASSIGNED', 'TEST') AS ML_use\n",
        "FROM\n",
        "    `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
        "WHERE\n",
        "    pickup_longitude IS NOT NULL\n",
        "    AND pickup_latitude IS NOT NULL\n",
        "    AND dropoff_longitude IS NOT NULL\n",
        "    AND dropoff_latitude IS NOT NULL\n",
        "    AND trip_miles > 0\n",
        "    AND trip_seconds > 0\n",
        "    AND fare > 0\n",
        "    AND EXTRACT(YEAR FROM trip_start_timestamp) = {YEAR}\n",
        "LIMIT {LIMIT}\n",
        "\"\"\"\n",
        "_ = run_bq_query(SQL_QUERY, PROJECT_ID, LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9ryiScCEapt"
      },
      "source": [
        "### Set a Ray cluster on Vertex AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwmK5nBBCgJa"
      },
      "source": [
        "Before running the code below, make sure to [set up](https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/set-up) Ray on Vertex AI and [create](https://cloud.google.com/vertex-ai/docs/open-source/ray-on-vertex-ai/create-cluster) at least one Ray cluster on Vertex AI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mlq_1NLEonF"
      },
      "outputs": [],
      "source": [
        "import vertex_ray\n",
        "from google.cloud import aiplatform as vertex_ai\n",
        "from vertex_ray import Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvDCnryTWp1j"
      },
      "source": [
        "#### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgJ53CttWp1j"
      },
      "outputs": [],
      "source": [
        "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15PYjIyOcx1d"
      },
      "source": [
        "#### Define cluster configuration\n",
        "\n",
        "To provision a Ray cluster on Vertex AI, you can use a default provisioning request or you can specify the replica count (number of nodes), machine type, disk_spec, and accelerator as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EX89VkNXDPp"
      },
      "outputs": [],
      "source": [
        "# Ray component\n",
        "VERTEX_RAY_IMAGE_URI = \"us-docker.pkg.dev/vertex-ai/training/ray-cpu.2-9.py310:latest\"  # @param [\"us-docker.pkg.dev/vertex-ai/training/ray-cpu.2-9.py310:latest\", \"europe-docker.pkg.dev/vertex-ai/training/ray-cpu.2-9.py310:latest\", \"asia-docker.pkg.dev/vertex-ai/training/ray-cpu.2-9.py310:latest\"] {allow-input: true}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjLG3rsRbiHr"
      },
      "outputs": [],
      "source": [
        "head_node_type = Resources(\n",
        "    machine_type=\"n1-standard-16\",\n",
        "    node_count=1,\n",
        "    custom_image=VERTEX_RAY_IMAGE_URI,\n",
        ")\n",
        "\n",
        "worker_node_types = [\n",
        "    Resources(\n",
        "        machine_type=\"n1-standard-16\",\n",
        "        node_count=2,\n",
        "        custom_image=VERTEX_RAY_IMAGE_URI,\n",
        "        accelerator_type=None,\n",
        "        accelerator_count=0,\n",
        "    )\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_O1xUMt7Z6r0"
      },
      "source": [
        "#### Create the Ray cluster\n",
        "\n",
        "Create the Ray cluster using the Vertex AI SDK for Python version used with Ray."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZkHOH3v2i1p"
      },
      "outputs": [],
      "source": [
        "cluster_name = \"ray-cluster-train-pipeline-tutorial\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-g6kLwqUj5n"
      },
      "outputs": [],
      "source": [
        "ray_cluster_name = vertex_ray.create_ray_cluster(\n",
        "    head_node_type=head_node_type,\n",
        "    worker_node_types=worker_node_types,\n",
        "    cluster_name=cluster_name,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmBlsbHAc2uO"
      },
      "source": [
        "#### Get the Ray cluster\n",
        "\n",
        "Use the Vertex AI SDK for Python to get the Ray cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UzG2WyXbZJi"
      },
      "outputs": [],
      "source": [
        "ray_cluster = vertex_ray.get_ray_cluster(ray_cluster_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Zxw-Lm5NNG_Y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pretty printing has been turned OFF\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "pprint(ray_cluster)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdvJRUWRNGHE"
      },
      "source": [
        "## Build a Ray-based ML pipeline using Vertex AI Pipelines\n",
        "\n",
        "In this tutorial, you build and run a simple ML pipeline using KFP SDK on Vertex AI Pipeline.\n",
        "\n",
        "The pipeline covers the following main tasks:\n",
        "\n",
        "- `create_tabular_dataset_task` to store the ML dataset as Vertex AI Tabular dataset.\n",
        "\n",
        "- `run_ray_training_task` to submit a Ray Train job on a Ray on Vertex AI cluster.\n",
        "\n",
        "- `get_eval_metrics_task` to collect evaluation metrics to use in a blessing condition to register the model in Vertex AI Model Registry.\n",
        "\n",
        "- `upload_model_task` to version the model in Vertex AI Model Registry.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6ZuQvqFnGKJ"
      },
      "source": [
        "### Import libraries\n",
        "\n",
        "Import the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEJbiJXOnGKJ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import sys\n",
        "# General\n",
        "import uuid\n",
        "from typing import NamedTuple\n",
        "\n",
        "# ML Pipeline\n",
        "import kfp\n",
        "import ray\n",
        "from google_cloud_pipeline_components.types.artifact_types import (\n",
        "    VertexDataset, VertexModel)\n",
        "from google_cloud_pipeline_components.v1.dataset import TabularDatasetCreateOp\n",
        "from kfp import compiler, dsl\n",
        "from kfp.dsl import Condition, Input, Metrics, Output, component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SzLoKKJT-Eh"
      },
      "outputs": [],
      "source": [
        "print(\"Ray version:\", ray.__version__)\n",
        "print(\"Python version:\", sys.version)\n",
        "print(\"Kfp version:\", kfp.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO3PpqX3eLmQ"
      },
      "source": [
        "### Define constants\n",
        "\n",
        "Define contants for the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c96YmkWDeK8V"
      },
      "outputs": [],
      "source": [
        "# Pipeline\n",
        "WORKING_URI = WORKSPACE_FOLDER_URI / \"src\"\n",
        "ARTIFACT_STORE = WORKSPACE_FOLDER_URI / \"artifacts\"\n",
        "PIPELINE_NAME = \"chicago-taxitrips-train-pipeline\"\n",
        "PIPELINE_FILE_PATH = \"pipeline.json\"\n",
        "PIPELINE_ROOT = str(ARTIFACT_STORE / PIPELINE_NAME)\n",
        "MODEL_STORE = ARTIFACT_STORE / \"models\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_lglIA4IvuV"
      },
      "source": [
        "### Define helpers\n",
        "\n",
        "Define helpers to use in the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJKanQZwIxvL"
      },
      "outputs": [],
      "source": [
        "def get_id(n=5):\n",
        "    \"\"\"Generate a random string of letters and digits.\"\"\"\n",
        "    return \"\".join(random.sample(str(uuid.uuid4()), n))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI-HcwhzFLi7"
      },
      "source": [
        "### Build Ray-based ML pipeline components\n",
        "\n",
        "You start building the component of your Ray-based ML pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14VxwnUrP-V6"
      },
      "source": [
        "#### Build Dataset component\n",
        "\n",
        "The `GetDatasetUriOp` component gets an [`VertexDataset`](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-2.0.0/api/artifact_types.html), an artifact representing a Vertex AI Dataset resource, and returns its BigQuery URI.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqbUIIEBQG1f"
      },
      "outputs": [],
      "source": [
        "@dsl.component(\n",
        "    base_image=\"python:3.9\",\n",
        "    packages_to_install=[\n",
        "        \"kfp\",\n",
        "        \"google-cloud-aiplatform\",\n",
        "        \"google-cloud-pipeline-components\",\n",
        "    ],\n",
        ")\n",
        "def GetDatasetUriOp(\n",
        "    dataset: Input[VertexDataset], project: str, region: str, bucket_uri: str\n",
        ") -> NamedTuple(\"outputs\", dataset_uri=str):\n",
        "    \"\"\"Get the dataset resource name.\"\"\"\n",
        "\n",
        "    import logging\n",
        "\n",
        "    from google.cloud import aiplatform as vertex_ai\n",
        "    from google.protobuf.json_format import MessageToDict\n",
        "\n",
        "    # Configure logging\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
        "    )\n",
        "\n",
        "    # Initialize the Vertex AI SDK\n",
        "    logging.info(\"Initializing Vertex AI SDK.\")\n",
        "    vertex_ai.init(project=project, location=region, staging_bucket=bucket_uri)\n",
        "\n",
        "    # Get the dataset resource name\n",
        "    logging.info(\n",
        "        f\"Getting dataset resource name from Vertex AI dataset '{dataset.metadata['resourceName']}'.\"\n",
        "    )\n",
        "    vertex_ai_dataset = vertex_ai.TabularDataset(\n",
        "        dataset_name=dataset.metadata[\"resourceName\"]\n",
        "    )\n",
        "    bq_uri = MessageToDict(vertex_ai_dataset.gca_resource._pb)[\"metadata\"][\n",
        "        \"inputConfig\"\n",
        "    ][\"bigquerySource\"][\"uri\"].replace(\"bq://\", \"\")\n",
        "\n",
        "    component_outputs = NamedTuple(\"outputs\", dataset_uri=str)\n",
        "    return component_outputs(bq_uri)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-EUrudybvu9"
      },
      "source": [
        "#### Build a `VertexRayJobOp` component to run training job using Ray Jobs API\n",
        "\n",
        "To orchestrate a Ray Job in a Vertex AI Pipeline, you need to create a KFP pipeline component which takes a Python script and submit the script to an existing Ray cluster on Vertex AI using the Ray Jobs API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QvQM3wVHyNp"
      },
      "source": [
        "##### Build the `VertexRayJobOp` component\n",
        "\n",
        "The `VertexRayJobOp` component submit the Ray Jobs API throught the public Ray dashboard address. **It is important to hightlight that  is accessible from outside the VPC, including the public internet. Use this component for experimentation only**. For production application, consider to set up connectivity from Vertex AI to your VPN and be sure to deploy the Ray on Vertex AI cluster in the same network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0qYJIxWpRL0"
      },
      "outputs": [],
      "source": [
        "@dsl.component(\n",
        "    base_image=VERTEX_RAY_IMAGE_URI,\n",
        "    packages_to_install=[\"google-cloud-aiplatform[ray]\"],\n",
        ")\n",
        "def VertexRayJobOp(\n",
        "    cluster_name: str,\n",
        "    entrypoint: str,\n",
        "    runtime_env: dict,\n",
        ") -> NamedTuple(\"outputs\", job_id=str):\n",
        "    \"\"\"\n",
        "    Submit a Ray job to a Vertex AI cluster.\n",
        "    \"\"\"\n",
        "\n",
        "    # Import Libraries\n",
        "    import logging\n",
        "    import time\n",
        "\n",
        "    from ray.job_submission import JobStatus, JobSubmissionClient\n",
        "\n",
        "    # Configure logging\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
        "    )\n",
        "\n",
        "    # Initialize Ray Job client\n",
        "    ray_job_client = JobSubmissionClient(f\"vertex_ray://{cluster_name}\")\n",
        "    logging.info(f\"Initialized Ray Job client for cluster '{cluster_name}'.\")\n",
        "\n",
        "    # Submit Ray Job\n",
        "    job_id = ray_job_client.submit_job(entrypoint=entrypoint, runtime_env=runtime_env)\n",
        "    logging.info(f\"Submitted Ray job with ID '{job_id}'.\")\n",
        "\n",
        "    # Monitor Ray Job\n",
        "    while True:\n",
        "        try:\n",
        "            ray_job_client = JobSubmissionClient(f\"vertex_ray://{cluster_name}\")\n",
        "            job_status = ray_job_client.get_job_status(job_id)\n",
        "            logging.info(f\"Job '{job_id}' status: {job_status.value}\")\n",
        "\n",
        "            if job_status == JobStatus.SUCCEEDED:\n",
        "                logging.info(f\"Job '{job_id}' succeeded!\")\n",
        "                break\n",
        "            elif job_status == JobStatus.FAILED:\n",
        "                log_message = f\"Job '{job_id}' failed! Logs: {ray_job_client.get_job_logs(job_id)}\"\n",
        "                logging.error(log_message)\n",
        "                raise Exception(\"Job failed!\")\n",
        "            else:\n",
        "                time.sleep(60)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while monitoring the job: {e}\")\n",
        "            raise e\n",
        "\n",
        "    component_outputs = NamedTuple(\"outputs\", job_id=str)\n",
        "    return component_outputs(job_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRPvvMvYYdPZ"
      },
      "source": [
        "##### Prepare the training component code\n",
        "\n",
        "Write the Python script to train the model using Ray."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNxctKO6AR2i"
      },
      "outputs": [],
      "source": [
        "training_script = \"\"\"\n",
        "\n",
        "# Libraries\n",
        "import argparse\n",
        "from uuid import uuid4\n",
        "\n",
        "import ray\n",
        "from ray.runtime_env import RuntimeEnv\n",
        "from ray.data import preprocessors\n",
        "from ray.train.xgboost import XGBoostTrainer, XGBoostCheckpoint\n",
        "from ray.train import ScalingConfig, RunConfig, CheckpointConfig\n",
        "\n",
        "\n",
        "\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--bq_dataset_uri', type=str, required=True)\n",
        "    parser.add_argument('--training_dir', type=str, required=True)\n",
        "    parser.add_argument('--training_run_id', type=str, required=True)\n",
        "    parser.add_argument('--project_id', type=str, required=True)\n",
        "    parser.add_argument('--region', type=str, required=True)\n",
        "    parser.add_argument('--bucket_uri', type=str, required=True)\n",
        "    parser.add_argument('--test_size', type=float, default=0.2)\n",
        "    parser.add_argument('--seed', type=int, default=8)\n",
        "    parser.add_argument('--objective', type=str, default='binary:logistic')\n",
        "    parser.add_argument('--eval_metric', type=str, default='error')\n",
        "    parser.add_argument('--eta', type=float, default=0.01)\n",
        "    parser.add_argument('--max_depth', type=int, default=30)\n",
        "    parser.add_argument('--subsample', type=float, default=0.2)\n",
        "    parser.add_argument('--n_estimators', type=int, default=100)\n",
        "    parser.add_argument('--use_gpu', action=argparse.BooleanOptionalAction)\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    ###################################\n",
        "    # Get arguments and set variables #\n",
        "    ###################################\n",
        "\n",
        "    args = get_args()\n",
        "\n",
        "    ############################################\n",
        "    # Initialize vertex ai sdk and ray session #\n",
        "    ############################################\n",
        "\n",
        "    ray.init()\n",
        "    ctx = ray.data.DataContext.get_current()\n",
        "    ctx.execution_options.preserve_order = True\n",
        "\n",
        "    #############\n",
        "    # Read data #\n",
        "    #############\n",
        "\n",
        "    query = f'''\n",
        "      SELECT\n",
        "        IF(trip_month IS NULL, -1, trip_month) trip_month,\n",
        "        IF(trip_day IS NULL, -1, trip_day) trip_day,\n",
        "        IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\n",
        "        IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\n",
        "        IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\n",
        "        IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\n",
        "        IF(payment_type IS NULL, 'NA', payment_type) payment_type,\n",
        "        IF(pickup_grid IS NULL, 'NA', pickup_grid) pickup_grid,\n",
        "        IF(dropoff_grid IS NULL, 'NA', dropoff_grid) dropoff_grid,\n",
        "        IF(euclidean IS NULL, -1, euclidean) euclidean,\n",
        "        IF(loc_cross IS NULL, 'NA', loc_cross) loc_cross,\n",
        "        tip_bin\n",
        "        FROM `{args.bq_dataset_uri}`\n",
        "        WHERE ML_use = 'UNASSIGNED'\n",
        "      '''\n",
        "\n",
        "    bq_dataset = ray.data.read_bigquery(\n",
        "      query=query,\n",
        "      project_id=args.project_id,\n",
        "    )\n",
        "\n",
        "    ################################################\n",
        "    # Train/test splitting and feature engineering #\n",
        "    ################################################\n",
        "\n",
        "     # train/test splitting\n",
        "    bq_train_dataset, bq_valid_dataset = bq_dataset.train_test_split(\n",
        "    test_size=args.test_size, seed=args.seed\n",
        "    )\n",
        "\n",
        "    # min_max_scaling\n",
        "    min_max_scaler = preprocessors.MinMaxScaler(\n",
        "    columns=[\"trip_seconds\", \"trip_miles\", \"euclidean\"]\n",
        "    )\n",
        "    min_max_scaler.fit(bq_train_dataset)\n",
        "    train_dataset = min_max_scaler.transform(bq_train_dataset)\n",
        "    valid_dataset = min_max_scaler.transform(bq_valid_dataset)\n",
        "\n",
        "    # ordinal encoding\n",
        "    ordinal_encoder = preprocessors.OrdinalEncoder(\n",
        "    columns=[\"trip_month\", \"trip_day\", \"trip_hour\", \"pickup_grid\",\n",
        "              \"dropoff_grid\", \"loc_cross\", \"payment_type\", \"trip_day_of_week\"]\n",
        "    )\n",
        "    ordinal_encoder.fit(bq_train_dataset)\n",
        "    train_dataset = ordinal_encoder.transform(bq_train_dataset)\n",
        "    valid_dataset = ordinal_encoder.transform(bq_valid_dataset)\n",
        "\n",
        "\n",
        "    ###############\n",
        "    # Train model #\n",
        "    ###############\n",
        "\n",
        "    # xgboost configuration\n",
        "    xgboost_config = {\n",
        "        'objective': args.objective,\n",
        "        'eval_metric': [args.eval_metric],\n",
        "        'eta': args.eta,\n",
        "        'max_depth': args.max_depth,\n",
        "        'subsample': args.subsample\n",
        "    }\n",
        "\n",
        "    additional_config = {\n",
        "        'num_boost_round': args.n_estimators\n",
        "    }\n",
        "\n",
        "    # scaling config\n",
        "    scaling_config = ScalingConfig(\n",
        "        num_workers=1,\n",
        "        use_gpu=args.use_gpu\n",
        "    )\n",
        "\n",
        "    # run config\n",
        "    run_config = RunConfig(\n",
        "      storage_path=args.training_dir,\n",
        "      checkpoint_config=CheckpointConfig(\n",
        "        num_to_keep=5\n",
        "      ),\n",
        "      name=args.training_run_id,\n",
        "    )\n",
        "\n",
        "    # train model\n",
        "    trainer = XGBoostTrainer(\n",
        "        scaling_config=scaling_config,\n",
        "        run_config=run_config,\n",
        "        label_column='tip_bin',\n",
        "        params=xgboost_config,\n",
        "        datasets={'train': train_dataset, 'valid': valid_dataset},\n",
        "        **additional_config\n",
        "    )\n",
        "\n",
        "    xgb_result = trainer.fit()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INQBk-8lL7wF"
      },
      "source": [
        "##### Upload the training code to the Cloud Bucket\n",
        "\n",
        "Upload the training code to the Cloud Bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvnKU33kL_17"
      },
      "outputs": [],
      "source": [
        "with (WORKING_URI / \"train.py\").open(\"w\") as train_file:\n",
        "    train_file.write(training_script)\n",
        "train_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btodOPgFkiqb"
      },
      "source": [
        "#### Build Model Evalution component\n",
        "\n",
        "The `GetMetricsOp` component reads evaluation metrics (error) and store them as Metrics artifact."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7gHWrN3kur-"
      },
      "outputs": [],
      "source": [
        "@dsl.component(\n",
        "    base_image=VERTEX_RAY_IMAGE_URI,\n",
        "    packages_to_install=[\n",
        "        \"google-cloud-aiplatform[ray]\",\n",
        "        \"ray[data]==2.9.3\",\n",
        "        \"ray[data]==2.9.3\",\n",
        "        \"ray[train]==2.9.3\",\n",
        "        \"kfp\",\n",
        "        \"google-cloud-pipeline-components\",\n",
        "        \"etils\",\n",
        "        \"importlib_resources\",\n",
        "    ],\n",
        ")\n",
        "def GetMetricsOp(\n",
        "    training_dir: str, training_run_id: str, metrics: Output[Metrics]\n",
        ") -> NamedTuple(\"outputs\", threshold_metric=float):\n",
        "    \"\"\"Get evaluation metrics from training run.\"\"\"\n",
        "\n",
        "    import logging\n",
        "\n",
        "    from etils import epath\n",
        "    from ray.tune import ExperimentAnalysis\n",
        "\n",
        "    eval_metrics = \"error\"\n",
        "    threshold_metric = \"valid-error\"\n",
        "\n",
        "    # Configure logging\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
        "    )\n",
        "\n",
        "    # restore training result and log metrics\n",
        "    logging.info(\"Restoring training result.\")\n",
        "    training_dir = training_dir.replace(\"gs://\", \"/gcs/\")\n",
        "    training_path = epath.Path(training_dir)\n",
        "    training_id_path = training_path / training_run_id\n",
        "    experiment_analysis = ExperimentAnalysis(training_id_path)\n",
        "    results = experiment_analysis.results_df.reset_index().to_dict(orient=\"records\")\n",
        "\n",
        "    # log metrics and store threshold metrics\n",
        "    logging.info(\"Logging metrics and store threshold metrics.\")\n",
        "    for result in results:\n",
        "        for key in result.keys():\n",
        "            if eval_metrics in key:\n",
        "                metrics.log_metric(key, round(result[key], 5))\n",
        "\n",
        "    threshold_metric = results[0].get(threshold_metric)\n",
        "\n",
        "    component_outputs = NamedTuple(\"outputs\", threshold_metric=float)\n",
        "    return component_outputs(round(threshold_metric, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-Ccx4uLDz4N"
      },
      "source": [
        "#### Build Model Versioning component\n",
        "\n",
        "The `UploadRayModelOp` component collects the best training checkpoint and it stores the model to Vertex AI Model Registry with/without explainability configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4ZFmAe7D0M0"
      },
      "outputs": [],
      "source": [
        "@component(\n",
        "    base_image=VERTEX_RAY_IMAGE_URI,\n",
        "    packages_to_install=[\n",
        "        \"google-cloud-aiplatform[ray]\",\n",
        "        \"ray[data]==2.9.3\",\n",
        "        \"ray[data]==2.9.3\",\n",
        "        \"ray[train]==2.9.3\",\n",
        "        \"xgboost==2.0.3\",\n",
        "        \"xgboost_ray==0.1.19\",\n",
        "        \"kfp\",\n",
        "        \"google-cloud-pipeline-components\",\n",
        "        \"etils\",\n",
        "        \"importlib_resources\",\n",
        "    ],\n",
        ")\n",
        "def UploadRayModelOp(\n",
        "    training_dir: str,\n",
        "    training_run_id: str,\n",
        "    dataset_uri: str,\n",
        "    model_uri: str,\n",
        "    explain: bool,\n",
        "    project: str,\n",
        "    region: str,\n",
        "    bucket_uri: str,\n",
        "    model: Output[VertexModel],\n",
        ") -> NamedTuple(\"outputs\", model_uri=str):\n",
        "    \"\"\"Upload the best training checkpoint to Vertex AI Model Registry.\"\"\"\n",
        "\n",
        "    import logging\n",
        "    import shutil\n",
        "\n",
        "    from etils import epath\n",
        "    from google.cloud import aiplatform as vertex_ai\n",
        "    from ray.train.xgboost import XGBoostCheckpoint\n",
        "    from ray.tune import ExperimentAnalysis\n",
        "    from vertex_ray.predict import xgboost as vertex_xgboost\n",
        "\n",
        "    threshold_metric = \"valid-error\"\n",
        "    mode = \"min\"\n",
        "\n",
        "    def get_explanation_config():\n",
        "        \"\"\"A function to get explanation config.\"\"\"\n",
        "        explanation_config = {\n",
        "            \"inputs\": {},\n",
        "            \"outputs\": {},\n",
        "            \"params\": {\"sampled_shapley_attribution\": {\"path_count\": 10}},\n",
        "        }\n",
        "\n",
        "        input_names = [\n",
        "            \"trip_month\",\n",
        "            \"trip_day\",\n",
        "            \"trip_day_of_week\",\n",
        "            \"trip_hour\",\n",
        "            \"trip_seconds\",\n",
        "            \"trip_miles\",\n",
        "            \"payment_type\",\n",
        "            \"pickup_grid\",\n",
        "            \"dropoff_grid\",\n",
        "            \"euclidean\",\n",
        "            \"loc_cross\",\n",
        "        ]\n",
        "        for input_name in input_names:\n",
        "            explanation_config[\"inputs\"][input_name] = {}\n",
        "\n",
        "        explanation_config[\"outputs\"][\"predictions\"] = {}\n",
        "\n",
        "        return explanation_config\n",
        "\n",
        "    def get_model_path(checkpoint_path):\n",
        "        \"\"\"A function to copy the model.\"\"\"\n",
        "\n",
        "        model_filename = \"model.json\"\n",
        "\n",
        "        # Extract the model filename from the checkpoint path\n",
        "        local_checkpoint_dir = checkpoint_path.replace(\"/gcs/\", \"\")\n",
        "        source_model_filepath = \"/gcs/\" + Path(local_checkpoint_dir) / model_filename\n",
        "\n",
        "        # Create the model destination path\n",
        "        destination_model_path = Path.cwd() / \"model\"\n",
        "        destination_model_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Copy the model\n",
        "        destination_model_filepath = destination_model_path / model_filename\n",
        "        shutil.copy(source_model_filepath, destination_model_filepath)\n",
        "\n",
        "        return destination_model_path\n",
        "\n",
        "    # Configure logging\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
        "    )\n",
        "\n",
        "    # Initialize the Vertex AI SDK\n",
        "    logging.info(\"Initializing Vertex AI SDK.\")\n",
        "    vertex_ai.init(project=project, location=region, staging_bucket=bucket_uri)\n",
        "\n",
        "    # Restore training result\n",
        "    logging.info(\"Restoring training result.\")\n",
        "    training_dir = training_dir.replace(\"gs://\", \"/gcs/\")\n",
        "    training_path = epath.Path(training_dir)\n",
        "    training_id_path = training_path / training_run_id\n",
        "    experiment_analysis = ExperimentAnalysis(training_id_path)\n",
        "\n",
        "    # Load the best xgb_model_checkpoint\n",
        "    logging.info(\"Loading the best xgb_model_checkpoint.\")\n",
        "    log_path = experiment_analysis.get_best_trial(metric=threshold_metric, mode=mode)\n",
        "    xgb_model_checkpoint = experiment_analysis.get_best_checkpoint(\n",
        "        log_path, metric=threshold_metric, mode=mode\n",
        "    )\n",
        "\n",
        "    # Register the model\n",
        "    logging.info(\"Registering the model.\")\n",
        "    with xgb_model_checkpoint.as_directory() as local_checkpoint_dir:\n",
        "\n",
        "        destination_model_path = get_model_path(local_checkpoint_dir)\n",
        "\n",
        "        if explain:\n",
        "            explanation_config = get_explanation_config()\n",
        "\n",
        "            explanation_metadata = vertex_ai.explain.ExplanationMetadata(\n",
        "                inputs=explanation_config[\"inputs\"],\n",
        "                outputs=explanation_config[\"outputs\"],\n",
        "            )\n",
        "            explanation_parameters = vertex_ai.explain.ExplanationParameters(\n",
        "                explanation_config[\"params\"]\n",
        "            )\n",
        "\n",
        "            registered_model = vertex_xgboost.register_xgboost(\n",
        "                checkpoint=XGBoostCheckpoint.from_directory(destination_model_path),\n",
        "                artifact_uri=model_uri,\n",
        "                explanation_metadata=explanation_metadata,\n",
        "                explanation_parameters=explanation_parameters,\n",
        "                labels={\n",
        "                    \"dataset\": dataset_uri.split(\".\")[-1],\n",
        "                    \"experiment\": training_run_id,\n",
        "                },\n",
        "            )\n",
        "        else:\n",
        "            registered_model = vertex_xgboost.register_xgboost(\n",
        "                checkpoint=XGBoostCheckpoint.from_directory(destination_model_path),\n",
        "                artifact_uri=model_uri,\n",
        "                labels={\n",
        "                    \"dataset\": dataset_uri.split(\".\")[-1],\n",
        "                    \"experiment\": training_run_id,\n",
        "                },\n",
        "            )\n",
        "\n",
        "    model.uri = registered_model.uri\n",
        "\n",
        "    component_outputs = NamedTuple(\"outputs\", model_uri=str)\n",
        "    return component_outputs(registered_model.uri)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y93AFtnxNbt"
      },
      "source": [
        "### Build a ML pipeline\n",
        "\n",
        "Define your workflow using Kubeflow Pipelines DSL package by assembling components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIVIMWUNxVcO"
      },
      "outputs": [],
      "source": [
        "@kfp.dsl.pipeline(name=\"Chicago Taxi Trips Pipeline\", pipeline_root=str(PIPELINE_ROOT))\n",
        "def pipeline(\n",
        "    bq_dataset_name: str,\n",
        "    bq_dataset_source: str,\n",
        "    training_entrypoint: str,\n",
        "    training_dir: str,\n",
        "    training_run_id: str,\n",
        "    cluster_name: str,\n",
        "    runtime_env: dict,\n",
        "    threshold: float,\n",
        "    model_uri: str,\n",
        "    explain: bool,\n",
        "    project: str,\n",
        "    region: str,\n",
        "    bucket_uri: str,\n",
        "):\n",
        "\n",
        "    # get data task\n",
        "    create_tabular_dataset_task = TabularDatasetCreateOp(\n",
        "        display_name=bq_dataset_name,\n",
        "        bq_source=bq_dataset_source,\n",
        "    ).set_display_name(\"Create Vertex AI BigQuery dataset\")\n",
        "\n",
        "    get_dataset_uri_task = (\n",
        "        GetDatasetUriOp(\n",
        "            dataset=create_tabular_dataset_task.outputs[\"dataset\"],\n",
        "            project=project,\n",
        "            region=region,\n",
        "            bucket_uri=bucket_uri,\n",
        "        )\n",
        "        .set_display_name(\"Get BigQuery table\")\n",
        "        .after(create_tabular_dataset_task)\n",
        "    )\n",
        "\n",
        "    # training task\n",
        "    run_ray_training_task = (\n",
        "        VertexRayJobOp(\n",
        "            entrypoint=f\"\"\"{training_entrypoint} --bq_dataset_uri={get_dataset_uri_task.outputs['dataset_uri']} \\\n",
        "                                        --training_dir={training_dir} \\\n",
        "                                        --training_run_id={training_run_id} \\\n",
        "                                        --project_id={project} \\\n",
        "                                        --region={region} \\\n",
        "                                        --bucket_uri={bucket_uri}\"\"\",\n",
        "            cluster_name=cluster_name,\n",
        "            runtime_env=runtime_env,\n",
        "        )\n",
        "        .set_display_name(\"Run Ray Training\")\n",
        "        .after(get_dataset_uri_task)\n",
        "    )\n",
        "\n",
        "    # get eval metrics task\n",
        "    get_eval_metrics_task = (\n",
        "        GetMetricsOp(training_dir=training_dir, training_run_id=training_run_id)\n",
        "        .set_display_name(\"Get Metrics\")\n",
        "        .after(run_ray_training_task)\n",
        "    )\n",
        "\n",
        "    # evaluate condition\n",
        "    with Condition(\n",
        "        get_eval_metrics_task.outputs[\"threshold_metric\"] < threshold,\n",
        "        name=\"Blessing condition\",\n",
        "    ):\n",
        "\n",
        "        _ = (\n",
        "            UploadRayModelOp(\n",
        "                training_dir=training_dir,\n",
        "                training_run_id=training_run_id,\n",
        "                dataset_uri=get_dataset_uri_task.outputs[\"dataset_uri\"],\n",
        "                model_uri=model_uri,\n",
        "                explain=explain,\n",
        "                project=project,\n",
        "                region=region,\n",
        "                bucket_uri=bucket_uri,\n",
        "            )\n",
        "            .set_display_name(\"Upload Model\")\n",
        "            .after(get_eval_metrics_task)\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLrxk2EY2DD6"
      },
      "source": [
        "### Compile your pipeline into a YAML file\n",
        "\n",
        "Compile the pipeline into YAML format. The YAML file includes all the information for executing your pipeline on Vertex AI Pipelines.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJKAjQdK2MwE"
      },
      "outputs": [],
      "source": [
        "compiler.Compiler().compile(pipeline_func=pipeline, package_path=PIPELINE_FILE_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dD3nRWMI3nzM"
      },
      "source": [
        "### Run your pipeline\n",
        "\n",
        "You use the Vertex AI Python client to submit and run your pipeline as Vertex AI Pipeline job."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2sgZKnBLbHM"
      },
      "source": [
        "#### Set the pipeline parameters\n",
        "\n",
        "You set the following parameters to run the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FAJIqYAMLmj"
      },
      "outputs": [],
      "source": [
        "# Unique identifier for the pipeline run, generated using the `get_id()\n",
        "ID = get_id()\n",
        "\n",
        "# Path to the working directory used for the pipeline.\n",
        "WORKING_DIR = str(WORKING_URI).replace(\"gs://\", \"/gcs/\")\n",
        "\n",
        "# Specifies the runtime environment for the Ray Train job.\n",
        "RUNTIME_ENV = {\n",
        "    \"working_dir\": WORKING_DIR,\n",
        "    \"pip\": [\n",
        "        \"google-cloud-bigquery-storage\",\n",
        "        \"google-cloud-aiplatform[ray]\",\n",
        "        \"ray[data]==2.9.3\",\n",
        "        \"ray[data]==2.9.3\",\n",
        "        \"ray[train]==2.9.3\",\n",
        "        \"xgboost==2.0.3\",\n",
        "        \"xgboost_ray==0.1.19\",\n",
        "        \"etils\",\n",
        "        \"importlib_resources\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "# The name of the Vertex AI BigQuery dataset to store the training dataset.\n",
        "VERTEX_BQ_DATASET_NAME = f\"chicago_taxitrips_{ID}\"\n",
        "\n",
        "# The fully qualified BigQuery URI pointing to the data source used for training.\n",
        "VERTEX_BQ_DATASET_URI = f\"bq://{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\"\n",
        "\n",
        "# The path where training code is stored.\n",
        "TRAINING_DIR = str(ARTIFACT_STORE / \"train_jobs\")\n",
        "\n",
        "# Unique ID for the training run, generated using the `ID`.\n",
        "TRAINING_RUN_ID = f\"chicago-taxitrips-xgboost-run-{ID}\"\n",
        "\n",
        "# The percentage of data to be used for testing.\n",
        "TEST_SIZE = 0.2\n",
        "\n",
        "# Random seed for reproducibility.\n",
        "SEED = 8\n",
        "\n",
        "# Specifies the learning objective for the XGBoost model (in this case, binary classification).\n",
        "OBJECTIVE = \"binary:logistic\"\n",
        "\n",
        "# The evaluation metric used to monitor model performance during training.\n",
        "EVAL_METRIC = \"error\"\n",
        "\n",
        "# The learning rate for the XGBoost model.\n",
        "ETA = 0.01\n",
        "\n",
        "# Maximum depth of trees in the XGBoost model.\n",
        "MAX_DEPTH = 30\n",
        "\n",
        "# Fraction of the training data to be used for each tree.\n",
        "SUBSAMPLE = 0.2\n",
        "\n",
        "# Number of trees to build in the XGBoost model.\n",
        "N_ESTIMATORS = 100\n",
        "\n",
        "# Specifies whether to use a GPU for training on Ray on Vertex AI (set to `False` here).\n",
        "USE_GPU = False\n",
        "\n",
        "# The entrypoint command to execute the training script (`train.py`) on Ray on Vertex AI cluster.\n",
        "TRAINING_ENTRYPOINT = f\"\"\"python3 train.py --test_size={TEST_SIZE} --seed={SEED} \\\n",
        "                        --objective={OBJECTIVE} --eval_metric={EVAL_METRIC} \\\n",
        "                        --eta={ETA} --max_depth={MAX_DEPTH} \\\n",
        "                        --subsample={SUBSAMPLE} --n_estimators={N_ESTIMATORS} \\\n",
        "                        {'--use_gpu' if USE_GPU else '--no-use_gpu'}\"\"\"\n",
        "\n",
        "# The dashboard address of the Ray cluster where the training runs.\n",
        "CLUSTER_NAME = ray_cluster.dashboard_address\n",
        "\n",
        "# The blessing threshold on training perfomance for registering the model in Vertex AI Model Registry.\n",
        "THRESHOLD = 0.5\n",
        "\n",
        "# The location to store the trained XGBoost model in the Vertex AI artifact store on Google Cloud bucket.\n",
        "MODEL_URI = str(MODEL_STORE / f\"chicago-taxitrips-xgboost-model-{ID}\")\n",
        "\n",
        "# Flag to enable Vertex AI Prediction Explainability configuration to deploy the model.\n",
        "EXPLAIN = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s54IXmbOLij6"
      },
      "source": [
        "#### Prepare the pipeline job\n",
        "\n",
        "Initiate the pipeline job with required configuration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cyj94F4F3pLL"
      },
      "outputs": [],
      "source": [
        "parameter_values = {\n",
        "    \"bq_dataset_name\": VERTEX_BQ_DATASET_NAME,\n",
        "    \"bq_dataset_source\": VERTEX_BQ_DATASET_URI,\n",
        "    \"training_entrypoint\": TRAINING_ENTRYPOINT,\n",
        "    \"training_dir\": TRAINING_DIR,\n",
        "    \"training_run_id\": TRAINING_RUN_ID,\n",
        "    \"cluster_name\": CLUSTER_NAME,\n",
        "    \"runtime_env\": RUNTIME_ENV,\n",
        "    \"threshold\": THRESHOLD,\n",
        "    \"model_uri\": MODEL_URI,\n",
        "    \"explain\": EXPLAIN,\n",
        "    \"project\": PROJECT_ID,\n",
        "    \"region\": REGION,\n",
        "    \"bucket_uri\": BUCKET_URI,\n",
        "}\n",
        "\n",
        "pipeline_job = vertex_ai.PipelineJob(\n",
        "    display_name=\"Chicago Taxi Trips Pipeline job\",\n",
        "    template_path=PIPELINE_FILE_PATH,\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        "    parameter_values=parameter_values,\n",
        "    enable_caching=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOUWNNY-LkPv"
      },
      "source": [
        "#### Run the pipeline\n",
        "\n",
        "Run the pipeline job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWjBlUiC6e5q"
      },
      "outputs": [],
      "source": [
        "pipeline_job.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juXKMaP4ZG8s"
      },
      "source": [
        "### Get the pipeline run\n",
        "\n",
        "Use the Vertex AI Pipeline SDK to collect information about your pipeline run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vgn31iRnZSW7"
      },
      "outputs": [],
      "source": [
        "pipeline_runs_df = vertex_ai.get_pipeline_df(pipeline=\"chicago-taxi-trips-pipeline\")\n",
        "pipeline_runs_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a4e033321ad"
      },
      "source": [
        "## Cleaning up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8mCiE9yOiiJ"
      },
      "outputs": [],
      "source": [
        "delete_pipeline_job = False\n",
        "delete_ray_cluster = False\n",
        "delete_bucket = False\n",
        "\n",
        "if delete_pipeline_job:\n",
        "    pipeline_job.delete()\n",
        "\n",
        "if delete_ray_cluster:\n",
        "    vertex_ray.delete_ray_cluster(ray_cluster.cluster_resource_name)\n",
        "\n",
        "if delete_bucket:\n",
        "    !gsutil -m rm -r $BUCKET_URI"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "3_model_training_formalization_with_ray_and_kfp.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

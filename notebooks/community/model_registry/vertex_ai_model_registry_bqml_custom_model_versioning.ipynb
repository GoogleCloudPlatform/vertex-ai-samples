{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Model Versioning with Vertex AI Model Registry\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>                                                                                               \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook will show you the model versioning capabilities of Vertex AI Model Registry.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "[BBC](http://mlg.ucd.ie/datasets/bbc.html) consists of 2225 documents from the BBC news website corresponding to stories in five topical areas (business, entertainment, politics, sport, tech) from 2004-2005. Each of the articles is in a .txt file.\n",
    "\n",
    "### Objective\n",
    "\n",
    "In this tutorial, you learn how to manage your models using the Vertex AI SDK and Vertex AI Model Registry.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services and resources:\n",
    "\n",
    "- BigQuery\n",
    "- Vertex AI Training\n",
    "- Vertex AI Model Registry\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Preprocess data using SparkNLP and load them into BQML\n",
    "- Train and register a Logistic Regression using BQML\n",
    "- Train and register a Naive Bayes Classifier using Sklearn\n",
    "- Review and validate BQML and Sklearn model. \n",
    "- Nominee a champion and approve the model to production by updating aliases with `production` alias\n",
    "- Deploy the default/production version of a Model resource.\n",
    "\n",
    "### Costs \n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze4-nDLfK4pw"
   },
   "source": [
    "### Set up your local development environment\n",
    "\n",
    "**If you are using Colab or Vertex AI Workbench Notebooks**, your environment already meets\n",
    "all the requirements to run this notebook. You can skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCuSR8GkAgzl"
   },
   "source": [
    "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
    "You need the following:\n",
    "\n",
    "* The Google Cloud SDK\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* Jupyter notebook running in a virtual environment with Python 3\n",
    "\n",
    "The Google Cloud guide to [Setting up a Python development\n",
    "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
    "installation guide](https://jupyter.org/install) provide detailed instructions\n",
    "for meeting these requirements. The following steps provide a condensed set of\n",
    "instructions:\n",
    "\n",
    "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "1. [Install\n",
    "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
    "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
    "\n",
    "1. To install Jupyter, run `pip3 install jupyter` on the\n",
    "command-line in a terminal shell.\n",
    "\n",
    "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
    "\n",
    "1. Open this notebook in the Jupyter Notebook Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "### Install additional packages\n",
    "\n",
    "Install the following packages required to execute this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install --upgrade tensorflow google-cloud-bigquery google-cloud-aiplatform {USER_FLAG} -q --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzrelQZ22IZj"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. [Enable APIs](https://console.cloud.google.com/flows/enableapi?apiid=iam.googleapis.com,aiplatform.googleapis.com,cloudresourcemanager.googleapis.com,artifactregistry.googleapis.com,dataproc.googleapis.com,cloudbuild.googleapis.com)\n",
    "\n",
    "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    "\n",
    "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJYoRfYng0XZ"
   },
   "source": [
    "Otherwise, set your project ID here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"vertex-model-registry-demo01\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
    "\n",
    "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vprrvv0Ey1CU"
   },
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55gWWcI15ZFH"
   },
   "source": [
    "#### UUID\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a uuid for each instance session, and append it onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uScy6YmD5ZFI"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr--iN2kAylZ"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "**If you are using Vertex AI Workbench Notebooks**, your environment is already\n",
    "authenticated. Skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "**If you are using Colab**, run the cell below and follow the instructions\n",
    "when prompted to authenticate your account via oAuth.\n",
    "\n",
    "**Otherwise**, follow these steps:\n",
    "\n",
    "1. In the Cloud Console, go to the [**Create service account key**\n",
    "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
    "\n",
    "2. Click **Create service account**.\n",
    "\n",
    "3. In the **Service account name** field, enter a name, and\n",
    "   click **Create**.\n",
    "\n",
    "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type and select\n",
    "the following role into the filter box:\n",
    "\n",
    "    * Artifact Registry Administrator\n",
    "    * Artifact Registry Repository Administrator\n",
    "    * BigQuery Admin\n",
    "    * Compute Network Admin\n",
    "    * Cloud Build Editor\n",
    "    * Dataproc Administrator\n",
    "    * Dataproc Worker\n",
    "    * Service Account User\n",
    "    * Service Usage Admin\n",
    "    * Storage Admin\n",
    "    * Storage Object Admin\n",
    "    * Vertex AI Administrator\n",
    "\n",
    "\n",
    "5. Click *Create*. A JSON file that contains your key downloads to your\n",
    "local environment.\n",
    "\n",
    "6. Enter the path to your service account key as the\n",
    "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If on Vertex AI Workbench, then don't execute this code\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
    "    \"DL_ANACONDA_HOME\"\n",
    "):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23988890fef6"
   },
   "source": [
    "#### Get your project number\n",
    "\n",
    "Now that the project ID is set, you get your corresponding project number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2d6950574e1d"
   },
   "outputs": [],
   "source": [
    "shell_output = ! gcloud projects list --filter=\"PROJECT_ID:'{PROJECT_ID}'\" --format='value(PROJECT_NUMBER)'\n",
    "PROJECT_NUMBER = shell_output[0]\n",
    "print(\"Project Number:\", PROJECT_NUMBER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all Cloud Storage buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cf221059d072"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
    "    BUCKET_NAME = PROJECT_ID + \"-aip-\" + UUID\n",
    "    BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucvCsknMCims"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account"
   },
   "source": [
    "#### Service Account\n",
    "\n",
    "If you do not want to use your project's Compute Engine service account, set `SERVICE_ACCOUNT` to another service account ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tfyNMaIy1CW"
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_service_account"
   },
   "outputs": [],
   "source": [
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your service account from gcloud\n",
    "    if not IS_COLAB:\n",
    "        shell_output = !gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "    else:  # IS_COLAB:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account:pipelines"
   },
   "source": [
    "#### Set service account access\n",
    "\n",
    "Run the following commands to grant your service account access to the bucket that you created in the previous step. You only need to run this step once per service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RL4BUlkPy1CX"
   },
   "outputs": [],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TRRTduWyx7f"
   },
   "source": [
    "### Enabling Private Google Access for Dataproc Serverless\n",
    "\n",
    "To execute Serverless Spark workloads, the VPC subnetwork must meet the [requirements](https://cloud.google.com/dataproc-serverless/docs/concepts/network) listed in Dataproc Serverless for Spark network configuration. In this tutorial we are going to use the default one and enable it to private ip access. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d-1ZXXtTyzI5"
   },
   "outputs": [],
   "source": [
    "SUBNETWORK = \"default\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3mSeas5axzyg"
   },
   "outputs": [],
   "source": [
    "!gcloud compute networks subnets list --regions=$REGION --filter=$SUBNETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTsxWbNMyFKy"
   },
   "outputs": [],
   "source": [
    "!gcloud compute networks subnets update $SUBNETWORK \\\n",
    "--region=$REGION \\\n",
    "--enable-private-ip-google-access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VsdtU6E-yjxl"
   },
   "outputs": [],
   "source": [
    "!gcloud compute networks subnets describe $SUBNETWORK \\\n",
    "--region=$REGION \\\n",
    "--format=\"get(privateIpGoogleAccess)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7p_RIzNM02N"
   },
   "source": [
    "### Create and configure the Docker repository\n",
    "\n",
    "You create a Docker repository in the Artefact Registry for the custom dataproc image we are going to create for NLP data preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KfTW_fMeWq3e"
   },
   "outputs": [],
   "source": [
    "REPO_NAME = \"vertex-ai-model-registry-demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oyXCqeEPT-Cu"
   },
   "outputs": [],
   "source": [
    "!gcloud artifacts repositories create $REPO_NAME \\\n",
    "    --repository-format=docker \\\n",
    "    --location=$REGION \\\n",
    "    --description=\"vertex ai model registry spark docker repository\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gc1ubsMoF7wn"
   },
   "source": [
    "### Set project template\n",
    "\n",
    "You create a set of repositories to organize your project locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "420y8i4KF_z4"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"data\"\n",
    "SRC_PATH = \"src\"\n",
    "BUILD_PATH = \"build\"\n",
    "CONFIG_PATH = \"config\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHvsHGncGB-B"
   },
   "outputs": [],
   "source": [
    "!mkdir -m 777 -p $DATA_PATH $SRC_PATH $BUILD_PATH $CONFIG_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grDrP5pcGH2m"
   },
   "source": [
    "### Get input data\n",
    "\n",
    "In the following code, you will download and extrect the tutorial dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x_Nnlf0P4wnS"
   },
   "outputs": [],
   "source": [
    "RAW_DATA_URI = \"http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ML7i-NVFGLS6"
   },
   "outputs": [],
   "source": [
    "!rm -Rf {DATA_PATH}/raw \n",
    "!wget --no-parent {RAW_DATA_URI} --directory-prefix={DATA_PATH}/raw \n",
    "!unzip -qo {DATA_PATH}/raw/bbc-fulltext.zip -d {DATA_PATH}/raw && mv {DATA_PATH}/raw/bbc/* {DATA_PATH}/raw/\n",
    "!rm -Rf {DATA_PATH}/raw/bbc-fulltext.zip {DATA_PATH}/raw/bbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wf3vIGViHYo3"
   },
   "source": [
    "### Set Bigquery dataset\n",
    "\n",
    "You create the BigQuery dataset for the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ZfxQL8JHdHE"
   },
   "outputs": [],
   "source": [
    "LOCATION = REGION.split(\"-\")[0]\n",
    "BQ_DATASET = \"bcc_sport\"\n",
    "\n",
    "! bq mk --location={LOCATION} --dataset {PROJECT_ID}:{BQ_DATASET}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "# General\n",
    "import csv\n",
    "import datetime as dt\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 3000)\n",
    "\n",
    "# Model Training\n",
    "import tensorflow as tf\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XPYUKWxx70wo"
   },
   "outputs": [],
   "source": [
    "print(\"BigQuery library version:\", bigquery.__version__)\n",
    "print(\"Vertex AI library version:\", vertex_ai.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcv3P_PTWCeR"
   },
   "source": [
    "### Set up variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3tPJ5YcWEys"
   },
   "outputs": [],
   "source": [
    "# General\n",
    "STAGING_BUCKET = f\"{BUCKET_URI}/jobs\"\n",
    "RAW_PATH = os.path.join(DATA_PATH, \"raw\")\n",
    "DATAPROC_IMAGE_BUILD_PATH = os.path.join(BUILD_PATH, \"dataproc_image\")\n",
    "PREPROCESS_DOCKERFILE_PATH = os.path.join(DATAPROC_IMAGE_BUILD_PATH, \"Dockerfile\")\n",
    "DATAPROC_RUNTIME_IMAGE = \"dataproc_serverless_custom_runtime\"\n",
    "IMAGE_TAG = \"1.0.0\"\n",
    "DATAPROC_RUNTIME_CONTAINER_IMAGE = (\n",
    "    f\"gcr.io/{PROJECT_ID}/{DATAPROC_RUNTIME_IMAGE}:{IMAGE_TAG}\"\n",
    ")\n",
    "INIT_PATH = os.path.join(SRC_PATH, \"__init__.py\")\n",
    "MODULE_URI = f\"{BUCKET_URI}/{SRC_PATH}\"\n",
    "VERTEX_AI_MODEL_ID = \"text-classifier-model\"\n",
    "\n",
    "# Ingest\n",
    "PREPARED_PATH = os.path.join(DATA_PATH, \"prepared\")\n",
    "PREPARED_FILE = \"prepared_data.csv\"\n",
    "PREPARED_FILE_PATH = os.path.join(PREPARED_PATH, PREPARED_FILE)\n",
    "PREPARED_FILE_URI = f\"{BUCKET_URI}/{PREPARED_FILE_PATH}\"\n",
    "\n",
    "# Preprocess\n",
    "PREPROCESS_MODULE_PATH = os.path.join(SRC_PATH, \"preprocess.py\")\n",
    "LEMMA_DICTIONARY_PATH = os.path.join(CONFIG_PATH, \"lemmas.txt\")\n",
    "LEMMA_DICTIONARY_URI = f\"{BUCKET_URI}/{CONFIG_PATH}/lemmas.txt\"\n",
    "PROCESS_PYTHON_FILE_URI = f\"{MODULE_URI}/preprocess.py\"\n",
    "PROCESS_DATA_PATH = os.path.join(DATA_PATH, \"processed\")\n",
    "BQ_OUTPUT_TABLE_URI = f\"{BQ_DATASET}.news_processed_{UUID}\"\n",
    "PROCESS_DATA_URI = f\"{BUCKET_URI}/{PROCESS_DATA_PATH}\"\n",
    "PROCESS_FILE_URI = f\"{PROCESS_DATA_URI}/*.parquet\"\n",
    "PREPROCESS_BATCH_ID = f\"nlp-preprocess-{UUID}\"\n",
    "\n",
    "# Training\n",
    "TRAIN_NAIVE_MODULE_PATH = os.path.join(SRC_PATH, \"train_naive.py\")\n",
    "NAIVE_TRAIN_JOB_NAME = f\"naive_training_job_{UUID}\"\n",
    "TRAIN_VERSION = \"scikit-learn-cpu.0-23\"\n",
    "NAIVE_TRAIN_CONTAINER_URI = (\n",
    "    f\"{REGION.split('-')[0]}-docker.pkg.dev/vertex-ai/training/{TRAIN_VERSION}:latest\"\n",
    ")\n",
    "NAIVE_TRAIN_REQUIREMENTS = [\"pyarrow\", \"fastparquet\", \"gcsfs\"]\n",
    "DEPLOY_VERSION = \"sklearn-cpu.0-23\"\n",
    "NAIVE_DEPLOY_CONTAINER_URI = f\"{REGION.split('-')[0]}-docker.pkg.dev/vertex-ai/prediction/{DEPLOY_VERSION}:latest\"\n",
    "NAIVE_MODEL_BASE_URI = f\"{BUCKET_URI}/deliverables/naive\"\n",
    "NAIVE_MODEL_URI = f\"{BUCKET_URI}/deliverables/naive/model\"\n",
    "NAIVE_METRICS_FILE_URI = f\"{NAIVE_MODEL_URI}/metrics.json\"\n",
    "\n",
    "# Deployment\n",
    "SERVING_BUILD_PATH = os.path.join(BUILD_PATH, \"serving\")\n",
    "SERVING_APP_BUILD_PATH = os.path.join(SERVING_BUILD_PATH, \"app\")\n",
    "SERVE_NAIVE_MODULE_PATH = os.path.join(SERVING_APP_BUILD_PATH, \"main.py\")\n",
    "SERVE_REQUIREMENTS_PATH = os.path.join(SERVING_BUILD_PATH, \"requirements.txt\")\n",
    "SERVE_DOCKERFILE_PATH = os.path.join(SERVING_BUILD_PATH, \"Dockerfile\")\n",
    "SERVE_AUTH_PATH = os.path.join(SERVING_BUILD_PATH, \"key.json\")\n",
    "SERVE_SCRIPT_PATH = os.path.join(SERVING_BUILD_PATH, \"copy_model.sh\")\n",
    "SERVING_RUNTIME_IMAGE = \"serving_custom_naive\"\n",
    "IMAGE_TAG = \"1.0.0\"\n",
    "SERVING_NAIVE_RUNTIME_CONTAINER_IMAGE = (\n",
    "    f\"gcr.io/{PROJECT_ID}/{SERVING_RUNTIME_IMAGE}:{IMAGE_TAG}\"\n",
    ")\n",
    "ENDPOINT_NAME = \"text-classifier-endpoint\"\n",
    "DEPLOYED_MODEL_NAME = \"naive-bayes-text-classifier\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Python SDKs for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gb46F2fk-ZTR"
   },
   "outputs": [],
   "source": [
    "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4MN3EoDvjga"
   },
   "source": [
    "### Helpers\n",
    "\n",
    "A set of helpers to facilitate some tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1I4LpAJg78Fp"
   },
   "outputs": [],
   "source": [
    "def prepare_data(input_path: str, output_path: str, file_name: str):\n",
    "    \"\"\"\n",
    "    This function prepares the data for the model registry demo.\n",
    "    Args:\n",
    "        input_path: The directory where the raw data is stored.\n",
    "        output_path: The directory where the prepared data will be stored.\n",
    "        file_name: The name of the file to be prepared.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Read folder names\n",
    "    categories = [f.name for f in os.scandir(input_path) if f.is_dir()]\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    # Create output file\n",
    "    with open(output_path + \"/\" + file_name, \"w\") as output_file:\n",
    "        csv_writer = csv.writer(output_file)\n",
    "        csv_writer.writerow([\"category\", \"text\"])\n",
    "\n",
    "        # For each category, read all files and write to output file\n",
    "        for category in categories:\n",
    "            # Read all files in category\n",
    "            for filename in glob.glob(os.path.join(input_path, category, \"*.txt\")):\n",
    "                # Read file\n",
    "                with open(filename, \"r\") as input_file:\n",
    "                    output_text = \"\".join([line.rstrip() for line in input_file])\n",
    "                    # Write to output file\n",
    "                    csv_writer.writerow([category, output_text])\n",
    "                    input_file.close()\n",
    "\n",
    "    # Close output file\n",
    "    output_file.close()\n",
    "\n",
    "\n",
    "def run_query(query):\n",
    "\n",
    "    \"\"\"\n",
    "    This function runs a query on the prepared data.\n",
    "    Args:\n",
    "        query: The query to be run.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct a BigQuery client object.\n",
    "    client = bigquery.Client(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "    # Run the query_job\n",
    "    query_job = client.query(query)\n",
    "\n",
    "    # Wait for the query to finish\n",
    "    result = query_job.result()\n",
    "\n",
    "    # Return table\n",
    "    table = query_job.ddl_target_table\n",
    "\n",
    "    return table, result\n",
    "\n",
    "\n",
    "def read_metrics_file(metrics_file_uri):\n",
    "    \"\"\"\n",
    "    This function reads metrics file on bucket\n",
    "    Args:\n",
    "      metrics_file_uri: The uri of the metrics file\n",
    "    Returns:\n",
    "      metrics_str: metrics string\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.io.gfile.GFile(metrics_file_uri, \"r\") as metrics_file:\n",
    "        metrics = metrics_file.read().replace(\"'\", '\"')\n",
    "    metrics_file.close()\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brRJTOTQmrcp"
   },
   "source": [
    "## Data Engineering with Dataproc Serverless\n",
    "\n",
    "Before to build a NLP machine learning model, there are some common pre-processing steps to use:\n",
    "\n",
    "1.   Preliminaries such as sentence segmentation and word tokenization\n",
    "2.   Frequent steps such as stop word removal, stemming and lemmatization, removing digits/punctuation, lowercasing, etc.\n",
    "\n",
    "Other steps are normalization, language detection other than POS tagging, parsing. \n",
    "\n",
    "In the following section you will ingest your dataset and you will use SparkNLP on Dataproc serverless to build and execute a simple NLP preprocessing pipeline. To do that you need:\n",
    "\n",
    "\n",
    "\n",
    "1.   Upload data on Google Cloud Bucket\n",
    "2.   Create a custom Dataproc Serverless image\n",
    "3.   Create and upload the `preprocess` module and its dependencies on Google Cloud Bucket\n",
    "\n",
    "Then you will run the Dataproc serverless job and the resulting data will be loaded into Bigquery. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pt4-0iDRwxSl"
   },
   "source": [
    "### Ingest data\n",
    "\n",
    "Below you will\n",
    "\n",
    "1.   Prepare data by extracting news from directories and create the associated csv file.\n",
    "2.   Upload the data to Google Cloud Bucket\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmSQUovoD8_C"
   },
   "source": [
    "#### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yCSAJMkQ9X0G"
   },
   "outputs": [],
   "source": [
    "prepare_data(RAW_PATH, PREPARED_PATH, PREPARED_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oeNQhmcEAtB"
   },
   "source": [
    "##### Quick peek at the CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKkkdTmiGacU"
   },
   "outputs": [],
   "source": [
    "! head $PREPARED_FILE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdov1WbQF5L1"
   },
   "source": [
    "#### Upload the data to bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-kp6EeiAF5C"
   },
   "outputs": [],
   "source": [
    "! gsutil cp $PREPARED_FILE_PATH $PREPARED_FILE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JeAdjD55KhTY"
   },
   "source": [
    "### Basic Data and Feature Engineering \n",
    "\n",
    "In this scenario, you will use a Spark pipeline to cover the following steps using Spark NLP\n",
    "\n",
    "1. Sentence segmentation\n",
    "2. Word tokenization\n",
    "3. Normalization\n",
    "4. Stopword removal\n",
    "5. Stemming\n",
    "6. Lemmatization\n",
    "\n",
    "Finally, you will create a BOW using `CountVectorizer` object.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68nYBB5GS9TB"
   },
   "source": [
    "#### Build a custom dataproc serverless image\n",
    "\n",
    "The `DataprocPySparkBatchOp` allows you to pass custom image you would like to use when the [provided Dataproc Serverless runtime versions](https://cloud.google.com/dataproc-serverless/docs/concepts/versions/spark-runtime-versions) does not respect your requirements. \n",
    "\n",
    "In this case, an image with Spark NLP library is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8DsE4wHqI7B"
   },
   "source": [
    "##### Download the spark job dependencies\n",
    "\n",
    "You download the spark dependencies required to run the NLP preprocessing pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Wxd4fqvl8CF"
   },
   "outputs": [],
   "source": [
    "! rm -rf $DATAPROC_IMAGE_BUILD_PATH\n",
    "! mkdir $DATAPROC_IMAGE_BUILD_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4N8m-K1ldt11"
   },
   "outputs": [],
   "source": [
    "!gsutil cp gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.22.2.jar $DATAPROC_IMAGE_BUILD_PATH\n",
    "!wget -P $DATAPROC_IMAGE_BUILD_PATH https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-4.0.2.jar\n",
    "!wget -P $DATAPROC_IMAGE_BUILD_PATH https://repo.anaconda.com/miniconda/Miniconda3-py38_4.9.2-Linux-x86_64.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GF9_5IGYqLAX"
   },
   "source": [
    "##### Define the Dataproc serverless custom runtime image\n",
    "\n",
    "You define the Dockerfile to create the custom image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWiGHdEibCcv"
   },
   "outputs": [],
   "source": [
    "dataproc_serverless_custom_runtime_image = \"\"\"\n",
    "# Debian 11 is recommended.\n",
    "FROM debian:11-slim\n",
    "\n",
    "# Suppress interactive prompts\n",
    "ENV DEBIAN_FRONTEND=noninteractive\n",
    "\n",
    "# (Required) Install utilities required by Spark scripts.\n",
    "RUN apt update && apt install -y procps tini\n",
    "\n",
    "# (Optional) Add extra jars.\n",
    "ENV SPARK_EXTRA_JARS_DIR=/opt/spark/jars/\n",
    "ENV SPARK_EXTRA_CLASSPATH='/opt/spark/jars/*'\n",
    "RUN mkdir -p \"${SPARK_EXTRA_JARS_DIR}\"\n",
    "COPY spark-bigquery-with-dependencies_2.12-0.22.2.jar \"${SPARK_EXTRA_JARS_DIR}\"\n",
    "COPY spark-nlp-assembly-4.0.2.jar \"${SPARK_EXTRA_JARS_DIR}\"\n",
    "\n",
    "# (Optional) Install and configure Miniconda3.\n",
    "ENV CONDA_HOME=/opt/miniconda3\n",
    "ENV PYSPARK_PYTHON=${CONDA_HOME}/bin/python\n",
    "ENV PATH=${CONDA_HOME}/bin:${PATH}\n",
    "COPY Miniconda3-py38_4.9.2-Linux-x86_64.sh .\n",
    "RUN bash Miniconda3-py38_4.9.2-Linux-x86_64.sh -b -p /opt/miniconda3 \\\n",
    "  && ${CONDA_HOME}/bin/conda config --system --set always_yes True \\\n",
    "  && ${CONDA_HOME}/bin/conda config --system --set auto_update_conda False \\\n",
    "  && ${CONDA_HOME}/bin/conda config --system --prepend channels conda-forge \\\n",
    "  && ${CONDA_HOME}/bin/conda config --system --set channel_priority strict\n",
    "\n",
    "# (Optional) Install Conda packages.\n",
    "#\n",
    "# The following packages are installed in the default image, it is strongly\n",
    "# recommended to include all of them.\n",
    "#\n",
    "# Use mamba to install packages quickly.\n",
    "RUN ${CONDA_HOME}/bin/conda install mamba -n base -c conda-forge \\\n",
    "    && ${CONDA_HOME}/bin/mamba install \\\n",
    "      conda \\\n",
    "      cython \\\n",
    "      gcsfs \\\n",
    "      google-cloud-bigquery-storage \\\n",
    "      google-cloud-bigquery[pandas] \\\n",
    "      google-cloud-dataproc \\\n",
    "      numpy \\\n",
    "      pandas \\\n",
    "      python \\\n",
    "      pyspark \\\n",
    "      findspark\n",
    "\n",
    "# Use conda to install spark-nlp\n",
    "RUN ${CONDA_HOME}/bin/conda install -n base -c johnsnowlabs spark-nlp\n",
    "\n",
    "# Add lemma dictionary\n",
    "# ENV CONFIG_DIR='/home/app/build'\n",
    "# RUN mkdir -p \"${CONFIG_DIR}\"\n",
    "# COPY lemmas.txt \"${CONFIG_DIR}\"\n",
    "\n",
    "# (Required) Create the 'spark' group/user.\n",
    "# The GID and UID must be 1099. Home directory is required.\n",
    "RUN groupadd -g 1099 spark\n",
    "RUN useradd -u 1099 -g 1099 -d /home/spark -m spark\n",
    "USER spark\n",
    "\"\"\"\n",
    "\n",
    "with open(PREPROCESS_DOCKERFILE_PATH, \"w\") as f:\n",
    "    f.write(dataproc_serverless_custom_runtime_image)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXzI2xInqb3V"
   },
   "source": [
    "##### Build the Dataproc serverless custom runtime using Google Cloud Build\n",
    "\n",
    "You use cloud build to create and register the container image to Artefact registry. \n",
    "\n",
    "Notice that `<PROJECT_ID>@cloudbuild.gserviceaccount.com` requires to have storage.objects.get access to the Google Cloud Storage object.\n",
    "\n",
    "**Notice**: This step would take ~5min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5QT7D1LkG7L"
   },
   "outputs": [],
   "source": [
    "CLOUD_BUILD_SERVICE_ACCOUNT = f\"{PROJECT_NUMBER}@cloudbuild.gserviceaccount.com\"\n",
    "\n",
    "! gsutil iam ch serviceAccount:{CLOUD_BUILD_SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "! gsutil iam ch serviceAccount:{CLOUD_BUILD_SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RnpRqGrFkPaH"
   },
   "outputs": [],
   "source": [
    "!gcloud builds submit --tag $DATAPROC_RUNTIME_CONTAINER_IMAGE $DATAPROC_IMAGE_BUILD_PATH --machine-type=N1_HIGHCPU_32 --timeout=900s --verbosity=info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lsCPf1KwImo"
   },
   "source": [
    "#### Prepare `preprocess` module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFWE_I8VfQFO"
   },
   "source": [
    "##### Create the preprocess module\n",
    "\n",
    "This module will preprocess the data and it covers the following steps:\n",
    "\n",
    "1. Sentence segmentation\n",
    "2. Word tokenization\n",
    "3. Normalization\n",
    "4. Stopword removal\n",
    "5. Stemming\n",
    "6. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_-MYpcbr_1N"
   },
   "outputs": [],
   "source": [
    "with open(INIT_PATH, \"w\") as init_file:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9Ouh6VMr8cB"
   },
   "outputs": [],
   "source": [
    "process_module = \"\"\"\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "'''\n",
    "This is a simple module to preprocess the data for the model registry demo.\n",
    "Steps:\n",
    "1. Sentence segmentation\n",
    "2. Word tokenization\n",
    "3. Normalization\n",
    "4. Stopword removal\n",
    "5. Stemming\n",
    "6. Lemmatization\n",
    "'''\n",
    "\n",
    "# Libraries\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "from pyspark.sql.types import StructType, StringType\n",
    "from pyspark.sql.functions import col, concat_ws, rand\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Variables ------------------------------------------------------------------------------------------------------------\n",
    "DATA_SCHEMA = (StructType()\n",
    "               .add(\"category\", StringType(), True)\n",
    "               .add(\"text\", StringType(), True))\n",
    "SEED=8\n",
    "\n",
    "# Helper functions -----------------------------------------------------------------------------------------------------\n",
    "def get_logger():\n",
    "    '''\n",
    "    This function returns a logger object.\n",
    "    Returns:\n",
    "        logger: The logger object.\n",
    "    '''\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    '''\n",
    "    This function returns the arguments from the command line.\n",
    "    Returns:\n",
    "        args: The arguments from the command line.\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--input_path', type=str, help='The input path uri without bucket prefix')\n",
    "    parser.add_argument('--lemmas_path', type=str, help='The lemma dictionary path without bucket prefix')\n",
    "    parser.add_argument('--gcs_output_path', type=str, help='The gcs path for preprocessed data without bucket prefix')\n",
    "    parser.add_argument('--bq_output_table_uri', type=str, help='The Bigquery output table URI')\n",
    "    parser.add_argument('--bucket', type=str, help='The staging bucket')\n",
    "    parser.add_argument('--project', type=str, help='The project id')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def build_preliminary_steps():\n",
    "    '''\n",
    "    This function builds the preliminary steps for the preprocessing.\n",
    "    Returns:\n",
    "        preliminary_steps: The preliminary steps for the preprocessing.\n",
    "    '''\n",
    "\n",
    "    document_assembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\").setCleanupMode('shrink_full')\n",
    "    sentence_detector = SentenceDetector().setInputCols(\"document\").setOutputCol(\"sentence\")\n",
    "    tokenizer = Tokenizer().setInputCols(\"sentence\").setOutputCol(\"token\")\n",
    "    preliminary_steps = [document_assembler, sentence_detector, tokenizer]\n",
    "    return preliminary_steps\n",
    "\n",
    "\n",
    "def build_common_preprocess_steps(lemma_uri):\n",
    "    '''\n",
    "    This function builds the common preprocessing steps.\n",
    "    Args:\n",
    "        lemma_uri: The uri of lemma dictionary\n",
    "    Returns:\n",
    "        common_preprocess_steps: The common preprocessing steps.\n",
    "    '''\n",
    "\n",
    "    normalizer = Normalizer().setInputCols(\"token\").setOutputCol(\"normalized_token\").setLowercase(True)\n",
    "    stopwords_cleaner = StopWordsCleaner().setInputCols(\"normalized_token\").setOutputCol(\n",
    "        \"cleaned_tokens\").setCaseSensitive(False)\n",
    "    stemmer = Stemmer().setInputCols(\"cleaned_tokens\").setOutputCol(\"stem\")\n",
    "    lemmatizer = Lemmatizer().setInputCols(\"stem\").setOutputCol(\"lemma\").setDictionary(lemma_uri, \"->\", \"\\t\")\n",
    "    finisher = Finisher().setInputCols(\"lemma\").setOutputCols([\"lemma_features\"]).setIncludeMetadata(\n",
    "        False).setOutputAsArray(True)\n",
    "    common_preprocess_steps = [normalizer, stopwords_cleaner, stemmer, lemmatizer, finisher]\n",
    "    return common_preprocess_steps\n",
    "\n",
    "\n",
    "def build_feature_extraction_steps():\n",
    "    '''\n",
    "    This function builds the feature extraction steps.\n",
    "    Returns:\n",
    "        feature_extraction_steps: The feature extraction steps.\n",
    "    '''\n",
    "\n",
    "    count_vectorizer = CountVectorizer().setInputCol(\"lemma_features\").setOutputCol(\"features\").setVocabSize(30)\n",
    "    feature_extraction_steps = [count_vectorizer]\n",
    "    return feature_extraction_steps\n",
    "\n",
    "\n",
    "def read_data(spark_session, data_schema, input_dir):\n",
    "    '''\n",
    "    This function reads the data from the input directory.\n",
    "    Args:\n",
    "        spark_session: The SparkSession object.\n",
    "        data_schema: The data schema.\n",
    "        input_dir: The input directory.\n",
    "    Returns:\n",
    "        raw_df: The raw dataframe.\n",
    "    '''\n",
    "\n",
    "    raw_df = (spark_session.read.option(\"header\", True)\n",
    "              .option(\"delimiter\", ',')\n",
    "              .schema(data_schema)\n",
    "              .csv(input_dir))\n",
    "    return raw_df\n",
    "\n",
    "\n",
    "def prepare_train_df(df):\n",
    "    '''\n",
    "    This function prepares the training dataframe.\n",
    "    Args:\n",
    "        df: The dataframe.\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    train_df = (df.withColumn(\"bow_col\", vector_to_array(\"features\"))\n",
    "                .withColumn(\"lemmas\", concat_ws(\" \", col(\"lemma_features\")))\n",
    "                .select([\"text\"] + [\"lemmas\"] + [col(\"bow_col\")[i] for i in range(30)] + [\"category\"]))\n",
    "\n",
    "    return train_df\n",
    "\n",
    "\n",
    "def save_data(data, bucket, gcs_path, bigquery_uri):\n",
    "    '''\n",
    "    This function saves the data to Bigquery.\n",
    "    Args:\n",
    "        data: The data to save.\n",
    "        bucket: The bucket.\n",
    "        gcs_path: The path to store processed data.\n",
    "        bigquery_uri: The URI of the Bigquery table.\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    # df_sample = data.sample(withReplacement=False, fraction=0.7, seed=SEED)\n",
    "    df_sample = data.orderBy(rand(SEED)).limit(1000)\n",
    "    df_sample.write.format('bigquery') \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"persistentGcsBucket\", bucket) \\\n",
    "        .option(\"persistentGcsPath\", gcs_path) \\\n",
    "        .save(bigquery_uri)\n",
    "\n",
    "\n",
    "# Main function --------------------------------------------------------------------------------------------------------\n",
    "def preprocess(args):\n",
    "    '''\n",
    "    preprocess function.\n",
    "    Args:\n",
    "        args: The arguments from the command line.\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    # Get logger\n",
    "    logger = get_logger()\n",
    "\n",
    "    # Initialize variables\n",
    "    input_path = args.input_path\n",
    "    lemma_path = args.lemmas_path\n",
    "    gcs_output_path = args.gcs_output_path\n",
    "    bq_output_table_uri = args.bq_output_table_uri\n",
    "    bucket = args.bucket\n",
    "    project = args.project\n",
    "    lemma_uri = f'gs://{bucket}/{lemma_path}'\n",
    "    input_uri = f'gs://{bucket}/{input_path}'\n",
    "\n",
    "    # Initialize SparkSession\n",
    "    logger.info('Starting preprocessing')\n",
    "    spark = sparknlp.start()\n",
    "    print(f\"Spark NLP version: {sparknlp.version()}\")\n",
    "    print(f\"Spark version: {spark.version}\")\n",
    "\n",
    "    # Build pipeline steps\n",
    "    logger.info('Building pipeline steps')\n",
    "    preliminary_steps = build_preliminary_steps()\n",
    "    common_preprocess_steps = build_common_preprocess_steps(lemma_uri)\n",
    "    feature_extraction_steps = build_feature_extraction_steps()\n",
    "    pipeline = Pipeline(stages=preliminary_steps + common_preprocess_steps + feature_extraction_steps)\n",
    "\n",
    "    # Read data\n",
    "    logger.info('Reading data')\n",
    "    raw_df = read_data(spark, DATA_SCHEMA, input_uri)\n",
    "\n",
    "    # Preprocess data\n",
    "    logger.info('Preprocessing data')\n",
    "    processed_pipeline = pipeline.fit(raw_df)\n",
    "    preprocessed_df = processed_pipeline.transform(raw_df)\n",
    "    preprocessed_df.show(10, truncate=False)\n",
    "\n",
    "    # Save data to Bigquery\n",
    "    logger.info('Saving data to Bigquery')\n",
    "    train_df = prepare_train_df(preprocessed_df)\n",
    "    save_data(train_df, bucket, gcs_output_path, bq_output_table_uri)\n",
    "    logging.info('done.')\n",
    "    spark.stop()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Get args\n",
    "    args = get_args()\n",
    "    preprocess(args)\n",
    "\"\"\"\n",
    "\n",
    "with open(PREPROCESS_MODULE_PATH, \"w\") as process_file:\n",
    "    process_file.write(process_module)\n",
    "process_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8Ic6_s3wPUC"
   },
   "source": [
    "##### Upload the module on bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SpLfaU6Xw4oF"
   },
   "outputs": [],
   "source": [
    "!gsutil cp $SRC_PATH/__init__.py $MODULE_URI/__init__.py\n",
    "!gsutil cp $SRC_PATH/preprocess.py $MODULE_URI/preprocess.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1K3F7QidiCd"
   },
   "source": [
    "##### Upload config file\n",
    "\n",
    "You use the lemma dictionary according to Spark NLP documentation and you will upload it to Google Cloud bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vxZuoy0dlfS"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/mahavivo/vocabulary/master/lemmas/AntBNC_lemmas_ver_001.txt -O $LEMMA_DICTIONARY_PATH\n",
    "!gsutil cp $LEMMA_DICTIONARY_PATH $LEMMA_DICTIONARY_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQVJBaXTz4sV"
   },
   "source": [
    "#### Run a preprocess spark job using dataproc serverless\n",
    "\n",
    "Now that you prepare the execution, you can sumbit the preprocessing Dataproc Serverless job. The explanation of this cli command is out of scope but you can have a look of all its option [in the official documentation](https://cloud.google.com/dataproc-serverless/docs/quickstarts/spark-batch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8fbik13otuQ7"
   },
   "outputs": [],
   "source": [
    "! gcloud beta dataproc batches submit pyspark $PROCESS_PYTHON_FILE_URI \\\n",
    "  --batch=$PREPROCESS_BATCH_ID \\\n",
    "  --container-image=$DATAPROC_RUNTIME_CONTAINER_IMAGE \\\n",
    "  --region=$REGION \\\n",
    "  --subnet='default' \\\n",
    "  --properties spark.executor.instances=2,spark.driver.cores=4,spark.executor.cores=4,spark.app.name=spark_preprocessing_job \\\n",
    "  -- --input_path=$PREPARED_FILE_PATH --lemmas_path=$LEMMA_DICTIONARY_PATH --gcs_output_path=$PROCESS_DATA_PATH --bq_output_table_uri=$BQ_OUTPUT_TABLE_URI --bucket=$BUCKET_NAME --project=$PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhMnSORpPkO0"
   },
   "source": [
    "## Model Training for Text Classification\n",
    "\n",
    "According to [Practical Natural Language Processing: A Comprehensive Guide to Building Real-World NLP Systems](https://www.oreilly.com/library/view/practical-natural-language/9781492054047/), there are different approaches to train text classifiers. For instance, you have\n",
    "\n",
    "- Traditional methods such Logistic Regression or Naive Bayes Classifier\n",
    "- Neural Embeddings methods\n",
    "- Deep Learning methods\n",
    "- Large, Pre-trained Language Models\n",
    "\n",
    "In the following section you are going to use the traditional methods and you will show how Vertex AI Model Registry will govern all of them. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGnLq4PEjOuT"
   },
   "source": [
    "#### Logistic Regression using BQML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJv2kKf4SGca"
   },
   "source": [
    "##### Train and register the model\n",
    "\n",
    "To register a BigQuery ML model to Vertex AI Model Registry, you must use `model_registry=\"vertex_ai\"`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FaDx8SdGSU3V"
   },
   "outputs": [],
   "source": [
    "train_lr_query = f\"\"\"\n",
    "CREATE OR REPLACE MODEL\n",
    "  `{PROJECT_ID}.{BQ_DATASET}.text_logit_classifier`\n",
    "OPTIONS\n",
    "  ( MODEL_TYPE='LOGISTIC_REG',\n",
    "    AUTO_CLASS_WEIGHTS=TRUE,\n",
    "    DATA_SPLIT_METHOD='RANDOM',\n",
    "    DATA_SPLIT_EVAL_FRACTION = .10,\n",
    "    INPUT_LABEL_COLS=['category'],\n",
    "    ENABLE_GLOBAL_EXPLAIN=TRUE,\n",
    "    MODEL_REGISTRY='vertex_ai',\n",
    "    VERTEX_AI_MODEL_ID='{VERTEX_AI_MODEL_ID}',\n",
    "    VERTEX_AI_MODEL_VERSION_ALIASES=['experimental', 'baseline', 'BQML', 'logistic_regression']\n",
    "  ) AS\n",
    "    SELECT * EXCEPT(text, lemmas)\n",
    "    FROM `{PROJECT_ID}.{BQ_OUTPUT_TABLE_URI}`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6DJtOe1uUtgm"
   },
   "outputs": [],
   "source": [
    "model_table, result = run_query(query=train_lr_query)\n",
    "print(f\"The {model_table.dataset_id}.{model_table.table_id} successfully created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6hcXNGITTym"
   },
   "source": [
    "#### Naive Bayes Classifier with Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBHgAA0xqyi-"
   },
   "source": [
    "###### Create naive training module\n",
    "\n",
    "With this module, you will train a simple Sklearn Naive Bayes estimator for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NuFd9fJAq4FV"
   },
   "outputs": [],
   "source": [
    "train_naive_module = \"\"\"\n",
    "#!/usr/bin/env python3\n",
    "'''\n",
    "This is a simple module to train a naive bayes model.\n",
    "'''\n",
    "\n",
    "import logging\n",
    "import argparse\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, log_loss, roc_auc_score\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "# Variables\n",
    "RANDOM_STATE = 8\n",
    "TEST_SIZE = 0.2\n",
    "EVAL_SIZE = 0.25\n",
    "\n",
    "\n",
    "\n",
    "# Helpers --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def get_logger():\n",
    "    '''\n",
    "    This function returns a logger object.\n",
    "    Returns:\n",
    "        logger: The logger object.\n",
    "    '''\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    '''\n",
    "    This function parses and return arguments passed in command line.\n",
    "    Returns:\n",
    "        args: Arguments list.\n",
    "    '''\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data_path\",\n",
    "                        type=str, help=\"The path of the training data.\")\n",
    "    parser.add_argument('--model_dir',\n",
    "                        type=str, help='The path of the model directory.')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def read_data(data_path: str):\n",
    "    '''\n",
    "    This function reads the data from the provided data path.\n",
    "    Args:\n",
    "        data_path: The path of the data.\n",
    "    Returns:\n",
    "        x_train: The training data.\n",
    "        y_train: The training labels.\n",
    "        x_test: The test data.\n",
    "        y_test: The test labels.\n",
    "    '''\n",
    "    # Read data\n",
    "    gs_prefix = 'gs://'\n",
    "    gcsfuse_prefix = '/gcs/'\n",
    "    if data_path.startswith(gs_prefix):\n",
    "        data_path = data_path.replace(gs_prefix, gcsfuse_prefix)\n",
    "    parquet_files = glob.glob(data_path)\n",
    "    dataframes = []\n",
    "    for parquet_file_path in parquet_files:\n",
    "        parquet_file_path = parquet_file_path.replace(gcsfuse_prefix, gs_prefix)\n",
    "        dataframes.append(pd.read_parquet(parquet_file_path, engine='fastparquet'))\n",
    "    df = pd.concat(dataframes, axis=0)\n",
    "    x = df.text\n",
    "    # y = np.where(df.category == 'sport', 1, 0)\n",
    "    y = df.category\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=RANDOM_STATE, test_size=TEST_SIZE)\n",
    "    x_train, x_eval, y_train, y_eval = train_test_split(x_train, y_train, random_state=RANDOM_STATE, test_size=EVAL_SIZE)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def get_weights(y_train):\n",
    "    '''\n",
    "    This function returns the class weights for the model.\n",
    "    Returns:\n",
    "        weights: The class weights.\n",
    "    '''\n",
    "    weights = compute_sample_weight('balanced', y_train)\n",
    "    return weights\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    '''\n",
    "    This function builds the model.\n",
    "    Returns:\n",
    "        model: The model.\n",
    "    '''\n",
    "    model = Pipeline([\n",
    "        ('count_vectorizer', CountVectorizer()),\n",
    "        ('classifier', MultinomialNB())\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(x_train, y_train, model):\n",
    "    '''\n",
    "    This function trains the model.\n",
    "    Args:\n",
    "        x_train: The training data.\n",
    "        y_train: The training labels.\n",
    "        model: The model to train.\n",
    "    Returns:\n",
    "        model: The trained model.\n",
    "    '''\n",
    "    model = model.fit(x_train, y_train, classifier__sample_weight=get_weights(y_train))\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, x_test, y_test):\n",
    "    '''\n",
    "    This function evaluates the model on the test data.\n",
    "    Parameters:\n",
    "        model: The model to evaluate.\n",
    "        x_test: The test data.\n",
    "        y_test: The test labels.\n",
    "    '''\n",
    "\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_proba = model.predict_proba(x_test)\n",
    "    metrics = {\n",
    "        \"precision\": round(precision_score(y_test, y_pred, sample_weight=get_weights(y_test), average=\"weighted\"), 5),\n",
    "        \"recall\": round(recall_score(y_test, y_pred, sample_weight=get_weights(y_test), average=\"weighted\"), 5),\n",
    "        \"accuracy\": round(accuracy_score(y_test, y_pred, sample_weight=get_weights(y_test)), 5),\n",
    "        \"f1_score\": round(f1_score(y_test, y_pred, sample_weight=get_weights(y_test), average=\"weighted\"), 5),\n",
    "        \"log_loss\": round(log_loss(y_test, y_pred_proba, sample_weight=get_weights(y_test)), 5),\n",
    "        \"roc_auc\": round(roc_auc_score(y_test, y_pred_proba, multi_class='ovr'), 5)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def save_model(model, model_dir):\n",
    "    '''\n",
    "    This function saves the model to the provided model directory.\n",
    "    Parameters:\n",
    "        model: The model to save.\n",
    "        model_dir: The directory to save the model to.\n",
    "    '''\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    gs_prefix = 'gs://'\n",
    "    gcsfuse_prefix = '/gcs/'\n",
    "    if model_dir.startswith(gs_prefix):\n",
    "        model_dir = model_dir.replace(gs_prefix, gcsfuse_prefix)\n",
    "    model_dir = os.path.join(model_dir, 'model')\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    model_path = os.path.join(model_dir, 'model.pkl')\n",
    "    with open(model_path, 'wb') as model_file:\n",
    "        pickle.dump(model, model_file)\n",
    "\n",
    "def save_metrics(metrics, model_dir):\n",
    "    '''\n",
    "    This function saves the metrics to the provided model directory.\n",
    "    Parameters:\n",
    "        metrics: The metrics to save.\n",
    "        model_dir: The directory to save the metrics to.\n",
    "    '''\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    gs_prefix = 'gs://'\n",
    "    gcsfuse_prefix = '/gcs/'\n",
    "    if model_dir.startswith(gs_prefix):\n",
    "        model_dir = model_dir.replace(gs_prefix, gcsfuse_prefix)\n",
    "    metrics_path = os.path.join(model_dir, 'model', 'metrics.json')\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        f.write(str(metrics))\n",
    "\n",
    "\n",
    "def train_naive(args):\n",
    "    '''\n",
    "    This function trains the model and saves it to the provided model directory.\n",
    "    Parameters:\n",
    "        args: The arguments from the command line.\n",
    "    '''\n",
    "    # Get logger\n",
    "    logger = get_logger()\n",
    "    logger.info('Starting model training...')\n",
    "\n",
    "    # Initialize variables\n",
    "    data_path = args.data_path\n",
    "    model_dir = args.model_dir\n",
    "\n",
    "    # Build model\n",
    "    model = build_model()\n",
    "\n",
    "    # Read data\n",
    "    logger.info('Reading data')\n",
    "    x_train, y_train, x_test, y_test = read_data(data_path)\n",
    "\n",
    "    # Train model\n",
    "    logger.info('Training model')\n",
    "    model = train_model(x_train, y_train, model)\n",
    "\n",
    "    # Evaluate model\n",
    "    logger.info('Evaluating model')\n",
    "    metrics = evaluate_model(model, x_test, y_test)\n",
    "    for key, value in metrics.items():\n",
    "        print(f'{key}: {value}')\n",
    "\n",
    "    # Save model\n",
    "    logger.info('Saving model')\n",
    "    save_model(model, model_dir)\n",
    "\n",
    "    # Save metrics\n",
    "    logger.info('Saving metrics')\n",
    "    save_metrics(metrics, model_dir)\n",
    "\n",
    "    logger.info('Training complete.')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Get args\n",
    "    args = get_args()\n",
    "    train_naive(args)\n",
    "\"\"\"\n",
    "\n",
    "with open(TRAIN_NAIVE_MODULE_PATH, \"w\") as train_naive_file:\n",
    "    train_naive_file.write(train_naive_module)\n",
    "train_naive_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21VTM2N_1Qtk"
   },
   "source": [
    "##### Train and register the model using Vertex AI Training\n",
    "\n",
    "To register a new custom model versioned trained using Vertex AI Training of an existing model, you will provide those additional arguments:\n",
    "\n",
    "*   `parent_model`: The parent resource name of an existing model to register a new version. \n",
    "*   `model_version_aliases`: The aliases of the model version to create.\n",
    "*   `model_version_description`: The description of the model version.\n",
    "*   `is_default_version`: Whether the model version is the default version.\n",
    "\n",
    "Once you run the training job, it would take **~5 min** to finish it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JhG57TLm1lUV"
   },
   "outputs": [],
   "source": [
    "naive_bayes_train_job = vertex_ai.CustomTrainingJob(\n",
    "    display_name=NAIVE_TRAIN_JOB_NAME,\n",
    "    script_path=TRAIN_NAIVE_MODULE_PATH,\n",
    "    container_uri=NAIVE_TRAIN_CONTAINER_URI,\n",
    "    requirements=NAIVE_TRAIN_REQUIREMENTS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tuEyTkim-GNq"
   },
   "outputs": [],
   "source": [
    "naive_model = naive_bayes_train_job.run(\n",
    "    args=[\"--data_path\", PROCESS_FILE_URI, \"--model_dir\", NAIVE_MODEL_BASE_URI],\n",
    "    replica_count=1,\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    base_output_dir=NAIVE_MODEL_BASE_URI,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgsDPyfG-zjP"
   },
   "source": [
    "## Model Governance with Vertex AI Model Registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oV9FimH-vrps"
   },
   "source": [
    "#### Initialize Vertex AI Model Registry\n",
    "\n",
    "To access different model versions of a Vertex AI Model resource, you can initialize a model registry instance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O32IGV9tgXw8"
   },
   "outputs": [],
   "source": [
    "registry = vertex_ai.models.ModelRegistry(VERTEX_AI_MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11xaykBni5mJ"
   },
   "source": [
    "#### Compare model versions\n",
    "\n",
    "Then, you use `ML.EVALUATE` to generate BQML model evaluation metrics and compare them to the same metrics you created with your custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9o0phVemTU0i"
   },
   "outputs": [],
   "source": [
    "evaluation_query = f\"\"\"\n",
    "SELECT *\n",
    "FROM\n",
    "  ML.EVALUATE(MODEL `{BQ_DATASET}.text_logit_classifier`)\n",
    "ORDER BY  roc_auc desc\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "_, result = run_query(query=evaluation_query)\n",
    "evaluation_df = result.to_dataframe().rename(index={0: \"bqml_text_logit_classifier\"})\n",
    "evaluation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o1H0e8c3VpM8"
   },
   "outputs": [],
   "source": [
    "naive_metrics = read_metrics_file(NAIVE_METRICS_FILE_URI)\n",
    "metrics_dict = [json.loads(naive_metrics)]\n",
    "naive_metrics_df = pd.DataFrame.from_dict(metrics_dict).rename(index={0: \"naive_bayes\"})\n",
    "evaluation_df = evaluation_df.append(naive_metrics_df, ignore_index=False)\n",
    "evaluation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nr1xRQbcm4i"
   },
   "source": [
    "### Register the `champion` model version\n",
    "\n",
    "Based on the model evalutions, the SkLearn Naive Bayes Classifier outperformed the BQML Logistic regression and it is the production candidate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWisrY3svDYA"
   },
   "source": [
    "##### Build and push the custom serving container to Artefact Registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdzX9WW4lNWK"
   },
   "source": [
    "###### Build the custom serving image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JP8MQJkenIz2"
   },
   "outputs": [],
   "source": [
    "! rm -rf $SERVING_BUILD_PATH\n",
    "! mkdir $SERVING_BUILD_PATH\n",
    "! mkdir $SERVING_APP_BUILD_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsqCwvWApgUC"
   },
   "source": [
    "######Build the serving app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wao9muHfprDL"
   },
   "outputs": [],
   "source": [
    "serve_naive_module = \"\"\"\n",
    "'''\n",
    "This is a simple web application to serve the naive bayes model.\n",
    "'''\n",
    "\n",
    "# Libraries ------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from flask import Flask, Response, request, jsonify\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Helpers --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def get_probabilities(model_classes, probabilities):\n",
    "    proba_classes = []\n",
    "    for probabilities_list in probabilities:\n",
    "      proba_classes.append({\"classes\": model_classes, \"scores\": probabilities_list})\n",
    "    return proba_classes\n",
    "\n",
    "\n",
    "# App ------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Initialize the app\n",
    "app = Flask(__name__)\n",
    "app.logger.setLevel(logging.INFO)\n",
    "\n",
    "# Load the model\n",
    "app.logger.info(\"Loading model...\")\n",
    "model = pickle.load(open('./model/model.pkl', 'rb'))\n",
    "app.logger.info(\"Model loaded.\")\n",
    "\n",
    "# classes = model.classes_\n",
    "classes = model.classes_.tolist()\n",
    "\n",
    "\n",
    "@app.route(os.environ['AIP_HEALTH_ROUTE'], methods=['GET'])\n",
    "def health():\n",
    "    '''\n",
    "    A health check endpoint.\n",
    "    '''\n",
    "    app.logger.info(\"Health check\")\n",
    "    return Response(response='OK', status=200)\n",
    "\n",
    "\n",
    "@app.route(os.environ['AIP_PREDICT_ROUTE'], methods=['POST'])\n",
    "def predict():\n",
    "    '''\n",
    "    A predict endpoint.\n",
    "    '''\n",
    "    app.logger.info(\"Predict\")\n",
    "\n",
    "    # Get instances\n",
    "    instances_dict = request.get_json()[\"instances\"]\n",
    "\n",
    "    # Generate predictions\n",
    "    instances_df = pd.DataFrame.from_records(instances_dict)\n",
    "    probabilities = model.predict_proba(instances_df.iloc[:, 0])\n",
    "\n",
    "    # Format predictions\n",
    "    fmt_probabilities = get_probabilities(classes, probabilities.tolist())\n",
    "\n",
    "    return jsonify({\"predictions\": fmt_probabilities})\n",
    "\n",
    "\n",
    "if __name__ == \"main\":\n",
    "    app.run(debug=True, host=\"0.0.0.0\", port=9999)\n",
    "\"\"\"\n",
    "\n",
    "with open(SERVE_NAIVE_MODULE_PATH, \"w\") as serve_naive_file:\n",
    "    serve_naive_file.write(serve_naive_module)\n",
    "serve_naive_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDM7-7KZn9MS"
   },
   "source": [
    "###### Copy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQezoOQMoCEH"
   },
   "outputs": [],
   "source": [
    "!gsutil cp -r $NAIVE_MODEL_URI $SERVING_APP_BUILD_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0HIQD70qqDn"
   },
   "source": [
    "###### Create `requirements` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9tJdqu1qtV7"
   },
   "outputs": [],
   "source": [
    "serve_requirements = \"\"\"\n",
    "flask==2.2.2\n",
    "gunicorn==20.1.0\n",
    "numpy==1.22.4\n",
    "pandas==1.4.3\n",
    "scikit-learn==0.23.1\n",
    "\"\"\"\n",
    "\n",
    "with open(SERVE_REQUIREMENTS_PATH, \"w\") as serve_requirements_file:\n",
    "    serve_requirements_file.write(serve_requirements)\n",
    "serve_requirements_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XihT5MpErihk"
   },
   "source": [
    "###### Create `Dockerfile` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sj98z12qrubZ"
   },
   "outputs": [],
   "source": [
    "serve_dockerfile = \"\"\"\n",
    "FROM python:3.8-slim\n",
    "\n",
    "# Update pip\n",
    "RUN pip3 install --upgrade pip\n",
    "\n",
    "# Install requirements\n",
    "COPY requirements.txt requirements.txt\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "# Create app folder and copy app files\n",
    "RUN mkdir /app\n",
    "COPY app /app\n",
    "WORKDIR /app\n",
    "\n",
    "# Run app\n",
    "EXPOSE 9999\n",
    "CMD [\"gunicorn\", \"main:app\", \"--timeout=0\", \"--preload\", \\\n",
    "     \"--workers=1\", \"--threads=4\", \"--bind=0.0.0.0:9999\"]\n",
    "\"\"\"\n",
    "\n",
    "with open(SERVE_DOCKERFILE_PATH, \"w\") as serve_dockerfile_file:\n",
    "    serve_dockerfile_file.write(serve_dockerfile)\n",
    "serve_dockerfile_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQ5jp2FSvSK7"
   },
   "source": [
    "###### Build and push the custom image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qAAkoXcHvVVY"
   },
   "outputs": [],
   "source": [
    "!gcloud builds submit --tag $SERVING_NAIVE_RUNTIME_CONTAINER_IMAGE $SERVING_BUILD_PATH --machine-type=N1_HIGHCPU_32 --timeout=900s --verbosity=info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmY1U5S9g7Li"
   },
   "source": [
    "##### Register the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QagCSDjXqFgV"
   },
   "outputs": [],
   "source": [
    "naive_model = vertex_ai.Model.upload(\n",
    "    parent_model=VERTEX_AI_MODEL_ID,\n",
    "    is_default_version=False,\n",
    "    version_aliases=[\"experimental\", \"challenger\", \"custom-training\", \"naive-bayes\"],\n",
    "    version_description=\"A Naive Bayes text classifier\",\n",
    "    serving_container_image_uri=SERVING_NAIVE_RUNTIME_CONTAINER_IMAGE,\n",
    "    serving_container_health_route=\"/health\",\n",
    "    serving_container_predict_route=\"/predict\",\n",
    "    serving_container_ports=[9999],\n",
    "    labels={\"created_by\": \"inardini\", \"team\": \"advocacy\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mveSg_Xtdpcq"
   },
   "source": [
    "### Listing versions of a model\n",
    "\n",
    "You can list all the model versions using `list_versions` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-LFYv8gdumh"
   },
   "outputs": [],
   "source": [
    "versions = registry.list_versions()\n",
    "for version in versions:\n",
    "    version_id = version.version_id\n",
    "    version_created_time = dt.datetime.fromtimestamp(\n",
    "        version.version_create_time.timestamp()\n",
    "    ).strftime(\"%m/%d/%Y %H:%M:%S\")\n",
    "    version_aliases = version.version_aliases\n",
    "    print(\n",
    "        \"\\n\",\n",
    "        f\"Model version {version_id} was created at {version_created_time} with aliases {version_aliases}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyVxqeebhVX0"
   },
   "source": [
    "###Getting all information about the `champion` model version\n",
    "\n",
    "To get all information about your `champion` model you can use `get_version_info` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YlTFBPJy0m5u"
   },
   "outputs": [],
   "source": [
    "CHAMPION_VERSION_ID = versions[-1].version_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KGaLLbt9hYNC"
   },
   "outputs": [],
   "source": [
    "champion_model_version_info = registry.get_version_info(CHAMPION_VERSION_ID)\n",
    "champion_model_version_info_df = pd.DataFrame(\n",
    "    champion_model_version_info,\n",
    "    columns=[\"model_version\"],\n",
    "    index=[\n",
    "        \"version_id\",\n",
    "        \"created_at\",\n",
    "        \"updated_at\",\n",
    "        \"model_display_name\",\n",
    "        \"model_resource_name\",\n",
    "        \"version_aliases\",\n",
    "        \"version_description\",\n",
    "    ],\n",
    ")\n",
    "champion_model_version_info_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etK2i-NA8rtK"
   },
   "source": [
    "### Set the champion model ready to `production` as `default` \n",
    "\n",
    "To update the aliases and change the state of the model from `experimental` to `production`, the Vertex AI SDK provides `add_version_aliases` and `remove_version_aliases` methods. \n",
    "\n",
    "Notice we set those aliases in respect of the online experimention phase discussed in the the [Practitioners guide to MLOps:\n",
    "A framework for continuous\n",
    "delivery and automation of\n",
    "machine learning.](https://services.google.com/fh/files/misc/practitioners_guide_to_mlops_whitepaper.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VniG9U_Q81R6"
   },
   "outputs": [],
   "source": [
    "registry.remove_version_aliases(\n",
    "    [\"experimental\", \"challenger\"], version=CHAMPION_VERSION_ID\n",
    ")\n",
    "registry.add_version_aliases([\"default\", \"production\"], version=CHAMPION_VERSION_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tiKo0Jk_axs"
   },
   "source": [
    "### Deploy the `champion` model\n",
    "\n",
    "Finally, you initiate the champion model ready to production and you deploy it to a Vertex AI Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8qojYvUbComG"
   },
   "outputs": [],
   "source": [
    "champion_model = registry.get_model(version=\"production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwErIIsOGMzk"
   },
   "source": [
    "#### Create the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5LWhrPXfGMGu"
   },
   "outputs": [],
   "source": [
    "endpoint = vertex_ai.Endpoint.create(\n",
    "    display_name=ENDPOINT_NAME,\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqhjQSrMKGyl"
   },
   "source": [
    "####Deploy the champion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EvMabDk0GxgV"
   },
   "outputs": [],
   "source": [
    "endpoint.deploy(\n",
    "    model=champion_model,\n",
    "    deployed_model_display_name=DEPLOYED_MODEL_NAME,\n",
    "    machine_type=\"n1-standard-8\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a53QDmCWKnQD"
   },
   "source": [
    "###Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0a3QkYvyHp5s"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"Rihanna to headline 2023 Super Bowl halftime show: 'It's on\"\"\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_Q47a6R9PPr"
   },
   "outputs": [],
   "source": [
    "instances = [{\"text\": text}]\n",
    "predictions = endpoint.predict(instances)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dksu_4joK2Hv"
   },
   "source": [
    "### Final thoughts \n",
    "\n",
    "As you can imagine you can also upload external models. Have a look at the documentation sample and the [sample notebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/get_started_with_model_registry.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t3GoM5NIrMML"
   },
   "outputs": [],
   "source": [
    "endpoint.undeploy_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KVl1YBPosAFf"
   },
   "outputs": [],
   "source": [
    "endpoint.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XgsfvmPNG00G"
   },
   "outputs": [],
   "source": [
    "drop_model_query = f\"DROP MODEL `{PROJECT_ID}.{BQ_DATASET}.text_logit_classifier`\"\n",
    "run_query(drop_model_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grvrV_Eal8x6"
   },
   "outputs": [],
   "source": [
    "versions = registry.list_versions()\n",
    "for version in versions:\n",
    "    registry.delete_version(version=version.version_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o6uIlUVQl2UG"
   },
   "outputs": [],
   "source": [
    "naive_bayes_train_job.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rr6mLp7wsgGl"
   },
   "outputs": [],
   "source": [
    "!gcloud dataproc batches delete $PREPROCESS_BATCH_ID --region=$REGION --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1kRq5qZ6k4b"
   },
   "outputs": [],
   "source": [
    "! bq rm -r -f -d $PROJECT_ID:$BQ_DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5jXiicf6kKR"
   },
   "outputs": [],
   "source": [
    "! gcloud artifacts repositories delete $REPO_NAME --location=$REGION --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UQ4s96N8ap-d"
   },
   "outputs": [],
   "source": [
    "delete_bucket = False\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZRstkQGyO6bu"
   },
   "outputs": [],
   "source": [
    "!rm -rf $DATA_PATH $SRC_PATH $BUILD_PATH $CONFIG_PATH"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Model versioning with Vertex AI Model Registry\n",
        "\n",
        "\n",
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>                                                                                               \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95963630ed29"
      },
      "source": [
        "## Overview\n",
        "\n",
        "In this notebook will show the model versioning capabilities of Vertex AI Model Registry with AutoML model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d0af502c20f"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to manage your models using the Vertex AI SDK and Vertex AI Model Registry.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services and resources:\n",
        "\n",
        "- Vertex AI AutoML\n",
        "- Vertex AI Model Registry\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Preprocess data using SparkNLP and load them into BQML\n",
        "- Train and register a AutoML Classifier using Vertex AI AutoML \n",
        "- Nominee a champion and approve the model to production by updating aliases with `production` alias\n",
        "- Deploy the default/production version of a Model resource."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b62a8b01d816"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "[BBC](http://mlg.ucd.ie/datasets/bbc.html) consists of 2225 documents from the BBC news website corresponding to stories in five topical areas (business, entertainment, politics, sport, tech) from 2004-2005. Each of the articles is in a .txt file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d26a1808b7c"
      },
      "source": [
        "### Costs \n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* BigQuery\n",
        "* Dataproc\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing), [BigQuery](https://cloud.google.com/bigquery/pricing), [Dataproc](https://cloud.google.com/dataproc/pricing) and [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing) and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze4-nDLfK4pw"
      },
      "source": [
        "### Set up your local development environment\n",
        "\n",
        "**If you are using Colab or Vertex AI Workbench Notebooks**, your environment already meets\n",
        "all the requirements to run this notebook. You can skip this step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCuSR8GkAgzl"
      },
      "source": [
        "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
        "You need the following:\n",
        "\n",
        "* The Google Cloud SDK\n",
        "* Git\n",
        "* Python 3\n",
        "* virtualenv\n",
        "* Jupyter notebook running in a virtual environment with Python 3\n",
        "\n",
        "The Google Cloud guide to [Setting up a Python development\n",
        "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
        "installation guide](https://jupyter.org/install) provide detailed instructions\n",
        "for meeting these requirements. The following steps provide a condensed set of\n",
        "instructions:\n",
        "\n",
        "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
        "\n",
        "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
        "\n",
        "1. [Install\n",
        "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
        "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
        "\n",
        "1. To install Jupyter, run `pip3 install jupyter` on the\n",
        "command-line in a terminal shell.\n",
        "\n",
        "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
        "\n",
        "1. Open this notebook in the Jupyter Notebook Dashboard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7EUnXsZhAGF"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Install the following packages required to execute this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b4ef9b72d43"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Vertex AI Workbench Notebook product has specific requirements\n",
        "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
        "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
        "    \"/opt/deeplearning/metadata/env_version\"\n",
        ")\n",
        "\n",
        "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_WORKBENCH_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\"\n",
        "\n",
        "! pip3 install --upgrade tensorflow google-cloud-bigquery google-cloud-aiplatform {USER_FLAG} -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhq5zEbGg0XX"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzrelQZ22IZj"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF1j6f9HApxa"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "1. [Enable APIs](https://console.cloud.google.com/flows/enableapi?apiid=iam.googleapis.com,aiplatform.googleapis.com,artifactregistry.googleapis.com,dataproc.googleapis.com,cloudbuild.googleapis.com)\n",
        "\n",
        "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
        "\n",
        "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WReHDGG5g0XY"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM1iC_MfAts1"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JX9pSDAbZ7_r"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID:\", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_gcloud_project_id"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
        "\n",
        "- Americas: `us-central1`\n",
        "- Europe: `europe-west4`\n",
        "- Asia Pacific: `asia-east1`\n",
        "\n",
        "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
        "\n",
        "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vprrvv0Ey1CU"
      },
      "outputs": [],
      "source": [
        "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
        "\n",
        "if REGION == \"[your-region]\":\n",
        "    REGION = \"us-central1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06571eb4063b"
      },
      "source": [
        "#### UUID\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a uuid for each instance session, and append it onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "697568e92bd6"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "\n",
        "# Generate a uuid of a specifed length(default=8)\n",
        "def generate_uuid(length: int = 8) -> str:\n",
        "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
        "\n",
        "\n",
        "UUID = generate_uuid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr--iN2kAylZ"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using Vertex AI Workbench Notebooks**, your environment is already\n",
        "authenticated. Skip this step.\n",
        "\n",
        "**If you are using Colab**, run the cell below and follow the instructions\n",
        "when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "1. In the Cloud Console, go to the [**Create service account key**\n",
        "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
        "\n",
        "2. Click **Create service account**.\n",
        "\n",
        "3. In the **Service account name** field, enter a name, and\n",
        "   click **Create**.\n",
        "\n",
        "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type and select\n",
        "the following role into the filter box:\n",
        "\n",
        "    *   BigQuery Admin\n",
        "    *   Dataproc Administrator\n",
        "    *   Dataproc Worker\n",
        "    *   Storage Admin\n",
        "    *   Storage Object Admin\n",
        "    *   Vertex AI Administrator\n",
        "\n",
        "5. Click *Create*. A JSON file that contains your key downloads to your\n",
        "local environment.\n",
        "\n",
        "6. Enter the path to your service account key as the\n",
        "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "outputs": [],
      "source": [
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# If on Vertex AI Workbench, then don't execute this code\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
        "    \"DL_ANACONDA_HOME\"\n",
        "):\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23988890fef6"
      },
      "source": [
        "#### Get your project number\n",
        "\n",
        "Now that the project ID is set, you get your corresponding project number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d6950574e1d"
      },
      "outputs": [],
      "source": [
        "shell_output = ! gcloud projects list --filter=\"PROJECT_ID:'{PROJECT_ID}'\" --format='value(PROJECT_NUMBER)'\n",
        "PROJECT_NUMBER = shell_output[0]\n",
        "print(\"Project Number:\", PROJECT_NUMBER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgPO1eR3CYjk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. It must be unique across all Cloud Storage buckets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzGDU7TWdts_"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf221059d072"
      },
      "outputs": [],
      "source": [
        "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
        "    BUCKET_NAME = PROJECT_ID + \"-aip-\" + UUID\n",
        "    BUCKET_URI = f\"gs://{BUCKET_NAME}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EcIXiGsCePi"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIq7R4HZCfIc"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucvCsknMCims"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhOb7YnwClBb"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "set_service_account"
      },
      "source": [
        "#### Service Account\n",
        "\n",
        "If you do not want to use your project's Compute Engine service account, set `SERVICE_ACCOUNT` to another service account ID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tfyNMaIy1CW"
      },
      "outputs": [],
      "source": [
        "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_service_account"
      },
      "outputs": [],
      "source": [
        "if (\n",
        "    SERVICE_ACCOUNT == \"\"\n",
        "    or SERVICE_ACCOUNT is None\n",
        "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
        "):\n",
        "    # Get your service account from gcloud\n",
        "    if not IS_COLAB:\n",
        "        shell_output = !gcloud auth list 2>/dev/null\n",
        "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
        "\n",
        "    else:  # IS_COLAB:\n",
        "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
        "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "\n",
        "    print(\"Service Account:\", SERVICE_ACCOUNT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "set_service_account:pipelines"
      },
      "source": [
        "#### Set service account access\n",
        "\n",
        "Run the following commands to grant your service account access to the bucket that you created in the previous step. You only need to run this step once per service account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RL4BUlkPy1CX"
      },
      "outputs": [],
      "source": [
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TRRTduWyx7f"
      },
      "source": [
        "### Enabling Private Google Access for Dataproc Serverless\n",
        "\n",
        "To execute Serverless Spark workloads, the VPC subnetwork must meet the [requirements](https://cloud.google.com/dataproc-serverless/docs/concepts/network) listed in Dataproc Serverless for Spark network configuration. In this tutorial we are going to use the default one and enable it to private ip access. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-1ZXXtTyzI5"
      },
      "outputs": [],
      "source": [
        "SUBNETWORK = \"default\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mSeas5axzyg"
      },
      "outputs": [],
      "source": [
        "!gcloud compute networks subnets list --regions=$REGION --filter=$SUBNETWORK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTsxWbNMyFKy"
      },
      "outputs": [],
      "source": [
        "!gcloud compute networks subnets update $SUBNETWORK \\\n",
        "--region=$REGION \\\n",
        "--enable-private-ip-google-access"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsdtU6E-yjxl"
      },
      "outputs": [],
      "source": [
        "!gcloud compute networks subnets describe $SUBNETWORK \\\n",
        "--region=$REGION \\\n",
        "--format=\"get(privateIpGoogleAccess)\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7p_RIzNM02N"
      },
      "source": [
        "### Create and configure the Docker repository\n",
        "\n",
        "You create a Docker repository in the Artefact Registry for the custom dataproc image we are going to create for NLP data preprocessing. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfTW_fMeWq3e"
      },
      "outputs": [],
      "source": [
        "REPO_NAME = \"vertex-ai-model-registry-demo\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyXCqeEPT-Cu"
      },
      "outputs": [],
      "source": [
        "!gcloud artifacts repositories create $REPO_NAME \\\n",
        "    --repository-format=docker \\\n",
        "    --location=$REGION \\\n",
        "    --description=\"vertex ai model registry spark docker repository\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gc1ubsMoF7wn"
      },
      "source": [
        "### Set project template\n",
        "\n",
        "You create a set of repositories to organize your project locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "420y8i4KF_z4"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = \"data\"\n",
        "SRC_PATH = \"src\"\n",
        "BUILD_PATH = \"build\"\n",
        "CONFIG_PATH = \"config\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHvsHGncGB-B"
      },
      "outputs": [],
      "source": [
        "!mkdir -m 777 -p $DATA_PATH $SRC_PATH $BUILD_PATH $CONFIG_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grDrP5pcGH2m"
      },
      "source": [
        "### Get input data\n",
        "\n",
        "In the following code, you download and extrect the tutorial dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_Nnlf0P4wnS"
      },
      "outputs": [],
      "source": [
        "RAW_DATA_URI = \"http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ML7i-NVFGLS6"
      },
      "outputs": [],
      "source": [
        "!rm -Rf {DATA_PATH}/raw \n",
        "!wget --no-parent {RAW_DATA_URI} --directory-prefix={DATA_PATH}/raw \n",
        "!unzip -qo {DATA_PATH}/raw/bbc-fulltext.zip -d {DATA_PATH}/raw && mv {DATA_PATH}/raw/bbc/* {DATA_PATH}/raw/\n",
        "!rm -Rf {DATA_PATH}/raw/bbc-fulltext.zip {DATA_PATH}/raw/bbc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf3vIGViHYo3"
      },
      "source": [
        "### Set Bigquery dataset\n",
        "\n",
        "You create the BigQuery dataset for the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZfxQL8JHdHE"
      },
      "outputs": [],
      "source": [
        "LOCATION = REGION.split(\"-\")[0]\n",
        "BQ_DATASET = \"bcc_sport\"\n",
        "\n",
        "! bq mk --location={LOCATION} --dataset {PROJECT_ID}:{BQ_DATASET}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRUOFELefqf1"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import glob\n",
        "# General\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", 3000)\n",
        "\n",
        "# Model Training\n",
        "import tensorflow as tf\n",
        "from google.cloud import aiplatform as vertex_ai\n",
        "from google.cloud import bigquery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPYUKWxx70wo"
      },
      "outputs": [],
      "source": [
        "print(\"BigQuery library version:\", bigquery.__version__)\n",
        "print(\"Vertex AI library version:\", vertex_ai.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcv3P_PTWCeR"
      },
      "source": [
        "### Set up variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3tPJ5YcWEys"
      },
      "outputs": [],
      "source": [
        "# General\n",
        "STAGING_BUCKET = f\"{BUCKET_URI}/jobs\"\n",
        "RAW_PATH = os.path.join(DATA_PATH, \"raw\")\n",
        "DATAPROC_IMAGE_BUILD_PATH = os.path.join(BUILD_PATH, \"dataproc_image\")\n",
        "PREPROCESS_DOCKERFILE_PATH = os.path.join(DATAPROC_IMAGE_BUILD_PATH, \"Dockerfile\")\n",
        "DATAPROC_RUNTIME_IMAGE = \"dataproc_serverless_custom_runtime\"\n",
        "IMAGE_TAG = \"1.0.0\"\n",
        "DATAPROC_RUNTIME_CONTAINER_IMAGE = (\n",
        "    f\"gcr.io/{PROJECT_ID}/{DATAPROC_RUNTIME_IMAGE}:{IMAGE_TAG}\"\n",
        ")\n",
        "INIT_PATH = os.path.join(SRC_PATH, \"__init__.py\")\n",
        "MODULE_URI = f\"{BUCKET_URI}/{SRC_PATH}\"\n",
        "VERTEX_AI_MODEL_ID = \"text-classifier-model\"\n",
        "\n",
        "# Ingest\n",
        "PREPARED_PATH = os.path.join(DATA_PATH, \"prepared\")\n",
        "PREPARED_FILE = \"prepared_data.csv\"\n",
        "PREPARED_FILE_PATH = os.path.join(PREPARED_PATH, PREPARED_FILE)\n",
        "PREPARED_FILE_URI = f\"{BUCKET_URI}/{PREPARED_FILE_PATH}\"\n",
        "\n",
        "# Preprocess\n",
        "PREPROCESS_MODULE_PATH = os.path.join(SRC_PATH, \"preprocess.py\")\n",
        "LEMMA_DICTIONARY_PATH = os.path.join(CONFIG_PATH, \"lemmas.txt\")\n",
        "LEMMA_DICTIONARY_URI = f\"{BUCKET_URI}/{CONFIG_PATH}/lemmas.txt\"\n",
        "PROCESS_PYTHON_FILE_URI = f\"{MODULE_URI}/preprocess.py\"\n",
        "PROCESS_DATA_PATH = os.path.join(DATA_PATH, \"processed\")\n",
        "BQ_OUTPUT_TABLE_URI = f\"{BQ_DATASET}.news_processed_{UUID}\"\n",
        "PROCESS_DATA_URI = f\"{BUCKET_URI}/{PROCESS_DATA_PATH}\"\n",
        "PROCESS_FILE_URI = f\"{PROCESS_DATA_URI}/*.parquet\"\n",
        "PREPROCESS_BATCH_ID = f\"nlp-preprocess-{UUID}\"\n",
        "\n",
        "# Training\n",
        "AUTOML_BQ_TABLE_URI = f\"{BQ_DATASET}.news_automl_dataset_table_{UUID}\"\n",
        "AUTOML_BQ_SOURCE = f\"bq://{PROJECT_ID}.{AUTOML_BQ_TABLE_URI}\"\n",
        "AUTOML_TEXT_DATASET = f\"sport_news_dataset_{UUID}\"\n",
        "AUTOML_BQ_EVALUATION_TABLE = (\n",
        "    f\"bq://{PROJECT_ID}.{BQ_DATASET}.news_automl_eval_table_{UUID}\"\n",
        ")\n",
        "\n",
        "# Deployment\n",
        "ENDPOINT_NAME = \"text-classifier-endpoint\"\n",
        "DEPLOYED_MODEL_NAME = \"naive-bayes-text-classifier\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for your project and corresponding bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gb46F2fk-ZTR"
      },
      "outputs": [],
      "source": [
        "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4MN3EoDvjga"
      },
      "source": [
        "### Helpers\n",
        "\n",
        "A set of helpers to facilitate some tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1I4LpAJg78Fp"
      },
      "outputs": [],
      "source": [
        "def prepare_data(input_path: str, output_path: str, file_name: str):\n",
        "    \"\"\"\n",
        "    This function prepares the data for the model registry demo.\n",
        "    Args:\n",
        "        input_path: The directory where the raw data is stored.\n",
        "        output_path: The directory where the prepared data will be stored.\n",
        "        file_name: The name of the file to be prepared.\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Read folder names\n",
        "    categories = [f.name for f in os.scandir(input_path) if f.is_dir()]\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    if not os.path.exists(output_path):\n",
        "        os.makedirs(output_path)\n",
        "\n",
        "    # Create output file\n",
        "    with open(output_path + \"/\" + file_name, \"w\") as output_file:\n",
        "        csv_writer = csv.writer(output_file)\n",
        "        csv_writer.writerow([\"category\", \"text\"])\n",
        "\n",
        "        # For each category, read all files and write to output file\n",
        "        for category in categories:\n",
        "            # Read all files in category\n",
        "            for filename in glob.glob(os.path.join(input_path, category, \"*.txt\")):\n",
        "                # Read file\n",
        "                with open(filename, \"r\") as input_file:\n",
        "                    output_text = \"\".join([line.rstrip() for line in input_file])\n",
        "                    # Write to output file\n",
        "                    csv_writer.writerow([category, output_text])\n",
        "                    input_file.close()\n",
        "\n",
        "        # Close output file\n",
        "        output_file.close()\n",
        "\n",
        "\n",
        "def run_query(query):\n",
        "\n",
        "    \"\"\"\n",
        "    This function runs a query on the prepared data.\n",
        "    Args:\n",
        "        query: The query to be run.\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    # Construct a BigQuery client object.\n",
        "    client = bigquery.Client(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "    # Run the query_job\n",
        "    query_job = client.query(query)\n",
        "\n",
        "    # Wait for the query to finish\n",
        "    result = query_job.result()\n",
        "\n",
        "    # Return table\n",
        "    table = query_job.ddl_target_table\n",
        "\n",
        "    return table, result\n",
        "\n",
        "\n",
        "def read_metrics_file(metrics_file_uri):\n",
        "    \"\"\"\n",
        "    This function reads metrics file on bucket\n",
        "    Args:\n",
        "      metrics_file_uri: The uri of the metrics file\n",
        "    Returns:\n",
        "      metrics_str: metrics string\n",
        "    \"\"\"\n",
        "\n",
        "    with tf.io.gfile.GFile(metrics_file_uri, \"r\") as metrics_file:\n",
        "        metrics = metrics_file.read().replace(\"'\", '\"')\n",
        "    metrics_file.close()\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brRJTOTQmrcp"
      },
      "source": [
        "## Data Engineering with Dataproc Serverless\n",
        "\n",
        "Before you build a NLP machine learning model, there are some common pre-processing steps to use:\n",
        "\n",
        "1.   Preliminaries such as sentence segmentation and word tokenization\n",
        "2.   Frequent steps such as stop word removal, stemming and lemmatization, removing digits/punctuation, lowercasing, etc.\n",
        "\n",
        "Other steps are normalization, language detection other than POS tagging, parsing. \n",
        "\n",
        "In the following section you ingest your dataset and you will use SparkNLP on Dataproc serverless to build and execute a simple NLP preprocessing pipeline. To do that you need:\n",
        "\n",
        "\n",
        "1.   Upload data on Google Cloud Bucket\n",
        "2.   Create a custom Dataproc Serverless image\n",
        "3.   Create and upload the `preprocess` module and its dependencies on Google Cloud Bucket\n",
        "\n",
        "Then you will run the Dataproc serverless job and the resulting data will be loaded into Bigquery. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt4-0iDRwxSl"
      },
      "source": [
        "### Ingest data\n",
        "\n",
        "Next, you do the following:\n",
        "\n",
        "1.   Prepare data by extracting news from directories and create the associated csv file.\n",
        "2.   Upload the data to Google Cloud Bucket\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmSQUovoD8_C"
      },
      "source": [
        "#### Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCSAJMkQ9X0G"
      },
      "outputs": [],
      "source": [
        "prepare_data(RAW_PATH, PREPARED_PATH, PREPARED_FILE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oeNQhmcEAtB"
      },
      "source": [
        "##### Quick peek at the CSV data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKkkdTmiGacU"
      },
      "outputs": [],
      "source": [
        "! head $PREPARED_FILE_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdov1WbQF5L1"
      },
      "source": [
        "#### Upload the data to bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-kp6EeiAF5C"
      },
      "outputs": [],
      "source": [
        "! gsutil cp $PREPARED_FILE_PATH $PREPARED_FILE_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeAdjD55KhTY"
      },
      "source": [
        "### Basic Data and Feature Engineering \n",
        "\n",
        "In this scenario, you will use a Spark pipeline to cover the following steps using Spark NLP\n",
        "\n",
        "1. Sentence segmentation\n",
        "2. Word tokenization\n",
        "3. Normalization\n",
        "4. Stopword removal\n",
        "5. Stemming\n",
        "6. Lemmatization\n",
        "\n",
        "Finally, you create a bag of words (BOW) using `CountVectorizer` object.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68nYBB5GS9TB"
      },
      "source": [
        "#### Build a custom dataproc serverless image\n",
        "\n",
        "The `DataprocPySparkBatchOp` allows you to pass custom image you would like to use when the [provided Dataproc Serverless runtime versions](https://cloud.google.com/dataproc-serverless/docs/concepts/versions/spark-runtime-versions) does not respect your requirements. \n",
        "\n",
        "In this case, an image with Spark NLP library is needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8DsE4wHqI7B"
      },
      "source": [
        "##### Download the spark job dependencies\n",
        "\n",
        "You download the spark dependencies required to run the NLP preprocessing pipeline. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Wxd4fqvl8CF"
      },
      "outputs": [],
      "source": [
        "! rm -rf $DATAPROC_IMAGE_BUILD_PATH\n",
        "! mkdir $DATAPROC_IMAGE_BUILD_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4N8m-K1ldt11"
      },
      "outputs": [],
      "source": [
        "!gsutil cp gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.22.2.jar $DATAPROC_IMAGE_BUILD_PATH\n",
        "!wget -P $DATAPROC_IMAGE_BUILD_PATH https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-4.0.2.jar\n",
        "!wget -P $DATAPROC_IMAGE_BUILD_PATH https://repo.anaconda.com/miniconda/Miniconda3-py38_4.9.2-Linux-x86_64.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF9_5IGYqLAX"
      },
      "source": [
        "##### Define the Dataproc serverless custom runtime image\n",
        "\n",
        "You define the Dockerfile to create the custom image. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWiGHdEibCcv"
      },
      "outputs": [],
      "source": [
        "dataproc_serverless_custom_runtime_image = \"\"\"\n",
        "# Debian 11 is recommended.\n",
        "FROM debian:11-slim\n",
        "\n",
        "# Suppress interactive prompts\n",
        "ENV DEBIAN_FRONTEND=noninteractive\n",
        "\n",
        "# (Required) Install utilities required by Spark scripts.\n",
        "RUN apt update && apt install -y procps tini\n",
        "\n",
        "# (Optional) Add extra jars.\n",
        "ENV SPARK_EXTRA_JARS_DIR=/opt/spark/jars/\n",
        "ENV SPARK_EXTRA_CLASSPATH='/opt/spark/jars/*'\n",
        "RUN mkdir -p \"${SPARK_EXTRA_JARS_DIR}\"\n",
        "COPY spark-bigquery-with-dependencies_2.12-0.22.2.jar \"${SPARK_EXTRA_JARS_DIR}\"\n",
        "COPY spark-nlp-assembly-4.0.2.jar \"${SPARK_EXTRA_JARS_DIR}\"\n",
        "\n",
        "# (Optional) Install and configure Miniconda3.\n",
        "ENV CONDA_HOME=/opt/miniconda3\n",
        "ENV PYSPARK_PYTHON=${CONDA_HOME}/bin/python\n",
        "ENV PATH=${CONDA_HOME}/bin:${PATH}\n",
        "COPY Miniconda3-py38_4.9.2-Linux-x86_64.sh .\n",
        "RUN bash Miniconda3-py38_4.9.2-Linux-x86_64.sh -b -p /opt/miniconda3 \\\n",
        "  && ${CONDA_HOME}/bin/conda config --system --set always_yes True \\\n",
        "  && ${CONDA_HOME}/bin/conda config --system --set auto_update_conda False \\\n",
        "  && ${CONDA_HOME}/bin/conda config --system --prepend channels conda-forge \\\n",
        "  && ${CONDA_HOME}/bin/conda config --system --set channel_priority strict\n",
        "\n",
        "# (Optional) Install Conda packages.\n",
        "#\n",
        "# The following packages are installed in the default image, it is strongly\n",
        "# recommended to include all of them.\n",
        "#\n",
        "# Use mamba to install packages quickly.\n",
        "RUN ${CONDA_HOME}/bin/conda install mamba -n base -c conda-forge \\\n",
        "    && ${CONDA_HOME}/bin/mamba install \\\n",
        "      conda \\\n",
        "      cython \\\n",
        "      gcsfs \\\n",
        "      google-cloud-bigquery-storage \\\n",
        "      google-cloud-bigquery[pandas] \\\n",
        "      google-cloud-dataproc \\\n",
        "      numpy \\\n",
        "      pandas \\\n",
        "      python \\\n",
        "      pyspark \\\n",
        "      findspark\n",
        "\n",
        "# Use conda to install spark-nlp\n",
        "RUN ${CONDA_HOME}/bin/conda install -n base -c johnsnowlabs spark-nlp\n",
        "\n",
        "# Add lemma dictionary\n",
        "# ENV CONFIG_DIR='/home/app/build'\n",
        "# RUN mkdir -p \"${CONFIG_DIR}\"\n",
        "# COPY lemmas.txt \"${CONFIG_DIR}\"\n",
        "\n",
        "# (Required) Create the 'spark' group/user.\n",
        "# The GID and UID must be 1099. Home directory is required.\n",
        "RUN groupadd -g 1099 spark\n",
        "RUN useradd -u 1099 -g 1099 -d /home/spark -m spark\n",
        "USER spark\n",
        "\"\"\"\n",
        "\n",
        "with open(PREPROCESS_DOCKERFILE_PATH, \"w\") as f:\n",
        "    f.write(dataproc_serverless_custom_runtime_image)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXzI2xInqb3V"
      },
      "source": [
        "##### Build the Dataproc serverless custom runtime using Google Cloud Build\n",
        "\n",
        "You use cloud build to create and register the container image to Artefact registry. \n",
        "\n",
        "Notice that `<PROJECT_ID>@cloudbuild.gserviceaccount.com` requires to have storage.objects.get access to the Google Cloud Storage object.\n",
        "\n",
        "**Notice**: This step would take ~5min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5QT7D1LkG7L"
      },
      "outputs": [],
      "source": [
        "CLOUD_BUILD_SERVICE_ACCOUNT = f\"{PROJECT_NUMBER}@cloudbuild.gserviceaccount.com\"\n",
        "\n",
        "! gsutil iam ch serviceAccount:{CLOUD_BUILD_SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
        "! gsutil iam ch serviceAccount:{CLOUD_BUILD_SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnpRqGrFkPaH"
      },
      "outputs": [],
      "source": [
        "!gcloud builds submit --tag $DATAPROC_RUNTIME_CONTAINER_IMAGE $DATAPROC_IMAGE_BUILD_PATH --machine-type=N1_HIGHCPU_32 --timeout=900s --verbosity=info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lsCPf1KwImo"
      },
      "source": [
        "#### Prepare `preprocess` module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFWE_I8VfQFO"
      },
      "source": [
        "##### Create the preprocess module\n",
        "\n",
        "This module will preprocess the data and it covers the following steps:\n",
        "\n",
        "1. Sentence segmentation\n",
        "2. Word tokenization\n",
        "3. Normalization\n",
        "4. Stopword removal\n",
        "5. Stemming\n",
        "6. Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_-MYpcbr_1N"
      },
      "outputs": [],
      "source": [
        "with open(INIT_PATH, \"w\") as init_file:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9Ouh6VMr8cB"
      },
      "outputs": [],
      "source": [
        "process_module = \"\"\"\n",
        "#!/usr/bin/env python3\n",
        "\n",
        "'''\n",
        "This is a simple module to preprocess the data for the model registry demo.\n",
        "Steps:\n",
        "1. Sentence segmentation\n",
        "2. Word tokenization\n",
        "3. Normalization\n",
        "4. Stopword removal\n",
        "5. Stemming\n",
        "6. Lemmatization\n",
        "'''\n",
        "\n",
        "# Libraries\n",
        "import logging\n",
        "import argparse\n",
        "\n",
        "from pyspark.sql.types import StructType, StringType\n",
        "from pyspark.sql.functions import col, concat_ws, rand\n",
        "from pyspark.ml.functions import vector_to_array\n",
        "import sparknlp\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from pyspark.ml.feature import CountVectorizer\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Variables ------------------------------------------------------------------------------------------------------------\n",
        "DATA_SCHEMA = (StructType()\n",
        "               .add(\"category\", StringType(), True)\n",
        "               .add(\"text\", StringType(), True))\n",
        "SEED=8\n",
        "\n",
        "# Helper functions -----------------------------------------------------------------------------------------------------\n",
        "def get_logger():\n",
        "    '''\n",
        "    This function returns a logger object.\n",
        "    Returns:\n",
        "        logger: The logger object.\n",
        "    '''\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.setLevel(logging.INFO)\n",
        "    handler = logging.StreamHandler()\n",
        "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "    handler.setFormatter(formatter)\n",
        "    logger.addHandler(handler)\n",
        "    return logger\n",
        "\n",
        "\n",
        "def get_args():\n",
        "    '''\n",
        "    This function returns the arguments from the command line.\n",
        "    Returns:\n",
        "        args: The arguments from the command line.\n",
        "    '''\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--input_path', type=str, help='The input path uri without bucket prefix')\n",
        "    parser.add_argument('--lemmas_path', type=str, help='The lemma dictionary path without bucket prefix')\n",
        "    parser.add_argument('--gcs_output_path', type=str, help='The gcs path for preprocessed data without bucket prefix')\n",
        "    parser.add_argument('--bq_output_table_uri', type=str, help='The Bigquery output table URI')\n",
        "    parser.add_argument('--bucket', type=str, help='The staging bucket')\n",
        "    parser.add_argument('--project', type=str, help='The project id')\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "\n",
        "def build_preliminary_steps():\n",
        "    '''\n",
        "    This function builds the preliminary steps for the preprocessing.\n",
        "    Returns:\n",
        "        preliminary_steps: The preliminary steps for the preprocessing.\n",
        "    '''\n",
        "\n",
        "    document_assembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\").setCleanupMode('shrink_full')\n",
        "    sentence_detector = SentenceDetector().setInputCols(\"document\").setOutputCol(\"sentence\")\n",
        "    tokenizer = Tokenizer().setInputCols(\"sentence\").setOutputCol(\"token\")\n",
        "    preliminary_steps = [document_assembler, sentence_detector, tokenizer]\n",
        "    return preliminary_steps\n",
        "\n",
        "\n",
        "def build_common_preprocess_steps(lemma_uri):\n",
        "    '''\n",
        "    This function builds the common preprocessing steps.\n",
        "    Args:\n",
        "        lemma_uri: The uri of lemma dictionary\n",
        "    Returns:\n",
        "        common_preprocess_steps: The common preprocessing steps.\n",
        "    '''\n",
        "\n",
        "    normalizer = Normalizer().setInputCols(\"token\").setOutputCol(\"normalized_token\").setLowercase(True)\n",
        "    stopwords_cleaner = StopWordsCleaner().setInputCols(\"normalized_token\").setOutputCol(\n",
        "        \"cleaned_tokens\").setCaseSensitive(False)\n",
        "    stemmer = Stemmer().setInputCols(\"cleaned_tokens\").setOutputCol(\"stem\")\n",
        "    lemmatizer = Lemmatizer().setInputCols(\"stem\").setOutputCol(\"lemma\").setDictionary(lemma_uri, \"->\", \"\\t\")\n",
        "    finisher = Finisher().setInputCols(\"lemma\").setOutputCols([\"lemma_features\"]).setIncludeMetadata(\n",
        "        False).setOutputAsArray(True)\n",
        "    common_preprocess_steps = [normalizer, stopwords_cleaner, stemmer, lemmatizer, finisher]\n",
        "    return common_preprocess_steps\n",
        "\n",
        "\n",
        "def build_feature_extraction_steps():\n",
        "    '''\n",
        "    This function builds the feature extraction steps.\n",
        "    Returns:\n",
        "        feature_extraction_steps: The feature extraction steps.\n",
        "    '''\n",
        "\n",
        "    count_vectorizer = CountVectorizer().setInputCol(\"lemma_features\").setOutputCol(\"features\").setVocabSize(30)\n",
        "    feature_extraction_steps = [count_vectorizer]\n",
        "    return feature_extraction_steps\n",
        "\n",
        "\n",
        "def read_data(spark_session, data_schema, input_dir):\n",
        "    '''\n",
        "    This function reads the data from the input directory.\n",
        "    Args:\n",
        "        spark_session: The SparkSession object.\n",
        "        data_schema: The data schema.\n",
        "        input_dir: The input directory.\n",
        "    Returns:\n",
        "        raw_df: The raw dataframe.\n",
        "    '''\n",
        "\n",
        "    raw_df = (spark_session.read.option(\"header\", True)\n",
        "              .option(\"delimiter\", ',')\n",
        "              .schema(data_schema)\n",
        "              .csv(input_dir))\n",
        "    return raw_df\n",
        "\n",
        "\n",
        "def prepare_train_df(df):\n",
        "    '''\n",
        "    This function prepares the training dataframe.\n",
        "    Args:\n",
        "        df: The dataframe.\n",
        "    Returns:\n",
        "        None\n",
        "    '''\n",
        "    train_df = (df.withColumn(\"bow_col\", vector_to_array(\"features\"))\n",
        "                .withColumn(\"lemmas\", concat_ws(\" \", col(\"lemma_features\")))\n",
        "                .select([\"text\"] + [\"lemmas\"] + [col(\"bow_col\")[i] for i in range(30)] + [\"category\"]))\n",
        "\n",
        "    return train_df\n",
        "\n",
        "\n",
        "def save_data(data, bucket, gcs_path, bigquery_uri):\n",
        "    '''\n",
        "    This function saves the data to Bigquery.\n",
        "    Args:\n",
        "        data: The data to save.\n",
        "        bucket: The bucket.\n",
        "        gcs_path: The path to store processed data.\n",
        "        bigquery_uri: The URI of the Bigquery table.\n",
        "    Returns:\n",
        "        None\n",
        "    '''\n",
        "    # df_sample = data.sample(withReplacement=False, fraction=0.7, seed=SEED)\n",
        "    df_sample = data.orderBy(rand(SEED)).limit(1000)\n",
        "    df_sample.write.format('bigquery') \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .option(\"persistentGcsBucket\", bucket) \\\n",
        "        .option(\"persistentGcsPath\", gcs_path) \\\n",
        "        .save(bigquery_uri)\n",
        "\n",
        "\n",
        "# Main function --------------------------------------------------------------------------------------------------------\n",
        "def preprocess(args):\n",
        "    '''\n",
        "    preprocess function.\n",
        "    Args:\n",
        "        args: The arguments from the command line.\n",
        "    Returns:\n",
        "        None\n",
        "    '''\n",
        "    # Get logger\n",
        "    logger = get_logger()\n",
        "\n",
        "    # Initialize variables\n",
        "    input_path = args.input_path\n",
        "    lemma_path = args.lemmas_path\n",
        "    gcs_output_path = args.gcs_output_path\n",
        "    bq_output_table_uri = args.bq_output_table_uri\n",
        "    bucket = args.bucket\n",
        "    project = args.project\n",
        "    lemma_uri = f'gs://{bucket}/{lemma_path}'\n",
        "    input_uri = f'gs://{bucket}/{input_path}'\n",
        "\n",
        "    # Initialize SparkSession\n",
        "    logger.info('Starting preprocessing')\n",
        "    spark = sparknlp.start()\n",
        "    print(f\"Spark NLP version: {sparknlp.version()}\")\n",
        "    print(f\"Spark version: {spark.version}\")\n",
        "\n",
        "    # Build pipeline steps\n",
        "    logger.info('Building pipeline steps')\n",
        "    preliminary_steps = build_preliminary_steps()\n",
        "    common_preprocess_steps = build_common_preprocess_steps(lemma_uri)\n",
        "    feature_extraction_steps = build_feature_extraction_steps()\n",
        "    pipeline = Pipeline(stages=preliminary_steps + common_preprocess_steps + feature_extraction_steps)\n",
        "\n",
        "    # Read data\n",
        "    logger.info('Reading data')\n",
        "    raw_df = read_data(spark, DATA_SCHEMA, input_uri)\n",
        "\n",
        "    # Preprocess data\n",
        "    logger.info('Preprocessing data')\n",
        "    processed_pipeline = pipeline.fit(raw_df)\n",
        "    preprocessed_df = processed_pipeline.transform(raw_df)\n",
        "    preprocessed_df.show(10, truncate=False)\n",
        "\n",
        "    # Save data to Bigquery\n",
        "    logger.info('Saving data to Bigquery')\n",
        "    train_df = prepare_train_df(preprocessed_df)\n",
        "    save_data(train_df, bucket, gcs_output_path, bq_output_table_uri)\n",
        "    logging.info('done.')\n",
        "    spark.stop()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Get args\n",
        "    args = get_args()\n",
        "    preprocess(args)\n",
        "\"\"\"\n",
        "\n",
        "with open(PREPROCESS_MODULE_PATH, \"w\") as process_file:\n",
        "    process_file.write(process_module)\n",
        "process_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8Ic6_s3wPUC"
      },
      "source": [
        "##### Upload the module on bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpLfaU6Xw4oF"
      },
      "outputs": [],
      "source": [
        "!gsutil cp $SRC_PATH/__init__.py $MODULE_URI/__init__.py\n",
        "!gsutil cp $SRC_PATH/preprocess.py $MODULE_URI/preprocess.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1K3F7QidiCd"
      },
      "source": [
        "##### Upload config file\n",
        "\n",
        "You use the lemma dictionary according to Spark NLP documentation and you will upload it to Google Cloud bucket. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vxZuoy0dlfS"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/mahavivo/vocabulary/master/lemmas/AntBNC_lemmas_ver_001.txt -O $LEMMA_DICTIONARY_PATH\n",
        "!gsutil cp $LEMMA_DICTIONARY_PATH $LEMMA_DICTIONARY_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQVJBaXTz4sV"
      },
      "source": [
        "#### Run a preprocess spark job using dataproc serverless\n",
        "\n",
        "Now that you prepare the execution, you can sumbit the preprocessing Dataproc Serverless job. The explanation of this cli command is out of scope but you can have a look of all its option [in the official documentation](https://cloud.google.com/dataproc-serverless/docs/quickstarts/spark-batch)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fbik13otuQ7"
      },
      "outputs": [],
      "source": [
        "! gcloud beta dataproc batches submit pyspark $PROCESS_PYTHON_FILE_URI \\\n",
        "  --batch=$PREPROCESS_BATCH_ID \\\n",
        "  --container-image=$DATAPROC_RUNTIME_CONTAINER_IMAGE \\\n",
        "  --region=$REGION \\\n",
        "  --subnet='default' \\\n",
        "  --properties spark.executor.instances=2,spark.driver.cores=4,spark.executor.cores=4,spark.app.name=spark_preprocessing_job \\\n",
        "  -- --input_path=$PREPARED_FILE_PATH --lemmas_path=$LEMMA_DICTIONARY_PATH --gcs_output_path=$PROCESS_DATA_PATH --bq_output_table_uri=$BQ_OUTPUT_TABLE_URI --bucket=$BUCKET_NAME --project=$PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhMnSORpPkO0"
      },
      "source": [
        "## Model Training for Text Classification\n",
        "\n",
        "According to [Practical Natural Language Processing: A Comprehensive Guide to Building Real-World NLP Systems](https://www.oreilly.com/library/view/practical-natural-language/9781492054047/), there are different approaches to train text classifiers. \n",
        "\n",
        "For instance, you have\n",
        "\n",
        "- Traditional methods such Logistic Regression or Naive Bayes Classifier\n",
        "- Neural Embeddings methods\n",
        "- Deep Learning methods\n",
        "- Large, Pre-trained Language Models\n",
        "\n",
        "In the following section you going to use Vertex AI AutoML and you show how Vertex AI Model Registry will govern it. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8-Ycf1BLVvt"
      },
      "source": [
        "### Deep Learning classifier using Vertex AI AutoML\n",
        "\n",
        "For simplicity, you use Vertex AI AutoML for training a deep text classifier model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vJ5nRSgfn-C"
      },
      "source": [
        "##### Prepare text data in Biguery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSuusK2fful9"
      },
      "outputs": [],
      "source": [
        "automl_table_query = f\"\"\"\n",
        "CREATE OR REPLACE TABLE {AUTOML_BQ_TABLE_URI} AS\n",
        "  SELECT text, category\n",
        "  FROM `{PROJECT_ID}.{BQ_OUTPUT_TABLE_URI}`\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMGmYJT3kCjT"
      },
      "outputs": [],
      "source": [
        "table, result = run_query(query=automl_table_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrgrTU7zXBaJ"
      },
      "source": [
        "##### Create a tabular dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUqRP5UuLvag"
      },
      "outputs": [],
      "source": [
        "automl_bq_dataset = vertex_ai.TabularDataset.create(\n",
        "    display_name=AUTOML_TEXT_DATASET, bq_source=AUTOML_BQ_SOURCE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry534SFldiTb"
      },
      "source": [
        "##### Train AutoML text classifier\n",
        "\n",
        "You will train an AutoML classifier which would minize the log loss. At the end, it will be registered as new version of the text classifier model. \n",
        "\n",
        "Notice it would take **~4hr**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoYzbaoGLvSk"
      },
      "outputs": [],
      "source": [
        "automl_pipeline_job = vertex_ai.AutoMLTabularTrainingJob(\n",
        "    display_name=f\"deep_text_classifier_{UUID}\",\n",
        "    optimization_prediction_type=\"classification\",\n",
        "    optimization_objective=\"minimize-log-loss\",\n",
        "    column_specs={\"text\": \"auto\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXSN1sbpXSSR"
      },
      "outputs": [],
      "source": [
        "automl_model = automl_pipeline_job.run(\n",
        "    dataset=automl_bq_dataset,\n",
        "    target_column=\"category\",\n",
        "    training_fraction_split=0.8,\n",
        "    validation_fraction_split=0.1,\n",
        "    test_fraction_split=0.1,\n",
        "    parent_model=VERTEX_AI_MODEL_ID,\n",
        "    model_version_aliases=[\"automl\", \"deep_classifier\"],\n",
        "    model_version_description=\"An Vertex AI AutoML text classifier\",\n",
        "    model_labels={\"created_by\": \"inardini\", \"team\": \"advocacy\"},\n",
        "    is_default_version=False,\n",
        "    disable_early_stopping=False,\n",
        "    export_evaluated_data_items=True,\n",
        "    export_evaluated_data_items_bigquery_destination_uri=AUTOML_BQ_EVALUATION_TABLE,\n",
        "    sync=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgsDPyfG-zjP"
      },
      "source": [
        "## Model Governance with Vertex AI Model Registry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oV9FimH-vrps"
      },
      "source": [
        "#### Initialize Vertex AI Model Registry\n",
        "\n",
        "To access different model versions of a Vertex AI Model resource, you can initialize a model registry instance of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O32IGV9tgXw8"
      },
      "outputs": [],
      "source": [
        "registry = vertex_ai.models.ModelRegistry(VERTEX_AI_MODEL_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11xaykBni5mJ"
      },
      "source": [
        "#### Compare model versions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-bvStzdzPs9"
      },
      "source": [
        "##### Evaluate the new candidate\n",
        "\n",
        "You will use the `list_model_evaluations` method to evaluate the new model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKOmAsgv2OeH"
      },
      "outputs": [],
      "source": [
        "automl_evaluations = automl_model.list_model_evaluations()\n",
        "\n",
        "for model_evaluation in automl_evaluations:\n",
        "    print(model_evaluation.to_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nr1xRQbcm4i"
      },
      "source": [
        "### Register the `champion` model version "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJIURXCyzevK"
      },
      "source": [
        "##### Validate the new candidate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gn9LIgPoVK-1"
      },
      "outputs": [],
      "source": [
        "versions = registry.list_versions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsB-BEXI1ECG"
      },
      "outputs": [],
      "source": [
        "CANDIDATE_VERSION_ID = versions[-1].version_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0FBdyEb1ECG"
      },
      "outputs": [],
      "source": [
        "candidate_model_version_info = registry.get_version_info(CANDIDATE_VERSION_ID)\n",
        "candidate_model_version_info_df = pd.DataFrame(\n",
        "    candidate_model_version_info,\n",
        "    columns=[\"model_version\"],\n",
        "    index=[\n",
        "        \"version_id\",\n",
        "        \"created_at\",\n",
        "        \"updated_at\",\n",
        "        \"model_display_name\",\n",
        "        \"model_resource_name\",\n",
        "        \"version_aliases\",\n",
        "        \"version_description\",\n",
        "    ],\n",
        ")\n",
        "candidate_model_version_info_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0dLEuDn1XkU"
      },
      "source": [
        "##### Promote to production"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWpPmReM1cbx"
      },
      "outputs": [],
      "source": [
        "registry.add_version_aliases([\"default\", \"production\"], version=CANDIDATE_VERSION_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqbiAONcwpM1"
      },
      "source": [
        "##### Deploy the new candidate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKOPUzyI1kp0"
      },
      "outputs": [],
      "source": [
        "candidate_model = registry.get_model(version=\"candidate\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwErIIsOGMzk"
      },
      "source": [
        "###### Create the endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LWhrPXfGMGu"
      },
      "outputs": [],
      "source": [
        "endpoint = vertex_ai.Endpoint.create(\n",
        "    display_name=ENDPOINT_NAME,\n",
        "    project=PROJECT_ID,\n",
        "    location=REGION,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqhjQSrMKGyl"
      },
      "source": [
        "###### Deploy the champion model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvMabDk0GxgV"
      },
      "outputs": [],
      "source": [
        "endpoint.deploy(\n",
        "    model=candidate_model,\n",
        "    deployed_model_display_name=DEPLOYED_MODEL_NAME,\n",
        "    machine_type=\"n1-standard-8\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iytsp3RN15aE"
      },
      "source": [
        "### Generate predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34wRUZkfbp05"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"The singer to headline the event halftime show: 'It's on\"\"\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqgNOvZ815aE"
      },
      "outputs": [],
      "source": [
        "instances = [{\"text\": text}]\n",
        "predictions = endpoint.predict(instances)\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dksu_4joK2Hv"
      },
      "source": [
        "### Final thoughts \n",
        "\n",
        "As you can imagine you can also upload external models. Have a look at the documentation sample and the [sample notebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/get_started_with_model_registry.ipynb). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3GoM5NIrMML"
      },
      "outputs": [],
      "source": [
        "endpoint.undeploy_all()\n",
        "\n",
        "endpoint.delete()\n",
        "\n",
        "versions = registry.list_versions()\n",
        "for version in versions:\n",
        "    registry.delete_version(version=version.version_id)\n",
        "\n",
        "automl_pipeline_job.delete()\n",
        "\n",
        "automl_bq_dataset.delete()\n",
        "\n",
        "!gcloud dataproc batches delete $PREPROCESS_BATCH_ID --region=$REGION --quiet\n",
        "\n",
        "! bq rm -r -f -d $PROJECT_ID:$BQ_DATASET\n",
        "\n",
        "! gcloud artifacts repositories delete $REPO_NAME --location=$REGION --quiet\n",
        "\n",
        "!rm -rf $DATA_PATH $SRC_PATH $BUILD_PATH $CONFIG_PATH"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "vertex_ai_model_registry_automl_model_versioning.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title:generic,gcp"
      },
      "source": [
        "# E2E ML on GCP: MLOps stage 5 : deployment: Get started with Vertex AI Endpoints\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage5/get_started_with_vertex_endpoints.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage5/get_started_with_vertex_endpoints.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/ml_ops/stage5/get_started_with_vertex_endpoints.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>         \n",
        "</table>\n",
        "<br/><br/><br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial demonstrates how to use Vertex AI SDK to create and use `Vertex AI Endpoint` resources for serving models. `Vertex AI Endpoints` provide the ability to virtualize the serving binaries and serving infrastructure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9402cfbdc2d"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to use `Vertex AI Endpoint` resources.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services and resources:\n",
        "\n",
        "- `Vertex AI Endpoints`\n",
        "- `Vertex AI Models`\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Creating an `Endpoint` resource.\n",
        "- List all `Endpoint` resources.\n",
        "- List `Endpoint` resources by query filter.\n",
        "- Configuring the serving binary of a `Model` resource for deployment to an `Endpoint` resource.\n",
        "- Deploying a single `Model` resource to an `Endpoint` resource.\n",
        "- Get deployment settings for a deployed `Model` resource.\n",
        "- Configuring auto-scaling.\n",
        "- Deploying multiple `Model` resources to an `Endpoint` resource and configuring a traffic split.\n",
        "- Dynamically change the traffic split for an `Endpoint` resource.\n",
        "- Undeploy a single `Model` resource to an `Endpoint` resource.\n",
        "- Undeploy all `Model` resources from an `Endpoint` resource.\n",
        "- Delete an `Endpoint` resource.\n",
        "- In pipeline: Create an `Endpoint` resource and deploy an existing `Model` resource to the `Endpoint` resource.\n",
        "- In pipeline: Deploy an existing `Model` resource to an existing `Endpoint` resource."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset:iris,lcn"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "This tutorial uses a pre-trained image classification model from TensorFlow Hub, which is trained on ImageNet dataset.\n",
        "\n",
        "Learn more about [ResNet V2 pretained model](https://tfhub.dev/google/imagenet/resnet_v2_101/classification/5). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "costs"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_aip"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Install the following packages to execute this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_aip"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Vertex AI Workbench Notebook product has specific requirements\n",
        "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
        "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
        "    \"/opt/deeplearning/metadata/env_version\"\n",
        ")\n",
        "\n",
        "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_WORKBENCH_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\"\n",
        "\n",
        "! pip3 install --upgrade google-cloud-aiplatform $USER_FLAG -q\n",
        "! pip3 install --upgrade google-cloud-pipeline-components $USER_FLAG -q\n",
        "! pip3 install tensorflow-hub $USER_FLAG -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhq5zEbGg0XX"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzrelQZ22IZj"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "before_you_begin"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### GPU runtime\n",
        "\n",
        "*Make sure you're running this notebook in a GPU runtime if you have that option. In Colab, select* **Runtime > Change Runtime Type > GPU**\n",
        "\n",
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "2. [Make sure that billing is enabled for your project.](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
        "\n",
        "3. [Enable the following APIs: Vertex AI APIs, Compute Engine APIs, and Cloud Storage.](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component,storage-component.googleapis.com)\n",
        "\n",
        "4. If you are running this notebook locally, you need to install the [Cloud SDK]((https://cloud.google.com/sdk)).\n",
        "\n",
        "5. Enter your project ID in the cell below. Then run the  cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "project_id"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_project_id"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_project_id"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID:\", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_gcloud_project_id"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
        "\n",
        "- Americas: `us-central1`\n",
        "- Europe: `europe-west4`\n",
        "- Asia Pacific: `asia-east1`\n",
        "\n",
        "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
        "\n",
        "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "region"
      },
      "outputs": [],
      "source": [
        "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
        "\n",
        "if REGION == \"[your-region]\":\n",
        "    REGION = \"us-central1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "timestamp"
      },
      "source": [
        "#### Timestamp\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "timestamp"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcp_authenticate"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using Vertex AI Workbench Notebooks**, your environment is already authenticated. \n",
        "\n",
        "**If you are using Colab**, run the cell below and follow the instructions when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "In the Cloud Console, go to the [Create service account key](https://console.cloud.google.com/apis/credentials/serviceaccountkey) page.\n",
        "\n",
        "**Click Create service account**.\n",
        "\n",
        "In the **Service account name** field, enter a name, and click **Create**.\n",
        "\n",
        "In the **Grant this service account access to project** section, click the Role drop-down list. Type \"Vertex\" into the filter box, and select **Vertex Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "\n",
        "Click Create. A JSON file that contains your key downloads to your local environment.\n",
        "\n",
        "Enter the path to your service account key as the GOOGLE_APPLICATION_CREDENTIALS variable in the cell below and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcp_authenticate"
      },
      "outputs": [],
      "source": [
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# If on Vertex AI Workbench, then don't execute this code\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
        "    \"DL_ANACONDA_HOME\"\n",
        "):\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bucket:mbsdk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "When you initialize the Vertex AI SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bucket"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_bucket"
      },
      "outputs": [],
      "source": [
        "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
        "    BUCKET_NAME = PROJECT_ID + \"aip-\" + TIMESTAMP\n",
        "    BUCKET_URI = f\"gs://{BUCKET_NAME}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_bucket"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "validate_bucket"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "validate_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_vars"
      },
      "source": [
        "### Set up variables\n",
        "\n",
        "Next, set up some variables used throughout the tutorial.\n",
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "import google.cloud.aiplatform as aip\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "accelerators:training,cpu,prediction,cpu,mbsdk"
      },
      "source": [
        "#### Set hardware accelerators\n",
        "\n",
        "You can set hardware accelerators for training and prediction.\n",
        "\n",
        "Set the variables `DEPLOY_GPU/DEPLOY_NGPU` to use a container image supporting a GPU and the number of GPUs allocated to the virtual machine (VM) instance. For example, to use a GPU container image with 4 Nvidia Telsa K80 GPUs allocated to each VM, you would specify:\n",
        "\n",
        "    (aip.AcceleratorType.NVIDIA_TESLA_K80, 4)\n",
        "\n",
        "\n",
        "Otherwise specify `(None, None)` to use a container image to run on a CPU.\n",
        "\n",
        "Learn more about [hardware accelerator support for your region](https://cloud.google.com/vertex-ai/docs/general/locations#accelerators).\n",
        "\n",
        "*Note*: TF releases before 2.3 for GPU support will fail to load the custom model in this tutorial. It is a known issue and fixed in TF 2.3. This is caused by static graph ops that are generated in the serving function. If you encounter this issue on your own custom models, use a container image for TF 2.3 with GPU support."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "accelerators:training,cpu,prediction,cpu,mbsdk"
      },
      "outputs": [],
      "source": [
        "if os.getenv(\"IS_TESTING_DEPLOY_GPU\"):\n",
        "    DEPLOY_GPU, DEPLOY_NGPU = (\n",
        "        aip.gapic.AcceleratorType.NVIDIA_TESLA_K80,\n",
        "        int(os.getenv(\"IS_TESTING_DEPLOY_GPU\")),\n",
        "    )\n",
        "else:\n",
        "    DEPLOY_GPU, DEPLOY_NGPU = (None, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "container:training,prediction"
      },
      "source": [
        "#### Set pre-built containers\n",
        "\n",
        "Set the pre-built Docker container image for prediction.\n",
        "\n",
        "\n",
        "For the latest list, see [Pre-built containers for prediction](https://cloud.google.com/ai-platform-unified/docs/predictions/pre-built-containers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "container:training,prediction"
      },
      "outputs": [],
      "source": [
        "if os.getenv(\"IS_TESTING_TF\"):\n",
        "    TF = os.getenv(\"IS_TESTING_TF\")\n",
        "else:\n",
        "    TF = \"2.5\".replace(\".\", \"-\")\n",
        "\n",
        "if TF[0] == \"2\":\n",
        "    if DEPLOY_GPU:\n",
        "        DEPLOY_VERSION = \"tf2-gpu.{}\".format(TF)\n",
        "    else:\n",
        "        DEPLOY_VERSION = \"tf2-cpu.{}\".format(TF)\n",
        "else:\n",
        "    if DEPLOY_GPU:\n",
        "        DEPLOY_VERSION = \"tf-gpu.{}\".format(TF)\n",
        "    else:\n",
        "        DEPLOY_VERSION = \"tf-cpu.{}\".format(TF)\n",
        "\n",
        "DEPLOY_IMAGE = \"{}-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(\n",
        "    REGION.split(\"-\")[0], DEPLOY_VERSION\n",
        ")\n",
        "\n",
        "print(\"Deployment:\", DEPLOY_IMAGE, DEPLOY_GPU, DEPLOY_NGPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "machine:training"
      },
      "source": [
        "#### Set machine type\n",
        "\n",
        "Next, set the machine type to use for prediction.\n",
        "\n",
        "- Set the variable `DEPLOY_COMPUTE` to configure  the compute resources for the VMs you will use for for prediction.\n",
        " - `machine type`\n",
        "     - `n1-standard`: 3.75GB of memory per vCPU.\n",
        "     - `n1-highmem`: 6.5GB of memory per vCPU\n",
        "     - `n1-highcpu`: 0.9 GB of memory per vCPU\n",
        " - `vCPUs`: number of \\[2, 4, 8, 16, 32, 64, 96 \\]\n",
        "\n",
        "*Note: You may also use n2 and e2 machine types for training and deployment, but they do not support GPUs*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "machine:training"
      },
      "outputs": [],
      "source": [
        "if os.getenv(\"IS_TESTING_DEPLOY_MACHINE\"):\n",
        "    MACHINE_TYPE = os.getenv(\"IS_TESTING_DEPLOY_MACHINE\")\n",
        "else:\n",
        "    MACHINE_TYPE = \"n1-standard\"\n",
        "\n",
        "VCPU = \"4\"\n",
        "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
        "print(\"Train machine type\", DEPLOY_COMPUTE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8128b8ff025"
      },
      "source": [
        "## Get pretrained model from TensorFlow Hub\n",
        "\n",
        "For demonstration purposes, this tutorial uses a pretrained model from TensorFlow Hub (TFHub), which is then uploaded to a `Vertex AI Model` resource. Once you have a `Vertex AI Model` resource, the model can be deployed to a `Vertex AI Endpoint` resource.\n",
        "\n",
        "### Download the pretrained model\n",
        "\n",
        "First, you download the pretrained model from TensorFlow Hub. The model gets downloaded as a TF.Keras layer. To finalize the model, in this example, you create a `Sequential()` model with the downloaded TFHub model as a layer, and specify the input shape to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c55fa4c826f7"
      },
      "outputs": [],
      "source": [
        "tfhub_model = tf.keras.Sequential(\n",
        "    [hub.KerasLayer(\"https://tfhub.dev/google/imagenet/resnet_v2_101/classification/5\")]\n",
        ")\n",
        "\n",
        "tfhub_model.build([None, 224, 224, 3])\n",
        "\n",
        "tfhub_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63de49055083"
      },
      "source": [
        "### Save the model artifacts\n",
        "\n",
        "At this point, the model is in memory. Next, you save the model artifacts to a Cloud Storage location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64618c713db9"
      },
      "outputs": [],
      "source": [
        "MODEL_DIR = BUCKET_URI + \"/model\"\n",
        "tfhub_model.save(MODEL_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8ce91147c93"
      },
      "source": [
        "### Upload the TensorFlow Hub model to a `Vertex AI Model` resource\n",
        "\n",
        "Finally, you upload the model artifacts from the TFHub model into a `Vertex AI Model` resource.\n",
        "\n",
        "*Note:* When you upload the model artifacts to a `Vertex Model` resource, you specify the corresponding deployment container image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad61e1429512"
      },
      "outputs": [],
      "source": [
        "model = aip.Model.upload(\n",
        "    display_name=\"example_\" + TIMESTAMP,\n",
        "    artifact_uri=MODEL_DIR,\n",
        "    serving_container_image_uri=DEPLOY_IMAGE,\n",
        ")\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "628de0914ba1"
      },
      "source": [
        "## Creating an `Endpoint` resource\n",
        "\n",
        "You create an `Endpoint` resource using the `Endpoint.create()` method. At a minimum, you specify the display name for the endpoint. Optionally, you can specify the project and location (region); otherwise the settings are inherited by the values you set when you initialized the Vertex AI SDK with the `init()` method.\n",
        "\n",
        "In this example, the following parameters are specified:\n",
        "\n",
        "- `display_name`: A human readable name for the `Endpoint` resource.\n",
        "- `project`: Your project ID.\n",
        "- `location`: Your region.\n",
        "- `labels`: (optional) User defined metadata for the `Endpoint` in the form of key/value pairs.\n",
        "\n",
        "This method returns an `Endpoint` object.\n",
        "\n",
        "Learn more about [Vertex AI Endpoints](https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ea443f9593b"
      },
      "outputs": [],
      "source": [
        "endpoint = aip.Endpoint.create(\n",
        "    display_name=\"example_\" + TIMESTAMP,\n",
        "    project=PROJECT_ID,\n",
        "    location=REGION,\n",
        "    labels={\"your_key\": \"your_value\"},\n",
        ")\n",
        "\n",
        "print(endpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15d2ebdb1a03"
      },
      "source": [
        "### Get details on an `Endpoint` resource\n",
        "\n",
        "You can get the underlying details of an `Endpoint` object with the property `gca_resource`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddb9410f2570"
      },
      "outputs": [],
      "source": [
        "print(endpoint.gca_resource)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97a8be4c9b43"
      },
      "source": [
        "### Listing `Endpoints`\n",
        "\n",
        "The method `Endpoint.list()` will return a list of all the `Endpoint` resources for your project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fbf34c5dccf"
      },
      "outputs": [],
      "source": [
        "endpoints = aip.Endpoint.list()\n",
        "print(len(endpoints))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fe21bfa19cb"
      },
      "source": [
        "### Listing `Endpoints` using a filter\n",
        "\n",
        "You can narrow the `Endpoint` resources returned from the `list()` method using the parameter `filter`. The format for the parameter is:\n",
        "\n",
        "    filter='<endpoint-property>=<value>[AND <endpoint-property>=<value>, ...]'\n",
        "    \n",
        "In this example, you filter on a single `Endpoint` property - `display_name`.\n",
        "\n",
        "If the `list()` method returns multiple `Endpoints`, you can sort them using the parameter `order_by`. The format for the parameter is:\n",
        "\n",
        "    order_by='<endpoint-property?'\n",
        "    \n",
        "In this example, you order the returned list by the `Endpoint` property - `create_time`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dde741d7e5cd"
      },
      "outputs": [],
      "source": [
        "aip.Endpoint.list(filter=\"display_name=example_\" + TIMESTAMP, order_by=\"create_time\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca3fa3f6a894"
      },
      "source": [
        "## Deploying `Model` resources to an `Endpoint` resource.\n",
        "\n",
        "You can deploy one of more `Vertex AI Model` resource instances to the same endpoint. Each `Vertex AI Model` resource that is deployed will have its own deployment container for the serving binary. \n",
        "\n",
        "*Note:* For this example, you specified the deployment container for the TFHub model in the previous step of uploading the model artifacts to a `Vertex AI Model` resource.\n",
        "\n",
        "### Deploying a single `Endpoint` resource\n",
        "\n",
        "In the next example, you deploy a single `Vertex AI Model` resource to a `Vertex AI Endpoint` resource. The `Vertex AI Model` resource already has defined for it the deployment container image. To deploy, you specify the following additional configuration settings:\n",
        "\n",
        "- The machine type.\n",
        "- The (if any) type and number of GPUs.\n",
        "- Static, manual or auto-scaling of VM instances.\n",
        "\n",
        "In this example, you deploy the model with the minimal amount of specified parameters, as follows:\n",
        "\n",
        "- `model`: The `Model` resource.\n",
        "- `deployed_model_displayed_name`: The human readable name for the deployed model instance.\n",
        "- `machine_type`: The machine type for each VM instance.\n",
        "\n",
        "Do to the requirements to provision the resource, this may take upto a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4e93b034a72f"
      },
      "outputs": [],
      "source": [
        "response = endpoint.deploy(\n",
        "    model=model,\n",
        "    deployed_model_display_name=\"example_\" + TIMESTAMP,\n",
        "    machine_type=DEPLOY_COMPUTE,\n",
        ")\n",
        "\n",
        "print(endpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1ae5a228adb"
      },
      "source": [
        "#### Get information on the deployed model\n",
        "\n",
        "You can get the deployment settings of the deployed model from the `Endpoint` resource configuration data `gca_resource.deployed_models`. In this example, only one model is deployed -- hence the reference to the subscript `[0]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5864deb1fd90"
      },
      "outputs": [],
      "source": [
        "print(endpoint.gca_resource.deployed_models[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cf4e3ce528b"
      },
      "source": [
        "### Undeploy `Model` resource from `Endpoint` resource\n",
        "\n",
        "When a `Model` resource is deployed to an `Endpoint` resource, the deployed `Model` resource instance is assigned an ID -- commonly referred to as the deployed model ID.\n",
        "\n",
        "You can undeploy a specific `Model` resource instance with the `undeploy()` method, with the following parameters:\n",
        "\n",
        "- `deployed_model_id`: The ID assigned to the deployed model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69557a907de7"
      },
      "outputs": [],
      "source": [
        "deployed_model_id = endpoint.gca_resource.deployed_models[0].id\n",
        "print(deployed_model_id)\n",
        "\n",
        "endpoint.undeploy(deployed_model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1d2930d207c"
      },
      "source": [
        "## Configuring GPU resources and scaling\n",
        "\n",
        "In the next example, you additionally configure the deployed model for hardware accelerators (GPUs) and the number of VM instances, with the following additional parameters:\n",
        "\n",
        "- `accelerator_type`: The type of hardware accelerator (GPU).\n",
        "- `accelerator_count`: The number of hardware accelerators per VM instance.\n",
        "- `min_replica_count`: The minimum number of VM instances for auto-scaling. *Note:* Must be at least one.\n",
        "- `max_replica_count`: The maximum number of VM instances for auto-scaling.\n",
        "\n",
        "`Vertex AI Endpoints` support the following types of scaling.\n",
        "\n",
        "- Single Instance: The min and max replica count are both set to 1.\n",
        "- Manual Scaling: The min and max replica count are set to the same value greater than 1. In this case, the number of (max) VM instances are provisioned at startup and remain constant.\n",
        "- Auto Scaling: The max replica count is greater than the min replica count. On startup, the minimum number of VM instances are provisioned, and may dynamically increase up to the max replica count and decrease back down to the min replica count, depending on the load."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e976cbb58988"
      },
      "outputs": [],
      "source": [
        "MIN_NODES = 1\n",
        "MAX_NODES = 2\n",
        "\n",
        "\n",
        "response = endpoint.deploy(\n",
        "    model=model,\n",
        "    deployed_model_display_name=\"example_\" + TIMESTAMP,\n",
        "    machine_type=DEPLOY_COMPUTE,\n",
        "    accelerator_type=DEPLOY_GPU,\n",
        "    accelerator_count=DEPLOY_NGPU,\n",
        "    min_replica_count=MIN_NODES,\n",
        "    max_replica_count=MAX_NODES,\n",
        ")\n",
        "\n",
        "deployed_model_id = endpoint.gca_resource.deployed_models[0].id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "805028a58684"
      },
      "source": [
        "### Deploy multiple `Model` resources to an `Endpoint` resource\n",
        "\n",
        "An `Endpoint` resource can have multiple deployed models. When a prediction request is sent to an `Endpoint`, it will get routed to one of the deployed models, based on load balancing. All the deployed models on the same `Endpoint` resource must be homogenous -- i.e., the same input vector and output.\n",
        "\n",
        "When you deploy more than one `Model` resource to an `Endpoint`, you specific how to split the prediction traffic between the deployed `Model` resources. A common use for this is `canary rollout`, where one has a new production version of a model and incremently rolls it out -- observing that there are no negative consequences to the new model.\n",
        "\n",
        "The parameter `traffic_split` has the format:\n",
        "\n",
        "{ \"0\": percent, deploy_model_id: percent, ... }\n",
        "\n",
        "The key \"0\" refers to the model to be deployed. In this example, it will get 10 percent. Each subsequent key/value pair refers to the existing deployed models, where the key is the deployed model ID and the value is the new percentage for that model. All the values must add up to 100 (100 percent)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0abec0bf236"
      },
      "outputs": [],
      "source": [
        "response = endpoint.deploy(\n",
        "    model=model,\n",
        "    deployed_model_display_name=\"example_\" + TIMESTAMP,\n",
        "    machine_type=DEPLOY_COMPUTE,\n",
        "    traffic_split={\"0\": 10, deployed_model_id: 90},\n",
        ")\n",
        "\n",
        "print(endpoint.gca_resource.deployed_models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "968938d30a69"
      },
      "source": [
        "### Reconfigure traffic split\n",
        "\n",
        "Currently, reconfiguring the traffic split is not supported in the SDK (but will be in the future). You can dynamically reconfigure the traffic split using the GAPIC API interface. An example of dynamically reconfiguring the traffic split is for a progressive canary rollout.\n",
        "\n",
        "To do this, you:\n",
        "\n",
        "1. Create a client interface with the call `gapic.EndpointServiceClient()`\n",
        "2. Obtain the GAPIC object for the corresponding endpoint using the method `get_endpoint()`\n",
        "3. Update the traffic split in the in-memory GAPIC endpoint object.\n",
        "4. Issue the method `update_endpoint()` to dynamically update the traffic split settings on the load balancer for the `Endpoint` resource.\n",
        "\n",
        "#### Create the client interface\n",
        "\n",
        "First, you create the GAPIC client interface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a214c49d5924"
      },
      "outputs": [],
      "source": [
        "# API service endpoint\n",
        "API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
        "\n",
        "# Vertex location root path for your dataset, model and endpoint resources\n",
        "PARENT = \"projects/\" + PROJECT_ID + \"/locations/\" + REGION\n",
        "\n",
        "# client options same for all services\n",
        "client_options = {\"api_endpoint\": API_ENDPOINT}\n",
        "\n",
        "\n",
        "def create_endpoint_client():\n",
        "    client = aip.gapic.EndpointServiceClient(client_options=client_options)\n",
        "    return client\n",
        "\n",
        "\n",
        "clients = {}\n",
        "clients[\"endpoint\"] = create_endpoint_client()\n",
        "\n",
        "traffic_split = endpoint.traffic_split\n",
        "print(traffic_split)\n",
        "new_traffic_split = {}\n",
        "for key, value in traffic_split.items():\n",
        "    if value == 90:\n",
        "        value = 80\n",
        "    else:\n",
        "        value = 20\n",
        "    new_traffic_split[key] = value\n",
        "print(new_traffic_split)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bc8decee6e5"
      },
      "source": [
        "#### Update the traffic split\n",
        "\n",
        "Next, you set the new traffic split on the in-memory GAPIC endpoint, and then push it out to the load balancer on the `Endpoint` resource using the `update_endpoint()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1792092972a5"
      },
      "outputs": [],
      "source": [
        "from google.protobuf.field_mask_pb2 import FieldMask\n",
        "\n",
        "gapic_endpoint = clients[\"endpoint\"].get_endpoint(name=endpoint.resource_name)\n",
        "\n",
        "gapic_endpoint.traffic_split = new_traffic_split\n",
        "gapic_endpoint.deployed_models = []\n",
        "\n",
        "clients[\"endpoint\"].update_endpoint(\n",
        "    endpoint=gapic_endpoint, update_mask=FieldMask(paths=[\"traffic_split\"])\n",
        ")\n",
        "\n",
        "# refetch the endpoint\n",
        "gapic_endpoint = clients[\"endpoint\"].get_endpoint(name=endpoint.resource_name)\n",
        "print(gapic_endpoint.traffic_split)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6ad5bae1489"
      },
      "source": [
        "### Undeploy a single `Model` resource from an `Endpoint` resource.\n",
        "\n",
        "Next, you undeploy a single `Model` instance from an `Endpoint` resource with multiple deployed `Model` instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ee048ee9157"
      },
      "outputs": [],
      "source": [
        "model_0 = endpoint.gca_resource.deployed_models[0].id\n",
        "model_1 = endpoint.gca_resource.deployed_models[1].id\n",
        "\n",
        "endpoint.undeploy(model_0)\n",
        "print(endpoint.gca_resource.deployed_models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d89e913932a"
      },
      "source": [
        "### Undeploy all `Model` resources from an `Endpoint` resource.\n",
        "\n",
        "Finally, you can undeploy all `Model` instances from an `Endpoint` resource using the `undeploy_all()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09ff279b71ec"
      },
      "outputs": [],
      "source": [
        "endpoint.undeploy_all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64b75c0a6f8e"
      },
      "source": [
        "### Delete an `Endpoint` resource\n",
        "\n",
        "You can delete an `Endpoint` resource with the `delete()` method if the `Endpoint` resource has no deployed models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d54790378636"
      },
      "outputs": [],
      "source": [
        "endpoint.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fc5f996f599"
      },
      "source": [
        "## `Endpoint` as part of a `Vertex AI Pipeline`\n",
        "\n",
        "The next sections demonstrate how to use an `Endpoint` resource in a `Vertex AI Pipeline`.\n",
        "\n",
        "### Create `Endpoint` in pipeline\n",
        "\n",
        "In this pipeline, you create an `Endpoint` resource, and then you deploy a `Model` resource to the `Endpoint` resource. The `Model` resource  to deploy is your existing TFHub model which you previously imported as a `Model` resource. The steps are:\n",
        "\n",
        "- For pipeline parameters, pass the resource name for the existing `Model` resource.\n",
        "- Use the `GetVertexModelOp()` component to create a `VertexModel` pipeline artifact for the model.\n",
        "- Create an `Endpoint` resource.\n",
        "- Using the `VertexModel` pipeline artifact, deploy the `Model` resource to the `Endpoint` resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e17eda0f7369"
      },
      "outputs": [],
      "source": [
        "from kfp import dsl\n",
        "from kfp.v2 import compiler\n",
        "from kfp.v2.dsl import Artifact, Output, component\n",
        "\n",
        "PIPELINE_ROOT = \"{}/pipeline_root/endpoint_example\".format(BUCKET_URI)\n",
        "\n",
        "\n",
        "@dsl.pipeline(\n",
        "    name=\"create-endpoint-deploy-model\",\n",
        "    description=\"create an endpoint and deploy a model\",\n",
        ")\n",
        "def pipeline(\n",
        "    display_name: str,\n",
        "    resource_name: str,\n",
        "    project: str = PROJECT_ID,\n",
        "    region: str = REGION,\n",
        "):\n",
        "    from google_cloud_pipeline_components.experimental.evaluation import \\\n",
        "        GetVertexModelOp\n",
        "    from google_cloud_pipeline_components.v1.endpoint import (EndpointCreateOp,\n",
        "                                                              ModelDeployOp)\n",
        "\n",
        "    model = GetVertexModelOp(model_resource_name=resource_name)\n",
        "\n",
        "    endpoint_op = EndpointCreateOp(\n",
        "        project=project,\n",
        "        location=region,\n",
        "        display_name=display_name,\n",
        "    )\n",
        "\n",
        "    _ = ModelDeployOp(\n",
        "        model=model.outputs[\"model\"],\n",
        "        endpoint=endpoint_op.outputs[\"endpoint\"],\n",
        "        dedicated_resources_min_replica_count=1,\n",
        "        dedicated_resources_max_replica_count=1,\n",
        "        dedicated_resources_machine_type=DEPLOY_COMPUTE,\n",
        "    )\n",
        "\n",
        "\n",
        "try:\n",
        "    compiler.Compiler().compile(\n",
        "        pipeline_func=pipeline, package_path=\"create_endpoint_and_deploy_model.json\"\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "306485da9ea0"
      },
      "source": [
        "### Execute the pipleline\n",
        "\n",
        "Next you execute the pipeline. The pipeline takes the following parameters, which are passed as the dictionary `parameter_values`:\n",
        "\n",
        "- `display_name`: The display name for the generated Vertex AI resources.\n",
        "- `resource_name`: The resource name of the existing `Model` resource.\n",
        "- `project`: The project ID.\n",
        "- `region`: The region."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56991772e65a"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    pipeline = aip.PipelineJob(\n",
        "        display_name=\"create-endpoint-deploy-pipeline\",\n",
        "        template_path=\"create_endpoint_and_deploy_model.json\",\n",
        "        pipeline_root=PIPELINE_ROOT,\n",
        "        parameter_values={\n",
        "            \"display_name\": \"create_endpoint_and_deploy_model_\" + TIMESTAMP,\n",
        "            \"resource_name\": model.resource_name,\n",
        "            \"project\": PROJECT_ID,\n",
        "            \"region\": REGION,\n",
        "        },\n",
        "        enable_caching=False,\n",
        "    )\n",
        "\n",
        "    pipeline.run()\n",
        "\n",
        "    ! rm -f create_endpoint_and_deploy_model.json\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view_pipeline_results:custom,icn"
      },
      "source": [
        "### View the pipeline results\n",
        "\n",
        "Finally, you will view the artifact outputs of each task in the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c29a34da7df3"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "\n",
        "def print_pipeline_output(job, output_task_name):\n",
        "    PROJECT_NUMBER = job.gca_resource.name.split(\"/\")[1]\n",
        "    print(PROJECT_NUMBER)\n",
        "\n",
        "    JOB_ID = job.name\n",
        "    print(JOB_ID)\n",
        "    for _ in range(len(job.gca_resource.job_detail.task_details)):\n",
        "        TASK_ID = job.gca_resource.job_detail.task_details[_].task_id\n",
        "        EXECUTE_OUTPUT = (\n",
        "            PIPELINE_ROOT\n",
        "            + \"/\"\n",
        "            + PROJECT_NUMBER\n",
        "            + \"/\"\n",
        "            + JOB_ID\n",
        "            + \"/\"\n",
        "            + output_task_name\n",
        "            + \"_\"\n",
        "            + str(TASK_ID)\n",
        "            + \"/executor_output.json\"\n",
        "        )\n",
        "        GCP_RESOURCES = (\n",
        "            PIPELINE_ROOT\n",
        "            + \"/\"\n",
        "            + PROJECT_NUMBER\n",
        "            + \"/\"\n",
        "            + JOB_ID\n",
        "            + \"/\"\n",
        "            + output_task_name\n",
        "            + \"_\"\n",
        "            + str(TASK_ID)\n",
        "            + \"/gcp_resources\"\n",
        "        )\n",
        "        EVAL_METRICS = (\n",
        "            PIPELINE_ROOT\n",
        "            + \"/\"\n",
        "            + PROJECT_NUMBER\n",
        "            + \"/\"\n",
        "            + JOB_ID\n",
        "            + \"/\"\n",
        "            + output_task_name\n",
        "            + \"_\"\n",
        "            + str(TASK_ID)\n",
        "            + \"/evaluation_metrics\"\n",
        "        )\n",
        "        if tf.io.gfile.exists(EXECUTE_OUTPUT):\n",
        "            ! gsutil cat $EXECUTE_OUTPUT\n",
        "            return EXECUTE_OUTPUT\n",
        "        elif tf.io.gfile.exists(GCP_RESOURCES):\n",
        "            ! gsutil cat $GCP_RESOURCES\n",
        "            return GCP_RESOURCES\n",
        "        elif tf.io.gfile.exists(EVAL_METRICS):\n",
        "            ! gsutil cat $EVAL_METRICS\n",
        "            return EVAL_METRICS\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "try:\n",
        "    print(\"endpoint-create\")\n",
        "    artifacts = print_pipeline_output(pipeline, \"endpoint-create\")\n",
        "    print(\"\\n\\n\")\n",
        "    output = !gsutil cat $artifacts\n",
        "    output = json.loads(output[0])\n",
        "    endpoint_id = output[\"artifacts\"][\"endpoint\"][\"artifacts\"][0][\"metadata\"][\n",
        "        \"resourceName\"\n",
        "    ]\n",
        "    print(\"model-deploy\")\n",
        "    artifacts = print_pipeline_output(pipeline, \"model-deploy\")\n",
        "    print(\"\\n\\n\")\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8b32877af1c"
      },
      "source": [
        "#### Clean up pipeline resources\n",
        "\n",
        "Delete all the `Vertex AI` resources created for this example -- except for the `Endpoint` resource, which you use in the next example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7c9af9d370e"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    endpoint = aip.Endpoint(endpoint_id)\n",
        "    endpoint.undeploy_all()\n",
        "\n",
        "    pipeline.delete()\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aa975a64792"
      },
      "source": [
        "### Pass existing `Endpoint` into a pipeline\n",
        "\n",
        "In this pipeline, you deploy a `Model` resource to the `Endpoint` resource. The `Model` resource  to deploy is your existing TFHub model which you previously imported as a `Model` resource. The `Endpoint` resource is the existing `Endpoint` from the previous pipeline example. The steps are:\n",
        "\n",
        "- For pipeline parameters, pass the resource names and resource URIs for the existing `Model` and `Endpoint` resource.\n",
        "- Use the `importer_node()` component to create a `VertexModel` pipeline artifact for the model.\n",
        "- Use the `GetVertexModelOp()` component to create a `VertexModel` pipeline artifact for the model.\n",
        "- Using the `VertexModel` and `VertexEndpoint` pipeline artifacts, deploy the `Model` resource to the `Endpoint` resource.\n",
        "\n",
        "*Note:* This example currently blocked by internal issue: b/219835305"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "910c4046993d"
      },
      "outputs": [],
      "source": [
        "PIPELINE_ROOT = \"{}/pipeline_root/endpoint_example_2\".format(BUCKET_URI)\n",
        "\n",
        "\n",
        "# (WORKAROUND  b/219835305)\n",
        "@component(\n",
        "    base_image=\"python:3.9\",\n",
        "    packages_to_install=[\"google-cloud-aiplatform\"],\n",
        ")\n",
        "def return_unmanaged_endpoint(resource_name: str, endpoint: Output[Artifact]):\n",
        "\n",
        "    endpoint.metadata[\"resourceName\"] = resource_name\n",
        "\n",
        "\n",
        "@dsl.pipeline(\n",
        "    name=\"deploy-model-existing-endpoint\",\n",
        "    description=\"deploy a model to an existing endpoint\",\n",
        ")\n",
        "def pipeline(\n",
        "    display_name: str,\n",
        "    model_resource_name: str,\n",
        "    endpoint_resource_uri: str,\n",
        "    endpoint_resource_name: str,\n",
        "    project: str = PROJECT_ID,\n",
        "    region: str = REGION,\n",
        "):\n",
        "    from google_cloud_pipeline_components.experimental.evaluation import \\\n",
        "        GetVertexModelOp\n",
        "    from google_cloud_pipeline_components.v1.endpoint import ModelDeployOp\n",
        "\n",
        "    # Desired sequence: blocked by b/219835305\n",
        "    \"\"\"\n",
        "    from kfp.v2.components import importer_node\n",
        "    from google_cloud_pipeline_components.types import artifact_types\n",
        "    \"\"\"\n",
        "\n",
        "    model = GetVertexModelOp(model_resource_name=model_resource_name)\n",
        "\n",
        "    # Desired sequence: blocked by b/219835305\n",
        "    \"\"\"\n",
        "    endpoint = importer_node.importer(\n",
        "        artifact_uri=endpoint_resource_uri,\n",
        "        artifact_class=artifact_types.VertexEndpoint,\n",
        "        metadata={\"resourceName\": endpoint_resource_name},\n",
        "    )\n",
        "    \"\"\"\n",
        "\n",
        "    # (WORKAROUND  b/219835305)\n",
        "    endpoint = return_unmanaged_endpoint(resource_name=endpoint_resource_name)\n",
        "\n",
        "    _ = ModelDeployOp(\n",
        "        model=model.outputs[\"model\"],\n",
        "        endpoint=endpoint.outputs[\"endpoint\"],\n",
        "        dedicated_resources_min_replica_count=1,\n",
        "        dedicated_resources_max_replica_count=1,\n",
        "        dedicated_resources_machine_type=DEPLOY_COMPUTE,\n",
        "    )\n",
        "\n",
        "\n",
        "try:\n",
        "    compiler.Compiler().compile(\n",
        "        pipeline_func=pipeline, package_path=\"deploy_model_existing_endpoint.json\"\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a672bc448095"
      },
      "source": [
        "### Execute the pipleline\n",
        "\n",
        "Next you execute the pipeline. The pipeline takes the following parameters, which are passed as the dictionary `parameter_values`:\n",
        "\n",
        "- `display_name`: The display name for the generated Vertex AI resources.\n",
        "- `model_resource_name`: The resource name of the existing `Model` resource.\n",
        "- `endpoint_resource_name`: The resource name of the existing `Endpoint` resource.\n",
        "- `endpoint_resource_uri`: The resource uri of the existing `Endpoint` resource.\n",
        "- `project`: The project ID.\n",
        "- `region`: The region."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68591a3ddad4"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    pipeline = aip.PipelineJob(\n",
        "        display_name=\"deploy-model-existing-endpoint\",\n",
        "        template_path=\"deploy_model_existing_endpoint.json\",\n",
        "        pipeline_root=PIPELINE_ROOT,\n",
        "        parameter_values={\n",
        "            \"display_name\": \"deploy_model_existing_endpoint_\" + TIMESTAMP,\n",
        "            \"model_resource_name\": model.resource_name,\n",
        "            \"endpoint_resource_name\": endpoint.resource_name,\n",
        "            \"endpoint_resource_uri\": \"https://us-central1-aiplatform.googleapis.com/v1/\"\n",
        "            + endpoint.resource_name,\n",
        "            \"project\": PROJECT_ID,\n",
        "            \"region\": REGION,\n",
        "        },\n",
        "    )\n",
        "\n",
        "    pipeline.run()\n",
        "\n",
        "    ! rm -f deploy_model_existing_endpoint.json\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view_pipeline_results:custom,icn"
      },
      "source": [
        "### View the pipeline results\n",
        "\n",
        "Finally, you will view the artifact outputs of each task in the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f91799c572a8"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    print(\"model-deploy\")\n",
        "    artifacts = print_pipeline_output(pipeline, \"model-deploy\")\n",
        "    print(\"\\n\\n\")\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6b1260a969e"
      },
      "source": [
        "#### Clean up pipeline resources\n",
        "\n",
        "Delete all the `Vertex AI` resources created for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc890855b55a"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    endpoint.undeploy_all()\n",
        "    endpoint.delete()\n",
        "    pipeline.delete()\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx_vKniMq9ZX"
      },
      "outputs": [],
      "source": [
        "delete_bucket = False\n",
        "delete_model = True\n",
        "\n",
        "if delete_model:\n",
        "    try:\n",
        "        model.delete()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
        "    ! gsutil rm -rf {BUCKET_URI}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "get_started_with_vertex_endpoints.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

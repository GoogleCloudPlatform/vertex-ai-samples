{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9f40d4af2746"
   },
   "source": [
    "This notebook is a revised version of notebook from [Amy Wu and Shen Zhimo](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/matching_engine/two-tower-model-introduction.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# E2E ML on GCP: MLOps stage 6 : serving: get started with Vertex AI Matching Engine and Two Towers builtin algorithm\n",
    "\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage6/get_started_with_matching_engine_twotowers.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage6/get_started_with_matching_engine_twotowers.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/ml_ops/stage6/get_started_with_matching_engine_twotowers.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e955277e5653"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This tutorial demonstrates how to use the `Vertex AI Two-Tower` built-in algorithm with `Vertex AI Matching Engine`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this notebook, you learn how to use the `Two-Tower` builtin algorithms for generating embeddings for a dataset, for use with generating an `Matching Engine Index`, with the `Vertex AI Matching Engine` service.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services:\n",
    " \n",
    "- `Vertex AI Two-Towers` builtin algorithm\n",
    "- `Vertex AI Matching Engine`\n",
    "- `Vertex AI Batch Prediction`\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "1. Train the `Two-Tower` algorithm to generate embeddings (encoder) for the dataset.\n",
    "2. Hyperparameter tune the trained `Two-Tower` encoder.\n",
    "3. Make example predictions (embeddings) from then trained encoder.\n",
    "4. Generate embeddings using the trained `Two-Tower` builtin algorithm.\n",
    "5. Store embeddings to format supported by `Matching Engine`.\n",
    "6. Create a `Matching Engine Index` for the embeddings.\n",
    "7. Deploy the `Matching Engine Index` to a `Index Endpoint`.\n",
    "8. Make a matching engine prediction request.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "604409190dde"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "This tutorial uses the `movielens_100k sample dataset` in the public bucket `gs://cloud-samples-data/vertex-ai/matching-engine/two-tower`, which was generated from the [MovieLens movie rating dataset](https://grouplens.org/datasets/movielens/100k/). For this tutorial, the data only includes the user id feature for users, and the movie id and movie title features for movies. In this example, the user is the query object and the movie is the candidate object, and each training example in the dataset contains a user and a movie they rated (only positive ratings are included in the dataset). The two-tower model embeds the user and the movie in the same embedding space, so that given a user, the model recommends movies for the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cfa2678eec4"
   },
   "source": [
    "### Costs \n",
    "\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze4-nDLfK4pw"
   },
   "source": [
    "### Set up your local development environment\n",
    "\n",
    "**If you are using Colab or Vertex AI Workbench Notebooks**, your environment already meets\n",
    "all the requirements to run this notebook. You can skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCuSR8GkAgzl"
   },
   "source": [
    "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
    "You need the following:\n",
    "\n",
    "* The Google Cloud SDK\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* Jupyter notebook running in a virtual environment with Python 3\n",
    "\n",
    "The Google Cloud guide to [Setting up a Python development\n",
    "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
    "installation guide](https://jupyter.org/install) provide detailed instructions\n",
    "for meeting these requirements. The following steps provide a condensed set of\n",
    "instructions:\n",
    "\n",
    "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "1. [Install\n",
    "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
    "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
    "\n",
    "1. To install Jupyter, run `pip3 install jupyter` on the\n",
    "command-line in a terminal shell.\n",
    "\n",
    "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
    "\n",
    "1. Open this notebook in the Jupyter Notebook Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_aip:mbsdk"
   },
   "source": [
    "## Installation\n",
    "\n",
    "Install the packages required for executing this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install {USER_FLAG} --upgrade tensorflow -q\n",
    "! pip3 install {USER_FLAG} --upgrade google-cloud-aiplatform tensorboard-plugin-profile -q\n",
    "! gcloud components update --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzrelQZ22IZj"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
    "\n",
    "1. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    "\n",
    "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you do not know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_project_id"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0861d5cc5fd"
   },
   "source": [
    "#### Get your project number\n",
    "\n",
    "Now that the project ID is set, you get your corresponding project number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dbb8cfc8a876"
   },
   "outputs": [],
   "source": [
    "shell_output = ! gcloud projects list --filter=\"PROJECT_ID:'{PROJECT_ID}'\" --format='value(PROJECT_NUMBER)'\n",
    "PROJECT_NUMBER = shell_output[0]\n",
    "print(\"Project Number:\", PROJECT_NUMBER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
    "\n",
    "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06571eb4063b"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append it onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "697568e92bd6"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr--iN2kAylZ"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "**If you are using Vertex AI Workbench Notebooks**, your environment is already\n",
    "authenticated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "**If you are using Colab**, run the cell below and follow the instructions\n",
    "when prompted to authenticate your account via oAuth.\n",
    "\n",
    "**Otherwise**, follow these steps:\n",
    "\n",
    "1. In the Cloud Console, go to the [**Create service account key**\n",
    "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
    "\n",
    "2. Click **Create service account**.\n",
    "\n",
    "3. In the **Service account name** field, enter a name, and\n",
    "   click **Create**.\n",
    "\n",
    "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
    "into the filter box, and select\n",
    "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
    "\n",
    "5. Click *Create*. A JSON file that contains your key downloads to your\n",
    "local environment.\n",
    "\n",
    "6. Enter the path to your service account key as the\n",
    "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If on Vertex AI Workbench, then don't execute this code\n",
    "IS_COLAB = False\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
    "    \"DL_ANACONDA_HOME\"\n",
    "):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        IS_COLAB = True\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "Before you submit a training job for the two-tower model, you need to upload your training data and schema to Cloud Storage. Vertex AI trains the model using this input data. In this tutorial, the Two-Tower built-in algorithm also saves the trained model that results from your job in the same bucket. Using this model artifact, you can then create Vertex AI model and endpoint resources in order to serve online predictions.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
    "Cloud Storage buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cf221059d072"
   },
   "outputs": [],
   "source": [
    "if BUCKET_URI == \"\" or BUCKET_URI is None or BUCKET_URI == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = PROJECT_ID + \"aip-\" + TIMESTAMP\n",
    "    BUCKET_URI = \"gs://\" + BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucvCsknMCims"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "machine:training"
   },
   "source": [
    "#### Set machine type\n",
    "\n",
    "Next, set the machine type to use for prediction.\n",
    "\n",
    "- Set the variable `DEPLOY_COMPUTE` to configure  the compute resources for the VMs you will use for for prediction.\n",
    " - `machine type`\n",
    "     - `n1-standard`: 3.75GB of memory per vCPU.\n",
    "     - `n1-highmem`: 6.5GB of memory per vCPU\n",
    "     - `n1-highcpu`: 0.9 GB of memory per vCPU\n",
    " - `vCPUs`: number of \\[2, 4, 8, 16, 32, 64, 96 \\]\n",
    "\n",
    "*Note: You may also use n2 and e2 machine types for training and deployment, but they do not support GPUs*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "machine:training"
   },
   "outputs": [],
   "source": [
    "if os.getenv(\"IS_TESTING_DEPLOY_MACHINE\"):\n",
    "    MACHINE_TYPE = os.getenv(\"IS_TESTING_DEPLOY_MACHINE\")\n",
    "else:\n",
    "    MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60bd00c56cc8"
   },
   "source": [
    "## Introduction to Two-Tower algorithm\n",
    "\n",
    "Two-tower models learn to represent two items of various types (such as user profiles, search queries, web documents, answer passages, or images) in the same vector space, so that similar or related items are close to each other. These two items are referred to as the query and candidate object, since when paired with a nearest neighbor search service such as Vertex Matching Engine, the two-tower model can retrieve candidate objects related to an input query object. These objects are encoded by a query and candidate encoder (the two \"towers\") respectively, which are trained on pairs of relevant items. This built-in algorithm exports trained query and candidate encoders as model artifacts, which can be deployed in Vertex Prediction for usage in a recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgsNm8aim0Ym"
   },
   "source": [
    "### Configure training parameters for the Two-Tower builtin algorithm\n",
    "\n",
    "The following table shows parameters that are common to all Vertex AI Training jobs created using the `gcloud ai custom-jobs create` command. See the [official documentation](https://cloud.google.com/sdk/gcloud/reference/ai/custom-jobs/create) for all the possible arguments.\n",
    "\n",
    "| Parameter | Data type | Description | Required |\n",
    "|--|--|--|--|\n",
    "| `display-name` | string | Name of the job. | Yes |\n",
    "| `worker-pool-spec` | string | Comma-separated list of arguments specifying a worker pool configuration (see below). | Yes |\n",
    "| `region` | string | Region to submit the job to. | No |\n",
    "\n",
    "The `worker-pool-spec` flag can be specified multiple times, one for each worker pool. The following table shows the arguments used to specify a worker pool.\n",
    "\n",
    "| Parameter | Data type | Description | Required |\n",
    "|--|--|--|--|\n",
    "| `machine-type` | string | Machine type for the pool. See the [official documentation](https://cloud.google.com/vertex-ai/docs/training/configure-compute) for supported machines. | Yes |\n",
    "| `replica-count` | int | The number of replicas of the machine in the pool. | No |\n",
    "| `container-image-uri` | string | Docker image to run on each worker. | No |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MvQ22Sbm8lh"
   },
   "source": [
    "The following table shows the parameters for the two-tower model training job:\n",
    "\n",
    "| Parameter | Data type | Description | Required |\n",
    "|--|--|--|--|\n",
    "| `training_data_path` | string | Cloud Storage pattern where training data is stored. | Yes |\n",
    "| `input_schema_path` | string | Cloud Storage path where the JSON input schema is stored. | Yes |\n",
    "| `input_file_format` | string | The file format of input. Currently supports `jsonl` and `tfrecord`. | No - default is `jsonl`. |\n",
    "| `job_dir` | string | Cloud Storage directory where the model output files will be stored. | Yes |\n",
    "| `eval_data_path` | string | Cloud Storage pattern where eval data is stored. | No |\n",
    "| `candidate_data_path` | string | Cloud Storage pattern where candidate data is stored. Only used for top_k_categorical_accuracy metrics. If not set, it's generated from training/eval data. | No |\n",
    "| `train_batch_size` | int | Batch size for training. | No - Default is 100. |\n",
    "| `eval_batch_size` | int | Batch size for evaluation. | No - Default is 100. |\n",
    "| `eval_split` | float | Split fraction to use for the evaluation dataset, if `eval_data_path` is not provided. | No - Default is 0.2 |\n",
    "| `optimizer` | string | Training optimizer. Lowercase string name of any TF2.3 Keras optimizer is supported ('sgd', 'nadam', 'ftrl', etc.). See [TensorFlow documentation](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers). | No - Default is 'adagrad'. |\n",
    "| `learning_rate` | float | Learning rate for training. | No - Default is the default learning rate of the specified optimizer. |\n",
    "| `momentum` | float | Momentum for optimizer, if specified. | No - Default is the default momentum value for the specified optimizer. |\n",
    "| `metrics` | string | Metrics used to evaluate the model. Can be either `auc`, `top_k_categorical_accuracy` or `precision_at_1`. | No - Default is `auc`. |\n",
    "| `num_epochs` | int | Number of epochs for training. | No - Default is 10. |\n",
    "| `num_hidden_layers` | int | Number of hidden layers. | No |\n",
    "| `num_nodes_hidden_layer{index}` | int | Num of nodes in hidden layer {index}. The range of index is 1 to 20. | No |\n",
    "| `output_dim` | int | The output embedding dimension for each encoder tower of the two-tower model. | No - Default is 64. |\n",
    "| `training_steps_per_epoch` | int | Number of steps per epoch to run the training for.  Only needed if you are using more than 1 machine or using a master machine with more than 1 gpu. | No - Default is None. |\n",
    "| `eval_steps_per_epoch` | int | Number of steps per epoch to run the evaluation for.  Only needed if you are using more than 1 machine or using a master machine with more than 1 gpu. | No - Default is None. |\n",
    "| `gpu_memory_alloc` | int | Amount of memory allocated per GPU (in MB). | No - Default is no limit. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2sEfn2ZVnI_s"
   },
   "outputs": [],
   "source": [
    "DATASET_NAME = \"movielens_100k\"  # Change to your dataset name.\n",
    "\n",
    "# Change to your data and schema paths. These are paths to the movielens_100k\n",
    "# sample data.\n",
    "TRAINING_DATA_PATH = f\"gs://cloud-samples-data/vertex-ai/matching-engine/two-tower/{DATASET_NAME}/training_data/*\"\n",
    "INPUT_SCHEMA_PATH = f\"gs://cloud-samples-data/vertex-ai/matching-engine/two-tower/{DATASET_NAME}/input_schema.json\"\n",
    "\n",
    "# URI of the two-tower training Docker image.\n",
    "LEARNER_IMAGE_URI = \"us-docker.pkg.dev/vertex-ai-restricted/builtin-algorithm/two-tower\"\n",
    "\n",
    "# Change to your output location.\n",
    "OUTPUT_DIR = f\"{BUCKET_URI}/experiment/output\"\n",
    "\n",
    "TRAIN_BATCH_SIZE = 100  # Batch size for training.\n",
    "NUM_EPOCHS = 3  # Number of epochs for training.\n",
    "\n",
    "print(f\"Dataset name: {DATASET_NAME}\")\n",
    "print(f\"Training data path: {TRAINING_DATA_PATH}\")\n",
    "print(f\"Input schema path: {INPUT_SCHEMA_PATH}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Train batch size: {TRAIN_BATCH_SIZE}\")\n",
    "print(f\"Number of epochs: {NUM_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upLZ8kcankwj"
   },
   "source": [
    "### Train on Vertex AI Training with CPU\n",
    "\n",
    "Submit the Two-Tower training job to `Vertex AI Training`. The following command uses a single CPU machine for training. When using single node training, `training_steps_per_epoch` and `eval_steps_per_epoch` do not need to be set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_custom_job_machine_specification"
   },
   "source": [
    "### Prepare your machine specification\n",
    "\n",
    "Now define the machine specification for your custom hyperparameter tuning job. This tells Vertex what type of machine instance to provision for the hyperparameter tuning.\n",
    "  - `machine_type`: The type of GCP instance to provision -- e.g., n1-standard-8.\n",
    "  - `accelerator_type`: The type, if any, of hardware accelerator. In this tutorial if you previously set the variable `TRAIN_GPU != None`, you are using a GPU; otherwise you will use a CPU.\n",
    "  - `accelerator_count`: The number of accelerators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_custom_job_machine_specification"
   },
   "outputs": [],
   "source": [
    "TRAIN_COMPUTE = \"n1-standard-8\"\n",
    "machine_spec = {\"machine_type\": TRAIN_COMPUTE, \"accelerator_count\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_custom_job_disk_specification"
   },
   "source": [
    "### Prepare your disk specification\n",
    "\n",
    "(optional) Now define the disk specification for your custom hyperparameter tuning job. This tells Vertex what type and size of disk to provision in each machine instance for the hyperparameter tuning.\n",
    "\n",
    "  - `boot_disk_type`: Either SSD or Standard. SSD is faster, and Standard is less expensive. Defaults to SSD.\n",
    "  - `boot_disk_size_gb`: Size of disk in GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_custom_job_disk_specification"
   },
   "outputs": [],
   "source": [
    "DISK_TYPE = \"pd-ssd\"  # [ pd-ssd, pd-standard]\n",
    "DISK_SIZE = 200  # GB\n",
    "\n",
    "disk_spec = {\"boot_disk_type\": DISK_TYPE, \"boot_disk_size_gb\": DISK_SIZE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_custom_job_worker_pool_specification:prebuilt_container"
   },
   "source": [
    "### Define the worker pool specification\n",
    "\n",
    "Next, you define the worker pool specification for your custom hyperparameter tuning job. The worker pool specification will consist of the following:\n",
    "\n",
    "- `replica_count`: The number of instances to provision of this machine type.\n",
    "- `machine_spec`: The hardware specification.\n",
    "- `disk_spec` : (optional) The disk storage specification.\n",
    "- `container_spec`: The training container containing the training package.\n",
    "\n",
    "Let's dive deeper now into the container specification:\n",
    "\n",
    "- `image_uri`: The training image.\n",
    "- `command`: The command to invoke in the training image. Defaults to the command entry point specified for the training image.\n",
    "- `args`: The command line arguments to pass to the corresponding command entry point in training image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_custom_job_worker_pool_specification:prebuilt_container"
   },
   "outputs": [],
   "source": [
    "JOB_NAME = \"twotowers_cpu_\" + TIMESTAMP\n",
    "MODEL_DIR = \"{}/{}\".format(BUCKET_URI, JOB_NAME)\n",
    "\n",
    "CMDARGS = [\n",
    "    f\"--training_data_path={TRAINING_DATA_PATH}\",\n",
    "    f\"--input_schema_path={INPUT_SCHEMA_PATH}\",\n",
    "    f\"--job-dir={OUTPUT_DIR}\",\n",
    "    f\"--train_batch_size={TRAIN_BATCH_SIZE}\",\n",
    "    f\"--num_epochs={NUM_EPOCHS}\",\n",
    "]\n",
    "\n",
    "worker_pool_spec = [\n",
    "    {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": machine_spec,\n",
    "        \"disk_spec\": disk_spec,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": LEARNER_IMAGE_URI,\n",
    "            \"command\": [],\n",
    "            \"args\": CMDARGS,\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_custom_job:mbsdk"
   },
   "source": [
    "### Create a custom job\n",
    "\n",
    "Use the class `CustomJob` to create a custom job, such as for hyperparameter tuning, with the following parameters:\n",
    "\n",
    "- `display_name`: A human readable name for the custom job.\n",
    "- `worker_pool_specs`: The specification for the corresponding VM instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_custom_job:mbsdk"
   },
   "outputs": [],
   "source": [
    "job = aiplatform.CustomJob(\n",
    "    display_name=\"twotower_cpu_\" + TIMESTAMP, worker_pool_specs=worker_pool_spec\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "618e71e0608a"
   },
   "source": [
    "### Execute the custom job\n",
    "\n",
    "Next, execute your custom job using the method `run()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58c8ad43fa2b"
   },
   "outputs": [],
   "source": [
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88c1e8a5f20a"
   },
   "source": [
    "#### View output\n",
    "\n",
    "After the job finishes successfully, you can view the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFPLfY4gsK1V"
   },
   "outputs": [],
   "source": [
    "! gsutil ls {OUTPUT_DIR}\n",
    "\n",
    "! gsutil rm -rf {OUTPUT_DIR}/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RaIkcFT2n4_U"
   },
   "source": [
    "### Train on Vertex AI Training with GPU\n",
    "\n",
    "Next, train the Two Tower model using a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_custom_job_worker_pool_specification:prebuilt_container"
   },
   "outputs": [],
   "source": [
    "JOB_NAME = \"twotowers_gpu_\" + TIMESTAMP\n",
    "MODEL_DIR = \"{}/{}\".format(BUCKET_URI, JOB_NAME)\n",
    "\n",
    "TRAIN_COMPUTE = \"n1-highmem-4\"\n",
    "TRAIN_GPU = \"NVIDIA_TESLA_K80\"\n",
    "machine_spec = {\n",
    "    \"machine_type\": TRAIN_COMPUTE,\n",
    "    \"accelerator_type\": TRAIN_GPU,\n",
    "    \"accelerator_count\": 1,\n",
    "}\n",
    "\n",
    "CMDARGS = [\n",
    "    f\"--training_data_path={TRAINING_DATA_PATH}\",\n",
    "    f\"--input_schema_path={INPUT_SCHEMA_PATH}\",\n",
    "    f\"--job-dir={OUTPUT_DIR}\",\n",
    "    \"--training_steps_per_epoch=1500\",\n",
    "    \"--eval_steps_per_epoch=1500\",\n",
    "]\n",
    "\n",
    "worker_pool_spec = [\n",
    "    {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": machine_spec,\n",
    "        \"disk_spec\": disk_spec,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": LEARNER_IMAGE_URI,\n",
    "            \"command\": [],\n",
    "            \"args\": CMDARGS,\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_custom_job:mbsdk"
   },
   "source": [
    "### Create and execute the custom job\n",
    "\n",
    "Next, create and execute the custom job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_custom_job:mbsdk"
   },
   "outputs": [],
   "source": [
    "job = aiplatform.CustomJob(\n",
    "    display_name=\"twotower_cpu_\" + TIMESTAMP, worker_pool_specs=worker_pool_spec\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3e7969a3bbd7"
   },
   "source": [
    "#### View output\n",
    "\n",
    "After the job finishes successfully, you can view the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFPLfY4gsK1V"
   },
   "outputs": [],
   "source": [
    "! gsutil ls {OUTPUT_DIR}\n",
    "\n",
    "! gsutil rm -rf {OUTPUT_DIR}/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94tmU59YrKfe"
   },
   "source": [
    "### Train on Vertex AI Training with TFRecords\n",
    "\n",
    "Next, train the Two Tower model using TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_custom_job_worker_pool_specification:prebuilt_container"
   },
   "outputs": [],
   "source": [
    "TRAINING_DATA_PATH = f\"gs://cloud-samples-data/vertex-ai/matching-engine/two-tower/{DATASET_NAME}/tfrecord/*\"\n",
    "\n",
    "JOB_NAME = \"twotowers_tfrec_\" + TIMESTAMP\n",
    "MODEL_DIR = \"{}/{}\".format(BUCKET_URI, JOB_NAME)\n",
    "\n",
    "TRAIN_COMPUTE = \"n1-standard-8\"\n",
    "machine_spec = {\"machine_type\": TRAIN_COMPUTE, \"accelerator_count\": 0}\n",
    "\n",
    "CMDARGS = [\n",
    "    f\"--training_data_path={TRAINING_DATA_PATH}\",\n",
    "    f\"--input_schema_path={INPUT_SCHEMA_PATH}\",\n",
    "    f\"--job-dir={OUTPUT_DIR}\",\n",
    "    f\"--train_batch_size={TRAIN_BATCH_SIZE}\",\n",
    "    f\"--num_epochs={NUM_EPOCHS}\",\n",
    "    \"--input_file_format=tfrecord\",\n",
    "]\n",
    "\n",
    "worker_pool_spec = [\n",
    "    {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": machine_spec,\n",
    "        \"disk_spec\": disk_spec,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": LEARNER_IMAGE_URI,\n",
    "            \"command\": [],\n",
    "            \"args\": CMDARGS,\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_custom_job:mbsdk"
   },
   "source": [
    "### Create and execute the custom job\n",
    "\n",
    "Next, create and execute the custom job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_custom_job:mbsdk"
   },
   "outputs": [],
   "source": [
    "job = aiplatform.CustomJob(\n",
    "    display_name=\"twotower_cpu_\" + TIMESTAMP, worker_pool_specs=worker_pool_spec\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0b41e96a839b"
   },
   "source": [
    "#### View output\n",
    "\n",
    "After the job finishes successfully, you can view the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFPLfY4gsK1V"
   },
   "outputs": [],
   "source": [
    "! gsutil ls {OUTPUT_DIR}\n",
    "\n",
    "! gsutil rm -rf {OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgs0qV_Nr-RN"
   },
   "source": [
    "### Tensorboard\n",
    "\n",
    "When the training starts, you can view the logs in TensorBoard. Colab users can use the TensorBoard widget below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzCrdxgAsGll"
   },
   "source": [
    "For Workbench AI Notebooks users, the TensorBoard widget above won't work. We recommend you to launch TensorBoard through the Cloud Shell.\n",
    "\n",
    "1. In your Cloud Shell, launch Tensorboard on port 8080:\n",
    "\n",
    "    ```\n",
    "    export TENSORBOARD_DIR=gs://xxxxx/tensorboard\n",
    "    tensorboard --logdir=${TENSORBOARD_DIR} --port=8080\n",
    "    ```\n",
    "\n",
    "2. Click the \"Web Preview\" button at the top-right of the Cloud Shell window (looks like an eye in a rectangle). \n",
    "\n",
    "3. Select \"Preview on port 8080\". This should launch the TensorBoard webpage in a new tab in your browser.\n",
    "\n",
    "After the job finishes successfully, you can view the output directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8SweSrkhr_DP"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    TENSORBOARD_DIR = os.path.join(OUTPUT_DIR, \"tensorboard\")\n",
    "    %tensorboard --logdir {TENSORBOARD_DIR}\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2nRxtLTt8xn"
   },
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "You may want to optimize the hyperparameters used during training to improve your model's accuracy and performance. \n",
    "\n",
    "For this example, the following command runs a Vertex AI hyperparameter tuning job with 8 trials that attempts to maximize the validation AUC metric. The hyperparameters it optimizes are the number of hidden layers, the size of the hidden layers, and the learning rate.\n",
    "\n",
    "Learn more about [Hyperparameter tuning overview](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_hpt_job:mbsdk,vizier"
   },
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "\n",
    "hpt_job = aiplatform.HyperparameterTuningJob(\n",
    "    display_name=\"twotowers_\" + TIMESTAMP,\n",
    "    custom_job=job,\n",
    "    metric_spec={\n",
    "        \"val_auc\": \"maximize\",\n",
    "    },\n",
    "    parameter_spec={\n",
    "        \"learning_rate\": hpt.DoubleParameterSpec(min=0.0001, max=0.1, scale=\"log\"),\n",
    "        \"num_hidden_layers\": hpt.IntegerParameterSpec(min=0, max=2, scale=\"linear\"),\n",
    "        \"num_nodes_hidden_layer1\": hpt.IntegerParameterSpec(\n",
    "            min=1, max=128, scale=\"log\"\n",
    "        ),\n",
    "        \"num_nodes_hidden_layer2\": hpt.IntegerParameterSpec(\n",
    "            min=1, max=128, scale=\"log\"\n",
    "        ),\n",
    "    },\n",
    "    search_algorithm=None,\n",
    "    max_trial_count=8,\n",
    "    parallel_trial_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_hpt_job:mbsdk"
   },
   "source": [
    "## Run the hyperparameter tuning job\n",
    "\n",
    "Use the `run()` method to execute the hyperparameter tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_hpt_job:mbsdk"
   },
   "outputs": [],
   "source": [
    "hpt_job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpt_trial_results:mbsdk"
   },
   "source": [
    "### Display the hyperparameter tuning job trial results\n",
    "\n",
    "After the hyperparameter tuning job has completed, the property `trials` will return the results for each trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpt_trial_results:mbsdk"
   },
   "outputs": [],
   "source": [
    "print(hpt_job.trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "best_trial:mbsdk"
   },
   "source": [
    "### Best trial\n",
    "\n",
    "Now look at which trial was the best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "best_trial:mbsdk"
   },
   "outputs": [],
   "source": [
    "best = (None, None, None, 0.0)\n",
    "for trial in hpt_job.trials:\n",
    "    # Keep track of the best outcome\n",
    "    if float(trial.final_measurement.metrics[0].value) > best[3]:\n",
    "        try:\n",
    "            best = (\n",
    "                trial.id,\n",
    "                float(trial.parameters[0].value),\n",
    "                float(trial.parameters[1].value),\n",
    "                float(trial.final_measurement.metrics[0].value),\n",
    "            )\n",
    "        except:\n",
    "            best = (\n",
    "                trial.id,\n",
    "                float(trial.parameters[0].value),\n",
    "                None,\n",
    "                float(trial.final_measurement.metrics[0].value),\n",
    "            )\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "delete_hpt_job"
   },
   "source": [
    "### Delete the hyperparameter tuning job\n",
    "\n",
    "The method 'delete()' will delete the hyperparameter tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "delete_hpt_job"
   },
   "outputs": [],
   "source": [
    "hpt_job.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5c0ddeded6a"
   },
   "source": [
    "#### View output\n",
    "\n",
    "After the job finishes successfully, you can view the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFPLfY4gsK1V"
   },
   "outputs": [],
   "source": [
    "BEST_MODEL = OUTPUT_DIR + \"/trial_\" + best[0]\n",
    "! gsutil ls {BEST_MODEL}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gidmXBWysaeP"
   },
   "source": [
    "### Upload the model to `Vertex AI Model` resource\n",
    "\n",
    "Your training job will export two TF SavedModels under `gs://<job_dir>/query_model` and `gs://<job_dir>/candidate_model`. These exported models can be used for online or batch prediction in Vertex Prediction. \n",
    "\n",
    "First, import the query (or candidate) model using the `upload()` method, with the following parameters:\n",
    "\n",
    "- `display_name`: A human readable name for the model resource.\n",
    "- `artifact_uri`: The Cloud Storage location of the model artifacts.\n",
    "- `serving_container_image_uri`: The deployment container. In this tutorial, you use the prebuilt Two-Tower deployment container.\n",
    "- `serving_container_health_route`: The URL for the service to periodically ping for a response to verify that the serving binary is running. For Two-Towers, this will be /v1/models/\\[model_name\\].\n",
    "- `serving_container_predict_route`: The URL for the service to periodically ping for a response to verify that the serving binary is running. For Two-Towers, this will be /v1/models/\\[model_name\\]:predict.\n",
    "- `serving_container_environment_variables`: Preset environment variables to pass into the deployment container.\n",
    "\n",
    "*Note:* The underlying deployment container is built on TensorFlow Serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yEFd3Og_sbdm"
   },
   "outputs": [],
   "source": [
    "# The following imports the query (user) encoder model.\n",
    "MODEL_TYPE = \"query\"\n",
    "# Use the following instead to import the candidate (movie) encoder model.\n",
    "# MODEL_TYPE = 'candidate'\n",
    "\n",
    "DISPLAY_NAME = f\"{DATASET_NAME}_{MODEL_TYPE}\"  # The display name of the model.\n",
    "MODEL_NAME = f\"{MODEL_TYPE}_model\"  # Used by the deployment container.\n",
    "\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    artifact_uri=BEST_MODEL,\n",
    "    serving_container_image_uri=\"us-central1-docker.pkg.dev/cloud-ml-algos/two-tower/deploy\",\n",
    "    serving_container_health_route=f\"/v1/models/{MODEL_NAME}\",\n",
    "    serving_container_predict_route=f\"/v1/models/{MODEL_NAME}:predict\",\n",
    "    serving_container_environment_variables={\n",
    "        \"MODEL_BASE_PATH\": \"$(AIP_STORAGE_URI)\",\n",
    "        \"MODEL_NAME\": MODEL_NAME,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3c8c48ee2fb"
   },
   "source": [
    "## Deploy the model to `Vertex AI Endpoint`\n",
    "\n",
    "Deploying the `Vertex AI Model` resoure to a `Vertex AI Endpoint` for online predictions:\n",
    "\n",
    "1. Create an `Endpoint` resource exposing an external interface to users consuming the model. \n",
    "2. After the `Endpoint` is ready, deploy one or more instances of a model to the `Endpoint`. The deployed model runs the custom container image running Two-Tower encoder to serve embeddings.\n",
    "\n",
    "Refer to Vertex AI Predictions guide to [Deploy a model using the Vertex AI API](https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api) for more information about the APIs used in the following cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1488b536cf8"
   },
   "source": [
    "### Create a `Vertex AI Endpoint`\n",
    "\n",
    "Next, you create the `Vertex AI Endpoint`, from which you subsequently deploy your `Vertex AI Model` resource to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "er94Wp82sxYW"
   },
   "outputs": [],
   "source": [
    "endpoint = aiplatform.Endpoint.create(display_name=DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PICvm8PhqtMw"
   },
   "source": [
    "### Deploying `Model` resources to an `Endpoint` resource.\n",
    "\n",
    "You can deploy one of more `Vertex AI Model` resource instances to the same endpoint. Each `Vertex AI Model` resource that is deployed will have its own deployment container for the serving binary. \n",
    "\n",
    "In the next example, you deploy the `Vertex AI Model` resource to a `Vertex AI Endpoint` resource. The `Vertex AI Model` resource already has defined for it the deployment container image. To deploy, you specify the following additional configuration settings:\n",
    "\n",
    "- The machine type.\n",
    "- The (if any) type and number of GPUs.\n",
    "- Static, manual or auto-scaling of VM instances.\n",
    "\n",
    "In this example, you deploy the model with the minimal amount of specified parameters, as follows:\n",
    "\n",
    "- `model`: The `Model` resource.\n",
    "- `deployed_model_displayed_name`: The human readable name for the deployed model instance.\n",
    "- `machine_type`: The machine type for each VM instance.\n",
    "\n",
    "Do to the requirements to provision the resource, this may take upto a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OUEF7Yces4uD"
   },
   "outputs": [],
   "source": [
    "response = endpoint.deploy(\n",
    "    model=model,\n",
    "    deployed_model_display_name=DISPLAY_NAME,\n",
    "    machine_type=DEPLOY_COMPUTE,\n",
    "    traffic_split={\"0\": 100},\n",
    ")\n",
    "\n",
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAkHJlY7tOmu"
   },
   "source": [
    "## Creating embeddings\n",
    "\n",
    "Now that you have deployed the query/candidate encoder model on `Vertex AI Prediction`, you can call the model to generate embeddings for new data. \n",
    "\n",
    "### Make an online prediction with SDK\n",
    "\n",
    "[Online prediction](https://cloud.google.com/vertex-ai/docs/predictions/online-predictions-custom-models) is used to synchronously query a model on a small batch of instances with minimal latency. The following function calls the deployed model using Vertex AI SDK for Python.\n",
    "\n",
    "The input data you want predicted embeddings on should be provided as a stringified JSON in the `data` field. Note that you should also provide a unique `key` field (of type str) for each input instance so that you can associate each output embedding with its corresponding input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E8Wt7wYgtg_f"
   },
   "outputs": [],
   "source": [
    "# Input items for the query model:\n",
    "input_items = [\n",
    "    {\"data\": '{\"user_id\": [\"1\"]}', \"key\": \"key1\"},\n",
    "    {\"data\": '{\"user_id\": [\"2\"]}', \"key\": \"key2\"},\n",
    "]\n",
    "\n",
    "# Input items for the candidate model:\n",
    "# input_items = [{\n",
    "#     'data' : '{\"movie_id\": [\"1\"], \"movie_title\": [\"fake title\"]}',\n",
    "#     'key': 'key1'\n",
    "# }]\n",
    "\n",
    "encodings = endpoint.predict(input_items)\n",
    "print(f\"Number of encodings: {len(encodings.predictions)}\")\n",
    "print(encodings.predictions[0][\"encoding\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1k-_XJzlthfP"
   },
   "source": [
    "### Make an online prediction with `gcloud`\n",
    "\n",
    "You can also do online prediction using the gcloud CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tn5L9V0utkpA"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "request = json.dumps({\"instances\": input_items})\n",
    "with open(\"request.json\", \"w\") as writer:\n",
    "    writer.write(f\"{request}\\n\")\n",
    "\n",
    "ENDPOINT_ID = endpoint.resource_name\n",
    "\n",
    "! gcloud ai endpoints predict {ENDPOINT_ID} \\\n",
    "  --region={REGION} \\\n",
    "  --json-request=request.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocLE_U6ftnA3"
   },
   "source": [
    "### Make a batch prediction\n",
    "\n",
    "[Batch prediction](https://cloud.google.com/vertex-ai/docs/predictions/batch-predictions) is used to asynchronously make predictions on a batch of input data.  This is recommended if you have a large input size and do not need an immediate response, such as getting embeddings for candidate objects in order to create an index for a nearest neighbor search service such as [Vertex Matching Engine](https://cloud.google.com/vertex-ai/docs/matching-engine/overview).\n",
    "\n",
    "### Create the batch input file\n",
    "\n",
    "Next, you generate the batch input file to generate embeddings for the dataset, which you subsequently use to create an index with `Vertex AI Matching Engine`. In this example, the dataset contains a 1000 unique identifiers (0...999). You will use the trained encoder to generate a predicted embedding for each unique identifier.\n",
    "\n",
    "The input data needs to be on Cloud Storage and in JSONL format. You can use the sample query object file provided below. Like with online prediction, it's recommended to have the `key` field so that you can associate each output embedding with its corresponding input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ar13RZ4VtquX"
   },
   "outputs": [],
   "source": [
    "QUERY_EMBEDDING_PATH = f\"{BUCKET_URI}/embeddings/train.jsonl\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.io.gfile.GFile(QUERY_EMBEDDING_PATH, \"w\") as f:\n",
    "    for i in range(0, 1000):\n",
    "        query = {\"data\": '{\"user_id\": [\"' + str(i) + '\"]}', \"key\": f\"key{i}\"}\n",
    "        f.write(json.dumps(query) + \"\\n\")\n",
    "\n",
    "print(\"\\nNumber of embeddings: \")\n",
    "! gsutil cat {QUERY_EMBEDDING_PATH} | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "send_prediction_request:image"
   },
   "source": [
    "### Send the prediction request\n",
    "\n",
    "To make a batch prediction request, call the model object's `batch_predict` method with the following parameters: \n",
    "- `instances_format`: The format of the batch prediction request file: \"jsonl\", \"csv\", \"bigquery\", \"tf-record\", \"tf-record-gzip\" or \"file-list\"\n",
    "- `prediction_format`: The format of the batch prediction response file: \"jsonl\", \"csv\", \"bigquery\", \"tf-record\", \"tf-record-gzip\" or \"file-list\"\n",
    "- `job_display_name`: The human readable name for the prediction job.\n",
    "- `gcs_source`: A list of one or more Cloud Storage paths to your batch prediction requests.\n",
    "- `gcs_destination_prefix`: The Cloud Storage path that the service will write the predictions to.\n",
    "- `model_parameters`: Additional filtering parameters for serving prediction results.\n",
    "- `machine_type`: The type of machine to use for training.\n",
    "- `accelerator_type`: The hardware accelerator type.\n",
    "- `accelerator_count`: The number of accelerators to attach to a worker replica.\n",
    "- `starting_replica_count`: The number of compute instances to initially provision.\n",
    "- `max_replica_count`: The maximum number of compute instances to scale to. In this tutorial, only one instance is provisioned.\n",
    "\n",
    "### Compute instance scaling\n",
    "\n",
    "You can specify a single instance (or node) to process your batch prediction request. This tutorial uses a single node, so the variables `MIN_NODES` and `MAX_NODES` are both set to `1`.\n",
    "\n",
    "If you want to use multiple nodes to process your batch prediction request, set `MAX_NODES` to the maximum number of nodes you want to use. Vertex AI autoscales the number of nodes used to serve your predictions, up to the maximum number you set. Refer to the [pricing page](https://cloud.google.com/vertex-ai/pricing#prediction-prices) to understand the costs of autoscaling with multiple nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2a24d90b637d"
   },
   "outputs": [],
   "source": [
    "MIN_NODES = 1\n",
    "MAX_NODES = 4\n",
    "\n",
    "batch_predict_job = model.batch_predict(\n",
    "    job_display_name=f\"batch_predict_{DISPLAY_NAME}\",\n",
    "    gcs_source=[QUERY_EMBEDDING_PATH],\n",
    "    gcs_destination_prefix=f\"{BUCKET_URI}/embeddings/output\",\n",
    "    machine_type=DEPLOY_COMPUTE,\n",
    "    starting_replica_count=MIN_NODES,\n",
    "    max_replica_count=MAX_NODES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "get_batch_prediction:mbsdk,custom,lrg"
   },
   "source": [
    "### Get the predicted embeddings\n",
    "\n",
    "Next, get the results from the completed batch prediction job.\n",
    "\n",
    "The results are written to the Cloud Storage output bucket you specified in the batch prediction request. You call the method `iter_outputs()` to get a list of each Cloud Storage file generated with the results. Each file contains one or more prediction requests in a JSON format:\n",
    "\n",
    "- `instance`: The prediction request.\n",
    "- `prediction`: The prediction response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "get_batch_prediction:mbsdk,custom,lrg"
   },
   "outputs": [],
   "source": [
    "bp_iter_outputs = batch_predict_job.iter_outputs()\n",
    "\n",
    "prediction_results = list()\n",
    "for blob in bp_iter_outputs:\n",
    "    if blob.name.split(\"/\")[-1].startswith(\"prediction\"):\n",
    "        prediction_results.append(blob.name)\n",
    "\n",
    "result_files = []\n",
    "for prediction_result in prediction_results:\n",
    "    result_file = f\"gs://{bp_iter_outputs.bucket.name}/{prediction_result}\"\n",
    "    result_files.append(result_file)\n",
    "\n",
    "print(result_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQIQSyF9GtSv"
   },
   "source": [
    "#### Save the embeddings in JSONL format\n",
    "\n",
    "Next, you store the predicted embeddings as a JSONL formatted file. Each embedding is stored as:\n",
    "\n",
    "    { 'id': .., 'embedding': [ ... ] }\n",
    "    \n",
    "The format of the embeddings for the index can be in either CSV, JSON, or Avro format.\n",
    "\n",
    "Learn more about [Embedding Formats for Indexing](https://cloud.google.com/vertex-ai/docs/matching-engine/using-matching-engine#json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dd9bde3980fa"
   },
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for result_file in result_files:\n",
    "\n",
    "    with tf.io.gfile.GFile(result_file, \"r\") as f:\n",
    "        instances = list(f)\n",
    "\n",
    "    for instance in instances:\n",
    "        instance = instance.replace('\\\\\"', \"'\")\n",
    "        result = json.loads(instance)\n",
    "        prediction = result[\"prediction\"]\n",
    "        key = prediction[\"key\"][3:]\n",
    "        encoding = prediction[\"encoding\"]\n",
    "\n",
    "        embedding = {\"id\": key, \"embedding\": encoding}\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "print(\"Number of embeddings\", len(embeddings))\n",
    "print(\"Encoding Dimensions\", len(embeddings[0][\"embedding\"]))\n",
    "print(\"Example embedding\", embeddings[0])\n",
    "\n",
    "with open(\"embeddings.json\", \"w\") as f:\n",
    "    for i in range(len(embeddings)):\n",
    "        f.write(json.dumps(embeddings[i]).replace('\"', \"'\"))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "! head -n 2 embeddings.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QuVl8DrWG8NS"
   },
   "source": [
    "#### Store the JSONL formatted embeddings in Cloud Storage\n",
    "\n",
    "Next, you upload the training data to your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3PgsA_vbI8Vg"
   },
   "outputs": [],
   "source": [
    "EMBEDDINGS_URI = f\"{BUCKET_URI}/embeddings/twotower/\"\n",
    "! gsutil cp embeddings.json {EMBEDDINGS_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhIBCQ7dDSbW"
   },
   "source": [
    "### Create Matching Engine Index\n",
    "\n",
    "Next, you create the index for your embeddings. Currently, two indexing algorithms are supported:\n",
    "\n",
    "- `create_tree_ah_index()`:  Shallow tree + Asymmetric hashing.\n",
    "- `create_brute_force_index()`: Linear search.\n",
    "\n",
    "In this tutorial, you use the `create_tree_ah_index()`for production scale. The method is called with the following parameters:\n",
    "\n",
    "- `display_name`: A human readable name for the index.\n",
    "- `contents_delta_uri`: A Cloud Storage location for the embeddings, which are either to be inserted, updated or deleted.\n",
    "- `dimensions`: The number of dimensions of the input vector\n",
    "- `approximate_neighbors_count`: (for Tree AH) The default number of neighbors to find via approximate search before exact reordering is performed. Exact reordering is a procedure where results returned by an approximate search algorithm are reordered via a more expensive distance computation.\n",
    "- `distance_measure_type`: The distance measure used in nearest neighbor search.\n",
    "    - `SQUARED_L2_DISTANCE`: Euclidean (L2) Distance\n",
    "    - `L1_DISTANCE`: Manhattan (L1) Distance\n",
    "    - `COSINE_DISTANCE`: Cosine Distance. Defined as 1 - cosine similarity.\n",
    "    - `DOT_PRODUCT_DISTANCE`: Default value. Defined as a negative of the dot product.\n",
    "- `description`: A human readble description of the index.\n",
    "- `labels`: User metadata in the form of a dictionary.\n",
    "- `leaf_node_embedding_count`: Number of embeddings on each leaf node. The default value is 1000 if not set.\n",
    "- `leaf_nodes_to_search_percent`: The default percentage of leaf nodes that any query may be searched. Must be in range 1-100, inclusive. The default value is 10 (means 10%) if not set.\n",
    "\n",
    "This may take upto 30 minutes.\n",
    "\n",
    "Learn more about [Configuring Matching Engine Indexes](https://cloud.google.com/vertex-ai/docs/matching-engine/configuring-indexes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xzY7TpUSJcTV"
   },
   "outputs": [],
   "source": [
    "DIMENSIONS = len(embeddings[0][\"embedding\"])\n",
    "DISPLAY_NAME = \"movies\"\n",
    "\n",
    "tree_ah_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    contents_delta_uri=EMBEDDINGS_URI,\n",
    "    dimensions=DIMENSIONS,\n",
    "    approximate_neighbors_count=50,\n",
    "    distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\n",
    "    description=\"Two tower generated embeddings\",\n",
    "    labels={\"label_name\": \"label_value\"},\n",
    "    # TreeAH specific parameters\n",
    "    leaf_node_embedding_count=100,\n",
    "    leaf_nodes_to_search_percent=7,\n",
    ")\n",
    "\n",
    "INDEX_RESOURCE_NAME = tree_ah_index.resource_name\n",
    "print(INDEX_RESOURCE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4f4f0bc64ddb"
   },
   "source": [
    "## Setup VPC peering network\n",
    "\n",
    "To use a `Matching Engine Index`, you setup a VPC peering network between your project and the `Vertex AI Matching Engine` service project. This eliminates additional hops in network traffic and allows using efficient gRPC protocol.\n",
    "\n",
    "Learn more about [VPC peering](https://cloud.google.com/vertex-ai/docs/general/vpc-peering).\n",
    "\n",
    "**IMPORTANT: you can only setup one VPC peering to servicenetworking.googleapis.com per project.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d85e8f48291a"
   },
   "source": [
    "### Create VPC peering for default network\n",
    "\n",
    "For simplicity, we setup VPC peering to the default network. You can create a different network for your project.\n",
    "\n",
    "If you setup VPC peering with any other network, make sure that the network already exists and that your VM is running on that network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a107544fbabf"
   },
   "outputs": [],
   "source": [
    "# This is for display only; you can name the range anything.\n",
    "PEERING_RANGE_NAME = \"vertex-ai-prediction-peering-range\"\n",
    "NETWORK = \"default\"\n",
    "\n",
    "# NOTE: `prefix-length=16` means a CIDR block with mask /16 will be\n",
    "# reserved for use by Google services, such as Vertex AI.\n",
    "! gcloud compute addresses create $PEERING_RANGE_NAME \\\n",
    "  --global \\\n",
    "  --prefix-length=16 \\\n",
    "  --description=\"peering range for Google service\" \\\n",
    "  --network=$NETWORK \\\n",
    "  --purpose=VPC_PEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8e29cad1a0be"
   },
   "source": [
    "### Create the VPC connection\n",
    "\n",
    "Next, create the connection for VPC peering.\n",
    "\n",
    "*Note:* If you get a PERMISSION DENIED, you may not have the neccessary role 'Compute Network Admin' set for your default service account. In the Cloud Console, do the following steps.\n",
    "\n",
    "1. Goto `IAM & Admin`\n",
    "2. Find your service account.\n",
    "3. Click edit icon.\n",
    "4. Select `Add Another Role`.\n",
    "5. Enter 'Compute Network Admin'.\n",
    "6. Select `Save`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3f6c85ffc63"
   },
   "outputs": [],
   "source": [
    "! gcloud services vpc-peerings connect \\\n",
    "  --service=servicenetworking.googleapis.com \\\n",
    "  --network=$NETWORK \\\n",
    "  --ranges=$PEERING_RANGE_NAME \\\n",
    "  --project=$PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "944d772b1397"
   },
   "source": [
    "Check the status of your peering connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b946ce37cc16"
   },
   "outputs": [],
   "source": [
    "! gcloud compute networks peerings list --network $NETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a5e1b83ae61"
   },
   "source": [
    "#### Construct the full network name\n",
    "\n",
    "You need to have the full network resource name when you subsequently create an `Matching Engine Index Endpoint` resource for VPC peering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cd58eb809f71"
   },
   "outputs": [],
   "source": [
    "full_network_name = f\"projects/{PROJECT_NUMBER}/global/networks/{NETWORK}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qV2xjAnDDObD"
   },
   "source": [
    "### Create an IndexEndpoint with VPC Network\n",
    "\n",
    "Next, you create a `Matching Engine Index Endpoint`, similar to the concept of creating a `Private Endpoint` for prediction with a peer-to-peer network.\n",
    "\n",
    "To create the `Index Endpoint` resource, you call the method `create()` with the following parameters:\n",
    "\n",
    "- `display_name`: A human readable name for the `Index Endpoint`.\n",
    "- `description`: A description for the `Index Endpoint`.\n",
    "- `network`: The VPC network resource name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QuARXzJVGyQX"
   },
   "outputs": [],
   "source": [
    "index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "    display_name=\"index_endpoint_for_demo\",\n",
    "    description=\"index endpoint description\",\n",
    "    network=full_network_name,\n",
    ")\n",
    "\n",
    "INDEX_ENDPOINT_NAME = index_endpoint.resource_name\n",
    "print(INDEX_ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Ew1UgcIIiJG"
   },
   "source": [
    "### Deploy the `Matching Engine Index` to the `Index Endpoint` resource\n",
    "\n",
    "Next, deploy your index to the `Index Endpoint` using the method `deploy_index()` with the following parameters:\n",
    "\n",
    "- `display_name`: A human readable name for the deployed index.\n",
    "- `index`: Your index.\n",
    "- `deployed_index_id`: A user assigned identifier for the deployed index.\n",
    "- `machine_type`: (optional) The VM instance type.\n",
    "- `min_replica_count`: (optional) Minimum number of VM instances for auto-scaling.\n",
    "- `max_replica_count`: (optional) Maximum number of VM instances for auto-scaling.\n",
    "\n",
    "Learn more about [Machine resources for Index Endpoint](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.indexEndpoints#DeployedIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_uK4WOgqN1NG"
   },
   "outputs": [],
   "source": [
    "DEPLOYED_INDEX_ID = \"tree_ah_twotower_deployed_\" + TIMESTAMP\n",
    "\n",
    "MIN_NODES = 1\n",
    "MAX_NODES = 2\n",
    "DEPLOY_COMPUTE = \"n1-standard-16\"\n",
    "\n",
    "index_endpoint.deploy_index(\n",
    "    display_name=\"deployed_index_for_demo\",\n",
    "    index=tree_ah_index,\n",
    "    deployed_index_id=DEPLOYED_INDEX_ID,\n",
    "    machine_type=DEPLOY_COMPUTE,\n",
    "    min_replica_count=MIN_NODES,\n",
    "    max_replica_count=MAX_NODES,\n",
    ")\n",
    "\n",
    "print(index_endpoint.deployed_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LCGvBNvBd8D"
   },
   "source": [
    "### Create and execute an online query\n",
    "\n",
    "Now that your index is deployed, you can make queries.\n",
    "\n",
    "First, you construct a vector `query` using synthetic data, to use as the example to return matches for.\n",
    "\n",
    "Next, you make the matching request using the method `match()`, with the following parameters:\n",
    "\n",
    "- `deployed_index_id`:  The identifier of the deployed index.\n",
    "- `queries`: A list of queries (instances).\n",
    "- `num_neighbors`: The number of closest matches to return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0549c26c8b3c"
   },
   "outputs": [],
   "source": [
    "# The number of nearest neighbors to be retrieved from database for each query.\n",
    "NUM_NEIGHBOURS = 10\n",
    "\n",
    "# Test query\n",
    "queries = [embeddings[0][\"embedding\"], embeddings[1][\"embedding\"]]\n",
    "\n",
    "matches = index_endpoint.match(\n",
    "    deployed_index_id=DEPLOYED_INDEX_ID, queries=queries, num_neighbors=NUM_NEIGHBOURS\n",
    ")\n",
    "\n",
    "for instance in matches:\n",
    "    print(\"INSTANCE\")\n",
    "    for match in instance:\n",
    "        print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx_vKniMq9ZX"
   },
   "outputs": [],
   "source": [
    "# Delete endpoint resource\n",
    "endpoint.delete(force=True)\n",
    "\n",
    "# Delete model resource\n",
    "model.delete()\n",
    "\n",
    "# Force undeployment of indexes and delete endpoint\n",
    "try:\n",
    "    index_endpoint.delete(force=True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Delete indexes\n",
    "try:\n",
    "    tree_ah_index.delete()\n",
    "    brute_force_index.delete()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Delete Cloud Storage objects that were created\n",
    "delete_bucket = False\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil -m rm -r $OUTPUT_DIR"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "get_started_with_matching_engine_twotowers.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

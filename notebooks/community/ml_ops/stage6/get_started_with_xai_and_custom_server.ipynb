{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dd626314fed"
      },
      "source": [
        "# E2E ML on GCP: MLOps stage 6 : serving: get started with Vertex Explainable AI using custom deployment container\n",
        "\n",
        "This is an updated version of a Colab notebook contributed by [Brian Kang and Siping Hu](https://colab.corp.google.com/drive/1aYERnouogPXqlCHlfDRCff04BMV1JpyE?resourcekey=0-BrkuuARc--pA7CvD5LS-oQ#scrollTo=cuKvd9SrmIQw).\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage6/get_started_with_xai_and_custom_server.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "        <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage6/get_started_with_xai_and_custom_server.ipynb\">\n",
        "        <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\\\" alt=\"Colab logo\"> Run in Colab\n",
        "        </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/ml_ops/stage6/get_started_with_xai_and_custom_server.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "<br/><br/><br/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "424e4efb7a8d"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. This tutorial covers stage 6 : serving: get started with Explainable AI for a custom deployment container."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn to build a custom container to serve a PyTorch model on `Vertex AI Endpoint`. You use the FastAPI Python web server framework to create the HTTP server for the serving binary. You then push the container to `Artifact Registry`, deploy the model and make predictions and explanations requests.\n",
        "\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services:\n",
        "\n",
        "- `Vertex AI Prediction`\n",
        "- `Vertex Explainable AI`\n",
        "- `Google Artifact Registry`\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Locally train a Pytorch tabular classifier.\n",
        "- Locally test the trained model.\n",
        "- Build a HTTP server using FastAPI.\n",
        "- Create a custom serving container with the trained model and FastAPI server.\n",
        "- Locally test the custom serving container.\n",
        "- Push the custom serving container to the Artifact Registry.\n",
        "- Upload the custom serving container as a `Model` resource.\n",
        "- Deploy the `Model` resource to an `Endpoint` resource.\n",
        "- Make a prediction request to the deployed custom serving container.\n",
        "- Make an explanation request to the deployed custom serving container."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c2f46b60759"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset used for this tutorial is the [Iris dataset](https://scikit-learn.org/stable/datasets/index.html#iris-dataset) from [Scikit-Learn Datasets](https://scikit-learn.org/stable/datasets/). This dataset does not require any feature engineering. The trained model predicts the type of Iris flower species from a class of three species: setosa, virginica, or versicolor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a61ca6d7eb5c"
      },
      "source": [
        "### Costs \n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing), and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze4-nDLfK4pw"
      },
      "source": [
        "### Set up your local development environment\n",
        "\n",
        "**If you are using Colab or Google Cloud Notebooks**, your environment already meets\n",
        "all the requirements to run this notebook. You can skip this step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCuSR8GkAgzl"
      },
      "source": [
        "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
        "You need the following:\n",
        "\n",
        "* Docker\n",
        "* Git\n",
        "* Google Cloud SDK (gcloud)\n",
        "* Python 3\n",
        "* virtualenv\n",
        "* Jupyter notebook running in a virtual environment with Python 3\n",
        "\n",
        "The Google Cloud guide to [Setting up a Python development\n",
        "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
        "installation guide](https://jupyter.org/install) provide detailed instructions\n",
        "for meeting these requirements. The following steps provide a condensed set of\n",
        "instructions:\n",
        "\n",
        "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
        "\n",
        "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
        "\n",
        "1. [Install\n",
        "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
        "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
        "\n",
        "1. To install Jupyter, run `pip install jupyter` on the\n",
        "command-line in a terminal shell.\n",
        "\n",
        "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
        "\n",
        "1. Open this notebook in the Jupyter Notebook Dashboard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_mlops"
      },
      "source": [
        "## Installations\n",
        "\n",
        "Install the packages required for executing this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd7b50e225c3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Vertex AI Workbench Notebook product has specific requirements\n",
        "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
        "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
        "    \"/opt/deeplearning/metadata/env_version\"\n",
        ")\n",
        "\n",
        "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_WORKBENCH_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\"\n",
        "\n",
        "! pip3 install joblib {USER_FLAG} -q\n",
        "! pip3 install numpy {USER_FLAG} -q\n",
        "! pip3 install scikit-learn {USER_FLAG} -q\n",
        "! pip3 install torch {USER_FLAG} -q\n",
        "! pip3 install \"uvicorn[standard]>=0.12.0,<0.14.0\" fastapi~=0.63 {USER_FLAG} -q\n",
        "! pip3 install --upgrade google-cloud-aiplatform {USER_FLAG} -q\n",
        "! pip3 install --upgrade google-cloud-storage {USER_FLAG} -q\n",
        "! pip3 install --upgrade python-tabulate $USER_FLAG -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhq5zEbGg0XX"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzrelQZ22IZj"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWEdiXsJg0XY"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF1j6f9HApxa"
      },
      "source": [
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "1. [Enable the Vertex AI API and Compute Engine API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component).\n",
        "\n",
        "1. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
        "\n",
        "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` or `%` as shell commands, and it interpolates Python variables with `$` or `{}` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WReHDGG5g0XY"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM1iC_MfAts1"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_project_id"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID:\", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_gcloud_project_id"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
        "\n",
        "- Americas: `us-central1`\n",
        "- Europe: `europe-west4`\n",
        "- Asia Pacific: `asia-east1`\n",
        "\n",
        "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
        "\n",
        "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "region"
      },
      "outputs": [],
      "source": [
        "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
        "\n",
        "if REGION == \"[your-region]\":\n",
        "    REGION = \"us-central1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "timestamp"
      },
      "source": [
        "#### Timestamp\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "timestamp"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ffa6b6c7cdb"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using Vertex AI Workbench Notebooks**, your environment is already authenticated. Skip this step.\n",
        "\n",
        "**If you are using Colab**, run the cell below and follow the instructions when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "In the Cloud Console, go to the [Create service account key](https://console.cloud.google.com/apis/credentials/serviceaccountkey) page.\n",
        "\n",
        "1. **Click Create service account**.\n",
        "\n",
        "2. In the **Service account name** field, enter a name, and click **Create**.\n",
        "\n",
        "3. In the **Grant this service account access to project** section, click the Role drop-down list. Type \"Vertex AI\" into the filter box, and select **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "\n",
        "4. Click Create. A JSON file that contains your key downloads to your local environment.\n",
        "\n",
        "5. Enter the path to your service account key as the GOOGLE_APPLICATION_CREDENTIALS variable in the cell below and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b72272258fc"
      },
      "outputs": [],
      "source": [
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# If on Vertex AI Workbench, then don't execute this code\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
        "    \"DL_ANACONDA_HOME\"\n",
        "):\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bucket:mbsdk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "When you initialize the Vertex SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bucket"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_bucket"
      },
      "outputs": [],
      "source": [
        "if BUCKET_URI == \"\" or BUCKET_URI is None or BUCKET_URI == \"gs://[your-bucket-name]\":\n",
        "    BUCKET_NAME = PROJECT_ID + \"aip-\" + TIMESTAMP\n",
        "    BUCKET_URI = \"gs://\" + BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_bucket"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "validate_bucket"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "validate_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_vars"
      },
      "source": [
        "### Set up variables\n",
        "\n",
        "Next, set up some variables used throughout the tutorial.\n",
        "\n",
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "import google.cloud.aiplatform as aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "### Configure project and resource names\n",
        "\n",
        "`IMAGE` - Name of the container image that will be pushed.\n",
        "\n",
        "`MODEL_DISPLAY_NAME` - Display name of Vertex AI Model resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzGDU7TWdts_"
      },
      "outputs": [],
      "source": [
        "IMAGE = \"xai-fastapi-server\"  # @param {type:\"string\"}\n",
        "MODEL_DISPLAY_NAME = \"xai-fastapi-custom-container\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gar_enable_api"
      },
      "source": [
        "### Enable Artifact Registry API\n",
        "\n",
        "First, you must enable the Artifact Registry API service for your project.\n",
        "\n",
        "Learn more about [Enabling service](https://cloud.google.com/artifact-registry/docs/enable-service)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gar_enable_api"
      },
      "outputs": [],
      "source": [
        "! gcloud services enable artifactregistry.googleapis.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gar_create_repo"
      },
      "source": [
        "### Create a private Docker repository\n",
        "\n",
        "Your first step is to create your own Docker repository in Google Artifact Registry.\n",
        "\n",
        "1. Run the `gcloud artifacts repositories create` command to create a new Docker repository with your region with the description \"docker repository\".\n",
        "\n",
        "2. Run the `gcloud artifacts repositories list` command to verify that your repository was created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gar_create_repo"
      },
      "outputs": [],
      "source": [
        "PRIVATE_REPO = \"my-docker-repo\"\n",
        "\n",
        "! gcloud artifacts repositories create {PRIVATE_REPO} --repository-format=docker --location={REGION} --description=\"Docker repository\"\n",
        "\n",
        "! gcloud artifacts repositories list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gar_auth"
      },
      "source": [
        "### Configure authentication to your private repo\n",
        "\n",
        "Before you push or pull container images, configure Docker to use the `gcloud` command-line tool to authenticate requests to `Artifact Registry` for your region."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gar_auth"
      },
      "outputs": [],
      "source": [
        "! gcloud auth configure-docker {REGION}-docker.pkg.dev --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b816cd52f4b"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Next, you create the training scripts for the model, and then train the model locally.\n",
        "\n",
        "### Scripts\n",
        "\n",
        "You create the following scripts:\n",
        "\n",
        "- `data.py`: Returns the preprocessed training data.\n",
        "- `model.py`: Returns the model architecture to train.\n",
        "- `train.py`: Returns the trained model.\n",
        "- `server.py`: Creates the model server.\n",
        "- `main.py`: Creates the HTTP server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e74556ea0b4"
      },
      "outputs": [],
      "source": [
        "%mkdir app"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "269be534fab2"
      },
      "source": [
        "### Create the data preprocessing script\n",
        "\n",
        "Next, you create the script `data.py` to get the dataset and preprocess the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MySs9l18mIQn"
      },
      "outputs": [],
      "source": [
        "%%writefile app/data.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def get_data():\n",
        "    # Dataset\n",
        "    iris = load_iris()\n",
        "    X = iris['data']\n",
        "    y = iris['target']\n",
        "    names = iris['target_names']\n",
        "    feature_names = iris['feature_names']\n",
        "\n",
        "    # Scale data to have mean 0 and variance 1 \n",
        "    # which is importance for convergence of the neural network\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Split the data set into training and testing\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_scaled, y, test_size=0.2, random_state=2)\n",
        "    \n",
        "    return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fca78723a98"
      },
      "source": [
        "Test the script locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74d91f8a330b"
      },
      "outputs": [],
      "source": [
        "! python3 app/data.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15f228bf14e7"
      },
      "source": [
        "### Create the get the model architecture script\n",
        "\n",
        "Next, you create the script that returns the model architecture to train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jb8K8TI8mIQn"
      },
      "outputs": [],
      "source": [
        "%%writefile app/model.py\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "# Build model\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Model, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 50)\n",
        "        self.layer2 = nn.Linear(50, 50)\n",
        "        self.layer3 = nn.Linear(50, 3)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        x = F.softmax(self.layer3(x), dim=1)\n",
        "        return x  \n",
        "    \n",
        "def get_model(X_train):\n",
        "    model = Model(X_train.shape[1])\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    loss_fn   = nn.CrossEntropyLoss()\n",
        "    print(model)\n",
        "    return model, optimizer, loss_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "584620fa3554"
      },
      "source": [
        "Test the script locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1311d9e7b9d3"
      },
      "outputs": [],
      "source": [
        "! python3 app/model.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9db8bddfa2f0"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "Next, you create the script to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jCpXDQ6mIQo"
      },
      "outputs": [],
      "source": [
        "%%writefile app/train.py\n",
        "import tqdm\n",
        "import os\n",
        "import numpy as np\n",
        "from data import get_data\n",
        "from model import get_model\n",
        "\n",
        "X_train, X_test, y_train, y_test = get_data()\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Train and save the model\n",
        "EPOCHS  = 100\n",
        "X_train = Variable(torch.from_numpy(X_train)).float()\n",
        "y_train = Variable(torch.from_numpy(y_train)).long()\n",
        "X_test  = Variable(torch.from_numpy(X_test)).float()\n",
        "y_test  = Variable(torch.from_numpy(y_test)).long()\n",
        "\n",
        "model, optimizer, loss_fn = get_model(X_train)\n",
        "\n",
        "loss_list = np.zeros((EPOCHS,))\n",
        "accuracy_list = np.zeros((EPOCHS,))\n",
        "\n",
        "for epoch in tqdm.trange(EPOCHS):\n",
        "    y_pred = model(X_train)\n",
        "    loss = loss_fn(y_pred, y_train)\n",
        "    loss_list[epoch] = loss.item()\n",
        "    \n",
        "    # Zero gradients\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        y_pred = model(X_test)\n",
        "        correct = (torch.argmax(y_pred, dim=1) == y_test).type(torch.FloatTensor)\n",
        "        accuracy_list[epoch] = correct.mean()\n",
        "\n",
        "# Save the model to a checkpoint\n",
        "print('Saving..')\n",
        "state = {\n",
        "    'net': model.state_dict(),\n",
        "}\n",
        "if not os.path.isdir('app'):\n",
        "    os.mkdir('app')\n",
        "torch.save(state, './app/model.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d00985dcb64c"
      },
      "source": [
        "Test the script locally. This will train the model and store the model artifacts in `app/model.pth`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "008d9fd058c8"
      },
      "outputs": [],
      "source": [
        "! python3 app/train.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d79932fd90be"
      },
      "source": [
        "### Create the model server\n",
        "\n",
        "Next, you create the script for serving the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zTr509nmIQo"
      },
      "outputs": [],
      "source": [
        "%%writefile app/server.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import os\n",
        "\n",
        "from model import Model\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import os\n",
        "\n",
        "class IrisClassifier:\n",
        "    def __init__(self, model_artifact):\n",
        "        self.net = Model(4)\n",
        "        self.checkpoint = torch.load(model_artifact)\n",
        "        self.net.load_state_dict(self.checkpoint['net'])\n",
        "        self.net.eval()\n",
        "        self.iris_type = {\n",
        "            0: 'setosa',\n",
        "            1: 'versicolor',\n",
        "            2: 'virginica'\n",
        "        }\n",
        "        \n",
        "    def predict(self, features:dict):\n",
        "        X = [features['sepal_length'], features['sepal_width'], features['petal_length'], features['petal_width']]\n",
        "        X = torch.tensor(X)\n",
        "        X  = torch.unsqueeze(X, 0)\n",
        "        with torch.no_grad():\n",
        "            output = self.net(X)\n",
        "            prob, clas =  output.max(1)\n",
        "\n",
        "        return {'class': self.iris_type[int(clas.cpu().detach().numpy()[0])],\n",
        "                'probability': float(prob.cpu().detach().numpy()[0])}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e1d823cec63"
      },
      "source": [
        "Test the script locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f037ed613e3"
      },
      "outputs": [],
      "source": [
        "! python3 app/server.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "465f90d67387"
      },
      "source": [
        "### Test executing the model server locally\n",
        "\n",
        "Next, test the model server by instantiating the model server and making local prediction requests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYuRVgB5mIQp"
      },
      "outputs": [],
      "source": [
        "%%writefile app/test.py\n",
        "import torch\n",
        "from server import IrisClassifier\n",
        "model = IrisClassifier('./app/model.pth')\n",
        "\n",
        "# The features are scaled\n",
        "pred1 = model.predict(features={\"sepal_length\": -1.38535265, \"sepal_width\": 0.32841405,\n",
        "                                \"petal_length\": -1.39706395, \"petal_width\": 1.3154443})\n",
        "pred2  = model.predict(features={\"sepal_length\": -1.02184904, \"sepal_width\": -2.43394714,\n",
        "                                 \"petal_length\": -0.14664056, \"petal_width\": -0.26238682})\n",
        "print(pred1)\n",
        "print(pred2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1e8ad052ce0"
      },
      "outputs": [],
      "source": [
        "! python3 app/test.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "480a1d88ecdb"
      },
      "source": [
        "### Build a FastAPI HTTP server\n",
        "\n",
        "Finally, you will need an HTTP server in the deployment container to handle the `predict` and `health` requests. You build the HTTP server using FastAPI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94af0ba5eadd"
      },
      "outputs": [],
      "source": [
        "%%writefile app/main.py\n",
        "from fastapi import FastAPI, Request\n",
        "from starlette.responses import JSONResponse\n",
        "\n",
        "import joblib\n",
        "import json\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "from google.cloud import storage\n",
        "from server import *\n",
        "\n",
        "\n",
        "app = FastAPI()\n",
        "'''\n",
        "gcs_client = storage.Client()\n",
        "\n",
        "with open(\"model.joblib\", 'wb') as model_f:\n",
        "    gcs_client.download_blob_to_file(\n",
        "        f\"{os.environ['AIP_STORAGE_URI']}/model.joblib\", model_f\n",
        "    )\n",
        "\n",
        "#_model = joblib.load(\"model.joblib\")\n",
        "'''\n",
        "\n",
        "@app.get(os.environ['AIP_HEALTH_ROUTE'], status_code=200)\n",
        "def health():\n",
        "    return {\"status\": \"healthy\"}\n",
        "\n",
        "\n",
        "@app.post(os.environ['AIP_PREDICT_ROUTE'])\n",
        "async def predict(request: Request):\n",
        "    body = await request.json()\n",
        "    print (body)\n",
        "    \n",
        "    import os\n",
        "    print(os.listdir())\n",
        "\n",
        "    model = IrisClassifier('./model.pth')\n",
        "    \n",
        "    instances = body[\"instances\"]\n",
        "    output = []\n",
        "    for i in instances:\n",
        "        output.append(model.predict(i))\n",
        "        print(model.predict(i))\n",
        "    #return 'class' and 'probability'\n",
        "    return JSONResponse({\"predictions\": output})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "469f55daf250"
      },
      "source": [
        "### Add the pre-start script\n",
        "\n",
        "FastAPI will execute this script before starting up the server. The `PORT` environment variable is set to equal `AIP_HTTP_PORT` in order to run FastAPI on same the port expected by Vertex AI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69f438aca35b"
      },
      "outputs": [],
      "source": [
        "%%writefile app/prestart.sh\n",
        "#!/bin/bash\n",
        "export PORT=$AIP_HTTP_PORT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b62ddf1def3"
      },
      "source": [
        "### Store test instances to use later\n",
        "\n",
        "Next, you create a JSON file for sending test prediction request to the model server.\n",
        "\n",
        "Learn more about [formatting requests for online prediction](https://cloud.google.com/vertex-ai/docs/predictions/online-predictions-custom-models#request-body-details)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6605e9e6186"
      },
      "outputs": [],
      "source": [
        "%%writefile instances.json\n",
        "{\n",
        "    \"instances\": [{\n",
        "        \"sepal_length\": -1.38535265,\n",
        "        \"sepal_width\": 0.32841405,\n",
        "        \"petal_length\": -1.39706395,\n",
        "        \"petal_width\": 1.3154443\n",
        "    },{\n",
        "        \"sepal_length\": -1.02184904,\n",
        "        \"sepal_width\": -2.43394714,\n",
        "        \"petal_length\": -0.14664056,\n",
        "        \"petal_width\": -0.26238682\n",
        "    }]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51e149fdec1b"
      },
      "source": [
        "## Build and push container to Artifact Registry\n",
        "\n",
        "Next, you will build the custom deployment container.\n",
        "\n",
        "\n",
        "### Create the requirements file\n",
        "\n",
        "First, create the requirements.txt file for the required installed packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c52b830d5a92"
      },
      "outputs": [],
      "source": [
        "%%writefile requirements.txt\n",
        "joblib~=1.0\n",
        "numpy~=1.20\n",
        "scikit-learn~=0.24\n",
        "google-cloud-storage>=1.26.0,<2.0.0dev\n",
        "torch~=1.11.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "240578ec9efe"
      },
      "source": [
        "### Build your container\n",
        "\n",
        "Write the Dockerfile, using `tiangolo/uvicorn-gunicorn-fastapi` as a base image. This will automatically run FastAPI for you using Gunicorn and Uvicorn. Visit [the FastAPI docs to read more about deploying FastAPI with Docker](https://fastapi.tiangolo.com/deployment/docker/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d3a6b9ed22b"
      },
      "outputs": [],
      "source": [
        "%%writefile Dockerfile\n",
        "\n",
        "FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7\n",
        "\n",
        "COPY ./app /app\n",
        "COPY requirements.txt requirements.txt\n",
        "\n",
        "RUN pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04c988201499"
      },
      "source": [
        "Build and tag the deployment image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1e7d639b9cc"
      },
      "outputs": [],
      "source": [
        "DEPLOY_IMAGE = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{PRIVATE_REPO}/{IMAGE}\"\n",
        "\n",
        "if not IS_COLAB:\n",
        "    ! docker build -t $DEPLOY_IMAGE .\n",
        "else:\n",
        "    # install docker daemon\n",
        "    ! apt-get -qq install docker.io"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "147a555f6c93"
      },
      "source": [
        "### Run and test the container locally (optional)\n",
        "\n",
        "Run the container locally in detached mode and provide the environment variables that the container requires. These env vars will be provided to the container by Vertex Prediction once deployed. Test the `/health` and `/predict` routes, then stop the running image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62ed2d334d0f"
      },
      "outputs": [],
      "source": [
        "if not IS_COLAB:\n",
        "\n",
        "    ! docker stop local-iris 2>/dev/null\n",
        "    ! docker rm local-iris 2>/dev/null\n",
        "    container_id = ! docker run -d -p 80:8080 \\\n",
        "        --name=local-iris \\\n",
        "        -e AIP_HTTP_PORT=8080 \\\n",
        "        -e AIP_HEALTH_ROUTE=/health \\\n",
        "        -e AIP_PREDICT_ROUTE=/predict \\\n",
        "        -e AIP_STORAGE_URI={BUCKET_URI}/{MODEL_ARTIFACT_DIR} \\\n",
        "        -e GOOGLE_APPLICATION_CREDENTIALS=credentials.json \\\n",
        "        {DEPLOY_IMAGE}\n",
        "\n",
        "    ! sleep 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b2efa66d50c"
      },
      "source": [
        "#### Test the health route\n",
        "\n",
        "Next, test the health route of the deployment container."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce629eea32fd"
      },
      "outputs": [],
      "source": [
        "if not IS_COLAB:\n",
        "    ! curl localhost/health"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "687aa32007c5"
      },
      "source": [
        "Display the corresponding log entries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "do4r-MI-5Vug"
      },
      "outputs": [],
      "source": [
        "if not IS_COLAB:\n",
        "    ! docker logs {container_id[0]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e9458d82ea8"
      },
      "source": [
        "#### Test the predict route\n",
        "\n",
        "Next, test the predict route of the deployment container."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeFu0W_GmIQt"
      },
      "outputs": [],
      "source": [
        "if not IS_COLAB:\n",
        "    ! curl -X POST \\\n",
        "      -d @instances.json \\\n",
        "      -H \"Content-Type: application/json; charset=utf-8\" \\\n",
        "      localhost/predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e1ed9fa1d14"
      },
      "source": [
        "Display the corresponding log entries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3f666f0e020c"
      },
      "outputs": [],
      "source": [
        "if not IS_COLAB:\n",
        "    ! docker logs {container_id[0]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eeed192b3ca"
      },
      "source": [
        "#### Stop the Docker image\n",
        "\n",
        "Now that you have tested the Docker image locally, you stop the execution of the Docker image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlpGgblsmIQt"
      },
      "outputs": [],
      "source": [
        "if not IS_COLAB:\n",
        "    ! docker stop local-iris"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "name_container:training"
      },
      "source": [
        "#### Push the container to the Artifact Registry\n",
        "\n",
        "Next, you will provide a name for your customer container that you will use when you submit it to the Google Artifact Registry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dd7448f4703"
      },
      "outputs": [],
      "source": [
        "if not IS_COLAB:\n",
        "    ! docker push {REGION}-docker.pkg.dev/{PROJECT_ID}/{PRIVATE_REPO}/{IMAGE}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f50e9c553fb7"
      },
      "source": [
        "*Executes in Colab*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7e8c98f1e56"
      },
      "outputs": [],
      "source": [
        "%%bash -s $IS_COLAB $DEPLOY_IMAGE\n",
        "if [ $1 == \"False\" ]; then\n",
        "  exit 0\n",
        "fi\n",
        "set -x\n",
        "dockerd -b none --iptables=0 -l warn &\n",
        "for i in $(seq 5); do [ ! -S \"/var/run/docker.sock\" ] && sleep 2 || break; done\n",
        "docker build . -t $2\n",
        "docker push $2\n",
        "kill $(jobs -p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b438bfa2129f"
      },
      "source": [
        "## Upload the custom serving container as Vertex AI `Model` resource.\n",
        "\n",
        "Next, you upload your custom serving container as a `Model` resource.\n",
        "\n",
        "### Setting explanation configuration\n",
        "\n",
        "First, you specify your explanation settings which you pass as a parameter when uploading your custom serving container.\n",
        "\n",
        "**Note**: When selecting methods to use for feature attributions, make sure the method is compatible with your model.  Integrated gradients and XRAI are only compatible with TensorFlow models and AutoML image models. Sampled Shapley works on tabular models, while cannot work on image models. \n",
        "\n",
        "Learn more about [Compare explanation methods](https://cloud.google.com/vertex-ai/docs/explainable-ai/overview#compare-methods)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "explanation_parameters:mbsdk"
      },
      "outputs": [],
      "source": [
        "XAI = \"shapley\"  # [ shapley, ig, xrai ]\n",
        "\n",
        "if XAI == \"shapley\":\n",
        "    PARAMETERS = {\"sampled_shapley_attribution\": {\"path_count\": 10}}\n",
        "elif XAI == \"ig\":\n",
        "    PARAMETERS = {\"integrated_gradients_attribution\": {\"step_count\": 50}}\n",
        "elif XAI == \"xrai\":\n",
        "    PARAMETERS = {\"xrai_attribution\": {\"step_count\": 50}}\n",
        "\n",
        "parameters = aiplatform.explain.ExplanationParameters(PARAMETERS)\n",
        "print(parameters)\n",
        "\n",
        "EXPLANATION_METADATA = aiplatform.explain.ExplanationMetadata(\n",
        "    inputs={\n",
        "        \"sepal_length\": {},\n",
        "        \"sepal_width\": {},\n",
        "        \"petal_length\": {},\n",
        "        \"petal_width\": {},\n",
        "    },\n",
        "    outputs={\"probability\": {}},\n",
        ")\n",
        "print(EXPLANATION_METADATA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ae19df6a33e"
      },
      "source": [
        "### Upload the custom serving container\n",
        "\n",
        "Next, you upload your custom serving container as a `Model` resource using the `upload()` method, with the following parameters:\n",
        "\n",
        "- `display_name`: The human readable name for the `Model` resource.\n",
        "- `serving_container_image_name`: Your custom serving container.\n",
        "- `explanation_parameters`: Parameters to configure explaining for Model's predictions.\n",
        "- `explanation_metadata`:Metadata describing the Model's input and output for explanation.\n",
        "\n",
        "*Note:* Since the model is embedded within the serving container, there is no need to specify the `artifacts_uri` paramater."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2738154345d5"
      },
      "outputs": [],
      "source": [
        "model = aiplatform.Model.upload(\n",
        "    display_name=MODEL_DISPLAY_NAME,\n",
        "    serving_container_image_uri=f\"{DEPLOY_IMAGE}\",\n",
        "    explanation_parameters=parameters,\n",
        "    explanation_metadata=EXPLANATION_METADATA,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd1b85afc7df"
      },
      "source": [
        "### Deploy the custom serving container to `Endpoint` resource\n",
        "\n",
        "Next, you deploy your `Model` resource, for the custom serving container, using the `deploy()` method to an `Endpoint` resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62cf66498a28"
      },
      "outputs": [],
      "source": [
        "endpoint = model.deploy(machine_type=\"n1-standard-4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6883e7b07143"
      },
      "source": [
        "### Send a prediction request\n",
        "\n",
        "Next, you make an online prediction request using the `predict()` method to your deployed custom serving container.\n",
        "\n",
        "*Note:* The examples have been pre-scaled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hj0lfGn5Vuo"
      },
      "outputs": [],
      "source": [
        "endpoint.predict(\n",
        "    instances=[\n",
        "        {\n",
        "            \"sepal_length\": -1.38535265,\n",
        "            \"sepal_width\": 0.32841405,\n",
        "            \"petal_length\": -1.39706395,\n",
        "            \"petal_width\": 1.3154443,\n",
        "        },\n",
        "        {\n",
        "            \"sepal_length\": -1.02184904,\n",
        "            \"sepal_width\": -2.43394714,\n",
        "            \"petal_length\": -0.14664056,\n",
        "            \"petal_width\": -0.26238682,\n",
        "        },\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91a7f0cb3f26"
      },
      "source": [
        "### Send an explanation request\n",
        "\n",
        "Next, you make an online explanation request using the `explain()` method to your deployed custom serving container."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuKvd9SrmIQw"
      },
      "outputs": [],
      "source": [
        "prediction = endpoint.explain(\n",
        "    instances=[\n",
        "        {\n",
        "            \"sepal_length\": -1.38535265,\n",
        "            \"sepal_width\": 0.32841405,\n",
        "            \"petal_length\": -1.39706395,\n",
        "            \"petal_width\": 1.3154443,\n",
        "        },\n",
        "        {\n",
        "            \"sepal_length\": -1.02184904,\n",
        "            \"sepal_width\": -2.43394714,\n",
        "            \"petal_length\": -0.14664056,\n",
        "            \"petal_width\": -0.26238682,\n",
        "        },\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "examine_feature_attributions"
      },
      "source": [
        "### Examine feature attributions\n",
        "\n",
        "Next you will look at the feature attributions for this particular example. Positive attribution values mean a particular feature pushed your model prediction up by that amount, and vice versa for negative attribution values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "examine_feature_attributions:mbsdk,xgboost,iris"
      },
      "outputs": [],
      "source": [
        "INSTANCE = {\n",
        "    \"sepal_length\": -1.38535265,\n",
        "    \"sepal_width\": 0.32841405,\n",
        "    \"petal_length\": -1.39706395,\n",
        "    \"petal_width\": 1.3154443,\n",
        "}\n",
        "\n",
        "from tabulate import tabulate\n",
        "\n",
        "feature_names = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
        "attributions = prediction.explanations[0].attributions[0].feature_attributions\n",
        "\n",
        "rows = []\n",
        "for i, val in enumerate(feature_names):\n",
        "    rows.append([val, INSTANCE[val], attributions[val]])\n",
        "print(tabulate(rows, headers=[\"Feature name\", \"Feature value\", \"Attribution value\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx_vKniMq9ZX"
      },
      "outputs": [],
      "source": [
        "delete_bucket = True\n",
        "\n",
        "# Undeploy model and delete endpoint\n",
        "try:\n",
        "    endpoint.delete(force=True)\n",
        "\n",
        "    # Delete the model resource\n",
        "    model.delete()\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "# Delete the container image from Artifact Registry\n",
        "!gcloud artifacts docker images delete \\\n",
        "    --quiet \\\n",
        "    --delete-tags \\\n",
        "    {DEPLOY_IMAGE}\n",
        "\n",
        "! rm -rf app\n",
        "\n",
        "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
        "    ! gsutil rm -rf {BUCKET_URI}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "get_started_with_xai_and_custom_server.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

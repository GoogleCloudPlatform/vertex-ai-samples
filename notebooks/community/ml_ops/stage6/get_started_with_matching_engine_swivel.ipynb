{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63084117b407"
   },
   "source": [
    "This notebook is a revised version of notebook from [Amy Wu](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/matching_engine/intro-swivel.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# E2E ML on GCP: MLOps stage 6 : serving: get started with Vertex AI Matching Engine and Swivel builtin algorithm\n",
    "\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage6/get_started_with_matching_engine_swivel.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage6/get_started_with_matching_engine_swivel.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/ml_ops/stage6/get_started_with_matching_engine_swivel.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bb1b0670754f"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrate how to train an embedding with Submatrix-wise Vector Embedding Learner ([Swivel](https://arxiv.org/abs/1602.02215)) using Vertex AI Pipelines. The purpose of the embedding learner is to compute cooccurrences between tokens in a given dataset and to use the cooccurrences to generate embeddings.\n",
    "\n",
    "Vertex AI provides a pipeline template for training with Swivel, so you don't need to design your own pipeline or write\n",
    "your own training code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this notebook, you learn how to train custom embeddings using Vertex AI Pipelines and subsequently train and deploy a matching engine index using the embeddings.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services:\n",
    "\n",
    "- `Vertex AI Swivel` builtin algorithm\n",
    "- `Vertex AI Matching Engine`\n",
    "- `Vertex AI Batch Prediction`\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "1. Train the `Swivel` algorithm to generate embeddings (encoder) for the dataset.\n",
    "2. Make example predictions (embeddings) from then trained encoder.\n",
    "3. Generate embeddings using the trained `Swivel` builtin algorithm.\n",
    "4. Store embeddings to format supported by `Matching Engine`.\n",
    "5. Create a `Matching Engine Index` for the embeddings.\n",
    "6. Deploy the `Matching Engine Index` to a `Index Endpoint`.\n",
    "7. Make a matching engine prediction request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfe5159ce8cd"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "This tutorial uses the `movielens sample dataset` in the public bucket `gs://cloud-samples-data/vertex-ai/matching-engine/swivel`, which was generated from the [MovieLens movie rating dataset](https://grouplens.org/datasets/movielens/100k/). This dataset is processed so that each line contains the movies that have same rating by the same user. The directory also includes `movies.csv`, which maps the movie ids to their names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "302344ccc14b"
   },
   "source": [
    "### Costs \n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Dataflow\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), and [Dataflow pricing](https://cloud.google.com/dataflow/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze4-nDLfK4pw"
   },
   "source": [
    "### Set up your local development environment\n",
    "\n",
    "**If you are using Colab or Vertex AI Workbench Notebooks**, your environment already meets\n",
    "all the requirements to run this notebook. You can skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCuSR8GkAgzl"
   },
   "source": [
    "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
    "You need the following:\n",
    "\n",
    "* The Google Cloud SDK\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* Jupyter notebook running in a virtual environment with Python 3\n",
    "\n",
    "The Google Cloud guide to [Setting up a Python development\n",
    "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
    "installation guide](https://jupyter.org/install) provide detailed instructions\n",
    "for meeting these requirements. The following steps provide a condensed set of\n",
    "instructions:\n",
    "\n",
    "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "1. [Install\n",
    "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
    "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
    "\n",
    "1. To install Jupyter, run `pip3 install jupyter` on the\n",
    "command-line in a terminal shell.\n",
    "\n",
    "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
    "\n",
    "1. Open this notebook in the Jupyter Notebook Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## Installation\n",
    "\n",
    "Install packages required for executing this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install {USER_FLAG} --upgrade pip -q\n",
    "! pip3 install {USER_FLAG} --upgrade scikit-learn -q\n",
    "! pip3 install {USER_FLAG} --upgrade google-cloud-aiplatform tensorboard-plugin-profile -q\n",
    "! pip3 install {USER_FLAG} --upgrade tensorflow -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzrelQZ22IZj"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. [Enable the Vertex AI API and Dataflow API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,dataflow.googleapis.com).\n",
    "\n",
    "1. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    "\n",
    "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_project_id"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0861d5cc5fd"
   },
   "source": [
    "#### Get your project number\n",
    "\n",
    "Now that the project ID is set, you get your corresponding project number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dbb8cfc8a876"
   },
   "outputs": [],
   "source": [
    "shell_output = ! gcloud projects list --filter=\"PROJECT_ID:'{PROJECT_ID}'\" --format='value(PROJECT_NUMBER)'\n",
    "PROJECT_NUMBER = shell_output[0]\n",
    "print(\"Project Number:\", PROJECT_NUMBER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
    "\n",
    "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CweX_c7eVSH"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append it onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aK4XnlYSeVSI"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr--iN2kAylZ"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "**If you are using Vertex AI Workbench Notebooks**, your environment is already\n",
    "authenticated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "**If you are using Colab**, run the cell below and follow the instructions\n",
    "when prompted to authenticate your account via oAuth.\n",
    "\n",
    "**Otherwise**, follow these steps:\n",
    "\n",
    "1. In the Cloud Console, go to the [**Create service account key**\n",
    "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
    "\n",
    "2. Click **Create service account**.\n",
    "\n",
    "3. In the **Service account name** field, enter a name, and\n",
    "   click **Create**.\n",
    "\n",
    "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
    "into the filter box, and select\n",
    "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
    "\n",
    "5. Click *Create*. A JSON file that contains your key downloads to your\n",
    "local environment.\n",
    "\n",
    "6. Enter the path to your service account key as the\n",
    "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If on Vertex AI Workbench, then don't execute this code\n",
    "IS_COLAB = False\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
    "    \"DL_ANACONDA_HOME\"\n",
    "):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        IS_COLAB = True\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "When you submit a built-in Swivel job using the Cloud SDK, you need a Cloud Storage bucket for storing the input dataset and pipeline artifacts (the trained model).\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all Cloud Storage buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cf221059d072"
   },
   "outputs": [],
   "source": [
    "if BUCKET_URI == \"\" or BUCKET_URI is None or BUCKET_URI == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = PROJECT_ID + \"aip-\" + TIMESTAMP\n",
    "    BUCKET_URI = \"gs://\" + BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucvCsknMCims"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d612cd762261"
   },
   "source": [
    "### Service Account\n",
    "\n",
    "**If you don't know your service account**, try to get your service account using gcloud command by executing the second cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "801acfa0ffbc"
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7864c293f9e"
   },
   "outputs": [],
   "source": [
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your service account from gcloud\n",
    "    if not IS_COLAB:\n",
    "        shell_output = !gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "    if IS_COLAB:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1390d2890e4e"
   },
   "source": [
    "#### Set service account access for Vertex AI Pipelines\n",
    "\n",
    "Run the following commands to grant your service account access to read and write pipeline artifacts in the bucket that you created in the previous step -- you only need to run these once per service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0f5ef291f226"
   },
   "outputs": [],
   "source": [
    "!gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_NAME\n",
    "\n",
    "!gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3pGuwT7eVSJ"
   },
   "source": [
    "### Import libraries and define constants\n",
    "Define constants used in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pARPYnT2eVSJ"
   },
   "outputs": [],
   "source": [
    "SOURCE_DATA_PATH = \"{}/swivel\".format(BUCKET_URI)\n",
    "PIPELINE_ROOT = \"{}/pipeline_root\".format(BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vg6BigD5eVSJ"
   },
   "source": [
    "Import packages used in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fiimME4YeVSJ"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from google.cloud import aiplatform\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "machine:training"
   },
   "source": [
    "#### Set machine type\n",
    "\n",
    "Next, set the machine type to use for prediction.\n",
    "\n",
    "- Set the variable `DEPLOY_COMPUTE` to configure  the compute resources for the VMs you will use for for prediction.\n",
    " - `machine type`\n",
    "     - `n1-standard`: 3.75GB of memory per vCPU.\n",
    "     - `n1-highmem`: 6.5GB of memory per vCPU\n",
    "     - `n1-highcpu`: 0.9 GB of memory per vCPU\n",
    " - `vCPUs`: number of \\[2, 4, 8, 16, 32, 64, 96 \\]\n",
    "\n",
    "*Note: You may also use n2 and e2 machine types for training and deployment, but they do not support GPUs*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "machine:training"
   },
   "outputs": [],
   "source": [
    "if os.getenv(\"IS_TESTING_DEPLOY_MACHINE\"):\n",
    "    MACHINE_TYPE = os.getenv(\"IS_TESTING_DEPLOY_MACHINE\")\n",
    "else:\n",
    "    MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9Uo3tifg1kx"
   },
   "source": [
    "### Copy the Swivel template\n",
    "\n",
    "First, download the provioded Swivel template and configuration script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "! gsutil cp gs://cloud-samples-data/vertex-ai/matching-engine/swivel/pipeline/* ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ElFBL6BeVSK"
   },
   "source": [
    "### Set your pipeline configurations\n",
    "\n",
    "Change your pipeline configurations: \n",
    "\n",
    "* pipeline_suffix: Suffix of your pipeline name (lowercase and hyphen are allowed).\n",
    "* machine_type: e.g. n1-standard-16.\n",
    "* accelerator_count: Number of GPUs in each machine.\n",
    "* accelerator_type: e.g. NVIDIA_TESLA_P100, NVIDIA_TESLA_V100.\n",
    "* region: e.g. us-east1 (optional, default is us-central1)\n",
    "* network_name: e.g., my_network_name (optional, otherwise it uses \"default\" network).\n",
    "\n",
    "### VPC Network peering, subnetwork and private IP address configuration\n",
    "\n",
    "Executing the following cell will generate two files:\n",
    "1. `swivel_pipeline_basic.json`: The basic template allows public IPs and default network for the Dataflow job, and doesn't require setting up VPC Network peering for Vertex AI and **you will use it in this notebook sample**.\n",
    "1. `swivel_pipeline.json`: This template enables private IPs and subnet configuration for the Dataflow job, also requires setting up VPC Network peering for the Vertex custom training. This template includes the following args:\n",
    "* \"--subnetwork=regions/%REGION%/subnetworks/%NETWORK_NAME%\",\n",
    "* \"--no_use_public_ips\",\n",
    "* \\\"network\\\": \\\"projects/%PROJECT_NUMBER%/global/networks/%NETWORK_NAME%\\\"\n",
    "\n",
    "**WARNING** In order to specify private IPs and configure VPC network, you need to [set up VPC Network peering for Vertex AI](https://cloud.google.com/vertex-ai/docs/general/vpc-peering#overview) for your subnetwork (e.g. \"default\" network on \"us-central1\") before submitting the following job. This is required for using private IP addresses for DataFlow and Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "190tiY-neVSK"
   },
   "outputs": [],
   "source": [
    "YOUR_PIPELINE_SUFFIX = \"swivel-pipeline-movie\"  # @param {type:\"string\"}\n",
    "MACHINE_TYPE = \"n1-standard-16\"  # @param {type:\"string\"}\n",
    "ACCELERATOR_COUNT = 2  # @param {type:\"integer\"}\n",
    "ACCELERATOR_TYPE = \"NVIDIA_TESLA_V100\"  # @param {type:\"string\"}\n",
    "\n",
    "! chmod +x swivel_template_configuration*\n",
    "\n",
    "! ./swivel_template_configuration_basic.sh -pipeline_suffix {YOUR_PIPELINE_SUFFIX} -project_number {PROJECT_NUMBER} -project_id {PROJECT_ID} -machine_type {MACHINE_TYPE} -accelerator_count {ACCELERATOR_COUNT} -accelerator_type {ACCELERATOR_TYPE} -pipeline_root {BUCKET_NAME}\n",
    "! ./swivel_template_configuration.sh -pipeline_suffix {YOUR_PIPELINE_SUFFIX} -project_number {PROJECT_NUMBER} -project_id {PROJECT_ID} -machine_type {MACHINE_TYPE} -accelerator_count {ACCELERATOR_COUNT} -accelerator_type {ACCELERATOR_TYPE} -pipeline_root {BUCKET_NAME}\n",
    "\n",
    "! sed \"s/\\\\t/    /g\" swivel_pipeline_basic.json > tmp.json\n",
    "! mv tmp.json swivel_pipeline_basic.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3f12048abec2"
   },
   "source": [
    "Both `swivel_pipeline_basic.json` and `swivel_pipeline.json` are generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcM5q2wfeVSK"
   },
   "source": [
    "## Create the Swivel job for MovieLens items embeddings\n",
    "\n",
    "You will submit the pipeline job by passing the compiled specification to the `create_run_from_job_spec()` method. Note that you are passing a `parameter_values` dictionary that specifies the pipeline input parameters to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af31EtxxeVSK"
   },
   "source": [
    "The following table shows the runtime parameters required by the Swivel job:\n",
    "\n",
    "| Parameter                  |Data type | Description                                                        | Required               |\n",
    "|----------------------------|----------|--------------------------------------------------------------------|------------------------|\n",
    "| `embedding_dim`            | int      | Dimensions of the embeddings to train.                         | No - Default is 100    |\n",
    "| `input_base`         | string   | Cloud Storage path where the input data is stored.                       | Yes                    |\n",
    "| `input_type`               | string   | Type of the input data.  Can be either 'text' (for wikipedia sample) or 'items'(for movielens sample).      | Yes                    |\n",
    "| `max_vocab_size`               | int      | Maximum vocabulary size to generate embeddings for.                | No - Default is 409600 |\n",
    "|`num_epochs` | int | Number of epochs for training. | No - Default is 20 |\n",
    "\n",
    "In short, the **items** input type means that each line of your input data should be space-separated item ids. Each line is tokenized by splitting on whitespace. The **text** input type means that each line of your input data should be equivalent to a sentence. Each line is tokenized by lowercasing, and splitting on whitespace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9SnVxxleVSK"
   },
   "outputs": [],
   "source": [
    "# MovieLens items embedding sample\n",
    "\n",
    "PARAMETER_VALUES = {\n",
    "    \"embedding_dim\": 100,  # <---CHANGE THIS (OPTIONAL)\n",
    "    \"input_base\": \"{}/movielens_25m/train\".format(SOURCE_DATA_PATH),\n",
    "    \"input_type\": \"items\",  # For movielens sample\n",
    "    \"max_vocab_size\": 409600,  # <---CHANGE THIS (OPTIONAL)\n",
    "    \"num_epochs\": 5,  # <---CHANGE THIS (OPTIONAL)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48f0cdf9c773"
   },
   "source": [
    "#### Copy the dataset to Cloud Storage\n",
    "\n",
    "Next, copy the MovieLens dataset to your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8cd9e3db9bff"
   },
   "outputs": [],
   "source": [
    "# Copy the MovieLens sample dataset\n",
    "! gsutil cp -r gs://cloud-samples-data/vertex-ai/matching-engine/swivel/movielens_25m/train/* {SOURCE_DATA_PATH}/movielens_25m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9f1cae770338"
   },
   "source": [
    "### Submit the pipeline job\n",
    "\n",
    "Next, submit the pipeline job to `Vertex AI Pipelines`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kUV5aYtPeVSK"
   },
   "outputs": [],
   "source": [
    "# Instantiate PipelineJob object\n",
    "pl = aiplatform.PipelineJob(\n",
    "    display_name=YOUR_PIPELINE_SUFFIX,\n",
    "    # Whether or not to enable caching\n",
    "    # True = always cache pipeline step result\n",
    "    # False = never cache pipeline step result\n",
    "    # None = defer to cache option for each pipeline component in the pipeline definition\n",
    "    enable_caching=False,\n",
    "    # Local or GCS path to a compiled pipeline definition\n",
    "    template_path=\"swivel_pipeline_basic.json\",\n",
    "    # Dictionary containing input parameters for your pipeline\n",
    "    parameter_values=PARAMETER_VALUES,\n",
    "    # GCS path to act as the pipeline root\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "\n",
    "# Submit the Pipeline to Vertex AI\n",
    "# Optionally you may specify the service account below: submit(service_account=SERVICE_ACCOUNT)\n",
    "# You must have iam.serviceAccounts.actAs permission on the service account to use it\n",
    "pl.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhznZuWceVSL"
   },
   "source": [
    "After the job is submitted successfully, you can view its details (including run name that you'll need below) and logs.\n",
    "\n",
    "### Use TensorBoard to check the model\n",
    "\n",
    "You may use the TensorBoard to check the model training process. In order to do that, you need to find the path to the trained model artifact. After the job finishes successfully (~ a few hours), you can view the trained model output path in the [Vertex ML Metadata](https://cloud.google.com/vertex-ai/docs/ml-metadata/introduction) browser. It is going to have the following format:\n",
    "\n",
    "* {BUCKET_URI}/pipeline_root/{PROJECT_NUMBER}/swivel-{TIMESTAMP}/EmbTrainerComponent_-{SOME_NUMBER}/model/\n",
    "\n",
    "You may copy this path for the MODELOUTPUT_DIR below.\n",
    "\n",
    "Alternatively, you can download a pretrained model to `{SOURCE_DATA_PATH}/movielens_model` and proceed. This pretrained model is for demo purpose and not optimized for production usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cc20afaf0e8a"
   },
   "outputs": [],
   "source": [
    "! gsutil -m cp -r gs://cloud-samples-data/vertex-ai/matching-engine/swivel/models/movielens/model {SOURCE_DATA_PATH}/movielens_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c1Na-orVeVSL"
   },
   "outputs": [],
   "source": [
    "SAVEDMODEL_DIR = os.path.join(SOURCE_DATA_PATH, \"movielens_model/model\")\n",
    "LOGS_DIR = os.path.join(SOURCE_DATA_PATH, \"movielens_model/tensorboard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PY5ipT0feVSL"
   },
   "source": [
    "When the training starts, you can view the logs in TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zcswl8-OeVSL"
   },
   "outputs": [],
   "source": [
    "# If on Vertex AI Workbench Notebooks, then don't execute this code.\n",
    "if not os.getenv(\"DL_ANACONDA_HOME\"):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        # Load the TensorBoard notebook extension.\n",
    "        %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjOzNEQseVSL"
   },
   "outputs": [],
   "source": [
    "# If on Vertex AI Workbench Notebooks, then don't execute this code.\n",
    "if not os.getenv(\"DL_ANACONDA_HOME\"):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        %tensorboard --logdir $LOGS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3PbqO2IeVSL"
   },
   "source": [
    "For **Vertex AI Workbench Notebooks**, you can do the following:\n",
    "\n",
    "1. Open Cloud Shell from the Google Cloud Console.\n",
    "2. Install dependencies: `pip3 install tensorflow tensorboard-plugin-profile`\n",
    "3. Run the following command: `tensorboard --logdir {LOGS_DIR}`. You will see a message \"TensorBoard 2.x.0 at http://localhost:<PORT>/ (Press CTRL+C to quit)\" as the output. Take note of the port number.\n",
    "4. You can click on the Web Preview button and view the TensorBoard dashboard and profiling results. You need to configure Web Preview's port to be the same port as you receive from step 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gidmXBWysaeP"
   },
   "source": [
    "### Upload the model to `Vertex AI Model` resource\n",
    "\n",
    "First, import the model using the `upload()` method, with the following parameters:\n",
    "\n",
    "- `display_name`: A human readable name for the model resource.\n",
    "- `artifact_uri`: The Cloud Storage location of the model artifacts.\n",
    "- `serving_container_image_uri`: The deployment container. In this tutorial, you use the prebuilt Two-Tower deployment container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-rHi00XeVSM"
   },
   "outputs": [],
   "source": [
    "DEPLOY_IMAGE = \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-4:latest\"\n",
    "\n",
    "# Upload the trained model to Model resource\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=\"movies_\" + TIMESTAMP,\n",
    "    artifact_uri=SAVEDMODEL_DIR,\n",
    "    serving_container_image_uri=DELOY_IMAGE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3c8c48ee2fb"
   },
   "source": [
    "## Deploy the model to `Vertex AI Endpoint`\n",
    "\n",
    "Deploying the `Vertex AI Model` resoure to a `Vertex AI Endpoint` for online predictions:\n",
    "\n",
    "1. Create an `Endpoint` resource exposing an external interface to users consuming the model. \n",
    "2. After the `Endpoint` is ready, deploy one or more instances of a model to the `Endpoint`. The deployed model runs the Swivel encoder to serve embeddings.\n",
    "\n",
    "Refer to Vertex AI Predictions guide to [Deploy a model using the Vertex AI API](https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api) for more information about the APIs used in the following cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1488b536cf8"
   },
   "source": [
    "### Create a `Vertex AI Endpoint`\n",
    "\n",
    "Next, you create the `Vertex AI Endpoint`, from which you subsequently deploy your `Vertex AI Model` resource to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "er94Wp82sxYW"
   },
   "outputs": [],
   "source": [
    "endpoint = aiplatform.Endpoint.create(display_name=\"swivel_embedding_\" + TIMESTAMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PICvm8PhqtMw"
   },
   "source": [
    "### Deploying `Model` resources to an `Endpoint` resource.\n",
    "\n",
    "You can deploy one of more `Vertex AI Model` resource instances to the same endpoint. Each `Vertex AI Model` resource that is deployed will have its own deployment container for the serving binary. \n",
    "\n",
    "In the next example, you deploy the `Vertex AI Model` resource to a `Vertex AI Endpoint` resource. The `Vertex AI Model` resource already has defined for it the deployment container image. To deploy, you specify the following additional configuration settings:\n",
    "\n",
    "- The machine type.\n",
    "- The (if any) type and number of GPUs.\n",
    "- Static, manual or auto-scaling of VM instances.\n",
    "\n",
    "In this example, you deploy the model with the minimal amount of specified parameters, as follows:\n",
    "\n",
    "- `model`: The `Model` resource.\n",
    "- `deployed_model_displayed_name`: The human readable name for the deployed model instance.\n",
    "- `machine_type`: The machine type for each VM instance.\n",
    "\n",
    "Do to the requirements to provision the resource, this may take upto a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OUEF7Yces4uD"
   },
   "outputs": [],
   "source": [
    "response = endpoint.deploy(\n",
    "    model=model,\n",
    "    deployed_model_display_name=DISPLAY_NAME,\n",
    "    machine_type=DEPLOY_COMPUTE,\n",
    "    traffic_split={\"0\": 100},\n",
    ")\n",
    "\n",
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9360794f914f"
   },
   "source": [
    "### Load the movie ids and titles for querying embeddings\n",
    "\n",
    "Next, download the movie IDs and titles sample dataset and read them into a pandas Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14fd3ab852a1"
   },
   "outputs": [],
   "source": [
    "! gsutil cp gs://cloud-samples-data/vertex-ai/matching-engine/swivel/movielens_25m/movies.csv ./movies.csv\n",
    "\n",
    "movies = pd.read_csv(\"movies.csv\")\n",
    "print(f\"Movie count: {len(movies.index)}\")\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAkHJlY7tOmu"
   },
   "source": [
    "## Creating embeddings\n",
    "\n",
    "Now that you have deployed the encoder model on `Vertex AI Prediction`, you can call the model to generate embeddings for new data. \n",
    "\n",
    "### Make an online prediction with SDK\n",
    "\n",
    "[Online prediction](https://cloud.google.com/vertex-ai/docs/predictions/online-predictions-custom-models) is used to synchronously query a model on a small batch of instances with minimal latency. The following function calls the deployed model using Vertex AI SDK for Python.\n",
    "\n",
    "BLAH The input data you want predicted embeddings on should be provided as a list of movie IDs. Note that you should also provide a unique `key` field (of type str) for each input instance so that you can associate each output embedding with its corresponding input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8aefb986c6c8"
   },
   "outputs": [],
   "source": [
    "# Change to your favourite movies.\n",
    "query_movies = [\n",
    "    \"Lion King, The (1994)\",\n",
    "    \"Aladdin (1992)\",\n",
    "    \"Star Wars: Episode IV - A New Hope (1977)\",\n",
    "    \"Star Wars: Episode VI - Return of the Jedi (1983)\",\n",
    "    \"Terminator 2: Judgment Day (1991)\",\n",
    "    \"Aliens (1986)\",\n",
    "    \"Godfather, The (1972)\",\n",
    "    \"Goodfellas (1990)\",\n",
    "]\n",
    "\n",
    "\n",
    "def get_movie_id(title):\n",
    "    return list(movies[movies.title == title].movieId)[0]\n",
    "\n",
    "\n",
    "instances = [str(get_movie_id(title)) for title in query_movies]\n",
    "print(instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDaCXinreVSM"
   },
   "source": [
    "### Make an online prediction request for embeddings\n",
    "\n",
    "Next, make the prediction request using the `predict()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3JTz0i_ieVSM"
   },
   "outputs": [],
   "source": [
    "predictions = endpoint.predict(instances=instances)\n",
    "embeddings = predictions.predictions\n",
    "print(\"Number of embeddings:\", len(embeddings))\n",
    "print(embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1c5f572dec7"
   },
   "source": [
    "#### Explore the movie embedding\n",
    "\n",
    "Using the embeddings, explore how similiar each movie in the sample movie list is similiar to each other movie in the sample movie list.\n",
    "\n",
    "*Note:* A value of 1.0 means they are the same embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8c81e656816a"
   },
   "outputs": [],
   "source": [
    "for idx1 in range(0, len(input_items) - 1, 2):\n",
    "    item1 = instances[idx1]\n",
    "    title1 = query_movies[idx1]\n",
    "    print(title1)\n",
    "    print(\"==================\")\n",
    "    embedding1 = embeddings[idx1]\n",
    "    for idx2 in range(0, len(instances)):\n",
    "        item2 = input_items[idx2]\n",
    "        embedding2 = embeddings[idx2]\n",
    "        similarity = round(cosine_similarity([embedding1], [embedding2])[0][0], 5)\n",
    "        title1 = query_movies[idx1]\n",
    "        title2 = query_movies[idx2]\n",
    "        print(f\" - Similarity to '{title2}' = {similarity}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71mBtTJ2eVSN"
   },
   "source": [
    "You can use the [TensorBoard Embedding Projector](https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin) to graphically represent high dimensional embeddings, which can be helpful in examining and understanding your embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1k-_XJzlthfP"
   },
   "source": [
    "### Make an online prediction with `gcloud`\n",
    "\n",
    "You can also do online prediction using the gcloud CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tn5L9V0utkpA"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "request = json.dumps({\"instances\": input_items})\n",
    "with open(\"request.json\", \"w\") as writer:\n",
    "    writer.write(f\"{request}\\n\")\n",
    "\n",
    "ENDPOINT_ID = endpoint.resource_name\n",
    "\n",
    "! gcloud ai endpoints predict {ENDPOINT_ID} \\\n",
    "  --region={REGION} \\\n",
    "  --json-request=request.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocLE_U6ftnA3"
   },
   "source": [
    "### Make a batch prediction\n",
    "\n",
    "[Batch prediction](https://cloud.google.com/vertex-ai/docs/predictions/batch-predictions) is used to asynchronously make predictions on a batch of input data.  This is recommended if you have a large input size and do not need an immediate response, such as getting embeddings for candidate objects in order to create an index for a nearest neighbor search service such as [Vertex Matching Engine](https://cloud.google.com/vertex-ai/docs/matching-engine/overview).\n",
    "\n",
    "### Create the batch input file\n",
    "\n",
    "Next, you generate the batch input file to generate embeddings for the dataset, which you subsequently use to create an index with `Vertex AI Matching Engine`. In this example, the dataset contains a 200000 unique identifiers (1...200000). You will use the trained encoder to generate a predicted embedding for each unique identifier.\n",
    "\n",
    "The input data needs to be on Cloud Storage and in JSONL format. You can use the sample query object file provided below. Like with online prediction, it's recommended to have the `key` field so that you can associate each output embedding with its corresponding input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ar13RZ4VtquX"
   },
   "outputs": [],
   "source": [
    "QUERY_EMBEDDING_PATH = f\"{BUCKET_URI}/embeddings/train.jsonl\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.io.gfile.GFile(QUERY_EMBEDDING_PATH, \"w\") as f:\n",
    "    for i in range(1, 200001):\n",
    "        query = str(i)\n",
    "        f.write(json.dumps(query) + \"\\n\")\n",
    "\n",
    "print(\"\\nNumber of embeddings: \")\n",
    "! gsutil cat {QUERY_EMBEDDING_PATH} | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6ff2ebfc666"
   },
   "outputs": [],
   "source": [
    "! gsutil cat {QUERY_EMBEDDING_PATH} | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "send_prediction_request:image"
   },
   "source": [
    "### Send the prediction request\n",
    "\n",
    "To make a batch prediction request, call the model object's `batch_predict` method with the following parameters: \n",
    "- `instances_format`: The format of the batch prediction request file: \"jsonl\", \"csv\", \"bigquery\", \"tf-record\", \"tf-record-gzip\" or \"file-list\"\n",
    "- `prediction_format`: The format of the batch prediction response file: \"jsonl\", \"csv\", \"bigquery\", \"tf-record\", \"tf-record-gzip\" or \"file-list\"\n",
    "- `job_display_name`: The human readable name for the prediction job.\n",
    "- `gcs_source`: A list of one or more Cloud Storage paths to your batch prediction requests.\n",
    "- `gcs_destination_prefix`: The Cloud Storage path that the service will write the predictions to.\n",
    "- `model_parameters`: Additional filtering parameters for serving prediction results.\n",
    "- `machine_type`: The type of machine to use for training.\n",
    "- `accelerator_type`: The hardware accelerator type.\n",
    "- `accelerator_count`: The number of accelerators to attach to a worker replica.\n",
    "- `starting_replica_count`: The number of compute instances to initially provision.\n",
    "- `max_replica_count`: The maximum number of compute instances to scale to. In this tutorial, only one instance is provisioned.\n",
    "\n",
    "### Compute instance scaling\n",
    "\n",
    "You can specify a single instance (or node) to process your batch prediction request. This tutorial uses a single node, so the variables `MIN_NODES` and `MAX_NODES` are both set to `1`.\n",
    "\n",
    "If you want to use multiple nodes to process your batch prediction request, set `MAX_NODES` to the maximum number of nodes you want to use. Vertex AI autoscales the number of nodes used to serve your predictions, up to the maximum number you set. Refer to the [pricing page](https://cloud.google.com/vertex-ai/pricing#prediction-prices) to understand the costs of autoscaling with multiple nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6735e645f0b0"
   },
   "outputs": [],
   "source": [
    "MIN_NODES = 1\n",
    "MAX_NODES = 4\n",
    "\n",
    "batch_predict_job = model.batch_predict(\n",
    "    job_display_name=f\"batch_predict_swivel\",\n",
    "    gcs_source=[QUERY_EMBEDDING_PATH],\n",
    "    gcs_destination_prefix=f\"{BUCKET_URI}/embeddings/output\",\n",
    "    machine_type=DEPLOY_COMPUTE,\n",
    "    starting_replica_count=MIN_NODES,\n",
    "    max_replica_count=MAX_NODES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "get_batch_prediction:mbsdk,custom,lrg"
   },
   "source": [
    "### Get the predicted embeddings\n",
    "\n",
    "Next, get the results from the completed batch prediction job.\n",
    "\n",
    "The results are written to the Cloud Storage output bucket you specified in the batch prediction request. You call the method `iter_outputs()` to get a list of each Cloud Storage file generated with the results. Each file contains one or more prediction requests in a JSON format:\n",
    "\n",
    "- `instance`: The prediction request.\n",
    "- `prediction`: The prediction response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "get_batch_prediction:mbsdk,custom,lrg"
   },
   "outputs": [],
   "source": [
    "bp_iter_outputs = batch_predict_job.iter_outputs()\n",
    "\n",
    "prediction_results = list()\n",
    "for blob in bp_iter_outputs:\n",
    "    if blob.name.split(\"/\")[-1].startswith(\"prediction\"):\n",
    "        prediction_results.append(blob.name)\n",
    "\n",
    "result_files = []\n",
    "for prediction_result in prediction_results:\n",
    "    result_file = f\"gs://{bp_iter_outputs.bucket.name}/{prediction_result}\"\n",
    "    result_files.append(result_file)\n",
    "\n",
    "print(result_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQIQSyF9GtSv"
   },
   "source": [
    "#### Save the embeddings in JSONL format\n",
    "\n",
    "Next, you store the predicted embeddings as a JSONL formatted file. Each embedding is stored as:\n",
    "\n",
    "    { 'id': .., 'embedding': [ ... ] }\n",
    "    \n",
    "The format of the embeddings for the index can be in either CSV, JSON, or Avro format.\n",
    "\n",
    "Learn more about [Embedding Formats for Indexing](https://cloud.google.com/vertex-ai/docs/matching-engine/using-matching-engine#json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dd9bde3980fa"
   },
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for result_file in result_files:\n",
    "\n",
    "    with tf.io.gfile.GFile(result_file, \"r\") as f:\n",
    "        instances = list(f)\n",
    "\n",
    "    for instance in instances:\n",
    "        instance = instance.replace('\\\\\"', \"'\")\n",
    "        result = json.loads(instance)\n",
    "        prediction = result[\"prediction\"]\n",
    "        key = result[\"instance\"]\n",
    "\n",
    "        embedding = {\"id\": key, \"embedding\": prediction}\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "print(\"Number of embeddings\", len(embeddings))\n",
    "print(\"Encoding Dimensions\", len(embeddings[0][\"embedding\"]))\n",
    "print(\"Example embedding\", embeddings[0])\n",
    "\n",
    "with open(\"embeddings.json\", \"w\") as f:\n",
    "    for i in range(len(embeddings)):\n",
    "        f.write(json.dumps(embeddings[i]).replace('\"', \"'\"))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "! head -n 2 embeddings.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QuVl8DrWG8NS"
   },
   "source": [
    "#### Store the JSONL formatted embeddings in Cloud Storage\n",
    "\n",
    "Next, you upload the training data to your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3PgsA_vbI8Vg"
   },
   "outputs": [],
   "source": [
    "EMBEDDINGS_URI = f\"{BUCKET_URI}/embeddings/swivel/\"\n",
    "! gsutil cp embeddings.json {EMBEDDINGS_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhIBCQ7dDSbW"
   },
   "source": [
    "### Create Matching Engine Index\n",
    "\n",
    "Next, you create the index for your embeddings. Currently, two indexing algorithms are supported:\n",
    "\n",
    "- `create_tree_ah_index()`:  Shallow tree + Asymmetric hashing.\n",
    "- `create_brute_force_index()`: Linear search.\n",
    "\n",
    "In this tutorial, you use the `create_tree_ah_index()`for production scale. The method is called with the following parameters:\n",
    "\n",
    "- `display_name`: A human readable name for the index.\n",
    "- `contents_delta_uri`: A Cloud Storage location for the embeddings, which are either to be inserted, updated or deleted.\n",
    "- `dimensions`: The number of dimensions of the input vector\n",
    "- `approximate_neighbors_count`: (for Tree AH) The default number of neighbors to find via approximate search before exact reordering is performed. Exact reordering is a procedure where results returned by an approximate search algorithm are reordered via a more expensive distance computation.\n",
    "- `distance_measure_type`: The distance measure used in nearest neighbor search.\n",
    "    - `SQUARED_L2_DISTANCE`: Euclidean (L2) Distance\n",
    "    - `L1_DISTANCE`: Manhattan (L1) Distance\n",
    "    - `COSINE_DISTANCE`: Cosine Distance. Defined as 1 - cosine similarity.\n",
    "    - `DOT_PRODUCT_DISTANCE`: Default value. Defined as a negative of the dot product.\n",
    "- `description`: A human readble description of the index.\n",
    "- `labels`: User metadata in the form of a dictionary.\n",
    "- `leaf_node_embedding_count`: Number of embeddings on each leaf node. The default value is 1000 if not set.\n",
    "- `leaf_nodes_to_search_percent`: The default percentage of leaf nodes that any query may be searched. Must be in range 1-100, inclusive. The default value is 10 (means 10%) if not set.\n",
    "\n",
    "This may take upto 30 minutes.\n",
    "\n",
    "Learn more about [Configuring Matching Engine Indexes](https://cloud.google.com/vertex-ai/docs/matching-engine/configuring-indexes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xzY7TpUSJcTV"
   },
   "outputs": [],
   "source": [
    "DIMENSIONS = len(embeddings[0][\"embedding\"])\n",
    "DISPLAY_NAME = \"movies\"\n",
    "\n",
    "tree_ah_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    contents_delta_uri=EMBEDDINGS_URI,\n",
    "    dimensions=DIMENSIONS,\n",
    "    approximate_neighbors_count=50,\n",
    "    distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\n",
    "    description=\"Two tower generated embeddings\",\n",
    "    labels={\"label_name\": \"label_value\"},\n",
    "    # TreeAH specific parameters\n",
    "    leaf_node_embedding_count=100,\n",
    "    leaf_nodes_to_search_percent=7,\n",
    ")\n",
    "\n",
    "INDEX_RESOURCE_NAME = tree_ah_index.resource_name\n",
    "print(INDEX_RESOURCE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4f4f0bc64ddb"
   },
   "source": [
    "## Setup VPC peering network\n",
    "\n",
    "To use a `Matching Engine Index`, you setup a VPC peering network between your project and the `Vertex AI Matching Engine` service project. This eliminates additional hops in network traffic and allows using efficient gRPC protocol.\n",
    "\n",
    "Learn more about [VPC peering](https://cloud.google.com/vertex-ai/docs/general/vpc-peering).\n",
    "\n",
    "**IMPORTANT: you can only setup one VPC peering to servicenetworking.googleapis.com per project.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d85e8f48291a"
   },
   "source": [
    "### Create VPC peering for default network\n",
    "\n",
    "For simplicity, we setup VPC peering to the default network. You can create a different network for your project.\n",
    "\n",
    "If you setup VPC peering with any other network, make sure that the network already exists and that your VM is running on that network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a107544fbabf"
   },
   "outputs": [],
   "source": [
    "# This is for display only; you can name the range anything.\n",
    "PEERING_RANGE_NAME = \"vertex-ai-prediction-peering-range\"\n",
    "NETWORK = \"default\"\n",
    "\n",
    "# NOTE: `prefix-length=16` means a CIDR block with mask /16 will be\n",
    "# reserved for use by Google services, such as Vertex AI.\n",
    "! gcloud compute addresses create $PEERING_RANGE_NAME \\\n",
    "  --global \\\n",
    "  --prefix-length=16 \\\n",
    "  --description=\"peering range for Google service\" \\\n",
    "  --network=$NETWORK \\\n",
    "  --purpose=VPC_PEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8e29cad1a0be"
   },
   "source": [
    "### Create the VPC connection\n",
    "\n",
    "Next, create the connection for VPC peering.\n",
    "\n",
    "*Note:* If you get a PERMISSION DENIED, you may not have the neccessary role 'Compute Network Admin' set for your default service account. In the Cloud Console, do the following steps.\n",
    "\n",
    "1. Goto `IAM & Admin`\n",
    "2. Find your service account.\n",
    "3. Click edit icon.\n",
    "4. Select `Add Another Role`.\n",
    "5. Enter 'Compute Network Admin'.\n",
    "6. Select `Save`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3f6c85ffc63"
   },
   "outputs": [],
   "source": [
    "! gcloud services vpc-peerings connect \\\n",
    "  --service=servicenetworking.googleapis.com \\\n",
    "  --network=$NETWORK \\\n",
    "  --ranges=$PEERING_RANGE_NAME \\\n",
    "  --project=$PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "944d772b1397"
   },
   "source": [
    "Check the status of your peering connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b946ce37cc16"
   },
   "outputs": [],
   "source": [
    "! gcloud compute networks peerings list --network $NETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a5e1b83ae61"
   },
   "source": [
    "#### Construct the full network name\n",
    "\n",
    "You need to have the full network resource name when you subsequently create an `Matching Engine Index Endpoint` resource for VPC peering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cd58eb809f71"
   },
   "outputs": [],
   "source": [
    "full_network_name = f\"projects/{PROJECT_NUMBER}/global/networks/{NETWORK}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qV2xjAnDDObD"
   },
   "source": [
    "### Create an IndexEndpoint with VPC Network\n",
    "\n",
    "Next, you create a `Matching Engine Index Endpoint`, similar to the concept of creating a `Private Endpoint` for prediction with a peer-to-peer network.\n",
    "\n",
    "To create the `Index Endpoint` resource, you call the method `create()` with the following parameters:\n",
    "\n",
    "- `display_name`: A human readable name for the `Index Endpoint`.\n",
    "- `description`: A description for the `Index Endpoint`.\n",
    "- `network`: The VPC network resource name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QuARXzJVGyQX"
   },
   "outputs": [],
   "source": [
    "index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "    display_name=\"index_endpoint_for_demo\",\n",
    "    description=\"index endpoint description\",\n",
    "    network=full_network_name,\n",
    ")\n",
    "\n",
    "INDEX_ENDPOINT_NAME = index_endpoint.resource_name\n",
    "print(INDEX_ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Ew1UgcIIiJG"
   },
   "source": [
    "### Deploy the `Matching Engine Index` to the `Index Endpoint` resource\n",
    "\n",
    "Next, deploy your index to the `Index Endpoint` using the method `deploy_index()` with the following parameters:\n",
    "\n",
    "- `display_name`: A human readable name for the deployed index.\n",
    "- `index`: Your index.\n",
    "- `deployed_index_id`: A user assigned identifier for the deployed index.\n",
    "- `machine_type`: (optional) The VM instance type.\n",
    "- `min_replica_count`: (optional) Minimum number of VM instances for auto-scaling.\n",
    "- `max_replica_count`: (optional) Maximum number of VM instances for auto-scaling.\n",
    "\n",
    "Learn more about [Machine resources for Index Endpoint](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.indexEndpoints#DeployedIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_uK4WOgqN1NG"
   },
   "outputs": [],
   "source": [
    "DEPLOYED_INDEX_ID = \"tree_ah_twotower_deployed_\" + TIMESTAMP\n",
    "\n",
    "MIN_NODES = 1\n",
    "MAX_NODES = 2\n",
    "DEPLOY_COMPUTE = \"n1-standard-16\"\n",
    "\n",
    "index_endpoint.deploy_index(\n",
    "    display_name=\"deployed_index_for_demo\",\n",
    "    index=tree_ah_index,\n",
    "    deployed_index_id=DEPLOYED_INDEX_ID,\n",
    "    machine_type=DEPLOY_COMPUTE,\n",
    "    min_replica_count=MIN_NODES,\n",
    "    max_replica_count=MAX_NODES,\n",
    ")\n",
    "\n",
    "print(index_endpoint.deployed_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LCGvBNvBd8D"
   },
   "source": [
    "### Create and execute an online query\n",
    "\n",
    "Now that your index is deployed, you can make queries.\n",
    "\n",
    "First, you construct a vector `query` using synthetic data, to use as the example to return matches for.\n",
    "\n",
    "Next, you make the matching request using the method `match()`, with the following parameters:\n",
    "\n",
    "- `deployed_index_id`:  The identifier of the deployed index.\n",
    "- `queries`: A list of queries (instances).\n",
    "- `num_neighbors`: The number of closest matches to return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0549c26c8b3c"
   },
   "outputs": [],
   "source": [
    "# The number of nearest neighbors to be retrieved from database for each query.\n",
    "NUM_NEIGHBOURS = 10\n",
    "\n",
    "# Test query\n",
    "queries = [embeddings[0][\"embedding\"], embeddings[1][\"embedding\"]]\n",
    "\n",
    "matches = index_endpoint.match(\n",
    "    deployed_index_id=DEPLOYED_INDEX_ID, queries=queries, num_neighbors=NUM_NEIGHBOURS\n",
    ")\n",
    "\n",
    "for instance in matches:\n",
    "    print(\"INSTANCE\")\n",
    "    for match in instance:\n",
    "        print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5GBb7X1reVSN"
   },
   "outputs": [],
   "source": [
    "# Delete endpoint resource\n",
    "endpoint.delete(force=True)\n",
    "\n",
    "# Delete model resource\n",
    "model.delete()\n",
    "\n",
    "# Force undeployment of indexes and delete endpoint\n",
    "try:\n",
    "    index_endpoint.delete(force=True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Delete indexes\n",
    "try:\n",
    "    tree_ah_index.delete()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Delete Cloud Storage objects that were created\n",
    "delete_bucket = False\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil -m rm -r $OUTPUT_DIR"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "get_started_with_matching_engine_swivel.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

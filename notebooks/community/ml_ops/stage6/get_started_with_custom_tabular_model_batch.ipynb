{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title:generic,gcp"
      },
      "source": [
        "# E2E ML on GCP: MLOps stage 6 : Get started with Vertex AI Batch Prediction for custom tabular models\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage6/get_started_with_custom_tabular_model_batch.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage6/get_started_with_custom_tabular_model_batch.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/ml_ops/stage6/get_started_with_custom_tabular_model_batch.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>        \n",
        "</table>\n",
        "<br/><br/><br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial demonstrates how to do batch predictions from a custom tabular model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9402cfbdc2d"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to use `Vertex AI Batch Prediction`  with a custom tabular model.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services and resources:\n",
        "\n",
        "- `Vertex AI Batch Prediction`\n",
        "- `Vertex AI Models`\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Upload a pretrained tabular model as a `Vertex AI Model` resource.\n",
        "- Make batch prediction to the `Model` resource, in JSONL format.\n",
        "- Make batch prediction to the `Model` resource, in CSV format.\n",
        "- Make batch prediction to the `Model` resource, in BigQuery format.\n",
        "\n",
        "There is one key difference between using batch prediction and using online prediction:\n",
        "\n",
        "* Prediction Service: Does an on-demand prediction for the entire set of instances (i.e., one or more data items) and returns the results in real-time.\n",
        "\n",
        "* Batch Prediction Service: Does a queued (batch) prediction for the entire set of instances in the background and stores the results in a Cloud Storage bucket when ready."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edba71dc9840"
      },
      "source": [
        "### Models\n",
        "\n",
        "**JSONL batch input example**\n",
        "\n",
        "This example in the tutorial uses a pre-trained model, where the model artifacts are stored in a public Cloud Storage bucket. \n",
        "\n",
        "The model is based on [the blog post](https://cloud.google.com/blog/topics/developers-practitioners/churn-prediction-game-developers-using-google-analytics-4-ga4-and-bigquery-ml). The idea behind this model is that your company has extensive log data describing how your game users have interacted with the site. The raw data contains the following categories of information:\n",
        "\n",
        "- identity - unique player identitity numbers\n",
        "- demographic features - information about the player, such as the geographic region in which a player is located\n",
        "- behavioral features - counts of the number of times a  player has triggered certain game events, such as reaching a new level\n",
        "- churn propensity - this is the label or target feature, it provides an estimated probability that this player may churn, i.e. stop being an active player.\n",
        "\n",
        "**CSV batch input example**\n",
        "\n",
        "This example in the tutorial uses a pre-trained model, where the model artifacts are stored in a public Cloud Storage bucket. The model is an XGBoost model trained on the [Iris dataset](https://www.tensorflow.org/datasets/catalog/iris) from [TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/overview). The trained model predicts the type of Iris flower species from a class of three species: setosa, virginica, or versicolor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "costs"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* BigQuery\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertext AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_aip"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Install the following packages to execute this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_aip"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Vertex AI Workbench Notebook product has specific requirements\n",
        "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
        "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
        "    \"/opt/deeplearning/metadata/env_version\"\n",
        ")\n",
        "\n",
        "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_WORKBENCH_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\"\n",
        "\n",
        "! pip3 install {USER_FLAG} --quiet --upgrade google-cloud-aiplatform\n",
        "! pip3 install {USER_FLAG} --quiet --upgrade google-cloud-bigquery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhq5zEbGg0XX"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzrelQZ22IZj"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "before_you_begin"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### GPU runtime\n",
        "\n",
        "*Make sure you're running this notebook in a GPU runtime if you have that option. In Colab, select* **Runtime > Change Runtime Type > GPU**\n",
        "\n",
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "2. [Make sure that billing is enabled for your project.](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
        "\n",
        "3. [Enable the following APIs: Vertex AI APIs, Compute Engine APIs, and Cloud Storage.](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component,storage-component.googleapis.com)\n",
        "\n",
        "4. If you are running this notebook locally, you need to install the [Cloud SDK]((https://cloud.google.com/sdk)).\n",
        "\n",
        "5. Enter your project ID in the cell below. Then run the  cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "project_id"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_project_id"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_project_id"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID:\", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_gcloud_project_id"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
        "\n",
        "- Americas: `us-central1`\n",
        "- Europe: `europe-west4`\n",
        "- Asia Pacific: `asia-east1`\n",
        "\n",
        "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
        "\n",
        "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "region"
      },
      "outputs": [],
      "source": [
        "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
        "\n",
        "if REGION == \"[your-region]\":\n",
        "    REGION = \"us-central1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "timestamp"
      },
      "source": [
        "#### UUID\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a uuid for each instance session, and append it onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "timestamp"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "\n",
        "# Generate a uuid of a specifed length(default=8)\n",
        "def generate_uuid(length: int = 8) -> str:\n",
        "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
        "\n",
        "\n",
        "UUID = generate_uuid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcp_authenticate"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using Vertex AI Workbench Notebooks**, your environment is already authenticated.  \n",
        "\n",
        "**If you are using Colab**, run the cell below and follow the instructions when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "In the Cloud Console, go to the [Create service account key](https://console.cloud.google.com/apis/credentials/serviceaccountkey) page.\n",
        "\n",
        "**Click Create service account**.\n",
        "\n",
        "In the **Service account name** field, enter a name, and click **Create**.\n",
        "\n",
        "In the **Grant this service account access to project** section, click the Role drop-down list. Type \"Vertex\" into the filter box, and select **Vertex Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "\n",
        "Click Create. A JSON file that contains your key downloads to your local environment.\n",
        "\n",
        "Enter the path to your service account key as the GOOGLE_APPLICATION_CREDENTIALS variable in the cell below and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcp_authenticate"
      },
      "outputs": [],
      "source": [
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# If on Vertex AI Workbench, then don't execute this code\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
        "    \"DL_ANACONDA_HOME\"\n",
        "):\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bucket:mbsdk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "When you initialize the Vertex AI SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bucket"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_bucket"
      },
      "outputs": [],
      "source": [
        "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
        "    BUCKET_NAME = PROJECT_ID + \"aip-\" + UUID\n",
        "    BUCKET_URI = \"gs://\" + BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_bucket"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "validate_bucket"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "validate_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_vars"
      },
      "source": [
        "### Set up variables\n",
        "\n",
        "Next, set up some variables used throughout the tutorial.\n",
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "import google.cloud.aiplatform as aiplatform\n",
        "from google.cloud import bigquery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_bq"
      },
      "source": [
        "### Create BigQuery client\n",
        "\n",
        "In this tutorial, you use data from the same public BigQuery table that was used to train the pre-trained model. You create a client interface, which you subsequently use to access the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_bq"
      },
      "outputs": [],
      "source": [
        "bqclient = bigquery.Client(project=PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "accelerators:training,cpu,prediction,cpu,mbsdk"
      },
      "source": [
        "#### Set hardware accelerators\n",
        "\n",
        "You can set hardware accelerators for training and prediction.\n",
        "\n",
        "Set the variables `DEPLOY_GPU/DEPLOY_NGPU` to use a container image supporting a GPU and the number of GPUs allocated to the virtual machine (VM) instance. For example, to use a GPU container image with 4 Nvidia Telsa K80 GPUs allocated to each VM, you would specify:\n",
        "\n",
        "    (aip.AcceleratorType.NVIDIA_TESLA_K80, 4)\n",
        "\n",
        "\n",
        "Otherwise specify `(None, None)` to use a container image to run on a CPU.\n",
        "\n",
        "Learn more about [hardware accelerator support for your region](https://cloud.google.com/vertex-ai/docs/general/locations#accelerators).\n",
        "\n",
        "*Note*: TF releases before 2.3 for GPU support fails to load the custom model in this tutorial. It is a known issue and fixed in TF 2.3. This is caused by static graph ops that are generated in the serving function. If you encounter this issue on your own custom models, use a container image for TF 2.3 with GPU support."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "accelerators:training,cpu,prediction,cpu,mbsdk"
      },
      "outputs": [],
      "source": [
        "if os.getenv(\"IS_TESTING_DEPLOY_GPU\"):\n",
        "    DEPLOY_GPU, DEPLOY_NGPU = (\n",
        "        aiplatform.gapic.AcceleratorType.NVIDIA_TESLA_K80,\n",
        "        int(os.getenv(\"IS_TESTING_DEPLOY_GPU\")),\n",
        "    )\n",
        "else:\n",
        "    DEPLOY_GPU, DEPLOY_NGPU = (None, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "container:training,prediction"
      },
      "source": [
        "#### Set pre-built containers\n",
        "\n",
        "Set the pre-built Docker container image for prediction.\n",
        "\n",
        "\n",
        "For the latest list, see [Pre-built containers for prediction](https://cloud.google.com/ai-platform-unified/docs/predictions/pre-built-containers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1u1mr18jlugv"
      },
      "outputs": [],
      "source": [
        "if DEPLOY_GPU:\n",
        "    DEPLOY_VERSION = \"tf2-gpu.2-5\"\n",
        "else:\n",
        "    DEPLOY_VERSION = \"tf2-cpu.2-5\"\n",
        "\n",
        "DEPLOY_IMAGE = \"{}-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(\n",
        "    REGION.split(\"-\")[0], DEPLOY_VERSION\n",
        ")\n",
        "\n",
        "print(\"Deployment:\", DEPLOY_IMAGE, DEPLOY_GPU, DEPLOY_NGPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "machine:training"
      },
      "source": [
        "#### Set machine type\n",
        "\n",
        "Next, set the machine type to use for prediction.\n",
        "\n",
        "- Set the variable `DEPLOY_COMPUTE` to configure  the compute resources for the VMs you use for for prediction.\n",
        " - `machine type`\n",
        "     - `n1-standard`: 3.75GB of memory per vCPU.\n",
        "     - `n1-highmem`: 6.5GB of memory per vCPU\n",
        "     - `n1-highcpu`: 0.9 GB of memory per vCPU\n",
        " - `vCPUs`: number of \\[2, 4, 8, 16, 32, 64, 96 \\]\n",
        "\n",
        "*Note: You may also use n2 and e2 machine types for training and deployment, but they do not support GPUs*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "machine:training"
      },
      "outputs": [],
      "source": [
        "if os.getenv(\"IS_TESTING_DEPLOY_MACHINE\"):\n",
        "    MACHINE_TYPE = os.getenv(\"IS_TESTING_DEPLOY_MACHINE\")\n",
        "else:\n",
        "    MACHINE_TYPE = \"n1-standard\"\n",
        "\n",
        "VCPU = \"4\"\n",
        "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
        "print(\"Train machine type\", DEPLOY_COMPUTE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bf06cd476e9"
      },
      "source": [
        "### Upload the model artifacts as a `Vertex AI Model` resource\n",
        "\n",
        "First, you upload the pre-trained custom tabular model artifacts as a `Vertex AI Model` resource using the `upload()` method, with the following parameters:\n",
        "\n",
        "- `display_name`: The human readable name for the `Model` resource.\n",
        "- `artifact_uri`: The Cloud Storage location of the model artifacts.\n",
        "- `serving_container_image`: The serving container image to use when the model is deployed to a `Vertex AI Endpoint` resource.\n",
        "- `sync`: Whether to wait for the process to complete, or return immediately (async)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0193f247e216"
      },
      "outputs": [],
      "source": [
        "MODEL_ARTIFACT_URI = \"gs://mco-mm/churn\"\n",
        "\n",
        "model = aiplatform.Model.upload(\n",
        "    display_name=\"churn_\" + UUID,\n",
        "    artifact_uri=MODEL_ARTIFACT_URI,\n",
        "    serving_container_image_uri=DEPLOY_IMAGE,\n",
        "    sync=True,\n",
        ")\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11e16f54bc90"
      },
      "source": [
        "## Introduction to Batch Prediction\n",
        "\n",
        "Batch prediction provides the ability to do offline batch processing of large amounts of prediction requests. Resources are only provisioned during the batch process and then deprovisioned when the batch request is completed. The results are stored in Cloud Storage, in contrast to online prediction where the results are returned as a HTTP response packet.\n",
        "\n",
        "The input format for your batch job is dependent on the format supported by your model server. Foremost, the web server in your model server must support a JSONL format, which the web server converts to a format support either directly by the model input intertace or a serving function interface. For batch prediction, this JSONL format is referred to as the `pivot` format.\n",
        "\n",
        "### Input format for batch prediction jobs\n",
        "\n",
        "The batch server accepts the following input formats for custom tabular models:\n",
        "\n",
        "- JSONL\n",
        "- CSV\n",
        "- BigQuery table\n",
        "\n",
        "### Output format for batch prediction jobs\n",
        "\n",
        "The batch server accepts the following output formats for custom tabular models:\n",
        "\n",
        "- JSONL\n",
        "- BigQuery table (when input is BigQuery table)\n",
        "\n",
        "### Pivot format\n",
        "\n",
        "The batch server converts the input format to the `pivot` (JSONL) format as follows:\n",
        "\n",
        "**JSONL**\n",
        "\n",
        "Each input line (request) should contain one and only one valid json value.\n",
        "\n",
        "    {\"values\": [1, 2, 3, 4], \"key\": 1}\n",
        "    {\"values\": [5, 6, 7, 8], \"key\": 2}\n",
        "\n",
        "The batch server generates the pivot data with the same format. The generated pivot data is then wrapped into a payload request:\n",
        "\n",
        "    {\"instances\": [\n",
        "      {\"values\": [1, 2, 3, 4], \"key\": 1},\n",
        "      {\"values\": [5, 6, 7, 8], \"key\": 2}\n",
        "    ]}\n",
        "\n",
        "**CSV**\n",
        "\n",
        "The csv header in the first line is always be ignored. String fields are required to be double quoted explicitly, otherwise the row is discarded and parsing error messages are outputted to error files. Non-quoted values are always transferred as floats.\n",
        "\n",
        "    col1,col2,col3\n",
        "    1,3,\"cat1\"\n",
        "    2,4,\"cat2\"\n",
        "\n",
        "The batch server converts each input row (request) to a JSON array.\n",
        "\n",
        "    {\"instances\": [\n",
        "     [1.0,3.0,\"cat1\"],\n",
        "     [2.0,4.0,\"cat2\"]\n",
        "    ]}\n",
        "    \n",
        "**BigQuery**\n",
        "\n",
        "Each row is converted to a JSON array. For example:\n",
        "\n",
        "    [1.0,3.0,\"cat1\"]\n",
        "    [2.0,4.0,\"cat2\"]\n",
        "    \n",
        "The batch server generates the pivot data with the same format. The generated pivot data is then wrapped into a payload request:\n",
        "\n",
        "    {\"instances\": [\n",
        "     [1.0,3.0,\"cat1\"],\n",
        "     [2.0,4.0,\"cat2\"]\n",
        "    ]}\n",
        "\n",
        "**TFRecords**\n",
        "\n",
        "Instances in TFRecord files are read as binary by apache_beam.io.tfrecordio module. The binary objects are then serialized as ASCII strings. Predictor server is responsible to know the decoder to recover the instance.  \n",
        "\n",
        "    {\"instances\": [\n",
        "     {\"b64\",\"b64EncodedASCIIString\"},\n",
        "     {\"b64\",\"b64EncodedASCIIString\"}\n",
        "    ]}\n",
        "\n",
        "**FileList**\n",
        "\n",
        "The FileList format contains a list of files. Each line in a “FileList” file specifies a single file path, specified as a Cloud Storage location.\n",
        "\n",
        "    gs://my-bucket/file1.txt\n",
        "    gs://my-bucket/file2.txt\n",
        "\n",
        "The batch server reads the files as binaries. The binary objects are serialized as ASCII strings.\n",
        "\n",
        "    {\"instances\": [\n",
        "     {\"b64\",\"b64EncodedASCIIString\"},\n",
        "     {\"b64\",\"b64EncodedASCIIString\"}\n",
        "    ]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "make_prediction"
      },
      "source": [
        "## Make a batch prediction request\n",
        "\n",
        "Send a batch prediction request to your `Vertex AI Model` resource."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26bac245f6c5"
      },
      "source": [
        "### Generate batch prediction data\n",
        "\n",
        "Next, you extract the first 10 instances from the BigQuery training table to use for batch prediction requests. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb26c3dea306"
      },
      "outputs": [],
      "source": [
        "DATASET_BQ_URI = \"bq://mco-mm.bqmlga4.train\"\n",
        "TARGET = \"churned\"\n",
        "\n",
        "# Download the table.\n",
        "table = bigquery.TableReference.from_string(DATASET_BQ_URI[5:])\n",
        "\n",
        "rows = bqclient.list_rows(table, max_results=10)\n",
        "\n",
        "instances = []\n",
        "for row in rows:\n",
        "    instance = {}\n",
        "    for key, value in row.items():\n",
        "        if key == TARGET:\n",
        "            continue\n",
        "        if value is None:\n",
        "            value = \"\"\n",
        "        instance[key] = value\n",
        "    instances.append(instance)\n",
        "\n",
        "print(instances[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1e29665076f"
      },
      "source": [
        "#### Prepare data for batch prediction for JSONL\n",
        "\n",
        "Before you can run the data through batch prediction, you need to save the data into one of a few possible formats.\n",
        "\n",
        "For this tutorial, use JSONL as it's compatible with the 3-dimensional list that each image is currently represented in. To do this:\n",
        "\n",
        "1. In a file, write each instance as JSON on its own line.\n",
        "2. Upload this file to Cloud Storage.\n",
        "\n",
        "For more details on batch prediction input formats: https://cloud.google.com/vertex-ai/docs/predictions/batch-predictions#batch_request_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e6b04d29c3b"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "BATCH_PREDICTION_INSTANCES_FILE = \"batch_prediction_instances.jsonl\"\n",
        "\n",
        "BATCH_PREDICTION_GCS_SOURCE = (\n",
        "    BUCKET_URI + \"/batch_prediction_instances/\" + BATCH_PREDICTION_INSTANCES_FILE\n",
        ")\n",
        "\n",
        "# Write instances at JSONL\n",
        "with open(BATCH_PREDICTION_INSTANCES_FILE, \"w\") as f:\n",
        "    for instance in instances:\n",
        "        f.write(json.dumps(instance) + \"\\n\")\n",
        "\n",
        "# Upload to Cloud Storage bucket\n",
        "! gsutil cp $BATCH_PREDICTION_INSTANCES_FILE $BATCH_PREDICTION_GCS_SOURCE\n",
        "\n",
        "print(\"Uploaded instances to: \", BATCH_PREDICTION_GCS_SOURCE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "send_prediction_request:image"
      },
      "source": [
        "### Send the prediction request\n",
        "\n",
        "To make a batch prediction request, call the model object's `batch_predict` method with the following parameters: \n",
        "- `instances_format`: The format of the batch prediction request file: \"jsonl\", \"csv\", \"bigquery\", \"tf-record\", \"tf-record-gzip\" or \"file-list\"\n",
        "- `prediction_format`: The format of the batch prediction response file: \"jsonl\", \"csv\", \"bigquery\", \"tf-record\", \"tf-record-gzip\" or \"file-list\"\n",
        "- `job_display_name`: The human readable name for the prediction job.\n",
        " - `gcs_source`: A list of one or more Cloud Storage paths to your batch prediction requests.\n",
        "- `gcs_destination_prefix`: The Cloud Storage path that the service writes the predictions to.\n",
        "- `model_parameters`: Additional filtering parameters for serving prediction results.\n",
        "- `machine_type`: The type of machine to use for training.\n",
        "- `accelerator_type`: The hardware accelerator type.\n",
        "- `accelerator_count`: The number of accelerators to attach to a worker replica.\n",
        "- `starting_replica_count`: The number of compute instances to initially provision.\n",
        "- `max_replica_count`: The maximum number of compute instances to scale to. In this tutorial, only one instance is provisioned.\n",
        "\n",
        "### Compute instance scaling\n",
        "\n",
        "You can specify a single instance (or node) to process your batch prediction request. This tutorial uses a single node, so the variables `MIN_NODES` and `MAX_NODES` are both set to `1`.\n",
        "\n",
        "If you want to use multiple nodes to process your batch prediction request, set `MAX_NODES` to the maximum number of nodes you want to use. Vertex AI autoscales the number of nodes used to serve your predictions, up to the maximum number you set. Refer to the [pricing page](https://cloud.google.com/vertex-ai/pricing#prediction-prices) to understand the costs of autoscaling with multiple nodes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cf1076178fc"
      },
      "outputs": [],
      "source": [
        "MIN_NODES = 1\n",
        "MAX_NODES = 1\n",
        "\n",
        "# The name of the job\n",
        "BATCH_PREDICTION_JOB_NAME = \"churn_batch-\" + UUID\n",
        "\n",
        "# Folder in the bucket to write results to\n",
        "DESTINATION_FOLDER = \"batch_prediction_results\"\n",
        "\n",
        "# The Cloud Storage bucket to upload results to\n",
        "BATCH_PREDICTION_GCS_DEST_PREFIX = BUCKET_URI + \"/\" + DESTINATION_FOLDER\n",
        "\n",
        "# Make SDK batch_predict method call\n",
        "batch_prediction_job = model.batch_predict(\n",
        "    instances_format=\"jsonl\",\n",
        "    predictions_format=\"jsonl\",\n",
        "    job_display_name=BATCH_PREDICTION_JOB_NAME,\n",
        "    gcs_source=BATCH_PREDICTION_GCS_SOURCE,\n",
        "    gcs_destination_prefix=BATCH_PREDICTION_GCS_DEST_PREFIX,\n",
        "    model_parameters=None,\n",
        "    machine_type=DEPLOY_COMPUTE,\n",
        "    accelerator_type=DEPLOY_GPU,\n",
        "    accelerator_count=DEPLOY_NGPU,\n",
        "    starting_replica_count=MIN_NODES,\n",
        "    max_replica_count=MAX_NODES,\n",
        "    sync=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "get_batch_prediction:mbsdk,custom,icn"
      },
      "source": [
        "### Get the predictions\n",
        "\n",
        "Next, get the results from the completed batch prediction job.\n",
        "\n",
        "The results are written to the Cloud Storage output bucket you specified in the batch prediction request. You call the method iter_outputs() to get a list of each Cloud Storage file generated with the results. Each file contains one or more prediction requests in a JSON format:\n",
        "\n",
        "- `instance`: The prediction request.\n",
        "- `prediction`: The prediction response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "get_batch_prediction:mbsdk,custom,icn"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "bp_iter_outputs = batch_prediction_job.iter_outputs()\n",
        "\n",
        "prediction_results = list()\n",
        "for blob in bp_iter_outputs:\n",
        "    if blob.name.split(\"/\")[-1].startswith(\"prediction\"):\n",
        "        prediction_results.append(blob.name)\n",
        "\n",
        "tags = list()\n",
        "for prediction_result in prediction_results:\n",
        "    gfile_name = f\"gs://{bp_iter_outputs.bucket.name}/{prediction_result}\"\n",
        "    with tf.io.gfile.GFile(name=gfile_name, mode=\"r\") as gfile:\n",
        "        for line in gfile.readlines():\n",
        "            line = json.loads(line)\n",
        "            print(line)\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7d6770721fe"
      },
      "source": [
        "#### Delete the batch prediction job\n",
        "\n",
        "You can delete your batch prediction job using the `delete()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f7f80787766"
      },
      "outputs": [],
      "source": [
        "batch_prediction_job.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9752e41c645c"
      },
      "source": [
        "#### Delete the model\n",
        "\n",
        "You can delete your model using the `delete()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0ae951df7ca"
      },
      "outputs": [],
      "source": [
        "model.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bcc1e8bd57d"
      },
      "source": [
        "## Batch prediction with CSV input format\n",
        "\n",
        "To use CSV as the input format, your model must take its input as a list (array) of values. The previous pretrained model (`churn`) took its input as a dictionary (key/value pair).\n",
        "\n",
        "For this example, you upload a pretrained XGBoost model, trained on the Iris dataset. This model takes its input as a list of values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0193f247e216"
      },
      "outputs": [],
      "source": [
        "MODEL_ARTIFACT_URI = \"gs://cloud-samples-data/vertex-ai/google-cloud-aiplatform-ci-artifacts/models/iris_xgboost/\"\n",
        "\n",
        "DEPLOY_IMAGE = (\n",
        "    f\"{REGION.split('-')[0]}-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-5:latest\"\n",
        ")\n",
        "\n",
        "model = aiplatform.Model.upload(\n",
        "    display_name=\"churn_\" + UUID,\n",
        "    artifact_uri=MODEL_ARTIFACT_URI,\n",
        "    serving_container_image_uri=DEPLOY_IMAGE,\n",
        "    sync=True,\n",
        ")\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b770c8154834"
      },
      "source": [
        "### Prepare batch prediction request for CSV \n",
        "\n",
        "Next, you format the prediction request instances in CSV format. In this example, you obtain the data for the instances from the dataset stored in a public Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "776346c90074"
      },
      "outputs": [],
      "source": [
        "data_file = \"gs://cloud-samples-data/ai-platform/iris/iris_data.csv\"\n",
        "! gsutil cat $data_file | head -n 10 > test.csv\n",
        "\n",
        "! cat test.csv\n",
        "\n",
        "BATCH_PREDICTION_INSTANCES_FILE = \"batch_prediction_instances.csv\"\n",
        "\n",
        "BATCH_PREDICTION_GCS_SOURCE = (\n",
        "    BUCKET_URI + \"/batch_prediction_instances/\" + BATCH_PREDICTION_INSTANCES_FILE\n",
        ")\n",
        "\n",
        "! gsutil cp test.csv $BATCH_PREDICTION_GCS_SOURCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "send_prediction_request:image"
      },
      "source": [
        "### Send the batch prediction request\n",
        "\n",
        "Again, you make a batch prediction request with the `batch_predict()` method, but where the parameter `instances_format` is set to \"csv\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cf1076178fc"
      },
      "outputs": [],
      "source": [
        "MIN_NODES = 1\n",
        "MAX_NODES = 1\n",
        "\n",
        "# The name of the job\n",
        "BATCH_PREDICTION_JOB_NAME = \"churn_batch-\" + UUID\n",
        "\n",
        "# Folder in the bucket to write results to\n",
        "DESTINATION_FOLDER = \"batch_prediction_results2\"\n",
        "\n",
        "# The Cloud Storage bucket to upload results to\n",
        "BATCH_PREDICTION_GCS_DEST_PREFIX = BUCKET_URI + \"/\" + DESTINATION_FOLDER\n",
        "\n",
        "# Make SDK batch_predict method call\n",
        "batch_prediction_job = model.batch_predict(\n",
        "    instances_format=\"csv\",\n",
        "    predictions_format=\"jsonl\",\n",
        "    job_display_name=BATCH_PREDICTION_JOB_NAME,\n",
        "    gcs_source=BATCH_PREDICTION_GCS_SOURCE,\n",
        "    gcs_destination_prefix=BATCH_PREDICTION_GCS_DEST_PREFIX,\n",
        "    model_parameters=None,\n",
        "    machine_type=DEPLOY_COMPUTE,\n",
        "    accelerator_type=DEPLOY_GPU,\n",
        "    accelerator_count=DEPLOY_NGPU,\n",
        "    starting_replica_count=MIN_NODES,\n",
        "    max_replica_count=MAX_NODES,\n",
        "    sync=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "get_batch_prediction:mbsdk,custom,icn"
      },
      "source": [
        "### Get the predictions\n",
        "\n",
        "Next, get the results from the completed batch prediction job.\n",
        "\n",
        "The results are written to the Cloud Storage output bucket you specified in the batch prediction request. You call the method iter_outputs() to get a list of each Cloud Storage file generated with the results. Each file contains one or more prediction requests in a JSON format:\n",
        "\n",
        "- `instance`: The prediction request.\n",
        "- `prediction`: The prediction response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "get_batch_prediction:mbsdk,custom,icn"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "bp_iter_outputs = batch_prediction_job.iter_outputs()\n",
        "\n",
        "prediction_results = list()\n",
        "for blob in bp_iter_outputs:\n",
        "    if blob.name.split(\"/\")[-1].startswith(\"prediction\"):\n",
        "        prediction_results.append(blob.name)\n",
        "\n",
        "tags = list()\n",
        "for prediction_result in prediction_results:\n",
        "    gfile_name = f\"gs://{bp_iter_outputs.bucket.name}/{prediction_result}\"\n",
        "    with tf.io.gfile.GFile(name=gfile_name, mode=\"r\") as gfile:\n",
        "        for line in gfile.readlines():\n",
        "            line = json.loads(line)\n",
        "            print(line)\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ce65fff362b"
      },
      "source": [
        "#### Delete the batch prediction job\n",
        "\n",
        "You can delete your batch prediction job using the `delete()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5943dfde5123"
      },
      "outputs": [],
      "source": [
        "batch_prediction_job.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d61289b87070"
      },
      "source": [
        "## Batch prediction with BigQuery input format\n",
        "\n",
        "Next, you do the same batch job, except the input format is a BigQuery table. When the input format is a BigQuery table, the output format has to be a BigQuery table as well. To use BigQuery as the input format, your model must take its input as a list (array) of values. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csv_to_bq"
      },
      "source": [
        "### Create a BigQuery dataset from CSV files\n",
        "\n",
        "You can create a BigQuery dataset from CSV files using the BigQuery `create_dataset()` and `load_table_from_uri()` methods, as follows:\n",
        "\n",
        "- `create_dataset()`: Creates an empty BigQuery dataset, with the following parameters:\n",
        " - `dataset_ref`: The `DatasetReference` created from the dataset_id -- e.g., samples.\n",
        "- `load_table_from_uri()`: Loads one or more CSV files into a table within the corresponding dataset, with the following parameters:\n",
        " - `url`: A set of one or more CVS files in Cloud Storage storage.\n",
        " - `table`: The `TableReference` for the table.\n",
        " - `job_config`: Specifications on how to load the CSV data.\n",
        "\n",
        "Learn more about [Importing CSV data into BigQuery](https://www.tensorflow.org/io/tutorials/bigquery#import_census_data_into_bigquery)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csv_to_bq"
      },
      "outputs": [],
      "source": [
        "LOCATION = \"us\"\n",
        "\n",
        "CSV_SCHEMA = [\n",
        "    bigquery.SchemaField(\"sepal_length\", \"FLOAT\"),\n",
        "    bigquery.SchemaField(\"sepal_width\", \"FLOAT\"),\n",
        "    bigquery.SchemaField(\"petal_length\", \"FLOAT\"),\n",
        "    bigquery.SchemaField(\"petal_width\", \"FLOAT\"),\n",
        "]\n",
        "\n",
        "DATASET_ID = \"batch\"\n",
        "TABLE_ID = \"slice1\"\n",
        "\n",
        "\n",
        "def create_bigquery_dataset(dataset_id):\n",
        "    dataset = bigquery.Dataset(\n",
        "        bigquery.dataset.DatasetReference(PROJECT_ID, dataset_id)\n",
        "    )\n",
        "    dataset.location = \"us\"\n",
        "\n",
        "    try:\n",
        "        dataset = bqclient.create_dataset(dataset)  # API request\n",
        "        return True\n",
        "    except Exception as err:\n",
        "        print(err)\n",
        "        if err.code != 409:  # http_client.CONFLICT\n",
        "            raise\n",
        "    return False\n",
        "\n",
        "\n",
        "def load_data_into_bigquery(url, dataset_id, table_id):\n",
        "    create_bigquery_dataset(dataset_id)\n",
        "    dataset = bqclient.dataset(dataset_id)\n",
        "    table = dataset.table(table_id)\n",
        "\n",
        "    job_config = bigquery.LoadJobConfig()\n",
        "    job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
        "    job_config.source_format = bigquery.SourceFormat.CSV\n",
        "    job_config.schema = CSV_SCHEMA\n",
        "    job_config.skip_leading_rows = 1  # heading\n",
        "\n",
        "    load_job = bqclient.load_table_from_uri(url, table, job_config=job_config)\n",
        "    print(\"Starting job {}\".format(load_job.job_id))\n",
        "\n",
        "    load_job.result()  # Waits for table load to complete.\n",
        "    print(\"Job finished.\")\n",
        "\n",
        "    destination_table = bqclient.get_table(table)\n",
        "    print(\"Loaded {} rows.\".format(destination_table.num_rows))\n",
        "\n",
        "    return destination_table\n",
        "\n",
        "\n",
        "bq_table = load_data_into_bigquery(data_file, DATASET_ID, TABLE_ID)\n",
        "print(bq_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "send_prediction_request:image"
      },
      "source": [
        "### Send the batch prediction request\n",
        "\n",
        "Again, you make a batch prediction request with the `batch_predict()` method, but with the following changes in parameters:\n",
        "\n",
        "- `instances_format`: Set to 'bigquery'\n",
        "- `predictions_format`: Set to 'bigquery'\n",
        "- `bigquery_source`: Used instead of `gcs_source`.\n",
        "- `bigquery_destination_prefix`: Used instead of `gcs_destination_prefix`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cf1076178fc"
      },
      "outputs": [],
      "source": [
        "MIN_NODES = 1\n",
        "MAX_NODES = 1\n",
        "\n",
        "# The name of the job\n",
        "BATCH_PREDICTION_JOB_NAME = \"churn_batch-\" + UUID\n",
        "\n",
        "# Folder in the bucket to write results to\n",
        "DESTINATION_FOLDER = \"batch_prediction_results_bq\"\n",
        "\n",
        "# The Cloud Storage bucket to upload results to\n",
        "BATCH_PREDICTION_GCS_DEST_PREFIX = BUCKET_URI + \"/\" + DESTINATION_FOLDER\n",
        "\n",
        "BQ_TABLE_SOURCE = f\"bq://{PROJECT_ID}.{bq_table.dataset_id}.{bq_table.table_id}\"\n",
        "\n",
        "# Make SDK batch_predict method call\n",
        "batch_prediction_job = model.batch_predict(\n",
        "    instances_format=\"bigquery\",\n",
        "    predictions_format=\"bigquery\",\n",
        "    job_display_name=BATCH_PREDICTION_JOB_NAME,\n",
        "    bigquery_source=BQ_TABLE_SOURCE,\n",
        "    bigquery_destination_prefix=f\"bq://{PROJECT_ID}.batch\",\n",
        "    model_parameters=None,\n",
        "    machine_type=DEPLOY_COMPUTE,\n",
        "    accelerator_type=DEPLOY_GPU,\n",
        "    accelerator_count=DEPLOY_NGPU,\n",
        "    starting_replica_count=MIN_NODES,\n",
        "    max_replica_count=MAX_NODES,\n",
        "    sync=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "get_batch_prediction:mbsdk,custom,icn"
      },
      "source": [
        "### Get the predictions\n",
        "\n",
        "Next, get the results from the completed batch prediction job.\n",
        "\n",
        "The results are written to a BigQuery table at the BigQuery dataset path you specified as the destination. The batch server creates the table, where the table location is specified by:\n",
        "\n",
        "`batch_prediction_job.output_info.bigquery_output_dataset`: The project and dataset components.\n",
        "`batch_prediction_job.output_info.bigquery_output_table`: The table component.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd515ed616c8"
      },
      "outputs": [],
      "source": [
        "BQ_RESULTS_TABLE = (\n",
        "    batch_prediction_job.output_info.bigquery_output_dataset\n",
        "    + \".\"\n",
        "    + batch_prediction_job.output_info.bigquery_output_table\n",
        ")\n",
        "\n",
        "print(BQ_RESULTS_TABLE)\n",
        "\n",
        "table = bigquery.TableReference.from_string(BQ_RESULTS_TABLE[5:])\n",
        "\n",
        "rows = bqclient.list_rows(table, max_results=10)\n",
        "\n",
        "for row in rows:\n",
        "    print(row)\n",
        "    for key, value in row.items():\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7cda79d0e42"
      },
      "source": [
        "#### Delete the batch prediction job\n",
        "\n",
        "You can delete your batch prediction job using the `delete()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de4b5c638c2a"
      },
      "outputs": [],
      "source": [
        "batch_prediction_job.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5d4b9c51e86"
      },
      "source": [
        "#### Delete the model\n",
        "\n",
        "You can delete your model using the `delete()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00fa6a7b4f24"
      },
      "outputs": [],
      "source": [
        "model.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx_vKniMq9ZX"
      },
      "outputs": [],
      "source": [
        "delete_bucket = False\n",
        "\n",
        "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
        "    ! gsutil rm -rf {BUCKET_URI}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "get_started_with_custom_tabular_model_batch.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

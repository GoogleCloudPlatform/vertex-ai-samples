{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title:generic,gcp"
      },
      "source": [
        "# E2E ML on GCP: MLOps stage 6 : Get started with Custom Prediction Routine (CPR)\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage6/get_started_with_custom_predictions.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/ai/platform/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage6/get_started_with_custom_predictions.ipynb\">\n",
        "      Open in Vertex Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "<br/><br/><br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial demonstrates how to use Vertex AI SDK to build a custom container that uses the Custom Prediction Routine model server to serve a scikit-learn model on Vertex AI Predictions. This is currently an **experimental** feature and is not yet officially supported by the Vertex AI SDK. In this tutorial, you'll be installing the Vertex AI SDK from an experimental branch on github. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abaeafc5d6f8"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to use Custom Prediction Routine (CPR) for `Vertex AI Predictions`.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services:\n",
        "\n",
        "- `Vertex AI Training`\n",
        "- `Vertex AI Predictions`\n",
        "- `Vertex AI Custom Predictions`\n",
        "- `Google Artifact Registry`\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Write a custom data preprocessor.\n",
        "- Train the model.\n",
        "- Build a custom scikit-learn serving container with custom data preprocessing using the Custom Prediction Routine model server.\n",
        "    - Test the model serving container locally.\n",
        "    - Upload and deploy the model serving container to Vertex AI Endpoint.\n",
        "    - Make a prediction request.\n",
        "- Build a custom scikit-learn serving container with custom predictor (post-processing) using the Custom Prediction Routine model server.\n",
        "    - Implement custom predictor.\n",
        "    - Test the model serving container locally.\n",
        "    - Upload and deploy the model serving container to Vertex AI Endpoint.\n",
        "    - Make a prediction request.\n",
        "- Build a custom scikit-learn serving container with custom predictor and HTTP request handler using the Custom Prediction Routine model server.\n",
        "    - Implement a custom handler.\n",
        "    - Test the model serving container locally.\n",
        "    - Upload and deploy the model serving container to Vertex AI Endpoint.\n",
        "    - Make a prediction request.\n",
        "- Customize the Dockerfile for a custom scikit-learn serving container with custom predictor and HTTP request handler using the Custom Prediction Routine model server.\n",
        "    - Implement a custom Dockerfile.\n",
        "    - Test the model serving container locally.\n",
        "    - Upload and deploy the model serving container to Vertex AI Endpoint.\n",
        "    - Make a prediction request."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset:iris,lcn"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset used for this tutorial is the [Iris dataset](https://www.tensorflow.org/datasets/catalog/iris) from [Tensorflow Datasets](https://www.tensorflow.org/datasets/catalog/overview). This dataset does not require any feature engineering. The version of the dataset you will use in this tutorial is stored in a public Cloud Storage bucket. The trained model predicts the type of Iris flower species from a class of three species: setosa, virginica, or versicolor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7EUnXsZhAGF"
      },
      "source": [
        "### Install additional packages\n",
        "\n",
        "Install additional package dependencies not installed in your notebook environment, such as NumPy, Scikit-learn, FastAPI, Uvicorn, and joblib. Use the latest major GA version of each package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ed1088af932"
      },
      "outputs": [],
      "source": [
        "! mkdir src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "747f59abb3a5"
      },
      "outputs": [],
      "source": [
        "%%writefile src/requirements.txt\n",
        "fastapi\n",
        "uvicorn\n",
        "joblib~=1.0\n",
        "numpy~=1.20\n",
        "scikit-learn~=0.24\n",
        "google-cloud-storage>=1.26.0,<2.0.0dev\n",
        "google-cloud-aiplatform[prediction] @ git+https://github.com/googleapis/python-aiplatform.git@custom-prediction-routine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faf22f3af1ce"
      },
      "source": [
        "**The model you deploy will have a different set of dependencies pre-installed than your notebook environment has. You should not assume that because things work in the notebook, they will work in the model. Instead, you will be very explicit about the dependencies for the model by listing them in requirements.txt and then use `pip install` to install the exact same dependencies in the notebook. Please note, of course, that there is a chance that a dependency is missed in requirements.txt that already exists in the notebook. If that's the case, things will run in the notebook, but not in the model. To guard against that, you will test the model locally before deploying to the cloud.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyy5Lbnzg5fi"
      },
      "outputs": [],
      "source": [
        "# Install the same dependencies used in the serving container in the notebook\n",
        "# environment.\n",
        "%pip install -U --user -r src/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhq5zEbGg0XX"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzrelQZ22IZj"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "project_id"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_project_id"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_project_id"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID:\", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_gcloud_project_id"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
        "\n",
        "- Americas: `us-central1`\n",
        "- Europe: `europe-west4`\n",
        "- Asia Pacific: `asia-east1`\n",
        "\n",
        "You may not use a multi-regional bucket for training or prediction with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
        "\n",
        "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "region"
      },
      "outputs": [],
      "source": [
        "REGION = \"us-central1\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "timestamp"
      },
      "source": [
        "#### Timestamp\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "timestamp"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bucket:mbsdk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "When you initialize the Vertex AI SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bucket"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_bucket"
      },
      "outputs": [],
      "source": [
        "if BUCKET_URI == \"\" or BUCKET_URI is None or BUCKET_URI == \"gs://[your-bucket-name]\":\n",
        "    BUCKET_URI = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_bucket"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "validate_bucket"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "validate_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_vars"
      },
      "source": [
        "### Set up variables\n",
        "\n",
        "Next, set up some variables used throughout the tutorial.\n",
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "import google.cloud.aiplatform as aip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c2d091d9e73"
      },
      "source": [
        "## Write your custom data preprocessing\n",
        "\n",
        "First, you write the module `preprocess.py` for data preprocessing of the training data. Since all the features are numeric, each feature column will be standardized - i.e.,  mean of 0 and a standard deviation of 1. This is also referred to as scaling the numeric feature values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58d843d21fa8"
      },
      "outputs": [],
      "source": [
        "%%writefile src/preprocess.py\n",
        "import numpy as np\n",
        "\n",
        "class MySimpleScaler(object):\n",
        "    def __init__(self):\n",
        "        self._means = None\n",
        "        self._stds = None\n",
        "\n",
        "    def preprocess(self, data):\n",
        "        if self._means is None:  # during training only\n",
        "            self._means = np.mean(data, axis=0)\n",
        "\n",
        "        if self._stds is None:  # during training only\n",
        "            self._stds = np.std(data, axis=0)\n",
        "            if not self._stds.all():\n",
        "                raise ValueError(\"At least one column has standard deviation of 0.\")\n",
        "\n",
        "        return (data - self._means) / self._stds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b816cd52f4b"
      },
      "source": [
        "## Train and store model and data preprocesing module\n",
        "\n",
        "Next, you train the model as follows:\n",
        "\n",
        "1. Use `preprocess.MySimpleScaler` to preprocess the Iris data\n",
        "2. Train a model using scikit-learn.\n",
        "3. Export your trained model as a joblib (`.joblib`) file .\n",
        "4. Export your `MySimpleScaler` instance as a pickle (`.pkl`) file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhaV7a37AWJv"
      },
      "outputs": [],
      "source": [
        "%mkdir model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43e47249f736"
      },
      "outputs": [],
      "source": [
        "%cd src\n",
        "\n",
        "import pickle\n",
        "\n",
        "import joblib\n",
        "from preprocess import MySimpleScaler\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "iris = load_iris()\n",
        "scaler = MySimpleScaler()\n",
        "\n",
        "X = scaler.preprocess(iris.data)\n",
        "y = iris.target\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X, y)\n",
        "\n",
        "joblib.dump(model, \"../model/model.joblib\")\n",
        "with open(\"../model/preprocessor.pkl\", \"wb\") as f:\n",
        "    pickle.dump(scaler, f)\n",
        "\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3849066a33bd"
      },
      "source": [
        "### Upload model artifacts and custom data preprocessor to Cloud Storage\n",
        "\n",
        "To deploy your model, the model artifacts `model.joblib` and data preprocessor `preprocessor.pkl` need to be stored in Cloud Storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ca67ee52d4d9"
      },
      "outputs": [],
      "source": [
        "! gsutil cp model/* {BUCKET_URI}/model/\n",
        "! gsutil ls {BUCKET_URI}/model/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "480a1d88ecdb"
      },
      "source": [
        "## Build a custom model serving container using the CPR model server: Scenario 1: implementing the pre and post processor\n",
        "\n",
        "Next, its time to build a custom serving container for the trained model and the data preprocessor. As for data preprocessing, we could fuse the data preprocessor to the model by using the AutoGraph compiler (e.g., @tf.function decorator) to conver the Python code to a static graph. There are a couple of limitations to this approach:\n",
        "\n",
        "    - Not all Python operations can be converted to a graph operation.\n",
        "    - Only static graph operations are supported.\n",
        "    \n",
        "While this simple data preprocessor could be converted to a static graph, many more complex pre and post-processing cannot. In this case, we want the pre and post-processing steps to be executed as pure Python code, where:\n",
        "\n",
        "    - The data preprocessing is inserted between the HTTP server and the model input.\n",
        "    - The data preprocessing is sandboxed, such that if an exception is thrown it does not bring down the model server.\n",
        "    \n",
        "The Vertex AI Custom Prediction Routine provides a template means for doing the above, that can be used out of the box.\n",
        "\n",
        "Learn more about [Custom Prediction Routine model server](https://github.com/googleapis/python-aiplatform/blob/custom-prediction-routine/google/cloud/aiplatform/prediction/model_server.py).\n",
        "\n",
        "A custom model serving container contains the follow three code components:\n",
        "\n",
        "1. [Model server](https://github.com/googleapis/python-aiplatform/blob/custom-prediction-routine/google/cloud/aiplatform/prediction/model_server.py)\n",
        "    * HTTP server that hosts the model\n",
        "    * Responsible for setting up routes/ports/etc.\n",
        "    * In this example we will use the `google.cloud.aiplatform.prediction.model_server.ModelServer` out of the box.\n",
        "2. [Request Handler](https://github.com/googleapis/python-aiplatform/blob/custom-prediction-routine/google/cloud/aiplatform/prediction/handler.py)\n",
        "    * Responsible for webserver aspects of handling a request, such as deserializing the request body, and serializing the reponse, setting response headers, etc.\n",
        "    * In this example, we will use the default Handler, `google.cloud.aiplatform.prediction.handler.PredictionHandler` provided in the SDK.\n",
        "3. [Predictor](https://github.com/googleapis/python-aiplatform/blob/custom-prediction-routine/google/cloud/aiplatform/prediction/predictor.py)\n",
        "    * Responsible for the ML logic for processing a prediction request.\n",
        "\n",
        "Each of these three components can be customized based on the requirements of the custom container. \n",
        "\n",
        "\n",
        "You use the predefined [`SklearnPredictor`](https://github.com/googleapis/python-aiplatform/blob/custom-prediction-routine/google/cloud/aiplatform/prediction/sklearn/predictor.py) as your `CprPredictor`'s base class. You only need to implement the `load`, `preprocess`, and `postprocess` methods.\n",
        "\n",
        "```\n",
        "class CprPredictor(SklearnPredictor):\n",
        "\n",
        "    def __init__(self):\n",
        "        return\n",
        "\n",
        "    def load(self, gcs_artifacts_uri: str):\n",
        "        \"\"\" (super) Loads the model artifact.\n",
        "            Loads the preprocessor module.\n",
        "        \"\"\"\n",
        "       \n",
        "\n",
        "    def preprocess(self, prediction_input: dict):\n",
        "        \"\"\" Apply the preprocessor to the input data\n",
        "        \"\"\"\n",
        "\n",
        "    def postprocess(self, prediction_results: np.ndarray):\n",
        "        \"\"\" Convert class indices to class names\n",
        "        \"\"\"\n",
        "       \n",
        "\n",
        "    def predict(self, instances: np.ndarray):\n",
        "        \"\"\" (super) Performs prediction.\n",
        "        \n",
        "```\n",
        "\n",
        "Note, the [`PredictionHandler`](https://github.com/googleapis/python-aiplatform/blob/custom-prediction-routine/google/cloud/aiplatform/prediction/handler.py) will be used for prediction request handling, and the following will be executed:\n",
        "```\n",
        "self._predictor.postprocess(self._predictor.predict(self._predictor.preprocess(prediction_input)))\n",
        "```\n",
        "\n",
        "First, implement a custom `Predictor` that loads in the preprocesor. The preprocessor will then be used at `preprocess` time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkh8DT7CCgQO"
      },
      "outputs": [],
      "source": [
        "%%writefile src/predictor.py\n",
        "\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "from google.cloud import storage\n",
        "from google.cloud.aiplatform.prediction.sklearn.predictor import SklearnPredictor\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "\n",
        "class CprPredictor(SklearnPredictor):\n",
        "    \n",
        "    def __init__(self):\n",
        "        return\n",
        "    \n",
        "    def load(self, gcs_artifacts_uri: str):\n",
        "        \"\"\"Loads the preprocessor artifacts.\"\"\"\n",
        "        super().load(gcs_artifacts_uri)\n",
        "        gcs_client = storage.Client()\n",
        "        with open(\"preprocessor.pkl\", 'wb') as preprocessor_f:\n",
        "            gcs_client.download_blob_to_file(\n",
        "                f\"{gcs_artifacts_uri}/preprocessor.pkl\", preprocessor_f\n",
        "            )\n",
        "\n",
        "        with open(\"preprocessor.pkl\", \"rb\") as f:\n",
        "            preprocessor = pickle.load(f)\n",
        "\n",
        "        self._class_names = load_iris().target_names\n",
        "        self._preprocessor = preprocessor\n",
        "    \n",
        "    def preprocess(self, prediction_input):\n",
        "        \"\"\"Perform scaling preprocessing\"\"\"\n",
        "        inputs = super().preprocess(prediction_input)\n",
        "        return self._preprocessor.preprocess(inputs)\n",
        "    \n",
        "    def postprocess(self, prediction_results):\n",
        "        \"\"\"Convert class indices to class names.\"\"\"\n",
        "        return {\"predictions\": [self._class_names[class_num] for class_num in prediction_results]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51e149fdec1b"
      },
      "source": [
        "## Build and push container to Artifact Registry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bdb9a7768a5"
      },
      "source": [
        "### Build your container\n",
        "\n",
        "To build a custom container, we also need to write an entrypoint of the image that starts the model server. However, with the Custom Prediction Routine feature, you don't need to write the entrypoint anymore. Vertex AI SDK will populate the entrypoint with the custom predictor you provide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8phJWBhe7u9u"
      },
      "source": [
        "#### Set up credentials (for local execution)\n",
        "\n",
        "Setting up credentials is only required to run the custom serving container locally. Credentials set up is required to execute the `Predictor`'s `load` function, which downloads the model artifacts from Cloud Storage.\n",
        "\n",
        "There are two options for setting up your credentials, depending on permissions granted to your service account.\n",
        "\n",
        "First enable the IAM API if it's not already enabled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mERTlLb6dluB"
      },
      "outputs": [],
      "source": [
        "! gcloud services enable iam.googleapis.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBCra4QMA2wR"
      },
      "source": [
        "#### Option 1: Service Account\n",
        "\n",
        "Follow these steps:\n",
        "\n",
        "1. In the Cloud Console, go to the [**Create service account key**\n",
        "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
        "\n",
        "2. Click **Create service account**.\n",
        "\n",
        "3. In the **Service account name** field, enter a name, and\n",
        "   click **Create**.\n",
        "\n",
        "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
        "into the filter box, and select\n",
        "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "\n",
        "Next, generate the service account key, and save it to `credentials.json` in the same directory you are running this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_service_account"
      },
      "outputs": [],
      "source": [
        "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_service_account"
      },
      "outputs": [],
      "source": [
        "if (\n",
        "    SERVICE_ACCOUNT == \"\"\n",
        "    or SERVICE_ACCOUNT is None\n",
        "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
        "):\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = !gcloud auth list 2>/dev/null\n",
        "    SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
        "    print(\"Service Account:\", SERVICE_ACCOUNT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4lduXfvdluB"
      },
      "outputs": [],
      "source": [
        "! gcloud iam service-accounts keys create credentials.json --iam-account=$SERVICE_ACCOUNT\n",
        "! gcloud auth application-default login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0310674cc35c"
      },
      "source": [
        "#### Option 2: User Account\n",
        "\n",
        "Follow these steps:\n",
        "\n",
        "1. Open a terminal and cd to the same directory that you are running the notebook.\n",
        "\n",
        "2. Execute the command `gcloud auth application-default login` -- answer yes to continue. This will open up an authentication browser tab. Follow the instructions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2e934aff1bbf"
      },
      "outputs": [],
      "source": [
        "CREDENTIALS_FILE = \"/home/jupyter/.config/gcloud/application_default_credentials.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "240578ec9efe"
      },
      "source": [
        "#### Build your custom model serving container\n",
        "\n",
        "To build a custom image, a Dockerfile is necessary where you need to implement what the image looks like. With the Custom Prediction Routine feature, Vertex AI SDK auto-generates the Dockerfile and builds the image for you.\n",
        "\n",
        "Using `python:3.7` as a base image by default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d3a6b9ed22b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from google.cloud.aiplatform.prediction import LocalModel\n",
        "from src.predictor import CprPredictor\n",
        "\n",
        "REPOSITORY = \"custom-preprocess-container-prediction\"  # @param {type:\"string\"}\n",
        "SERVER_IMAGE = \"sklearn-cpr-preprocess-server\"  # @param {type:\"string\"}\n",
        "\n",
        "local_model = LocalModel.create_cpr_model(\n",
        "    \"src\",\n",
        "    f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{SERVER_IMAGE}\",\n",
        "    predictor=CprPredictor,\n",
        "    requirements_path=\"src/requirements.txt\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04c988201499"
      },
      "source": [
        "#### Get the specification for the serving container\n",
        "\n",
        "Next, display the specification of the custom serving container you just built."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1e7d639b9cc"
      },
      "outputs": [],
      "source": [
        "local_model.get_serving_container_spec()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b62ddf1def3"
      },
      "source": [
        "### Create example data\n",
        "\n",
        "Next, create some synthetic example data, and store the examples in a JSON format for prediction.\n",
        "\n",
        "Learn more about [formatting input instances in JSON](https://cloud.google.com/vertex-ai/docs/predictions/online-predictions-custom-models#request-body-details)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2gh8JrJAWJ0"
      },
      "outputs": [],
      "source": [
        "INPUT_FILE = \"instances.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6605e9e6186"
      },
      "outputs": [],
      "source": [
        "%%writefile $INPUT_FILE\n",
        "{\n",
        "    \"instances\": [\n",
        "        [6.7, 3.1, 4.7, 1.5],\n",
        "        [4.6, 3.1, 1.5, 0.2]\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "147a555f6c93"
      },
      "source": [
        "### Test the custom model serving container locally\n",
        "\n",
        "Next, you test your custom model serving container, with CPR, locally. In this example, the container executes a prediction request and a health check.\n",
        "\n",
        "*Note:* You need to have the credentials set up in the previous step and pass the path to the credentials while running the container. The service account should have the **Storage Object Admin** permission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62ed2d334d0f"
      },
      "outputs": [],
      "source": [
        "with local_model.deploy_to_local_endpoint(\n",
        "    artifact_uri=f\"{BUCKET_URI}/model\",\n",
        "    credential_path=CREDENTIALS_FILE,  # Update this to the path to your credentials.\n",
        ") as local_endpoint:\n",
        "    predict_response = local_endpoint.predict(\n",
        "        request_file=INPUT_FILE,\n",
        "        headers={\"Content-Type\": \"application/json\"},\n",
        "    )\n",
        "\n",
        "    health_check_response = local_endpoint.run_health_check()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtsAtOCeAWJ1"
      },
      "source": [
        "Print out the predict response and its content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce629eea32fd"
      },
      "outputs": [],
      "source": [
        "predict_response, predict_response.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tq0Iqk5AWJ2"
      },
      "source": [
        "Print out the health check response and its content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56986f93438e"
      },
      "outputs": [],
      "source": [
        "health_check_response, health_check_response.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a29fcbbe0188"
      },
      "source": [
        "Also print out all the container logs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGK96YCYAWJ2"
      },
      "outputs": [],
      "source": [
        "local_endpoint.print_container_logs(show_all=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "212b2935ea12"
      },
      "source": [
        "### Push the container to artifact registry\n",
        "\n",
        "#### Configure Docker to access Artifact Registry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSFXCj3LdluJ"
      },
      "outputs": [],
      "source": [
        "! gcloud services enable artifactregistry.googleapis.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09ffe2434e3d"
      },
      "outputs": [],
      "source": [
        "! gcloud beta artifacts repositories create {REPOSITORY} \\\n",
        "    --repository-format=docker \\\n",
        "    --location=$REGION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "293437024749"
      },
      "outputs": [],
      "source": [
        "! gcloud auth configure-docker {REGION}-docker.pkg.dev --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqAhftktAWJ3"
      },
      "source": [
        "#### Push your container image to your Artifact Registry repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dd7448f4703"
      },
      "outputs": [],
      "source": [
        "local_model.push_image()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b438bfa2129f"
      },
      "source": [
        "## Deploy custom model serving container to Vertex AI\n",
        "\n",
        "### Upload the custom serving container to a `Vertex AI Model` resource\n",
        "\n",
        "Use the LocalModel instance to upload the custom serving container to a `Vertex AI Model` resource. It will populate the container specification automatically for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2738154345d5"
      },
      "outputs": [],
      "source": [
        "model = local_model.upload(\n",
        "    display_name=\"iris_\" + TIMESTAMP,\n",
        "    artifact_uri=f\"{BUCKET_URI}/model\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd1b85afc7df"
      },
      "source": [
        "### Deploy the model to `Vertex AI Endpoint` resource\n",
        "\n",
        "Next, deploy the Vertex AI Model resource to a Vertex AI Endpoint resource, for prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62cf66498a28"
      },
      "outputs": [],
      "source": [
        "endpoint = model.deploy(machine_type=\"n1-standard-4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6883e7b07143"
      },
      "source": [
        "## Make predictions to deployed model\n",
        "\n",
        "### Using Vertex AI SDK\n",
        "\n",
        "First, you make a prediction request using the Vertex AI SDK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d69ed411c2d3"
      },
      "outputs": [],
      "source": [
        "endpoint.predict(instances=[[6.7, 3.1, 4.7, 1.5], [4.6, 3.1, 1.5, 0.2]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "370d22f53427"
      },
      "source": [
        "### Using REST\n",
        "\n",
        "Next, you repeat the same, but use the REST interface to make a prediction request."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba55bc560d58"
      },
      "outputs": [],
      "source": [
        "ENDPOINT_ID = endpoint.name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95c562b4e98b"
      },
      "outputs": [],
      "source": [
        "! curl \\\n",
        "-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
        "-H \"Content-Type: application/json\" \\\n",
        "-d @instances.json \\\n",
        "https://{REGION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/endpoints/{ENDPOINT_ID}:predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa71174a7dd0"
      },
      "source": [
        "### Using gcloud CLI\n",
        "\n",
        "Finally, you repeat the same, but use the gcloud command line interface to make a prediction request."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23b8e807b02c"
      },
      "outputs": [],
      "source": [
        "! gcloud ai endpoints predict $ENDPOINT_ID \\\n",
        "  --region=$REGION \\\n",
        "  --json-request=instances.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5abba410f12"
      },
      "source": [
        "### Cleanup: Scenario 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx_vKniMq9ZX"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    # Undeploy model and delete endpoint\n",
        "    endpoint.delete(force=True)\n",
        "\n",
        "    # Delete the model resource\n",
        "    model.delete()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Delete the container image from Artifact Registry\n",
        "! gcloud artifacts docker images delete \\\n",
        "    --quiet \\\n",
        "    --delete-tags \\\n",
        "    {REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{SERVER_IMAGE}\n",
        "\n",
        "! rm -rf model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "480a1d88ecdb"
      },
      "source": [
        "## Build a custom model serving container using the CPR model server: Scenario 2: implementing the predictor\n",
        "\n",
        "Next, you will implement a custom `predictor()` method for the CPR model server, instead of using a pre-built predictor. The `predictor()` method handles the sending the instances data to the model and receiving the prediction request. It will also, call the `preprocess()` method to preprocess the input data to the method before sending it to the model. In this example, you inherit the base class `Predictor`, and implement the corresponding `predictor()` method.\n",
        "\n",
        "```\n",
        "class CprPredictor(Predictor):\n",
        "    \"\"\"Default Predictor implementation for Sklearn models.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        return\n",
        "\n",
        "    def load(self, gcs_artifacts_uri: str):\n",
        "        \"\"\" Loads the model artifact.\n",
        "            Loads the preprocessor module.\n",
        "        \"\"\"\n",
        "       \n",
        "\n",
        "    def preprocess(self, prediction_input: dict):\n",
        "        \"\"\" (super) Apply the preprocessor to the input data\n",
        "        \"\"\"\n",
        "       \n",
        "\n",
        "    def predict(self, instances: np.ndarray):\n",
        "        \"\"\" Performs prediction.\n",
        "        \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkh8DT7CCgQO"
      },
      "outputs": [],
      "source": [
        "%%writefile src/predictor.py\n",
        "\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "from google.cloud import storage\n",
        "from google.cloud.aiplatform.prediction.predictor import Predictor\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "\n",
        "class CprPredictor(Predictor):\n",
        "    \n",
        "    def __init__(self):\n",
        "        return\n",
        "    \n",
        "    def load(self, gcs_artifacts_uri: str):\n",
        "        \"\"\"Loads the preprocessor and model artifacts.\"\"\"\n",
        "        gcs_client = storage.Client()\n",
        "        with open(\"preprocessor.pkl\", 'wb') as preprocessor_f, open(\"model.joblib\", 'wb') as model_f:\n",
        "            gcs_client.download_blob_to_file(\n",
        "                f\"{gcs_artifacts_uri}/preprocessor.pkl\", preprocessor_f\n",
        "            )\n",
        "            gcs_client.download_blob_to_file(\n",
        "                f\"{gcs_artifacts_uri}/model.joblib\", model_f\n",
        "            )\n",
        "\n",
        "        with open(\"preprocessor.pkl\", \"rb\") as f:\n",
        "            preprocessor = pickle.load(f)\n",
        "\n",
        "        self._class_names = load_iris().target_names\n",
        "        self._model = joblib.load(\"model.joblib\")\n",
        "        self._preprocessor = preprocessor\n",
        "\n",
        "    def predict(self, instances):\n",
        "        \"\"\"Performs prediction.\"\"\"\n",
        "        instances = instances[\"instances\"]\n",
        "        inputs = np.asarray(instances)\n",
        "        preprocessed_inputs = self._preprocessor.preprocess(inputs)\n",
        "        outputs = self._model.predict(preprocessed_inputs)\n",
        "\n",
        "        return {\"predictions\": [self._class_names[class_num] for class_num in outputs]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51e149fdec1b"
      },
      "source": [
        "## Build the custom model serving container\n",
        "\n",
        "Next, you build the custom model serving container."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d3a6b9ed22b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from google.cloud.aiplatform.prediction import LocalModel\n",
        "from src.predictor import CprPredictor\n",
        "\n",
        "local_model = LocalModel.create_cpr_model(\n",
        "    \"src\",\n",
        "    f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{SERVER_IMAGE}\",\n",
        "    predictor=CprPredictor,\n",
        "    requirements_path=os.path.join(\"src\", \"requirements.txt\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04c988201499"
      },
      "source": [
        "#### Get the specification for the serving container\n",
        "\n",
        "Next, display the specification of the custom serving container you just built."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1e7d639b9cc"
      },
      "outputs": [],
      "source": [
        "local_model.get_serving_container_spec()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "147a555f6c93"
      },
      "source": [
        "### Test the custom model serving container locally\n",
        "\n",
        "Next, you test your custom model serving container, with CPR, locally. In this example, the container executes a prediction request and a health check.\n",
        "\n",
        "*Note:* You need to have the credentials set up in the previous step and pass the path to the credentials while running the container. The service account should have the **Storage Object Admin** permission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62ed2d334d0f"
      },
      "outputs": [],
      "source": [
        "with local_model.deploy_to_local_endpoint(\n",
        "    artifact_uri=f\"{BUCKET_URI}/model\",\n",
        "    credential_path=CREDENTIALS_FILE,\n",
        ") as local_endpoint:\n",
        "    predict_response = local_endpoint.predict(\n",
        "        request_file=INPUT_FILE,\n",
        "        headers={\"Content-Type\": \"application/json\"},\n",
        "    )\n",
        "\n",
        "    health_check_response = local_endpoint.run_health_check()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtsAtOCeAWJ1"
      },
      "source": [
        "Print out the predict response and its content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce629eea32fd"
      },
      "outputs": [],
      "source": [
        "predict_response, predict_response.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tq0Iqk5AWJ2"
      },
      "source": [
        "Print out the health check response and its content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56986f93438e"
      },
      "outputs": [],
      "source": [
        "health_check_response, health_check_response.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a29fcbbe0188"
      },
      "source": [
        "Also print out all the container logs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGK96YCYAWJ2"
      },
      "outputs": [],
      "source": [
        "local_endpoint.print_container_logs(show_all=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "212b2935ea12"
      },
      "source": [
        "### Push the container to artifact registry\n",
        "\n",
        "#### Configure Docker to access Artifact Registry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "293437024749"
      },
      "outputs": [],
      "source": [
        "! gcloud auth configure-docker {REGION}-docker.pkg.dev --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqAhftktAWJ3"
      },
      "source": [
        "#### Push your container image to your Artifact Registry repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dd7448f4703"
      },
      "outputs": [],
      "source": [
        "local_model.push_image()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b438bfa2129f"
      },
      "source": [
        "## Deploy custom model serving container to Vertex AI\n",
        "\n",
        "### Upload the custom serving container to a `Vertex AI Model` resource\n",
        "\n",
        "Use the LocalModel instance to upload the custom serving container to a `Vertex AI Model` resource. It will populate the container specification automatically for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2738154345d5"
      },
      "outputs": [],
      "source": [
        "model = local_model.upload(\n",
        "    display_name=\"iris_\" + TIMESTAMP,\n",
        "    artifact_uri=f\"{BUCKET_URI}/model\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd1b85afc7df"
      },
      "source": [
        "### Deploy the model to `Vertex AI Endpoint` resource\n",
        "\n",
        "Next, deploy the Vertex AI Model resource to a Vertex AI Endpoint resource, for prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62cf66498a28"
      },
      "outputs": [],
      "source": [
        "endpoint = model.deploy(machine_type=\"n1-standard-4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6883e7b07143"
      },
      "source": [
        "## Make predictions to deployed model\n",
        "\n",
        "### Using Vertex AI SDK\n",
        "\n",
        "Make a prediction request using the Vertex AI SDK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d69ed411c2d3"
      },
      "outputs": [],
      "source": [
        "endpoint.predict(instances=[[6.7, 3.1, 4.7, 1.5], [4.6, 3.1, 1.5, 0.2]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6615c21bf7d4"
      },
      "source": [
        "### Cleanup: Scenario 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx_vKniMq9ZX"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    # Undeploy model and delete endpoint\n",
        "    endpoint.delete(force=True)\n",
        "\n",
        "    # Delete the model resource\n",
        "    model.delete()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Delete the container image from Artifact Registry\n",
        "! gcloud artifacts docker images delete \\\n",
        "    --quiet \\\n",
        "    --delete-tags \\\n",
        "    {REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{SERVER_IMAGE}\n",
        "\n",
        "! rm -rf model src/entrypoint.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83fb8ce349c6"
      },
      "source": [
        "## Build a custom model serving container using the CPR model server: Scenario 3: implementing the predictor and request handler\n",
        "\n",
        "Next, you will implement a custom `handler()` method for the CPR model server, instead of using a pre-built http request handler. The `handler()` method handles the extraction of the prediction request from the HTTP request message. It will also, call the `predictor()` method to pass the extraction instances data for the prediction request.\n",
        "\n",
        "A [`Handler`](https://github.com/googleapis/python-aiplatform/blob/custom-prediction-routine/google/cloud/aiplatform/prediction/handler.py) must implement the following interface.\n",
        "\n",
        "```\n",
        "class CprHandler(PredictionHandler):\n",
        "    \"\"\"Interface for Handler class to handle prediction requests.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, gcs_artifacts_uri: str, predictor: Optional[Type[Predictor]] = None,\n",
        "    ):\n",
        "        \"\"\"Initializes a Handler instance.\n",
        "        Args:\n",
        "            gcs_artifacts_uri (str):\n",
        "                Required. The value of the environment variable AIP_STORAGE_URI.\n",
        "            predictor (Type[Predictor]):\n",
        "                Optional. The Predictor class this handler uses to initiate predictor\n",
        "                instance if given.\n",
        "        \"\"\"\n",
        "\n",
        "    def handle(self, request: Request) -> Response:\n",
        "        \"\"\"Handles a prediction request.\n",
        "        Args:\n",
        "            request (Request):\n",
        "                The request sent to the application.\n",
        "        Returns:\n",
        "            The response of the prediction request.\n",
        "        \"\"\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbc513f396d9"
      },
      "outputs": [],
      "source": [
        "%%writefile src/handler.py\n",
        "\n",
        "import csv\n",
        "from io import StringIO\n",
        "import json\n",
        "\n",
        "from fastapi import Response\n",
        "\n",
        "from google.cloud.aiplatform.prediction.handler import PredictionHandler\n",
        "\n",
        "class CprHandler(PredictionHandler):\n",
        "    \"\"\"Default prediction handler for the prediction requests sent to the application.\"\"\"\n",
        "\n",
        "    async def handle(self, request):\n",
        "        \"\"\"Handles a prediction request.\"\"\"\n",
        "        request_body = await request.body()\n",
        "        prediction_instances = self._convert_csv_to_list(request_body)\n",
        "        prediction_instances = {\"instances\": prediction_instances}\n",
        "\n",
        "        prediction_results = self._predictor.postprocess(\n",
        "            self._predictor.predict(self._predictor.preprocess(prediction_instances))\n",
        "        )\n",
        "\n",
        "        return Response(content=json.dumps(prediction_results))\n",
        "    \n",
        "    def _convert_csv_to_list(self, data):\n",
        "        \"\"\"Converts list of string in csv format to list of float.\n",
        "        \n",
        "        Example input:\n",
        "          b\"1.1,2.2,3.3,4.4\\n2.3,3.4,4.5,5.6\\n\"\n",
        "          \n",
        "        Example output:\n",
        "            [\n",
        "                [1.1, 2.2, 3.3, 4.4],\n",
        "                [2.3, 3.4, 4.5, 5.6],\n",
        "            ]\n",
        "        \"\"\"\n",
        "        res = []\n",
        "        for r in csv.reader(StringIO(data.decode(\"utf-8\")), quoting=csv.QUOTE_NONNUMERIC):\n",
        "            res.append(r)\n",
        "        return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51e149fdec1b"
      },
      "source": [
        "## Build the custom model serving container\n",
        "\n",
        "Next, you build the custom model serving container."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d3a6b9ed22b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from google.cloud.aiplatform.prediction import LocalModel\n",
        "from src.handler import CprHandler\n",
        "from src.predictor import CprPredictor\n",
        "\n",
        "local_model = LocalModel.create_cpr_model(\n",
        "    \"src\",\n",
        "    f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{SERVER_IMAGE}\",\n",
        "    predictor=CprPredictor,\n",
        "    handler=CprHandler,\n",
        "    requirements_path=os.path.join(\"src\", \"requirements.txt\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04c988201499"
      },
      "source": [
        "#### Get the specification for the serving container\n",
        "\n",
        "Next, display the specification of the custom serving container you just built."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1e7d639b9cc"
      },
      "outputs": [],
      "source": [
        "local_model.get_serving_container_spec()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b62ddf1def3"
      },
      "source": [
        "### Create example data\n",
        "\n",
        "Next, create some synthetic example data, and store the examples in a CSV format for prediction.\n",
        "\n",
        "To send input instances in CSV, need to use raw predict to use an arbitrary HTTP payload rather than JSON format. \n",
        "\n",
        "Learn more about [Raw Predict](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/rawPredict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "613ac2f0c5f8"
      },
      "outputs": [],
      "source": [
        "INPUT_FILE = \"instances.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40e06c49d3b1"
      },
      "outputs": [],
      "source": [
        "%%writefile $INPUT_FILE\n",
        "6.7,3.1,4.7,1.5\n",
        "4.6,3.1,1.5,0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "147a555f6c93"
      },
      "source": [
        "### Test the custom model serving container locally\n",
        "\n",
        "Next, you test your custom model serving container, with CPR, locally. In this example, the container executes a prediction request and a health check.\n",
        "\n",
        "*Note:* You need to have the credentials set up in the previous step and pass the path to the credentials while running the container. The service account should have the **Storage Object Admin** permission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62ed2d334d0f"
      },
      "outputs": [],
      "source": [
        "with local_model.deploy_to_local_endpoint(\n",
        "    artifact_uri=f\"{BUCKET_URI}/model\",\n",
        "    credential_path=CREDENTIALS_FILE,\n",
        ") as local_endpoint:\n",
        "    predict_response = local_endpoint.predict(\n",
        "        request_file=INPUT_FILE,\n",
        "        headers={\"Content-Type\": \"application/json\"},\n",
        "    )\n",
        "\n",
        "    health_check_response = local_endpoint.run_health_check()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtsAtOCeAWJ1"
      },
      "source": [
        "Print out the predict response and its content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce629eea32fd"
      },
      "outputs": [],
      "source": [
        "predict_response, predict_response.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tq0Iqk5AWJ2"
      },
      "source": [
        "Print out the health check response and its content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56986f93438e"
      },
      "outputs": [],
      "source": [
        "health_check_response, health_check_response.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a29fcbbe0188"
      },
      "source": [
        "Also print out all the container logs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGK96YCYAWJ2"
      },
      "outputs": [],
      "source": [
        "local_endpoint.print_container_logs(show_all=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "212b2935ea12"
      },
      "source": [
        "### Push the container to artifact registry\n",
        "\n",
        "#### Configure Docker to access Artifact Registry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "293437024749"
      },
      "outputs": [],
      "source": [
        "! gcloud auth configure-docker {REGION}-docker.pkg.dev --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqAhftktAWJ3"
      },
      "source": [
        "#### Push your container image to your Artifact Registry repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dd7448f4703"
      },
      "outputs": [],
      "source": [
        "local_model.push_image()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b438bfa2129f"
      },
      "source": [
        "## Deploy custom model serving container to Vertex AI\n",
        "\n",
        "### Upload the custom serving container to a `Vertex AI Model` resource\n",
        "\n",
        "Use the LocalModel instance to upload the custom serving container to a `Vertex AI Model` resource. It will populate the container specification automatically for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2738154345d5"
      },
      "outputs": [],
      "source": [
        "model = local_model.upload(\n",
        "    display_name=\"iris_\" + TIMESTAMP,\n",
        "    artifact_uri=f\"{BUCKET_URI}/model\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd1b85afc7df"
      },
      "source": [
        "### Deploy the model to `Vertex AI Endpoint` resource\n",
        "\n",
        "Next, deploy the Vertex AI Model resource to a Vertex AI Endpoint resource, for prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62cf66498a28"
      },
      "outputs": [],
      "source": [
        "endpoint = model.deploy(machine_type=\"n1-standard-4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6883e7b07143"
      },
      "source": [
        "## Make predictions to deployed model\n",
        "\n",
        "### Using Vertex AI SDK\n",
        "\n",
        "Make a prediction request using the Vertex AI SDK `rawPredict()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d69ed411c2d3"
      },
      "outputs": [],
      "source": [
        "from google.api import httpbody_pb2\n",
        "from google.cloud import aiplatform_v1 as gapic\n",
        "\n",
        "prediction_client = gapic.PredictionServiceClient(\n",
        "    client_options={\"api_endpoint\": f\"{REGION}-aiplatform.googleapis.com\"}\n",
        ")\n",
        "\n",
        "with open(INPUT_FILE) as f:\n",
        "    http_body = httpbody_pb2.HttpBody(\n",
        "        data=f.read().encode(\"utf-8\"),\n",
        "        content_type=\"text/csv\",\n",
        "    )\n",
        "\n",
        "request = gapic.RawPredictRequest(\n",
        "    endpoint=endpoint.resource_name,\n",
        "    http_body=http_body,\n",
        ")\n",
        "\n",
        "prediction_client.raw_predict(request=request)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7f3a59b2a90"
      },
      "source": [
        "### Cleanup: Scenario 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx_vKniMq9ZX"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    # Undeploy model and delete endpoint\n",
        "    endpoint.delete(force=True)\n",
        "\n",
        "    # Delete the model resource\n",
        "    model.delete()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Delete the container image from Artifact Registry\n",
        "! gcloud artifacts docker images delete \\\n",
        "    --quiet \\\n",
        "    --delete-tags \\\n",
        "    {REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{SERVER_IMAGE}\n",
        "\n",
        "! rm -rf model src/entrypoint.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62cfac5782e7"
      },
      "source": [
        "## Build a custom model serving container using the CPR model server: Scenario 4: implementing the Docker build process\n",
        "\n",
        "Next, you will implement the Docker build process, instead of using the predefined Docker build process.\n",
        "\n",
        "First, you write the container's entrypoint file that will launch the custom model server. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2168b4651335"
      },
      "outputs": [],
      "source": [
        "%%writefile src/entrypoint.py\n",
        "\n",
        "import os\n",
        "from typing import Optional, Type\n",
        "\n",
        "from google.cloud.aiplatform import prediction\n",
        "\n",
        "from predictor import CprPredictor\n",
        "from handler import CprHandler\n",
        "\n",
        "\n",
        "def main(\n",
        "    predictor_class: Optional[Type[prediction.predictor.Predictor]] = None,\n",
        "    handler_class: Type[prediction.handler.Handler] = prediction.handler.PredictionHandler,\n",
        "    model_server_class: Type[prediction.model_server.ModelServer] = prediction.model_server.ModelServer,\n",
        "):\n",
        "    handler = handler_class(\n",
        "        os.environ.get(\"AIP_STORAGE_URI\"), predictor=predictor_class\n",
        "    )\n",
        "\n",
        "    return model_server_class(handler).start()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(\n",
        "        predictor_class=CprPredictor,\n",
        "        handler_class=CprHandler\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bf858cfb9df"
      },
      "source": [
        "### Build the custom model serving container\n",
        "\n",
        "#### Write the Docker file.\n",
        "\n",
        "First, build the Docker file. *Note:* You specify the entrypoint as the entry point module you defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35c57e3371b3"
      },
      "outputs": [],
      "source": [
        "%%writefile Dockerfile\n",
        "\n",
        "# Users select base images.\n",
        "FROM python:3.7\n",
        "\n",
        "# Sets the directories' permissions so that any user can access the folder.\n",
        "RUN mkdir -m 777 -p /home /usr/app\n",
        "ENV HOME=/home\n",
        "WORKDIR /usr/app\n",
        "\n",
        "# Copies all the stuff to the image.\n",
        "COPY src /usr/app/src\n",
        "COPY src/requirements.txt /usr/app/requirements.txt\n",
        "\n",
        "# Installs python dependencies.\n",
        "RUN pip3 install --no-cache-dir -r /usr/app/requirements.txt\n",
        "\n",
        "# Informs Docker that the container listens on the specified ports at runtime.\n",
        "EXPOSE 8080\n",
        "\n",
        "# Sets up an entrypoint to start the model server.\n",
        "ENTRYPOINT [\"python3\", \"/usr/app/src/entrypoint.py\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee00de7e1a5c"
      },
      "source": [
        "#### Build the container image\n",
        "\n",
        "Next, build the container image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cb46b465ec3"
      },
      "outputs": [],
      "source": [
        "! docker build --tag={REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{SERVER_IMAGE} ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "147a555f6c93"
      },
      "source": [
        "### Test the custom model serving container locally\n",
        "\n",
        "Next, you test your custom model serving container, with CPR, locally. In this example, the container executes a prediction request and a health check.\n",
        "\n",
        "*Note:* You need to have the credentials set up in the previous step and pass the path to the credentials while running the container. The service account should have the **Storage Object Admin** permission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb210d970d92"
      },
      "outputs": [],
      "source": [
        "! docker run -d -p 80:8080 \\\n",
        "    --name=local-iris-custom \\\n",
        "    -e AIP_HTTP_PORT=8080 \\\n",
        "    -e AIP_HEALTH_ROUTE=/health \\\n",
        "    -e AIP_PREDICT_ROUTE=/predict \\\n",
        "    -e AIP_STORAGE_URI={BUCKET_URI}/model \\\n",
        "    -e GOOGLE_APPLICATION_CREDENTIALS=/usr/app/credentials.json \\\n",
        "    -e GOOGLE_CLOUD_PROJECT={PROJECT_ID} \\\n",
        "    -v {CREDENTIALS_FILE}:/usr/app/credentials.json \\\n",
        "    {REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{SERVER_IMAGE}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tq0Iqk5AWJ2"
      },
      "source": [
        "Print out the health check response and its content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd233b54f51b"
      },
      "outputs": [],
      "source": [
        "! curl localhost/health"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtsAtOCeAWJ1"
      },
      "source": [
        "Print out the predict response and its content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2254c71d1322"
      },
      "outputs": [],
      "source": [
        "! curl -X POST \\\n",
        "  -d @instances.csv \\\n",
        "  -H \"Content-Type: application/json; charset=utf-8\" \\\n",
        "  localhost/predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "641a5ac98f7d"
      },
      "source": [
        "Shutdown the Docker service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "707d86aabf42"
      },
      "outputs": [],
      "source": [
        "! docker stop local-iris-custom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "212b2935ea12"
      },
      "source": [
        "### Push the container to artifact registry\n",
        "\n",
        "#### Configure Docker to access Artifact Registry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "293437024749"
      },
      "outputs": [],
      "source": [
        "! gcloud auth configure-docker {REGION}-docker.pkg.dev --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqAhftktAWJ3"
      },
      "source": [
        "#### Push your container image to your Artifact Registry repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dd7448f4703"
      },
      "outputs": [],
      "source": [
        "local_model.push_image()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b438bfa2129f"
      },
      "source": [
        "## Deploy custom model serving container to Vertex AI\n",
        "\n",
        "### Upload the custom serving container to a `Vertex AI Model` resource\n",
        "\n",
        "Use the LocalModel instance to upload the custom serving container to a `Vertex AI Model` resource. It will populate the container specification automatically for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2738154345d5"
      },
      "outputs": [],
      "source": [
        "model = local_model.upload(\n",
        "    display_name=\"iris_\" + TIMESTAMP,\n",
        "    artifact_uri=f\"{BUCKET_URI}/model\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd1b85afc7df"
      },
      "source": [
        "### Deploy the model to `Vertex AI Endpoint` resource\n",
        "\n",
        "Next, deploy the Vertex AI Model resource to a Vertex AI Endpoint resource, for prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62cf66498a28"
      },
      "outputs": [],
      "source": [
        "endpoint = model.deploy(machine_type=\"n1-standard-4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6883e7b07143"
      },
      "source": [
        "## Make predictions to deployed model\n",
        "\n",
        "### Using Vertex AI SDK\n",
        "\n",
        "Make a prediction request using the Vertex AI SDK `rawPredict()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d69ed411c2d3"
      },
      "outputs": [],
      "source": [
        "from google.api import httpbody_pb2\n",
        "from google.cloud import aiplatform_v1 as gapic\n",
        "\n",
        "prediction_client = gapic.PredictionServiceClient(\n",
        "    client_options={\"api_endpoint\": f\"{REGION}-aiplatform.googleapis.com\"}\n",
        ")\n",
        "\n",
        "with open(INPUT_FILE) as f:\n",
        "    http_body = httpbody_pb2.HttpBody(\n",
        "        data=f.read().encode(\"utf-8\"),\n",
        "        content_type=\"text/csv\",\n",
        "    )\n",
        "\n",
        "request = gapic.RawPredictRequest(\n",
        "    endpoint=endpoint.resource_name,\n",
        "    http_body=http_body,\n",
        ")\n",
        "\n",
        "prediction_client.raw_predict(request=request)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e2c0eecf4ad"
      },
      "source": [
        "### Cleanup: Scenario 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx_vKniMq9ZX"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    # Undeploy model and delete endpoint\n",
        "    endpoint.delete(force=True)\n",
        "\n",
        "    # Delete the model resource\n",
        "    model.delete()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Delete the container image from Artifact Registry\n",
        "! gcloud artifacts docker images delete \\\n",
        "    --quiet \\\n",
        "    --delete-tags \\\n",
        "    {REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{SERVER_IMAGE}\n",
        "\n",
        "! rm -rf model src/entrypoint.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx_vKniMq9ZX"
      },
      "outputs": [],
      "source": [
        "! gsutil rm -rf {BUCKET_URI}\n",
        "\n",
        "! rm -rf src model instances.json instances.csv Dockerfile"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "get_started_with_cpr.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

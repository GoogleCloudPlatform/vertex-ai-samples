{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "copyright"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title:generic,gcp"
      },
      "source": [
        "# E2E ML on GCP: MLOps stage 4 : formalization: get started with Vertex ML Metadata\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage4/get_started_with_vertex_ml_metadata.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage4/get_started_with_vertex_ml_metadata.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/ml_ops/stage4/get_started_with_vertex_ml_metadata.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>            \n",
        "</table>\n",
        "<br/><br/><br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview:mlops"
      },
      "source": [
        "## Overview\n",
        "\n",
        "\n",
        "This tutorial demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. This tutorial covers stage 4 : formalization: get started with Vertex ML Metadata."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "objective:mlops,stage4,get_started_vertex_ml_metadata"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to use `Vertex ML Metadata`.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services:\n",
        "\n",
        "- `Vertex ML Metadata`\n",
        "- `Vertex AI Pipelines`\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Create a `Metadatastore` resource.\n",
        "- Create (record)/List an `Artifact`, with artifacts and metadata.\n",
        "- Create (record)/List an `Execution`.\n",
        "- Create (record)/List a `Context`.\n",
        "- Add `Artifact` to `Execution` as events.\n",
        "- Add `Execution` and `Artifact` into the `Context`\n",
        "- Delete `Artifact`, `Execution` and `Context`.\n",
        "- Create and run a `Vertex AI Pipeline` ML workflow to train and deploy a scikit-learn model.\n",
        "    - Create custom pipeline components that generate artifacts and metadata.\n",
        "    - Compare Vertex AI Pipelines runs.\n",
        "    - Trace the lineage for pipeline-generated artifacts.\n",
        "    - Query your pipeline run metadata."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset:beans,lcn"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset used for this tutorial is the UCI Machine Learning ['Dry beans dataset'](https://archive.ics.uci.edu/ml/datasets/Dry+Bean+Dataset), from: KOKLU, M. and OZKAN, I.A., (2020), \"Multiclass Classification of Dry Beans Using Computer Vision and Machine Learning Techniques.\"ÂIn Computers and Electronics in Agriculture, 174, 105507. [DOI](https://doi.org/10.1016/j.compag.2020.105507)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35bee437737d"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_mlops"
      },
      "source": [
        "## Installations\n",
        "\n",
        "Install the packages required for executing the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_mlops"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Vertex AI Workbench Notebook product has specific requirements\n",
        "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
        "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
        "    \"/opt/deeplearning/metadata/env_version\"\n",
        ")\n",
        "\n",
        "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_WORKBENCH_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\"\n",
        "\n",
        "! pip3 install --upgrade google-cloud-aiplatform[tensorboard] $USER_FLAG -q\n",
        "! pip3 install --upgrade google-cloud-pipeline-components $USER_FLAG -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "restart"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "restart"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "before_you_begin"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### GPU runtime\n",
        "\n",
        "*Make sure you're running this notebook in a GPU runtime if you have that option. In Colab, select* **Runtime > Change Runtime Type > GPU**\n",
        "\n",
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "2. [Make sure that billing is enabled for your project.](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
        "\n",
        "3. [Enable the following APIs: Vertex AI APIs, Compute Engine APIs, and Cloud Storage.](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component,storage-component.googleapis.com)\n",
        "\n",
        "4. If you are running this notebook locally, you need to install the [Cloud SDK]((https://cloud.google.com/sdk)).\n",
        "\n",
        "5. Enter your project ID in the cell below. Then run the  cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "project_id"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_project_id"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_project_id"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID:\", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_gcloud_project_id"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
        "\n",
        "- Americas: `us-central1`\n",
        "- Europe: `europe-west4`\n",
        "- Asia Pacific: `asia-east1`\n",
        "\n",
        "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
        "\n",
        "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "region"
      },
      "outputs": [],
      "source": [
        "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
        "\n",
        "if REGION == \"[your-region]\":\n",
        "    REGION = \"us-central1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "timestamp"
      },
      "source": [
        "#### Timestamp\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "timestamp"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcp_authenticate"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using Vertex AI Workbench Notebooks**, your environment is already authenticated. Skip this step.\n",
        "\n",
        "**If you are using Colab**, run the cell below and follow the instructions when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "In the Cloud Console, go to the [Create service account key](https://console.cloud.google.com/apis/credentials/serviceaccountkey) page.\n",
        "\n",
        "**Click Create service account**.\n",
        "\n",
        "In the **Service account name** field, enter a name, and click **Create**.\n",
        "\n",
        "In the **Grant this service account access to project** section, click the Role drop-down list. Type \"Vertex\" into the filter box, and select **Vertex Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "\n",
        "Click Create. A JSON file that contains your key downloads to your local environment.\n",
        "\n",
        "Enter the path to your service account key as the GOOGLE_APPLICATION_CREDENTIALS variable in the cell below and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcp_authenticate"
      },
      "outputs": [],
      "source": [
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# If on Vertex AI Workbench, then don't execute this code\n",
        "IS_COLAB = False\n",
        "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
        "    \"DL_ANACONDA_HOME\"\n",
        "):\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        IS_COLAB = True\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bucket:mbsdk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "When you initialize the Vertex SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bucket"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_bucket"
      },
      "outputs": [],
      "source": [
        "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
        "    BUCKET_NAME = PROJECT_ID + \"aip-\" + TIMESTAMP\n",
        "    BUCKET_URI = \"gs://\" + BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_bucket"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "validate_bucket"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "validate_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "set_service_account"
      },
      "source": [
        "#### Service Account\n",
        "\n",
        "**If you don't know your service account**, try to get your service account using `gcloud` command by executing the second cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_service_account"
      },
      "outputs": [],
      "source": [
        "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_service_account"
      },
      "outputs": [],
      "source": [
        "if (\n",
        "    SERVICE_ACCOUNT == \"\"\n",
        "    or SERVICE_ACCOUNT is None\n",
        "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
        "):\n",
        "    # Get your service account from gcloud\n",
        "    if not IS_COLAB:\n",
        "        shell_output = !gcloud auth list 2>/dev/null\n",
        "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
        "\n",
        "    if IS_COLAB:\n",
        "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
        "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "\n",
        "    print(\"Service Account:\", SERVICE_ACCOUNT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "set_service_account:pipelines"
      },
      "source": [
        "#### Set service account access for Vertex AI Pipelines\n",
        "\n",
        "Run the following commands to grant your service account access to read and write pipeline artifacts in the bucket that you created in the previous step -- you only need to run these once per service account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_service_account:pipelines"
      },
      "outputs": [],
      "source": [
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
        "\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_vars"
      },
      "source": [
        "### Set up variables\n",
        "\n",
        "Next, set up some variables used throughout the tutorial.\n",
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "import google.cloud.aiplatform as aip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_aip:v1beta1"
      },
      "source": [
        "#### Import Vertex AI SDK\n",
        "\n",
        "Import the Vertex AI SDK into your Python environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_aip:v1beta1"
      },
      "outputs": [],
      "source": [
        "import google.cloud.aiplatform_v1beta1 as aip_beta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aip_constants:fs"
      },
      "source": [
        "#### Vertex AI constants\n",
        "\n",
        "Setup up the following constants for Vertex AI:\n",
        "\n",
        "- `API_ENDPOINT`: The Vertex AI API service endpoint for `ML Metadata` services."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aip_constants:fs"
      },
      "outputs": [],
      "source": [
        "# API service endpoint\n",
        "API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
        "\n",
        "# Vertex location root path for your dataset, model and endpoint resources\n",
        "PARENT = \"projects/\" + PROJECT_ID + \"/locations/\" + REGION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clients:metadata"
      },
      "source": [
        "## Set up clients\n",
        "\n",
        "The Vertex  works as a client/server model. On your side (the Python script) you will create a client that sends requests and receives responses from the Vertex AI server.\n",
        "\n",
        "You will use different clients in this tutorial for different steps in the workflow. So set them all up upfront.\n",
        "\n",
        "- Metadata Service for creating recording, searching and analyzing artifacts and metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clients:metadata"
      },
      "outputs": [],
      "source": [
        "# client options same for all services\n",
        "client_options = {\"api_endpoint\": API_ENDPOINT}\n",
        "\n",
        "\n",
        "def create_metadata_client():\n",
        "    client = aip_beta.MetadataServiceClient(client_options=client_options)\n",
        "    return client\n",
        "\n",
        "\n",
        "clients = {}\n",
        "clients[\"metadata\"] = create_metadata_client()\n",
        "\n",
        "for client in clients.items():\n",
        "    print(client)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_metadata"
      },
      "source": [
        "## Introduction to Vertex AI Metadata\n",
        "\n",
        "The `Vertex ML Metadata` service provides you with the ability to record, and subsequently search and analyze, the artifacts and corresponding metadata produced by your ML workflows. For example, during experimentation one might desire to record the location of the model artifacts, as artifacts, and the training hyperparameters and evaluation metrics as the corresponding metadata.\n",
        "\n",
        "The service supports recording ML metadata both manually and automatically, with the later occurring when you use Vertex AI Pipelines.\n",
        "\n",
        "### Concepts and organization\n",
        "\n",
        "Vertex ML Metadata describes your ML system's metadata as a graph.\n",
        "\n",
        "**Artifacts**: Artifacts are pieces of data that ML systems consume or produce, such as: datasets, models, or logs. For large artifacts like datasets or models, the artifact record includes the URI where the data is stored.\n",
        "\n",
        "**Executions**: Executions describe a single step in your ML system's workflow.\n",
        "\n",
        "**Events**: Executions can depend on artifacts as inputs or produce artifacts as outputs. Events describe the relationship between artifacts and executions to help you determine the lineage of artifacts. For example, an event is created to record that a dataset is used by an execution, and another event is created to record that this execution produced a model.\n",
        "\n",
        "**Contexts**: Contexts let you group artifacts and executions together in a single, queryable, and typed category.\n",
        "\n",
        "### ML artifact lineage\n",
        "\n",
        "Vertex ML Metadata provides the ability to understand changes in the performance of your machine ML system, and analyze the metadata produced by your ML workflow and the lineage of its artifacts. An artifact's lineage includes all the factors that contributed to its creation, as well as artifacts and metadata that descend from this artifact.\n",
        "\n",
        "Learn more about [Introduction to Vertex ML Metadata ](https://cloud.google.com/vertex-ai/docs/ml-metadata/introduction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_metadatastore"
      },
      "source": [
        "### Create a `MetadataStore` resource\n",
        "\n",
        "Each project may have one or more `MetadataStore` resources. By default, if none is explicity created, each project has a default, which is specified as:\n",
        "\n",
        "    projects/<project_id>/locations/<region>/metadataStores/<name>\n",
        "\n",
        "You create a `MetadataStore` resource using the `create_metadata_store()` method, with the following parameters:\n",
        "\n",
        "- `parent`: The fully qualified subpath for all resources in your project, i.e., projects/<project_id>/locations/<location>\n",
        "- `metadata_store_id`: The name of the `MetadataStore` resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_metadatastore"
      },
      "outputs": [],
      "source": [
        "metadata_store = clients[\"metadata\"].create_metadata_store(\n",
        "    parent=PARENT, metadata_store_id=\"my-metadata-store\"\n",
        ")\n",
        "\n",
        "metadata_store_id = str(metadata_store.result())[7:-2]\n",
        "print(metadata_store_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "list_metadata_schemas"
      },
      "source": [
        "### List metadata schemas\n",
        "\n",
        "When you create an `Artifact`, `Execution` or `Context` resource, you specify a schema that describes the corresponding metadata. The schemas must be pre-registered for your `Metadatastore` resource.\n",
        "\n",
        "You can get a list of all registered schemas, default and user defined, using the `list_metadata_schemas()` method, with the following parameters:\n",
        "\n",
        "- `name`: The fully qualified resource identifier for the `MetadataStore` resource.\n",
        "\n",
        "Learn more about [Metadata system schemas](https://cloud.google.com/vertex-ai/docs/ml-metadata/system-schemas)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "list_metadata_schemas"
      },
      "outputs": [],
      "source": [
        "schemas = clients[\"metadata\"].list_metadata_schemas(parent=metadata_store_id)\n",
        "\n",
        "for schema in schemas:\n",
        "    print(schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_metadata_artifact"
      },
      "source": [
        "### Create an `Artifact` resource\n",
        "\n",
        "You create an `Artifact` resource using the `create_artifact()` method, with the following parameters:\n",
        "\n",
        "- `parent`: The fully qualified resource identifier to the `Metadatastore` resource.\n",
        "- `artifact`: The definition of the `Artifact` resource\n",
        "    - `display_name`: The human readable name for the `Artifact` resource.\n",
        "    - `uri`: The uniform resource identifier of the artifact file. May be empty if there is no actual artifact file.\n",
        "    - `labels`: User defined labels to assign to the `Artifact` resource.\n",
        "    - `schema_title`: The title of the schema that describes the metadata.\n",
        "    - `metadata`: The metadata key/value pairs to associate with the `Artifact` resource.\n",
        "- `artifact_id`: (optional) A user defined short ID for the `Artifact` resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_metadata_artifact"
      },
      "outputs": [],
      "source": [
        "from google.cloud.aiplatform_v1beta1.types import Artifact\n",
        "\n",
        "artifact_item = Artifact(\n",
        "    display_name=\"my_example_artifact\",\n",
        "    uri=\"my_url\",\n",
        "    labels={\"my_label\": \"value\"},\n",
        "    schema_title=\"system.Artifact\",\n",
        "    metadata={\"param\": \"value\"},\n",
        ")\n",
        "\n",
        "artifact = clients[\"metadata\"].create_artifact(\n",
        "    parent=metadata_store_id,\n",
        "    artifact=artifact_item,\n",
        "    artifact_id=\"myartifactid\",\n",
        ")\n",
        "\n",
        "print(artifact)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "list_metadata_artifacts"
      },
      "source": [
        "### List `Artifact` resources in a `Metadatastore`\n",
        "\n",
        "You can list all `Artifact` resources using the `list_artifacts()` method, with the following parameters:\n",
        "\n",
        "- `parent`: The fully qualified resource identifier for the `MetadataStore` resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "list_metadata_artifacts"
      },
      "outputs": [],
      "source": [
        "artifacts = clients[\"metadata\"].list_artifacts(parent=metadata_store_id)\n",
        "\n",
        "for _artifact in artifacts:\n",
        "    print(_artifact)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_metadata_execution"
      },
      "source": [
        "### Create an `Execution` resource\n",
        "\n",
        "You create an `Execution` resource using the `create_execution()` method, with the following parameters:\n",
        "\n",
        "- `parent`: The fully qualified resource identifier to the `Metadatastore` resource.\n",
        "- `execution`:\n",
        "    - `display_name`: A human readable name for the `Execution` resource.\n",
        "    - `schema_title`: The title of the schema that describes the metadata.\n",
        "    - `metadata`: The metadata key/value pairs to associate with the `Execution` resource.\n",
        "- `execution_id`: (optional) A user defined short ID for the `Execution` resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_metadata_execution"
      },
      "outputs": [],
      "source": [
        "from google.cloud.aiplatform_v1beta1.types import Execution\n",
        "\n",
        "execution = clients[\"metadata\"].create_execution(\n",
        "    parent=metadata_store_id,\n",
        "    execution=Execution(\n",
        "        display_name=\"my_execution\",\n",
        "        schema_title=\"system.CustomJobExecution\",\n",
        "        metadata={\"value\": \"param\"},\n",
        "    ),\n",
        "    execution_id=\"myexecutionid\",\n",
        ")\n",
        "\n",
        "print(execution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "list_metadata_executions"
      },
      "source": [
        "### List `Execution` resources in a `Metadatastore`\n",
        "\n",
        "You can list all `Execution` resources using the `list_executions()` method, with the following parameters:\n",
        "\n",
        "- `parent`: The fully qualified resource identifier for the `MetadataStore` resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "list_metadata_executions"
      },
      "outputs": [],
      "source": [
        "executions = clients[\"metadata\"].list_executions(parent=metadata_store_id)\n",
        "\n",
        "for _execution in executions:\n",
        "    print(_execution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_metadata_context"
      },
      "source": [
        "### Create a `Context` resource\n",
        "\n",
        "You create an `Context` resource using the `create_context()` method, with the following parameters:\n",
        "\n",
        "- `parent`: The fully qualified resource identifier to the `Metadatastore` resource.\n",
        "- `context`:\n",
        "    - `display_name`: A human readable name for the `Execution` resource.\n",
        "    - `schema_title`: The title of the schema that describes the metadata.\n",
        "    - `labels`: User defined labels to assign to the `Context` resource.\n",
        "    - `metadata`: The metadata key/value pairs to associate with the `Execution` resource.\n",
        "- `context_id`: (optional) A user defined short ID for the `Context` resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_metadata_context"
      },
      "outputs": [],
      "source": [
        "from google.cloud.aiplatform_v1beta1.types import Context\n",
        "\n",
        "context = clients[\"metadata\"].create_context(\n",
        "    parent=metadata_store_id,\n",
        "    context=Context(\n",
        "        display_name=\"my_context\",\n",
        "        labels=[{\"my_label\", \"my_value\"}],\n",
        "        schema_title=\"system.Pipeline\",\n",
        "        metadata={\"param\": \"value\"},\n",
        "    ),\n",
        "    context_id=\"mycontextid\",\n",
        ")\n",
        "\n",
        "print(context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "list_metadata_contexts"
      },
      "source": [
        "### List `Context` resources in a `Metadatastore`\n",
        "\n",
        "You can list all `Context` resources using the `list_contexts()` method, with the following parameters:\n",
        "\n",
        "- `parent`: The fully qualified resource identifier for the `MetadataStore` resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "list_metadata_contexts"
      },
      "outputs": [],
      "source": [
        "contexts = clients[\"metadata\"].list_contexts(parent=metadata_store_id)\n",
        "\n",
        "for _context in contexts:\n",
        "    print(_context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "add_metadata_execution_events"
      },
      "source": [
        "### Add events to `Execution` resource\n",
        "\n",
        "An `Execution` resource consists of a sequence of events that occurred during the execution. Each event consists of an artifact that is either an input or an output of the `Execution` resource.\n",
        "\n",
        "You can add execution events to an `Execution` resource using the `add_execution_events()` method, with the following parameters:\n",
        "\n",
        "- `execution`: The fully qualified resource identifier for the `Execution` resource.\n",
        "- `events`: The sequence of events constituting the execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "add_metadata_execution_events"
      },
      "outputs": [],
      "source": [
        "from google.cloud.aiplatform_v1beta1.types import Event\n",
        "\n",
        "clients[\"metadata\"].add_execution_events(\n",
        "    execution=execution.name,\n",
        "    events=[\n",
        "        Event(\n",
        "            artifact=artifact.name,\n",
        "            type_=Event.Type.INPUT,\n",
        "            labels={\"my_label\": \"my_value\"},\n",
        "        )\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "metadata_finish_context"
      },
      "source": [
        "### Combine Artifacts and Executions into a Context\n",
        "\n",
        "A Context is used to group `Artifact` resources and `Execution` resources together under a single, queryable, and typed category. Contexts can be used to represent sets of metadata.\n",
        "\n",
        "You can combine a set of `Artifact` and `Execution` resources into a `Context` resource using the `add_context_artifacts_and_executions()` method, with the following parameters:\n",
        "\n",
        "- `context`: The fully qualified resource identifier of the `Context` resource.\n",
        "- `artifacts`: A list of fully qualified resource identifiers of the `Artifact` resources.\n",
        "- `executions`: A list of fully qualified resource identifiers of the `Execution` resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "metadata_finish_context"
      },
      "outputs": [],
      "source": [
        "clients[\"metadata\"].add_context_artifacts_and_executions(\n",
        "    context=context.name, artifacts=[artifact.name], executions=[execution.name]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "query_metadata_context"
      },
      "source": [
        "### Query a context\n",
        "\n",
        "You can query the subgraph of a `Context` resource using the method `query_context_lineage_subgraph()` method, with the following parameters:\n",
        "\n",
        "- `context`: The fully qualified resource identifier of the `Context` resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "query_metadata_context"
      },
      "outputs": [],
      "source": [
        "subgraph = clients[\"metadata\"].query_context_lineage_subgraph(context=context.name)\n",
        "\n",
        "print(subgraph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "delete_metadata_artifact"
      },
      "source": [
        "### Delete an `Artifact` resource\n",
        "\n",
        "You can delete an `Artifact` resource using the `delete_artifact()` method, with the following parameters:\n",
        "\n",
        "- `name`: The fully qualified resource identifier for the `Artifact` resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "delete_metadata_artifact"
      },
      "outputs": [],
      "source": [
        "clients[\"metadata\"].delete_artifact(name=artifact.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "delete_metadata_execution"
      },
      "source": [
        "### Delete an `Execution` resource\n",
        "\n",
        "You can delete an `Execution` resource using the `delete_execution()` method, with the following parameters:\n",
        "\n",
        "- `name`: The fully qualified resource identifier for the `Execution` resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "delete_metadata_execution"
      },
      "outputs": [],
      "source": [
        "clients[\"metadata\"].delete_execution(name=execution.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "delete_metadata_context"
      },
      "source": [
        "### Delete a `Context` resource\n",
        "\n",
        "You can delete an `Context` resource using the `delete_context()` method, with the following parameters:\n",
        "\n",
        "- `name`: The fully qualified resource identifier for the `Context` resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "delete_metadata_context"
      },
      "outputs": [],
      "source": [
        "clients[\"metadata\"].delete_context(name=context.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro_pipeline_metadata"
      },
      "source": [
        "## Introduction to tracking ML Metadata in a `Vertex AI Pipeline`\n",
        "\n",
        "Vertex AI Pipelines automatically records the metrics and artifacts created when the pipeline is exeuted. You can then use the SDK to track and analyze the metrics and artifacts across pipeline runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_kfp:basic"
      },
      "outputs": [],
      "source": [
        "from kfp.v2 import compiler, dsl\n",
        "from kfp.v2.dsl import (Artifact, Dataset, Input, Metrics, Model, Output,\n",
        "                        OutputPath, component, pipeline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "metadata_3step_pipeline"
      },
      "source": [
        "### Creating a 3-step pipeline with custom components\n",
        "\n",
        "First, you create a pipeline to run on `Vertex AI Pipelines`, consisting of the following custom components:\n",
        "\n",
        "* `get_dataframe`: Retrieve data from a BigQuery table and convert it into a pandas DataFrame.\n",
        "* `sklearn_train`: Use the pandas DataFrame to train and export a scikit-learn model, along with some metrics.\n",
        "* `deploy_model`: Deploy the exported scikit-learn model to a `Vertex AI Endpoint` resource.\n",
        "\n",
        "#### get_dataframe component\n",
        "\n",
        "This component does the following:\n",
        "\n",
        "* Creates a reference to a BigQuery table using the BigQuery client library\n",
        "* Downloads the BigQuery table and converts it to a shuffled pandas DataFrame\n",
        "* Exports the DataFrame to a CSV file\n",
        "\n",
        "#### sklearn_train component\n",
        "\n",
        "This component does the following:\n",
        "\n",
        "* Imports a CSV as a pandas DataFrame\n",
        "* Splits the DataFrame into train and test sets\n",
        "* Trains a scikit-learn model\n",
        "* Logs metrics from the model\n",
        "* Saves the model artifacts as a local `model.joblib` file\n",
        "\n",
        "#### deploy_model component\n",
        "\n",
        "This component does the following:\n",
        "\n",
        "* Uploads the scikit-learn model to a `Vertex AI Model` resource.\n",
        "* Deploys the model to a `Vertex AI Endpoint` resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "metadata_3step_pipeline"
      },
      "outputs": [],
      "source": [
        "@component(\n",
        "    packages_to_install=[\"google-cloud-bigquery\", \"pandas\", \"pyarrow\"],\n",
        "    base_image=\"python:3.9\",\n",
        "    output_component_file=\"create_dataset.yaml\",\n",
        ")\n",
        "def get_dataframe(bq_table: str, output_data_path: OutputPath(\"Dataset\")):\n",
        "    from google.cloud import bigquery\n",
        "\n",
        "    bqclient = bigquery.Client()\n",
        "    table = bigquery.TableReference.from_string(bq_table)\n",
        "    rows = bqclient.list_rows(table)\n",
        "    dataframe = rows.to_dataframe(\n",
        "        create_bqstorage_client=True,\n",
        "    )\n",
        "    dataframe = dataframe.sample(frac=1, random_state=2)\n",
        "    dataframe.to_csv(output_data_path)\n",
        "\n",
        "\n",
        "@component(\n",
        "    packages_to_install=[\"sklearn\", \"pandas\", \"joblib\"],\n",
        "    base_image=\"python:3.9\",\n",
        "    output_component_file=\"beans_model_component.yaml\",\n",
        ")\n",
        "def sklearn_train(\n",
        "    dataset: Input[Dataset], metrics: Output[Metrics], model: Output[Model]\n",
        "):\n",
        "    import pandas as pd\n",
        "    from joblib import dump\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "    df = pd.read_csv(dataset.path)\n",
        "    labels = df.pop(\"Class\").tolist()\n",
        "    data = df.values.tolist()\n",
        "    x_train, x_test, y_train, y_test = train_test_split(data, labels)\n",
        "\n",
        "    skmodel = DecisionTreeClassifier()\n",
        "    skmodel.fit(x_train, y_train)\n",
        "    score = skmodel.score(x_test, y_test)\n",
        "    print(\"accuracy is:\", score)\n",
        "\n",
        "    metrics.log_metric(\"accuracy\", (score * 100.0))\n",
        "    metrics.log_metric(\"framework\", \"Scikit Learn\")\n",
        "    metrics.log_metric(\"dataset_size\", len(df))\n",
        "    dump(skmodel, model.path + \".joblib\")\n",
        "\n",
        "\n",
        "@component(\n",
        "    packages_to_install=[\"google-cloud-aiplatform\"],\n",
        "    base_image=\"python:3.9\",\n",
        "    output_component_file=\"beans_deploy_component.yaml\",\n",
        ")\n",
        "def deploy_model(\n",
        "    model: Input[Model],\n",
        "    project: str,\n",
        "    region: str,\n",
        "    vertex_endpoint: Output[Artifact],\n",
        "    vertex_model: Output[Model],\n",
        "):\n",
        "    from google.cloud import aiplatform\n",
        "\n",
        "    aiplatform.init(project=project, location=region)\n",
        "\n",
        "    deployed_model = aiplatform.Model.upload(\n",
        "        display_name=\"beans-model-pipeline\",\n",
        "        artifact_uri=model.uri.replace(\"model\", \"\"),\n",
        "        serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-24:latest\",\n",
        "    )\n",
        "    endpoint = deployed_model.deploy(machine_type=\"n1-standard-4\")\n",
        "\n",
        "    # Save data to the output params\n",
        "    vertex_endpoint.uri = endpoint.resource_name\n",
        "    vertex_model.uri = deployed_model.resource_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_3step_pipeline"
      },
      "source": [
        "### Construct and compile the pipeline\n",
        "\n",
        "Next, construct the pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_3step_pipeline"
      },
      "outputs": [],
      "source": [
        "PIPELINE_ROOT = f\"{BUCKET_URI}/pipeline_root/3step\"\n",
        "\n",
        "\n",
        "@dsl.pipeline(\n",
        "    # Default pipeline root. You can override it when submitting the pipeline.\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        "    # A name for the pipeline.\n",
        "    name=\"mlmd-pipeline\",\n",
        ")\n",
        "def pipeline(\n",
        "    bq_table: str = \"\",\n",
        "    output_data_path: str = \"data.csv\",\n",
        "    project: str = PROJECT_ID,\n",
        "    region: str = REGION,\n",
        "):\n",
        "    dataset_task = get_dataframe(bq_table)\n",
        "\n",
        "    model_task = sklearn_train(dataset_task.output)\n",
        "\n",
        "    deploy_model(model=model_task.outputs[\"model\"], project=project, region=region)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_pipeline:3step"
      },
      "source": [
        "### Compile and execute two runs of the pipeline\n",
        "\n",
        "Next, you compile the pipeline and then run two separate instances of the pipeline. In the first instance, you train the model with a small version of the dataset and in the second instance you train it with a larger version of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_pipeline:3step"
      },
      "outputs": [],
      "source": [
        "NOW = datetime.now().isoformat().replace(\".\", \":\")[:-7]\n",
        "\n",
        "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"mlmd_pipeline.json\")\n",
        "\n",
        "run1 = aip.PipelineJob(\n",
        "    display_name=\"mlmd-pipeline\",\n",
        "    template_path=\"mlmd_pipeline.json\",\n",
        "    job_id=\"mlmd-pipeline-small-{}\".format(TIMESTAMP),\n",
        "    parameter_values={\"bq_table\": \"sara-vertex-demos.beans_demo.small_dataset\"},\n",
        "    enable_caching=True,\n",
        ")\n",
        "\n",
        "run2 = aip.PipelineJob(\n",
        "    display_name=\"mlmd-pipeline\",\n",
        "    template_path=\"mlmd_pipeline.json\",\n",
        "    job_id=\"mlmd-pipeline-large-{}\".format(TIMESTAMP),\n",
        "    parameter_values={\"bq_table\": \"sara-vertex-demos.beans_demo.large_dataset\"},\n",
        "    enable_caching=True,\n",
        ")\n",
        "\n",
        "run1.run()\n",
        "run2.run()\n",
        "\n",
        "run1.delete()\n",
        "run2.delete()\n",
        "\n",
        "! rm -f mlmd_pipeline.json *.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "compare_runs:3step"
      },
      "source": [
        "### Compare the pipeline runs\n",
        "\n",
        "Now that you have two pipeline completed pipeline runs, you can compare the runs.\n",
        "\n",
        "You can use the `get_pipeline_df()` method to access the metadata from the runs. The `mlmd-pipeline` parameter here refers to the name you gave to your pipeline:\n",
        "\n",
        "**Alternately, for guidance on inspecting pipeline artifacts and metadata in the Vertex AI Console, see [this codelab](https://codelabs.developers.google.com/vertex-mlmd-pipelines#5).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "compare_runs:3step"
      },
      "outputs": [],
      "source": [
        "df = aip.get_pipeline_df(pipeline=\"mlmd-pipeline\")\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualize_runs:3step"
      },
      "source": [
        "### Visualize the pipeline runs\n",
        "\n",
        "Next, you create a custom visualization with matplotlib to see the relationship between your model's accuracy and the amount of data used for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualize_runs:3step"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(df[\"metric.dataset_size\"], df[\"metric.accuracy\"], label=\"Accuracy\")\n",
        "plt.title(\"Accuracy and dataset size\")\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "list_metadata_artifacts:3tep"
      },
      "source": [
        "### Quering your `Metadatastore` resource\n",
        "\n",
        "Finally, you query your `Metadatastore` resource by specifying a `filter` parameter when calling the `list_artifacts()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "list_metadata_artifacts:3tep"
      },
      "outputs": [],
      "source": [
        "FILTER = f'create_time >= \"{NOW}\" AND state = LIVE'\n",
        "artifact_req = {\n",
        "    \"parent\": metadata_store_id,\n",
        "    \"filter\": FILTER,\n",
        "}\n",
        "\n",
        "artifacts = clients[\"metadata\"].list_artifacts(artifact_req)\n",
        "\n",
        "for _artifact in artifacts:\n",
        "    print(_artifact)\n",
        "    clients[\"metadata\"].delete_artifact(name=_artifact.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "delete_metadatastore"
      },
      "source": [
        "### Delete a `MetadataStore` resource\n",
        "\n",
        "You can delete a `MetadataStore` resource using the `delete_metadata_store()` method, with the following parameters:\n",
        "\n",
        "- `name`: The fully qualified resource identifier for the `MetadataStore` resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "delete_metadatastore"
      },
      "outputs": [],
      "source": [
        "clients[\"metadata\"].delete_metadata_store(name=metadata_store_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cleanup:mbsdk"
      },
      "source": [
        "# Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup:mbsdk"
      },
      "outputs": [],
      "source": [
        "delete_bucket = False\n",
        "\n",
        "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
        "    ! gsutil rm -r $BUCKET_URI"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "get_started_with_vertex_ml_metadata.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

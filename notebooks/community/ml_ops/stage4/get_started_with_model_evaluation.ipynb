{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "copyright"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title:generic,gcp"
      },
      "source": [
        "# E2E ML on GCP: MLOps stage 4 : evaluation: get started with Vertex AI Model Evaluation\n",
        "\n",
        "\n",
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage4/get_started_with_model_evaluation.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage4/get_started_with_model_evaluation.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/ml_ops/stage4/get_started_with_model_evaluation.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>                                                                                               \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview:mlops"
      },
      "source": [
        "## Overview\n",
        "\n",
        "\n",
        "This tutorial demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. This tutorial covers stage 4 : evaluation: get started with Vertex AI Model Evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "objective:mlops,stage4,get_started_vertex_model_evaluation"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to use `Vertex AI Model Evaluation`.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services:\n",
        "\n",
        "- `Vertex AI AutoML`\n",
        "- `BigQuery ML`\n",
        "- `Vertex AI Training`\n",
        "- `Vertex AI Batch Prediction`\n",
        "- `Vertex AI Model Evaluation`\n",
        "- `Google Cloud Pipeline Components`\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "**SDK**\n",
        "\n",
        "- Evaluate an `AutoML` model.\n",
        "    - Train an `AutoML` image classification model.\n",
        "    - Retrieve the default evaluation metrics from training.\n",
        "    - Do a batch evaluation for a custom evaluation slice.\n",
        "- Evaluate a BigQuery ML model.\n",
        "    - Train a `BigQuery ML` tabular classification model.\n",
        "    - Retrieve the default evaluation metrics from training.\n",
        "    - Do a batch evaluation for a custom evaluation slice.\n",
        "- Evaluate a custom model.\n",
        "    - Do a batch evaluation for a custom evaluation slice.\n",
        "    - Add an evaluation to the `Model Registry` for the `Model` resource.\n",
        "    \n",
        "**Pipeline Components**\n",
        "\n",
        "- Evaluate an `AutoML` model.\n",
        "    - Train an `AutoML` image classification model.\n",
        "    - Retrieve the default evaluation metrics from training.\n",
        "    - Do a batch evaluation for a custom evaluation slice.\n",
        "- Evaluate a BigQuery ML model.\n",
        "    - Train a `BigQuery ML` tabular classification model.\n",
        "    - Retrieve the default evaluation metrics from training.\n",
        "    - Do a batch evaluation for a custom evaluation slice.\n",
        "- Evaluate a custom model.\n",
        "    - Do a batch evaluation for a custom evaluation slice.\n",
        "    - Add an evaluation to the `Model Registry` for the `Model` resource."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset:bank,lbn"
      },
      "source": [
        "### Datasets\n",
        "\n",
        "**AutoML image model**\n",
        "\n",
        "The dataset used for this tutorial is the [Flowers dataset](https://www.tensorflow.org/datasets/catalog/tf_flowers) from [TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/overview). The version of the dataset in this tutorial is stored in a public Cloud Storage bucket. The trained model predicts the type of flower an image is from a class of five flowers: daisy, dandelion, rose, sunflower, or tulip.\n",
        "\n",
        "**BigQuery ML tabular model**\n",
        "\n",
        "The dataset used for this tutorial is the Penguins dataset from [BigQuery public datasets](https://cloud.google.com/bigquery/public-data). This version of the dataset is used to predict the species of penguins from the available features like culmen-length, flipper-depth etc.\n",
        "\n",
        "**Custom model**\n",
        "\n",
        "This tutorial uses a pre-trained image classification model from TensorFlow Hub, which is trained on ImageNet dataset.\n",
        "\n",
        "Learn more about [ResNet V2 pretained model](https://tfhub.dev/google/imagenet/resnet_v2_101/classification/5). \n",
        "\n",
        "\n",
        "**Pipeline**\n",
        "\n",
        "The dataset used for this tutorial is the [Bank Marketing](https://pantheon.corp.google.com/storage/browser/_details/cloud-ml-tables-data/bank-marketing.csv) . This dataset does not require any feature engineering. The version of the dataset in this tutorial is stored in a public Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c997d8d92ce"
      },
      "source": [
        "### Costs\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "- Vertex AI\n",
        "- Cloud Storage\n",
        "- BigQuery\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing) and [BigQuery pricing](https://cloud.google.com/bigquery/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_mlops"
      },
      "source": [
        "## Installations\n",
        "\n",
        "Install the packages required for executing this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_mlops"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Vertex AI Workbench Notebook product has specific requirements\n",
        "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
        "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
        "    \"/opt/deeplearning/metadata/env_version\"\n",
        ")\n",
        "\n",
        "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_WORKBENCH_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\"\n",
        "\n",
        "# Install the packages\n",
        "! pip3 install --upgrade google-cloud-aiplatform $USER_FLAG -q\n",
        "! pip3 install --upgrade google-cloud-pipeline-components $USER_FLAG -q\n",
        "! pip3 install --upgrade google-cloud-bigquery $USER_FLAG -q\n",
        "! pip3 install --upgrade tensorflow $USER_FLAG -q\n",
        "! pip3 install --upgrade tensorflow-hub $USER_FLAG -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "restart"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "restart"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "before_you_begin"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### GPU runtime\n",
        "\n",
        "*Make sure you're running this notebook in a GPU runtime if you have that option. In Colab, select* **Runtime > Change Runtime Type > GPU**\n",
        "\n",
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "2. [Make sure that billing is enabled for your project.](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
        "\n",
        "3. [Enable the following APIs: Vertex AI APIs, Compute Engine APIs, and Cloud Storage.](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component,storage-component.googleapis.com)\n",
        "\n",
        "4. If you are running this notebook locally, you need to install the [Cloud SDK]((https://cloud.google.com/sdk)).\n",
        "\n",
        "5. Enter your project ID in the cell below. Then run the  cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "project_id"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_project_id"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_project_id"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID:\", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_gcloud_project_id"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
        "\n",
        "- Americas: `us-central1`\n",
        "- Europe: `europe-west4`\n",
        "- Asia Pacific: `asia-east1`\n",
        "\n",
        "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
        "\n",
        "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "region"
      },
      "outputs": [],
      "source": [
        "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
        "\n",
        "if REGION == \"[your-region]\":\n",
        "    REGION = \"us-central1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "timestamp"
      },
      "source": [
        "#### Timestamp\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "timestamp"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcp_authenticate"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using Vertex AI Workbench Notebooks**, your environment is already authenticated. Skip this step.\n",
        "\n",
        "**If you are using Colab**, run the cell below and follow the instructions when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "In the Cloud Console, go to the [Create service account key](https://console.cloud.google.com/apis/credentials/serviceaccountkey) page.\n",
        "\n",
        "**Click Create service account**.\n",
        "\n",
        "In the **Service account name** field, enter a name, and click **Create**.\n",
        "\n",
        "In the **Grant this service account access to project** section, click the Role drop-down list. Type \"Vertex\" into the filter box, and select **Vertex Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "\n",
        "Click Create. A JSON file that contains your key downloads to your local environment.\n",
        "\n",
        "Enter the path to your service account key as the GOOGLE_APPLICATION_CREDENTIALS variable in the cell below and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcp_authenticate"
      },
      "outputs": [],
      "source": [
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# If on Vertex AI Workbench, then don't execute this code\n",
        "IS_COLAB = False\n",
        "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
        "    \"DL_ANACONDA_HOME\"\n",
        "):\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        IS_COLAB = True\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bucket:mbsdk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "When you initialize the Vertex SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bucket"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_bucket"
      },
      "outputs": [],
      "source": [
        "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
        "    BUCKET_NAME = PROJECT_ID + \"aip-\" + TIMESTAMP\n",
        "    BUCKET_URI = \"gs://\" + BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_bucket"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "validate_bucket"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "validate_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "set_service_account"
      },
      "source": [
        "#### Service Account\n",
        "\n",
        "**If you don't know your service account**, try to get your service account using `gcloud` command by executing the second cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_service_account"
      },
      "outputs": [],
      "source": [
        "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_service_account"
      },
      "outputs": [],
      "source": [
        "if (\n",
        "    SERVICE_ACCOUNT == \"\"\n",
        "    or SERVICE_ACCOUNT is None\n",
        "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
        "):\n",
        "    # Get your service account from gcloud\n",
        "    if not IS_COLAB:\n",
        "        shell_output = !gcloud auth list 2>/dev/null\n",
        "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
        "\n",
        "    if IS_COLAB:\n",
        "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
        "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "\n",
        "    print(\"Service Account:\", SERVICE_ACCOUNT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "set_service_account:pipelines"
      },
      "source": [
        "#### Set service account access for Vertex AI Pipelines\n",
        "\n",
        "Run the following commands to grant your service account access to read and write pipeline artifacts in the bucket that you created in the previous step -- you only need to run these once per service account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_service_account:pipelines"
      },
      "outputs": [],
      "source": [
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
        "\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_vars"
      },
      "source": [
        "### Set up variables\n",
        "\n",
        "Next, set up some variables used throughout the tutorial.\n",
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_kfp"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "import google.cloud.aiplatform as aiplatform\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from kfp import dsl\n",
        "from kfp.v2 import compiler\n",
        "from kfp.v2.dsl import component"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_bq"
      },
      "source": [
        "#### Import BigQuery\n",
        "\n",
        "Import the BigQuery package into your Python environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_bq"
      },
      "outputs": [],
      "source": [
        "from google.cloud import bigquery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "outputs": [],
      "source": [
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_bq"
      },
      "source": [
        "### Create BigQuery client\n",
        "\n",
        "Create the BigQuery client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_bq"
      },
      "outputs": [],
      "source": [
        "bqclient = bigquery.Client()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "accelerators:prediction,mbsdk"
      },
      "source": [
        "#### Set hardware accelerators\n",
        "\n",
        "You can set hardware accelerators for prediction.\n",
        "\n",
        "Set the variable `DEPLOY_GPU/DEPLOY_NGPU` to use a container image supporting a GPU and the number of GPUs allocated to the virtual machine (VM) instance. For example, to use a GPU container image with 4 Nvidia Telsa K80 GPUs allocated to each VM, you would specify:\n",
        "\n",
        "    (aip.AcceleratorType.NVIDIA_TESLA_K80, 4)\n",
        "\n",
        "Otherwise specify `(None, None)` to use a container image to run on a CPU.\n",
        "\n",
        "Learn more [here](https://cloud.google.com/vertex-ai/docs/general/locations#accelerators) hardware accelerator support for your region"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "accelerators:prediction,mbsdk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if os.getenv(\"IS_TESTING_DEPLOY_GPU\"):\n",
        "    DEPLOY_GPU, DEPLOY_NGPU = (\n",
        "        aiplatform.gapic.AcceleratorType.NVIDIA_TESLA_K80,\n",
        "        int(os.getenv(\"IS_TESTING_DEPLOY_GPU\")),\n",
        "    )\n",
        "else:\n",
        "    DEPLOY_GPU, DEPLOY_NGPU = (aiplatform.gapic.AcceleratorType.NVIDIA_TESLA_K80, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "container:prediction"
      },
      "source": [
        "#### Set pre-built containers\n",
        "\n",
        "Set the pre-built Docker container image for prediction.\n",
        "\n",
        "- Set the variable `TF` to the TensorFlow version of the container image. For example, `2-1` would be version 2.1, and `1-15` would be version 1.15. The following list shows some of the pre-built images available:\n",
        "\n",
        "\n",
        "For the latest list, see [Pre-built containers for prediction](https://cloud.google.com/ai-platform-unified/docs/predictions/pre-built-containers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "container:prediction"
      },
      "outputs": [],
      "source": [
        "if os.getenv(\"IS_TESTING_TF\"):\n",
        "    TF = os.getenv(\"IS_TESTING_TF\")\n",
        "else:\n",
        "    TF = \"2-5\".replace(\".\", \"-\")\n",
        "\n",
        "if TF[0] == \"2\":\n",
        "    if DEPLOY_GPU:\n",
        "        DEPLOY_VERSION = \"tf2-gpu.{}\".format(TF)\n",
        "    else:\n",
        "        DEPLOY_VERSION = \"tf2-cpu.{}\".format(TF)\n",
        "else:\n",
        "    if DEPLOY_GPU:\n",
        "        DEPLOY_VERSION = \"tf-gpu.{}\".format(TF)\n",
        "    else:\n",
        "        DEPLOY_VERSION = \"tf-cpu.{}\".format(TF)\n",
        "\n",
        "DEPLOY_IMAGE = \"{}-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(\n",
        "    REGION.split(\"-\")[0], DEPLOY_VERSION\n",
        ")\n",
        "\n",
        "print(\"Deployment:\", DEPLOY_IMAGE, DEPLOY_GPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "machine:prediction"
      },
      "source": [
        "#### Set machine type\n",
        "\n",
        "Next, set the machine type to use for prediction.\n",
        "\n",
        "- Set the variable `DEPLOY_COMPUTE` to configure the compute resources for the VM you will use for prediction.\n",
        " - `machine type`\n",
        "     - `n1-standard`: 3.75GB of memory per vCPU.\n",
        "     - `n1-highmem`: 6.5GB of memory per vCPU\n",
        "     - `n1-highcpu`: 0.9 GB of memory per vCPU\n",
        " - `vCPUs`: number of \\[2, 4, 8, 16, 32, 64, 96 \\]\n",
        "\n",
        "*Note: You may also use n2 and e2 machine types for training and deployment, but they do not support GPUs*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "machine:prediction"
      },
      "outputs": [],
      "source": [
        "if os.getenv(\"IS_TESTING_DEPLOY_MACHINE\"):\n",
        "    MACHINE_TYPE = os.getenv(\"IS_TESTING_DEPLOY_MACHINE\")\n",
        "else:\n",
        "    MACHINE_TYPE = \"n1-standard\"\n",
        "\n",
        "VCPU = \"4\"\n",
        "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
        "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro:model_eval,automl"
      },
      "source": [
        "## Introduction to Vertex AI Model Evaluation for AutoML models.\n",
        "\n",
        "For AutoML models, you can retrieve the model evaluation metrics that were obtained during training from the dataset split into train and test, using the `Vertex AI Model Evaluation` service. Additionally, you can further evaluate the model with custom evaluation slices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_file:u_dataset,csv"
      },
      "source": [
        "#### Location of Cloud Storage training data.\n",
        "\n",
        "Now set the variable `IMPORT_FILE` to the location of the CSV index file in Cloud Storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_file:flowers,csv,icn"
      },
      "outputs": [],
      "source": [
        "IMPORT_FILE = (\n",
        "    \"gs://cloud-samples-data/vision/automl_classification/flowers/all_data_v2.csv\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_dataset:image,icn"
      },
      "source": [
        "### Create the Dataset\n",
        "\n",
        "Next, create the `Dataset` resource using the `create` method for the `ImageDataset` class, which takes the following parameters:\n",
        "\n",
        "- `display_name`: The human readable name for the `Dataset` resource.\n",
        "- `gcs_source`: A list of one or more dataset index files to import the data items into the `Dataset` resource.\n",
        "- `import_schema_uri`: The data labeling schema for the data items.\n",
        "\n",
        "This operation may take several minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_dataset:image,icn"
      },
      "outputs": [],
      "source": [
        "dataset = aiplatform.ImageDataset.create(\n",
        "    display_name=\"Flowers\" + \"_\" + TIMESTAMP,\n",
        "    gcs_source=[IMPORT_FILE],\n",
        "    import_schema_uri=aiplatform.schema.dataset.ioformat.image.single_label_classification,\n",
        ")\n",
        "\n",
        "print(dataset.resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_automl_pipeline:image,icn"
      },
      "source": [
        "### Create and run training pipeline\n",
        "\n",
        "To train an AutoML model, you perform two steps: 1) create a training pipeline, and 2) run the pipeline.\n",
        "\n",
        "#### Create training pipeline\n",
        "\n",
        "An AutoML training pipeline is created with the `AutoMLImageTrainingJob` class, with the following parameters:\n",
        "\n",
        "- `display_name`: The human readable name for the `TrainingJob` resource.\n",
        "- `prediction_type`: The type task to train the model for.\n",
        "  - `classification`: An image classification model.\n",
        "  - `object_detection`: An image object detection model.\n",
        "- `multi_label`: If a classification task, whether single (`False`) or multi-labeled (`True`).\n",
        "- `model_type`: The type of model for deployment.\n",
        "  - `CLOUD`: Deployment on Google Cloud\n",
        "  - `CLOUD_HIGH_ACCURACY_1`: Optimized for accuracy over latency for deployment on Google Cloud.\n",
        "  - `CLOUD_LOW_LATENCY_`: Optimized for latency over accuracy for deployment on Google Cloud.\n",
        "  - `MOBILE_TF_VERSATILE_1`: Deployment on an edge device.\n",
        "  - `MOBILE_TF_HIGH_ACCURACY_1`:Optimized for accuracy over latency for deployment on an edge device.\n",
        "  - `MOBILE_TF_LOW_LATENCY_1`: Optimized for latency over accuracy for deployment on an edge device.\n",
        "- `base_model`: (optional) Transfer learning from existing `Model` resource -- supported for image classification only.\n",
        "\n",
        "The instantiated object is the DAG (directed acyclic graph) for the training job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_automl_pipeline:image,icn"
      },
      "outputs": [],
      "source": [
        "dag = aiplatform.AutoMLImageTrainingJob(\n",
        "    display_name=\"flowers_\" + TIMESTAMP,\n",
        "    prediction_type=\"classification\",\n",
        "    multi_label=False,\n",
        "    model_type=\"CLOUD\",\n",
        "    base_model=None,\n",
        ")\n",
        "\n",
        "print(dag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_automl_pipeline:image"
      },
      "source": [
        "#### Run the training pipeline\n",
        "\n",
        "Next, you run the DAG to start the training job by invoking the method `run`, with the following parameters:\n",
        "\n",
        "- `dataset`: The `Dataset` resource to train the model.\n",
        "- `model_display_name`: The human readable name for the trained model.\n",
        "- `training_fraction_split`: The percentage of the dataset to use for training.\n",
        "- `test_fraction_split`: The percentage of the dataset to use for test (holdout data).\n",
        "- `validation_fraction_split`: The percentage of the dataset to use for validation.\n",
        "- `budget_milli_node_hours`: (optional) Maximum training time specified in unit of millihours (1000 = hour).\n",
        "- `disable_early_stopping`: If `True`, training maybe completed before using the entire budget if the service believes it cannot further improve on the model objective measurements.\n",
        "\n",
        "The `run` method when completed returns the `Model` resource.\n",
        "\n",
        "The execution of the training pipeline will take upto 20 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_automl_pipeline:image"
      },
      "outputs": [],
      "source": [
        "model = dag.run(\n",
        "    dataset=dataset,\n",
        "    model_display_name=\"flowers_\" + TIMESTAMP,\n",
        "    training_fraction_split=0.8,\n",
        "    validation_fraction_split=0.1,\n",
        "    test_fraction_split=0.1,\n",
        "    budget_milli_node_hours=8000,\n",
        "    disable_early_stopping=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c1cb501c4ad"
      },
      "source": [
        "### Retrieving the default evaluation for `AutoML Model` resource\n",
        "\n",
        "BLAH GAPIC Placeholder\n",
        "After your model has finished training, you can review the evaluation scores for it.\n",
        "\n",
        "First, you need to get a reference to the new model. As with datasets, you can either use the reference to the model variable you created when you deployed the model or you can list all of the models in your project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bfc39c62ff5"
      },
      "outputs": [],
      "source": [
        "model_evaluations = model.list_model_evaluations()\n",
        "\n",
        "for model_evaluation in model_evaluations:\n",
        "    print(model_evaluation.to_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d448681f2724"
      },
      "source": [
        "### Evaluating on a custom evaluation slice\n",
        "\n",
        "PLACEHOLDER - BLAH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c89cd59d390f"
      },
      "source": [
        "### Make the batch input file\n",
        "\n",
        "Now make a batch input file, which you store in your local Cloud Storage bucket. The batch input file must be in JSONL format. In For JSONL file, you make one dictionary entry per line for each data item (instance). The dictionary contains the key/value pairs:\n",
        "\n",
        "- `content`: The Cloud Storage path to the image.\n",
        "- `mime_type`: The content type. In our example, it is a `jpeg` file.\n",
        "\n",
        "For example:\n",
        "\n",
        "                        {'content': '[your-bucket]/file1.jpg', 'mime_type': 'jpeg'}\n",
        "    \n",
        "For demonstration purposes, to create an evaluation slice, you use a portion of the training data -- as if it was separate (non-training) data, such as instances seen in production."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3661bc67e841"
      },
      "outputs": [],
      "source": [
        "EVAL_SLICE = BUCKET_URI + \"/flowers_eval.jsonl\"\n",
        "\n",
        "! gsutil cat {IMPORT_FILE} | head -n 200 >tmp.csv\n",
        "\n",
        "import csv\n",
        "\n",
        "entries = []\n",
        "with open(\"tmp.csv\", \"r\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    for row in reader:\n",
        "        path = row[0]\n",
        "        label = row[1]\n",
        "\n",
        "        file = path.split(\"/\")[-1]\n",
        "\n",
        "        new_path = BUCKET_URI + \"/flowers/\" + file\n",
        "\n",
        "        ! gsutil cp {path} {new_path} >/dev/null\n",
        "        entries.append({\"content\": new_path, \"mime_type\": \"jpeg\"})\n",
        "\n",
        "import json\n",
        "\n",
        "with open(\"tmp.jsonl\", \"w\") as f:\n",
        "    for entry in entries:\n",
        "        f.write(json.dumps(entry) + \"\\n\")\n",
        "\n",
        "! gsutil cp tmp.jsonl {EVAL_SLICE}\n",
        "#! rm tmp.csv tmp.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "batch_request:mbsdk"
      },
      "source": [
        "### Make the batch prediction request\n",
        "\n",
        "Now that your Model resource is trained, you can make a batch prediction by invoking the batch_predict() method, with the following parameters:\n",
        "\n",
        "- `job_display_name`: The human readable name for the batch prediction job.\n",
        "- `instances_format`: The format of the prediction request; can only be JSONL (default).\n",
        "- `gcs_source`: A list of one or more batch request input files.\n",
        "- `gcs_destination_prefix`: The Cloud Storage location for storing the batch prediction resuls.\n",
        "- `sync`: If set to True, the call will block while waiting for the asynchronous batch job to complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "batch_request:mbsdk"
      },
      "outputs": [],
      "source": [
        "batch_predict_job = model.batch_predict(\n",
        "    job_display_name=\"flowers_\" + TIMESTAMP,\n",
        "    instances_format=\"jsonl\",\n",
        "    gcs_source=EVAL_SLICE,\n",
        "    gcs_destination_prefix=BUCKET_URI,\n",
        "    sync=True,\n",
        ")\n",
        "\n",
        "print(batch_predict_job)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c400027f1b4"
      },
      "source": [
        "### TODO: Get batch results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "get_batch_prediction:mbsdk,icn"
      },
      "source": [
        "### Get the predictions\n",
        "\n",
        "Next, get the results from the completed batch prediction job.\n",
        "\n",
        "The results are written to the Cloud Storage output bucket you specified in the batch prediction request. You call the method iter_outputs() to get a list of each Cloud Storage file generated with the results. Each file contains one or more prediction requests in a JSON format:\n",
        "\n",
        "- `content`: The prediction request.\n",
        "- `prediction`: The prediction response.\n",
        " - `ids`: The internal assigned unique identifiers for each prediction request.\n",
        " - `displayNames`: The class names for each class label.\n",
        " - `confidences`: The predicted confidence, between 0 and 1, per class label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "get_batch_prediction:mbsdk,icn"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "bp_iter_outputs = batch_predict_job.iter_outputs()\n",
        "\n",
        "prediction_results = list()\n",
        "for blob in bp_iter_outputs:\n",
        "    if blob.name.split(\"/\")[-1].startswith(\"prediction\"):\n",
        "        prediction_results.append(blob.name)\n",
        "\n",
        "tags = list()\n",
        "for prediction_result in prediction_results:\n",
        "    gfile_name = f\"gs://{bp_iter_outputs.bucket.name}/{prediction_result}\"\n",
        "    with tf.io.gfile.GFile(name=gfile_name, mode=\"r\") as gfile:\n",
        "        for line in gfile.readlines():\n",
        "            line = json.loads(line)\n",
        "            print(line)\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e6dc2a7bb53"
      },
      "source": [
        "#### Delete temporary resources\n",
        "\n",
        "Next, you delete all the temporary resources created by this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5181cf916f12"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    dag.delete()\n",
        "    model.delete()\n",
        "    batch_predict_job.delete()\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro:model_eval,automl"
      },
      "source": [
        "## Introduction to Vertex AI Model Evaluation for BigQuery ML models.\n",
        "\n",
        "For BigQuery ML models, you can retrieve the model evaluation metrics that were obtained during training from the dataset split into train and test, using the `Vertex AI Model Evaluation` service. Additionally, you can further evaluate the model with custom evaluation slices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "807327fe3ec8"
      },
      "source": [
        "### Location of the BigQuery training data\n",
        "\n",
        "Now set the variable `IMPORT_FILE` and `BQ_TABLE` to the location of the training data in `BigQuery`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_file:penguins,bq,lcn"
      },
      "outputs": [],
      "source": [
        "IMPORT_FILE = \"bq://bigquery-public-data.ml_datasets.penguins\"\n",
        "BQ_TABLE = \"bigquery-public-data.ml_datasets.penguins\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqml_create_dataset"
      },
      "source": [
        "### Create BQ dataset resource\n",
        "\n",
        "First, you create an empty dataset resource in your project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqml_create_dataset"
      },
      "outputs": [],
      "source": [
        "BQ_DATASET_NAME = \"penguins\"\n",
        "DATASET_QUERY = f\"\"\"CREATE SCHEMA {BQ_DATASET_NAME}\n",
        "\"\"\"\n",
        "\n",
        "job = bqclient.query(DATASET_QUERY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b4498ca6fea"
      },
      "source": [
        "### Setting permissions to automatically register the model\n",
        "\n",
        "You need to set some additional IAM permissions for BigQuery ML to automatically upload and register the model after training. Depending on your service account, the setting of the permissions below may fail. In this case, we recommend executing the permissions in a Cloud Shell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0472e888105e"
      },
      "outputs": [],
      "source": [
        "! gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
        "      --member='serviceAccount:cloud-dataengine@system.gserviceaccount.com' \\\n",
        "      --role='roles/aiplatform.admin'\n",
        "\n",
        "! gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
        "      --member='user:cloud-dataengine@prod.google.com' \\\n",
        "      --role='roles/aiplatform.admin'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqml_create_model"
      },
      "source": [
        "### Training and registering the BigQuery ML model\n",
        "\n",
        "Next, you create and train a BigQuery ML tabular classification model from the public dataset penguins and store the model in your project using the `CREATE MODEL` statement. The model configuration is specified in the `OPTIONS` statement as follows:\n",
        "\n",
        "- `model_type`: The type and archictecture of tabular model to train, e.g., DNN classification.\n",
        "- `labels`: The column which are the labels.\n",
        "- `model_registry`: Set to \"vertex_ai\" to indicate automatic registation to `Vertex AI Model Registry`.\n",
        "- `vertex_ai_model_id`: The human readable display name for the registered model.\n",
        "- `vertex_ai_model_version_aliases`: Alternate names for the model.\n",
        "\n",
        "Learn more about [The CREATE MODEL statement](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqml_create_model"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"penguins\"\n",
        "MODEL_QUERY = f\"\"\"\n",
        "CREATE OR REPLACE MODEL `{BQ_DATASET_NAME}.{MODEL_NAME}`\n",
        "OPTIONS(\n",
        "    model_type='DNN_CLASSIFIER',\n",
        "    labels = ['species'],\n",
        "    model_registry=\"vertex_ai\",\n",
        "    vertex_ai_model_id=\"bqml_model_{TIMESTAMP}\", \n",
        "    vertex_ai_model_version_aliases=[\"1\"]\n",
        "    )\n",
        "AS\n",
        "SELECT *\n",
        "FROM `{BQ_TABLE}`\n",
        "\"\"\"\n",
        "\n",
        "job = bqclient.query(MODEL_QUERY)\n",
        "print(job.errors, job.state)\n",
        "\n",
        "while job.running():\n",
        "    from time import sleep\n",
        "\n",
        "    sleep(30)\n",
        "    print(\"Running ...\")\n",
        "print(job.errors, job.state)\n",
        "\n",
        "tblname = job.ddl_target_table\n",
        "tblname = \"{}.{}\".format(tblname.dataset_id, tblname.table_id)\n",
        "print(\"{} created in {}\".format(tblname, job.ended - job.started))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqml_eval_model"
      },
      "source": [
        "### Evaluate the trained BigQuery model using BigQuery\n",
        "\n",
        "Next, retrieve the model evaluation from within BigQuery for the trained BigQuery ML model.\n",
        "\n",
        "Learn more about [The ML.EVALUATE function](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-evaluate)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqml_eval_model"
      },
      "outputs": [],
      "source": [
        "EVAL_QUERY = f\"\"\"\n",
        "SELECT *\n",
        "FROM\n",
        "  ML.EVALUATE(MODEL {BQ_DATASET_NAME}.{MODEL_NAME})\n",
        "ORDER BY  roc_auc desc\n",
        "LIMIT 1\"\"\"\n",
        "\n",
        "job = bqclient.query(EVAL_QUERY)\n",
        "results = job.result().to_dataframe()\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b4970272040"
      },
      "source": [
        "### Find the model in the `Vertex AI Model Registry`\n",
        "\n",
        "Finally, you can use the `Vertex AI Model` list() method with a filter query to find the automatically registered model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76c22674ba99"
      },
      "outputs": [],
      "source": [
        "models = aiplatform.Model.list(filter=\"display_name=bqml_model_\" + TIMESTAMP)\n",
        "model = models[0]\n",
        "\n",
        "print(model.gca_resource)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41aecf259370"
      },
      "source": [
        "### Retrieving the default evaluation for `BigQuery ML Model` resource from `Vertex AI Model Registry`\n",
        "\n",
        "BLAH GAPIC Placeholder\n",
        "After your model has finished training, you can review the evaluation scores for it.\n",
        "\n",
        "First, you need to get a reference to the new model. As with datasets, you can either use the reference to the model variable you created when you deployed the model or you can list all of the models in your project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab9abfb7184e"
      },
      "outputs": [],
      "source": [
        "# Get a reference to the Model Service client\n",
        "client_options = {\"api_endpoint\": f\"{REGION}-aiplatform.googleapis.com\"}\n",
        "model_service_client = aiplatform.gapic.ModelServiceClient(\n",
        "    client_options=client_options\n",
        ")\n",
        "\n",
        "model_evaluations = model_service_client.list_model_evaluations(\n",
        "    parent=model.resource_name\n",
        ")\n",
        "model_evaluation = list(model_evaluations)[0]\n",
        "print(model_evaluation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae5b591a2105"
      },
      "source": [
        "### Evaluating on a custom evaluation slice\n",
        "\n",
        "PLACEHOLDER - BLAH\n",
        "\n",
        "BLAH - BATCH FILE FORMAT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04c37f78af15"
      },
      "source": [
        "#### Delete temporary resources\n",
        "\n",
        "Next, you delete all the temporary resources created by this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4418968b5dd"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    model.delete()\n",
        "    batch_predict_job.delete()\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "try:\n",
        "    # Delete the created BigQuery dataset\n",
        "    ! bq rm -r -f $PROJECT_ID:$BQ_DATASET_NAME\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "MODEL_QUERY = f\"\"\"\n",
        "DROP MODEL `{BQ_DATASET_NAME}.{MODEL_NAME}`\n",
        "\"\"\"\n",
        "\n",
        "job = bqclient.query(MODEL_QUERY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro:model_eval,automl"
      },
      "source": [
        "## Introduction to Vertex AI Model Evaluation for custom models.\n",
        "\n",
        "For custom models, you can retrieve the model evaluation metrics that were \n",
        "\n",
        "BLAH\n",
        "\n",
        "obtained during training from the dataset split into train and test, using the `Vertex AI Model Evaluation` service. Additionally, you can further evaluate the model with custom evaluation slices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8128b8ff025"
      },
      "source": [
        "## Get pretrained model from TensorFlow Hub\n",
        "\n",
        "For demonstration purposes, this tutorial uses a pretrained model from TensorFlow Hub (TFHub), which is then uploaded to a `Vertex AI Model` resource. Once you have a `Vertex AI Model` resource, the model can be deployed to a `Vertex AI Endpoint` resource.\n",
        "\n",
        "### Download the pretrained model\n",
        "\n",
        "First, you download the pretrained model from TensorFlow Hub. The model gets downloaded as a TF.Keras layer. To finalize the model, in this example, you create a `Sequential()` model with the downloaded TFHub model as a layer, and specify the input shape to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c55fa4c826f7"
      },
      "outputs": [],
      "source": [
        "tfhub_model = tf.keras.Sequential(\n",
        "    [hub.KerasLayer(\"https://tfhub.dev/google/imagenet/resnet_v2_101/classification/5\")]\n",
        ")\n",
        "\n",
        "tfhub_model.build([None, 224, 224, 3])\n",
        "\n",
        "tfhub_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63de49055083"
      },
      "source": [
        "### Save the model artifacts\n",
        "\n",
        "At this point, the model is in memory. Next, you save the model artifacts to a Cloud Storage location.\n",
        "\n",
        "*Note:* For TF Serving, the MODEL_DIR must end in a subfolder that is a number, e.g., 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64618c713db9"
      },
      "outputs": [],
      "source": [
        "MODEL_DIR = BUCKET_URI + \"/model/1\"\n",
        "tfhub_model.save(MODEL_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "how_serving_function_works"
      },
      "source": [
        "## Upload the model for serving\n",
        "\n",
        "Next, you will upload your TF.Keras model from the custom job to Vertex `Model` service, which will create a Vertex `Model` resource for your custom model. During upload, you need to define a serving function to convert data to the format your model expects. If you send encoded data to Vertex AI, your serving function ensures that the data is decoded on the model server before it is passed as input to your model.\n",
        "\n",
        "### How does the serving function work\n",
        "\n",
        "When you send a request to an online prediction server, the request is received by a HTTP server. The HTTP server extracts the prediction request from the HTTP request content body. The extracted prediction request is forwarded to the serving function. For Google pre-built prediction containers, the request content is passed to the serving function as a `tf.string`.\n",
        "\n",
        "The serving function consists of two parts:\n",
        "\n",
        "- `preprocessing function`:\n",
        "  - Converts the input (`tf.string`) to the input shape and data type of the underlying model (dynamic graph).\n",
        "  - Performs the same preprocessing of the data that was done during training the underlying model -- e.g., normalizing, scaling, etc.\n",
        "- `post-processing function`:\n",
        "  - Converts the model output to format expected by the receiving application -- e.q., compresses the output.\n",
        "  - Packages the output for the the receiving application -- e.g., add headings, make JSON object, etc.\n",
        "\n",
        "Both the preprocessing and post-processing functions are converted to static graphs which are fused to the model. The output from the underlying model is passed to the post-processing function. The post-processing function passes the converted/packaged output back to the HTTP server. The HTTP server returns the output as the HTTP response content.\n",
        "\n",
        "One consideration you need to consider when building serving functions for TF.Keras models is that they run as static graphs. That means, you cannot use TF graph operations that require a dynamic graph. If you do, you will get an error during the compile of the serving function which will indicate that you are using an EagerTensor which is not supported."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "serving_function_image:post"
      },
      "source": [
        "### Serving function for image data\n",
        "\n",
        "#### Preprocessing\n",
        "\n",
        "To pass images to the prediction service, you encode the compressed (e.g., JPEG) image bytes into base 64 -- which makes the content safe from modification while transmitting binary data over the network. Since this deployed model expects input data as raw (uncompressed) bytes, you need to ensure that the base 64 encoded data gets converted back to raw bytes, and then preprocessed to match the model input requirements, before it is passed as input to the deployed model.\n",
        "\n",
        "To resolve this, you define a serving function (`serving_fn`) and attach it to the model as a preprocessing step. Add a `@tf.function` decorator so the serving function is fused to the underlying model (instead of upstream on a CPU).\n",
        "\n",
        "When you send a prediction or explanation request, the content of the request is base 64 decoded into a Tensorflow string (`tf.string`), which is passed to the serving function (`serving_fn`). The serving function preprocesses the `tf.string` into raw (uncompressed) numpy bytes (`preprocess_fn`) to match the input requirements of the model:\n",
        "\n",
        "- `io.decode_jpeg`- Decompresses the JPG image which is returned as a Tensorflow tensor with three channels (RGB).\n",
        "- `image.convert_image_dtype` - Changes integer pixel values to float 32, and rescales pixel data between 0 and 1.\n",
        "- `image.resize` - Resizes the image to match the input shape for the model.\n",
        "\n",
        "At this point, the data can be passed to the model (`m_call`), via a concrete function. The serving function is a static graph, while the model is a dynamic graph. The concrete function performs the tasks of marshalling the input data from the serving function to the model, and marshalling the prediction result from the model back to the serving function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "serving_function_image"
      },
      "outputs": [],
      "source": [
        "CONCRETE_INPUT = \"numpy_inputs\"\n",
        "\n",
        "\n",
        "def _preprocess(bytes_input):\n",
        "    decoded = tf.io.decode_jpeg(bytes_input, channels=3)\n",
        "    decoded = tf.image.convert_image_dtype(decoded, tf.float32)\n",
        "    resized = tf.image.resize(decoded, size=(224, 224))\n",
        "    return resized\n",
        "\n",
        "\n",
        "@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])\n",
        "def preprocess_fn(bytes_inputs):\n",
        "    decoded_images = tf.map_fn(\n",
        "        _preprocess, bytes_inputs, dtype=tf.float32, back_prop=False\n",
        "    )\n",
        "    return {\n",
        "        CONCRETE_INPUT: decoded_images\n",
        "    }  # User needs to make sure the key matches model's input\n",
        "\n",
        "\n",
        "@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])\n",
        "def serving_fn(bytes_inputs):\n",
        "    images = preprocess_fn(bytes_inputs)\n",
        "    prob = m_call(**images)\n",
        "    return prob\n",
        "\n",
        "\n",
        "m_call = tf.function(tfhub_model.call).get_concrete_function(\n",
        "    [tf.TensorSpec(shape=[None, 224, 224, 3], dtype=tf.float32, name=CONCRETE_INPUT)]\n",
        ")\n",
        "\n",
        "tf.saved_model.save(tfhub_model, MODEL_DIR, signatures={\"serving_default\": serving_fn})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "serving_function_signature:image"
      },
      "source": [
        "## Get the serving function signature\n",
        "\n",
        "You can get the signatures of your model's input and output layers by reloading the model into memory, and querying it for the signatures corresponding to each layer.\n",
        "\n",
        "For your purpose, you need the signature of the serving function. Why? Well, when we send our data for prediction as a HTTP request packet, the image data is base64 encoded, and our TF.Keras model takes numpy input. Your serving function will do the conversion from base64 to a numpy array.\n",
        "\n",
        "When making a prediction request, you need to route the request to the serving function instead of the model, so you need to know the input layer name of the serving function -- which you will use later when you make a prediction request."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "serving_function_signature:image"
      },
      "outputs": [],
      "source": [
        "loaded = tf.saved_model.load(MODEL_DIR)\n",
        "\n",
        "serving_input = list(\n",
        "    loaded.signatures[\"serving_default\"].structured_input_signature[1].keys()\n",
        ")[0]\n",
        "print(\"Serving function input:\", serving_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8ce91147c93"
      },
      "source": [
        "### Upload the TensorFlow Hub model to a `Vertex AI Model` resource\n",
        "\n",
        "Finally, you upload the model artifacts from the TFHub model into a `Vertex AI Model` resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad61e1429512"
      },
      "outputs": [],
      "source": [
        "model = aiplatform.Model.upload(\n",
        "    display_name=\"example_\" + TIMESTAMP,\n",
        "    artifact_uri=MODEL_DIR,\n",
        "    serving_container_image_uri=DEPLOY_IMAGE,\n",
        ")\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "222ef231f3d7"
      },
      "source": [
        "### BLAH Do batch prediction on custom model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d21a9f6a7798"
      },
      "source": [
        "### BLAH register the custom evaluation metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aeb23e71108"
      },
      "source": [
        "## Model evaluation using `Vertex AI Pipeline` components\n",
        "\n",
        "In this section, you perform model evaluations on `AutoML`, `BigQuery ML` and custom models using `Vertex AI Pipeline` components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "572dd365e30b"
      },
      "source": [
        "### AutoML model evaluation pipeline component\n",
        "\n",
        "BLAH\n",
        "\n",
        "Additionally, you can evaluate an AutoML model with custom evaluation slices using the combination of BatchPredictionOp and ModelEvaluationOp components, as:\n",
        "\n",
        "- The custom evaluation slice data contains the label values (ground truths).\n",
        "- Perform a batch prediction on the custom evaluation slice.\n",
        "- Perform a model evaluation with the batch prediction results and label values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_file:u_dataset,csv"
      },
      "source": [
        "#### Location of Cloud Storage training data.\n",
        "\n",
        "Now set the variable `IMPORT_FILE` to the location of the CSV index file in Cloud Storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_file:bank,lbn,split"
      },
      "outputs": [],
      "source": [
        "IMPORT_FILE = \"gs://cloud-ml-tables-data/bank-marketing.csv\"\n",
        "! gsutil cat {IMPORT_FILE} | head -n 40000 > train.csv\n",
        "! gsutil cat {IMPORT_FILE} | head -n 1 >eval.csv\n",
        "! gsutil cat {IMPORT_FILE} | tail -n 5200 >> eval.csv\n",
        "\n",
        "IMPORT_TRAIN = BUCKET_NAME + \"/train.csv\"\n",
        "IMPORT_EVAL = BUCKET_NAME + \"/eval.csv\"\n",
        "\n",
        "! gsutil cp train.csv {IMPORT_TRAIN}\n",
        "! gsutil cp eval.csv {IMPORT_EVAL}\n",
        "\n",
        "! rm -f train.csv eval.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_automl_eval_component"
      },
      "source": [
        "### Create AutoML model evaluation component\n",
        "\n",
        "The Vertex AI pre-built pipeline components does not currently have a component for retrieiving the model evaluations for a AutoML model. So, you will first write your own component, as follows:\n",
        "\n",
        "- Takes as input the region and Model artifacts returned from an AutoML training component.\n",
        "- Create a client interface to the Vertex AI Model service (`metadata[\"resource_name\"]).\n",
        "- Construct the resource ID for the model from the model artifact parameter.\n",
        "- Retrieve the model evaluation\n",
        "- Return the model evaluation as a string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_automl_eval_component"
      },
      "outputs": [],
      "source": [
        "from kfp.v2.dsl import Artifact, Input, Model\n",
        "\n",
        "\n",
        "@component(packages_to_install=[\"google-cloud-aiplatform\"])\n",
        "def evaluateAutoMLModelOp(model: Input[Artifact], region: str) -> str:\n",
        "    import logging\n",
        "\n",
        "    import google.cloud.aiplatform.gapic as gapic\n",
        "\n",
        "    # Get a reference to the Model Service client\n",
        "    client_options = {\"api_endpoint\": f\"{region}-aiplatform.googleapis.com\"}\n",
        "    model_service_client = gapic.ModelServiceClient(client_options=client_options)\n",
        "\n",
        "    model_id = model.metadata[\"resourceName\"]\n",
        "\n",
        "    model_evaluations = model_service_client.list_model_evaluations(parent=model_id)\n",
        "    model_evaluation = list(model_evaluations)[0]\n",
        "    logging.info(model_evaluation)\n",
        "    return str(model_evaluation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_automl_pipeline:lbn,eval"
      },
      "source": [
        "### Construct pipeline for AutoML training, and batch model evaluation\n",
        "\n",
        "Next, construct the pipeline with the following tasks:\n",
        "\n",
        "- Create a Vertex AI Dataset resource.\n",
        "- Train a AutoML tabular classification model.\n",
        "- Retrieve the AutoML evaluation statistics.\n",
        "- Make a batch prediction with the AutoML model, using an evaluation slice that was not used during training.\n",
        "- Evaluate the AutoML model using the results from the batch prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_automl_pipeline:lbn,eval"
      },
      "outputs": [],
      "source": [
        "PIPELINE_ROOT = \"{}/pipeline_root/automl_lbn_training\".format(BUCKET_NAME)\n",
        "\n",
        "\n",
        "@dsl.pipeline(\n",
        "    name=\"automl-lbn-training\", description=\"AutoML tabular classification training\"\n",
        ")\n",
        "def pipeline(\n",
        "    import_file: str,\n",
        "    batch_files: list,\n",
        "    display_name: str,\n",
        "    bucket: str = PIPELINE_ROOT,\n",
        "    project: str = PROJECT_ID,\n",
        "    region: str = REGION,\n",
        "):\n",
        "    from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
        "    from google_cloud_pipeline_components.experimental.evaluation import \\\n",
        "        ModelEvaluationOp\n",
        "    from google_cloud_pipeline_components.v1.batch_predict_job import \\\n",
        "        ModelBatchPredictOp\n",
        "\n",
        "    dataset_op = gcc_aip.TabularDatasetCreateOp(\n",
        "        project=project, display_name=display_name, gcs_source=import_file\n",
        "    )\n",
        "\n",
        "    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n",
        "        project=project,\n",
        "        display_name=display_name,\n",
        "        optimization_prediction_type=\"classification\",\n",
        "        dataset=dataset_op.outputs[\"dataset\"],\n",
        "        model_display_name=display_name,\n",
        "        training_fraction_split=0.8,\n",
        "        validation_fraction_split=0.1,\n",
        "        test_fraction_split=0.1,\n",
        "        budget_milli_node_hours=8000,\n",
        "        optimization_objective=\"minimize-log-loss\",\n",
        "        target_column=\"Deposit\",\n",
        "    )\n",
        "\n",
        "    eval_op = evaluateAutoMLModelOp(model=training_op.outputs[\"model\"], region=region)\n",
        "\n",
        "    batch_op = ModelBatchPredictOp(\n",
        "        project=project,\n",
        "        job_display_name=\"batch_predict_job\",\n",
        "        model=training_op.outputs[\"model\"],\n",
        "        gcs_source_uris=batch_files,\n",
        "        gcs_destination_output_uri_prefix=bucket,\n",
        "        instances_format=\"csv\",\n",
        "        predictions_format=\"jsonl\",\n",
        "        model_parameters={},\n",
        "        machine_type=DEPLOY_COMPUTE,\n",
        "        starting_replica_count=1,\n",
        "        max_replica_count=1,\n",
        "    ).after(eval_op)\n",
        "\n",
        "    batch_eval_op = ModelEvaluationOp(\n",
        "        project=project,\n",
        "        root_dir=bucket,\n",
        "        problem_type=\"classification\",\n",
        "        classification_type=\"multiclass\",\n",
        "        ground_truth_column=\"Deposit\",\n",
        "        class_names=[\"0\", \"1\"],\n",
        "        predictions_format=\"jsonl\",\n",
        "        batch_prediction_job=batch_op.outputs[\"batchpredictionjob\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_automl_pipeline:lbn,eval"
      },
      "source": [
        "### Compile and execute the AutoML training, and batch model evaluation pipeline\n",
        "\n",
        "Next, you compile the pipeline and then execute it. The pipeline takes the following parameters, which are passed as the dictionary `parameter_values`:\n",
        "\n",
        "- `import_file`: The Cloud Storage location of the training data.\n",
        "- `batch_files`: A list of one or more Cloud Storage locations of evaluation data.\n",
        "- `display_name`: Display name for Vertex AI Model and Endpoint resources.\n",
        "- `project`: The project ID.\n",
        "- `region`: The region."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_automl_pipeline:lbn,eval"
      },
      "outputs": [],
      "source": [
        "compiler.Compiler().compile(\n",
        "    pipeline_func=pipeline, package_path=\"automl_lbn_training.json\"\n",
        ")\n",
        "\n",
        "pipeline = aip.PipelineJob(\n",
        "    display_name=\"automl_lbn_training\",\n",
        "    template_path=\"automl_lbn_training.json\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        "    parameter_values={\n",
        "        \"import_file\": IMPORT_TRAIN,\n",
        "        \"batch_files\": [IMPORT_EVAL],\n",
        "        \"display_name\": \"bank\" + TIMESTAMP,\n",
        "        \"project\": PROJECT_ID,\n",
        "        \"region\": REGION,\n",
        "    },\n",
        ")\n",
        "\n",
        "pipeline.run()\n",
        "\n",
        "! rm -f automl_lbn_training.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view_pipeline_results:automl,eval,lbn"
      },
      "source": [
        "### View the AutoML training and batch evaluation pipeline results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "view_pipeline_results:automl,eval,lbn"
      },
      "outputs": [],
      "source": [
        "PROJECT_NUMBER = pipeline.gca_resource.name.split(\"/\")[1]\n",
        "print(PROJECT_NUMBER)\n",
        "\n",
        "\n",
        "def print_pipeline_output(job, output_task_name):\n",
        "    JOB_ID = job.name\n",
        "    print(JOB_ID)\n",
        "    for _ in range(len(job.gca_resource.job_detail.task_details)):\n",
        "        TASK_ID = job.gca_resource.job_detail.task_details[_].task_id\n",
        "        EXECUTE_OUTPUT = (\n",
        "            PIPELINE_ROOT\n",
        "            + \"/\"\n",
        "            + PROJECT_NUMBER\n",
        "            + \"/\"\n",
        "            + JOB_ID\n",
        "            + \"/\"\n",
        "            + output_task_name\n",
        "            + \"_\"\n",
        "            + str(TASK_ID)\n",
        "            + \"/executor_output.json\"\n",
        "        )\n",
        "        GCP_RESOURCES = (\n",
        "            PIPELINE_ROOT\n",
        "            + \"/\"\n",
        "            + PROJECT_NUMBER\n",
        "            + \"/\"\n",
        "            + JOB_ID\n",
        "            + \"/\"\n",
        "            + output_task_name\n",
        "            + \"_\"\n",
        "            + str(TASK_ID)\n",
        "            + \"/gcp_resources\"\n",
        "        )\n",
        "        if tf.io.gfile.exists(EXECUTE_OUTPUT):\n",
        "            ! gsutil cat $EXECUTE_OUTPUT\n",
        "            break\n",
        "        elif tf.io.gfile.exists(GCP_RESOURCES):\n",
        "            ! gsutil cat $GCP_RESOURCES\n",
        "            break\n",
        "\n",
        "    return EXECUTE_OUTPUT\n",
        "\n",
        "\n",
        "print(\"tabular-dataset-create\")\n",
        "artifacts = print_pipeline_output(pipeline, \"tabular-dataset-create\")\n",
        "print(\"\\n\\n\")\n",
        "print(\"automl-tabular-training-job\")\n",
        "artifacts = print_pipeline_output(pipeline, \"automl-tabular-training-job\")\n",
        "print(\"\\n\\n\")\n",
        "print(\"evaluateautomlmodelop\")\n",
        "artifacts = print_pipeline_output(pipeline, \"evaluateautomlmodelop\")\n",
        "output = !gsutil cat $artifacts\n",
        "output = json.loads(output[0])\n",
        "metrics = output[\"parameters\"][\"Output\"][\"stringValue\"]\n",
        "print(\"\\n\")\n",
        "print(metrics)\n",
        "print(\"\\n\\n\")\n",
        "print(\"model-batch-predict\")\n",
        "artifacts = print_pipeline_output(pipeline, \"model-batch-predict\")\n",
        "output = !gsutil cat $artifacts\n",
        "output = json.loads(output[0])\n",
        "print(\"\\n\\n\")\n",
        "print(\n",
        "    output[\"artifacts\"][\"batchpredictionjob\"][\"artifacts\"][0][\"metadata\"][\n",
        "        \"gcsOutputDirectory\"\n",
        "    ]\n",
        ")\n",
        "print(\"model-evaluation\")\n",
        "artifacts = print_pipeline_output(pipeline, \"model-evaluation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "delete_pipeline"
      },
      "source": [
        "### Delete a pipeline job\n",
        "\n",
        "After a pipeline job is completed, you can delete the pipeline job with the method `delete()`.  Prior to completion, a pipeline job can be canceled with the method `cancel()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "delete_pipeline"
      },
      "outputs": [],
      "source": [
        "pipeline.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro:model_eval,bqml"
      },
      "source": [
        "## Introduction to Vertex AI Model Evaluation for BigQuery ML models.\n",
        "\n",
        "For BigQuery ML models, you can retrieve the model evaluation metrics that were obtained during training from the dataset split into train and test, using the `BigQuery ML` service.\n",
        "\n",
        "Additionally, you can evaluate an BigQuery ML model with custom evaluation slices using the combination of BLAH\n",
        "`BatchPredictionOp` and `ModelEvaluationOp` components, as:\n",
        "\n",
        "    - The custom evaluation slice data contains the label values (ground truths).\n",
        "    - Perform a batch prediction on the custom evaluation slice.\n",
        "    - Perform a model evaluation with the batch prediction results and label values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_file:penguins,bq,lcn"
      },
      "outputs": [],
      "source": [
        "IMPORT_FILE = \"bq://bigquery-public-data.ml_datasets.penguins\"\n",
        "BQ_TABLE = \"bigquery-public-data.ml_datasets.penguins\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49d953424b88"
      },
      "outputs": [],
      "source": [
        "BQ_TABLE = \"bigquery-public-data.ml_datasets.penguins\"\n",
        "BQ_DATASET = BQ_TABLE.split(\".\")[1]\n",
        "\n",
        "\n",
        "def get_data(slice_name, limit):\n",
        "    query = f\"\"\"\n",
        "    CREATE OR REPLACE TABLE `{slice_name}`\n",
        "    AS (\n",
        "        WITH\n",
        "          penguins AS (\n",
        "          SELECT\n",
        "            island,\n",
        "            sex,\n",
        "            culmen_length_mm,\n",
        "            culmen_depth_mm,\n",
        "            flipper_length_mm,\n",
        "            body_mass_g,\n",
        "            species\n",
        "          FROM\n",
        "            `{BQ_TABLE}`\n",
        "        )\n",
        "\n",
        "        SELECT\n",
        "          island,\n",
        "          sex,\n",
        "          culmen_length_mm,\n",
        "          culmen_depth_mm,\n",
        "          flipper_length_mm,\n",
        "          body_mass_g,\n",
        "          species\n",
        "        FROM\n",
        "          penguins\n",
        "        LIMIT {limit}\n",
        "    )\n",
        "    \"\"\"\n",
        "\n",
        "    response = bqclient.query(query)\n",
        "    _ = response.result()\n",
        "\n",
        "\n",
        "BQ_TABLE_EVAL = f\"{PROJECT_ID}.{BQ_DATASET}.penguins_eval\"\n",
        "IMPORT_EVAL = f\"bq://{BQ_TABLE_EVAL}\"\n",
        "LIMIT = 44\n",
        "get_data(BQ_TABLE_EVAL, LIMIT)\n",
        "\n",
        "BQ_TABLE_TRAIN = f\"{PROJECT_ID}.{BQ_DATASET}.penguins_train\"\n",
        "IMPORT_TRAIN = f\"bq://{BQ_TABLE_TRAIN}\"\n",
        "LIMIT = \"300 OFFSET 44\"\n",
        "get_data(BQ_TABLE_TRAIN, LIMIT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_bqml_pipeline:lcn,eval"
      },
      "source": [
        "### Construct pipeline for BigQuery ML training, and batch model evaluation\n",
        "\n",
        "Next, construct the pipeline with the following tasks:\n",
        "\n",
        "- Create a BigQuery ML Dataset resource.\n",
        "- Train a BigQuery ML tabular classification model.\n",
        "- Retrieve the BigQuery ML evaluation statistics.\n",
        "- Make a batch prediction with the BigQuery ML model, using an evaluation slice that was not used during training.\n",
        "- Evaluate the BigQuery ML model using the results from the batch prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_bqml_pipeline:lcn,eval"
      },
      "outputs": [],
      "source": [
        "PIPELINE_ROOT = f\"{BUCKET_NAME}/bq_query\"\n",
        "\n",
        "\n",
        "@dsl.pipeline(name=\"bq-hello-world\", pipeline_root=PIPELINE_ROOT)\n",
        "def pipeline(\n",
        "    bq_train_table: str,\n",
        "    bq_eval_table: str,\n",
        "    label: str,\n",
        "    class_names: list,\n",
        "    dataset: str,\n",
        "    model: str,\n",
        "    artifact_uri: str,\n",
        "    # num_trials: int,\n",
        "    deploy_image: str,\n",
        "    machine_type: str,\n",
        "    min_replica_count: int,\n",
        "    max_replica_count: int,\n",
        "    display_name: str,\n",
        "    bucket: str,\n",
        "    accelerator_type: str = \"\",\n",
        "    accelerator_count: int = 0,\n",
        "    project: str = PROJECT_ID,\n",
        "    location: str = \"US\",\n",
        "    region: str = \"us-central1\",\n",
        "):\n",
        "    from google_cloud_pipeline_components.experimental.evaluation import \\\n",
        "        ModelEvaluationOp\n",
        "    from google_cloud_pipeline_components.v1.batch_predict_job import \\\n",
        "        ModelBatchPredictOp\n",
        "    from google_cloud_pipeline_components.v1.bigquery import (\n",
        "        BigqueryCreateModelJobOp, BigqueryEvaluateModelJobOp,\n",
        "        BigqueryExportModelJobOp, BigqueryQueryJobOp)\n",
        "    from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
        "\n",
        "    bq_dataset = BigqueryQueryJobOp(\n",
        "        project=project, location=\"US\", query=f\"CREATE SCHEMA {dataset}\"\n",
        "    )\n",
        "\n",
        "    bq_model = BigqueryCreateModelJobOp(\n",
        "        project=project,\n",
        "        location=location,\n",
        "        query=f\"CREATE OR REPLACE MODEL {dataset}.{model} OPTIONS (model_type='dnn_classifier', labels=['{label}']) AS SELECT * FROM `{bq_train_table}` WHERE body_mass_g IS NOT NULL AND sex IS NOT NULL\",\n",
        "    ).after(bq_dataset)\n",
        "\n",
        "    bq_eval = BigqueryEvaluateModelJobOp(\n",
        "        project=PROJECT_ID, location=\"US\", model=bq_model.outputs[\"model\"]\n",
        "    ).after(bq_model)\n",
        "\n",
        "    bq_export = BigqueryExportModelJobOp(\n",
        "        project=project,\n",
        "        location=location,\n",
        "        model=bq_model.outputs[\"model\"],\n",
        "        model_destination_path=artifact_uri,\n",
        "    ).after(bq_model)\n",
        "\n",
        "    model_upload = ModelUploadOp(\n",
        "        display_name=display_name,\n",
        "        artifact_uri=artifact_uri,\n",
        "        serving_container_image_uri=deploy_image,\n",
        "        project=project,\n",
        "        location=region,\n",
        "    ).after(bq_export)\n",
        "\n",
        "    batch_predict = ModelBatchPredictOp(\n",
        "        project=project,\n",
        "        job_display_name=\"batch_predict_job\",\n",
        "        model=model_upload.outputs[\"model\"],\n",
        "        bigquery_source_input_uri=bq_eval_table,\n",
        "        bigquery_destination_output_uri=f\"bq://{project}\",\n",
        "        instances_format=\"bigquery\",\n",
        "        predictions_format=\"bigquery\",\n",
        "        model_parameters={},\n",
        "        machine_type=DEPLOY_COMPUTE,\n",
        "        starting_replica_count=min_replica_count,\n",
        "        max_replica_count=max_replica_count,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "    ).after(model_upload)\n",
        "\n",
        "    batch_eval = ModelEvaluationOp(\n",
        "        project=project,\n",
        "        root_dir=bucket,\n",
        "        problem_type=\"classification\",\n",
        "        classification_type=\"multiclass\",\n",
        "        ground_truth_column=label,\n",
        "        class_names=class_names,\n",
        "        predictions_format=\"jsonl\",\n",
        "        batch_prediction_job=batch_predict.outputs[\"batchpredictionjob\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_pipeline:bqml,eval"
      },
      "source": [
        "### Compile and execute the BigQuery ML training, and batch model evaluation pipeline\n",
        "\n",
        "Next, you compile the pipeline and then execute it. The pipeline takes the following parameters, which are passed as the dictionary `parameter_values`:\n",
        "\n",
        "- `bq_train_table`: The BigQuery table containing the training data.\n",
        "- `bq_eval_table`: The BigQuery table containing the evaluation data.\n",
        "- `label`: The corresponding label for the BigQuery dataset.\n",
        "- `dataset`: The BigQuery dataset component name.\n",
        "- `model`: The BigQuery model component name.\n",
        "- `artifact_uri`: The Cloud Storage location to export the BigQuery model artifacts.\n",
        "- `num_trials`: If greater than one, will perform hyperparameter tuning for the specified number of trials using the Vertex AI Vizier service.\n",
        "- `deploy_image`: The container image for serving predictions.\n",
        "- `machine_type`: The VM for serving predictions.\n",
        "- `min_replica_count`/`max_replica_count`: The number of virtual machines for auto-scaling predictions.\n",
        "- `display_name`: Display name for Vertex AI Model resource.\n",
        "- `project`: The project ID.\n",
        "- `region`: The region."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_pipeline:bqml,eval"
      },
      "outputs": [],
      "source": [
        "MODEL_DIR = BUCKET_NAME + \"/bqmodel\"\n",
        "\n",
        "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"bqml.json\")\n",
        "\n",
        "pipeline = aip.PipelineJob(\n",
        "    display_name=\"bqml\",\n",
        "    template_path=\"bqml.json\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        "    parameter_values={\n",
        "        \"bq_train_table\": BQ_TABLE_TRAIN,\n",
        "        \"bq_eval_table\": IMPORT_EVAL,\n",
        "        \"label\": \"species\",\n",
        "        \"class_names\": [\n",
        "            \"Adelie Penguin (Pygoscelis adeliae)\",\n",
        "            \"Chinstrap penguin (Pygoscelis antarctica)\",\n",
        "            \"Gentoo penguin (Pygoscelis papua)\",\n",
        "        ],\n",
        "        \"dataset\": \"bqml_tutorial\",\n",
        "        \"model\": \"penguins_model\",\n",
        "        \"artifact_uri\": MODEL_DIR,\n",
        "        #'num_trials': 1,\n",
        "        \"deploy_image\": DEPLOY_IMAGE,\n",
        "        \"display_name\": \"penguins\",\n",
        "        \"machine_type\": DEPLOY_COMPUTE,\n",
        "        \"min_replica_count\": 1,\n",
        "        \"max_replica_count\": 1,\n",
        "        \"accelerator_type\": DEPLOY_GPU.name,\n",
        "        \"accelerator_count\": 1,\n",
        "        \"bucket\": BUCKET_NAME,\n",
        "        \"project\": PROJECT_ID,\n",
        "        \"location\": \"US\",\n",
        "    },\n",
        "    # enable_caching=False\n",
        ")\n",
        "\n",
        "pipeline.run()\n",
        "\n",
        "! rm -rf bqml.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view_pipeline_results:bqml,eval,lcn"
      },
      "source": [
        "### View the BigQuery ML training and batch evaluation pipeline results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "view_pipeline_results:bqml,eval,lcn"
      },
      "outputs": [],
      "source": [
        "PROJECT_NUMBER = pipeline.gca_resource.name.split(\"/\")[1]\n",
        "print(PROJECT_NUMBER)\n",
        "\n",
        "\n",
        "def print_pipeline_output(job, output_task_name):\n",
        "    JOB_ID = job.name\n",
        "    print(JOB_ID)\n",
        "    for _ in range(len(job.gca_resource.job_detail.task_details)):\n",
        "        TASK_ID = job.gca_resource.job_detail.task_details[_].task_id\n",
        "        EXECUTE_OUTPUT = (\n",
        "            PIPELINE_ROOT\n",
        "            + \"/\"\n",
        "            + PROJECT_NUMBER\n",
        "            + \"/\"\n",
        "            + JOB_ID\n",
        "            + \"/\"\n",
        "            + output_task_name\n",
        "            + \"_\"\n",
        "            + str(TASK_ID)\n",
        "            + \"/executor_output.json\"\n",
        "        )\n",
        "        GCP_RESOURCES = (\n",
        "            PIPELINE_ROOT\n",
        "            + \"/\"\n",
        "            + PROJECT_NUMBER\n",
        "            + \"/\"\n",
        "            + JOB_ID\n",
        "            + \"/\"\n",
        "            + output_task_name\n",
        "            + \"_\"\n",
        "            + str(TASK_ID)\n",
        "            + \"/gcp_resources\"\n",
        "        )\n",
        "        if tf.io.gfile.exists(EXECUTE_OUTPUT):\n",
        "            ! gsutil cat $EXECUTE_OUTPUT\n",
        "            break\n",
        "        elif tf.io.gfile.exists(GCP_RESOURCES):\n",
        "            ! gsutil cat $GCP_RESOURCES\n",
        "            break\n",
        "\n",
        "    return EXECUTE_OUTPUT\n",
        "\n",
        "\n",
        "print(\"bigquery-query-job\")\n",
        "artifacts = print_pipeline_output(pipeline, \"bigquery-query-job\")\n",
        "print(\"\\n\\n\")\n",
        "print(\"bigquery-create-model-job\")\n",
        "artifacts = print_pipeline_output(pipeline, \"bigquery-create-model-job\")\n",
        "print(\"\\n\\n\")\n",
        "print(\"bigquery-evaluate-model-job\")\n",
        "artifacts = print_pipeline_output(pipeline, \"bigquery-evaluate-model-job\")\n",
        "print(\"\\n\\n\")\n",
        "print(\"bigquery-export-model-job\")\n",
        "artifacts = print_pipeline_output(pipeline, \"bigquery-export-model-job\")\n",
        "print(\"\\n\\n\")\n",
        "print(\"model-upload\")\n",
        "artifacts = print_pipeline_output(pipeline, \"model-upload\")\n",
        "print(\"\\n\\n\")\n",
        "print(\"model-batch-predict\")\n",
        "artifacts = print_pipeline_output(pipeline, \"model-batch-predict\")\n",
        "output = !gsutil cat $artifacts\n",
        "output = json.loads(output[0])\n",
        "print(\"\\n\\n\")\n",
        "print(\n",
        "    output[\"artifacts\"][\"batchpredictionjob\"][\"artifacts\"][0][\"metadata\"][\n",
        "        \"gcsOutputDirectory\"\n",
        "    ]\n",
        ")\n",
        "print(\"model-evaluation\")\n",
        "artifacts = print_pipeline_output(pipeline, \"model-evaluation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "delete_pipeline"
      },
      "source": [
        "### Delete a pipeline job\n",
        "\n",
        "After a pipeline job is completed, you can delete the pipeline job with the method `delete()`.  Prior to completion, a pipeline job can be canceled with the method `cancel()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "delete_pipeline"
      },
      "outputs": [],
      "source": [
        "pipeline.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "delete:bqml,penguins"
      },
      "source": [
        "#### Delete the BigQuery model and dataset\n",
        "\n",
        "Next, delete the BigQuery model and dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "delete:bqml,penguins"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    job = bqclient.delete_model(\"bqml_tutorial.penguins_model\")\n",
        "except:\n",
        "    pass\n",
        "job = bqclient.delete_dataset(\"bqml_tutorial\", delete_contents=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cleanup:mbsdk"
      },
      "source": [
        "# Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial.\n",
        "\n",
        "- Dataset\n",
        "- Pipeline\n",
        "- Model\n",
        "- Endpoint\n",
        "- AutoML Training Job\n",
        "- Batch Job\n",
        "- Custom Job\n",
        "- Hyperparameter Tuning Job\n",
        "- Cloud Storage Bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup:mbsdk"
      },
      "outputs": [],
      "source": [
        "delete_all = True\n",
        "\n",
        "if delete_all:\n",
        "    # Delete the dataset using the Vertex dataset object\n",
        "    try:\n",
        "        if \"dataset\" in globals():\n",
        "            dataset.delete()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    # Delete the model using the Vertex model object\n",
        "    try:\n",
        "        if \"model\" in globals():\n",
        "            model.delete()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    # Delete the endpoint using the Vertex endpoint object\n",
        "    try:\n",
        "        if \"endpoint\" in globals():\n",
        "            endpoint.undeploy_all()\n",
        "            endpoint.delete()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    # Delete the AutoML or Pipeline training job\n",
        "    try:\n",
        "        if \"dag\" in globals():\n",
        "            dag.delete()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    # Delete the custom training job\n",
        "    try:\n",
        "        if \"job\" in globals():\n",
        "            job.delete()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    # Delete the batch prediction job using the Vertex batch prediction object\n",
        "    try:\n",
        "        if \"batch_predict_job\" in globals():\n",
        "            batch_predict_job.delete()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    # Delete the hyperparameter tuning job using the Vertex hyperparameter tuning object\n",
        "    try:\n",
        "        if \"hpt_job\" in globals():\n",
        "            hpt_job.delete()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    if \"BUCKET_NAME\" in globals():\n",
        "        ! gsutil rm -r $BUCKET_NAME"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "get_started_with_model_evaluation.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

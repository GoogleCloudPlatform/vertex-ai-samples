{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title:generic,gcp",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "# E2E ML on GCP: MLOps stage 1 : data management: get started with Vertex datasets\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/tree/master/notebooks/official/automl/ml_ops_stage1/get_started_vertex_datasets.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/ai/platform/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/tree/master/notebooks/official/automl/ml_ops_stage1/get_started_vertex_datasets.ipynb\">\n",
    "      Open in Google Cloud Notebooks\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:mlops",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Overview\n",
    "\n",
    "\n",
    "This tutorial demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. This tutorial covers stage 1 : data management: get started with Vertex datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:mlops,stage1,get_started_vertex",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you learn how to use `Vertex Dataset` for training with `Vertex AI`.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services:\n",
    "\n",
    "- `Vertex Datasets`\n",
    "- `BigQuery Datasets`\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Create a Vertex `Dataset` resource for:\n",
    "    - image data\n",
    "    - text data\n",
    "    - video data\n",
    "    - tabular data\n",
    "    - forecasting data\n",
    "\n",
    "\n",
    "- Search `Dataset` resources using a filter.\n",
    "- Read a sample of a `BigQuery` dataset into a dataframe.\n",
    "- Generate statistics and data schema using TensorFlow Data Validation from the samples in the dataframe.\n",
    "- Detect anomalies in new data using TensorFlow Data Validation.\n",
    "- Generate a TFRecord feature specification using TensorFlow Transform from the data schema.\n",
    "- Export a dataset and convert to TFRecords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "recommendation:mlops,stage1,vertex,datasets",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Recommendations\n",
    "\n",
    "When doing E2E MLOps on Google Cloud, the following best practices with Vertex Datasets:\n",
    "\n",
    "- Use CSV index file format for image data\n",
    "- Use CSV index file format for text data:\n",
    "    - For short text strings, embed the text string in the CSV file.\n",
    "    - For long text strings, place text in referenced text file.\n",
    "\n",
    "\n",
    "- Use JSON index file format for video data\n",
    "- For tabular data:\n",
    "    - For small datasets use CSV index file format.\n",
    "    - For large datasets use BigQuery table.\n",
    "\n",
    "\n",
    "- Use `filter` and `order_by` parameters in the `list()` methods to find the latest versions of datasets.\n",
    "\n",
    "- When custom training with a `Vertex Dataset`:\n",
    "    - tabular data :\n",
    "        - Use the CSV index file or BigQuery table reference.\n",
    "        - Create a tf.data.Dataset generator from the CSV index file/BigQuery table.\n",
    "    - image/video data:\n",
    "        - Export the data to a JSONL index file.\n",
    "        - Using the index file, convert the images/videos and labels to TFRecords.\n",
    "        - Create a tf.data.Dataset generator from the TFRercords.\n",
    "    - text data:\n",
    "        - If text strings are embedded:\n",
    "            - Convert to CSV file.\n",
    "            - Create a tf.data.Dataset generator from the CSV index file.\n",
    "        - If text strings are in text files:\n",
    "            - Using the JSON index file, convert the text files and labels to TFRecords.\n",
    "            - Create a tf.data.Dataset from the TFRecords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_aip:mbsdk",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "## Installation\n",
    "\n",
    "Install the latest version of Vertex SDK for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_aip:mbsdk",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Google Cloud Notebook\n",
    "if os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n",
    "    USER_FLAG = '--user'\n",
    "else:\n",
    "    USER_FLAG = ''\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_tfdv",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "Install the latest GA version of *TensorFlow Data Validation* library as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_tfdv",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "! pip3 install -U tensorflow-data-validation $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_tft",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "Install the latest GA version of *TensorFlow Transform* library as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_tft",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "! pip3 install -U tensorflow-transform $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "restart",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_gcloud_project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
    "\n",
    "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "region",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "REGION = 'us-central1'  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "timestamp",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "timestamp",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:custom",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "When you submit a custom training job using the Vertex SDK, you upload a Python package\n",
    "containing your training code to a Cloud Storage bucket. Vertex AI runs\n",
    "the code from this package. In this tutorial, Vertex AI also saves the\n",
    "trained model that results from your job in the same bucket. You can then\n",
    "create an `Endpoint` resource based on this output in order to serve\n",
    "online predictions.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validate_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "validate_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Set up variables\n",
    "\n",
    "Next, set up some variables used throughout the tutorial.\n",
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_aip:mbsdk",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_bq",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Import BigQuery\n",
    "\n",
    "Import the BigQuery package into your Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_bq",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_tfdv",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Import TFDV\n",
    "\n",
    "Import the TensorFlow Data Validation (TFDV) package into your Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_tfdv",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "import tensorflow_data_validation as tfdv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_tft",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Import TensorFlow Transform\n",
    "\n",
    "Import the TensorFlow Transform (TFT) package into your Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_tft",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "import tensorflow_transform as tft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,region",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Initialize Vertex SDK for Python\n",
    "\n",
    "Initialize the Vertex SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk,region",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_bq",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Create BigQuery client\n",
    "\n",
    "Create the BigQuery client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_bq",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "bqclient = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset_intro",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "## Vertex Datasets\n",
    "\n",
    "Vertex `Datasets` are the means for managing your datasets within Vertex AI services. Vertex Datasets are also referred to as `Dataset` resources. There are four types of `Dataset` resources, specific to the data type:\n",
    "\n",
    "- `ImageDataset`: image data\n",
    "- `TabularDataset`: tabular (structured) data\n",
    "- `TextDataset`: text (natural language) data\n",
    "- `VideoDataset`: video data\n",
    "- `TimeSeriesDataset`: forecasting data\n",
    "\n",
    "A Vertex `Dataset` provides the following capabilities:\n",
    "\n",
    "- A unique internal identifier for automatic (programatic) processes.\n",
    "- A user specificed (display name) identifier for interactive processes.\n",
    "- Compatible with AutoML training.\n",
    "- Exporting dataset for custom training.\n",
    "- Dataset search capability.\n",
    "- Creation/update timestamps.\n",
    "- Statistics\n",
    "\n",
    "Learn more about [All dataset documentation](https://cloud.google.com/vertex-ai/docs/datasets/datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_file:flowers,csv,icn",
    "repo": "snippets_common.ipynb"
   },
   "outputs": [],
   "source": [
    "IMPORT_FILE = 'gs://cloud-samples-data/vision/automl_classification/flowers/all_data_v2.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_dataset:image,icn",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "### Create the Dataset\n",
    "\n",
    "Next, create the `Dataset` resource using the `create` method for the `ImageDataset` class, which takes the following parameters:\n",
    "\n",
    "- `display_name`: The human readable name for the `Dataset` resource.\n",
    "- `gcs_source`: A list of one or more dataset index files to import the data items into the `Dataset` resource.\n",
    "- `import_schema_uri`: The data labeling schema for the data items:\n",
    "  - `single_label`: Binary and multi-class classification\n",
    "  - `multi_label`: Multi-label multi-class classification\n",
    "  - `bounding_box`: Object detection\n",
    "  - `image_segmentation`: Segmentation\n",
    "\n",
    "Learn more about [ImageDataset](https://cloud.google.com/vertex-ai/docs/datasets/prepare-image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_dataset:image,icn",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [],
   "source": [
    "dataset = aip.ImageDataset.create(\n",
    "    display_name=\"example\" + \"_\" + TIMESTAMP,\n",
    "    gcs_source=[IMPORT_FILE],\n",
    "    import_schema_uri=aip.schema.dataset.ioformat.image.single_label_classification\n",
    ")\n",
    "\n",
    "print(dataset.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_file:hmdb,csv,vcn",
    "repo": "snippets_common.ipynb"
   },
   "outputs": [],
   "source": [
    "IMPORT_FILE = 'gs://automl-video-demo-data/hmdb_split1_5classes_train_inf.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_dataset:video,vcn",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "### Create the Dataset\n",
    "\n",
    "Next, create the `Dataset` resource using the `create` method for the `VideoDataset` class, which takes the following parameters:\n",
    "\n",
    "- `display_name`: The human readable name for the `Dataset` resource.\n",
    "- `gcs_source`: A list of one or more dataset index files to import the data items into the `Dataset` resource.\n",
    "- `import_schema_uri`: The data labeling schema for the data items.\n",
    "  - `classification`: Binary and multi-class classification\n",
    "  - `object_tracking`: Object tracking\n",
    "  - `action_recognition`: Action recognition\n",
    "\n",
    "Learn more about [VideoDataset](https://cloud.google.com/vertex-ai/docs/datasets/prepare-video)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_dataset:video,vcn",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [],
   "source": [
    "dataset = aip.VideoDataset.create(\n",
    "    display_name=\"example\" + \"_\" + TIMESTAMP,\n",
    "    gcs_source=[IMPORT_FILE],\n",
    "    import_schema_uri=aip.schema.dataset.ioformat.video.classification\n",
    ")\n",
    "\n",
    "print(dataset.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_file:happydb,csv,tcn",
    "repo": "snippets_common.ipynb"
   },
   "outputs": [],
   "source": [
    "IMPORT_FILE = 'gs://cloud-ml-data/NL-classification/happiness.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_dataset:text,tcn",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "### Create the Dataset\n",
    "\n",
    "Next, create the `Dataset` resource using the `create` method for the `TextDataset` class, which takes the following parameters:\n",
    "\n",
    "- `display_name`: The human readable name for the `Dataset` resource.\n",
    "- `gcs_source`: A list of one or more dataset index files to import the data items into the `Dataset` resource.\n",
    "- `import_schema_uri`: The data labeling schema for the data items.\n",
    "  - `single_label`: Binary and multi-class classification\n",
    "  - `multi_label`: Multi-label multi-class classification\n",
    "  - `sentiment`: Sentiment analysis\n",
    "  - `extraction`: Entity extraction\n",
    "\n",
    "Learn more about [TextDataset](https://cloud.google.com/vertex-ai/docs/datasets/prepare-text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_dataset:text,tcn",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [],
   "source": [
    "dataset = aip.TextDataset.create(\n",
    "    display_name=\"example\" + \"_\" + TIMESTAMP,\n",
    "    gcs_source=[IMPORT_FILE],\n",
    "    import_schema_uri=aip.schema.dataset.ioformat.text.single_label_classification\n",
    ")\n",
    "\n",
    "print(dataset.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_file:gsod,bq,lrg",
    "repo": "snippets_common.ipynb"
   },
   "outputs": [],
   "source": [
    "IMPORT_FILE = \"bq://bigquery-public-data.samples.gsod\"\n",
    "BQ_TABLE = 'bigquery-public-data.samples.gsod'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_dataset:tabular,bq,lrg,v2",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "### Create the Dataset\n",
    "\n",
    "#### CSV input data\n",
    "\n",
    "Next, create the `Dataset` resource using the `create` method for the `TabularDataset` class, which takes the following parameters:\n",
    "\n",
    "- `display_name`: The human readable name for the `Dataset` resource.\n",
    "- `gcs_source`: A list of one or more dataset index files to import the data items into the `Dataset` resource.\n",
    "\n",
    "Learn more about [TabularDataset from CSV files](https://cloud.google.com/vertex-ai/docs/datasets/create-dataset-api#aiplatform_create_dataset_tabular_gcs_sample-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_dataset:tabular,bq,lrg,v2",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [],
   "source": [
    "dataset = aip.TabularDataset.create(\n",
    "    display_name=\"example\" + \"_\" + TIMESTAMP,\n",
    "    bq_source=[IMPORT_FILE]\n",
    ")\n",
    "\n",
    "print(dataset.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_file:covid,csv,forecast",
    "repo": "snippets_common.ipynb"
   },
   "outputs": [],
   "source": [
    "IMPORT_FILE = 'gs://cloud-samples-data/ai-platform/covid/bigquery-public-covid-nyt-us-counties-train.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_dataset:tabular,forecast,v2",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "### Create the Dataset\n",
    "\n",
    "Next, create the `Dataset` resource using the `create` method for the `TimeSeriesDataset` class, which takes the following parameters:\n",
    "\n",
    "- `display_name`: The human readable name for the `Dataset` resource.\n",
    "- `gcs_source`: A list of one or more dataset index files to import the data items into the `Dataset` resource.\n",
    "- `bq_source`: Alternatively, import data items from a BigQuery table into the `Dataset` resource.\n",
    "\n",
    "Learn more about [TimeSeriesDataset](https://cloud.google.com/vertex-ai/docs/datasets/prepare-tabular)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_dataset:tabular,forecast,v2",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [],
   "source": [
    "dataset = aip.TimeSeriesDataset.create(\n",
    "    display_name=\"example\" + \"_\" + TIMESTAMP,\n",
    "    gcs_source=[IMPORT_FILE],\n",
    ")\n",
    "\n",
    "print(dataset.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset_intro:methods",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "## Vertex Dataset properties and methods\n",
    "\n",
    "The following are the `Dataset` methods:\n",
    "\n",
    "- `list()`: List instances of a `Dataset` resource.\n",
    "- `import_data()`: Import additional data into a `Dataset` resource.\n",
    "- `export_data()`: Export dataset index file for custom training.\n",
    "- `delete()`: Delete the dataset.\n",
    "- `update()`: Not implemented yet.\n",
    "\n",
    "Get more information on each method, by executing in Python: help(method_name)\n",
    "\n",
    "The following are the `Dataset` properties:\n",
    "\n",
    "- `name`: The internal unique identifier.\n",
    "- `resource_name`: The fully qualified internal unique identifier.\n",
    "- `display_name`: The human assigned identifier.\n",
    "- `create_time`: The timestamp when the dataset was created.\n",
    "- `update_time`: The timestamp when the dataset was last updated.\n",
    "- `metadata_schema_uri`: The data labeling schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "list_dataset:tabular",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "### List datasets\n",
    "\n",
    "The `list()` method returns all datasets, as a list, of the corresponding data type -- e.g., Tabular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "list_dataset:tabular",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [],
   "source": [
    "datasets = aip.TabularDataset.list()\n",
    "for dataset in datasets:\n",
    "    print(dataset.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "list_dataset:tabular,filter",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "### List datasets matching a filter\n",
    "\n",
    "The `list()` method supports returning only datasets that match a `filter`. For example, all datasets where the `display_name` matches the specified display name:\n",
    "```\n",
    "    list(filter='display_name=my_display_name')\n",
    "```\n",
    "\n",
    "When the search matches multiple datasets, one can sort the list based on the dataset properties. For eample, sort by creation time, where the first dataset is the latest:\n",
    "\n",
    "```\n",
    "    list(filter='display_name=my_display_name,order_by=create_time')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "list_dataset:tabular,filter",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [],
   "source": [
    "datasets = aip.TabularDataset.list(filter=f'display_name=\"example_{TIMESTAMP}\"',order_by='create_time')\n",
    "latest_dataset = datasets[0]\n",
    "print(latest_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfdv_intro",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## TensorFlow Data Validation\n",
    "\n",
    "The TensorFlow Data Validation (TFDV) package is used in conjunction with Vertex and BigQuery datasets for:\n",
    "\n",
    "- Generating dataset statistics.\n",
    "- Generating data schema for data validation.\n",
    "- Detecting anomalies in new data using the data schema.\n",
    "- Generating feature specifications for data conversion to TFRecords.\n",
    "\n",
    "Learn more about [TensorFlow Data Validation: Get Started](https://www.tensorflow.org/tfx/data_validation/get_started)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq_to_dataframe:gsod",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Read the BigQuery dataset into a pandas dataframe\n",
    "\n",
    "Next, you read a sample of the dataset into a pandas dataframe using BigQuery `list_rows()` and `to_dataframe()` method, as follows:\n",
    "\n",
    "- `list_rows()`: Performs a query on the specified table and returns a row iterator to the query results. Optionally specify:\n",
    " - `selected_fields`: Subset of fields (columns) to return.\n",
    " - `max_results`: The maximum number of rows to return. Same as SQL LIMIT command.\n",
    "\n",
    "\n",
    "- `rows.to_dataframe()`: Invokes the row iterator and reads in the data into a pandas dataframe.\n",
    "\n",
    "Learn more about [Loading BigQuery table into a dataframe](https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bq_to_dataframe:gsod",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "# Download a table.\n",
    "table = bigquery.TableReference.from_string(\n",
    "    \"bigquery-public-data.samples.gsod\"\n",
    ")\n",
    "\n",
    "rows = bqclient.list_rows(\n",
    "    table,\n",
    "    max_results=500,\n",
    "    selected_fields=[\n",
    "        bigquery.SchemaField(\"station_number\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"year\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"month\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"day\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n",
    "    ]\n",
    "\n",
    ")\n",
    "\n",
    "dataframe = rows.to_dataframe()\n",
    "print(dataframe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfdv_stats:dataframe",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "###  Generate dataset statistics\n",
    "\n",
    "#### Dataframe input data\n",
    "\n",
    "Generate statistics on the dataset with the TensorFlow Data Validation (TFDV) package. Use the `generate_statistics_from_dataframe()` method, with the following parameters:\n",
    "\n",
    "- `dataframe`: The dataset in an in-memory pandas dataframe.\n",
    "- `stats_options`: The selected statistics options:\n",
    "  - `label_feature`: The column which is the label to predict.\n",
    "  - `sample_rate`: The sampling rate. If specified, statistics is computed over the sample.\n",
    "  - `num_top_values`: number of most frequent feature values to keep for string features.\n",
    "\n",
    "Learn about [TensorFlow Data Validation (TFDV)](https://www.tensorflow.org/tfx/data_validation/get_started)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfdv_stats:dataframe",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "stats = tfdv.generate_statistics_from_dataframe(\n",
    "    dataframe=dataframe,\n",
    "    stats_options=tfdv.StatsOptions(\n",
    "        label_feature='mean_temp',\n",
    "        sample_rate=1,\n",
    "        num_top_values=50\n",
    "    )\n",
    ")\n",
    "\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfdv_visualize_stats",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Visualize dataset statistics\n",
    "\n",
    "A visualization of the dataset statistics can be displayed using the TFDV `visualize_statistics()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfdv_visualize_stats",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "tfdv.visualize_statistics(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfdv_schema",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "###  Generate the raw data schema\n",
    "\n",
    "Generate the data schema on the dataset with the TensorFlow Data Validation (TFDV) package. Use the `infer_schema()` method, with the following parameters:\n",
    "\n",
    "- `statistics`: The statistics generated by TFDV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfdv_schema",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "schema = tfdv.infer_schema(statistics=stats)\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfdv_anomalies:gsod",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Detect Anomalizes in additional data\n",
    "\n",
    "When additional data is available for a dataset, you can check for anomalies between the new and previous data using the TFDV `validate_statistics` method.\n",
    "\n",
    "The accompanying code example mimics new data by getting the next slice of data (2nd 500 rows) and including an additional field into a dataframe and generating statistics for the new data.\n",
    "\n",
    "The `validate_statistics()` method is called with the following parameters:\n",
    "\n",
    "- `statistics`: The statistics for the new data.\n",
    "- `schema`: The data schema for the previous data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfdv_anomalies:gsod",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "# Download a table.\n",
    "table = bigquery.TableReference.from_string(\n",
    "    \"bigquery-public-data.samples.gsod\"\n",
    ")\n",
    "\n",
    "rows = bqclient.list_rows(\n",
    "    table,\n",
    "    max_results=500,\n",
    "    start_index=500,\n",
    "    selected_fields=[\n",
    "        bigquery.SchemaField(\"station_number\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"year\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"month\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"day\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"num_mean_temp_samples\", \"INTEGER\"),\n",
    "    ]\n",
    "\n",
    ")\n",
    "\n",
    "dataframe = rows.to_dataframe()\n",
    "\n",
    "new_stats = tfdv.generate_statistics_from_dataframe(\n",
    "    dataframe=dataframe,\n",
    "    stats_options=tfdv.StatsOptions(\n",
    "        label_feature='mean_temp',\n",
    "        sample_rate=1,\n",
    "        num_top_values=50\n",
    "    )\n",
    ")\n",
    "\n",
    "anomalies = tfdv.validate_statistics(statistics=new_stats, schema=schema)\n",
    "print(anomalies.anomaly_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tft_feature_spec",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Generate the feature specification\n",
    "\n",
    "Generate the feature specification, compatible with TFRecords, on the dataset with the TensorFlow Transform (TFT) package. Use the `schema_as_feature_spec()` method, with the following parameters:\n",
    "\n",
    "- `schema`: The data schema generated by TFDV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tft_feature_spec",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "feature_spec = tft.tf_metadata.schema_utils.schema_as_feature_spec(\n",
    "        schema\n",
    "    ).feature_spec\n",
    "\n",
    "print(feature_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "text_dataset_to_pandas",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Export `Text Dataset` to pandas dataframe.\n",
    "\n",
    "The property `gca_resource.metadata['inputConfig']['gcsSource']['uri']` contains the list of the one or more imported CSV files.\n",
    "\n",
    "To create a dataframe from multiple CSV sources, you read each CSV file and concatenate the dataframes together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "text_dataset_to_pandas",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "all_files = dataset.gca_resource.metadata['inputConfig']['gcsSource']['uri']\n",
    "df = pd.concat((pd.read_csv(f) for f in all_files))\n",
    "\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "export_dataset:mbsdk",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "### Export dataset index\n",
    "\n",
    "Next, you export the dataset index to a JSONL file which will then be used by your custom training job to get the data and corresponding labels for training your model. The JSONL index file is generated by calling the export_data() method, with the following parameters:\n",
    " - `output_dir`: The Cloud Storage bucket to write the JSONL dataset index file to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "list_dataset:image,filter",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [],
   "source": [
    "datasets = aip.ImageDataset.list(filter=f'display_name=\"example_{TIMESTAMP}\"',order_by='create_time')\n",
    "latest_dataset = datasets[0]\n",
    "print(latest_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "latest_dataset",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [],
   "source": [
    "dataset = latest_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export_dataset:mbsdk",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [],
   "source": [
    "EXPORTED_DIR = f'{BUCKET_NAME}/exported'\n",
    "exported_files = dataset.export_data(output_dir=EXPORTED_DIR)\n",
    "\n",
    "! gsutil ls $EXPORTED_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "export_dataset_quick_peak:mbsdk,icn",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "#### Quick peak at your exported dataset index file\n",
    "\n",
    "Take a quick peak at the contents of the exported dataset index file. When the `export_data()` completed, the method returned a list of the paths to the exported dataset index files.\n",
    "\n",
    "You get the path to the exported dataset index file (`exported_files[0]`) and then display the first ten JSON objects in the file -- i.e., data items.\n",
    "\n",
    "The JSONL format for each data item is:\n",
    "\n",
    "    { \"imageGcsUri\": path_to_the_image, \"classificationAnnotation\": { \"displayName\": label } }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export_dataset_quick_peak:mbsdk,icn",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [],
   "source": [
    "jsonl_index = exported_files[0]\n",
    "\n",
    "! gsutil cat $jsonl_index | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "export_dataset_read_file:icn",
    "repo": "snippets_common.ipynb"
   },
   "source": [
    "#### Reading the index file\n",
    "\n",
    "You will need to add code to your custom training Python script to read the exported dataset index, so that you can generate training batches for custom training your model.\n",
    "\n",
    "Below is an example of how you might each the exported dataset index file:\n",
    "\n",
    "1. Use Tensorflow's Cloud Storage file methods to open the file (`tf.io.gfile.GFile()`) and read all the lines (`f.readlines()`), where each line is a data item represented as a JSONL object.\n",
    "\n",
    "2. For each line in the file, convert the line to a JSON object (`json.loads()`).\n",
    "\n",
    "3. Extract the path to the image (`['imageGcsUri']`) and label (`['classificationAnnotation']['displayName']`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export_dataset_read_file:icn",
    "repo": "snippets_common.ipynb"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "with tf.io.gfile.GFile(jsonl_index, 'r') as f:\n",
    "    export_data_items = f.readlines()\n",
    "\n",
    "for _ in range(10):\n",
    "    j = json.loads(export_data_items[_])\n",
    "    print(j['imageGcsUri'], j['classificationAnnotation']['displayName'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_tfrecord:image",
    "repo": "snippets_custom.ipynb"
   },
   "source": [
    "#### Create TFRecords\n",
    "\n",
    "Next, we needs to create a feeder mechanism to feed data to the model you will train from the dataset index file. There are lots of choices for how to construct a feeder. We will cover a two options here, both using TFRecords:\n",
    "\n",
    "    1. Storing the image data as raw uncompressed image data (1 byte per pixel).\n",
    "    2. Storing the image data as preprocessed data -- machine learning ready --  (4 bytes per pixel).\n",
    "\n",
    "These two methods demonstrate a trade-off between disk storage and compute time. In both cases, we do a prepass over the image data to cache the data into a form that will accelerate the training time for the model. But, by caching we are both using disk space and increasing I/O traffic from the disk to the compute device -- e.g., CPU, GPU, TPU.\n",
    "\n",
    "In the raw uncompressed format, you are minimizing the size on disk and I/O traffic for the cache data, but have the overhead that on each epoch, the preprocessing of the image data has to be repeated. In the preprocessed format, you are minimizing the compute time by preprocessing once and caching the preprocessed data -- i.e., machine learning ready. The amount of data on disk will be four times the size when training as Float32, and you are increasing by the same amount disk space and I/O traffic from the disk to the compute engine.\n",
    "\n",
    "The helper functions `TFExampleImageUncompressed` and `TFExampleImagePreprocessed` both take the parameters:\n",
    "\n",
    "- `path`: The Cloud Storage path to the image file.\n",
    "- `label`: The corresponding label for the image file.\n",
    "- `shape`: The (H,W) input shape to resize the image. If `None`, no resizing occurs.\n",
    "\n",
    "The helper function `TFExampleImagePreprocessed` has an additional parameter:\n",
    "\n",
    "- `dtype`: The floating point representation after the pixel data has been normalized. By default, it is set to 32-bit float (np.float32). If you are using NVIDIA GPUs or TPUs you can alternatively train in 16-bit float, by setting `dtype = np.float16`. There are two benefits to training with 16-bit float, when it does not effect the accuracy or number of epochs:\n",
    "\n",
    "    1. Each matrix multiply operation is 4 times faster than the 32-bit equivalent -- albeit the model weights need to be stored as 16-bit as well.\n",
    "    2. The disk space and I/O bandwidth is reduced by 1/2.\n",
    "\n",
    "Let's look at bit deeper into the functions for creating `TFRecord` training data. First, `TFRecord` is a serialized binary encoding of the training data. As an encoding, one needs to specify a schema for how the fields are encoded, which is then used later to decode during when feeding training data to your model.\n",
    "\n",
    "The schema is defined as an instance of `tf.train.Example` per data item in the training data. Each instance of `tf.train.Example` consists of a sequence fields, each defined as a key/value pair. In our helper function, the key entries are:\n",
    "\n",
    "- `image`: The encoded raw image data.\n",
    "- `label`: The label assigned to the image.\n",
    "- `shape`: The shape of the image when decoded.\n",
    "\n",
    "The value for each key/value pair is an instance of `tf.train.Feature`, where:\n",
    "\n",
    "- `bytes_list`: the data to encode is a byte string.\n",
    "- `int64_list`: the data to encode is an array of one or more integer values.\n",
    "- `float_list`: the data to encode is an array of one or more floating point values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_tfrecord:image",
    "repo": "snippets_custom.ipynb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def TFExampleImageUncompressed(path, label, shape=None):\n",
    "        ''' The uncompressed version of the image '''\n",
    "\n",
    "        # read in (and uncompress) the image\n",
    "        with tf.io.gfile.GFile(path, 'rb') as f:\n",
    "            data = f.read()\n",
    "        image = tf.io.decode_image(data)\n",
    "\n",
    "        if shape:\n",
    "            image = tf.image.resize(image, shape)\n",
    "        image = image.numpy()\n",
    "        shape = image.shape\n",
    "\n",
    "        # make the record\n",
    "        return tf.train.Example(features = tf.train.Features(feature = {\n",
    "        'image': tf.train.Feature(bytes_list = tf.train.BytesList(value =\n",
    "                                  [image.tostring()])),\n",
    "        'label': tf.train.Feature(int64_list = tf.train.Int64List(value =\n",
    "                                  [label])),\n",
    "        'shape': tf.train.Feature(int64_list = tf.train.Int64List(value =\n",
    "                                  [shape[0], shape[1], shape[2]]))\n",
    "        }))\n",
    "\n",
    "def TFExampleImagePreprocessed(path, label, shape=None, dtype=np.float32):\n",
    "        ''' The normalized version of the image '''\n",
    "\n",
    "        # read in (uncompress) the image and normalize the pixel data\n",
    "        image = (cv2.imread(path) / 255.0).astype(dtype)\n",
    "\n",
    "        if shape:\n",
    "            image = tf.image.resize(image, shape)\n",
    "        image = image.numpy()\n",
    "        shape = image.shape\n",
    "\n",
    "        # make the record\n",
    "        return tf.train.Example(features = tf.train.Features(feature = {\n",
    "        'image': tf.train.Feature(bytes_list = tf.train.BytesList(value =\n",
    "                                  [image.tostring()])),\n",
    "        'label': tf.train.Feature(int64_list = tf.train.Int64List(value =\n",
    "                                  [label])),\n",
    "        'shape': tf.train.Feature(int64_list = tf.train.Int64List(value =\n",
    "                                  [shape[0], shape[1], shape[2]]))\n",
    "        }))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "write_tfrecord:image",
    "repo": "snippets_custom.ipynb"
   },
   "source": [
    "#### Write training data to TFRecord file\n",
    "\n",
    "Next, you will create a single TFRecord for all the training data specified in the exported dataset index:\n",
    "\n",
    "- Specify the cache method by setting the variable `CACHE` to either `TFExampleImageUncompressed` or `TFExampleImagePreprocessed`.\n",
    "- Convert class names from the dataset to integer labels, using `cls2label`.\n",
    "- Read in the data item list from the exported dataset index file -- `tf.io.gfile.GFile(jsonl_index, 'r')`.\n",
    "- Set the Cloud Storage location to store the cached TFRecord file -- `GCS_TFRECORD_URI`.\n",
    "- Generate the cached data using `tf.io.TFRecordWriter(gcs_tfrecord_uri)` for each data item in the exported dataset index.\n",
    " - Extract the Cloud Storage path and class name - `json.loads(data_item)`\n",
    " - Convert class name to integer label - `label = cls2label[cls]`\n",
    " - Encode the data item - `example = CACHE(image, label)`\n",
    " - Write the encoded data item to the TFRecord file - `writer.write(example.SerializeToString())`\n",
    "\n",
    "This may take about 20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "write_tfrecord:image",
    "repo": "snippets_custom.ipynb"
   },
   "outputs": [],
   "source": [
    "# Select TFRecord method of encoding\n",
    "CACHE = TFExampleImageUncompressed  # [ TFExampleImageUncompressed, TFExampleImagePreprocessed]\n",
    "\n",
    "# Map labels to class names\n",
    "cls2label = {\n",
    "    'daisy': 0,\n",
    "    'dandelion': 1,\n",
    "    'roses': 2,\n",
    "    'sunflowers': 3,\n",
    "    'tulips': 4\n",
    "}\n",
    "\n",
    "# Read in each example from exported dataset index\n",
    "with tf.io.gfile.GFile(jsonl_index, 'r') as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "# The path to the TFRecord cached file.\n",
    "GCS_TFRECORD_URI = BUCKET_NAME + '/flowers.tfrecord'\n",
    "\n",
    "# Create the TFRecord cached file\n",
    "with tf.io.TFRecordWriter(GCS_TFRECORD_URI) as writer:\n",
    "    n=0\n",
    "    for data_item in data:\n",
    "        j = json.loads(data_item)\n",
    "        image = j['imageGcsUri']\n",
    "        cls = j['classificationAnnotation']['displayName']\n",
    "        label = cls2label[cls]\n",
    "        example = CACHE(image, label, shape=(128, 128))\n",
    "        writer.write(example.SerializeToString())\n",
    "        n += 1\n",
    "        if n % 10 == 0:\n",
    "            print(n, image)\n",
    "\n",
    "listing = ! gsutil ls -la $GCS_TFRECORD_URI\n",
    "print(\"TFRecord File\", listing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:mbsdk",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "# Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n",
    "- Dataset\n",
    "- Pipeline\n",
    "- Model\n",
    "- Endpoint\n",
    "- AutoML Training Job\n",
    "- Batch Job\n",
    "- Custom Job\n",
    "- Hyperparameter Tuning Job\n",
    "- Cloud Storage Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup:mbsdk",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [],
   "source": [
    "delete_all = True\n",
    "\n",
    "if delete_all:\n",
    "    # Delete the dataset using the Vertex dataset object\n",
    "    try:\n",
    "        if 'dataset' in globals():\n",
    "            dataset.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the model using the Vertex model object\n",
    "    try:\n",
    "        if 'model' in globals():\n",
    "            model.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the endpoint using the Vertex endpoint object\n",
    "    try:\n",
    "        if 'endpoint' in globals():\n",
    "            endpoint.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the AutoML or Pipeline trainig job\n",
    "    try:\n",
    "        if 'dag' in globals():\n",
    "            dag.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the custom trainig job\n",
    "    try:\n",
    "        if 'job' in globals():\n",
    "            job.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the batch prediction job using the Vertex batch prediction object\n",
    "    try:\n",
    "        if 'batch_predict_job' in globals():\n",
    "            batch_predict_job.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the hyperparameter tuning job using the Vertex hyperparameter tuning object\n",
    "    try:\n",
    "        if 'hpt_job' in globals():\n",
    "            hpt_job.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    if 'BUCKET_NAME' in globals():\n",
    "        ! gsutil rm -r $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m71"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vars": {
   "AIP": "AI Platform",
   "AUTOML": "AutoML",
   "AUTOML_URL": "https://cloud.google.com/vertex-ai/docs/start/automl-users",
   "BQ": "BigQuery",
   "COLAB": "Colab",
   "DATASET_ALIAS": "example",
   "DATASET_NAME": "example",
   "DOCKER": "Docker",
   "EDGE": "Edge",
   "EMAIL": "cdpe@gmail.com",
   "GAPIC": "client library",
   "GAPICP": "SDK",
   "GCONSOLE": "Cloud Console",
   "GCP": "Google Cloud",
   "GCS": "Cloud Storage",
   "GNOTEBOOK": "Google Cloud Notebook",
   "LABEL_COLUMN": "mean_temp",
   "NOTEBOOK": "ml_ops_stage1/get_started_vertex_datasets.ipynb",
   "REGION": "us-central1",
   "REPO": "vertex-ai-samples/tree/master/notebooks/official/automl",
   "SDKP": "SDK for Python",
   "STAGE": "1 : data management: get started with Vertex datasets",
   "TENSORFLOW": "TensorFlow",
   "TFLite": "TFLite",
   "TFServing": "TF Serving",
   "TITLE": "E2E ML on GCP: MLOps stage 1 : data management: get started with Vertex datasets",
   "VERTEX": "Vertex",
   "VERTEX_AI": "Vertex AI",
   "YEAR": "2021",
   "null": "null",
   "uCAIP": "Vertex"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "copyright"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title:generic,gcp"
      },
      "source": [
        "# E2E ML on GCP: MLOps stage 1 : data management: get started with Vertex AI datasets\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage1/get_started_vertex_datasets.ipynb\">\n",
        "     <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage1/get_started_vertex_datasets.ipynb\">\n",
        "<img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "   <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/ml_ops/stage1/get_started_vertex_datasets.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>   \n",
        "</table>\n",
        "<br/><br/><br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview:mlops"
      },
      "source": [
        "## Overview\n",
        "\n",
        "\n",
        "This tutorial demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. This tutorial covers stage 1 : data management: get started with Vertex datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "objective:mlops,stage1,get_started_vertex_datasets"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to use `Vertex AI Dataset` for training with `Vertex AI`.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services:\n",
        "\n",
        "- `Vertex AI Datasets`\n",
        "- `BigQuery Datasets`\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Create a Vertex AI `Dataset` resource for:\n",
        "    - image data\n",
        "    - text data\n",
        "    - video data\n",
        "    - tabular data\n",
        "    - forecasting data\n",
        "\n",
        "\n",
        "- Search `Dataset` resources using a filter.\n",
        "- Read a sample of a `BigQuery` dataset into a dataframe.\n",
        "- Generate statistics and data schema using TensorFlow Data Validation from the samples in the dataframe.\n",
        "- Detect anomalies in new data using TensorFlow Data Validation.\n",
        "- Generate a TFRecord feature specification using TensorFlow Transform from the data schema.\n",
        "- Export a dataset and convert to TFRecords."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "recommendation:mlops,stage1,vertex,datasets"
      },
      "source": [
        "### Recommendations\n",
        "\n",
        "When doing E2E MLOps on Google Cloud, the following best practices with Vertex AI Datasets:\n",
        "\n",
        "- Use CSV index file format for image data\n",
        "- Use CSV index file format for text data:\n",
        "    - For short text strings, embed the text string in the CSV file.\n",
        "    - For long text strings, place text in referenced text file.\n",
        "\n",
        "\n",
        "- Use JSON index file format for video data\n",
        "- For tabular data:\n",
        "    - For small datasets use CSV index file format.\n",
        "    - For large datasets use BigQuery table.\n",
        "\n",
        "\n",
        "- Use `filter` and `order_by` parameters in the `list()` methods to find the latest versions of datasets.\n",
        "\n",
        "- When custom training with a `Vertex AI Dataset`:\n",
        "    - tabular data :\n",
        "        - Use the CSV index file or BigQuery table reference.\n",
        "        - Create a tf.data.Dataset generator from the CSV index file/BigQuery table.\n",
        "    - image/video data:\n",
        "        - Export the data to a JSONL index file.\n",
        "        - Using the index file, convert the images/videos and labels to TFRecords.\n",
        "        - Create a tf.data.Dataset generator from the TFRercords.\n",
        "    - text data:\n",
        "        - If text strings are embedded:\n",
        "            - Convert to CSV file.\n",
        "            - Create a tf.data.Dataset generator from the CSV index file.\n",
        "        - If text strings are in text files:\n",
        "            - Using the JSON index file, convert the text files and labels to TFRecords.\n",
        "            - Create a tf.data.Dataset from the TFRecords."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "533dd6fe83c8"
      },
      "source": [
        "### Datasets\n",
        "\n",
        "This tutorial uses a variety of public datasets to demonstrate using a `Vertex AI` managed dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e483012a752"
      },
      "source": [
        "### Costs\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "- Vertex AI\n",
        "- Cloud Storage\n",
        "- BigQuery\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing) and [BigQuery pricing](https://cloud.google.com/bigquery/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_mlops"
      },
      "source": [
        "## Installations\n",
        "\n",
        "Install the packages required for executing this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_mlops"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Vertex AI Workbench Notebook product has specific requirements\n",
        "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
        "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
        "    \"/opt/deeplearning/metadata/env_version\"\n",
        ")\n",
        "\n",
        "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_WORKBENCH_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\"\n",
        "\n",
        "! pip3 install -U tensorflow $USER_FLAG -q\n",
        "! pip3 install -U tensorflow-data-validation $USER_FLAG -q\n",
        "! pip3 install -U tensorflow-transform $USER_FLAG -q\n",
        "! pip3 install -U tensorflow-io $USER_FLAG -q\n",
        "! pip3 install --upgrade google-cloud-aiplatform $USER_FLAG -q\n",
        "! pip3 install --upgrade google-cloud-bigquery $USER_FLAG -q\n",
        "! pip3 install -U tensorflow-io==0.18 $USER_FLAG -q\n",
        "! pip3 install --upgrade db-dtypes $USER_FLAG -q! pip3 install --upgrade future $USER_FLAG -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "restart"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "restart"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb082379ed5b"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "1. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). \n",
        "\n",
        "1. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
        "\n",
        "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "project_id"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_project_id"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWlzLu5ELxWd"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID:\", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c021ca495967"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
        "\n",
        "- Americas: `us-central1`\n",
        "- Europe: `europe-west4`\n",
        "- Asia Pacific: `asia-east1`\n",
        "\n",
        "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
        "\n",
        "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "region"
      },
      "outputs": [],
      "source": [
        "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
        "\n",
        "if REGION == \"[your-region]\":\n",
        "    REGION = \"us-central1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "timestamp"
      },
      "source": [
        "#### Timestamp\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "timestamp"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "927085b84a07"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using Vertex AI Workbench Notebooks**, your environment is already authenticated. Skip this step.\n",
        "\n",
        "**If you are using Colab**, run the cell below and follow the instructions when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "In the Cloud Console, go to the [Create service account key](https://console.cloud.google.com/apis/credentials/serviceaccountkey) page.\n",
        "\n",
        "**Click Create service account**.\n",
        "\n",
        "In the **Service account name** field, enter a name, and click **Create**.\n",
        "\n",
        "In the **Grant this service account access to project** section, click the Role drop-down list. Type \"Vertex\" into the filter box, and select **Vertex Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "\n",
        "Click Create. A JSON file that contains your key downloads to your local environment.\n",
        "\n",
        "Enter the path to your service account key as the GOOGLE_APPLICATION_CREDENTIALS variable in the cell below and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89788a802687"
      },
      "outputs": [],
      "source": [
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# If on Vertex AI Workbench, then don't execute this code\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
        "    \"DL_ANACONDA_HOME\"\n",
        "):\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bucket:custom"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "When you submit a custom training job using the Vertex SDK, you upload a Python package\n",
        "containing your training code to a Cloud Storage bucket. Vertex AI runs\n",
        "the code from this package. In this tutorial, Vertex AI also saves the\n",
        "trained model that results from your job in the same bucket. You can then\n",
        "create an `Endpoint` resource based on this output in order to serve\n",
        "online predictions.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bucket"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_bucket"
      },
      "outputs": [],
      "source": [
        "if BUCKET_URI == \"\" or BUCKET_URI is None or BUCKET_URI == \"gs://[your-bucket-name]\":\n",
        "    BUCKET_URI = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_bucket"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "validate_bucket"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "validate_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_vars"
      },
      "source": [
        "### Set up variables\n",
        "\n",
        "Next, set up some variables used throughout the tutorial.\n",
        "### Import libraries and define constants\n",
        "\n",
        "Import the BigQuery package, TensorFlow Data Validation (TFDV) package and TensorFlow Data Validation package into your Python environment. \n",
        "\n",
        "Import TensorFlow Transform (TFT) package and pandas into your Python environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "import google.cloud.aiplatform as aip\n",
        "import pandas as pd\n",
        "import tensorflow_data_validation as tfdv\n",
        "import tensorflow_transform as tft\n",
        "from google.cloud import bigquery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk,region"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_aip:mbsdk,region"
      },
      "outputs": [],
      "source": [
        "aip.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_bq"
      },
      "source": [
        "### Create BigQuery client\n",
        "\n",
        "Create the BigQuery client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_bq"
      },
      "outputs": [],
      "source": [
        "bqclient = bigquery.Client()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset_intro"
      },
      "source": [
        "## Vertex AI Datasets\n",
        "\n",
        "Vertex AI `Datasets` are the means for managing your datasets within Vertex AI services. Vertex AI Datasets are also referred to as `Dataset` resources. There are four types of `Dataset` resources, specific to the data type:\n",
        "\n",
        "- `ImageDataset`: image data\n",
        "- `TabularDataset`: tabular (structured) data\n",
        "- `TextDataset`: text (natural language) data\n",
        "- `VideoDataset`: video data\n",
        "- `TimeSeriesDataset`: forecasting data\n",
        "\n",
        "A Vertex AI `Dataset` provides the following capabilities:\n",
        "\n",
        "- A unique internal identifier for automatic (programatic) processes.\n",
        "- A user specificed (display name) identifier for interactive processes.\n",
        "- Compatible with AutoML training.\n",
        "- Exporting dataset for custom training.\n",
        "- Dataset search capability.\n",
        "- Creation/update timestamps.\n",
        "- Statistics\n",
        "\n",
        "Learn more about [All dataset documentation](https://cloud.google.com/vertex-ai/docs/datasets/datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_dataset:image,icn"
      },
      "source": [
        "### Create an Image Dataset\n",
        "\n",
        "Next, create the `Dataset` resource using the `create` method for the `ImageDataset` class, which takes the following parameters:\n",
        "\n",
        "- `display_name`: The human readable name for the `Dataset` resource.\n",
        "- `gcs_source`: A list of one or more dataset index files to import the data items into the `Dataset` resource.\n",
        "- `import_schema_uri`: The data labeling schema for the data items:\n",
        "  - `single_label`: Binary and multi-class classification\n",
        "  - `multi_label`: Multi-label multi-class classification\n",
        "  - `bounding_box`: Object detection\n",
        "  - `image_segmentation`: Segmentation\n",
        "\n",
        "Learn more about [ImageDataset](https://cloud.google.com/vertex-ai/docs/datasets/prepare-image)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_file:flowers,csv,icn"
      },
      "outputs": [],
      "source": [
        "IMPORT_FILE = (\n",
        "    \"gs://cloud-samples-data/vision/automl_classification/flowers/all_data_v2.csv\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_dataset:image,icn"
      },
      "outputs": [],
      "source": [
        "dataset = aip.ImageDataset.create(\n",
        "    display_name=\"example\" + \"_\" + TIMESTAMP,\n",
        "    gcs_source=[IMPORT_FILE],\n",
        "    import_schema_uri=aip.schema.dataset.ioformat.image.single_label_classification,\n",
        ")\n",
        "\n",
        "print(dataset.resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_dataset:video,vcn"
      },
      "source": [
        "### Create a Video Dataset\n",
        "\n",
        "Next, create the `Dataset` resource using the `create` method for the `VideoDataset` class, which takes the following parameters:\n",
        "\n",
        "- `display_name`: The human readable name for the `Dataset` resource.\n",
        "- `gcs_source`: A list of one or more dataset index files to import the data items into the `Dataset` resource.\n",
        "- `import_schema_uri`: The data labeling schema for the data items.\n",
        "  - `classification`: Binary and multi-class classification\n",
        "  - `object_tracking`: Object tracking\n",
        "  - `action_recognition`: Action recognition\n",
        "\n",
        "Learn more about [VideoDataset](https://cloud.google.com/vertex-ai/docs/datasets/prepare-video)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_file:hmdb,csv,vcn"
      },
      "outputs": [],
      "source": [
        "IMPORT_FILE = \"gs://automl-video-demo-data/hmdb_split1_5classes_train_inf.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_dataset:video,vcn"
      },
      "outputs": [],
      "source": [
        "dataset = aip.VideoDataset.create(\n",
        "    display_name=\"example\" + \"_\" + TIMESTAMP,\n",
        "    gcs_source=[IMPORT_FILE],\n",
        "    import_schema_uri=aip.schema.dataset.ioformat.video.classification,\n",
        ")\n",
        "\n",
        "print(dataset.resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_dataset:text,tcn"
      },
      "source": [
        "### Create a Text Dataset\n",
        "\n",
        "Next, create the `Dataset` resource using the `create` method for the `TextDataset` class, which takes the following parameters:\n",
        "\n",
        "- `display_name`: The human readable name for the `Dataset` resource.\n",
        "- `gcs_source`: A list of one or more dataset index files to import the data items into the `Dataset` resource.\n",
        "- `import_schema_uri`: The data labeling schema for the data items.\n",
        "  - `single_label`: Binary and multi-class classification\n",
        "  - `multi_label`: Multi-label multi-class classification\n",
        "  - `sentiment`: Sentiment analysis\n",
        "  - `extraction`: Entity extraction\n",
        "\n",
        "Learn more about [TextDataset](https://cloud.google.com/vertex-ai/docs/datasets/prepare-text)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_file:happydb,csv,tcn"
      },
      "outputs": [],
      "source": [
        "IMPORT_FILE = \"gs://cloud-ml-data/NL-classification/happiness.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_dataset:text,tcn"
      },
      "outputs": [],
      "source": [
        "dataset = aip.TextDataset.create(\n",
        "    display_name=\"example\" + \"_\" + TIMESTAMP,\n",
        "    gcs_source=[IMPORT_FILE],\n",
        "    import_schema_uri=aip.schema.dataset.ioformat.text.single_label_classification,\n",
        ")\n",
        "\n",
        "print(dataset.resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_dataset:tabular,bq,lrg,v2"
      },
      "source": [
        "### Create a Tabular Dataset\n",
        "\n",
        "#### CSV input data\n",
        "\n",
        "Next, create the `Dataset` resource using the `create` method for the `TabularDataset` class for CSV input data, which takes the following parameters:\n",
        "\n",
        "- `display_name`: The human readable name for the `Dataset` resource.\n",
        "- `gcs_source`: A list of one or more dataset index files to import the data items into the `Dataset` resource.\n",
        "\n",
        "Learn more about [TabularDataset from CSV files](https://cloud.google.com/vertex-ai/docs/datasets/create-dataset-api#aiplatform_create_dataset_tabular_gcs_sample-python)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_file:gsod,bq,lrg"
      },
      "outputs": [],
      "source": [
        "IMPORT_FILE = \"gs://cloud-samples-data/tables/iris_1000.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_dataset:tabular,bq,lrg,v2"
      },
      "outputs": [],
      "source": [
        "dataset = aip.TabularDataset.create(\n",
        "    display_name=\"example\" + \"_\" + TIMESTAMP, gcs_source=[IMPORT_FILE]\n",
        ")\n",
        "\n",
        "print(dataset.resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "854dd1e0195c"
      },
      "source": [
        "#### BigQuery input data\n",
        "\n",
        "Next, create the `Dataset` resource using the `create` method for the `TabularDataset` class for BigQuery table input, which takes the following parameters:\n",
        "\n",
        "- `display_name`: The human readable name for the `Dataset` resource.\n",
        "- `bq_source`: A list of one or more BigQuery tables to import the data items into the `Dataset` resource.\n",
        "\n",
        "Learn more about [TabularDataset from BigQuery table](https://cloud.google.com/vertex-ai/docs/datasets/create-dataset-api#aiplatform_create_dataset_tabular_bigquery_sample-pythonn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86343c146300"
      },
      "outputs": [],
      "source": [
        "IMPORT_FILE = \"bq://bigquery-public-data.samples.gsod\"\n",
        "BQ_TABLE = \"bigquery-public-data.samples.gsod\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_dataset:tabular,bq,lrg,v2"
      },
      "outputs": [],
      "source": [
        "dataset = aip.TabularDataset.create(\n",
        "    display_name=\"example\" + \"_\" + TIMESTAMP, bq_source=[IMPORT_FILE]\n",
        ")\n",
        "\n",
        "print(dataset.resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82e9fe20ce71"
      },
      "source": [
        "#### Dataframe input data\n",
        "\n",
        "Next, create the `Dataset` resource using the `create_from_dataframe` method for the `TabularDataset` class for pandas dataframe input, which takes the following parameters:\n",
        "\n",
        "- `display_name`: The human readable name for the `Dataset` resource.\n",
        "- `df_source`: The pandas dataframe to import the data items into the `Dataset` resource.\n",
        "- `staging_path`: The BigQuery table to store the imported data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3805f945ffdd"
      },
      "outputs": [],
      "source": [
        "# Download the table.\n",
        "table = bigquery.TableReference.from_string(BQ_TABLE)\n",
        "\n",
        "rows = bqclient.list_rows(\n",
        "    table,\n",
        "    max_results=10000,\n",
        "    selected_fields=[\n",
        "        bigquery.SchemaField(\"station_number\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"year\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"month\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"day\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n",
        "    ],\n",
        ")\n",
        "\n",
        "dataframe = rows.to_dataframe()\n",
        "print(dataframe.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_dataset:tabular,bq,lrg,v2"
      },
      "outputs": [],
      "source": [
        "dataset = aip.TabularDataset.create_from_dataframe(\n",
        "    display_name=\"example\" + \"_\" + TIMESTAMP,\n",
        "    df_source=dataframe,\n",
        "    staging_path=f\"bq://{PROJECT_ID}.samples.gsod\",\n",
        ")\n",
        "\n",
        "print(dataset.resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_dataset:tabular,forecast,v2"
      },
      "source": [
        "### Create a Time Series Dataset\n",
        "\n",
        "Next, create the `Dataset` resource using the `create` method for the `TimeSeriesDataset` class, which takes the following parameters:\n",
        "\n",
        "- `display_name`: The human readable name for the `Dataset` resource.\n",
        "- `gcs_source`: A list of one or more dataset index files to import the data items into the `Dataset` resource.\n",
        "- `bq_source`: Alternatively, import data items from a BigQuery table into the `Dataset` resource.\n",
        "\n",
        "Learn more about [TimeSeriesDataset](https://cloud.google.com/vertex-ai/docs/datasets/prepare-tabular)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_file:covid,csv,forecast"
      },
      "outputs": [],
      "source": [
        "IMPORT_FILE = \"gs://cloud-samples-data/ai-platform/covid/bigquery-public-covid-nyt-us-counties-train.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_dataset:tabular,forecast,v2"
      },
      "outputs": [],
      "source": [
        "dataset = aip.TimeSeriesDataset.create(\n",
        "    display_name=\"example\" + \"_\" + TIMESTAMP,\n",
        "    gcs_source=[IMPORT_FILE],\n",
        ")\n",
        "\n",
        "print(dataset.resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset_intro:methods"
      },
      "source": [
        "## Vertex AI Dataset properties and methods\n",
        "\n",
        "The following are the `Dataset` methods:\n",
        "\n",
        "- `list()`: List instances of a `Dataset` resource.\n",
        "- `import_data()`: Import additional data into a `Dataset` resource.\n",
        "- `export_data()`: Export dataset index file for custom training.\n",
        "- `delete()`: Delete the dataset.\n",
        "- `update()`: Not implemented yet.\n",
        "\n",
        "Get more information on each method, by executing in Python: help(method_name)\n",
        "\n",
        "The following are the `Dataset` properties:\n",
        "\n",
        "- `name`: The internal unique identifier.\n",
        "- `resource_name`: The fully qualified internal unique identifier.\n",
        "- `display_name`: The human assigned identifier.\n",
        "- `create_time`: The timestamp when the dataset was created.\n",
        "- `update_time`: The timestamp when the dataset was last updated.\n",
        "- `metadata_schema_uri`: The data labeling schema."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "list_dataset:tabular"
      },
      "source": [
        "### List datasets\n",
        "\n",
        "The `list()` method returns all datasets, as a list, of the corresponding data type -- e.g., Tabular."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "list_dataset:tabular"
      },
      "outputs": [],
      "source": [
        "datasets = aip.TabularDataset.list()\n",
        "for _dataset in datasets:\n",
        "    print(_dataset.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "list_dataset:tabular,filter"
      },
      "source": [
        "### List datasets matching a filter\n",
        "\n",
        "The `list()` method supports returning only datasets that match a `filter`. For example, all datasets where the `display_name` matches the specified display name:\n",
        "```\n",
        "    list(filter='display_name=my_display_name')\n",
        "```\n",
        "\n",
        "When the search matches multiple datasets, one can sort the list based on the dataset properties. For eample, sort by creation time, where the first dataset is the latest:\n",
        "\n",
        "```\n",
        "    list(filter='display_name=my_display_name,order_by=create_time')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "list_dataset:tabular,filter"
      },
      "outputs": [],
      "source": [
        "datasets = aip.TabularDataset.list(\n",
        "    filter=f'display_name=\"example_{TIMESTAMP}\"', order_by=\"create_time\"\n",
        ")\n",
        "latest_dataset = datasets[0]\n",
        "print(latest_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfdv_intro"
      },
      "source": [
        "## TensorFlow Data Validation\n",
        "\n",
        "The TensorFlow Data Validation (TFDV) package is used in conjunction with Vertex AI and BigQuery datasets for:\n",
        "\n",
        "- Generating dataset statistics.\n",
        "- Generating data schema for data validation.\n",
        "- Detecting anomalies in new data using the data schema.\n",
        "- Generating feature specifications for data conversion to TFRecords.\n",
        "\n",
        "Learn more about [TensorFlow Data Validation: Get Started](https://www.tensorflow.org/tfx/data_validation/get_started)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq_to_dataframe:gsod"
      },
      "source": [
        "### Read the BigQuery dataset into a pandas dataframe\n",
        "\n",
        "Next, you read a sample of the dataset into a pandas dataframe using BigQuery `list_rows()` and `to_dataframe()` method, as follows:\n",
        "\n",
        "- `list_rows()`: Performs a query on the specified table and returns a row iterator to the query results. Optionally specify:\n",
        " - `selected_fields`: Subset of fields (columns) to return.\n",
        " - `max_results`: The maximum number of rows to return. Same as SQL LIMIT command.\n",
        "\n",
        "\n",
        "- `rows.to_dataframe()`: Invokes the row iterator and reads in the data into a pandas dataframe.\n",
        "\n",
        "Learn more about [Loading BigQuery table into a dataframe](https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bq_to_dataframe:gsod"
      },
      "outputs": [],
      "source": [
        "# Download a table.\n",
        "table = bigquery.TableReference.from_string(\"bigquery-public-data.samples.gsod\")\n",
        "\n",
        "rows = bqclient.list_rows(\n",
        "    table,\n",
        "    max_results=500,\n",
        "    selected_fields=[\n",
        "        bigquery.SchemaField(\"station_number\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"year\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"month\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"day\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n",
        "    ],\n",
        ")\n",
        "\n",
        "dataframe = rows.to_dataframe()\n",
        "print(dataframe.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfdv_stats:dataframe"
      },
      "source": [
        "###  Generate dataset statistics\n",
        "\n",
        "#### Dataframe input data\n",
        "\n",
        "Generate statistics on the dataset with the TensorFlow Data Validation (TFDV) package. Use the `generate_statistics_from_dataframe()` method, with the following parameters:\n",
        "\n",
        "- `dataframe`: The dataset in an in-memory pandas dataframe.\n",
        "- `stats_options`: The selected statistics options:\n",
        "  - `label_feature`: The column which is the label to predict.\n",
        "  - `sample_rate`: The sampling rate. If specified, statistics is computed over the sample.\n",
        "  - `num_top_values`: number of most frequent feature values to keep for string features.\n",
        "\n",
        "Learn about [TensorFlow Data Validation (TFDV)](https://www.tensorflow.org/tfx/data_validation/get_started)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfdv_stats:dataframe"
      },
      "outputs": [],
      "source": [
        "stats = tfdv.generate_statistics_from_dataframe(\n",
        "    dataframe=dataframe,\n",
        "    stats_options=tfdv.StatsOptions(\n",
        "        label_feature=\"mean_temp\", sample_rate=1, num_top_values=50\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfdv_visualize_stats"
      },
      "source": [
        "### Visualize dataset statistics\n",
        "\n",
        "A visualization of the dataset statistics can be displayed using the TFDV `visualize_statistics()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfdv_visualize_stats"
      },
      "outputs": [],
      "source": [
        "tfdv.visualize_statistics(stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfdv_schema"
      },
      "source": [
        "###  Generate the raw data schema\n",
        "\n",
        "Generate the data schema on the dataset with the TensorFlow Data Validation (TFDV) package. Use the `infer_schema()` method, with the following parameters:\n",
        "\n",
        "- `statistics`: The statistics generated by TFDV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfdv_schema"
      },
      "outputs": [],
      "source": [
        "schema = tfdv.infer_schema(statistics=stats)\n",
        "print(schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfdv_anomalies:gsod"
      },
      "source": [
        "### Detect Anomalizes in additional data\n",
        "\n",
        "When additional data is available for a dataset, you can check for anomalies between the new and previous data using the TFDV `validate_statistics` method.\n",
        "\n",
        "The accompanying code example mimics new data by getting the next slice of data (2nd 500 rows) and including an additional field into a dataframe and generating statistics for the new data.\n",
        "\n",
        "The `validate_statistics()` method is called with the following parameters:\n",
        "\n",
        "- `statistics`: The statistics for the new data.\n",
        "- `schema`: The data schema for the previous data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfdv_anomalies:gsod"
      },
      "outputs": [],
      "source": [
        "# Download a table.\n",
        "table = bigquery.TableReference.from_string(\"bigquery-public-data.samples.gsod\")\n",
        "\n",
        "rows = bqclient.list_rows(\n",
        "    table,\n",
        "    max_results=500,\n",
        "    start_index=500,\n",
        "    selected_fields=[\n",
        "        bigquery.SchemaField(\"station_number\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"year\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"month\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"day\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"num_mean_temp_samples\", \"INTEGER\"),\n",
        "    ],\n",
        ")\n",
        "\n",
        "dataframe = rows.to_dataframe()\n",
        "\n",
        "new_stats = tfdv.generate_statistics_from_dataframe(\n",
        "    dataframe=dataframe,\n",
        "    stats_options=tfdv.StatsOptions(\n",
        "        label_feature=\"mean_temp\", sample_rate=1, num_top_values=50\n",
        "    ),\n",
        ")\n",
        "\n",
        "anomalies = tfdv.validate_statistics(statistics=new_stats, schema=schema)\n",
        "print(anomalies.anomaly_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tft_feature_spec"
      },
      "source": [
        "### Generate the feature specification\n",
        "\n",
        "Generate the feature specification, compatible with TFRecords, on the dataset with the TensorFlow Transform (TFT) package. Use the `schema_as_feature_spec()` method, with the following parameters:\n",
        "\n",
        "- `schema`: The data schema generated by TFDV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tft_feature_spec"
      },
      "outputs": [],
      "source": [
        "feature_spec = tft.tf_metadata.schema_utils.schema_as_feature_spec(schema).feature_spec\n",
        "\n",
        "print(feature_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq_extract"
      },
      "source": [
        "### Copy the dataset to Cloud Storage\n",
        "\n",
        "Next, you make a copy of the BigQuery dataset, as a CSV file, to Cloud Storage using the BigQuery extract command.\n",
        "\n",
        "Learn more about [BigQuery command line interface](https://cloud.google.com/bigquery/docs/reference/bq-cli-reference)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bq_extract"
      },
      "outputs": [],
      "source": [
        "comps = BQ_TABLE.split(\".\")\n",
        "BQ_PROJECT_DATASET_TABLE = comps[0] + \":\" + comps[1] + \".\" + comps[2]\n",
        "\n",
        "! bq --location=us extract --destination_format CSV $BQ_PROJECT_DATASET_TABLE $BUCKET_URI/mydata*.csv\n",
        "\n",
        "IMPORT_FILES = ! gsutil ls $BUCKET_URI/mydata*.csv\n",
        "\n",
        "print(IMPORT_FILES)\n",
        "\n",
        "EXAMPLE_FILE = IMPORT_FILES[0]\n",
        "\n",
        "! gsutil cat $EXAMPLE_FILE | head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfdv_stats:csv"
      },
      "source": [
        "###  Generate dataset statistics\n",
        "\n",
        "#### CSV input data\n",
        "\n",
        "Generate statistics on the dataset with the TensorFlow Data Validation (TFDV) package. Use the `generate_statistics_from_csv()` method, with the following parameters:\n",
        "\n",
        "- `data_location`: The dataset Cloud Storage file location.\n",
        "- `stats_options`: The selected statistics options:\n",
        "  - `label_feature`: The column which is the label to predict.\n",
        "  - `num_top_values`: number of most frequent feature values to keep for string features.\n",
        "\n",
        "Learn about [TensorFlow Data Validation (TFDV)](https://www.tensorflow.org/tfx/data_validation/get_started)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfdv_stats:csv"
      },
      "outputs": [],
      "source": [
        "stats = tfdv.generate_statistics_from_csv(\n",
        "    data_location=EXAMPLE_FILE,\n",
        "    stats_options=tfdv.StatsOptions(label_feature=\"mean_temp\", num_top_values=50),\n",
        ")\n",
        "\n",
        "print(stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "text_dataset_to_pandas"
      },
      "source": [
        "### Export `Text Dataset` to pandas dataframe.\n",
        "\n",
        "The property `gca_resource.metadata['inputConfig']['gcsSource']['uri']` contains the list of the one or more imported CSV files.\n",
        "\n",
        "To create a dataframe from multiple CSV sources, you read each CSV file and concatenate the dataframes together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcd2e4e0703b"
      },
      "source": [
        "If you are running this notebook on Colab, run the following cell to install packages fsspec and gcsfs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "927bd3f92268"
      },
      "outputs": [],
      "source": [
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# If on Workbench AI Notebook, then don't execute this code\n",
        "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        ! pip3 install fsspec\n",
        "        ! pip3 install gcsfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "text_dataset_to_pandas"
      },
      "outputs": [],
      "source": [
        "all_files = dataset.gca_resource.metadata[\"inputConfig\"][\"gcsSource\"][\"uri\"]\n",
        "df = pd.concat(pd.read_csv(f) for f in all_files)\n",
        "\n",
        "print(df.head)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "export_dataset:mbsdk"
      },
      "source": [
        "### Export dataset index\n",
        "\n",
        "Next, you export the dataset index to a JSONL file which will then be used by your custom training job to get the data and corresponding labels for training your model. The JSONL index file is generated by calling the export_data() method, with the following parameters:\n",
        " - `output_dir`: The Cloud Storage bucket to write the JSONL dataset index file to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "list_dataset:image,filter"
      },
      "outputs": [],
      "source": [
        "datasets = aip.ImageDataset.list(\n",
        "    filter=f'display_name=\"example_{TIMESTAMP}\"', order_by=\"create_time\"\n",
        ")\n",
        "latest_dataset = datasets[0]\n",
        "print(latest_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "latest_dataset"
      },
      "outputs": [],
      "source": [
        "dataset = latest_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "export_dataset:mbsdk"
      },
      "outputs": [],
      "source": [
        "EXPORTED_DIR = f\"{BUCKET_URI}/exported\"\n",
        "exported_files = dataset.export_data(output_dir=EXPORTED_DIR)\n",
        "\n",
        "! gsutil ls $EXPORTED_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "export_dataset_quick_peak:mbsdk,icn"
      },
      "source": [
        "#### Quick peak at your exported dataset index file\n",
        "\n",
        "Take a quick peak at the contents of the exported dataset index file. When the `export_data()` completed, the method returned a list of the paths to the exported dataset index files.\n",
        "\n",
        "You get the path to the exported dataset index file (`exported_files[0]`) and then display the first ten JSON objects in the file -- i.e., data items.\n",
        "\n",
        "The JSONL format for each data item is:\n",
        "\n",
        "    { \"imageGcsUri\": path_to_the_image, \"classificationAnnotation\": { \"displayName\": label } }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "export_dataset_quick_peak:mbsdk,icn"
      },
      "outputs": [],
      "source": [
        "jsonl_index = exported_files[0]\n",
        "\n",
        "! gsutil cat $jsonl_index | head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "export_dataset_read_file:icn"
      },
      "source": [
        "#### Reading the index file\n",
        "\n",
        "You will need to add code to your custom training Python script to read the exported dataset index, so that you can generate training batches for custom training your model.\n",
        "\n",
        "Below is an example of how you might each the exported dataset index file:\n",
        "\n",
        "1. Use Tensorflow's Cloud Storage file methods to open the file (`tf.io.gfile.GFile()`) and read all the lines (`f.readlines()`), where each line is a data item represented as a JSONL object.\n",
        "\n",
        "2. For each line in the file, convert the line to a JSON object (`json.loads()`).\n",
        "\n",
        "3. Extract the path to the image (`['imageGcsUri']`) and label (`['classificationAnnotation']['displayName']`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "export_dataset_read_file:icn"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "with tf.io.gfile.GFile(jsonl_index, \"r\") as f:\n",
        "    export_data_items = f.readlines()\n",
        "\n",
        "for _ in range(10):\n",
        "    j = json.loads(export_data_items[_])\n",
        "    print(j[\"imageGcsUri\"], j[\"classificationAnnotation\"][\"displayName\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_tfrecord:image"
      },
      "source": [
        "#### Create TFRecords\n",
        "\n",
        "Next, we needs to create a feeder mechanism to feed data to the model you will train from the dataset index file. There are lots of choices for how to construct a feeder. We will cover a two options here, both using TFRecords:\n",
        "\n",
        "    1. Storing the image data as raw uncompressed image data (1 byte per pixel).\n",
        "    2. Storing the image data as preprocessed data -- machine learning ready --  (4 bytes per pixel).\n",
        "\n",
        "These two methods demonstrate a trade-off between disk storage and compute time. In both cases, we do a prepass over the image data to cache the data into a form that will accelerate the training time for the model. But, by caching we are both using disk space and increasing I/O traffic from the disk to the compute device -- e.g., CPU, GPU, TPU.\n",
        "\n",
        "In the raw uncompressed format, you are minimizing the size on disk and I/O traffic for the cache data, but have the overhead that on each epoch, the preprocessing of the image data has to be repeated. In the preprocessed format, you are minimizing the compute time by preprocessing once and caching the preprocessed data -- i.e., machine learning ready. The amount of data on disk will be four times the size when training as Float32, and you are increasing by the same amount disk space and I/O traffic from the disk to the compute engine.\n",
        "\n",
        "The helper functions `TFExampleImageUncompressed` and `TFExampleImagePreprocessed` both take the parameters:\n",
        "\n",
        "- `path`: The Cloud Storage path to the image file.\n",
        "- `label`: The corresponding label for the image file.\n",
        "- `shape`: The (H,W) input shape to resize the image. If `None`, no resizing occurs.\n",
        "\n",
        "The helper function `TFExampleImagePreprocessed` has an additional parameter:\n",
        "\n",
        "- `dtype`: The floating point representation after the pixel data has been normalized. By default, it is set to 32-bit float (np.float32). If you are using NVIDIA GPUs or TPUs you can alternatively train in 16-bit float, by setting `dtype = np.float16`. There are two benefits to training with 16-bit float, when it does not effect the accuracy or number of epochs:\n",
        "\n",
        "    1. Each matrix multiply operation is 4 times faster than the 32-bit equivalent -- albeit the model weights need to be stored as 16-bit as well.\n",
        "    2. The disk space and I/O bandwidth is reduced by 1/2.\n",
        "\n",
        "Let's look at bit deeper into the functions for creating `TFRecord` training data. First, `TFRecord` is a serialized binary encoding of the training data. As an encoding, one needs to specify a schema for how the fields are encoded, which is then used later to decode during when feeding training data to your model.\n",
        "\n",
        "The schema is defined as an instance of `tf.train.Example` per data item in the training data. Each instance of `tf.train.Example` consists of a sequence fields, each defined as a key/value pair. In our helper function, the key entries are:\n",
        "\n",
        "- `image`: The encoded raw image data.\n",
        "- `label`: The label assigned to the image.\n",
        "- `shape`: The shape of the image when decoded.\n",
        "\n",
        "The value for each key/value pair is an instance of `tf.train.Feature`, where:\n",
        "\n",
        "- `bytes_list`: the data to encode is a byte string.\n",
        "- `int64_list`: the data to encode is an array of one or more integer values.\n",
        "- `float_list`: the data to encode is an array of one or more floating point values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_tfrecord:image"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def TFExampleImageUncompressed(path, label, shape=None):\n",
        "    \"\"\"The uncompressed version of the image\"\"\"\n",
        "\n",
        "    # read in (and uncompress) the image\n",
        "    with tf.io.gfile.GFile(path, \"rb\") as f:\n",
        "        data = f.read()\n",
        "    image = tf.io.decode_image(data)\n",
        "\n",
        "    if shape:\n",
        "        image = tf.image.resize(image, shape)\n",
        "    image = image.numpy()\n",
        "    shape = image.shape\n",
        "\n",
        "    # make the record\n",
        "    return tf.train.Example(\n",
        "        features=tf.train.Features(\n",
        "            feature={\n",
        "                \"image\": tf.train.Feature(\n",
        "                    bytes_list=tf.train.BytesList(value=[image.tostring()])\n",
        "                ),\n",
        "                \"label\": tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n",
        "                \"shape\": tf.train.Feature(\n",
        "                    int64_list=tf.train.Int64List(value=[shape[0], shape[1], shape[2]])\n",
        "                ),\n",
        "            }\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "def TFExampleImagePreprocessed(path, label, shape=None, dtype=np.float32):\n",
        "    \"\"\"The normalized version of the image\"\"\"\n",
        "\n",
        "    # read in (uncompress) the image and normalize the pixel data\n",
        "    image = (cv2.imread(path) / 255.0).astype(dtype)\n",
        "\n",
        "    if shape:\n",
        "        image = tf.image.resize(image, shape)\n",
        "    image = image.numpy()\n",
        "    shape = image.shape\n",
        "\n",
        "    # make the record\n",
        "    return tf.train.Example(\n",
        "        features=tf.train.Features(\n",
        "            feature={\n",
        "                \"image\": tf.train.Feature(\n",
        "                    bytes_list=tf.train.BytesList(value=[image.tostring()])\n",
        "                ),\n",
        "                \"label\": tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n",
        "                \"shape\": tf.train.Feature(\n",
        "                    int64_list=tf.train.Int64List(value=[shape[0], shape[1], shape[2]])\n",
        "                ),\n",
        "            }\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "write_tfrecord:image"
      },
      "source": [
        "#### Write training data to TFRecord file\n",
        "\n",
        "Next, you will create a single TFRecord for all the training data specified in the exported dataset index:\n",
        "\n",
        "- Specify the cache method by setting the variable `CACHE` to either `TFExampleImageUncompressed` or `TFExampleImagePreprocessed`.\n",
        "- Convert class names from the dataset to integer labels, using `cls2label`.\n",
        "- Read in the data item list from the exported dataset index file -- `tf.io.gfile.GFile(jsonl_index, 'r')`.\n",
        "- Set the Cloud Storage location to store the cached TFRecord file -- `GCS_TFRECORD_URI`.\n",
        "- Generate the cached data using `tf.io.TFRecordWriter(gcs_tfrecord_uri)` for each data item in the exported dataset index.\n",
        " - Extract the Cloud Storage path and class name - `json.loads(data_item)`\n",
        " - Convert class name to integer label - `label = cls2label[cls]`\n",
        " - Encode the data item - `example = CACHE(image, label)`\n",
        " - Write the encoded data item to the TFRecord file - `writer.write(example.SerializeToString())`\n",
        "\n",
        "This may take about 20 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "write_tfrecord:image"
      },
      "outputs": [],
      "source": [
        "# Select TFRecord method of encoding\n",
        "CACHE = TFExampleImageUncompressed  # [ TFExampleImageUncompressed, TFExampleImagePreprocessed]\n",
        "\n",
        "# Map labels to class names\n",
        "cls2label = {\"daisy\": 0, \"dandelion\": 1, \"roses\": 2, \"sunflowers\": 3, \"tulips\": 4}\n",
        "\n",
        "# Read in each example from exported dataset index\n",
        "with tf.io.gfile.GFile(jsonl_index, \"r\") as f:\n",
        "    data = f.readlines()\n",
        "\n",
        "# The path to the TFRecord cached file.\n",
        "GCS_TFRECORD_URI = BUCKET_URI + \"/flowers.tfrecord\"\n",
        "\n",
        "# Create the TFRecord cached file\n",
        "with tf.io.TFRecordWriter(GCS_TFRECORD_URI) as writer:\n",
        "    n = 0\n",
        "    for data_item in data:\n",
        "        j = json.loads(data_item)\n",
        "        image = j[\"imageGcsUri\"]\n",
        "        cls = j[\"classificationAnnotation\"][\"displayName\"]\n",
        "        label = cls2label[cls]\n",
        "        example = CACHE(image, label, shape=(128, 128))\n",
        "        writer.write(example.SerializeToString())\n",
        "        n += 1\n",
        "        if n % 10 == 0:\n",
        "            print(n, image)\n",
        "\n",
        "listing = ! gsutil ls -la $GCS_TFRECORD_URI\n",
        "print(\"TFRecord File\", listing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cleanup:mbsdk"
      },
      "source": [
        "# Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:\n",
        "\n",
        "- Dataset\n",
        "- Bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup:mbsdk"
      },
      "outputs": [],
      "source": [
        "delete_bucket = False\n",
        "\n",
        "# Delete the dataset using the Vertex dataset object\n",
        "datasets = aip.TabularDataset.list(filter=f'display_name=\"example_{TIMESTAMP}\"')\n",
        "for dataset in datasets:\n",
        "    dataset.delete()\n",
        "\n",
        "# Delete the bucket\n",
        "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
        "    ! gsutil rm -r $BUCKET_URI"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "get_started_vertex_datasets.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

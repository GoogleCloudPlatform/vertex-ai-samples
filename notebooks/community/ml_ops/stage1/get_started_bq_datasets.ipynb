{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "# Copyright 2020 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title:generic,gcp",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "# E2E ML on GCP: MLOps stage 1 : data management: get started with BigQuery datasets\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/tree/master/notebooks/official/automl/ml_ops_stage1/get_started_bq_datasets.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/ai/platform/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/tree/master/notebooks/official/automl/ml_ops_stage1/get_started_bq_datasets.ipynb\">\n",
    "      Open in Google Cloud Notebooks\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:mlops",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Overview\n",
    "\n",
    "\n",
    "This tutorial demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. This tutorial covers stage 1 : data management: get started with BigQuery datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:gsod,lrg",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset used for this tutorial is the GSOD dataset from [BigQuery public datasets](https://cloud.google.com/bigquery/public-data). The version of the dataset you use only the fields year, month and day to predict the value of mean daily temperature (mean_temp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:mlops,stage1,get_started_bq",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you learn how to use `BigQuery` as a dataset for training with `Vertex AI`.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services:\n",
    "\n",
    "- `Vertex Datasets`\n",
    "- `BigQuery Datasets`\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Create a Vertex `Dataset` resource from `BigQuery` table -- compatible for `AutoML` training.\n",
    "- Extract a copy of the dataset from `BigQuery` to a CSV file in Cloud Storage -- compatible for `AutoML` or custom training.\n",
    "- Select rows from a `BigQuery` dataset into a `pandas` dataframe -- compatible for custom training.\n",
    "- Select rows from a `BigQuery` dataset into a `tf.data.Dataset` -- compatible for custom training `TensorFlow` models.\n",
    "- Select rows from extracted CSV files into a `tf.data.Dataset` -- compatible for custom training `TensorFlow` models.\n",
    "- Create a BigQuery dataset from CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "recommendation:mlops,stage1,tabular,bq",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Recommendations\n",
    "\n",
    "When doing E2E MLOps on Google Cloud, the following best practices with structured (tabular) data in BigQuery:\n",
    "\n",
    "- For AutoML training:\n",
    "  - Create a managed dataset with Vertex `TabularDataset`.\n",
    "  - Use the BigQuery table as the input to the dataset.\n",
    "  - Specify columns and columns transformations when running the AutoML training pipeline job.\n",
    "\n",
    "\n",
    "- For custom training:\n",
    "  - For small datasets:\n",
    "    - Extract the BigQuery to a pandas dataframe.\n",
    "    - Preprocess the data in the dataframe.\n",
    "  - For large datasets:\n",
    "    - Create a tf.data.dataset generator from the BigQuery table.\n",
    "    - Specify the columns for the custrom training.\n",
    "    - Preprocess the data either:\n",
    "      - Within the generator (upstream)\n",
    "      - Within the model (downstream)\n",
    "\n",
    "\n",
    "- Alternately:\n",
    "    - Extract the BigQuery table to CSV files.\n",
    "    - Preprocess the CSV files.\n",
    "    - Create a tf.data.dataset generator from the CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_aip:mbsdk",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "## Installation\n",
    "\n",
    "Install the latest version of Vertex SDK for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_aip:mbsdk",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Google Cloud Notebook\n",
    "if os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n",
    "    USER_FLAG = '--user'\n",
    "else:\n",
    "    USER_FLAG = ''\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_bq",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "Install the latest GA version of *BigQuery* library as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_bq",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "! pip3 install -U google-cloud-bigquery $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_tfio",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "Install the latest GA version of *TensorFlow IO* library as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_tfio",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "! pip3 install -U tensorflow-io pyarrow $USER_FLAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_tensorflow",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "if os.environ[\"IS_TESTING\"]:\n",
    "    ! pip3 install --upgrade tensorflow $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "restart",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_gcloud_project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
    "\n",
    "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "region",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "REGION = 'us-central1'  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "timestamp",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "timestamp",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:custom",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "When you submit a custom training job using the Vertex SDK, you upload a Python package\n",
    "containing your training code to a Cloud Storage bucket. Vertex AI runs\n",
    "the code from this package. In this tutorial, Vertex AI also saves the\n",
    "trained model that results from your job in the same bucket. You can then\n",
    "create an `Endpoint` resource based on this output in order to serve\n",
    "online predictions.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validate_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "validate_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Set up variables\n",
    "\n",
    "Next, set up some variables used throughout the tutorial.\n",
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_aip:mbsdk",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_bq",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Import BigQuery\n",
    "\n",
    "Import the BigQuery package into your Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_bq",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,region",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Initialize Vertex SDK for Python\n",
    "\n",
    "Initialize the Vertex SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk,region",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_bq",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Create BigQuery client\n",
    "\n",
    "Create the BigQuery client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_bq",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "bqclient = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_file:u_dataset,bq",
    "repo": "snippets_common.ipynb"
   },
   "source": [
    "#### Location of BigQuery training data.\n",
    "\n",
    "Now set the variable `IMPORT_FILE` to the location of the data table in BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_file:gsod,bq,lrg",
    "repo": "snippets_common.ipynb"
   },
   "outputs": [],
   "source": [
    "IMPORT_FILE = \"bq://bigquery-public-data.samples.gsod\"\n",
    "BQ_TABLE = 'bigquery-public-data.samples.gsod'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_dataset:tabular,bq,lrg",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Create the Dataset\n",
    "\n",
    "#### BigQuery input data\n",
    "\n",
    "Next, create the `Dataset` resource using the `create` method for the `TabularDataset` class, which takes the following parameters:\n",
    "\n",
    "- `display_name`: The human readable name for the `Dataset` resource.\n",
    "- `bq_source`: Import data items from a BigQuery table into the `Dataset` resource.\n",
    "\n",
    "Learn more about [TabularDataset from BigQuery table](https://cloud.google.com/vertex-ai/docs/datasets/create-dataset-api#aiplatform_create_dataset_tabular_bigquery_sample-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_dataset:tabular,bq,lrg",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "dataset = aip.TabularDataset.create(\n",
    "    display_name=\"NOAA historical weather data\" + \"_\" + TIMESTAMP,\n",
    "    bq_source=[IMPORT_FILE]\n",
    ")\n",
    "\n",
    "label_column=\"mean_temp\"\n",
    "\n",
    "print(dataset.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq_extract:gsod",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Copy the dataset to Cloud Storage\n",
    "\n",
    "Next, you make a copy of the BigQuery dataset, as a CSV file, to Cloud Storage using the BigQuery extract command.\n",
    "\n",
    "Learn more about [BigQuery command line interface](https://cloud.google.com/bigquery/docs/reference/bq-cli-reference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bq_extract:gsod",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "! bq --location=us extract --destination_format CSV bigquery-public-data:samples.gsod $BUCKET_NAME/mydata*.csv\n",
    "\n",
    "IMPORT_FILES = ! gsutil ls $BUCKET_NAME/mydata*.csv\n",
    "\n",
    "print(IMPORT_FILES)\n",
    "\n",
    "EXAMPLE_FILE = IMPORT_FILES[0]\n",
    "\n",
    "! gsutil cat $EXAMPLE_FILE | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_dataset:tabular,lrg",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Create the Dataset\n",
    "\n",
    "#### CSV input data\n",
    "\n",
    "Next, create the `Dataset` resource using the `create` method for the `TabularDataset` class, which takes the following parameters:\n",
    "\n",
    "- `display_name`: The human readable name for the `Dataset` resource.\n",
    "- `gcs_source`: A list of one or more dataset index files to import the data items into the `Dataset` resource.\n",
    "\n",
    "Learn more about [TabularDataset from CSV files](https://cloud.google.com/vertex-ai/docs/datasets/create-dataset-api#aiplatform_create_dataset_tabular_gcs_sample-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_dataset:tabular,lrg",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "if 'IMPORT_FILES' in globals():\n",
    "    gcs_source = IMPORT_FILES\n",
    "else:\n",
    "    gcs_source = [IMPORT_FILE]\n",
    "\n",
    "dataset = aip.TabularDataset.create(\n",
    "    display_name=\"NOAA historical weather data\" + \"_\" + TIMESTAMP,\n",
    "    gcs_source=gcs_source\n",
    ")\n",
    "\n",
    "\n",
    "label_column=\"mean_temp\"\n",
    "\n",
    "print(dataset.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq_view",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Create a view of the BigQuery dataset\n",
    "\n",
    "Alternatively, you can create a logical view of a BigQuery dataset that has a subset of the fields.\n",
    "\n",
    "Learn more about [Creating BigQuery views](https://cloud.google.com/bigquery/docs/views)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bq_view",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "BQ_MY_DATASET = 'mydataset'\n",
    "BQ_MY_TABLE = 'myview'\n",
    "! bq --location=US mk -d \\\n",
    "$PROJECT_ID:$BQ_MY_DATASET\n",
    "\n",
    "sql_script = f'''\n",
    "CREATE OR REPLACE VIEW `{PROJECT_ID}.{BQ_MY_DATASET}.{BQ_MY_TABLE}`\n",
    "AS SELECT station_number,year,month,day,mean_temp FROM `{BQ_TABLE}`\n",
    "'''\n",
    "print(sql_script)\n",
    "\n",
    "query = bqclient.query(sql_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq_to_dataframe:gsod",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Read the BigQuery dataset into a pandas dataframe\n",
    "\n",
    "Next, you read a sample of the dataset into a pandas dataframe using BigQuery `list_rows()` and `to_dataframe()` method, as follows:\n",
    "\n",
    "- `list_rows()`: Performs a query on the specified table and returns a row iterator to the query results. Optionally specify:\n",
    " - `selected_fields`: Subset of fields (columns) to return.\n",
    " - `max_results`: The maximum number of rows to return. Same as SQL LIMIT command.\n",
    "\n",
    "\n",
    "- `rows.to_dataframe()`: Invokes the row iterator and reads in the data into a pandas dataframe.\n",
    "\n",
    "Learn more about [Loading BigQuery table into a dataframe](https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bq_to_dataframe:gsod",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "# Download a table.\n",
    "table = bigquery.TableReference.from_string(\n",
    "    \"bigquery-public-data.samples.gsod\"\n",
    ")\n",
    "\n",
    "rows = bqclient.list_rows(\n",
    "    table,\n",
    "    max_results=500,\n",
    "    selected_fields=[\n",
    "        bigquery.SchemaField(\"station_number\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"year\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"month\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"day\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n",
    "    ]\n",
    "\n",
    ")\n",
    "\n",
    "dataframe = rows.to_dataframe(\n",
    "    # Optionally, explicitly request to use the BigQuery Storage API. As of\n",
    "    # google-cloud-bigquery version 1.26.0 and above, the BigQuery Storage\n",
    "    # API is used by default.\n",
    "    create_bqstorage_client=True,\n",
    ")\n",
    "print(dataframe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq_to_dataset:gsod",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Read the BigQuery dataset into a tf.data.Dataset\n",
    "\n",
    "Next, you read a sample of the dataset into a tf.data.Dataset using TensorFlow IO `BigQueryClient()` and `read_session()` method, with the following parameters:\n",
    "\n",
    "- `parent`: Your project ID.\n",
    "- `project_id`: The project ID of the BigQuery table.\n",
    "- `dataset_id`: The ID of the BigQuery dataset.\n",
    "- `table_id`. The ID of the table within the corresponding BigQuery dataset.\n",
    "- `selected_fields`: Subset of fields (columns) to return.\n",
    "- `output_types`: The output types of the corresponding fields.\n",
    "- `requested_streams`: The number of parallel readers.\n",
    "\n",
    "Learn more about [BigQuery TensorFlow reader](https://www.tensorflow.org/io/tutorials/bigquery).\n",
    "\n",
    "Learn more about [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bq_to_dataset:gsod",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "from tensorflow_io.bigquery import BigQueryReadSession\n",
    "from tensorflow.python.framework import dtypes\n",
    "\n",
    "feature_names = \"station_number,year,month,day\".split(',')\n",
    "\n",
    "target_name = \"mean_temp\"\n",
    "\n",
    "def read_bigquery(project, dataset, table):\n",
    "    tensorflow_io_bigquery_client = BigQueryClient()\n",
    "    read_session = tensorflow_io_bigquery_client.read_session(\n",
    "          parent=\"projects/\" + PROJECT_ID,\n",
    "          project_id=project,\n",
    "          dataset_id=dataset,\n",
    "          table_id=table,\n",
    "          selected_fields=feature_names + [target_name],\n",
    "          output_types=[dtypes.string] + [dtypes.int32] * 3 + [dtypes.float32],\n",
    "          requested_streams=2)\n",
    "\n",
    "    dataset = read_session.parallel_read_rows()\n",
    "    return dataset\n",
    "\n",
    "PROJECT, DATASET, TABLE = IMPORT_FILE.split('/')[-1].split('.')\n",
    "tf_dataset = read_bigquery(PROJECT, DATASET, TABLE)\n",
    "\n",
    "print(tf_dataset.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csv_to_dataset:gsod",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Read CSV files into a tf.data.Dataset\n",
    "\n",
    "Alternatively, when your data is in CSV files, you can load the dataset into a tf.data.Dataset using `tf.data.experimental.CsvDataset`, with the following parameters:\n",
    "\n",
    "- `filenames`: A list of one or more CSV files.\n",
    "- `header`: Whether CSV file(s) contain a header.\n",
    "- `select_cols`: Subset of fields (columns) to return.\n",
    "- `record_defaults`: The output types of the corresponding fields.\n",
    "\n",
    "Learn more about [tf.data CsvDataset](https://www.tensorflow.org/api_docs/python/tf/data/experimental/CsvDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "csv_to_dataset:gsod",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "feature_names = [\"station_number,year,month,day\".split(',')]\n",
    "\n",
    "target_name = \"mean_temp\"\n",
    "\n",
    "tf_dataset = tf.data.experimental.CsvDataset(\n",
    "    filenames=IMPORT_FILES,\n",
    "    header=True,\n",
    "    select_cols=feature_names.append(target_name),\n",
    "    record_defaults=[dtypes.string] + [dtypes.int32] * 3 + [dtypes.float32],\n",
    ")\n",
    "\n",
    "print(tf_dataset.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csv_to_bq",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Create a BigQuery dataset from CSV files\n",
    "\n",
    "You can create a BigQuery dataset from CSV files using the BigQuery `create_dataset()` and `load_table_from_uri()` methods, as follows:\n",
    "\n",
    "- `create_dataset()`: Creates an empty BigQuery dataset, with the following parameters:\n",
    " - `dataset_ref`: The `DatasetReference` created from the dataset_id -- e.g., samples.\n",
    "- `load_table_from_uri()`: Loads one or more CSV files into a table within the corresponding dataset, with the following parameters:\n",
    " - `url`: A set of one or more CVS files in Cloud Storage storage.\n",
    " - `table`: The `TableReference` for the table.\n",
    " - `job_config`: Specifications on how to load the CSV data.\n",
    "\n",
    "Learn more about [Importing CSV data into BigQuery](https://www.tensorflow.org/io/tutorials/bigquery#import_census_data_into_bigquery)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "csv_to_bq",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "LOCATION = \"us\"\n",
    "\n",
    "CSV_SCHEMA=[\n",
    "        bigquery.SchemaField(\"station_number\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"wban_number\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"year\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"month\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"day\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"num_mean_temp_samples\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"mean_dew_point\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"num_mean_dew_point_samples\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"mean_sealevel_pressure\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"num_mean_sealevel_pressure_samples\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"mean_station_pressure\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"num_mean_station_pressure_samples\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"mean_visibility\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"num_mean_visibility_samples\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"mean_wind_speed\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"num_mean_wind_speed_samples\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"max_sustained_wind_speed\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"max_gust_wind_speed\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"max_temperature\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"max_temperature_explicit\", \"BOOLEAN\"),\n",
    "        bigquery.SchemaField(\"min_temperature\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"min_temperature_explicit\", \"BOOLEAN\"),\n",
    "        bigquery.SchemaField(\"total_percipitation\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"snow_depth\", \"FLOAT\"),\n",
    "        bigquery.SchemaField(\"fog\", \"BOOLEAN\"),\n",
    "        bigquery.SchemaField(\"rain\", \"BOOLEAN\"),\n",
    "        bigquery.SchemaField(\"snow\", \"BOOLEAN\"),\n",
    "        bigquery.SchemaField(\"hail\", \"BOOLEAN\"),\n",
    "        bigquery.SchemaField(\"thunder\", \"BOOLEAN\"),\n",
    "        bigquery.SchemaField(\"tornado\", \"BOOLEAN\")\n",
    "]\n",
    "\n",
    "\n",
    "DATASET_ID = \"samples\"\n",
    "TABLE_ID = \"gsod\"\n",
    "\n",
    "def create_bigquery_dataset(dataset_id):\n",
    "    dataset = bigquery.Dataset(bigquery.dataset.DatasetReference(PROJECT_ID, dataset_id))\n",
    "    dataset.location = \"us\"\n",
    "\n",
    "    try:\n",
    "        dataset = bqclient.create_dataset(dataset)  # API request\n",
    "        return True\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        if err.code != 409: # http_client.CONFLICT\n",
    "              raise\n",
    "    return False\n",
    "\n",
    "def load_data_into_bigquery(url, dataset_id, table_id):\n",
    "    create_bigquery_dataset(dataset_id)\n",
    "    dataset = bqclient.dataset(dataset_id)\n",
    "    table = dataset.table(table_id)\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    "    job_config.source_format = bigquery.SourceFormat.CSV\n",
    "    job_config.schema = CSV_SCHEMA\n",
    "    job_config.skip_leading_rows = 1 # heading\n",
    "\n",
    "    load_job = bqclient.load_table_from_uri(\n",
    "      url, table, job_config=job_config\n",
    "    )\n",
    "    print(\"Starting job {}\".format(load_job.job_id))\n",
    "\n",
    "    load_job.result()  # Waits for table load to complete.\n",
    "    print(\"Job finished.\")\n",
    "\n",
    "    destination_table = bqclient.get_table(table_id)\n",
    "    print(\"Loaded {} rows.\".format(destination_table.num_rows))\n",
    "\n",
    "load_data_into_bigquery(IMPORT_FILES, DATASET_ID, TABLE_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:mbsdk",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "# Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n",
    "- Dataset\n",
    "- Pipeline\n",
    "- Model\n",
    "- Endpoint\n",
    "- AutoML Training Job\n",
    "- Batch Job\n",
    "- Custom Job\n",
    "- Hyperparameter Tuning Job\n",
    "- Cloud Storage Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup:mbsdk",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [],
   "source": [
    "delete_all = True\n",
    "\n",
    "if delete_all:\n",
    "    # Delete the dataset using the Vertex dataset object\n",
    "    try:\n",
    "        if 'dataset' in globals():\n",
    "            dataset.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the model using the Vertex model object\n",
    "    try:\n",
    "        if 'model' in globals():\n",
    "            model.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the endpoint using the Vertex endpoint object\n",
    "    try:\n",
    "        if 'endpoint' in globals():\n",
    "            endpoint.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the AutoML or Pipeline trainig job\n",
    "    try:\n",
    "        if 'dag' in globals():\n",
    "            dag.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the custom trainig job\n",
    "    try:\n",
    "        if 'job' in globals():\n",
    "            job.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the batch prediction job using the Vertex batch prediction object\n",
    "    try:\n",
    "        if 'batch_predict_job' in globals():\n",
    "            batch_predict_job.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the hyperparameter tuning job using the Vertex hyperparameter tuning object\n",
    "    try:\n",
    "        if 'hpt_job' in globals():\n",
    "            hpt_job.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    if 'BUCKET_NAME' in globals():\n",
    "        ! gsutil rm -r $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m71"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vars": {
   "AIP": "AI Platform",
   "AUTOML": "AutoML",
   "AUTOML_URL": "https://cloud.google.com/vertex-ai/docs/start/automl-users",
   "BQ": "BigQuery",
   "COLAB": "Colab",
   "DATASET_ALIAS": "gsod",
   "DATASET_NAME": "NOAA historical weather data",
   "DATA_TYPE": "tabular",
   "DOCKER": "Docker",
   "EDGE": "Edge",
   "EMAIL": "cdpe@gmail.com",
   "FEATURE_COLUMNS": "station_number,year,month,day",
   "GAPIC": "client library",
   "GAPICP": "SDK",
   "GCONSOLE": "Cloud Console",
   "GCP": "Google Cloud",
   "GCS": "Cloud Storage",
   "GNOTEBOOK": "Google Cloud Notebook",
   "IMPORT_FORMAT": "bq",
   "LABEL_COLUMN": "mean_temp",
   "MODEL_TYPE": "tabular regression",
   "NOTEBOOK": "ml_ops_stage1/get_started_bq_datasets.ipynb",
   "REGION": "us-central1",
   "REPO": "vertex-ai-samples/tree/master/notebooks/official/automl",
   "SDKP": "SDK for Python",
   "STAGE": "1 : data management: get started with BigQuery datasets",
   "TENSORFLOW": "TensorFlow",
   "TFLite": "TFLite",
   "TFServing": "TF Serving",
   "TIME_COLUMN": "date",
   "TIME_SERIES_ID_COLUMN": "county",
   "TITLE": "E2E ML on GCP: MLOps stage 1 : data management: get started with BigQuery datasets",
   "VERTEX": "Vertex",
   "VERTEX_AI": "Vertex AI",
   "YEAR": "2020",
   "null": "null",
   "uCAIP": "Vertex"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

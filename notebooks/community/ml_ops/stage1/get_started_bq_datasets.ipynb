{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "copyright"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title:generic,gcp"
      },
      "source": [
        "# E2E ML on GCP: MLOps stage 1 : data management: get started with BigQuery datasets\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage1/get_started_bq_datasets.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "    <td>\n",
        "        <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage1/get_started_bq_datasets.ipynb\">\n",
        "        <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\\\" alt=\"Colab logo\"> Run in Colab\n",
        "        </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage1/get_started_bq_datasets.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "<br/><br/><br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview:mlops"
      },
      "source": [
        "## Overview\n",
        "\n",
        "\n",
        "This tutorial demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. This tutorial covers stage 1 : data management: get started with BigQuery datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset:gsod,lrg"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset used for this tutorial is the GSOD dataset from [BigQuery public datasets](https://cloud.google.com/bigquery/public-data). In this version of the dataset you consider the fields year, month and day to predict the value of mean daily temperature (mean_temp)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "objective:mlops,stage1,get_started_bq"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to use `BigQuery` as a dataset for training with `Vertex AI`.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services:\n",
        "\n",
        "- `Vertex AI Datasets`\n",
        "- `BigQuery Datasets`\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Create a Vertex AI `Dataset` resource from `BigQuery` table -- compatible for `AutoML` training.\n",
        "- Extract a copy of the dataset from `BigQuery` to a CSV file in Cloud Storage -- compatible for `AutoML` or custom training.\n",
        "- Select rows from a `BigQuery` dataset into a `pandas` dataframe -- compatible for custom training.\n",
        "- Select rows from a `BigQuery` dataset into a `tf.data.Dataset` -- compatible for custom training `TensorFlow` models.\n",
        "- Select rows from extracted CSV files into a `tf.data.Dataset` -- compatible for custom training `TensorFlow` models.\n",
        "- Create a `BigQuery` dataset from CSV files.\n",
        "- Extract data from `BigQuery` table into a `DMatrix` -- compatible for custom training `XGBoost` models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "recommendation:mlops,stage1,tabular,bq"
      },
      "source": [
        "### Recommendations\n",
        "\n",
        "When doing E2E MLOps on Google Cloud, following are the best practices when dealing with structured (tabular) data in BigQuery:\n",
        "\n",
        "- For AutoML training:\n",
        "  - Create a managed dataset with Vertex AI `TabularDataset`.\n",
        "  - Use the BigQuery table as the input to the dataset.\n",
        "  - Specify columns and columns transformations when running the AutoML training pipeline job.\n",
        "\n",
        "\n",
        "- For custom training:\n",
        "  - For small datasets:\n",
        "    - Extract the BigQuery to a pandas dataframe.\n",
        "    - Preprocess the data in the dataframe.\n",
        "  - For large datasets:\n",
        "    - TensorFlow model training:\n",
        "      - Create a tf.data.Dataset generator from the BigQuery table.\n",
        "      - Specify the columns for the custrom training.\n",
        "      - Preprocess the data either:\n",
        "        - Within the generator (upstream)\n",
        "        - Within the model (downstream)\n",
        "    - XGBoost model training:\n",
        "      - Use BigQuery ML built-in XGBoost training.\n",
        "      - Alternatively, create a DMatrix generator from CSV files extracted from BigQuery table.\n",
        "    - Pytorch model training:\n",
        "        - Extract the BigQuery to a pandas dataframe.\n",
        "        - Preprocess the data in the dataframe.\n",
        "        - Create a DataLoader generator from the pandas dataframe.\n",
        "\n",
        "\n",
        "- Alternatively:\n",
        "    - Extract the BigQuery table to CSV files.\n",
        "    - Preprocess the CSV files.\n",
        "    - Create a tf.data.Dataset generator from the CSV files.\n",
        "    \n",
        "### Costs\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "- Vertex AI\n",
        "- Cloud Storage\n",
        "- BigQuery\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing) and [BigQuery pricing](https://cloud.google.com/bigquery/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_mlops"
      },
      "source": [
        "## Installations\n",
        "\n",
        "Install the following packages to execute this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_mlops"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\"\n",
        "\n",
        "# Install the packages\n",
        "! pip3 install --upgrade pyarrow $USER_FLAG\n",
        "! pip3 install --upgrade google-cloud-bigquery $USER_FLAG\n",
        "! pip3 install --upgrade google-cloud-aiplatform[tensorboard] $USER_FLAG\n",
        "! pip3 install -U xgboost $USER_FLAG\n",
        "! pip3 install -U tensorflow-io==0.18 $USER_FLAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "restart"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "restart"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a47846030fef"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84cd83853240"
      },
      "source": [
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "1. [Enable the Vertex AI, BigQuery, Compute Engine and Cloud Storage APIs](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,bigquery,compute_component,storage_component).\n",
        "\n",
        "1. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
        "\n",
        "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "project_id"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_project_id"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_project_id"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID:\", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_gcloud_project_id"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
        "\n",
        "- Americas: `us-central1`\n",
        "- Europe: `europe-west4`\n",
        "- Asia Pacific: `asia-east1`\n",
        "\n",
        "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
        "\n",
        "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "region"
      },
      "outputs": [],
      "source": [
        "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
        "\n",
        "if REGION == \"[your-region]\":\n",
        "    REGION = \"us-central1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "timestamp"
      },
      "source": [
        "#### Timestamp\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "timestamp"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77c385f0db59"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using Google Cloud Notebooks**, your environment is already authenticated. Skip this step.\n",
        "\n",
        "**If you are using Colab**, run the cell below and follow the instructions when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "In the Cloud Console, go to the [Create service account key](https://console.cloud.google.com/apis/credentials/serviceaccountkey) page.\n",
        "\n",
        "1. **Click Create service account**.\n",
        "\n",
        "2. In the **Service account name** field, enter a name, and click **Create**.\n",
        "\n",
        "3. In the **Grant this service account access to project** section, click the Role drop-down list. Type \"Vertex AI\" into the filter box, and select **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "\n",
        "4. Click Create. A JSON file that contains your key downloads to your local environment.\n",
        "\n",
        "5. Enter the path to your service account key as the GOOGLE_APPLICATION_CREDENTIALS variable in the cell below and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "535223fa4b84"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# If on Google Cloud Notebooks, then don't execute this code\n",
        "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bucket:custom"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "When you create a dataset resource using the Vertex SDK, you can provide a Cloud Storage bucket that contains the data. Vertex AI creates the dataset resource from the data. In this tutorial, Vertex AI also creates a dataset resource from your data in the Cloud Storage bucket.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bucket"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_bucket"
      },
      "outputs": [],
      "source": [
        "if BUCKET_URI == \"\" or BUCKET_URI is None or BUCKET_URI == \"gs://[your-bucket-name]\":\n",
        "    BUCKET_URI = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_bucket"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "validate_bucket"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "validate_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_vars"
      },
      "source": [
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "import google.cloud.aiplatform as aiplatform\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from google.cloud import bigquery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk,region"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_aip:mbsdk,region"
      },
      "outputs": [],
      "source": [
        "aiplatform.init(project=PROJECT_ID, location=REGION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_bq"
      },
      "source": [
        "### Create BigQuery client\n",
        "\n",
        "Create the BigQuery client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_bq"
      },
      "outputs": [],
      "source": [
        "bqclient = bigquery.Client(project=PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_file:u_dataset,bq"
      },
      "source": [
        "#### Location of BigQuery training data.\n",
        "\n",
        "Now, set the variable `IMPORT_FILE` to the location of the data table in BigQuery and `BQ_TABLE` with the table id."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_file:gsod,bq,lrg"
      },
      "outputs": [],
      "source": [
        "IMPORT_FILE = \"bq://bigquery-public-data.samples.gsod\"\n",
        "BQ_TABLE = \"bigquery-public-data.samples.gsod\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_dataset:tabular,bq,lrg"
      },
      "source": [
        "### Create the Dataset\n",
        "\n",
        "#### BigQuery input data\n",
        "\n",
        "Next, create the `Dataset` resource using the `create` method for the `TabularDataset` class, which takes the following parameters:\n",
        "\n",
        "- `display_name`: The human readable name for the `Dataset` resource.\n",
        "- `bq_source`: Import data items from a BigQuery table into the `Dataset` resource.\n",
        "- `labels`: User defined metadata. In this example, you store the location of the Cloud Storage bucket containing the user defined data.\n",
        "\n",
        "Learn more about [TabularDataset from BigQuery table](https://cloud.google.com/vertex-ai/docs/datasets/create-dataset-api#aiplatform_create_dataset_tabular_bigquery_sample-python)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_dataset:tabular,bq,lrg"
      },
      "outputs": [],
      "source": [
        "dataset = aiplatform.TabularDataset.create(\n",
        "    display_name=\"NOAA historical weather data\" + \"_\" + TIMESTAMP,\n",
        "    bq_source=[IMPORT_FILE],\n",
        "    labels={\"user_metadata\": BUCKET_URI[5:]},\n",
        ")\n",
        "\n",
        "label_column = \"mean_temp\"\n",
        "\n",
        "print(dataset.resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq_extract"
      },
      "source": [
        "### Copy the dataset to Cloud Storage\n",
        "\n",
        "Next, you make a copy of the BigQuery table as a CSV file, to Cloud Storage using the BigQuery extract command.\n",
        "\n",
        "Learn more about [BigQuery command line interface](https://cloud.google.com/bigquery/docs/reference/bq-cli-reference)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bq_extract"
      },
      "outputs": [],
      "source": [
        "comps = BQ_TABLE.split(\".\")\n",
        "BQ_PROJECT_DATASET_TABLE = comps[0] + \":\" + comps[1] + \".\" + comps[2]\n",
        "\n",
        "! bq --location=us extract --destination_format CSV $BQ_PROJECT_DATASET_TABLE $BUCKET_URI/mydata*.csv\n",
        "\n",
        "IMPORT_FILES = ! gsutil ls $BUCKET_URI/mydata*.csv\n",
        "\n",
        "print(IMPORT_FILES)\n",
        "\n",
        "EXAMPLE_FILE = IMPORT_FILES[0]\n",
        "\n",
        "! gsutil cat $EXAMPLE_FILE | head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_dataset:tabular,lrg"
      },
      "source": [
        "### Create the Dataset\n",
        "\n",
        "#### CSV input data\n",
        "\n",
        "Next, create the `Dataset` resource using the `create` method for the `TabularDataset` class, which takes the following parameters:\n",
        "\n",
        "- `display_name`: The human readable name for the `Dataset` resource.\n",
        "- `gcs_source`: A list of one or more dataset index files to import the data items into the `Dataset` resource.\n",
        "- `labels`: User defined metadata. In this example, you store the location of the Cloud Storage bucket containing the user defined data.\n",
        "\n",
        "Learn more about [TabularDataset from CSV files](https://cloud.google.com/vertex-ai/docs/datasets/create-dataset-api#aiplatform_create_dataset_tabular_gcs_sample-python)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_dataset:tabular,lrg"
      },
      "outputs": [],
      "source": [
        "gcs_source = IMPORT_FILES\n",
        "\n",
        "dataset = aiplatform.TabularDataset.create(\n",
        "    display_name=\"NOAA historical weather data\" + \"_\" + TIMESTAMP,\n",
        "    gcs_source=gcs_source,\n",
        "    labels={\"user_metadata\": BUCKET_URI[5:]},\n",
        ")\n",
        "\n",
        "\n",
        "label_column = \"mean_temp\"\n",
        "\n",
        "print(dataset.resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq_view"
      },
      "source": [
        "### Create a view of the BigQuery dataset\n",
        "\n",
        "Alternatively, you can create a logical view of a BigQuery dataset that has a subset of the fields.\n",
        "\n",
        "Learn more about [Creating BigQuery views](https://cloud.google.com/bigquery/docs/views)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dc142433e50"
      },
      "outputs": [],
      "source": [
        "# Set dataset name and view name in BigQuery\n",
        "BQ_MY_DATASET = \"[your-dataset-name]\"\n",
        "BQ_MY_TABLE = \"[your-view-name]\"\n",
        "\n",
        "# Otherwise, use the default names\n",
        "if (\n",
        "    BQ_MY_DATASET == \"\"\n",
        "    or BQ_MY_DATASET is None\n",
        "    or BQ_MY_DATASET == \"[your-dataset-name]\"\n",
        "):\n",
        "    BQ_MY_DATASET = \"mlops_dataset_\" + TIMESTAMP\n",
        "\n",
        "if BQ_MY_TABLE == \"\" or BQ_MY_TABLE is None or BQ_MY_TABLE == \"[your-view-name]\":\n",
        "    BQ_MY_TABLE = \"mlops_view_\" + TIMESTAMP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bq_view"
      },
      "outputs": [],
      "source": [
        "# Create the resources\n",
        "! bq --location=US mk -d \\\n",
        "$PROJECT_ID:$BQ_MY_DATASET\n",
        "\n",
        "sql_script = f'''\n",
        "CREATE OR REPLACE VIEW `{PROJECT_ID}.{BQ_MY_DATASET}.{BQ_MY_TABLE}`\n",
        "AS SELECT station_number,year,month,day,mean_temp FROM `{BQ_TABLE}`\n",
        "'''\n",
        "print(sql_script)\n",
        "\n",
        "query = bqclient.query(sql_script)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq_to_dataframe:gsod"
      },
      "source": [
        "### Read the BigQuery dataset into a pandas dataframe\n",
        "\n",
        "Next, you read a sample of the dataset into a pandas dataframe using BigQuery `list_rows()` and `to_dataframe()` method, as follows:\n",
        "\n",
        "- `list_rows()`: Performs a query on the specified table and returns a row iterator to the query results. Optionally specify:\n",
        " - `selected_fields`: Subset of fields (columns) to return.\n",
        " - `max_results`: The maximum number of rows to return. Same as SQL LIMIT command.\n",
        "\n",
        "\n",
        "- `rows.to_dataframe()`: Invokes the row iterator and reads in the data into a pandas dataframe.\n",
        "\n",
        "Learn more about [Loading BigQuery table into a dataframe](https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bq_to_dataframe:gsod"
      },
      "outputs": [],
      "source": [
        "# Download the table.\n",
        "table = bigquery.TableReference.from_string(BQ_TABLE)\n",
        "\n",
        "rows = bqclient.list_rows(\n",
        "    table,\n",
        "    max_results=500,\n",
        "    selected_fields=[\n",
        "        bigquery.SchemaField(\"station_number\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"year\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"month\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"day\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n",
        "    ],\n",
        ")\n",
        "\n",
        "dataframe = rows.to_dataframe()\n",
        "print(dataframe.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq_to_dataset:gsod"
      },
      "source": [
        "### Read the BigQuery dataset into a tf.data.Dataset\n",
        "\n",
        "Next, you read a sample of the dataset into a tf.data.Dataset using TensorFlow IO `BigQueryClient()` and `read_session()` method, with the following parameters:\n",
        "\n",
        "- `parent`: Your project ID.\n",
        "- `project_id`: The project ID of the BigQuery table.\n",
        "- `dataset_id`: The ID of the BigQuery dataset.\n",
        "- `table_id`. The ID of the table within the corresponding BigQuery dataset.\n",
        "- `selected_fields`: Subset of fields (columns) to return.\n",
        "- `output_types`: The output types of the corresponding fields.\n",
        "- `requested_streams`: The number of parallel readers.\n",
        "\n",
        "Learn more about [BigQuery TensorFlow reader](https://www.tensorflow.org/io/tutorials/bigquery).\n",
        "\n",
        "Learn more about [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bq_to_dataset:gsod"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.framework import dtypes\n",
        "from tensorflow_io.bigquery import BigQueryClient\n",
        "\n",
        "feature_names = \"station_number,year,month,day\".split(\",\")\n",
        "\n",
        "target_name = \"mean_temp\"\n",
        "\n",
        "\n",
        "def read_bigquery(project, dataset, table):\n",
        "    tensorflow_io_bigquery_client = BigQueryClient()\n",
        "    read_session = tensorflow_io_bigquery_client.read_session(\n",
        "        parent=\"projects/\" + PROJECT_ID,\n",
        "        project_id=project,\n",
        "        dataset_id=dataset,\n",
        "        table_id=table,\n",
        "        selected_fields=feature_names + [target_name],\n",
        "        output_types=[dtypes.string] + [dtypes.int32] * 3 + [dtypes.float32],\n",
        "        requested_streams=2,\n",
        "    )\n",
        "\n",
        "    dataset = read_session.parallel_read_rows()\n",
        "    return dataset\n",
        "\n",
        "\n",
        "PROJECT, DATASET, TABLE = IMPORT_FILE.split(\"/\")[-1].split(\".\")\n",
        "tf_dataset = read_bigquery(PROJECT, DATASET, TABLE)\n",
        "\n",
        "print(tf_dataset.take(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csv_to_dataset:gsod"
      },
      "source": [
        "### Read CSV files into a tf.data.Dataset\n",
        "\n",
        "Alternatively, when your data is in CSV files, you can load the dataset into a tf.data.Dataset using `tf.data.experimental.CsvDataset`, with the following parameters:\n",
        "\n",
        "- `filenames`: A list of one or more CSV files.\n",
        "- `header`: Whether CSV file(s) contain a header.\n",
        "- `select_cols`: Subset of fields (columns) to return.\n",
        "- `record_defaults`: The output types of the corresponding fields.\n",
        "\n",
        "Learn more about [tf.data CsvDataset](https://www.tensorflow.org/api_docs/python/tf/data/experimental/CsvDataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csv_to_dataset:gsod"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "feature_names = [\"station_number,year,month,day\".split(\",\")]\n",
        "\n",
        "target_name = \"mean_temp\"\n",
        "\n",
        "tf_dataset = tf.data.experimental.CsvDataset(\n",
        "    filenames=IMPORT_FILES,\n",
        "    header=True,\n",
        "    select_cols=feature_names.append(target_name),\n",
        "    record_defaults=[dtypes.string] + [dtypes.int32] * 3 + [dtypes.float32],\n",
        ")\n",
        "\n",
        "print(tf_dataset.take(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataframe_to_bq"
      },
      "source": [
        "### Create a BigQuery dataset from a pandas dataframe\n",
        "\n",
        "You can create a BigQuery dataset from a pandas dataframe using the BigQuery `create_dataset()` and `load_table_from_dataframe()` methods, as follows:\n",
        "\n",
        "- `create_dataset()`: Creates an empty BigQuery dataset, with the following parameters:\n",
        " - `dataset_ref`: The `DatasetReference` created from the dataset_id -- e.g., samples.\n",
        "- `load_table_from_dataframe()`: Loads one or more CSV files into a table within the corresponding dataset, with the following parameters:\n",
        " - `dataframe`: The dataframe.\n",
        " - `table`: The `TableReference` for the table.\n",
        " - `job_config`: Specifications on how to load the dataframe data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataframe_to_bq"
      },
      "outputs": [],
      "source": [
        "LOCATION = \"us\"\n",
        "\n",
        "SCHEMA = [\n",
        "    bigquery.SchemaField(\"station_number\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"year\", \"INTEGER\"),\n",
        "    bigquery.SchemaField(\"month\", \"INTEGER\"),\n",
        "    bigquery.SchemaField(\"day\", \"INTEGER\"),\n",
        "    bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n",
        "]\n",
        "\n",
        "\n",
        "DATASET_ID = \"samples\"\n",
        "TABLE_ID = \"gsod\"\n",
        "\n",
        "\n",
        "def create_bigquery_dataset(dataset_id):\n",
        "    dataset = bigquery.Dataset(\n",
        "        bigquery.dataset.DatasetReference(PROJECT_ID, dataset_id)\n",
        "    )\n",
        "    dataset.location = \"us\"\n",
        "\n",
        "    try:\n",
        "        dataset = bqclient.create_dataset(dataset)  # API request\n",
        "        return True\n",
        "    except Exception as err:\n",
        "        print(err)\n",
        "        if err.code != 409:  # http_client.CONFLICT\n",
        "            raise\n",
        "    return False\n",
        "\n",
        "\n",
        "def load_data_into_bigquery(dataframe, dataset_id, table_id):\n",
        "    create_bigquery_dataset(dataset_id)\n",
        "    dataset = bqclient.dataset(dataset_id)\n",
        "    table = dataset.table(table_id)\n",
        "\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        # Specify a (partial) schema. All columns are always written to the\n",
        "        # table. The schema is used to assist in data type definitions.\n",
        "        schema=[\n",
        "            bigquery.SchemaField(\"station_number\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"year\", \"INTEGER\"),\n",
        "            bigquery.SchemaField(\"month\", \"INTEGER\"),\n",
        "            bigquery.SchemaField(\"day\", \"INTEGER\"),\n",
        "            bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n",
        "        ],\n",
        "        # Optionally, set the write disposition. BigQuery appends loaded rows\n",
        "        # to an existing table by default, but with WRITE_TRUNCATE write\n",
        "        # disposition it replaces the table with the loaded data.\n",
        "        write_disposition=\"WRITE_TRUNCATE\",\n",
        "    )\n",
        "\n",
        "    NEW_BQ_TABLE = f\"{PROJECT_ID}.{dataset_id}.{table_id}\"\n",
        "\n",
        "    job = bqclient.load_table_from_dataframe(\n",
        "        dataframe, NEW_BQ_TABLE, job_config=job_config\n",
        "    )  # Make an API request.\n",
        "    job.result()  # Wait for the job to complete.\n",
        "\n",
        "    table = bqclient.get_table(NEW_BQ_TABLE)  # Make an API request.\n",
        "    print(\n",
        "        \"Loaded {} rows and {} columns to {}\".format(\n",
        "            table.num_rows, len(table.schema), NEW_BQ_TABLE\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "load_data_into_bigquery(dataframe, DATASET_ID, TABLE_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csv_to_bq"
      },
      "source": [
        "### Create a BigQuery dataset from CSV files\n",
        "\n",
        "You can create a BigQuery dataset from CSV files using the BigQuery `create_dataset()` and `load_table_from_uri()` methods, as follows:\n",
        "\n",
        "- `create_dataset()`: Creates an empty BigQuery dataset, with the following parameters:\n",
        " - `dataset_ref`: The `DatasetReference` created from the dataset_id -- e.g., samples.\n",
        "- `load_table_from_uri()`: Loads one or more CSV files into a table within the corresponding dataset, with the following parameters:\n",
        " - `url`: A set of one or more CVS files in Cloud Storage storage.\n",
        " - `table`: The `TableReference` for the table.\n",
        " - `job_config`: Specifications on how to load the CSV data.\n",
        "\n",
        "Learn more about [Importing CSV data into BigQuery](https://www.tensorflow.org/io/tutorials/bigquery#import_census_data_into_bigquery)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csv_to_bq"
      },
      "outputs": [],
      "source": [
        "LOCATION = \"us\"\n",
        "\n",
        "CSV_SCHEMA = [\n",
        "    bigquery.SchemaField(\"station_number\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"wban_number\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"year\", \"INTEGER\"),\n",
        "    bigquery.SchemaField(\"month\", \"INTEGER\"),\n",
        "    bigquery.SchemaField(\"day\", \"INTEGER\"),\n",
        "    bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n",
        "    bigquery.SchemaField(\"num_mean_temp_samples\", \"INTEGER\"),\n",
        "    bigquery.SchemaField(\"mean_dew_point\", \"FLOAT\"),\n",
        "    bigquery.SchemaField(\"num_mean_dew_point_samples\", \"INTEGER\"),\n",
        "    bigquery.SchemaField(\"mean_sealevel_pressure\", \"FLOAT\"),\n",
        "    bigquery.SchemaField(\"num_mean_sealevel_pressure_samples\", \"INTEGER\"),\n",
        "    bigquery.SchemaField(\"mean_station_pressure\", \"FLOAT\"),\n",
        "    bigquery.SchemaField(\"num_mean_station_pressure_samples\", \"INTEGER\"),\n",
        "    bigquery.SchemaField(\"mean_visibility\", \"FLOAT\"),\n",
        "    bigquery.SchemaField(\"num_mean_visibility_samples\", \"INTEGER\"),\n",
        "    bigquery.SchemaField(\"mean_wind_speed\", \"FLOAT\"),\n",
        "    bigquery.SchemaField(\"num_mean_wind_speed_samples\", \"INTEGER\"),\n",
        "    bigquery.SchemaField(\"max_sustained_wind_speed\", \"FLOAT\"),\n",
        "    bigquery.SchemaField(\"max_gust_wind_speed\", \"FLOAT\"),\n",
        "    bigquery.SchemaField(\"max_temperature\", \"FLOAT\"),\n",
        "    bigquery.SchemaField(\"max_temperature_explicit\", \"BOOLEAN\"),\n",
        "    bigquery.SchemaField(\"min_temperature\", \"FLOAT\"),\n",
        "    bigquery.SchemaField(\"min_temperature_explicit\", \"BOOLEAN\"),\n",
        "    bigquery.SchemaField(\"total_percipitation\", \"FLOAT\"),\n",
        "    bigquery.SchemaField(\"snow_depth\", \"FLOAT\"),\n",
        "    bigquery.SchemaField(\"fog\", \"BOOLEAN\"),\n",
        "    bigquery.SchemaField(\"rain\", \"BOOLEAN\"),\n",
        "    bigquery.SchemaField(\"snow\", \"BOOLEAN\"),\n",
        "    bigquery.SchemaField(\"hail\", \"BOOLEAN\"),\n",
        "    bigquery.SchemaField(\"thunder\", \"BOOLEAN\"),\n",
        "    bigquery.SchemaField(\"tornado\", \"BOOLEAN\"),\n",
        "]\n",
        "\n",
        "\n",
        "DATASET_ID = \"samples\"\n",
        "TABLE_ID = \"gsod\"\n",
        "\n",
        "\n",
        "def load_data_into_bigquery(url, dataset_id, table_id):\n",
        "    create_bigquery_dataset(dataset_id)\n",
        "    dataset = bqclient.dataset(dataset_id)\n",
        "    table = dataset.table(table_id)\n",
        "\n",
        "    job_config = bigquery.LoadJobConfig()\n",
        "    job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
        "    job_config.source_format = bigquery.SourceFormat.CSV\n",
        "    job_config.schema = CSV_SCHEMA\n",
        "    job_config.skip_leading_rows = 1  # heading\n",
        "\n",
        "    load_job = bqclient.load_table_from_uri(url, table, job_config=job_config)\n",
        "    print(\"Starting job {}\".format(load_job.job_id))\n",
        "\n",
        "    load_job.result()  # Waits for table load to complete.\n",
        "    print(\"Job finished.\")\n",
        "\n",
        "    destination_table = bqclient.get_table(table)\n",
        "    print(\"Loaded {} rows.\".format(destination_table.num_rows))\n",
        "\n",
        "\n",
        "load_data_into_bigquery(IMPORT_FILES, DATASET_ID, TABLE_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq_to_xgboost"
      },
      "source": [
        "### Read BigQuery table into XGboost DMatrix\n",
        "\n",
        "Currently, there is no direct data feeding connector between BigQuery and the open source XGBoost. The BigQuery ML service has a built-in XGBoost training module.\n",
        "\n",
        "Alernatively, you extract the data either as a pandas dataframe or as CSV files. The extracted data is then given as an input to a `DMatrix` object when training the model.\n",
        "\n",
        "Learn more about [Getting started with built-in XGBoost](https://cloud.google.com/ai-platform/training/docs/algorithms/xgboost-start)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pandas_to_xgboost:gsod"
      },
      "source": [
        "### Read pandas table into XGboost DMatrix\n",
        "\n",
        "Next, you load the pandas dataframe into a `DMatrix` object. XGBoost does not support non-numeric inputs. Any column that is categorical need to be one-hot encoded prior to loading the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pandas_to_xgboost:gsod"
      },
      "outputs": [],
      "source": [
        "dataframe[\"station_number\"] = pd.to_numeric(dataframe[\"station_number\"])\n",
        "labels = dataframe[\"mean_temp\"]\n",
        "data = dataframe.drop(4)\n",
        "\n",
        "dtrain = xgb.DMatrix(data, label=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csv_to_xgboost:gsod"
      },
      "source": [
        "### Read CSV files into XGboost DMatrix\n",
        "\n",
        "Currently, there is no Cloud Storage support in XGBoost. If you use CSV files for input, you need to download them locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csv_to_xgboost:gsod"
      },
      "outputs": [],
      "source": [
        "! gsutil cp $EXAMPLE_FILE data.csv\n",
        "\n",
        "dtrain = xgb.DMatrix(\"data.csv?format=csv&label_column=4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cleanup:mbsdk"
      },
      "source": [
        "# Clean up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:\n",
        "\n",
        "- Vertex AI Dataset resource\n",
        "- Cloud Storage Bucket\n",
        "- BigQuery Dataset\n",
        "\n",
        "Set `delete_storage` to _True_ to delete the storage resources used in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47ad926d84e8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Delete the dataset using the Vertex dataset object\n",
        "dataset.delete()\n",
        "# Delete the temporary BigQuery dataset\n",
        "! bq rm -r -f $PROJECT_ID:$DATASET_ID\n",
        "\n",
        "delete_storage = False\n",
        "if delete_storage or os.getenv(\"IS_TESTING\"):\n",
        "    # Delete the created GCS bucket\n",
        "    ! gsutil rm -r $BUCKET_URI\n",
        "    # Delete the created BigQuery datasets\n",
        "    ! bq rm -r -f $PROJECT_ID:$BQ_MY_DATASET"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "get_started_bq_datasets.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "copyright"
      },
      "outputs": [],
      "source": [
        "# Copyright 2021 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title:generic,gcp"
      },
      "source": [
        "# E2E ML on GCP: MLOps stage 1 : data management\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/tree/master/notebooks/official/automl/ml_ops_stage1/mlops_data_management.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/ai/platform/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/tree/master/notebooks/official/automl/ml_ops_stage1/mlops_data_management.ipynb\">\n",
        "      Open in Google Cloud Notebooks\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "<br/><br/><br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview:mlops"
      },
      "source": [
        "## Overview\n",
        "\n",
        "\n",
        "This tutorial demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. This tutorial covers stage 1 : data management."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset:bq,chicago,lbn"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset used for this tutorial is the [Chicago Taxi](https://www.kaggle.com/chicago/chicago-taxi-trips-bq). The version of the dataset you will use in this tutorial is stored in a public BigQuery table. The trained model predicts whether someone would leave a tip for a taxi fare."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "objective:mlops,stage1,tabular"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you create a MLOps stage 1: data management process.\n",
        "\n",
        "This tutorial uses the following Vertex AI and Data Analytics services:\n",
        "\n",
        "- `Vertex Datasets`\n",
        "- `BigQuery`\n",
        "- `Dataflow`\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Explore and visualize the data.\n",
        "- Create a Vertex `Dataset` resource from `BigQuery` table -- for AutoML training.\n",
        "- Extract a copy of the dataset to a CSV file in Cloud Storage.\n",
        "- Create a Vertex `Dataset` resource from CSV files -- alternative for AutoML training.\n",
        "- Read a sample of the `BigQuery` dataset into a dataframe.\n",
        "- Generate statistics and data schema using TensorFlow Data Validation from the samples in the dataframe.\n",
        "- Generate a TFRecord feature specification using TensorFlow Data Validation from the data schema.\n",
        "- Preprocess a portion of the BigQuery data using `Dataflow` -- for custom training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "recommendation:mlops,stage1,tabular"
      },
      "source": [
        "### Recommendations\n",
        "\n",
        "When doing E2E MLOps on Google Cloud, the following best practices with structured (tabular) data are recommended:\n",
        "\n",
        " - For large amounts of data, use BigQuery table. Otherwise, use a CSV file stored in Cloud Storage.\n",
        " - When storing a large amount of data in CSV file, shard the data at 10,000 rows per shard.\n",
        " - Create a managed dataset with Vertex `TabularDataset`.\n",
        " - Preprocess the data with `Dataflow`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_aip:mbsdk"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Install the latest version of Vertex SDK for Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Google Cloud Notebook\n",
        "if os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n",
        "    USER_FLAG = \"--user\"\n",
        "else:\n",
        "    USER_FLAG = \"\"\n",
        "\n",
        "! pip3 install --upgrade google-cloud-aiplatform $USER_FLAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_dataflow"
      },
      "source": [
        "Install the latest GA version of *Dataflow* library as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_dataflow"
      },
      "outputs": [],
      "source": [
        "! pip3 install -U apache-beam[gcp] $USER_FLAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_tfdv"
      },
      "source": [
        "Install the latest GA version of *TensorFlow Data Validation* library as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_tfdv"
      },
      "outputs": [],
      "source": [
        "! pip3 install -U tensorflow-data-validation $USER_FLAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_tft"
      },
      "source": [
        "Install the latest GA version of *TensorFlow Transform* library as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_tft"
      },
      "outputs": [],
      "source": [
        "! pip3 install -U tensorflow-transform $USER_FLAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "restart"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "restart"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "project_id"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_project_id"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_project_id"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID:\", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_gcloud_project_id"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
        "\n",
        "- Americas: `us-central1`\n",
        "- Europe: `europe-west4`\n",
        "- Asia Pacific: `asia-east1`\n",
        "\n",
        "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
        "\n",
        "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "region"
      },
      "outputs": [],
      "source": [
        "REGION = \"us-central1\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "timestamp"
      },
      "source": [
        "#### Timestamp\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "timestamp"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bucket:custom"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "When you submit a custom training job using the Vertex SDK, you upload a Python package\n",
        "containing your training code to a Cloud Storage bucket. Vertex AI runs\n",
        "the code from this package. In this tutorial, Vertex AI also saves the\n",
        "trained model that results from your job in the same bucket. You can then\n",
        "create an `Endpoint` resource based on this output in order to serve\n",
        "online predictions.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bucket"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_bucket"
      },
      "outputs": [],
      "source": [
        "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
        "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_bucket"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "validate_bucket"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "validate_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_vars"
      },
      "source": [
        "### Set up variables\n",
        "\n",
        "Next, set up some variables used throughout the tutorial.\n",
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "import google.cloud.aiplatform as aip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_bq"
      },
      "source": [
        "#### Import BigQuery\n",
        "\n",
        "Import the BigQuery package into your Python environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_bq"
      },
      "outputs": [],
      "source": [
        "from google.cloud import bigquery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_beam"
      },
      "source": [
        "#### Import Apache Beam\n",
        "\n",
        "Import the Apache Beam package into your Python environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_beam"
      },
      "outputs": [],
      "source": [
        "import apache_beam as beam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_tfdv"
      },
      "source": [
        "#### Import TFDV\n",
        "\n",
        "Import the TensorFlow Data Validation (TFDV) package into your Python environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_tfdv"
      },
      "outputs": [],
      "source": [
        "import tensorflow_data_validation as tfdv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_tft"
      },
      "source": [
        "#### Import TensorFlow Transform\n",
        "\n",
        "Import the TensorFlow Transform (TFT) package into your Python environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_tft"
      },
      "outputs": [],
      "source": [
        "import tensorflow_transform as tft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk"
      },
      "source": [
        "### Initialize Vertex SDK for Python\n",
        "\n",
        "Initialize the Vertex SDK for Python for your project and corresponding bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_bq"
      },
      "source": [
        "### Create BigQuery client\n",
        "\n",
        "Create the BigQuery client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_bq"
      },
      "outputs": [],
      "source": [
        "bqclient = bigquery.Client()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "recommendation:dataset,tabular"
      },
      "source": [
        "## Datasets\n",
        "\n",
        "Next, you look at options for creating managed datasets:\n",
        "\n",
        "* `BigQuery`: Create a Vertex `TabularDataset` resource.\n",
        "* `CSV`: Create a Vertex `TabularDataset` resource.\n",
        "* `TFRecords`: Self-manage the dataset on Cloud Storage storage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_file:u_dataset,bq"
      },
      "source": [
        "#### Location of BigQuery training data.\n",
        "\n",
        "Now set the variable `IMPORT_FILE` to the location of the data table in BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_file:chicago,bq,lbn"
      },
      "outputs": [],
      "source": [
        "IMPORT_FILE = \"bq://bigquery-public-data.chicago_taxi_trips.taxi_trips\"\n",
        "BQ_TABLE = \"bigquery-public-data.chicago_taxi_trips.taxi_trips\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "explore_bq:chicago"
      },
      "source": [
        "### Explore BigQuery dataset\n",
        "\n",
        "Explore the contents of the BigQuery table:\n",
        "\n",
        "- Get all examples from 2015\n",
        "- Sort by the day of the week\n",
        "- Count the number of examples for each day of the week"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "explore_bq:chicago"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"SELECT\n",
        "    CAST(EXTRACT(DAYOFWEEK FROM trip_start_timestamp) AS string) AS trip_dayofweek,\n",
        "    FORMAT_DATE('%A',cast(trip_start_timestamp as date)) AS trip_dayname,\n",
        "    COUNT(*) as trip_count,\n",
        "FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
        "WHERE\n",
        "    EXTRACT(YEAR FROM trip_start_timestamp) = 2015\n",
        "GROUP BY\n",
        "    trip_dayofweek,\n",
        "    trip_dayname\n",
        "ORDER BY\n",
        "    trip_dayofweek\"\"\"\n",
        "\n",
        "_ = bqclient.query(query)\n",
        "rows = _.result()\n",
        "dataframe = rows.to_dataframe()\n",
        "print(dataframe.head(7))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot_bq:chicago"
      },
      "outputs": [],
      "source": [
        "dataframe.plot(kind=\"bar\", x=\"trip_dayname\", y=\"trip_count\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq_copy:chicago"
      },
      "source": [
        "### Make a private copy of subset of BigQuery table\n",
        "\n",
        "Next, you make a private copy of the BigQuery table:\n",
        "- Select a subset of columns\n",
        "- Select a subset of rows (LIMIT)\n",
        "- Set conditions (WHERE)\n",
        "- Do feature engineering on geolocation coords.\n",
        "- Pre-split the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bq_copy:chicago"
      },
      "outputs": [],
      "source": [
        "BQ_DATASET = BQ_TABLE.split(\".\")[1]\n",
        "BQ_TABLE_COPY = f\"{PROJECT_ID}.{BQ_DATASET}.taxi_trips\"\n",
        "LIMIT = 1000000\n",
        "YEAR = 2020\n",
        "\n",
        "query = f\"\"\"\n",
        "CREATE OR REPLACE TABLE `{BQ_TABLE_COPY}`\n",
        "AS (\n",
        "    WITH\n",
        "      taxitrips AS (\n",
        "      SELECT\n",
        "        trip_start_timestamp,\n",
        "        trip_seconds,\n",
        "        trip_miles,\n",
        "        payment_type,\n",
        "        pickup_longitude,\n",
        "        pickup_latitude,\n",
        "        dropoff_longitude,\n",
        "        dropoff_latitude,\n",
        "        tips,\n",
        "        fare\n",
        "      FROM\n",
        "        `{BQ_TABLE}`\n",
        "      WHERE pickup_longitude IS NOT NULL\n",
        "      AND pickup_latitude IS NOT NULL\n",
        "      AND dropoff_longitude IS NOT NULL\n",
        "      AND dropoff_latitude IS NOT NULL\n",
        "      AND trip_miles > 0\n",
        "      AND trip_seconds > 0\n",
        "      AND fare > 0\n",
        "      AND EXTRACT(YEAR FROM trip_start_timestamp) = {YEAR}\n",
        "    )\n",
        "\n",
        "    SELECT\n",
        "      EXTRACT(MONTH from trip_start_timestamp) as trip_month,\n",
        "      EXTRACT(DAY from trip_start_timestamp) as trip_day,\n",
        "      EXTRACT(DAYOFWEEK from trip_start_timestamp) as trip_day_of_week,\n",
        "      EXTRACT(HOUR from trip_start_timestamp) as trip_hour,\n",
        "      trip_seconds,\n",
        "      trip_miles,\n",
        "      payment_type,\n",
        "      ST_AsText(\n",
        "          ST_SnapToGrid(ST_GeogPoint(pickup_longitude, pickup_latitude), 0.1)\n",
        "      ) AS pickup_grid,\n",
        "      ST_AsText(\n",
        "          ST_SnapToGrid(ST_GeogPoint(dropoff_longitude, dropoff_latitude), 0.1)\n",
        "      ) AS dropoff_grid,\n",
        "      ST_Distance(\n",
        "          ST_GeogPoint(pickup_longitude, pickup_latitude),\n",
        "          ST_GeogPoint(dropoff_longitude, dropoff_latitude)\n",
        "      ) AS euclidean,\n",
        "      CONCAT(\n",
        "          ST_AsText(ST_SnapToGrid(ST_GeogPoint(pickup_longitude,\n",
        "              pickup_latitude), 0.1)),\n",
        "          ST_AsText(ST_SnapToGrid(ST_GeogPoint(dropoff_longitude,\n",
        "              dropoff_latitude), 0.1))\n",
        "      ) AS loc_cross,\n",
        "      IF((tips/fare >= 0.2), 1, 0) AS tip_bin,\n",
        "    FROM\n",
        "      taxitrips\n",
        "    LIMIT {LIMIT}\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "response = bqclient.query(query)\n",
        "_ = response.result()\n",
        "\n",
        "BQ_TABLE = BQ_TABLE_COPY\n",
        "IMPORT_FILE = f\"bq://{BQ_TABLE_COPY}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_dataset:tabular,bq,lbn"
      },
      "source": [
        "### Create the Dataset\n",
        "\n",
        "#### BigQuery input data\n",
        "\n",
        "Next, create the `Dataset` resource using the `create` method for the `TabularDataset` class, which takes the following parameters:\n",
        "\n",
        "- `display_name`: The human readable name for the `Dataset` resource.\n",
        "- `bq_source`: Import data items from a BigQuery table into the `Dataset` resource.\n",
        "\n",
        "Learn more about [TabularDataset from BigQuery table](https://cloud.google.com/vertex-ai/docs/datasets/create-dataset-api#aiplatform_create_dataset_tabular_bigquery_sample-python)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_dataset:tabular,bq,lbn"
      },
      "outputs": [],
      "source": [
        "dataset = aip.TabularDataset.create(\n",
        "    display_name=\"Chicago Taxi\" + \"_\" + TIMESTAMP, bq_source=[IMPORT_FILE]\n",
        ")\n",
        "\n",
        "label_column = \"tip_bin\"\n",
        "\n",
        "print(dataset.resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq_extract"
      },
      "source": [
        "### Copy the dataset to Cloud Storage\n",
        "\n",
        "Next, you make a copy of the BigQuery dataset, as a CSV file, to Cloud Storage using the BigQuery extract command.\n",
        "\n",
        "Learn more about [BigQuery command line interface](https://cloud.google.com/bigquery/docs/reference/bq-cli-reference)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bq_extract"
      },
      "outputs": [],
      "source": [
        "comps = BQ_TABLE.split(\".\")\n",
        "BQ_PROJECT_DATASET_TABLE = comps[0] + \":\" + comps[1] + \".\" + comps[2]\n",
        "\n",
        "! bq --location=us extract --destination_format CSV $BQ_PROJECT_DATASET_TABLE $BUCKET_NAME/mydata*.csv\n",
        "\n",
        "IMPORT_FILES = ! gsutil ls $BUCKET_NAME/mydata*.csv\n",
        "\n",
        "print(IMPORT_FILES)\n",
        "\n",
        "EXAMPLE_FILE = IMPORT_FILES[0]\n",
        "\n",
        "! gsutil cat $EXAMPLE_FILE | head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_dataset:tabular,lbn"
      },
      "source": [
        "### Create the Dataset\n",
        "\n",
        "#### CSV input data\n",
        "\n",
        "Next, create the `Dataset` resource using the `create` method for the `TabularDataset` class, which takes the following parameters:\n",
        "\n",
        "- `display_name`: The human readable name for the `Dataset` resource.\n",
        "- `gcs_source`: A list of one or more dataset index files to import the data items into the `Dataset` resource.\n",
        "\n",
        "Learn more about [TabularDataset from CSV files](https://cloud.google.com/vertex-ai/docs/datasets/create-dataset-api#aiplatform_create_dataset_tabular_gcs_sample-python)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_dataset:tabular,lbn"
      },
      "outputs": [],
      "source": [
        "if \"IMPORT_FILES\" in globals():\n",
        "    gcs_source = IMPORT_FILES\n",
        "else:\n",
        "    gcs_source = [IMPORT_FILE]\n",
        "\n",
        "dataset = aip.TabularDataset.create(\n",
        "    display_name=\"Chicago Taxi\" + \"_\" + TIMESTAMP, gcs_source=gcs_source\n",
        ")\n",
        "\n",
        "\n",
        "label_column = \"tip_bin\"\n",
        "\n",
        "print(dataset.resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq_to_dataframe:all"
      },
      "source": [
        "### Read the BigQuery dataset into a pandas dataframe\n",
        "\n",
        "Next, you read a sample of the dataset into a pandas dataframe using BigQuery `list_rows()` and `to_dataframe()` method, as follows:\n",
        "\n",
        "- `list_rows()`: Performs a query on the specified table and returns a row iterator to the query results. Optionally specify:\n",
        " - `selected_fields`: Subset of fields (columns) to return.\n",
        " - `max_results`: The maximum number of rows to return. Same as SQL LIMIT command.\n",
        "\n",
        "\n",
        "- `rows.to_dataframe()`: Invokes the row iterator and reads in the data into a pandas dataframe.\n",
        "\n",
        "Learn more about [Loading BigQuery table into a dataframe](https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bq_to_dataframe:all"
      },
      "outputs": [],
      "source": [
        "# Download a table.\n",
        "table = bigquery.TableReference.from_string(BQ_TABLE)\n",
        "\n",
        "rows = bqclient.list_rows(table, max_results=2000)\n",
        "\n",
        "dataframe = rows.to_dataframe()\n",
        "print(dataframe.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfdv_stats:dataframe"
      },
      "source": [
        "###  Generate dataset statistics\n",
        "\n",
        "#### Dataframe input data\n",
        "\n",
        "Generate statistics on the dataset with the TensorFlow Data Validation (TFDV) package. Use the `generate_statistics_from_dataframe()` method, with the following parameters:\n",
        "\n",
        "- `dataframe`: The dataset in an in-memory pandas dataframe.\n",
        "- `stats_options`: The selected statistics options:\n",
        "  - `label_feature`: The column which is the label to predict.\n",
        "  - `sample_rate`: The sampling rate. If specified, statistics is computed over the sample.\n",
        "  - `num_top_values`: number of most frequent feature values to keep for string features.\n",
        "\n",
        "Learn about [TensorFlow Data Validation (TFDV)](https://www.tensorflow.org/tfx/data_validation/get_started)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfdv_stats:dataframe"
      },
      "outputs": [],
      "source": [
        "stats = tfdv.generate_statistics_from_dataframe(\n",
        "    dataframe=dataframe,\n",
        "    stats_options=tfdv.StatsOptions(\n",
        "        label_feature=\"tip_bin\", sample_rate=1, num_top_values=50\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfdv_visualize_stats"
      },
      "source": [
        "### Visualize dataset statistics\n",
        "\n",
        "A visualization of the dataset statistics can be displayed using the TFDV `visualize_statistics()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfdv_visualize_stats"
      },
      "outputs": [],
      "source": [
        "tfdv.visualize_statistics(stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfdv_stats:csv"
      },
      "source": [
        "###  Generate dataset statistics\n",
        "\n",
        "#### CSV input data\n",
        "\n",
        "Generate statistics on the dataset with the TensorFlow Data Validation (TFDV) package. Use the `generate_statistics_from_csv()` method, with the following parameters:\n",
        "\n",
        "- `data_location`: The dataset Cloud Storage file location.\n",
        "- `stats_options`: The selected statistics options:\n",
        "  - `label_feature`: The column which is the label to predict.\n",
        "  - `num_top_values`: number of most frequent feature values to keep for string features.\n",
        "\n",
        "Learn about [TensorFlow Data Validation (TFDV)](https://www.tensorflow.org/tfx/data_validation/get_started)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfdv_stats:csv"
      },
      "outputs": [],
      "source": [
        "stats = tfdv.generate_statistics_from_csv(\n",
        "    data_location=EXAMPLE_FILE,\n",
        "    stats_options=tfdv.StatsOptions(label_feature=\"tip_bin\", num_top_values=50),\n",
        ")\n",
        "\n",
        "print(stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfdv_schema"
      },
      "source": [
        "###  Generate the raw data schema\n",
        "\n",
        "Generate the data schema on the dataset with the TensorFlow Data Validation (TFDV) package. Use the `infer_schema()` method, with the following parameters:\n",
        "\n",
        "- `statistics`: The statistics generated by TFDV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfdv_schema"
      },
      "outputs": [],
      "source": [
        "schema = tfdv.infer_schema(statistics=stats)\n",
        "print(schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfdv_schema:write"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "SCHEMA_LOCATION = os.path.join(BUCKET_NAME, \"schema.txt\")\n",
        "tfdv.write_schema_text(output_path=SCHEMA_LOCATION, schema=schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tft_feature_spec"
      },
      "source": [
        "### Generate the feature specification\n",
        "\n",
        "Generate the feature specification, compatible with TFRecords, on the dataset with the TensorFlow Transform (TFT) package. Use the `schema_as_feature_spec()` method, with the following parameters:\n",
        "\n",
        "- `schema`: The data schema generated by TFDV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tft_feature_spec"
      },
      "outputs": [],
      "source": [
        "feature_spec = tft.tf_metadata.schema_utils.schema_as_feature_spec(schema).feature_spec\n",
        "\n",
        "print(feature_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataflow:transform,bq,chicago"
      },
      "source": [
        "### Preprocess data with Dataflow\n",
        "\n",
        "#### Data Preprocessing\n",
        "\n",
        "Next, you preprocess the data using Dataflow. In this example, you query the BigQuery table and split the examples into training and evaluation datasets and preprocess feature columns:\n",
        "\n",
        "- `Numeric`: Rescale the values with `tft.scale_to_z_score`.\n",
        "- `Categorical`: Encode as a categorical column with `tft.compute_and_apply_vocabulary`.\n",
        "\n",
        "For expendiency, the number of examples from the dataset is limited to 500."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataflow:transform,bq,chicago"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform.beam as tft_beam\n",
        "\n",
        "NUMERIC_FEATURES = [\"trip_miles\", \"euclidean\"]\n",
        "CAT_FEATURES = [\"payment_type\", \"pickup_grid\", \"dropoff_grid\", \"loc_cross\"]\n",
        "\n",
        "\n",
        "def parse_bq_record(bq_record):\n",
        "    \"\"\"Parses a bq_record to a dictionary.\"\"\"\n",
        "    output = {}\n",
        "    for key in bq_record:\n",
        "        output[key] = [bq_record[key]]\n",
        "    return output\n",
        "\n",
        "\n",
        "def split_dataset(bq_row, num_partitions, ratio):\n",
        "    \"\"\"Returns a partition number for a given bq_row.\"\"\"\n",
        "    import json\n",
        "\n",
        "    assert num_partitions == len(ratio)\n",
        "    bucket = sum(map(ord, json.dumps(bq_row))) % sum(ratio)\n",
        "    total = 0\n",
        "    for i, part in enumerate(ratio):\n",
        "        total += part\n",
        "        if bucket < total:\n",
        "            return i\n",
        "    return len(ratio) - 1\n",
        "\n",
        "\n",
        "def preprocessing_fn(inputs):\n",
        "    outputs = {}\n",
        "    for key in inputs.keys():\n",
        "        if key in NUMERIC_FEATURES:\n",
        "            outputs[key] = tft.scale_to_z_score(inputs[key])\n",
        "        elif key in CAT_FEATURES:\n",
        "            outputs[key] = tft.compute_and_apply_vocabulary(\n",
        "                inputs[key],\n",
        "                num_oov_buckets=1,\n",
        "                vocab_filename=key,\n",
        "            )\n",
        "        else:\n",
        "            outputs[key] = inputs[key]\n",
        "        outputs[key] = tf.squeeze(outputs[key], -1)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def run_pipeline(args):\n",
        "    \"\"\"Runs a Beam pipeline to split the dataset\"\"\"\n",
        "\n",
        "    pipeline_options = beam.pipeline.PipelineOptions(flags=[], **args)\n",
        "\n",
        "    raw_data_query = args[\"raw_data_query\"]\n",
        "    exported_data_prefix = args[\"exported_data_prefix\"]\n",
        "    transformed_data_prefix = args[\"transformed_data_prefix\"]\n",
        "    transform_artifact_dir = args[\"transform_artifact_dir\"]\n",
        "    temp_location = args[\"temp_location\"]\n",
        "    project = args[\"project\"]\n",
        "\n",
        "    schema = tfdv.load_schema_text(SCHEMA_LOCATION)\n",
        "    feature_spec = tft.tf_metadata.schema_utils.schema_as_feature_spec(\n",
        "        schema\n",
        "    ).feature_spec\n",
        "\n",
        "    raw_metadata = tft.tf_metadata.dataset_metadata.DatasetMetadata(\n",
        "        tft.tf_metadata.schema_utils.schema_from_feature_spec(feature_spec)\n",
        "    )\n",
        "\n",
        "    with beam.Pipeline(options=pipeline_options) as pipeline:\n",
        "        with tft_beam.Context(temp_location):\n",
        "\n",
        "            # Read raw BigQuery data.\n",
        "            raw_train_data, raw_eval_data = (\n",
        "                pipeline\n",
        "                | \"Read Raw Data\"\n",
        "                >> beam.io.ReadFromBigQuery(\n",
        "                    query=raw_data_query,\n",
        "                    project=project,\n",
        "                    use_standard_sql=True,\n",
        "                )\n",
        "                | \"Parse Data\" >> beam.Map(parse_bq_record)\n",
        "                | \"Split\" >> beam.Partition(split_dataset, 2, ratio=[8, 2])\n",
        "            )\n",
        "\n",
        "            # Create a train_dataset from the data and schema.\n",
        "            raw_train_dataset = (raw_train_data, raw_metadata)\n",
        "\n",
        "            # Analyze and transform raw_train_dataset to produced transformed_train_dataset and transform_fn.\n",
        "            transformed_train_dataset, transform_fn = (\n",
        "                raw_train_dataset\n",
        "                | \"Analyze & Transform\"\n",
        "                >> tft_beam.AnalyzeAndTransformDataset(preprocessing_fn)\n",
        "            )\n",
        "\n",
        "            # Get data and schema separately from the transformed_dataset.\n",
        "            transformed_train_data, transformed_metadata = transformed_train_dataset\n",
        "\n",
        "            # write transformed train data.\n",
        "            _ = (\n",
        "                transformed_train_data\n",
        "                | \"Write Transformed Train Data\"\n",
        "                >> beam.io.tfrecordio.WriteToTFRecord(\n",
        "                    file_path_prefix=os.path.join(\n",
        "                        transformed_data_prefix, \"train/data\"\n",
        "                    ),\n",
        "                    file_name_suffix=\".gz\",\n",
        "                    coder=tft.coders.ExampleProtoCoder(transformed_metadata.schema),\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Create a eval_dataset from the data and schema.\n",
        "            raw_eval_dataset = (raw_eval_data, raw_metadata)\n",
        "\n",
        "            # Transform raw_eval_dataset to produced transformed_eval_dataset using transform_fn.\n",
        "            transformed_eval_dataset = (\n",
        "                raw_eval_dataset,\n",
        "                transform_fn,\n",
        "            ) | \"Transform\" >> tft_beam.TransformDataset()\n",
        "\n",
        "            # Get data from the transformed_eval_dataset.\n",
        "            transformed_eval_data, _ = transformed_eval_dataset\n",
        "\n",
        "            # write transformed eval data.\n",
        "            _ = (\n",
        "                transformed_eval_data\n",
        "                | \"Write Transformed Eval Data\"\n",
        "                >> beam.io.tfrecordio.WriteToTFRecord(\n",
        "                    file_path_prefix=os.path.join(transformed_data_prefix, \"eval/data\"),\n",
        "                    file_name_suffix=\".gz\",\n",
        "                    coder=tft.coders.ExampleProtoCoder(transformed_metadata.schema),\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Write transform_fn.\n",
        "            _ = transform_fn | \"Write Transform Artifacts\" >> tft_beam.WriteTransformFn(\n",
        "                transform_artifact_dir\n",
        "            )\n",
        "\n",
        "\n",
        "EXPORTED_DATA_PREFIX = os.path.join(BUCKET_NAME, \"exported_data\")\n",
        "TRANSFORMED_DATA_PREFIX = os.path.join(BUCKET_NAME, \"transformed_data\")\n",
        "TRANSFORM_ARTIFACTS_DIR = os.path.join(BUCKET_NAME, \"transformed_artifacts\")\n",
        "\n",
        "QUERY_STRING = \"SELECT * FROM {} LIMIT 500\".format(BQ_TABLE)\n",
        "JOB_NAME = \"chicago\" + TIMESTAMP\n",
        "\n",
        "args = {\n",
        "    \"runner\": \"DirectRunner\",\n",
        "    \"raw_data_query\": QUERY_STRING,\n",
        "    \"exported_data_prefix\": EXPORTED_DATA_PREFIX,\n",
        "    \"transformed_data_prefix\": TRANSFORMED_DATA_PREFIX,\n",
        "    \"transform_artifact_dir\": TRANSFORM_ARTIFACTS_DIR,\n",
        "    \"temp_location\": os.path.join(BUCKET_NAME, \"temp\"),\n",
        "    \"project\": PROJECT_ID,\n",
        "}\n",
        "\n",
        "print(\"Data preprocessing started...\")\n",
        "run_pipeline(args)\n",
        "print(\"Data preprocessing completed.\")\n",
        "\n",
        "! gsutil ls $TRANSFORMED_DATA_PREFIX/train\n",
        "! gsutil ls $TRANSFORMED_DATA_PREFIX/eval\n",
        "! gsutil ls $TRANSFORM_ARTIFACTS_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cleanup:mbsdk"
      },
      "source": [
        "# Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:\n",
        "\n",
        "- Dataset\n",
        "- Pipeline\n",
        "- Model\n",
        "- Endpoint\n",
        "- AutoML Training Job\n",
        "- Batch Job\n",
        "- Custom Job\n",
        "- Hyperparameter Tuning Job\n",
        "- Cloud Storage Bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup:mbsdk"
      },
      "outputs": [],
      "source": [
        "delete_all = True\n",
        "\n",
        "if delete_all:\n",
        "    # Delete the dataset using the Vertex dataset object\n",
        "    try:\n",
        "        if \"dataset\" in globals():\n",
        "            dataset.delete()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    # Delete the model using the Vertex model object\n",
        "    try:\n",
        "        if \"model\" in globals():\n",
        "            model.delete()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    # Delete the endpoint using the Vertex endpoint object\n",
        "    try:\n",
        "        if \"endpoint\" in globals():\n",
        "            endpoint.delete()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    # Delete the AutoML or Pipeline trainig job\n",
        "    try:\n",
        "        if \"dag\" in globals():\n",
        "            dag.delete()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    # Delete the custom trainig job\n",
        "    try:\n",
        "        if \"job\" in globals():\n",
        "            job.delete()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    # Delete the batch prediction job using the Vertex batch prediction object\n",
        "    try:\n",
        "        if \"batch_predict_job\" in globals():\n",
        "            batch_predict_job.delete()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    # Delete the hyperparameter tuning job using the Vertex hyperparameter tuning object\n",
        "    try:\n",
        "        if \"hpt_job\" in globals():\n",
        "            hpt_job.delete()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    if \"BUCKET_NAME\" in globals():\n",
        "        ! gsutil rm -r $BUCKET_NAME"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "mlops_data_management.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

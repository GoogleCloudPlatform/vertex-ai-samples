{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "copyright"
      },
      "outputs": [],
      "source": [
        "# Copyright 2021 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title:generic,gcp"
      },
      "source": [
        "# E2E ML on GCP: MLOps stage 1 : data management\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage1/mlops_data_management.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "        <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage1/mlops_data_management.ipynb\">\n",
        "        <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\\\" alt=\"Colab logo\"> Run in Colab\n",
        "        </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/ml_ops/stage1/mlops_data_management.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "<br/><br/><br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview:mlops"
      },
      "source": [
        "## Overview\n",
        "\n",
        "\n",
        "This tutorial demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. This tutorial covers stage 1 : data management."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "objective:mlops,stage1,tabular"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you create a MLOps stage 1: data management process.\n",
        "\n",
        "This tutorial uses the following Vertex AI and Data Analytics services:\n",
        "\n",
        "- `Vertex AI Datasets`\n",
        "- `BigQuery`\n",
        "- `Dataflow`\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Explore and visualize the data.\n",
        "- Create a Vertex AI `Dataset` resource from `BigQuery` table -- for AutoML training.\n",
        "- Extract a copy of the dataset to a CSV file in Cloud Storage.\n",
        "- Create a Vertex AI `Dataset` resource from CSV files -- alternative for AutoML training.\n",
        "- Read a sample of the `BigQuery` dataset into a dataframe.\n",
        "- Generate statistics and data schema using TensorFlow Data Validation from the samples in the dataframe.\n",
        "- Generate a TFRecord feature specification using TensorFlow Data Validation from the data schema.\n",
        "- Preprocess a portion of the BigQuery data using `Dataflow` -- for custom training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset:bq,chicago,lbn"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset used for this tutorial is the [Chicago Taxi](https://www.kaggle.com/chicago/chicago-taxi-trips-bq). The version of the dataset used in this tutorial is stored in a public BigQuery table. The trained model predicts whether someone leaves a tip for a taxi fare."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "recommendation:mlops,stage1,tabular"
      },
      "source": [
        "### Recommendations\n",
        "\n",
        "When doing E2E MLOps on Google Cloud for data management, the following best practices with structured (tabular) data are recommended:\n",
        "\n",
        " - For large amounts of data, use BigQuery table. Otherwise, use a CSV file stored in Cloud Storage.\n",
        " - When storing a large amount of data in CSV file, shard the data at 10,000 rows per shard.\n",
        " - Create a managed dataset with Vertex AI `TabularDataset`.\n",
        " - Preprocess the data with `Dataflow`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_mlops"
      },
      "source": [
        "## Installations\n",
        "\n",
        "Install *one time* the packages for executing the MLOps notebooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_mlops"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Vertex AI Workbench Notebook product has specific requirements\n",
        "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
        "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
        "    \"/opt/deeplearning/metadata/env_version\"\n",
        ")\n",
        "\n",
        "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_WORKBENCH_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\"\n",
        "\n",
        "ONCE_ONLY = True\n",
        "if ONCE_ONLY:\n",
        "    ! pip3 install -U tensorflow==2.5 $USER_FLAG\n",
        "    ! pip3 install -U tensorflow-data-validation==1.2 $USER_FLAG\n",
        "    ! pip3 install -U tensorflow-transform==1.2 $USER_FLAG\n",
        "    ! pip3 install -U tensorflow-io==0.18 $USER_FLAG\n",
        "    ! pip3 install --upgrade google-cloud-aiplatform[tensorboard] $USER_FLAG\n",
        "    ! pip3 install --upgrade google-cloud-pipeline-components $USER_FLAG\n",
        "    ! pip3 install --upgrade google-cloud-bigquery $USER_FLAG\n",
        "    ! pip3 install --upgrade google-cloud-logging $USER_FLAG\n",
        "    ! pip3 install --upgrade apache-beam[gcp]==2.33.0 $USER_FLAG\n",
        "    ! pip3 install --upgrade pyarrow $USER_FLAG\n",
        "    ! pip3 install --upgrade cloudml-hypertune $USER_FLAG\n",
        "    ! pip3 install --upgrade kfp $USER_FLAG\n",
        "    ! pip3 install future"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "restart"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "restart"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84cd83853240"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "1. [Enable the Vertex AI, BigQuery, Compute Engine and Cloud Storage APIs](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,bigquery,compute_component,storage_component).\n",
        "\n",
        "1. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
        "\n",
        "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "project_id"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_project_id"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_project_id"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID:\", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_gcloud_project_id"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
        "\n",
        "- Americas: `us-central1`\n",
        "- Europe: `europe-west4`\n",
        "- Asia Pacific: `asia-east1`\n",
        "\n",
        "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
        "\n",
        "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "region"
      },
      "outputs": [],
      "source": [
        "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
        "\n",
        "if REGION == \"[your-region]\":\n",
        "    REGION = \"us-central1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "timestamp"
      },
      "source": [
        "#### Timestamp\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "timestamp"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77c385f0db59"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using Vertex AI Workbench Notebooks**, your environment is already authenticated. Skip this step.\n",
        "\n",
        "**If you are using Colab**, run the cell below and follow the instructions when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "In the Cloud Console, go to the [Create service account key](https://console.cloud.google.com/apis/credentials/serviceaccountkey) page.\n",
        "\n",
        "1. **Click Create service account**.\n",
        "\n",
        "2. In the **Service account name** field, enter a name, and click **Create**.\n",
        "\n",
        "3. In the **Grant this service account access to project** section, click the Role drop-down list. Type \"Vertex AI\" into the filter box, and select **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "\n",
        "4. Click Create. A JSON file that contains your key downloads to your local environment.\n",
        "\n",
        "5. Enter the path to your service account key as the GOOGLE_APPLICATION_CREDENTIALS variable in the cell below and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "535223fa4b84"
      },
      "outputs": [],
      "source": [
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# If on Vertex AI Workbench, then don't execute this code\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
        "    \"DL_ANACONDA_HOME\"\n",
        "):\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bucket:custom"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "When you submit a custom training job using the Vertex SDK, you upload a Python package\n",
        "containing your training code to a Cloud Storage bucket. Vertex AI runs\n",
        "the code from this package. In this tutorial, Vertex AI also saves the\n",
        "trained model that results from your job in the same bucket. You can then\n",
        "create an `Endpoint` resource based on this output in order to serve\n",
        "online predictions.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bucket"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_bucket"
      },
      "outputs": [],
      "source": [
        "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
        "    BUCKET_NAME = PROJECT_ID + \"aip-\" + TIMESTAMP\n",
        "    BUCKET_URI = \"gs://\" + BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_bucket"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "validate_bucket"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "validate_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_vars"
      },
      "source": [
        "### Set up variables\n",
        "\n",
        "Next, set up some variables used throughout the tutorial.\n",
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "import google.cloud.aiplatform as aip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_bq"
      },
      "source": [
        "#### Import BigQuery\n",
        "\n",
        "Import the BigQuery package into your Python environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_bq"
      },
      "outputs": [],
      "source": [
        "from google.cloud import bigquery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_beam"
      },
      "source": [
        "#### Import Apache Beam\n",
        "\n",
        "Import the Apache Beam package into your Python environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_beam"
      },
      "outputs": [],
      "source": [
        "import apache_beam as beam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_tf"
      },
      "source": [
        "#### Import TensorFlow\n",
        "\n",
        "Import the TensorFlow package into your Python environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_tf"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_tfdv"
      },
      "source": [
        "#### Import TensorFlow Data Validation\n",
        "\n",
        "Import the TensorFlow Data Validation (TFDV) package into your Python environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_tfdv"
      },
      "outputs": [],
      "source": [
        "import tensorflow_data_validation as tfdv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_tft"
      },
      "source": [
        "#### Import TensorFlow Transform\n",
        "\n",
        "Import the TensorFlow Transform (TFT) package into your Python environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_tft"
      },
      "outputs": [],
      "source": [
        "import tensorflow_transform as tft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_bq"
      },
      "source": [
        "### Create BigQuery client\n",
        "\n",
        "Create the BigQuery client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_bq"
      },
      "outputs": [],
      "source": [
        "bqclient = bigquery.Client(project=PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "recommendation:dataset,tabular"
      },
      "source": [
        "## Datasets\n",
        "\n",
        "Next, you look at options for creating managed datasets:\n",
        "\n",
        "* `BigQuery`: Create a Vertex `TabularDataset` resource.\n",
        "* `CSV`: Create a Vertex `TabularDataset` resource.\n",
        "* `TFRecords`: Self-manage the dataset on Cloud Storage storage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_file:u_dataset,bq"
      },
      "source": [
        "#### Location of BigQuery training data.\n",
        "\n",
        "Now set the variable `IMPORT_FILE` to the location of the data table in BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_file:chicago,bq,lbn"
      },
      "outputs": [],
      "source": [
        "IMPORT_FILE = \"bq://bigquery-public-data.chicago_taxi_trips.taxi_trips\"\n",
        "BQ_TABLE = \"bigquery-public-data.chicago_taxi_trips.taxi_trips\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "explore_bq:chicago"
      },
      "source": [
        "### Explore BigQuery dataset\n",
        "\n",
        "Explore the contents of the BigQuery table:\n",
        "\n",
        "- Get all examples from 2015\n",
        "- Sort by the day of the week\n",
        "- Count the number of examples for each day of the week"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "explore_bq:chicago"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"SELECT\n",
        "    CAST(EXTRACT(DAYOFWEEK FROM trip_start_timestamp) AS string) AS trip_dayofweek,\n",
        "    FORMAT_DATE('%A',cast(trip_start_timestamp as date)) AS trip_dayname,\n",
        "    COUNT(*) as trip_count,\n",
        "FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
        "WHERE\n",
        "    EXTRACT(YEAR FROM trip_start_timestamp) = 2015\n",
        "GROUP BY\n",
        "    trip_dayofweek,\n",
        "    trip_dayname\n",
        "ORDER BY\n",
        "    trip_dayofweek\"\"\"\n",
        "\n",
        "_ = bqclient.query(query)\n",
        "rows = _.result()\n",
        "dataframe = rows.to_dataframe()\n",
        "print(dataframe.head(7))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot_bq:chicago"
      },
      "outputs": [],
      "source": [
        "dataframe.plot(kind=\"bar\", x=\"trip_dayname\", y=\"trip_count\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq_copy:chicago"
      },
      "source": [
        "### Make a private copy of subset of BigQuery table\n",
        "\n",
        "Next, you make a private copy of the BigQuery table:\n",
        "- Select a subset of columns\n",
        "- Select a subset of rows (LIMIT)\n",
        "- Set conditions (WHERE)\n",
        "- Do feature engineering on geolocation coords.\n",
        "- Pre-split the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bq_copy:chicago"
      },
      "outputs": [],
      "source": [
        "BQ_DATASET = BQ_TABLE.split(\".\")[1]\n",
        "BQ_TABLE_COPY = f\"{PROJECT_ID}.{BQ_DATASET}.taxi_trips\"\n",
        "LIMIT = 300000\n",
        "YEAR = 2020\n",
        "\n",
        "# First, create the dataset entry\n",
        "dataset = bigquery.Dataset(f\"{PROJECT_ID}.{BQ_DATASET}\")\n",
        "dataset.location = \"US\"\n",
        "dataset = bqclient.create_dataset(dataset, timeout=30)\n",
        "\n",
        "query = f\"\"\"\n",
        "CREATE OR REPLACE TABLE `{BQ_TABLE_COPY}`\n",
        "AS (\n",
        "    WITH\n",
        "      taxitrips AS (\n",
        "      SELECT\n",
        "        trip_start_timestamp,\n",
        "        trip_seconds,\n",
        "        trip_miles,\n",
        "        payment_type,\n",
        "        pickup_longitude,\n",
        "        pickup_latitude,\n",
        "        dropoff_longitude,\n",
        "        dropoff_latitude,\n",
        "        tips,\n",
        "        fare\n",
        "      FROM\n",
        "        `{BQ_TABLE}`\n",
        "      WHERE pickup_longitude IS NOT NULL\n",
        "      AND pickup_latitude IS NOT NULL\n",
        "      AND dropoff_longitude IS NOT NULL\n",
        "      AND dropoff_latitude IS NOT NULL\n",
        "      AND trip_miles > 0\n",
        "      AND trip_seconds > 0\n",
        "      AND fare > 0\n",
        "      AND EXTRACT(YEAR FROM trip_start_timestamp) = {YEAR}\n",
        "    )\n",
        "\n",
        "    SELECT\n",
        "      EXTRACT(MONTH from trip_start_timestamp) as trip_month,\n",
        "      EXTRACT(DAY from trip_start_timestamp) as trip_day,\n",
        "      EXTRACT(DAYOFWEEK from trip_start_timestamp) as trip_day_of_week,\n",
        "      EXTRACT(HOUR from trip_start_timestamp) as trip_hour,\n",
        "      CAST(trip_seconds AS FLOAT64) as trip_seconds,\n",
        "      trip_miles,\n",
        "      payment_type,\n",
        "      ST_AsText(\n",
        "          ST_SnapToGrid(ST_GeogPoint(pickup_longitude, pickup_latitude), 0.1)\n",
        "      ) AS pickup_grid,\n",
        "      ST_AsText(\n",
        "          ST_SnapToGrid(ST_GeogPoint(dropoff_longitude, dropoff_latitude), 0.1)\n",
        "      ) AS dropoff_grid,\n",
        "      ST_Distance(\n",
        "          ST_GeogPoint(pickup_longitude, pickup_latitude),\n",
        "          ST_GeogPoint(dropoff_longitude, dropoff_latitude)\n",
        "      ) AS euclidean,\n",
        "      CONCAT(\n",
        "          ST_AsText(ST_SnapToGrid(ST_GeogPoint(pickup_longitude,\n",
        "              pickup_latitude), 0.1)),\n",
        "          ST_AsText(ST_SnapToGrid(ST_GeogPoint(dropoff_longitude,\n",
        "              dropoff_latitude), 0.1))\n",
        "      ) AS loc_cross,\n",
        "      IF((tips/fare >= 0.2), 1, 0) AS tip_bin,\n",
        "    FROM\n",
        "      taxitrips\n",
        "    LIMIT {LIMIT}\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "response = bqclient.query(query)\n",
        "_ = response.result()\n",
        "\n",
        "BQ_TABLE = BQ_TABLE_COPY\n",
        "IMPORT_FILE = f\"bq://{BQ_TABLE_COPY}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_dataset:tabular,bq,lbn"
      },
      "source": [
        "### Create the Dataset\n",
        "\n",
        "#### BigQuery input data\n",
        "\n",
        "Next, create the `Dataset` resource using the `create` method for the `TabularDataset` class, which takes the following parameters:\n",
        "\n",
        "- `display_name`: The human readable name for the `Dataset` resource.\n",
        "- `bq_source`: Import data items from a BigQuery table into the `Dataset` resource.\n",
        "- `labels`: User defined metadata. In this example, you store the location of the Cloud Storage bucket containing the user defined data.\n",
        "\n",
        "Learn more about [TabularDataset from BigQuery table](https://cloud.google.com/vertex-ai/docs/datasets/create-dataset-api#aiplatform_create_dataset_tabular_bigquery_sample-python)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_dataset:tabular,bq,lbn"
      },
      "outputs": [],
      "source": [
        "dataset = aip.TabularDataset.create(\n",
        "    display_name=\"Chicago Taxi\" + \"_\" + TIMESTAMP,\n",
        "    bq_source=[IMPORT_FILE],\n",
        "    labels={\"user_metadata\": BUCKET_NAME},\n",
        ")\n",
        "\n",
        "label_column = \"tip_bin\"\n",
        "\n",
        "print(dataset.resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq_to_dataframe:all"
      },
      "source": [
        "### Read the BigQuery dataset into a pandas dataframe\n",
        "\n",
        "Next, you read a sample of the dataset into a pandas dataframe using BigQuery `list_rows()` and `to_dataframe()` method, as follows:\n",
        "\n",
        "- `list_rows()`: Performs a query on the specified table and returns a row iterator to the query results. Optionally specify:\n",
        " - `selected_fields`: Subset of fields (columns) to return.\n",
        " - `max_results`: The maximum number of rows to return. Same as SQL LIMIT command.\n",
        "\n",
        "\n",
        "- `rows.to_dataframe()`: Invokes the row iterator and reads in the data into a pandas dataframe.\n",
        "\n",
        "Learn more about [Loading BigQuery table into a dataframe](https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bq_to_dataframe:all"
      },
      "outputs": [],
      "source": [
        "# Download a table.\n",
        "table = bigquery.TableReference.from_string(BQ_TABLE)\n",
        "\n",
        "rows = bqclient.list_rows(table, max_results=300000)\n",
        "\n",
        "dataframe = rows.to_dataframe()\n",
        "print(dataframe.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfdv_stats:dataframe"
      },
      "source": [
        "###  Generate dataset statistics\n",
        "\n",
        "#### Dataframe input data\n",
        "\n",
        "Generate statistics on the dataset with the TensorFlow Data Validation (TFDV) package. Use the `generate_statistics_from_dataframe()` method, with the following parameters:\n",
        "\n",
        "- `dataframe`: The dataset in an in-memory pandas dataframe.\n",
        "- `stats_options`: The selected statistics options:\n",
        "  - `label_feature`: The column which is the label to predict.\n",
        "  - `sample_rate`: The sampling rate. If specified, statistics is computed over the sample.\n",
        "  - `num_top_values`: number of most frequent feature values to keep for string features.\n",
        "\n",
        "Learn about [TensorFlow Data Validation (TFDV)](https://www.tensorflow.org/tfx/data_validation/get_started)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfdv_stats:dataframe"
      },
      "outputs": [],
      "source": [
        "stats = tfdv.generate_statistics_from_dataframe(\n",
        "    dataframe=dataframe,\n",
        "    stats_options=tfdv.StatsOptions(\n",
        "        label_feature=\"tip_bin\", sample_rate=1, num_top_values=50\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfdv_visualize_stats"
      },
      "source": [
        "### Visualize dataset statistics\n",
        "\n",
        "A visualization of the dataset statistics can be displayed using the TFDV `visualize_statistics()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfdv_visualize_stats"
      },
      "outputs": [],
      "source": [
        "tfdv.visualize_statistics(stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stats_extract_features"
      },
      "source": [
        "### Extract feature grouping from statistics\n",
        "\n",
        "Next, you extract from the statistics the feature names and data types, from which you group features into:\n",
        "\n",
        "- numeric: float\n",
        "- categorical: string, int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stats_extract_features"
      },
      "outputs": [],
      "source": [
        "NUMERIC_FEATURES = []\n",
        "CATEGORICAL_FEATURES = []\n",
        "for _ in range(len(stats.datasets[0].features)):\n",
        "    if stats.datasets[0].features[_].path.step[0] == label_column:\n",
        "        continue\n",
        "    if stats.datasets[0].features[_].type == 0:  # int\n",
        "        CATEGORICAL_FEATURES.append(stats.datasets[0].features[_].path.step[0])\n",
        "    elif stats.datasets[0].features[_].type == 1:  # float\n",
        "        NUMERIC_FEATURES.append(stats.datasets[0].features[_].path.step[0])\n",
        "    elif stats.datasets[0].features[_].type == 2:  # string\n",
        "        CATEGORICAL_FEATURES.append(stats.datasets[0].features[_].path.step[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "retain_feature_info:tabular"
      },
      "source": [
        "### Retain feature column information\n",
        "\n",
        "Next, you retain information on the feature columns in the dataset. In this example, you add this user-defined metadata as a JSON file to the Cloud Storage bucket you associated with the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "retain_feature_info:tabular"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "metadata = {\n",
        "    \"label_column\": label_column,\n",
        "    \"numeric_features\": NUMERIC_FEATURES,\n",
        "    \"categorical_features\": CATEGORICAL_FEATURES,\n",
        "}\n",
        "with tf.io.gfile.GFile(\n",
        "    \"gs://\" + dataset.labels[\"user_metadata\"] + \"/metadata.jsonl\", \"w\"\n",
        ") as f:\n",
        "    json.dump(metadata, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfdv_stats:write"
      },
      "source": [
        "#### Retain statistics for the dataset\n",
        "\n",
        "Next, you write the statistics for the dataset to the dataset's Cloud Storage bucket, and retain the Cloud Storage location of the statistics file. In this example, you add it to the user-defined metadata for this dataset, which is stored in the dataset's Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfdv_stats:write"
      },
      "outputs": [],
      "source": [
        "STATISTICS_SCHEMA = BUCKET_URI + \"/statistics.jsonl\"\n",
        "\n",
        "tfdv.write_stats_text(stats, BUCKET_URI + \"/statistics.jsonl\")\n",
        "\n",
        "with tf.io.gfile.GFile(\n",
        "    \"gs://\" + dataset.labels[\"user_metadata\"] + \"/metadata.jsonl\", \"r\"\n",
        ") as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "metadata[\"statistics\"] = STATISTICS_SCHEMA\n",
        "with tf.io.gfile.GFile(\n",
        "    \"gs://\" + dataset.labels[\"user_metadata\"] + \"/metadata.jsonl\", \"w\"\n",
        ") as f:\n",
        "    json.dump(metadata, f)\n",
        "\n",
        "! gsutil cat $BUCKET_URI/metadata.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfdv_schema"
      },
      "source": [
        "###  Generate the raw data schema\n",
        "\n",
        "Generate the data schema on the dataset with the TensorFlow Data Validation (TFDV) package. Use the `infer_schema()` method, with the following parameters:\n",
        "\n",
        "- `statistics`: The statistics generated by TFDV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfdv_schema"
      },
      "outputs": [],
      "source": [
        "schema = tfdv.infer_schema(statistics=stats)\n",
        "print(schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfdv_schema:save"
      },
      "source": [
        "#### Save schema for the dataset to Cloud Storage\n",
        "\n",
        "Next, you write the schema for the dataset to the dataset's Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfdv_schema:save"
      },
      "outputs": [],
      "source": [
        "SCHEMA_LOCATION = BUCKET_URI + \"/schema.txt\"\n",
        "\n",
        "# When running Apache Beam directly (file is directly accessed)\n",
        "tfdv.write_schema_text(output_path=SCHEMA_LOCATION, schema=schema)\n",
        "# When running with Dataflow (file is uploaded to worker pool)\n",
        "tfdv.write_schema_text(output_path=\"schema.txt\", schema=schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfdv_schema:write"
      },
      "source": [
        "#### Retain schema for the dataset\n",
        "\n",
        "Next, you retain the Cloud Storage location of the schema file. In this example, you add it to the user-defined metadata for this dataset, which is stored in the dataset's Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfdv_schema:write"
      },
      "outputs": [],
      "source": [
        "with tf.io.gfile.GFile(\n",
        "    \"gs://\" + dataset.labels[\"user_metadata\"] + \"/metadata.jsonl\", \"r\"\n",
        ") as f:\n",
        "    metadata = json.load(f)\n",
        "metadata[\"schema\"] = SCHEMA_LOCATION\n",
        "\n",
        "with tf.io.gfile.GFile(\n",
        "    \"gs://\" + dataset.labels[\"user_metadata\"] + \"/metadata.jsonl\", \"w\"\n",
        ") as f:\n",
        "    json.dump(metadata, f)\n",
        "\n",
        "! gsutil cat $BUCKET_URI/metadata.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tft_feature_spec"
      },
      "source": [
        "### Generate the feature specification\n",
        "\n",
        "Generate the feature specification, compatible with TFRecords, on the dataset with the TensorFlow Transform (TFT) package. Use the `schema_as_feature_spec()` method, with the following parameters:\n",
        "\n",
        "- `schema`: The data schema generated by TFDV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tft_feature_spec"
      },
      "outputs": [],
      "source": [
        "feature_spec = tft.tf_metadata.schema_utils.schema_as_feature_spec(schema).feature_spec\n",
        "\n",
        "print(feature_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataflow_setup:transform"
      },
      "source": [
        "#### Prepare package requirements for Dataflow job.\n",
        "\n",
        "Before you can run a Dataflow job, you need to specify the package requirements for the worker pool that will execute the job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataflow_setup:transform"
      },
      "outputs": [],
      "source": [
        "%%writefile setup.py\n",
        "import setuptools\n",
        "\n",
        "REQUIRED_PACKAGES = [\n",
        "    \"google-cloud-aiplatform\",\n",
        "    \"tensorflow-transform==1.2.0\",\n",
        "    \"tensorflow-data-validation==1.2.0\",\n",
        "]\n",
        "\n",
        "setuptools.setup(\n",
        "    name=\"executor\",\n",
        "    version=\"0.0.1\",\n",
        "    install_requires=REQUIRED_PACKAGES,\n",
        "    packages=setuptools.find_packages(),\n",
        "    include_package_data=True,\n",
        "    package_data={\"./\": [\"schema.txt\"]}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataflow:preprocess,chicago"
      },
      "source": [
        "#### Create preprocessing function\n",
        "\n",
        "Next, you create a preprocessing function specific to your dataset. In this example, you write the preprocessing function to a separate python module and add a __init__.py to make it appear as a package. Why? When you run the Apache beam pipeline in Dataflow, your scripts are ran across one or more workers. The preprocessing function runs in a separate worker than the pipeline, and thus does not contain the run-time of the pipeline, like values of global variables. To resolve this, you hard-code all the dependencies and values into the preprocesing package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataflow:preprocess,chicago"
      },
      "outputs": [],
      "source": [
        "! rm -rf src\n",
        "! mkdir src\n",
        "! touch src/__init__.py\n",
        "\n",
        "with open(\"src/features.py\", \"w\") as f:\n",
        "    f.write(\"import tensorflow as tf\\n\")\n",
        "    f.write(\"import tensorflow_transform as tft\\n\")\n",
        "\n",
        "    f.write(\"def preprocessing_fn(inputs):\\n\")\n",
        "    f.write(\"\toutputs = {}\\n\")\n",
        "    f.write(\"\tfor key in inputs.keys():\\n\")\n",
        "    f.write(f\"\t\tif key in {NUMERIC_FEATURES}:\\n\")\n",
        "    f.write(\"\t\t\toutputs[key] = tft.scale_to_z_score(inputs[key])\\n\")\n",
        "    f.write(f\"\t\telif key in {CATEGORICAL_FEATURES}:\\n\")\n",
        "    f.write(\"\t\t\toutputs[key] = tft.compute_and_apply_vocabulary(\\n\")\n",
        "    f.write(\"\t\t\t\tinputs[key],\\n\")\n",
        "    f.write(\"\t\t\t\tnum_oov_buckets=1,\\n\")\n",
        "    f.write(\"\t\t\t\tvocab_filename=key,\\n\")\n",
        "    f.write(\"\t\t\t)\\n\")\n",
        "    f.write(\"\t\telse:\\n\")\n",
        "    f.write(\"\t\t\toutputs[key] = inputs[key]\\n\")\n",
        "    f.write(\"\t\toutputs[key] = tf.squeeze(outputs[key], -1)\\n\")\n",
        "    f.write(\"\treturn outputs\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataflow:transform,bq,chicago"
      },
      "source": [
        "### Preprocess data with Dataflow\n",
        "\n",
        "#### Data Preprocessing\n",
        "\n",
        "Next, you preprocess the data using Dataflow. In this example, you query the BigQuery table and split the examples into training, validation and test (eval) datasets and preprocess feature columns:\n",
        "\n",
        "- `Numeric`: Rescale the values with `tft.scale_to_z_score`.\n",
        "- `Categorical`: Encode as a categorical column with `tft.compute_and_apply_vocabulary`.\n",
        "\n",
        "In addition to the preprocessed (transformed) data, raw versions of the test data are generated in both tf.Example and JSONL format. The tranform artifacts are stored as well to be used by subsequent serving function for transforming raw data into transformed data.\n",
        "\n",
        "In summary, the outputs produced are:\n",
        "\n",
        "- transformed training data\n",
        "- transformed validation data\n",
        "- tranformed test dats\n",
        "- raw test data as JSONL\n",
        "- raw test data as tf.Example\n",
        "- transform function artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataflow:transform,bq,chicago"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import tensorflow_transform.beam as tft_beam\n",
        "from src import features\n",
        "\n",
        "RUNNER = \"DataflowRunner\"  # DirectRunner for local running w/o Dataflow\n",
        "\n",
        "\n",
        "def parse_bq_record(bq_record):\n",
        "    \"\"\"Parses a bq_record to a dictionary.\"\"\"\n",
        "    output = {}\n",
        "    for key in bq_record:\n",
        "        output[key] = [bq_record[key]]\n",
        "    return output\n",
        "\n",
        "\n",
        "def split_dataset(bq_row, num_partitions, ratio):\n",
        "    \"\"\"Returns a partition number for a given bq_row.\"\"\"\n",
        "    import json\n",
        "\n",
        "    assert num_partitions == len(ratio)\n",
        "    bucket = sum(map(ord, json.dumps(bq_row))) % sum(ratio)\n",
        "    total = 0\n",
        "    for i, part in enumerate(ratio):\n",
        "        total += part\n",
        "        if bucket < total:\n",
        "            return i\n",
        "    return len(ratio) - 1\n",
        "\n",
        "\n",
        "def convert_to_jsonl(data, label=None):\n",
        "    \"\"\"Converts a parsed record to JSON\"\"\"\n",
        "    import json\n",
        "\n",
        "    if label:\n",
        "        del data[label]\n",
        "    return json.dumps(data)\n",
        "\n",
        "\n",
        "def run_pipeline(args):\n",
        "    \"\"\"Runs a Beam pipeline to split the dataset\"\"\"\n",
        "\n",
        "    pipeline_options = beam.pipeline.PipelineOptions(flags=[], **args)\n",
        "\n",
        "    raw_data_query = args[\"raw_data_query\"]\n",
        "    label = args[\"label\"]\n",
        "    transformed_data_prefix = args[\"transformed_data_prefix\"]\n",
        "    transform_artifact_dir = args[\"transform_artifact_dir\"]\n",
        "    exported_jsonl_prefix = args[\"exported_jsonl_prefix\"]\n",
        "    exported_tfrec_prefix = args[\"exported_tfrec_prefix\"]\n",
        "    temp_location = args[\"temp_location\"]\n",
        "    project = args[\"project\"]\n",
        "\n",
        "    schema = tfdv.load_schema_text(SCHEMA_LOCATION)\n",
        "    feature_spec = tft.tf_metadata.schema_utils.schema_as_feature_spec(\n",
        "        schema\n",
        "    ).feature_spec\n",
        "\n",
        "    raw_metadata = tft.tf_metadata.dataset_metadata.DatasetMetadata(\n",
        "        tft.tf_metadata.schema_utils.schema_from_feature_spec(feature_spec)\n",
        "    )\n",
        "\n",
        "    with beam.Pipeline(options=pipeline_options) as pipeline:\n",
        "        with tft_beam.Context(temp_location):\n",
        "\n",
        "            # Read raw BigQuery data.\n",
        "            raw_train_data, raw_val_data, raw_test_data = (\n",
        "                pipeline\n",
        "                | \"Read Raw Data\"\n",
        "                >> beam.io.ReadFromBigQuery(\n",
        "                    query=raw_data_query,\n",
        "                    project=project,\n",
        "                    use_standard_sql=True,\n",
        "                )\n",
        "                | \"Parse Data\" >> beam.Map(parse_bq_record)\n",
        "                | \"Split\" >> beam.Partition(split_dataset, 3, ratio=[8, 1, 1])\n",
        "            )\n",
        "\n",
        "            # Create a train_dataset from the data and schema.\n",
        "            raw_train_dataset = (raw_train_data, raw_metadata)\n",
        "\n",
        "            # Analyze and transform raw_train_dataset to produced transformed_train_dataset and transform_fn.\n",
        "            transformed_train_dataset, transform_fn = (\n",
        "                raw_train_dataset\n",
        "                | \"Analyze & Transform\"\n",
        "                >> tft_beam.AnalyzeAndTransformDataset(features.preprocessing_fn)\n",
        "            )\n",
        "\n",
        "            # Get data and schema separately from the transformed_dataset.\n",
        "            transformed_train_data, transformed_metadata = transformed_train_dataset\n",
        "\n",
        "            # write transformed train data.\n",
        "            _ = (\n",
        "                transformed_train_data\n",
        "                | \"Write Transformed Train Data\"\n",
        "                >> beam.io.tfrecordio.WriteToTFRecord(\n",
        "                    file_path_prefix=os.path.join(\n",
        "                        transformed_data_prefix, \"train/data\"\n",
        "                    ),\n",
        "                    file_name_suffix=\".gz\",\n",
        "                    coder=tft.coders.ExampleProtoCoder(transformed_metadata.schema),\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Create a val_dataset from the data and schema.\n",
        "            raw_val_dataset = (raw_val_data, raw_metadata)\n",
        "\n",
        "            # Transform raw_val_dataset to produced transformed_val_dataset using transform_fn.\n",
        "            transformed_val_dataset = (\n",
        "                raw_val_dataset,\n",
        "                transform_fn,\n",
        "            ) | \"Transform Validation Data\" >> tft_beam.TransformDataset()\n",
        "\n",
        "            # Get data from the transformed_val_dataset.\n",
        "            transformed_val_data, _ = transformed_val_dataset\n",
        "\n",
        "            # write transformed val data.\n",
        "            _ = (\n",
        "                transformed_val_data\n",
        "                | \"Write Transformed Validation Data\"\n",
        "                >> beam.io.tfrecordio.WriteToTFRecord(\n",
        "                    file_path_prefix=os.path.join(transformed_data_prefix, \"val/data\"),\n",
        "                    file_name_suffix=\".gz\",\n",
        "                    coder=tft.coders.ExampleProtoCoder(transformed_metadata.schema),\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Create a test_dataset from the data and schema.\n",
        "            raw_test_dataset = (raw_test_data, raw_metadata)\n",
        "\n",
        "            # Transform raw_test_dataset to produced transformed_test_dataset using transform_fn.\n",
        "            transformed_test_dataset = (\n",
        "                raw_test_dataset,\n",
        "                transform_fn,\n",
        "            ) | \"Transform Test Data\" >> tft_beam.TransformDataset()\n",
        "\n",
        "            # Get data from the transformed_test_dataset.\n",
        "            transformed_test_data, _ = transformed_test_dataset\n",
        "\n",
        "            # write transformed test data.\n",
        "            _ = (\n",
        "                transformed_test_data\n",
        "                | \"Write Transformed Test Data\"\n",
        "                >> beam.io.tfrecordio.WriteToTFRecord(\n",
        "                    file_path_prefix=os.path.join(transformed_data_prefix, \"test/data\"),\n",
        "                    file_name_suffix=\".gz\",\n",
        "                    coder=tft.coders.ExampleProtoCoder(transformed_metadata.schema),\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Write transform_fn.\n",
        "            _ = transform_fn | \"Write Transform Artifacts\" >> tft_beam.WriteTransformFn(\n",
        "                transform_artifact_dir\n",
        "            )\n",
        "\n",
        "            # Write raw test data to GCS as TF Records\n",
        "            _ = (\n",
        "                raw_test_data\n",
        "                | \"Write TF Test Data\"\n",
        "                >> beam.io.tfrecordio.WriteToTFRecord(\n",
        "                    file_path_prefix=os.path.join(exported_tfrec_prefix, \"data\"),\n",
        "                    file_name_suffix=\".tfrecord\",\n",
        "                    coder=tft.coders.ExampleProtoCoder(raw_metadata.schema),\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Convert raw test data to JSON (for batch prediction)\n",
        "            json_test_data = (raw_test_data) | \"Convert Batch Test Data\" >> beam.Map(\n",
        "                convert_to_jsonl, label=label\n",
        "            )\n",
        "\n",
        "            # Write raw test data to GCS as JSONL files.\n",
        "            _ = json_test_data | \"Write JSONL Test Data\" >> beam.io.WriteToText(\n",
        "                file_path_prefix=exported_jsonl_prefix, file_name_suffix=\".jsonl\"\n",
        "            )\n",
        "\n",
        "\n",
        "EXPORTED_JSONL_PREFIX = os.path.join(BUCKET_URI, \"exported_data/jsonl\")\n",
        "EXPORTED_TFREC_PREFIX = os.path.join(BUCKET_URI, \"exported_data/tfrec\")\n",
        "TRANSFORMED_DATA_PREFIX = os.path.join(BUCKET_URI, \"transformed_data\")\n",
        "TRANSFORM_ARTIFACTS_DIR = os.path.join(BUCKET_URI, \"transformed_artifacts\")\n",
        "\n",
        "QUERY_STRING = \"SELECT * FROM {} LIMIT 300000\".format(BQ_TABLE)\n",
        "JOB_NAME = \"chicago\" + TIMESTAMP\n",
        "\n",
        "args = {\n",
        "    \"runner\": RUNNER,\n",
        "    \"raw_data_query\": QUERY_STRING,\n",
        "    \"label\": label_column,\n",
        "    \"transformed_data_prefix\": TRANSFORMED_DATA_PREFIX,\n",
        "    \"transform_artifact_dir\": TRANSFORM_ARTIFACTS_DIR,\n",
        "    \"exported_jsonl_prefix\": EXPORTED_JSONL_PREFIX,\n",
        "    \"exported_tfrec_prefix\": EXPORTED_TFREC_PREFIX,\n",
        "    \"temp_location\": os.path.join(BUCKET_URI, \"temp\"),\n",
        "    \"project\": PROJECT_ID,\n",
        "    \"region\": REGION,\n",
        "    \"setup_file\": \"./setup.py\",\n",
        "}\n",
        "\n",
        "print(\"Data preprocessing started...\")\n",
        "run_pipeline(args)\n",
        "print(\"Data preprocessing completed.\")\n",
        "\n",
        "! gsutil ls $TRANSFORMED_DATA_PREFIX/train\n",
        "! gsutil ls $TRANSFORMED_DATA_PREFIX/val\n",
        "! gsutil ls $TRANSFORMED_DATA_PREFIX/test\n",
        "! gsutil ls $TRANSFORM_ARTIFACTS_DIR\n",
        "! gsutil ls {EXPORTED_JSONL_PREFIX}*\n",
        "! gsutil ls $EXPORTED_TFREC_PREFIX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataflow:transform,retain"
      },
      "source": [
        "#### Retain transformed dataset references\n",
        "\n",
        "Next, you retain the Cloud Storage location of the transformed data with the dataset. In this example, you add it to the user-defined metadata for this dataset, which is stored in the dataset's Cloud Storage bucket.\n",
        "\n",
        "During the transformation of the data, the transformat function calculated the number of unique occurrences per feature. Some of the categorical values (string, int) may have a large number of unique values. In this case, its better to reduce their dimensionality by moving them from being categorical to embedding feature.\n",
        "\n",
        "The code uses a rule-of-thumb that the embedding size should be the sqrt() of the number of unique values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataflow:transform,retain"
      },
      "outputs": [],
      "source": [
        "with tf.io.gfile.GFile(\n",
        "    \"gs://\" + dataset.labels[\"user_metadata\"] + \"/metadata.jsonl\", \"r\"\n",
        ") as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "tft_output = tft.TFTransformOutput(TRANSFORM_ARTIFACTS_DIR)\n",
        "\n",
        "CATEGORICAL_FEATURES = []\n",
        "EMBEDDING_FEATURES = []\n",
        "categorical_features = metadata[\"categorical_features\"]\n",
        "for feature in categorical_features:\n",
        "    unique = tft_output.vocabulary_size_by_name(feature)\n",
        "    if unique > 10:\n",
        "        EMBEDDING_FEATURES.append(feature)\n",
        "        print(\"Convert to embedding\", feature, unique)\n",
        "    else:\n",
        "        CATEGORICAL_FEATURES.append(feature)\n",
        "\n",
        "metadata[\"categorical_features\"] = CATEGORICAL_FEATURES\n",
        "metadata[\"embedding_features\"] = EMBEDDING_FEATURES\n",
        "\n",
        "metadata[\"transformed_data_prefix\"] = TRANSFORMED_DATA_PREFIX\n",
        "metadata[\"transform_artifacts_dir\"] = TRANSFORM_ARTIFACTS_DIR\n",
        "metadata[\"exported_jsonl_prefix\"] = EXPORTED_JSONL_PREFIX\n",
        "metadata[\"exported_tfrec_prefix\"] = EXPORTED_TFREC_PREFIX\n",
        "with tf.io.gfile.GFile(\n",
        "    \"gs://\" + dataset.labels[\"user_metadata\"] + \"/metadata.jsonl\", \"w\"\n",
        ") as f:\n",
        "    json.dump(metadata, f)\n",
        "\n",
        "! gsutil cat $BUCKET_URI/metadata.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cleanup:mbsdk"
      },
      "source": [
        "# Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial.\n",
        "\n",
        "*Note:* stage2/mlops_experimentation is dependent on the resources created by this stage1 notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup:stage1"
      },
      "outputs": [],
      "source": [
        "delete_all = False\n",
        "\n",
        "if delete_all:\n",
        "    # Delete the dataset using the Vertex dataset object\n",
        "    try:\n",
        "        if \"dataset\" in globals():\n",
        "            dataset.delete()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    if \"BUCKET_URI\" in globals():\n",
        "        ! gsutil rm -r $BUCKET_URI"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "mlops_data_management.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

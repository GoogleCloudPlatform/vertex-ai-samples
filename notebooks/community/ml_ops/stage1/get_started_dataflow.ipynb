{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "copyright"
      },
      "outputs": [],
      "source": [
        "# Copyright 2021 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title:generic,gcp"
      },
      "source": [
        "# E2E ML on GCP: MLOps stage 1 : data management: get started with Dataflow\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage1/get_started_dataflow.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/ai/platform/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage1/get_started_dataflow.ipynb\">\n",
        "      Open in Google Cloud Notebooks\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "<br/><br/><br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview:mlops"
      },
      "source": [
        "## Overview\n",
        "\n",
        "\n",
        "This tutorial demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. This tutorial covers stage 1 : data management: get started with Dataflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset:gsod,lrg"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset used for this tutorial is the GSOD dataset from [BigQuery public datasets](https://cloud.google.com/bigquery/public-data). The version of the dataset you use only the fields year, month and day to predict the value of mean daily temperature (mean_temp)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "objective:mlops,stage1,get_started_dataflow"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to use `Dataflow` for training with `Vertex AI`.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services:\n",
        "\n",
        "- `Dataflow`\n",
        "- `BigQuery Datasets`\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Offline preprocessing of data:\n",
        "    - Serially - w/o dataflow\n",
        "    - Parallel - with dataflow\n",
        "- Upstream preprocessing of data:\n",
        "    - tabular data\n",
        "    - image data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "recommendation:mlops,stage1,dataflow"
      },
      "source": [
        "### Recommendations\n",
        "\n",
        "When doing E2E MLOps on Google Cloud, the following best practices for preprocessing and feeding data during training of custom models:\n",
        "\n",
        "#### Preprocessing\n",
        "\n",
        "Data is preprocessed either:\n",
        "\n",
        "- Offline: The data is preprocessed and stored prior to training.\n",
        "    - Small datasets: reprocessed and stored when new data.\n",
        "- Upstream: The data is preprocessed upstream from the model while the data is feed for training.\n",
        "    - Training on a CPU.\n",
        "- Downstream: The data is preprocessed downstream in the model while the data is feed for training.\n",
        "    - Training on a HW accelerator (e.g., GPU/TPU).\n",
        "\n",
        "#### Model Feeding\n",
        "\n",
        "Data is feed for model feeding either:\n",
        "\n",
        "- In-memory: small dataset.\n",
        "- From disk: large dataset, quick training.\n",
        "- `Dataflow` from disk: massive dataset, extended training.\n",
        "\n",
        "#### AutoML\n",
        "\n",
        "For AutoML training, preprocessing and model feeding are automatically handled.\n",
        "\n",
        "Alternately for AutoML tabular model training, you can reconfigure the otherwise default preprocessing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_mlops"
      },
      "source": [
        "## Installations\n",
        "\n",
        "Install *one time* the packages for executing the MLOps notebooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_mlops"
      },
      "outputs": [],
      "source": [
        "ONCE_ONLY = False\n",
        "if ONCE_ONLY:\n",
        "    ! pip3 install -U tensorflow==2.5 $USER_FLAG\n",
        "    ! pip3 install -U tensorflow-data-validation==1.2 $USER_FLAG\n",
        "    ! pip3 install -U tensorflow-transform==1.2 $USER_FLAG\n",
        "    ! pip3 install -U tensorflow-io==0.18 $USER_FLAG\n",
        "    ! pip3 install --upgrade google-cloud-aiplatform[tensorboard] $USER_FLAG\n",
        "    ! pip3 install --upgrade google-cloud-pipeline-components $USER_FLAG\n",
        "    ! pip3 install --upgrade google-cloud-bigquery $USER_FLAG\n",
        "    ! pip3 install --upgrade google-cloud-logging $USER_FLAG\n",
        "    ! pip3 install --upgrade apache-beam[gcp] $USER_FLAG\n",
        "    ! pip3 install --upgrade pyarrow $USER_FLAG\n",
        "    ! pip3 install --upgrade cloudml-hypertune $USER_FLAG\n",
        "    ! pip3 install --upgrade kfp $USER_FLAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "restart"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "restart"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "project_id"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_project_id"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_project_id"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID:\", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_gcloud_project_id"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
        "\n",
        "- Americas: `us-central1`\n",
        "- Europe: `europe-west4`\n",
        "- Asia Pacific: `asia-east1`\n",
        "\n",
        "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
        "\n",
        "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "region"
      },
      "outputs": [],
      "source": [
        "REGION = \"us-central1\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "timestamp"
      },
      "source": [
        "#### Timestamp\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "timestamp"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bucket:custom"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "When you submit a custom training job using the Vertex SDK, you upload a Python package\n",
        "containing your training code to a Cloud Storage bucket. Vertex AI runs\n",
        "the code from this package. In this tutorial, Vertex AI also saves the\n",
        "trained model that results from your job in the same bucket. You can then\n",
        "create an `Endpoint` resource based on this output in order to serve\n",
        "online predictions.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bucket"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_bucket"
      },
      "outputs": [],
      "source": [
        "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
        "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_bucket"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "validate_bucket"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "validate_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_vars"
      },
      "source": [
        "### Set up variables\n",
        "\n",
        "Next, set up some variables used throughout the tutorial.\n",
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "import google.cloud.aiplatform as aip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_beam"
      },
      "source": [
        "#### Import Apache Beam\n",
        "\n",
        "Import the Apache Beam package into your Python environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_beam"
      },
      "outputs": [],
      "source": [
        "import apache_beam as beam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_bq"
      },
      "source": [
        "#### Import BigQuery\n",
        "\n",
        "Import the BigQuery package into your Python environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_bq"
      },
      "outputs": [],
      "source": [
        "from google.cloud import bigquery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_pandas"
      },
      "source": [
        "#### Import pandas\n",
        "\n",
        "Import the pandas package into your Python environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_pandas"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_numpy"
      },
      "source": [
        "#### Import numpy\n",
        "\n",
        "Import the numpy package into your Python environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_numpy"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_tfdv"
      },
      "source": [
        "#### Import TensorFlow Data Validation\n",
        "\n",
        "Import the TensorFlow Data Validation (TFDV) package into your Python environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_tfdv"
      },
      "outputs": [],
      "source": [
        "import tensorflow_data_validation as tfdv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_tft"
      },
      "source": [
        "#### Import TensorFlow Transform\n",
        "\n",
        "Import the TensorFlow Transform (TFT) package into your Python environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_tft"
      },
      "outputs": [],
      "source": [
        "import tensorflow_transform as tft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk,region"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_aip:mbsdk,region"
      },
      "outputs": [],
      "source": [
        "aip.init(project=PROJECT_ID, location=REGION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_bq"
      },
      "source": [
        "### Create BigQuery client\n",
        "\n",
        "Create the BigQuery client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_bq"
      },
      "outputs": [],
      "source": [
        "bqclient = bigquery.Client()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "offline_preprocess:bq"
      },
      "source": [
        "## Offline preprocessing data with BigQuery table using pandas dataframe\n",
        "\n",
        "- Offline: The BigQuery table is preprocessed in-memory and stored prior to training.\n",
        "\n",
        "    - Extract the tabular data into a pandas dataframe.\n",
        "    - Preprocess the data, per column, within the dataframe.\n",
        "    - Write the preprocessed dataframe to a new BigQuery table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_file:gsod,bq,lrg"
      },
      "outputs": [],
      "source": [
        "IMPORT_FILE = \"bq://bigquery-public-data.samples.gsod\"\n",
        "BQ_TABLE = \"bigquery-public-data.samples.gsod\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq_to_dataframe:gsod"
      },
      "source": [
        "### Read the BigQuery dataset into a pandas dataframe\n",
        "\n",
        "Next, you read a sample of the dataset into a pandas dataframe using BigQuery `list_rows()` and `to_dataframe()` method, as follows:\n",
        "\n",
        "- `list_rows()`: Performs a query on the specified table and returns a row iterator to the query results. Optionally specify:\n",
        " - `selected_fields`: Subset of fields (columns) to return.\n",
        " - `max_results`: The maximum number of rows to return. Same as SQL LIMIT command.\n",
        "\n",
        "\n",
        "- `rows.to_dataframe()`: Invokes the row iterator and reads in the data into a pandas dataframe.\n",
        "\n",
        "Learn more about [Loading BigQuery table into a dataframe](https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bq_to_dataframe:gsod"
      },
      "outputs": [],
      "source": [
        "# Download a table.\n",
        "table = bigquery.TableReference.from_string(\"bigquery-public-data.samples.gsod\")\n",
        "\n",
        "rows = bqclient.list_rows(\n",
        "    table,\n",
        "    max_results=500,\n",
        "    selected_fields=[\n",
        "        bigquery.SchemaField(\"station_number\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"year\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"month\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"day\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n",
        "    ],\n",
        ")\n",
        "\n",
        "dataframe = rows.to_dataframe()\n",
        "print(dataframe.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataframe_transform:gsod"
      },
      "source": [
        "### Transform data within pandas dataframe.\n",
        "\n",
        "Next, you preprocess the data within the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataframe_transform:gsod"
      },
      "outputs": [],
      "source": [
        "dataframe[\"station_number\"] = pd.to_numeric(dataframe[\"station_number\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqml_create_dataset"
      },
      "source": [
        "### Create BQ dataset resource\n",
        "\n",
        "First, you create an empty dataset resource in your project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqml_create_dataset"
      },
      "outputs": [],
      "source": [
        "BQ_MY_DATASET = 'samples'\n",
        "BQ_MY_TABLE = 'gsod'\n",
        "! bq --location=US mk -d \\\n",
        "$PROJECT_ID:$BQ_MY_DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataframe_to_bq:transformed,gsod"
      },
      "outputs": [],
      "source": [
        "job_config = bigquery.LoadJobConfig(\n",
        "    # Specify a (partial) schema. All columns are always written to the\n",
        "    # table. The schema is used to assist in data type definitions.\n",
        "    schema=[\n",
        "        bigquery.SchemaField(\"station_number\", \"FLOAT\"),  # <-- after one hot encoding\n",
        "        bigquery.SchemaField(\"year\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"month\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"day\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n",
        "    ],\n",
        "    # Optionally, set the write disposition. BigQuery appends loaded rows\n",
        "    # to an existing table by default, but with WRITE_TRUNCATE write\n",
        "    # disposition it replaces the table with the loaded data.\n",
        "    write_disposition=\"WRITE_TRUNCATE\",\n",
        ")\n",
        "\n",
        "NEW_BQ_TABLE = f\"{PROJECT_ID}.samples.gsod_transformed\"\n",
        "\n",
        "job = bqclient.load_table_from_dataframe(\n",
        "    dataframe, NEW_BQ_TABLE, job_config=job_config\n",
        ")  # Make an API request.\n",
        "job.result()  # Wait for the job to complete.\n",
        "\n",
        "table = bqclient.get_table(NEW_BQ_TABLE)  # Make an API request.\n",
        "print(\n",
        "    \"Loaded {} rows and {} columns to {}\".format(\n",
        "        table.num_rows, len(table.schema), NEW_BQ_TABLE\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upstream_preprocess:image"
      },
      "source": [
        "## Upstream preprocessing data with tf.data.Dataset generator\n",
        "\n",
        "### Image data\n",
        "\n",
        "- Upstream: The data is preprocessed upstream from the model while the data is feed for training.\n",
        "\n",
        "    - Define preprocessing function:\n",
        "        - Input: unprocessed batch of tensors\n",
        "        - Output: preprocessed batch of tensors\n",
        "    - Use tf.data.Dataset `map()` method to map the preprocessing function to the generator output.\n",
        "\n",
        "In this example:\n",
        "\n",
        "- Load CIFAR10 dataset into memory as numpy arrays.\n",
        "- Create a tf.data.Dataset generator for the in-memory CIFAR10 dataset. *Note*: The pixel data is casted to FLOAT32 to be compatiable with the preprocessing function which outputs the pixel data as FLOAT32.\n",
        "- Define a preprocessing function to rescale the pixel data by 1/255.0\n",
        "- Map the preprocessing function to the generator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upstream_preprocess:image"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "tf_dataset = tf.data.Dataset.from_tensor_slices((x_train.astype(np.float32), y_train))\n",
        "\n",
        "print(\"Before preprocessing\")\n",
        "for batch in tf_dataset:\n",
        "    print(batch)\n",
        "    break\n",
        "\n",
        "\n",
        "def preprocess_fn(inputs, labels):\n",
        "    inputs /= 255.0\n",
        "    return tf.cast(inputs, tf.float32), labels\n",
        "\n",
        "\n",
        "tf_dataset = tf_dataset.map(preprocess_fn)\n",
        "\n",
        "print(\"After preprocessing\")\n",
        "for batch in tf_dataset:\n",
        "    print(batch)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upstream_preprocess:tabular"
      },
      "source": [
        "## Upstream preprocessing data with tf.data.Dataset generator\n",
        "\n",
        "### Tabular data\n",
        "\n",
        "- Upstream: The data is preprocessed upstream from the model while the data is feed for training.\n",
        "\n",
        "    - Define preprocessing function:\n",
        "        - Input: unprocessed batch of tensors\n",
        "        - Output: preprocessed batch of tensors\n",
        "    - Use tf.data.Dataset `map()` method to map the preprocessing function to the generator output.\n",
        "\n",
        "In this example:\n",
        "\n",
        "- Create tf.data.Dataset generator for Boston Housing data.\n",
        "- Iterate a single batch before preprocessing.\n",
        "- Define preprocessing function to scale all the features between 0 and 1.\n",
        "- Map the preprocessing function to the dataset.\n",
        "- Iterate once through the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upstream_preprocess:tabular"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import boston_housing\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
        "\n",
        "tf_dataset = tf.data.Dataset.from_tensor_slices((x_train.astype(np.float32), y_train))\n",
        "\n",
        "print(\"Before preprocessing\")\n",
        "for batch in tf_dataset:\n",
        "    print(batch)\n",
        "    break\n",
        "\n",
        "\n",
        "def preprocessing_fn(inputs, labels):\n",
        "    inputs = tft.scale_to_0_1(inputs)\n",
        "    return tf.cast(inputs, tf.float32), labels\n",
        "\n",
        "\n",
        "tf_dataset = tf_dataset.map(preprocessing_fn)\n",
        "\n",
        "print(\"After preprocessing\")\n",
        "for batch in tf_dataset:\n",
        "    print(batch)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "offline_preprocess:dataflow"
      },
      "source": [
        "## Offline preprocessing with Dataflow\n",
        "\n",
        "- Generate data chema from BigQuery table.\n",
        "- Define Beam pipeline to:\n",
        "    - Split data from BigQuery table into train and eval datasets.\n",
        "    - Encode datasets as TFRecords, using the data schema.\n",
        "    - Save the TFRecords as compressed files to Cloud Storage\n",
        "- Run the pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq_to_dataframe:gsod"
      },
      "source": [
        "### Read the BigQuery dataset into a pandas dataframe\n",
        "\n",
        "Next, you read a sample of the dataset into a pandas dataframe using BigQuery `list_rows()` and `to_dataframe()` method, as follows:\n",
        "\n",
        "- `list_rows()`: Performs a query on the specified table and returns a row iterator to the query results. Optionally specify:\n",
        " - `selected_fields`: Subset of fields (columns) to return.\n",
        " - `max_results`: The maximum number of rows to return. Same as SQL LIMIT command.\n",
        "\n",
        "\n",
        "- `rows.to_dataframe()`: Invokes the row iterator and reads in the data into a pandas dataframe.\n",
        "\n",
        "Learn more about [Loading BigQuery table into a dataframe](https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bq_to_dataframe:gsod"
      },
      "outputs": [],
      "source": [
        "# Download a table.\n",
        "table = bigquery.TableReference.from_string(\"bigquery-public-data.samples.gsod\")\n",
        "\n",
        "rows = bqclient.list_rows(\n",
        "    table,\n",
        "    max_results=500,\n",
        "    selected_fields=[\n",
        "        bigquery.SchemaField(\"station_number\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"year\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"month\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"day\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n",
        "    ],\n",
        ")\n",
        "\n",
        "dataframe = rows.to_dataframe()\n",
        "print(dataframe.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfdv_stats:dataframe"
      },
      "source": [
        "###  Generate dataset statistics\n",
        "\n",
        "#### Dataframe input data\n",
        "\n",
        "Generate statistics on the dataset with the TensorFlow Data Validation (TFDV) package. Use the `generate_statistics_from_dataframe()` method, with the following parameters:\n",
        "\n",
        "- `dataframe`: The dataset in an in-memory pandas dataframe.\n",
        "- `stats_options`: The selected statistics options:\n",
        "  - `label_feature`: The column which is the label to predict.\n",
        "  - `sample_rate`: The sampling rate. If specified, statistics is computed over the sample.\n",
        "  - `num_top_values`: number of most frequent feature values to keep for string features.\n",
        "\n",
        "Learn about [TensorFlow Data Validation (TFDV)](https://www.tensorflow.org/tfx/data_validation/get_started)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfdv_stats:dataframe"
      },
      "outputs": [],
      "source": [
        "stats = tfdv.generate_statistics_from_dataframe(\n",
        "    dataframe=dataframe,\n",
        "    stats_options=tfdv.StatsOptions(\n",
        "        label_feature=\"mean_temp\", sample_rate=1, num_top_values=50\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfdv_schema"
      },
      "source": [
        "###  Generate the raw data schema\n",
        "\n",
        "Generate the data schema on the dataset with the TensorFlow Data Validation (TFDV) package. Use the `infer_schema()` method, with the following parameters:\n",
        "\n",
        "- `statistics`: The statistics generated by TFDV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfdv_schema"
      },
      "outputs": [],
      "source": [
        "schema = tfdv.infer_schema(statistics=stats)\n",
        "print(schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfdv_schema:save"
      },
      "source": [
        "#### Save schema for the dataset to Cloud Storage\n",
        "\n",
        "Next, you write the schema for the dataset to the dataset's Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfdv_schema:save"
      },
      "outputs": [],
      "source": [
        "SCHEMA_LOCATION = BUCKET_NAME + \"/schema.txt\"\n",
        "\n",
        "# When running Apache Beam directly (file is directly accessed)\n",
        "tfdv.write_schema_text(output_path=SCHEMA_LOCATION, schema=schema)\n",
        "# When running with Dataflow (file is uploaded to worker pool)\n",
        "tfdv.write_schema_text(output_path=\"schema.txt\", schema=schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataflow_setup:transform"
      },
      "source": [
        "#### Prepare package requirements for Dataflow job.\n",
        "\n",
        "Before you can run a Dataflow job, you need to specify the package requirements for the worker pool that will execute the job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataflow_setup:transform"
      },
      "outputs": [],
      "source": [
        "%%writefile setup.py\n",
        "import setuptools\n",
        "\n",
        "REQUIRED_PACKAGES = [\n",
        "    \"google-cloud-aiplatform==1.4.2\",\n",
        "    \"tensorflow-transform==1.2.0\",\n",
        "    \"tensorflow-data-validation==1.2.0\",\n",
        "]\n",
        "\n",
        "setuptools.setup(\n",
        "    name=\"executor\",\n",
        "    version=\"0.0.1\",\n",
        "    install_requires=REQUIRED_PACKAGES,\n",
        "    packages=setuptools.find_packages(),\n",
        "    include_package_data=True,\n",
        "    package_data={\"./\": [\"schema.txt\"]}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataflow:split,bq,gsod"
      },
      "source": [
        "### Preprocess data with Dataflow\n",
        "\n",
        "#### Dataset splitting\n",
        "\n",
        "Next, you preprocess the data using Dataflow. In this example, you query the BigQuery table and split the examples into training and evaluation datasets. For expendiency, the number of examples from the dataset is limited to 500."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataflow:split,bq,gsod"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import tensorflow_transform.beam as tft_beam\n",
        "\n",
        "RUNNER = \"DataflowRunner\"  # DirectRunner for local running w/o Dataflow\n",
        "\n",
        "\n",
        "def parse_bq_record(bq_record):\n",
        "    \"\"\"Parses a bq_record to a dictionary.\"\"\"\n",
        "    output = {}\n",
        "    for key in bq_record:\n",
        "        output[key] = [bq_record[key]]\n",
        "    return output\n",
        "\n",
        "\n",
        "def split_dataset(bq_row, num_partitions, ratio):\n",
        "    \"\"\"Returns a partition number for a given bq_row.\"\"\"\n",
        "    import json\n",
        "\n",
        "    assert num_partitions == len(ratio)\n",
        "    bucket = sum(map(ord, json.dumps(bq_row))) % sum(ratio)\n",
        "    total = 0\n",
        "    for i, part in enumerate(ratio):\n",
        "        total += part\n",
        "        if bucket < total:\n",
        "            return i\n",
        "    return len(ratio) - 1\n",
        "\n",
        "\n",
        "def run_pipeline(args):\n",
        "    \"\"\"Runs a Beam pipeline to split the dataset\"\"\"\n",
        "\n",
        "    pipeline_options = beam.pipeline.PipelineOptions(flags=[], **args)\n",
        "\n",
        "    raw_data_query = args[\"raw_data_query\"]\n",
        "    exported_data_prefix = args[\"exported_data_prefix\"]\n",
        "    temp_location = args[\"temp_location\"]\n",
        "    project = args[\"project\"]\n",
        "\n",
        "    schema = tfdv.load_schema_text(SCHEMA_LOCATION)\n",
        "\n",
        "    with beam.Pipeline(options=pipeline_options) as pipeline:\n",
        "        with tft_beam.Context(temp_location):\n",
        "\n",
        "            # Read raw BigQuery data.\n",
        "            raw_train_data, raw_eval_data = (\n",
        "                pipeline\n",
        "                | \"Read Raw Data\"\n",
        "                >> beam.io.ReadFromBigQuery(\n",
        "                    query=raw_data_query,\n",
        "                    project=project,\n",
        "                    use_standard_sql=True,\n",
        "                )\n",
        "                | \"Parse Data\" >> beam.Map(parse_bq_record)\n",
        "                | \"Split\" >> beam.Partition(split_dataset, 2, ratio=[8, 2])\n",
        "            )\n",
        "\n",
        "            _ = (\n",
        "                raw_train_data\n",
        "                | \"Write Raw Train Data\"\n",
        "                >> beam.io.tfrecordio.WriteToTFRecord(\n",
        "                    file_path_prefix=os.path.join(exported_data_prefix, \"train/\"),\n",
        "                    file_name_suffix=\".gz\",\n",
        "                    coder=tft.coders.ExampleProtoCoder(schema),\n",
        "                )\n",
        "            )\n",
        "\n",
        "            _ = (\n",
        "                raw_eval_data\n",
        "                | \"Write Raw Eval Data\"\n",
        "                >> beam.io.tfrecordio.WriteToTFRecord(\n",
        "                    file_path_prefix=os.path.join(exported_data_prefix, \"eval/\"),\n",
        "                    file_name_suffix=\".gz\",\n",
        "                    coder=tft.coders.ExampleProtoCoder(schema),\n",
        "                )\n",
        "            )\n",
        "\n",
        "\n",
        "EXPORTED_DATA_PREFIX = os.path.join(BUCKET_NAME, \"exported_data\")\n",
        "\n",
        "QUERY_STRING = \"SELECT {},{} FROM {} LIMIT 500\".format(\n",
        "    \"CAST(station_number as STRING) AS station_number,year,month,day\",\n",
        "    \"mean_temp\",\n",
        "    IMPORT_FILE[5:],\n",
        ")\n",
        "JOB_NAME = \"gsod\" + TIMESTAMP\n",
        "\n",
        "args = {\n",
        "    \"runner\": RUNNER,\n",
        "    \"raw_data_query\": QUERY_STRING,\n",
        "    \"exported_data_prefix\": EXPORTED_DATA_PREFIX,\n",
        "    \"temp_location\": os.path.join(BUCKET_NAME, \"temp\"),\n",
        "    \"project\": PROJECT_ID,\n",
        "    \"region\": REGION,\n",
        "    \"setup_file\": \"./setup.py\",\n",
        "}\n",
        "\n",
        "print(\"Data preprocessing started...\")\n",
        "run_pipeline(args)\n",
        "print(\"Data preprocessing completed.\")\n",
        "\n",
        "! gsutil ls $EXPORTED_DATA_PREFIX/train\n",
        "! gsutil ls $EXPORTED_DATA_PREFIX/eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cleanup:mbsdk"
      },
      "source": [
        "# Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:\n",
        "\n",
        "- Dataset\n",
        "- Pipeline\n",
        "- Model\n",
        "- Endpoint\n",
        "- AutoML Training Job\n",
        "- Batch Job\n",
        "- Custom Job\n",
        "- Hyperparameter Tuning Job\n",
        "- Cloud Storage Bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup:mbsdk"
      },
      "outputs": [],
      "source": [
        "delete_all = True\n",
        "\n",
        "if delete_all:\n",
        "    # Delete the dataset using the Vertex dataset object\n",
        "    try:\n",
        "        if \"dataset\" in globals():\n",
        "            dataset.delete()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    # Delete the model using the Vertex model object\n",
        "    try:\n",
        "        if \"model\" in globals():\n",
        "            model.delete()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    # Delete the endpoint using the Vertex endpoint object\n",
        "    try:\n",
        "        if \"endpoint\" in globals():\n",
        "            endpoint.undeploy_all()\n",
        "            endpoint.delete()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    # Delete the AutoML or Pipeline training job\n",
        "    try:\n",
        "        if \"dag\" in globals():\n",
        "            dag.delete()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    # Delete the custom training job\n",
        "    try:\n",
        "        if \"job\" in globals():\n",
        "            job.delete()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    # Delete the batch prediction job using the Vertex batch prediction object\n",
        "    try:\n",
        "        if \"batch_predict_job\" in globals():\n",
        "            batch_predict_job.delete()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    # Delete the hyperparameter tuning job using the Vertex hyperparameter tuning object\n",
        "    try:\n",
        "        if \"hpt_job\" in globals():\n",
        "            hpt_job.delete()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    if \"BUCKET_NAME\" in globals():\n",
        "        ! gsutil rm -r $BUCKET_NAME"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "get_started_dataflow.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

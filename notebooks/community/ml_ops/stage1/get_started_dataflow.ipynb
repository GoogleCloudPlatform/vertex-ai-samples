{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright"
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title:generic,gcp"
   },
   "source": [
    "# E2E ML on GCP: MLOps stage 1 : data management: get started with Dataflow\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage1/get_started_dataflow.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "        <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage1/get_started_dataflow.ipynb\">\n",
    "        <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\\\" alt=\"Colab logo\"> Run in Colab\n",
    "        </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage1/get_started_dataflow.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:mlops"
   },
   "source": [
    "## Overview\n",
    "\n",
    "\n",
    "This tutorial demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. This tutorial covers stage 1 : data management: get started with Dataflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:mlops,stage1,get_started_dataflow"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you learn how to use `Dataflow` for training with `Vertex AI`.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services:\n",
    "\n",
    "- `Dataflow`\n",
    "- `BigQuery Datasets`\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Offline preprocessing of data:\n",
    "    - Serially - w/o dataflow\n",
    "    - Parallel - with dataflow\n",
    "- Upstream preprocessing of data:\n",
    "    - tabular data\n",
    "    - image data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "recommendation:mlops,stage1,dataflow"
   },
   "source": [
    "### Recommendations\n",
    "\n",
    "When doing E2E MLOps on Google Cloud, the following best practices for preprocessing and feeding data during training of custom models:\n",
    "\n",
    "#### Preprocessing\n",
    "\n",
    "Data is preprocessed either:\n",
    "\n",
    "- Offline: The data is preprocessed and stored prior to training.\n",
    "    - Small datasets: reprocessed and stored when new data.\n",
    "- Upstream: The data is preprocessed upstream from the model while the data is feed for training.\n",
    "    - Training on a CPU.\n",
    "- Downstream: The data is preprocessed downstream in the model while the data is feed for training.\n",
    "    - Training on a HW accelerator (e.g., GPU/TPU).\n",
    "\n",
    "#### Model Feeding\n",
    "\n",
    "Data is feed for model feeding either:\n",
    "\n",
    "- In-memory: small dataset.\n",
    "- From disk: large dataset, quick training.\n",
    "- `Dataflow` from disk: massive dataset, extended training.\n",
    "\n",
    "#### AutoML\n",
    "\n",
    "For AutoML training, preprocessing and model feeding are automatically handled.\n",
    "\n",
    "Alternately for AutoML tabular model training, you can reconfigure the otherwise default preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:gsod,lrg"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset used for this tutorial is the GSOD dataset from [BigQuery public datasets](https://cloud.google.com/bigquery/public-data). The version of the dataset you use only the fields year, month and day to predict the value of mean daily temperature (mean_temp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9e483012a752"
   },
   "source": [
    "### Costs\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "- Vertex AI\n",
    "- Cloud Storage\n",
    "- BigQuery\n",
    "- Dataflow\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), [BigQuery pricing](https://cloud.google.com/bigquery/pricing), and [Dataflow pricing](https://cloud.google.com/dataflow/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_mlops"
   },
   "source": [
    "## Installations\n",
    "\n",
    "Install the following packages to execute this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_mlops"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "extra_pkgs = \"tensorflow==2.5 tensorflow-data-validation==1.2 tensorflow-transform==1.2 \\\n",
    "              tensorflow-io==0.18 pyarrow pandas apache-beam[gcp] google-cloud-bigquery\"\n",
    "! pip3 install --upgrade --quiet {USER_FLAG} google-cloud-aiplatform $extra_pkgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "restart"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common setup\n",
    "\n",
    "Now, execute the common setup for the notebook tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "001a0fcd5d78"
   },
   "outputs": [],
   "source": [
    "# Common code setup for notebook tutorials\n",
    "\n",
    "! wget https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/setup.py -O setup.py\n",
    "\n",
    "%run setup.py --bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d809f07a8935"
   },
   "outputs": [],
   "source": [
    "# Other Common setup instructions for notebook tutorials\n",
    "\n",
    "! wget https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/setup.md -O setup.md\n",
    "\n",
    "%pfile setup.md "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### Set up variables\n",
    "\n",
    "Next, set up some variables used throughout the tutorial.\n",
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_beam"
   },
   "source": [
    "#### Import Apache Beam\n",
    "\n",
    "Import the Apache Beam package into your Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_beam"
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_bq"
   },
   "source": [
    "#### Import BigQuery\n",
    "\n",
    "Import the BigQuery package into your Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_bq"
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_pandas"
   },
   "source": [
    "#### Import pandas\n",
    "\n",
    "Import the pandas package into your Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_pandas"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_numpy"
   },
   "source": [
    "#### Import numpy\n",
    "\n",
    "Import the numpy package into your Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_numpy"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_tfdv"
   },
   "source": [
    "#### Import TensorFlow Data Validation\n",
    "\n",
    "Import the TensorFlow Data Validation (TFDV) package into your Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_tfdv"
   },
   "outputs": [],
   "source": [
    "import tensorflow_data_validation as tfdv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_tft"
   },
   "source": [
    "#### Import TensorFlow Transform\n",
    "\n",
    "Import the TensorFlow Transform (TFT) package into your Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_tft"
   },
   "outputs": [],
   "source": [
    "import tensorflow_transform as tft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,region"
   },
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk,region"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_bq"
   },
   "source": [
    "### Create BigQuery client\n",
    "\n",
    "Create the BigQuery client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_bq"
   },
   "outputs": [],
   "source": [
    "bqclient = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "offline_preprocess:bq"
   },
   "source": [
    "## Offline preprocessing data with BigQuery table using pandas dataframe\n",
    "\n",
    "- Offline: The BigQuery table is preprocessed in-memory and stored prior to training.\n",
    "\n",
    "    - Extract the tabular data into a pandas dataframe.\n",
    "    - Preprocess the data, per column, within the dataframe.\n",
    "    - Write the preprocessed dataframe to a new BigQuery table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_file:gsod,bq,lrg"
   },
   "outputs": [],
   "source": [
    "IMPORT_FILE = \"bq://bigquery-public-data.samples.gsod\"\n",
    "BQ_TABLE = \"bigquery-public-data.samples.gsod\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq_to_dataframe:gsod"
   },
   "source": [
    "### Read the BigQuery dataset into a pandas dataframe\n",
    "\n",
    "Next, you read a sample of the dataset into a pandas dataframe using BigQuery `list_rows()` and `to_dataframe()` method, as follows:\n",
    "\n",
    "- `list_rows()`: Performs a query on the specified table and returns a row iterator to the query results. Optionally specify:\n",
    " - `selected_fields`: Subset of fields (columns) to return.\n",
    " - `max_results`: The maximum number of rows to return. Same as SQL LIMIT command.\n",
    "\n",
    "\n",
    "- `rows.to_dataframe()`: Invokes the row iterator and reads in the data into a pandas dataframe.\n",
    "\n",
    "Learn more about [Loading BigQuery table into a dataframe](https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bq_to_dataframe:gsod"
   },
   "outputs": [],
   "source": [
    "# Download a table.\n",
    "table = bigquery.TableReference.from_string(\"bigquery-public-data.samples.gsod\")\n",
    "\n",
    "rows = bqclient.list_rows(\n",
    "    table,\n",
    "    max_results=500,\n",
    "    selected_fields=[\n",
    "        bigquery.SchemaField(\"station_number\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"year\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"month\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"day\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "dataframe = rows.to_dataframe()\n",
    "print(dataframe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataframe_transform:gsod"
   },
   "source": [
    "### Transform data within pandas dataframe.\n",
    "\n",
    "Next, you preprocess the data within the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataframe_transform:gsod"
   },
   "outputs": [],
   "source": [
    "dataframe[\"station_number\"] = pd.to_numeric(dataframe[\"station_number\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqml_create_dataset"
   },
   "source": [
    "### Create BQ dataset resource\n",
    "\n",
    "First, you create an empty dataset resource in your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bqml_create_dataset"
   },
   "outputs": [],
   "source": [
    "BQ_MY_DATASET = 'samples'\n",
    "BQ_MY_TABLE = 'gsod'\n",
    "! bq --location=US mk -d \\\n",
    "$PROJECT_ID:$BQ_MY_DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataframe_to_bq:transformed,gsod"
   },
   "outputs": [],
   "source": [
    "job_config = bigquery.LoadJobConfig(\n",
    "    # Specify a (partial) schema. All columns are always written to the\n",
    "    # table. The schema is used to assist in data type definitions.\n",
    "    schema=[\n",
    "        bigquery.SchemaField(\"station_number\", \"FLOAT\"),  # <-- after one hot encoding\n",
    "        bigquery.SchemaField(\"year\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"month\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"day\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n",
    "    ],\n",
    "    # Optionally, set the write disposition. BigQuery appends loaded rows\n",
    "    # to an existing table by default, but with WRITE_TRUNCATE write\n",
    "    # disposition it replaces the table with the loaded data.\n",
    "    write_disposition=\"WRITE_TRUNCATE\",\n",
    ")\n",
    "\n",
    "NEW_BQ_TABLE = f\"{PROJECT_ID}.samples.gsod_transformed\"\n",
    "\n",
    "job = bqclient.load_table_from_dataframe(\n",
    "    dataframe, NEW_BQ_TABLE, job_config=job_config\n",
    ")  # Make an API request.\n",
    "job.result()  # Wait for the job to complete.\n",
    "\n",
    "table = bqclient.get_table(NEW_BQ_TABLE)  # Make an API request.\n",
    "print(\n",
    "    \"Loaded {} rows and {} columns to {}\".format(\n",
    "        table.num_rows, len(table.schema), NEW_BQ_TABLE\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upstream_preprocess:image"
   },
   "source": [
    "## Upstream preprocessing data with tf.data.Dataset generator\n",
    "\n",
    "### Image data\n",
    "\n",
    "- Upstream: The data is preprocessed upstream from the model while the data is feed for training.\n",
    "\n",
    "    - Define preprocessing function:\n",
    "        - Input: unprocessed batch of tensors\n",
    "        - Output: preprocessed batch of tensors\n",
    "    - Use tf.data.Dataset `map()` method to map the preprocessing function to the generator output.\n",
    "\n",
    "In this example:\n",
    "\n",
    "- Load CIFAR10 dataset into memory as numpy arrays.\n",
    "- Create a tf.data.Dataset generator for the in-memory CIFAR10 dataset. *Note*: The pixel data is casted to FLOAT32 to be compatiable with the preprocessing function which outputs the pixel data as FLOAT32.\n",
    "- Define a preprocessing function to rescale the pixel data by 1/255.0\n",
    "- Map the preprocessing function to the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upstream_preprocess:image"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "tf_dataset = tf.data.Dataset.from_tensor_slices((x_train.astype(np.float32), y_train))\n",
    "\n",
    "print(\"Before preprocessing\")\n",
    "for batch in tf_dataset:\n",
    "    print(batch)\n",
    "    break\n",
    "\n",
    "\n",
    "def preprocess_fn(inputs, labels):\n",
    "    inputs /= 255.0\n",
    "    return tf.cast(inputs, tf.float32), labels\n",
    "\n",
    "\n",
    "tf_dataset = tf_dataset.map(preprocess_fn)\n",
    "\n",
    "print(\"After preprocessing\")\n",
    "for batch in tf_dataset:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upstream_preprocess:tabular"
   },
   "source": [
    "## Upstream preprocessing data with tf.data.Dataset generator\n",
    "\n",
    "### Tabular data\n",
    "\n",
    "- Upstream: The data is preprocessed upstream from the model while the data is feed for training.\n",
    "\n",
    "    - Define preprocessing function:\n",
    "        - Input: unprocessed batch of tensors\n",
    "        - Output: preprocessed batch of tensors\n",
    "    - Use tf.data.Dataset `map()` method to map the preprocessing function to the generator output.\n",
    "\n",
    "In this example:\n",
    "\n",
    "- Create tf.data.Dataset generator for Boston Housing data.\n",
    "- Iterate a single batch before preprocessing.\n",
    "- Define preprocessing function to scale all the features between 0 and 1.\n",
    "- Map the preprocessing function to the dataset.\n",
    "- Iterate once through the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upstream_preprocess:tabular"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import boston_housing\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "tf_dataset = tf.data.Dataset.from_tensor_slices((x_train.astype(np.float32), y_train))\n",
    "\n",
    "print(\"Before preprocessing\")\n",
    "for batch in tf_dataset:\n",
    "    print(batch)\n",
    "    break\n",
    "\n",
    "\n",
    "def preprocessing_fn(inputs, labels):\n",
    "    inputs = tft.scale_to_0_1(inputs)\n",
    "    return tf.cast(inputs, tf.float32), labels\n",
    "\n",
    "\n",
    "tf_dataset = tf_dataset.map(preprocessing_fn)\n",
    "\n",
    "print(\"After preprocessing\")\n",
    "for batch in tf_dataset:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "offline_preprocess:dataflow"
   },
   "source": [
    "## Offline preprocessing with Dataflow\n",
    "\n",
    "- Generate data chema from BigQuery table.\n",
    "- Define Beam pipeline to:\n",
    "    - Split data from BigQuery table into train and eval datasets.\n",
    "    - Encode datasets as TFRecords, using the data schema.\n",
    "    - Save the TFRecords as compressed files to Cloud Storage\n",
    "- Run the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq_to_dataframe:gsod"
   },
   "source": [
    "### Read the BigQuery dataset into a pandas dataframe\n",
    "\n",
    "Next, you read a sample of the dataset into a pandas dataframe using BigQuery `list_rows()` and `to_dataframe()` method, as follows:\n",
    "\n",
    "- `list_rows()`: Performs a query on the specified table and returns a row iterator to the query results. Optionally specify:\n",
    " - `selected_fields`: Subset of fields (columns) to return.\n",
    " - `max_results`: The maximum number of rows to return. Same as SQL LIMIT command.\n",
    "\n",
    "\n",
    "- `rows.to_dataframe()`: Invokes the row iterator and reads in the data into a pandas dataframe.\n",
    "\n",
    "Learn more about [Loading BigQuery table into a dataframe](https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bq_to_dataframe:gsod"
   },
   "outputs": [],
   "source": [
    "# Download a table.\n",
    "table = bigquery.TableReference.from_string(\"bigquery-public-data.samples.gsod\")\n",
    "\n",
    "rows = bqclient.list_rows(\n",
    "    table,\n",
    "    max_results=500,\n",
    "    selected_fields=[\n",
    "        bigquery.SchemaField(\"station_number\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"year\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"month\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"day\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "dataframe = rows.to_dataframe()\n",
    "print(dataframe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfdv_stats:dataframe"
   },
   "source": [
    "###  Generate dataset statistics\n",
    "\n",
    "#### Dataframe input data\n",
    "\n",
    "Generate statistics on the dataset with the TensorFlow Data Validation (TFDV) package. Use the `generate_statistics_from_dataframe()` method, with the following parameters:\n",
    "\n",
    "- `dataframe`: The dataset in an in-memory pandas dataframe.\n",
    "- `stats_options`: The selected statistics options:\n",
    "  - `label_feature`: The column which is the label to predict.\n",
    "  - `sample_rate`: The sampling rate. If specified, statistics is computed over the sample.\n",
    "  - `num_top_values`: number of most frequent feature values to keep for string features.\n",
    "\n",
    "Learn about [TensorFlow Data Validation (TFDV)](https://www.tensorflow.org/tfx/data_validation/get_started)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfdv_stats:dataframe"
   },
   "outputs": [],
   "source": [
    "stats = tfdv.generate_statistics_from_dataframe(\n",
    "    dataframe=dataframe,\n",
    "    stats_options=tfdv.StatsOptions(\n",
    "        label_feature=\"mean_temp\", sample_rate=1, num_top_values=50\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfdv_schema"
   },
   "source": [
    "###  Generate the raw data schema\n",
    "\n",
    "Generate the data schema on the dataset with the TensorFlow Data Validation (TFDV) package. Use the `infer_schema()` method, with the following parameters:\n",
    "\n",
    "- `statistics`: The statistics generated by TFDV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfdv_schema"
   },
   "outputs": [],
   "source": [
    "schema = tfdv.infer_schema(statistics=stats)\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfdv_schema:save"
   },
   "source": [
    "#### Save schema for the dataset to Cloud Storage\n",
    "\n",
    "Next, you write the schema for the dataset to the dataset's Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfdv_schema:save"
   },
   "outputs": [],
   "source": [
    "SCHEMA_LOCATION = BUCKET_URI + \"/schema.txt\"\n",
    "\n",
    "# When running Apache Beam directly (file is directly accessed)\n",
    "tfdv.write_schema_text(output_path=SCHEMA_LOCATION, schema=schema)\n",
    "# When running with Dataflow (file is uploaded to worker pool)\n",
    "tfdv.write_schema_text(output_path=\"schema.txt\", schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataflow_setup:transform"
   },
   "source": [
    "#### Prepare package requirements for Dataflow job.\n",
    "\n",
    "Before you can run a Dataflow job, you need to specify the package requirements for the worker pool that will execute the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataflow_setup:transform"
   },
   "outputs": [],
   "source": [
    "%%writefile setup.py\n",
    "import setuptools\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    \"google-cloud-aiplatform==1.4.2\",\n",
    "    \"tensorflow-transform==1.2.0\",\n",
    "    \"tensorflow-data-validation==1.2.0\",\n",
    "]\n",
    "\n",
    "setuptools.setup(\n",
    "    name=\"executor\",\n",
    "    version=\"0.0.1\",\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=setuptools.find_packages(),\n",
    "    include_package_data=True,\n",
    "    package_data={\"./\": [\"schema.txt\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataflow:split,bq,gsod"
   },
   "source": [
    "### Preprocess data with Dataflow\n",
    "\n",
    "#### Dataset splitting\n",
    "\n",
    "Next, you preprocess the data using Dataflow. In this example, you query the BigQuery table and split the examples into training and evaluation datasets. For expendiency, the number of examples from the dataset is limited to 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataflow:split,bq,gsod"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow_transform.beam as tft_beam\n",
    "\n",
    "RUNNER = \"DataflowRunner\"  # DirectRunner for local running w/o Dataflow\n",
    "\n",
    "\n",
    "def parse_bq_record(bq_record):\n",
    "    \"\"\"Parses a bq_record to a dictionary.\"\"\"\n",
    "    output = {}\n",
    "    for key in bq_record:\n",
    "        output[key] = [bq_record[key]]\n",
    "    return output\n",
    "\n",
    "\n",
    "def split_dataset(bq_row, num_partitions, ratio):\n",
    "    \"\"\"Returns a partition number for a given bq_row.\"\"\"\n",
    "    import json\n",
    "\n",
    "    assert num_partitions == len(ratio)\n",
    "    bucket = sum(map(ord, json.dumps(bq_row))) % sum(ratio)\n",
    "    total = 0\n",
    "    for i, part in enumerate(ratio):\n",
    "        total += part\n",
    "        if bucket < total:\n",
    "            return i\n",
    "    return len(ratio) - 1\n",
    "\n",
    "\n",
    "def run_pipeline(args):\n",
    "    \"\"\"Runs a Beam pipeline to split the dataset\"\"\"\n",
    "\n",
    "    pipeline_options = beam.pipeline.PipelineOptions(flags=[], **args)\n",
    "\n",
    "    raw_data_query = args[\"raw_data_query\"]\n",
    "    exported_data_prefix = args[\"exported_data_prefix\"]\n",
    "    temp_location = args[\"temp_location\"]\n",
    "    project = args[\"project\"]\n",
    "\n",
    "    schema = tfdv.load_schema_text(SCHEMA_LOCATION)\n",
    "\n",
    "    with beam.Pipeline(options=pipeline_options) as pipeline:\n",
    "        with tft_beam.Context(temp_location):\n",
    "\n",
    "            # Read raw BigQuery data.\n",
    "            raw_train_data, raw_eval_data = (\n",
    "                pipeline\n",
    "                | \"Read Raw Data\"\n",
    "                >> beam.io.ReadFromBigQuery(\n",
    "                    query=raw_data_query,\n",
    "                    project=project,\n",
    "                    use_standard_sql=True,\n",
    "                )\n",
    "                | \"Parse Data\" >> beam.Map(parse_bq_record)\n",
    "                | \"Split\" >> beam.Partition(split_dataset, 2, ratio=[8, 2])\n",
    "            )\n",
    "\n",
    "            _ = (\n",
    "                raw_train_data\n",
    "                | \"Write Raw Train Data\"\n",
    "                >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                    file_path_prefix=os.path.join(exported_data_prefix, \"train/\"),\n",
    "                    file_name_suffix=\".gz\",\n",
    "                    coder=tft.coders.ExampleProtoCoder(schema),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            _ = (\n",
    "                raw_eval_data\n",
    "                | \"Write Raw Eval Data\"\n",
    "                >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                    file_path_prefix=os.path.join(exported_data_prefix, \"eval/\"),\n",
    "                    file_name_suffix=\".gz\",\n",
    "                    coder=tft.coders.ExampleProtoCoder(schema),\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "EXPORTED_DATA_PREFIX = os.path.join(BUCKET_URI, \"exported_data\")\n",
    "\n",
    "QUERY_STRING = \"SELECT {},{} FROM {} LIMIT 500\".format(\n",
    "    \"CAST(station_number as STRING) AS station_number,year,month,day\",\n",
    "    \"mean_temp\",\n",
    "    IMPORT_FILE[5:],\n",
    ")\n",
    "JOB_NAME = \"gsod\" + TIMESTAMP\n",
    "\n",
    "args = {\n",
    "    \"runner\": RUNNER,\n",
    "    \"raw_data_query\": QUERY_STRING,\n",
    "    \"exported_data_prefix\": EXPORTED_DATA_PREFIX,\n",
    "    \"temp_location\": os.path.join(BUCKET_URI, \"temp\"),\n",
    "    \"project\": PROJECT_ID,\n",
    "    \"region\": REGION,\n",
    "    \"setup_file\": \"./setup.py\",\n",
    "}\n",
    "\n",
    "print(\"Data preprocessing started...\")\n",
    "run_pipeline(args)\n",
    "print(\"Data preprocessing completed.\")\n",
    "\n",
    "! gsutil ls $EXPORTED_DATA_PREFIX/train\n",
    "! gsutil ls $EXPORTED_DATA_PREFIX/eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "source": [
    "# Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "outputs": [],
   "source": [
    "delete_storage = False\n",
    "\n",
    "if delete_storage or os.getenv(\"IS_TESTING\"):\n",
    "    if \"BUCKET_URI\" in globals():\n",
    "        ! gsutil rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "get_started_dataflow.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

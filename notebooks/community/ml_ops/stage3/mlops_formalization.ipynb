{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "copyright"
      },
      "outputs": [],
      "source": [
        "# Copyright 2021 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title:generic,gcp"
      },
      "source": [
        "# E2E ML on GCP: MLOps stage 3 : formalization\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/mlops_formalization.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/ai/platform/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/mlops_formalization.ipynb\">\n",
        "      Open in Google Cloud Notebooks\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "<br/><br/><br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview:mlops"
      },
      "source": [
        "## Overview\n",
        "\n",
        "\n",
        "This tutorial demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. This tutorial covers stage 3 : formalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset:bq,chicago,lbn"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset used for this tutorial is the [Chicago Taxi](https://www.kaggle.com/chicago/chicago-taxi-trips-bq). The version of the dataset you will use in this tutorial is stored in a public BigQuery table. The trained model predicts whether someone would leave a tip for a taxi fare."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "objective:mlops,stage3,tabular"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you create a MLOps stage 3: formalization process.\n",
        "\n",
        "This tutorial uses the following Vertex AI:\n",
        "\n",
        "- `Vertex AI Pipelines`\n",
        "- `Vertex AI Training`\n",
        "- `Google Cloud Pipeline Components`\n",
        "- `Vertex AI Dataset, and Model resources\n",
        "- `Dataflow`\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Obtain resources from the experimentation stage.\n",
        "    - Baseline model.\n",
        "    - Dataset schema/statistics for baseline model.\n",
        "- Formalize a data preprocessing pipeline.\n",
        "    - Extract columns/rows from BigQuery table to local BigQuery table.\n",
        "    - Use Tensorflow Data Validation library to determine statistics, schema, and features.\n",
        "    - Use Dataflow to preprocess the data.\n",
        "    - Create a Vertex AI Dataset.\n",
        "- Formalize a build model architecture pipeline.\n",
        "    - Create the Vertex AI Model base model.\n",
        "- Formalize a training pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "recommendation:mlops,stage3,tabular"
      },
      "source": [
        "### Recommendations\n",
        "\n",
        "When doing E2E MLOps on Google Cloud for formalization, the following best practices with structured (tabular) data are recommended:\n",
        "\n",
        "- Use pipeline components to automate the training of the model.\n",
        "   - Use pre-built Vertex AI components where available.\n",
        "\n",
        "\n",
        "- Decompose the pipeline into the following sub-pipelines:\n",
        "\n",
        "   - Data pipeline\n",
        "   - Model architecture pipeline\n",
        "   - Training pipeline\n",
        "\n",
        "- The data pipeline should perform the following tasks:\n",
        "    - Do satistical analysis on the dataset using Tensorflow Data Validation library.\n",
        "    - Split the dataset examples into training, validation and test datasets using `Dataflow` components.\n",
        "    - Preprocess and transform the split datasets into machine learning ready format, i.e., `TFRecord`, using `Dataflow` components.\n",
        "    - Preprocess copies of test dataset for testing serving model using `Dataflow` components.\n",
        "    - Create a `Vertex AI Dataset resource` using `Vertex AI Dataset` components.\n",
        "       - Store as metadata the statistics, schema, transformation function, transformed data features, and location of transformed datasets.\n",
        "\n",
        "\n",
        "- The model architecture pipeline should perform the following tasks:\n",
        "    - Retrieve the metadata for the corresponding `Vertex AI Dataset`.\n",
        "    - Construct the input layer to the model architecture according to the transformed feature information in the metadata.\n",
        "    - Construct the body of the model architecture.\n",
        "    - Upload the model architecture's model artifacts as a `Vertex AI Model` using `Vertex AI Model` components.\n",
        "    - Label the `Vertex AI Model` resource as the `base model architecture`.\n",
        "\n",
        "\n",
        "- The training pipeline should perform the following tasks:\n",
        "    - Start an `Vertex AI Experiment` for this training run, and log corresponding tasks, parameters and results.\n",
        "    - Load the corresponding `Vertex AI Dataset` resource.\n",
        "        - Retrieve the metadata for the locations of the training, test and validation datasets.\n",
        "    - Load the corresponding `Vertex AI Model` resource.\n",
        "        - From the metadata, retrieve the location of the model artifacts for the model architectures.\n",
        "        - From the metadata, retrieve the hyperparameters for the current model baseline.\n",
        "        - Load and compile the model artifacts.\n",
        "    - Train the model.\n",
        "        - Train the model with corresponding hyperparameters.\n",
        "        - Track the training with a `Vertex AI Tensorboard` instance.\n",
        "        - Store the trained model artifacts on Cloud Storage.\n",
        "    - Evaluate the model.\n",
        "        - Evaluate the model using the test dataset.\n",
        "        - Store the model's evaluation metrics as metadata.\n",
        "    - Upload the model artifacts to a `Vertex AI Model` resource.\n",
        "        - Add a serving function to the model artifacts.\n",
        "        - Upload the model artifacts + serving function.\n",
        "        - Add label for metadata, including location of training parameters, evaluation metrics and tagging the model instance as a candidate model.\n",
        "    - Create a `Vertex AI Endpoint` resource.\n",
        "    - Deploy the trained `Vertex AI Model` resource to the `Vertex AI Endpoint` resource."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_mlops"
      },
      "source": [
        "## Installations\n",
        "\n",
        "Install *one time* the packages for executing the MLOps notebooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_mlops"
      },
      "outputs": [],
      "source": [
        "ONCE_ONLY = False\n",
        "if ONCE_ONLY:\n",
        "    ! pip3 install -U tensorflow==2.5 $USER_FLAG\n",
        "    ! pip3 install -U tensorflow-data-validation==1.2 $USER_FLAG\n",
        "    ! pip3 install -U tensorflow-transform==1.2 $USER_FLAG\n",
        "    ! pip3 install -U tensorflow-io==0.18 $USER_FLAG\n",
        "    ! pip3 install --upgrade google-cloud-aiplatform[tensorboard] $USER_FLAG\n",
        "    ! pip3 install --upgrade google-cloud-pipeline-components $USER_FLAG\n",
        "    ! pip3 install --upgrade google-cloud-bigquery $USER_FLAG\n",
        "    ! pip3 install --upgrade google-cloud-logging $USER_FLAG\n",
        "    ! pip3 install --upgrade apache-beam[gcp] $USER_FLAG\n",
        "    ! pip3 install --upgrade pyarrow $USER_FLAG\n",
        "    ! pip3 install --upgrade cloudml-hypertune $USER_FLAG\n",
        "    ! pip3 install --upgrade kfp $USER_FLAG\n",
        "    ! pip3 install --upgrade torchvision $USER_FLAG\n",
        "    ! pip3 install --upgrade rpy2 $USER_FLAG\n",
        "    ! pip3 install --upgrade python-tabulate $USER_FLAG\n",
        "    ! pip3 install -U opencv-python-headless==4.5.2.52 $USER_FLAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "restart"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "restart"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "project_id"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_project_id"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_project_id"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID:\", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_gcloud_project_id"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
        "\n",
        "- Americas: `us-central1`\n",
        "- Europe: `europe-west4`\n",
        "- Asia Pacific: `asia-east1`\n",
        "\n",
        "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
        "\n",
        "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "region"
      },
      "outputs": [],
      "source": [
        "REGION = \"us-central1\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "timestamp"
      },
      "source": [
        "#### Timestamp\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "timestamp"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bucket:mbsdk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "When you initialize the Vertex SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bucket"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_bucket"
      },
      "outputs": [],
      "source": [
        "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
        "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_bucket"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "validate_bucket"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "validate_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "set_service_account"
      },
      "source": [
        "#### Service Account\n",
        "\n",
        "**If you don't know your service account**, try to get your service account using `gcloud` command by executing the second cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_service_account"
      },
      "outputs": [],
      "source": [
        "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_service_account"
      },
      "outputs": [],
      "source": [
        "if (\n",
        "    SERVICE_ACCOUNT == \"\"\n",
        "    or SERVICE_ACCOUNT is None\n",
        "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
        "):\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = !gcloud auth list 2>/dev/null\n",
        "    SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
        "    print(\"Service Account:\", SERVICE_ACCOUNT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "set_service_account:pipelines"
      },
      "source": [
        "#### Set service account access for Vertex AI Pipelines\n",
        "\n",
        "Run the following commands to grant your service account access to read and write pipeline artifacts in the bucket that you created in the previous step -- you only need to run these once per service account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_service_account:pipelines"
      },
      "outputs": [],
      "source": [
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_NAME\n",
        "\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_vars"
      },
      "source": [
        "### Set up variables\n",
        "\n",
        "Next, set up some variables used throughout the tutorial.\n",
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "import google.cloud.aiplatform as aip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_tf"
      },
      "source": [
        "#### Import TensorFlow\n",
        "\n",
        "Import the TensorFlow package into your Python environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_tf"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_kfp:namedtuple"
      },
      "outputs": [],
      "source": [
        "from typing import NamedTuple\n",
        "\n",
        "from kfp import dsl\n",
        "from kfp.v2 import compiler\n",
        "from kfp.v2.dsl import component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_gcpc:dataflow"
      },
      "outputs": [],
      "source": [
        "from google_cloud_pipeline_components.v1.dataflow import DataflowPythonJobOp\n",
        "from google_cloud_pipeline_components.v1.wait_gcp_resources import \\\n",
        "    WaitGcpResourcesOp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "outputs": [],
      "source": [
        "aip.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "accelerators:training,prediction,ngpu,mbsdk"
      },
      "source": [
        "#### Set hardware accelerators\n",
        "\n",
        "You can set hardware accelerators for training and prediction.\n",
        "\n",
        "Set the variables `TRAIN_GPU/TRAIN_NGPU` and `DEPLOY_GPU/DEPLOY_NGPU` to use a container image supporting a GPU and the number of GPUs allocated to the virtual machine (VM) instance. For example, to use a GPU container image with 4 Nvidia Telsa K80 GPUs allocated to each VM, you would specify:\n",
        "\n",
        "    (aip.AcceleratorType.NVIDIA_TESLA_K80, 4)\n",
        "\n",
        "\n",
        "Otherwise specify `(None, None)` to use a container image to run on a CPU.\n",
        "\n",
        "Learn more about [hardware accelerator support for your region](https://cloud.google.com/vertex-ai/docs/general/locations#accelerators).\n",
        "\n",
        "*Note*: TF releases before 2.3 for GPU support will fail to load the custom model in this tutorial. It is a known issue and fixed in TF 2.3. This is caused by static graph ops that are generated in the serving function. If you encounter this issue on your own custom models, use a container image for TF 2.3 with GPU support."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "accelerators:training,prediction,ngpu,mbsdk"
      },
      "outputs": [],
      "source": [
        "if os.getenv(\"IS_TESTING_TRAIN_GPU\"):\n",
        "    TRAIN_GPU, TRAIN_NGPU = (\n",
        "        aip.gapic.AcceleratorType.NVIDIA_TESLA_K80,\n",
        "        int(os.getenv(\"IS_TESTING_TRAIN_GPU\")),\n",
        "    )\n",
        "else:\n",
        "    TRAIN_GPU, TRAIN_NGPU = (aip.gapic.AcceleratorType.NVIDIA_TESLA_K80, 1)\n",
        "\n",
        "if os.getenv(\"IS_TESTING_DEPLOY_GPU\"):\n",
        "    DEPLOY_GPU, DEPLOY_NGPU = (\n",
        "        aip.gapic.AcceleratorType.NVIDIA_TESLA_K80,\n",
        "        int(os.getenv(\"IS_TESTING_DEPLOY_GPU\")),\n",
        "    )\n",
        "else:\n",
        "    DEPLOY_GPU, DEPLOY_NGPU = (None, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "container:training,prediction"
      },
      "source": [
        "#### Set pre-built containers\n",
        "\n",
        "Set the pre-built Docker container image for training and prediction.\n",
        "\n",
        "\n",
        "For the latest list, see [Pre-built containers for training](https://cloud.google.com/ai-platform-unified/docs/training/pre-built-containers).\n",
        "\n",
        "\n",
        "For the latest list, see [Pre-built containers for prediction](https://cloud.google.com/ai-platform-unified/docs/predictions/pre-built-containers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "container:training,prediction"
      },
      "outputs": [],
      "source": [
        "if os.getenv(\"IS_TESTING_TF\"):\n",
        "    TF = os.getenv(\"IS_TESTING_TF\")\n",
        "else:\n",
        "    TF = \"2.5\".replace(\".\", \"-\")\n",
        "\n",
        "if TF[0] == \"2\":\n",
        "    if TRAIN_GPU:\n",
        "        TRAIN_VERSION = \"tf-gpu.{}\".format(TF)\n",
        "    else:\n",
        "        TRAIN_VERSION = \"tf-cpu.{}\".format(TF)\n",
        "    if DEPLOY_GPU:\n",
        "        DEPLOY_VERSION = \"tf2-gpu.{}\".format(TF)\n",
        "    else:\n",
        "        DEPLOY_VERSION = \"tf2-cpu.{}\".format(TF)\n",
        "else:\n",
        "    if TRAIN_GPU:\n",
        "        TRAIN_VERSION = \"tf-gpu.{}\".format(TF)\n",
        "    else:\n",
        "        TRAIN_VERSION = \"tf-cpu.{}\".format(TF)\n",
        "    if DEPLOY_GPU:\n",
        "        DEPLOY_VERSION = \"tf-gpu.{}\".format(TF)\n",
        "    else:\n",
        "        DEPLOY_VERSION = \"tf-cpu.{}\".format(TF)\n",
        "\n",
        "TRAIN_IMAGE = \"{}-docker.pkg.dev/vertex-ai/training/{}:latest\".format(\n",
        "    REGION.split(\"-\")[0], TRAIN_VERSION\n",
        ")\n",
        "DEPLOY_IMAGE = \"{}-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(\n",
        "    REGION.split(\"-\")[0], DEPLOY_VERSION\n",
        ")\n",
        "\n",
        "print(\"Training:\", TRAIN_IMAGE, TRAIN_GPU, TRAIN_NGPU)\n",
        "print(\"Deployment:\", DEPLOY_IMAGE, DEPLOY_GPU, DEPLOY_NGPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "machine:training,prediction"
      },
      "source": [
        "#### Set machine type\n",
        "\n",
        "Next, set the machine type to use for training and prediction.\n",
        "\n",
        "- Set the variables `TRAIN_COMPUTE` and `DEPLOY_COMPUTE` to configure  the compute resources for the VMs you will use for for training and prediction.\n",
        " - `machine type`\n",
        "     - `n1-standard`: 3.75GB of memory per vCPU.\n",
        "     - `n1-highmem`: 6.5GB of memory per vCPU\n",
        "     - `n1-highcpu`: 0.9 GB of memory per vCPU\n",
        " - `vCPUs`: number of \\[2, 4, 8, 16, 32, 64, 96 \\]\n",
        "\n",
        "*Note: The following is not supported for training:*\n",
        "\n",
        " - `standard`: 2 vCPUs\n",
        " - `highcpu`: 2, 4 and 8 vCPUs\n",
        "\n",
        "*Note: You may also use n2 and e2 machine types for training and deployment, but they do not support GPUs*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "machine:training,prediction"
      },
      "outputs": [],
      "source": [
        "if os.getenv(\"IS_TESTING_TRAIN_MACHINE\"):\n",
        "    MACHINE_TYPE = os.getenv(\"IS_TESTING_TRAIN_MACHINE\")\n",
        "else:\n",
        "    MACHINE_TYPE = \"n1-standard\"\n",
        "\n",
        "VCPU = \"4\"\n",
        "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
        "print(\"Train machine type\", TRAIN_COMPUTE)\n",
        "\n",
        "if os.getenv(\"IS_TESTING_DEPLOY_MACHINE\"):\n",
        "    MACHINE_TYPE = os.getenv(\"IS_TESTING_DEPLOY_MACHINE\")\n",
        "else:\n",
        "    MACHINE_TYPE = \"n1-standard\"\n",
        "\n",
        "VCPU = \"4\"\n",
        "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
        "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_file:u_dataset,bq"
      },
      "source": [
        "#### Location of BigQuery training data.\n",
        "\n",
        "Now set the variable `IMPORT_FILE` to the location of the data table in BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_file:chicago,bq,lbn"
      },
      "outputs": [],
      "source": [
        "IMPORT_FILE = \"bq://bigquery-public-data.chicago_taxi_trips.taxi_trips\"\n",
        "BQ_TABLE = \"bigquery-public-data.chicago_taxi_trips.taxi_trips\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "find_dataset:bq"
      },
      "source": [
        "### Retrieve the dataset from stage 1\n",
        "\n",
        "Next, retrieve the dataset you created during stage 1 with the helper function `find_dataset()`. This helper function finds all the datasets whose display name matches the specified prefix and import format (e.g., bq). Finally it sorts the matches by create time and returns the latest version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "find_dataset:bq"
      },
      "outputs": [],
      "source": [
        "def find_dataset(display_name_prefix, import_format):\n",
        "    matches = []\n",
        "    datasets = aip.TabularDataset.list()\n",
        "    for dataset in datasets:\n",
        "        if dataset.display_name.startswith(display_name_prefix):\n",
        "            try:\n",
        "                if (\n",
        "                    \"bq\" == import_format\n",
        "                    and dataset.to_dict()[\"metadata\"][\"inputConfig\"][\"bigquerySource\"]\n",
        "                ):\n",
        "                    matches.append(dataset)\n",
        "                if (\n",
        "                    \"csv\" == import_format\n",
        "                    and dataset.to_dict()[\"metadata\"][\"inputConfig\"][\"gcsSource\"]\n",
        "                ):\n",
        "                    matches.append(dataset)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    create_time = None\n",
        "    for match in matches:\n",
        "        if create_time is None or match.create_time > create_time:\n",
        "            create_time = match.create_time\n",
        "            dataset = match\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "dataset = find_dataset(\"Chicago Taxi\", \"bq\")\n",
        "\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_dataset_user_metadata"
      },
      "source": [
        "### Load dataset's user metadata\n",
        "\n",
        "Load the user metadata for the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_dataset_user_metadata"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "try:\n",
        "    with tf.io.gfile.GFile(\n",
        "        \"gs://\" + dataset.labels[\"user_metadata\"] + \"/metadata.jsonl\", \"r\"\n",
        "    ) as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    print(metadata)\n",
        "except:\n",
        "    print(\"no metadata\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "find_model"
      },
      "source": [
        "### Retrieve the model architecture and baseline model from stage 2\n",
        "\n",
        "Next, retrieve the model architecture and baseline trained model you created during stage 2 with the helper function `find_model()`. This helper function finds all the models whose display name matches the specified prefix and contains the specified label. Finally it sorts the matches by create time and returns the latest version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "find_model"
      },
      "outputs": [],
      "source": [
        "def find_model(display_name_prefix, label=None):\n",
        "    matches = []\n",
        "    models = aip.Model.list()\n",
        "    for model in models:\n",
        "        if model.display_name.startswith(display_name_prefix):\n",
        "            try:\n",
        "                if label in model.to_dict()[\"labels\"].keys():\n",
        "                    matches.append(model)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    model = None\n",
        "    create_time = None\n",
        "    for match in matches:\n",
        "        if create_time is None or match.create_time > create_time:\n",
        "            create_time = match.create_time\n",
        "            model = match\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "base_model = find_model(\"chicago\", \"base_model\")\n",
        "baseline_model = find_model(\"chicago\", \"user_metadata\")\n",
        "\n",
        "print(base_model)\n",
        "print(baseline_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_model_user_metadata:baseline"
      },
      "source": [
        "### Load baseline models's user metadata\n",
        "\n",
        "Load the user metadata for the baseline model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_model_user_metadata:baseline"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "try:\n",
        "    with tf.io.gfile.GFile(\n",
        "        \"gs://\" + baseline_model.labels[\"user_metadata\"] + \"/metadata.jsonl\", \"r\"\n",
        "    ) as f:\n",
        "        baseline_metadata = json.load(f)\n",
        "        print(baseline_metadata)\n",
        "\n",
        "        with tf.io.gfile.GFile(baseline_metadata[\"train_eval_metrics\"], \"r\") as f:\n",
        "            baseline_metrics = json.load(f)\n",
        "            print(baseline_metrics)\n",
        "except:\n",
        "    print(\"no metadata\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "formalize_pipeline_intro"
      },
      "source": [
        "## Formalizing pipelines introduction\n",
        "\n",
        "A primary reason for formalizing the training and deployment of a model into a pipeline, is that overtime things will change and you will want to rebuild/retrain your model. A pipeline provides the ability to integrate these tasks as an automated process within a CI/CD process.\n",
        "\n",
        "While one generally represents a formalized pipeline as a single e2e pipeline, in practice you decompose the e2e pipeline into the following sub-pipelines:\n",
        "\n",
        "- data pipeline\n",
        "    - data analysis\n",
        "    - data preprocessing\n",
        "- model pipeline\n",
        "    - model architecture construction\n",
        "    - base model storage\n",
        "- training pipeline\n",
        "    - model training\n",
        "    - model evaluation\n",
        "- deployment pipeline\n",
        "    - model candidate\n",
        "    - pre-production deployment evaluations\n",
        "    - deployment to production"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "formalize_data_pipeline_intro"
      },
      "source": [
        "## Formalizing data pipeline introduction\n",
        "\n",
        "The data pipeline consists of data analysis and data preprocessing tasks.\n",
        "\n",
        "### Data analysis task\n",
        "\n",
        "This task performs an analysis of the dataset to determine it's statistical distribution. This distribution is then used to build a dataset schema. The schema is then used by the data preprocessing task. Additionally for tabular data, the default feature types per feature are determined -- i.e., categorical, numeric.\n",
        "\n",
        "### Data preprocessing task\n",
        "\n",
        "This task performs a conversion of the raw dataset data into one or more machine learning ready formats. The dataset schema is used to determine how to preprocess the data. Other tasks include: splitting the dataset into training, test and validation, and encoding and storing the preprocessed data to disk.\n",
        "\n",
        "### Triggers\n",
        "\n",
        "Within the CI/CD process, the data pipeline is triggered for one or more of the following example reasons, while not exhaustive:\n",
        "\n",
        "- New data added to the dataset.\n",
        "- Addition or subtraction of features.\n",
        "- Code changes to the preprocessing tasks.\n",
        "- Code changes to the feature engineering tasks.\n",
        "- Input layer changes that invalidate the stored preprocessed data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "make_bq_dataset_component:chicago"
      },
      "source": [
        "### Create component for creating a local BigQuery dataset\n",
        "\n",
        "Next, you create a component which makes a local copy, -- i.e., in your project, of the BigQuery Chicago Taxi dataset, where:\n",
        "\n",
        "- Select features to include\n",
        "- Select criteria for including rows\n",
        "- Perform feature engineering.\n",
        "\n",
        "This component returns as an artifact the BigQuery path to the local dataset copy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "make_bq_dataset_component:chicago"
      },
      "outputs": [],
      "source": [
        "@component(packages_to_install=[\"bigquery\"])\n",
        "def make_chicago_bq_dataset(bq_table: str, year: int, limit: int, project: str) -> str:\n",
        "    from google.cloud import bigquery\n",
        "\n",
        "    bqclient = bigquery.Client(project=project)\n",
        "\n",
        "    BQ_DATASET = bq_table.split(\".\")[1]\n",
        "    BQ_TABLE_COPY = f\"{project}.{BQ_DATASET}.taxi_trips\"\n",
        "\n",
        "    if bq_table.startswith(\"bq://\"):\n",
        "        bq_table = bq_table[5:]\n",
        "\n",
        "    query = f\"\"\"\n",
        "    CREATE OR REPLACE TABLE `{BQ_TABLE_COPY}`\n",
        "    AS (\n",
        "        WITH\n",
        "          taxitrips AS (\n",
        "          SELECT\n",
        "            trip_start_timestamp,\n",
        "            trip_seconds,\n",
        "            trip_miles,\n",
        "            payment_type,\n",
        "            pickup_longitude,\n",
        "            pickup_latitude,\n",
        "            dropoff_longitude,\n",
        "            dropoff_latitude,\n",
        "            tips,\n",
        "            fare\n",
        "          FROM\n",
        "            `{bq_table}`\n",
        "          WHERE pickup_longitude IS NOT NULL\n",
        "          AND pickup_latitude IS NOT NULL\n",
        "          AND dropoff_longitude IS NOT NULL\n",
        "          AND dropoff_latitude IS NOT NULL\n",
        "          AND trip_miles > 0\n",
        "          AND trip_seconds > 0\n",
        "          AND fare > 0\n",
        "          AND EXTRACT(YEAR FROM trip_start_timestamp) = {year}\n",
        "        )\n",
        "\n",
        "        SELECT\n",
        "          EXTRACT(MONTH from trip_start_timestamp) as trip_month,\n",
        "          EXTRACT(DAY from trip_start_timestamp) as trip_day,\n",
        "          EXTRACT(DAYOFWEEK from trip_start_timestamp) as trip_day_of_week,\n",
        "          EXTRACT(HOUR from trip_start_timestamp) as trip_hour,\n",
        "          CAST(trip_seconds AS FLOAT64) as trip_seconds,\n",
        "          trip_miles,\n",
        "          payment_type,\n",
        "          ST_AsText(\n",
        "              ST_SnapToGrid(ST_GeogPoint(pickup_longitude, pickup_latitude), 0.1)\n",
        "          ) AS pickup_grid,\n",
        "          ST_AsText(\n",
        "              ST_SnapToGrid(ST_GeogPoint(dropoff_longitude, dropoff_latitude), 0.1)\n",
        "          ) AS dropoff_grid,\n",
        "          ST_Distance(\n",
        "              ST_GeogPoint(pickup_longitude, pickup_latitude),\n",
        "              ST_GeogPoint(dropoff_longitude, dropoff_latitude)\n",
        "          ) AS euclidean,\n",
        "          CONCAT(\n",
        "              ST_AsText(ST_SnapToGrid(ST_GeogPoint(pickup_longitude,\n",
        "                  pickup_latitude), 0.1)),\n",
        "              ST_AsText(ST_SnapToGrid(ST_GeogPoint(dropoff_longitude,\n",
        "                  dropoff_latitude), 0.1))\n",
        "          ) AS loc_cross,\n",
        "          IF((tips/fare >= 0.2), 1, 0) AS tip_bin,\n",
        "        FROM\n",
        "          taxitrips\n",
        "        LIMIT {limit}\n",
        "    )\n",
        "    \"\"\"\n",
        "\n",
        "    response = bqclient.query(query)\n",
        "    _ = response.result()\n",
        "\n",
        "    return BQ_TABLE_COPY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "make_data_analysis_component"
      },
      "source": [
        "### Create component for performing data analysis on a BigQuery dataset table\n",
        "\n",
        "Next, you create a component which performs data analysis on a BigQuery dataset table using Tensorflow Data Validation library, where:\n",
        "\n",
        "- Create a client connection to BigQuery.\n",
        "- Extract the BigQuery table to a pandas dataframe.\n",
        "- Use Tensorflow Data Validation library (TFDV) to generate the dataset statistics\n",
        "- Use Tensorflow Data Validation library (TFDV) to generate the dataset schema\n",
        "- Determine feature types (numeric vs categorical) from the statistics.\n",
        "- Write statistics and data to Cloud Storage bucket.\n",
        "\n",
        "This component returns as an artifact a dictionary representing the dataset metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "make_data_analysis_component"
      },
      "outputs": [],
      "source": [
        "@component(\n",
        "    packages_to_install=[\n",
        "        \"tensorflow\",\n",
        "        \"tensorflow-data-validation==1.2\",\n",
        "        \"google-cloud-bigquery\",\n",
        "    ]\n",
        ")\n",
        "def data_analysis(\n",
        "    bq_table: str, label_column: str, data_bucket: str, project: str\n",
        ") -> dict:\n",
        "\n",
        "    import json\n",
        "\n",
        "    import tensorflow as tf\n",
        "    import tensorflow_data_validation as tfdv\n",
        "    from google.cloud import bigquery\n",
        "\n",
        "    bqclient = bigquery.Client(project=project)\n",
        "\n",
        "    table = bigquery.TableReference.from_string(bq_table)\n",
        "\n",
        "    rows = bqclient.list_rows(table)\n",
        "\n",
        "    dataframe = rows.to_dataframe()\n",
        "\n",
        "    stats = tfdv.generate_statistics_from_dataframe(\n",
        "        dataframe=dataframe,\n",
        "        stats_options=tfdv.StatsOptions(\n",
        "            label_feature=label_column, sample_rate=1, num_top_values=50\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    tfdv.write_stats_text(stats, data_bucket + \"/statistics.jsonl\")\n",
        "\n",
        "    NUMERIC_FEATURES = []\n",
        "    CATEGORICAL_FEATURES = []\n",
        "    for _ in range(len(stats.datasets[0].features)):\n",
        "        if stats.datasets[0].features[_].path.step[0] == label_column:\n",
        "            continue\n",
        "        if stats.datasets[0].features[_].type == 0:  # int\n",
        "            CATEGORICAL_FEATURES.append(stats.datasets[0].features[_].path.step[0])\n",
        "        elif stats.datasets[0].features[_].type == 1:  # float\n",
        "            NUMERIC_FEATURES.append(stats.datasets[0].features[_].path.step[0])\n",
        "        elif stats.datasets[0].features[_].type == 2:  # string\n",
        "            CATEGORICAL_FEATURES.append(stats.datasets[0].features[_].path.step[0])\n",
        "\n",
        "    schema = tfdv.infer_schema(statistics=stats)\n",
        "\n",
        "    tfdv.write_schema_text(output_path=data_bucket + \"/schema.txt\", schema=schema)\n",
        "\n",
        "    metadata = {\n",
        "        \"label_column\": label_column,\n",
        "        \"statistics\": data_bucket + \"/statistics.jsonl\",\n",
        "        \"schema\": data_bucket + \"/schema.txt\",\n",
        "        \"numeric_features\": NUMERIC_FEATURES,\n",
        "        \"categorical_features\": CATEGORICAL_FEATURES,\n",
        "    }\n",
        "\n",
        "    with tf.io.gfile.GFile(data_bucket + \"/metadata.jsonl\", \"w\") as f:\n",
        "        json.dump(metadata, f)\n",
        "\n",
        "    return metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "make_dataflow_args:tabular"
      },
      "source": [
        "### Create constructing the run arguments for Dataflow component\n",
        "\n",
        "Next, you create a component for constructing the run arguments for the subsequent Dataflow component."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "make_dataflow_args:tabular"
      },
      "outputs": [],
      "source": [
        "@component()\n",
        "def make_dataflow_args(\n",
        "    bucket: str,\n",
        "    bq_table: str,\n",
        "    setup_file: str,\n",
        "    metadata: dict,\n",
        "    transformed_data_prefix: str,\n",
        "    transform_artifacts_dir: str,\n",
        "    exported_tfrec_prefix: str,\n",
        "    exported_jsonl_prefix: str,\n",
        "    label: str,\n",
        ") -> list:\n",
        "    return [\n",
        "        \"--bucket\",\n",
        "        bucket,\n",
        "        \"--bq-table\",\n",
        "        bq_table,\n",
        "        \"--runner\",\n",
        "        \"DataflowRunner\",\n",
        "        \"--setup_file\",\n",
        "        setup_file,\n",
        "        \"--metadata\",\n",
        "        str(metadata),\n",
        "        \"--transformed-data-prefix\",\n",
        "        transformed_data_prefix,\n",
        "        \"--transform-artifacts-dir\",\n",
        "        transform_artifacts_dir,\n",
        "        \"--exported-tfrec-prefix\",\n",
        "        exported_tfrec_prefix,\n",
        "        \"--exported-jsonl-prefix\",\n",
        "        exported_jsonl_prefix,\n",
        "        \"--label\",\n",
        "        label,\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "write_dataflow_script:tabular"
      },
      "source": [
        "### Write the Dataflow Python module for preprocessing the data.\n",
        "\n",
        "Next, you write the Python script for preprocessing the data. This script will be used by the subsequent Dataflow component.\n",
        "\n",
        "#### Dataset splitting\n",
        "\n",
        "- Query the BigQuery table for all examples (parse_bq_record).\n",
        "- Split the examples into training, evaluation and test datasets (split_dataset).\n",
        "\n",
        "#### Data preprocessing\n",
        "\n",
        "- Preprocess each example (preprocessing_fn).\n",
        "- Write the preprocessed data to a Cloud Storage bucket as TFRecords.\n",
        "- Write the transformation function artifacts to a Cloud Storage bucket.\n",
        "- Write the raw (unprocessed) examples to a Cloud Storage bucket as TFRecords.\n",
        "- Write the raw (unprocessed) examples to a Cloud Storage bucket as JSONL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "write_dataflow_script:tabular"
      },
      "outputs": [],
      "source": [
        "%%writefile preprocess.py\n",
        "\n",
        "import argparse\n",
        "import logging\n",
        "import json\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "import tensorflow_transform.beam as tft_beam\n",
        "import tensorflow_data_validation as tfdv\n",
        "\n",
        "import apache_beam as beam\n",
        "from apache_beam.io import ReadFromText\n",
        "from apache_beam.io import WriteToText\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "from apache_beam.options.pipeline_options import SetupOptions\n",
        "\n",
        "def run(argv=None):\n",
        "    \"\"\" Main entry: data management\"\"\"\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--bq-table', dest='bq_table', type=str)\n",
        "    parser.add_argument('--bucket', dest='bucket', type=str)\n",
        "    parser.add_argument('--metadata', dest='metadata', type=str)\n",
        "    parser.add_argument('--transformed-data-prefix', dest='transformed_data_prefix', type=str)\n",
        "    parser.add_argument('--transform-artifacts-dir', dest='transform_artifacts_dir', type=str)\n",
        "    parser.add_argument('--exported-tfrec-prefix', dest='exported_tfrec_prefix', type=str)\n",
        "    parser.add_argument('--exported-jsonl-prefix', dest='exported_jsonl_prefix', type=str)\n",
        "    parser.add_argument('--label', dest='label', type=str)\n",
        "\n",
        "    args, pipeline_args = parser.parse_known_args(argv)\n",
        "\n",
        "    logging.info(\"ARGS\")\n",
        "    logging.info(args)\n",
        "    logging.info(\"PIPELINE ARGS\")\n",
        "    logging.info(pipeline_args)\n",
        "\n",
        "    metadata = json.loads(args.metadata.replace(\"'\", '\"'))\n",
        "\n",
        "    numeric_features = metadata['numeric_features']\n",
        "    categorical_features = metadata['categorical_features']\n",
        "    schema_location = metadata['schema']\n",
        "\n",
        "    for i in range(0, len(pipeline_args), 2):\n",
        "        if \"--temp_location\" == pipeline_args[i]:\n",
        "            temp_location = pipeline_args[i+1]\n",
        "        elif \"--project\" == pipeline_args[i]:\n",
        "            project = pipeline_args[i+1]\n",
        "\n",
        "    exported_train = args.bucket + '/exported_data/train'\n",
        "    exported_eval  = args.bucket + '/exported_data/eval'\n",
        "\n",
        "    logging.info(\"Get schema\")\n",
        "    schema = tfdv.load_schema_text(schema_location)\n",
        "    feature_spec = tft.tf_metadata.schema_utils.schema_as_feature_spec(\n",
        "        schema\n",
        "    ).feature_spec\n",
        "\n",
        "    raw_metadata = tft.tf_metadata.dataset_metadata.DatasetMetadata(\n",
        "        tft.tf_metadata.schema_utils.schema_from_feature_spec(feature_spec)\n",
        "    )\n",
        "    query = f\"SELECT * FROM {args.bq_table}\"\n",
        "\n",
        "    logging.info(\"Preprocess the data\")\n",
        "    pipeline_options = PipelineOptions(pipeline_args)\n",
        "    pipeline_options.view_as(SetupOptions).save_main_session = True\n",
        "    with beam.Pipeline(options=pipeline_options) as pipeline:\n",
        "        with tft_beam.Context(temp_location):\n",
        "\n",
        "            def parse_bq_record(bq_record):\n",
        "                \"\"\"Parses a bq_record to a dictionary.\"\"\"\n",
        "                output = {}\n",
        "                for key in bq_record:\n",
        "                    output[key] = [bq_record[key]]\n",
        "                return output\n",
        "\n",
        "            def split_dataset(bq_row, num_partitions, ratio):\n",
        "                \"\"\"Returns a partition number for a given bq_row.\"\"\"\n",
        "                import json\n",
        "\n",
        "                assert num_partitions == len(ratio)\n",
        "                bucket = sum(map(ord, json.dumps(bq_row))) % sum(ratio)\n",
        "                total = 0\n",
        "                for i, part in enumerate(ratio):\n",
        "                    total += part\n",
        "                    if bucket < total:\n",
        "                        return i\n",
        "                return len(ratio) - 1\n",
        "\n",
        "            def convert_to_jsonl(data, label=None):\n",
        "                ''' Converts a parsed record to JSON '''\n",
        "                if label:\n",
        "                    del data[label]\n",
        "                return json.dumps(data)\n",
        "\n",
        "            def preprocessing_fn(inputs):\n",
        "                outputs = {}\n",
        "                for key in inputs.keys():\n",
        "                    if key in numeric_features:\n",
        "                        outputs[key] = tft.scale_to_z_score(inputs[key])\n",
        "                    elif key in categorical_features:\n",
        "                        outputs[key] = tft.compute_and_apply_vocabulary(\n",
        "                                            inputs[key],\n",
        "                                            num_oov_buckets=1,\n",
        "                                            vocab_filename=key,\n",
        "                                        )\n",
        "                    else:\n",
        "                        outputs[key] = inputs[key]\n",
        "                    outputs[key] = tf.squeeze(outputs[key], -1)\n",
        "                return outputs\n",
        "\n",
        "\n",
        "            # Read raw BigQuery data.\n",
        "            raw_train_data, raw_val_data, raw_test_data = (\n",
        "                pipeline\n",
        "                | \"Read Raw Data\"\n",
        "                >> beam.io.ReadFromBigQuery(\n",
        "                    query=query,\n",
        "                    project=project,\n",
        "                    use_standard_sql=True,\n",
        "                )\n",
        "                | \"Parse Data\" >> beam.Map(parse_bq_record)\n",
        "                | \"Split\" >> beam.Partition(split_dataset, 3, ratio=[8, 1, 1])\n",
        "            )\n",
        "\n",
        "            # Create a train_dataset from the data and schema.\n",
        "            raw_train_dataset = (raw_train_data, raw_metadata)\n",
        "\n",
        "            # Analyze and transform raw_train_dataset to produced transformed_train_dataset and transform_fn.\n",
        "            transformed_train_dataset, transform_fn = (\n",
        "                raw_train_dataset\n",
        "                | \"Analyze & Transform\"\n",
        "                >> tft_beam.AnalyzeAndTransformDataset(preprocessing_fn)\n",
        "            )\n",
        "\n",
        "            # Get data and schema separately from the transformed_dataset.\n",
        "            transformed_train_data, transformed_metadata = transformed_train_dataset\n",
        "\n",
        "            # Get data and schema separately from the transformed_dataset.\n",
        "            transformed_train_data, transformed_metadata = transformed_train_dataset\n",
        "\n",
        "            # Write transformed train data.\n",
        "            _ = (\n",
        "                transformed_train_data\n",
        "                | \"Write Transformed Train Data\"\n",
        "                >> beam.io.tfrecordio.WriteToTFRecord(\n",
        "                    file_path_prefix=os.path.join(\n",
        "                        args.transformed_data_prefix, \"train/data\"\n",
        "                    ),\n",
        "                    file_name_suffix=\".gz\",\n",
        "                    coder=tft.coders.ExampleProtoCoder(transformed_metadata.schema),\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Create a val_dataset from the data and schema.\n",
        "            raw_val_dataset = (raw_val_data, raw_metadata)\n",
        "\n",
        "            # Transform raw_val_dataset to produced transformed_val_dataset using transform_fn.\n",
        "            transformed_val_dataset = (\n",
        "                raw_val_dataset,\n",
        "                transform_fn,\n",
        "            ) | \"Transform Validation Data\" >> tft_beam.TransformDataset()\n",
        "\n",
        "            # Get data from the transformed_val_dataset.\n",
        "            transformed_val_data, _ = transformed_val_dataset\n",
        "\n",
        "            # Write transformed val data.\n",
        "            _ = (\n",
        "                transformed_val_data\n",
        "                | \"Write Transformed Validation Data\"\n",
        "                >> beam.io.tfrecordio.WriteToTFRecord(\n",
        "                    file_path_prefix=os.path.join(args.transformed_data_prefix, \"val/data\"),\n",
        "                    file_name_suffix=\".gz\",\n",
        "                    coder=tft.coders.ExampleProtoCoder(transformed_metadata.schema),\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Create a test_dataset from the data and schema.\n",
        "            raw_test_dataset = (raw_test_data, raw_metadata)\n",
        "\n",
        "            # Transform raw_test_dataset to produced transformed_test_dataset using transform_fn.\n",
        "            transformed_test_dataset = (\n",
        "                raw_test_dataset,\n",
        "                transform_fn,\n",
        "            ) | \"Transform Test Data\" >> tft_beam.TransformDataset()\n",
        "\n",
        "\n",
        "            # Get data from the transformed_test_dataset.\n",
        "            transformed_test_data, _ = transformed_test_dataset\n",
        "\n",
        "            # write transformed test data.\n",
        "            _ = (\n",
        "                transformed_test_data\n",
        "                | \"Write Transformed Test Data\"\n",
        "                >> beam.io.tfrecordio.WriteToTFRecord(\n",
        "                    file_path_prefix=os.path.join(args.transformed_data_prefix, \"test/data\"),\n",
        "                    file_name_suffix=\".gz\",\n",
        "                    coder=tft.coders.ExampleProtoCoder(transformed_metadata.schema),\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Write transform_fn.\n",
        "            _ = transform_fn | \"Write Transform Artifacts\" >> tft_beam.WriteTransformFn(\n",
        "                args.transform_artifacts_dir\n",
        "            )\n",
        "\n",
        "            # Write raw test data to GCS as TF Records\n",
        "            _ = raw_test_data | \"Write TF Test Data\" >> beam.io.tfrecordio.WriteToTFRecord(\n",
        "                file_path_prefix=os.path.join(args.exported_tfrec_prefix, \"data\"),\n",
        "                file_name_suffix=\".tfrecord\",\n",
        "                coder=tft.coders.ExampleProtoCoder(raw_metadata.schema),\n",
        "            )\n",
        "\n",
        "            # Convert raw test data to JSON (for batch prediction)\n",
        "            json_test_data = (\n",
        "                raw_test_data\n",
        "            ) | \"Convert Batch Test Data\" >> beam.Map(convert_to_jsonl, label=args.label)\n",
        "\n",
        "            # Write raw test data to GCS as JSONL files.\n",
        "            _ = json_test_data | \"Write JSONL Test Data\" >> beam.io.WriteToText(\n",
        "                file_path_prefix=args.exported_jsonl_prefix, file_name_suffix=\".jsonl\"\n",
        "            )\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "    run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "write_dataflow_requirements:tabular"
      },
      "source": [
        "#### Write the requirements (installs) for the Dataflow (Apache Beam) pipeline module\n",
        "\n",
        "Next, create the `requirements.txt` file to specify Python modules that are required to be installed for executing the Apache Beam pipeline module -- in this case, `apache-beam`, `tensorflow-transform` and `tensorflow-data-validation` are required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "write_dataflow_requirements:tabular"
      },
      "outputs": [],
      "source": [
        "%%writefile requirements.txt\n",
        "apache-beam\n",
        "tensorflow-transform==1.2.0\n",
        "tensorflow-data-validation==1.2\n",
        "future"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "write_dataflow_setup:tabular"
      },
      "source": [
        "#### Prepare package requirements for Dataflow job.\n",
        "\n",
        "Before you can run a Dataflow job, you need to specify the package requirements for the worker pool that will execute the job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "write_dataflow_setup:tabular"
      },
      "outputs": [],
      "source": [
        "%%writefile setup.py\n",
        "import setuptools\n",
        "\n",
        "REQUIRED_PACKAGES = [\n",
        "    'tensorflow-transform==1.2.0',\n",
        "    'tensorflow-data-validation==1.2',\n",
        "    'future'\n",
        "]\n",
        "PACKAGE_NAME = 'my_package'\n",
        "PACKAGE_VERSION = '0.0.1'\n",
        "setuptools.setup(\n",
        "    name=PACKAGE_NAME,\n",
        "    version=PACKAGE_VERSION,\n",
        "    description='preprocessing transformation',\n",
        "    install_requires=REQUIRED_PACKAGES,\n",
        "    author=\"cdpe@google.com\",\n",
        "    packages=setuptools.find_packages()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "copy_to_gcs:preprocess"
      },
      "source": [
        "#### Copy python module and requirements file to Cloud Storage\n",
        "\n",
        "Next, you copy the Python module, requirements and setup file to your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "copy_to_gcs:preprocess"
      },
      "outputs": [],
      "source": [
        "GCS_PREPROCESS_PY = BUCKET_NAME + \"/preprocess.py\"\n",
        "! gsutil cp preprocess.py $GCS_PREPROCESS_PY\n",
        "GCS_REQUIREMENTS_TXT = BUCKET_NAME + \"/requirements.txt\"\n",
        "! gsutil cp requirements.txt $GCS_REQUIREMENTS_TXT\n",
        "GCS_SETUP_PY = BUCKET_NAME + \"/setup.py\"\n",
        "! gsutil cp setup.py $GCS_SETUP_PY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "make_transform_analysis_component"
      },
      "source": [
        "### Create transformed data analysis component\n",
        "\n",
        "Next, you create a component which performs data analysis on the transformed training data using Tensorflow Transform, where:\n",
        "\n",
        "- Load the transformation function artifacts output as `TFTTransformOutput`.\n",
        "- Using the transformed output, determine the number of unique instances per categorical feature.\n",
        "- If the number of unique instances > 10, convert from categorical to embedding feature type.\n",
        "- Update the metadata file for the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "make_transform_analysis_component"
      },
      "outputs": [],
      "source": [
        "@component(packages_to_install=[\"tensorflow\", \"tensorflow-transform==1.2.0\", \"future\"])\n",
        "def transformed_data_analysis(\n",
        "    metadata_location: str,\n",
        "    transformed_data_prefix: str,\n",
        "    transform_artifacts_dir: str,\n",
        "    exported_jsonl_prefix: str,\n",
        "    exported_tfrec_prefix: str,\n",
        ") -> dict:\n",
        "    import json\n",
        "\n",
        "    import tensorflow as tf\n",
        "    import tensorflow_transform as tft\n",
        "\n",
        "    tft_output = tft.TFTransformOutput(transform_artifacts_dir)\n",
        "\n",
        "    with tf.io.gfile.GFile(metadata_location, \"r\") as f:\n",
        "        metadata = json.load(f)\n",
        "    categorical_features = metadata[\"categorical_features\"]\n",
        "\n",
        "    CATEGORICAL_FEATURES = []\n",
        "    EMBEDDING_FEATURES = []\n",
        "    for feature in categorical_features:\n",
        "        unique = tft_output.vocabulary_size_by_name(feature)\n",
        "        if unique > 10:\n",
        "            EMBEDDING_FEATURES.append(feature)\n",
        "            print(\"Convert to embedding\", feature, unique)\n",
        "        else:\n",
        "            CATEGORICAL_FEATURES.append(feature)\n",
        "\n",
        "    metadata[\"categorical_features\"] = CATEGORICAL_FEATURES\n",
        "    metadata[\"embedding_features\"] = EMBEDDING_FEATURES\n",
        "\n",
        "    metadata[\"transformed_data_prefix\"] = transformed_data_prefix\n",
        "    metadata[\"transform_artifacts_dir\"] = transform_artifacts_dir\n",
        "    metadata[\"exported_jsonl_prefix\"] = exported_jsonl_prefix\n",
        "    metadata[\"exported_tfrec_prefix\"] = exported_tfrec_prefix\n",
        "\n",
        "    with tf.io.gfile.GFile(metadata_location, \"w\") as f:\n",
        "        json.dump(metadata, f)\n",
        "\n",
        "    return metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_data_pipeline:data_preprocess"
      },
      "source": [
        "### Construct pipeline for data analysis and preprocessing\n",
        "\n",
        "Next, construct the pipeline with the following tasks:\n",
        "\n",
        "- Create the local copy BigQuery dataset.\n",
        "- Perform data analysis on the dataset.\n",
        "- Prepare run arguments for Dataflow script.\n",
        "- Execute the Dataflow script.\n",
        "- Create a Vertex AI Dataset resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_data_pipeline:data_preprocess"
      },
      "outputs": [],
      "source": [
        "@dsl.pipeline(name=\"data-preprocessing\", description=\"Prepare the dataset\")\n",
        "def pipeline(\n",
        "    bq_table: str,\n",
        "    display_name: str,\n",
        "    transformed_data_prefix: str,\n",
        "    transform_artifacts_dir: str,\n",
        "    exported_tfrec_prefix: str,\n",
        "    exported_jsonl_prefix: str,\n",
        "    label_column: str,\n",
        "    python_file_path: str,\n",
        "    requirements_file_path: str,\n",
        "    setup_file: str,\n",
        "    staging_dir: str,\n",
        "    data_bucket: str,\n",
        "    metadata_location: str,\n",
        "    dataset_labels: dict,\n",
        "    year: int,\n",
        "    limit: int,\n",
        "    project: str = PROJECT_ID,\n",
        "    region: str = REGION,\n",
        "):\n",
        "    from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
        "\n",
        "    bq_op = make_chicago_bq_dataset(\n",
        "        bq_table=bq_table, year=year, limit=limit, project=project\n",
        "    )\n",
        "\n",
        "    analysis_op = data_analysis(\n",
        "        bq_table=bq_op.output,\n",
        "        label_column=label_column,\n",
        "        data_bucket=data_bucket,\n",
        "        project=project,\n",
        "    )\n",
        "\n",
        "    args_op = make_dataflow_args(\n",
        "        bucket=data_bucket,\n",
        "        setup_file=setup_file,\n",
        "        bq_table=bq_op.output,\n",
        "        metadata=analysis_op.output,\n",
        "        transformed_data_prefix=transformed_data_prefix,\n",
        "        transform_artifacts_dir=transform_artifacts_dir,\n",
        "        exported_tfrec_prefix=exported_tfrec_prefix,\n",
        "        exported_jsonl_prefix=exported_jsonl_prefix,\n",
        "        label=label_column,\n",
        "    )\n",
        "\n",
        "    dataflow_python_op = DataflowPythonJobOp(\n",
        "        project=project,\n",
        "        location=region,\n",
        "        python_module_path=python_file_path,\n",
        "        temp_location=staging_dir,\n",
        "        requirements_file_path=requirements_file_path,\n",
        "        args=args_op.output,\n",
        "    ).after(bq_op)\n",
        "\n",
        "    dataflow_wait_op = WaitGcpResourcesOp(\n",
        "        gcp_resources=dataflow_python_op.outputs[\"gcp_resources\"]\n",
        "    )\n",
        "\n",
        "    transformed_analysis_op = transformed_data_analysis(\n",
        "        metadata_location=metadata_location,\n",
        "        transformed_data_prefix=transformed_data_prefix,\n",
        "        transform_artifacts_dir=transform_artifacts_dir,\n",
        "        exported_jsonl_prefix=exported_jsonl_prefix,\n",
        "        exported_tfrec_prefix=exported_tfrec_prefix,\n",
        "    ).after(dataflow_wait_op)\n",
        "\n",
        "    dataset_op = gcc_aip.TabularDatasetCreateOp(\n",
        "        project=project,\n",
        "        display_name=display_name,\n",
        "        bq_source=bq_table,\n",
        "        labels=dataset_labels,\n",
        "    ).after(transformed_analysis_op)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_pipeline:data_preprocess"
      },
      "source": [
        "### Compile and execute the data analysis and preprocessing pipeline\n",
        "\n",
        "Next, you compile the pipeline and then exeute it. The pipeline takes the following parameters, which are passed as the dictionary `parameter_values`:\n",
        "\n",
        "- `bq_table`: The BigQuery table used for training the model.\n",
        "- `display_name`: The display name for the generated Vertex AI resources.\n",
        "- `transformed_data_prefix`: The Cloud Storage location of the preprocessed training, test and validation data.\n",
        "- `transform_artifacts_dir`: The Cloud Storage location of the transform function artifacts.\n",
        "- `exported_tfrec_prefix`: The Cloud Storage location of the debug/test data for the serving model as TFRecords.\n",
        "- `exported_jsonl_prefix`: The Cloud Storage location of the debug/test data for the serving model in JSONL format.\n",
        "- `label_column`: The name of the label column.\n",
        "- `python_file_path`: The Cloud Storage location of the Dataflow Python script for preprocessing the data.\n",
        "- `requirements_file_path`: The Cloud Storage location of the requirements.txt file for the Dataflow component.\n",
        "- `setup_file`: The Cloud Storage location of the setup.py script for the Dataflow component.\n",
        "- `staging_dir`: The Cloud Storage location for the temporary location for the Apache Beam pipeline.\n",
        "- `data_bucket`: The Cloud Storage location for data analysis artifacts.\n",
        "- `metadata_location`: The Cloud Storage location for the Vertex AI Dataset user metadata.\n",
        "- `dataset_labels`: User defined labels to add to the Vertex AI Dataset -- i.e., metadata location\n",
        "- `project`: The project ID.\n",
        "- `region`: The region."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_pipeline:data_preprocess"
      },
      "outputs": [],
      "source": [
        "PIPELINE_ROOT = \"{}/pipeline_root/data_preprocess\".format(BUCKET_NAME)\n",
        "\n",
        "EXPORTED_JSONL_PREFIX = os.path.join(BUCKET_NAME, \"exported_data/jsonl\")\n",
        "EXPORTED_TFREC_PREFIX = os.path.join(BUCKET_NAME, \"exported_data/tfrec\")\n",
        "TRANSFORMED_DATA_PREFIX = os.path.join(BUCKET_NAME, \"transformed_data\")\n",
        "TRANSFORM_ARTIFACTS_DIR = os.path.join(BUCKET_NAME, \"transformed_artifacts\")\n",
        "\n",
        "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"data_preprocess.json\")\n",
        "\n",
        "pipeline = aip.PipelineJob(\n",
        "    display_name=\"data_preprocess\",\n",
        "    template_path=\"data_preprocess.json\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        "    parameter_values={\n",
        "        \"bq_table\": IMPORT_FILE,\n",
        "        \"display_name\": \"Chicago Taxi\" + TIMESTAMP,\n",
        "        \"transformed_data_prefix\": TRANSFORMED_DATA_PREFIX,\n",
        "        \"transform_artifacts_dir\": TRANSFORM_ARTIFACTS_DIR,\n",
        "        \"exported_tfrec_prefix\": EXPORTED_TFREC_PREFIX,\n",
        "        \"exported_jsonl_prefix\": EXPORTED_JSONL_PREFIX,\n",
        "        \"label_column\": \"tip_bin\",\n",
        "        \"python_file_path\": GCS_PREPROCESS_PY,\n",
        "        \"requirements_file_path\": GCS_REQUIREMENTS_TXT,\n",
        "        \"setup_file\": GCS_SETUP_PY,\n",
        "        \"staging_dir\": PIPELINE_ROOT,\n",
        "        \"data_bucket\": BUCKET_NAME,\n",
        "        \"metadata_location\": BUCKET_NAME + \"/metadata.jsonl\",\n",
        "        \"dataset_labels\": {\"user_metadata\": BUCKET_NAME[5:]},\n",
        "        \"year\": 2020,\n",
        "        \"limit\": 300000,\n",
        "        \"project\": PROJECT_ID,\n",
        "        \"region\": REGION,\n",
        "    },\n",
        ")\n",
        "\n",
        "pipeline.run()\n",
        "\n",
        "! rm -f data_preprocess.json requirements.txt setup.py preprocess.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view_pipleline_results:data_preprocess,chicago"
      },
      "source": [
        "### View the data pipeline execution results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "view_pipleline_results:data_preprocess,chicago"
      },
      "outputs": [],
      "source": [
        "PROJECT_NUMBER = pipeline.gca_resource.name.split(\"/\")[1]\n",
        "print(PROJECT_NUMBER)\n",
        "\n",
        "\n",
        "def print_pipeline_output(job, output_task_name):\n",
        "    JOB_ID = job.name\n",
        "    print(JOB_ID)\n",
        "    for _ in range(len(job.gca_resource.job_detail.task_details)):\n",
        "        TASK_ID = job.gca_resource.job_detail.task_details[_].task_id\n",
        "        EXECUTE_OUTPUT = (\n",
        "            PIPELINE_ROOT\n",
        "            + \"/\"\n",
        "            + PROJECT_NUMBER\n",
        "            + \"/\"\n",
        "            + JOB_ID\n",
        "            + \"/\"\n",
        "            + output_task_name\n",
        "            + \"_\"\n",
        "            + str(TASK_ID)\n",
        "            + \"/executor_output.json\"\n",
        "        )\n",
        "        GCP_RESOURCES = (\n",
        "            PIPELINE_ROOT\n",
        "            + \"/\"\n",
        "            + PROJECT_NUMBER\n",
        "            + \"/\"\n",
        "            + JOB_ID\n",
        "            + \"/\"\n",
        "            + output_task_name\n",
        "            + \"_\"\n",
        "            + str(TASK_ID)\n",
        "            + \"/gcp_resources\"\n",
        "        )\n",
        "        EVAL_METRICS = (\n",
        "            PIPELINE_ROOT\n",
        "            + \"/\"\n",
        "            + PROJECT_NUMBER\n",
        "            + \"/\"\n",
        "            + JOB_ID\n",
        "            + \"/\"\n",
        "            + output_task_name\n",
        "            + \"_\"\n",
        "            + str(TASK_ID)\n",
        "            + \"/evaluation_metrics\"\n",
        "        )\n",
        "        if tf.io.gfile.exists(EXECUTE_OUTPUT):\n",
        "            ! gsutil cat $EXECUTE_OUTPUT\n",
        "            return EXECUTE_OUTPUT\n",
        "        elif tf.io.gfile.exists(GCP_RESOURCES):\n",
        "            ! gsutil cat $GCP_RESOURCES\n",
        "            return GCP_RESOURCES\n",
        "        elif tf.io.gfile.exists(EVAL_METRICS):\n",
        "            ! gsutil cat $EVAL_METRICS\n",
        "            return EVAL_METRICS\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "print(\"make-chicago-bq-dataset\")\n",
        "artifacts = print_pipeline_output(pipeline, \"make-chicago-bq-dataset\")\n",
        "print(\"\\n\")\n",
        "print(\"data-analysis\")\n",
        "artifacts = print_pipeline_output(pipeline, \"data-analysis\")\n",
        "print(\"\\n\")\n",
        "print(\"make-dataflow-args\")\n",
        "artifacts = print_pipeline_output(pipeline, \"make-dataflow-args\")\n",
        "print(\"\\n\")\n",
        "print(\"dataflow-python\")\n",
        "artifacts = print_pipeline_output(pipeline, \"dataflow-python\")\n",
        "print(\"\\n\")\n",
        "print(\"wait-gcp-resources\")\n",
        "artifacts = print_pipeline_output(pipeline, \"wait-gcp-resources\")\n",
        "print(\"\\n\")\n",
        "print(\"transformed-data-analysis\")\n",
        "artifacts = print_pipeline_output(pipeline, \"transformed-data-analysis\")\n",
        "print(\"\\n\")\n",
        "print(\"tabular-dataset-create\")\n",
        "artifacts = print_pipeline_output(pipeline, \"tabular-dataset-create\")\n",
        "print(\"\\n\")\n",
        "\n",
        "output = !gsutil cat $artifacts\n",
        "output = json.loads(output[0])\n",
        "dataset_id = output[\"artifacts\"][\"dataset\"][\"artifacts\"][0][\"metadata\"][\"resourceName\"]\n",
        "print(dataset_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "formalize_model_pipeline_intro"
      },
      "source": [
        "## Formalizing model pipeline introduction\n",
        "\n",
        "The model pipeline consists of building the model architecture task.\n",
        "\n",
        "### Build model architecture task\n",
        "\n",
        "- Construct the model architecture for the base model and save as a Vertex AI Model resource.\n",
        "\n",
        "### Triggers\n",
        "\n",
        "Within the CI/CD process, the model pipeline is triggered for one or more of the following example reasons, while not exhaustive:\n",
        "\n",
        "- If the data pipeline is re-executed.\n",
        "- Code changes to the build model architecture task.\n",
        "- The model architecture is being replaced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_model_build_component"
      },
      "source": [
        "### Create build model architecture component\n",
        "\n",
        "Next, you create a component which creates the base model architecture. Note, the base model is untrained. In this example, the model architecture is for a tabular model, where:\n",
        "\n",
        "- Load the corresonding Vertex AI Dataset,\n",
        "- Load the dataset metadata.\n",
        "- Use the metadata information on the feature types to build the input layer.\n",
        "- Build the DNN body of the model.\n",
        "- Save the base model artifacts to the Cloud Storage location.\n",
        "\n",
        "The component returns the full resource name of the generated Vertex AI Model resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_model_build_component"
      },
      "outputs": [],
      "source": [
        "@component(packages_to_install=[\"tensorflow==2.5\", \"tensorflow-transform\", \"future\"])\n",
        "def build_model(\n",
        "    dataset_id: str, display_name: str, deploy_image: str, bucket: str, project: str\n",
        ") -> str:\n",
        "\n",
        "    import subprocess\n",
        "\n",
        "    subprocess.call([\"pip3\", \"install\", \"google-cloud-aiplatform\"])\n",
        "\n",
        "    import json\n",
        "    import logging\n",
        "    from math import sqrt\n",
        "\n",
        "    import google.cloud.aiplatform as aip\n",
        "    import tensorflow as tf\n",
        "    import tensorflow_transform as tft\n",
        "    from tensorflow.keras import Model, Sequential\n",
        "    from tensorflow.keras.layers import (Activation, Concatenate, Dense,\n",
        "                                         Embedding, Input, experimental)\n",
        "\n",
        "    logging.info(\"Tensorflow version: \" + tf.__version__)\n",
        "\n",
        "    aip.init(project=project, staging_bucket=bucket, experiment=display_name)\n",
        "    aip.start_run(run=\"retrain\")\n",
        "\n",
        "    # Load the dataset resource from the dataset resource ID.\n",
        "    dataset = aip.TabularDataset(dataset_id)\n",
        "\n",
        "    # Load the metadata for this dataset\n",
        "    with tf.io.gfile.GFile(\n",
        "        \"gs://\" + dataset.labels[\"user_metadata\"] + \"/metadata.jsonl\", \"r\"\n",
        "    ) as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    def create_model_inputs(\n",
        "        numeric_features=None, categorical_features=None, embedding_features=None\n",
        "    ):\n",
        "        inputs = {}\n",
        "        for feature_name in numeric_features:\n",
        "            inputs[feature_name] = Input(name=feature_name, shape=[], dtype=tf.float32)\n",
        "        for feature_name in categorical_features:\n",
        "            inputs[feature_name] = Input(name=feature_name, shape=[], dtype=tf.int64)\n",
        "        for feature_name in embedding_features:\n",
        "            inputs[feature_name] = Input(name=feature_name, shape=[], dtype=tf.int64)\n",
        "\n",
        "        return inputs\n",
        "\n",
        "    input_layers = create_model_inputs(\n",
        "        numeric_features=metadata[\"numeric_features\"],\n",
        "        categorical_features=metadata[\"categorical_features\"],\n",
        "        embedding_features=metadata[\"embedding_features\"],\n",
        "    )\n",
        "\n",
        "    logging.info(\"Created input layers for model\")\n",
        "    logging.info(input_layers)\n",
        "\n",
        "    def create_binary_classifier(\n",
        "        input_layers,\n",
        "        tft_output,\n",
        "        metaparams,\n",
        "        numeric_features,\n",
        "        categorical_features,\n",
        "        embedding_features,\n",
        "    ):\n",
        "        layers = []\n",
        "        for feature_name in input_layers:\n",
        "            if feature_name in embedding_features:\n",
        "                vocab_size = tft_output.vocabulary_size_by_name(feature_name)\n",
        "                embedding_size = int(sqrt(vocab_size))\n",
        "                embedding_output = Embedding(\n",
        "                    input_dim=vocab_size + 1,\n",
        "                    output_dim=embedding_size,\n",
        "                    name=f\"{feature_name}_embedding\",\n",
        "                )(input_layers[feature_name])\n",
        "                layers.append(embedding_output)\n",
        "            elif feature_name in categorical_features:\n",
        "                vocab_size = tft_output.vocabulary_size_by_name(feature_name)\n",
        "                onehot_layer = experimental.preprocessing.CategoryEncoding(\n",
        "                    num_tokens=vocab_size,\n",
        "                    output_mode=\"binary\",\n",
        "                    name=f\"{feature_name}_onehot\",\n",
        "                )(input_layers[feature_name])\n",
        "                layers.append(onehot_layer)\n",
        "            elif feature_name in numeric_features:\n",
        "                numeric_layer = tf.expand_dims(input_layers[feature_name], -1)\n",
        "                layers.append(numeric_layer)\n",
        "            else:\n",
        "                pass\n",
        "\n",
        "        logging.info(\"Created layers for model\")\n",
        "        logging.info(layers)\n",
        "\n",
        "        joined = Concatenate(name=\"combines_inputs\")(layers)\n",
        "        feedforward_output = Sequential(\n",
        "            [Dense(units, activation=\"relu\") for units in metaparams[\"hidden_units\"]],\n",
        "            name=\"feedforward_network\",\n",
        "        )(joined)\n",
        "        logits = Dense(units=1, name=\"logits\")(feedforward_output)\n",
        "        pred = Activation(\"sigmoid\")(logits)\n",
        "\n",
        "        model = Model(inputs=input_layers, outputs=[pred])\n",
        "        return model\n",
        "\n",
        "    TRANSFORM_ARTIFACTS_DIR = metadata[\"transform_artifacts_dir\"]\n",
        "    tft_output = tft.TFTransformOutput(TRANSFORM_ARTIFACTS_DIR)\n",
        "\n",
        "    metaparams = {\"hidden_units\": [128, 64]}\n",
        "    aip.log_params(metaparams)\n",
        "\n",
        "    model = create_binary_classifier(\n",
        "        input_layers,\n",
        "        tft_output,\n",
        "        metaparams,\n",
        "        numeric_features=metadata[\"numeric_features\"],\n",
        "        categorical_features=metadata[\"categorical_features\"],\n",
        "        embedding_features=metadata[\"embedding_features\"],\n",
        "    )\n",
        "\n",
        "    logging.info(\"Created binary classifier model\")\n",
        "    logging.info(model.summary)\n",
        "\n",
        "    logging.info(\"Save base model architecture\")\n",
        "    MODEL_DIR = f\"{bucket}/base_model\"\n",
        "    model.save(MODEL_DIR)\n",
        "\n",
        "    return MODEL_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_model_pipeline:tabular"
      },
      "source": [
        "### Construct pipeline for building the model architecture\n",
        "\n",
        "Next, construct the pipeline with the following tasks:\n",
        "\n",
        "- Build the base model architecture.\n",
        "- Create a Vertex AI Model resource for the base model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_model_pipeline:tabular"
      },
      "outputs": [],
      "source": [
        "@dsl.pipeline(name=\"build-model\", description=\"Build the base model architecture\")\n",
        "def pipeline(\n",
        "    dataset_id: str,\n",
        "    display_name: str,\n",
        "    deploy_image: str,\n",
        "    bucket: str,\n",
        "    project: str = PROJECT_ID,\n",
        "    region: str = REGION,\n",
        "    labels: dict = {\"base_model\": \"1\"},\n",
        "):\n",
        "    from google_cloud_pipeline_components.types import artifact_types\n",
        "    from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
        "    from kfp.v2.components import importer_node\n",
        "\n",
        "    model_build_op = build_model(\n",
        "        dataset_id=dataset_id,\n",
        "        display_name=display_name,\n",
        "        deploy_image=deploy_image,\n",
        "        bucket=bucket,\n",
        "        project=project,\n",
        "    )\n",
        "\n",
        "    import_unmanaged_model_task = importer_node.importer(\n",
        "        artifact_uri=model_build_op.output,\n",
        "        artifact_class=artifact_types.UnmanagedContainerModel,\n",
        "        metadata={\n",
        "            \"containerSpec\": {\n",
        "                \"imageUri\": DEPLOY_IMAGE,\n",
        "            },\n",
        "        },\n",
        "    ).after(model_build_op)\n",
        "\n",
        "    model_upload = ModelUploadOp(\n",
        "        project=project,\n",
        "        display_name=display_name,\n",
        "        unmanaged_container_model=import_unmanaged_model_task.outputs[\"artifact\"],\n",
        "    ).after(import_unmanaged_model_task)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_pipeline:model_build"
      },
      "source": [
        "### Compile and execute the build model architecture pipeline\n",
        "\n",
        "Next, you compile the pipeline and then execute it. The pipeline takes the following parameters, which are passed as the dictionary `parameter_values`:\n",
        "\n",
        "- `dataset_id`: The full resource name of the corresponding Vertex AI Dataset.\n",
        "- `display_name`: The display name for the generated Vertex AI Model resource.\n",
        "- `deploy_image`: The associated deployment container image.\n",
        "- `bucket`: The Cloud Storage location to store the model artifacts.\n",
        "- `project`: The project ID.\n",
        "- `region`: The region."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_pipeline:model_build"
      },
      "outputs": [],
      "source": [
        "PIPELINE_ROOT = \"{}/pipeline_root/model_build\".format(BUCKET_NAME)\n",
        "\n",
        "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"model_build.json\")\n",
        "\n",
        "pipeline = aip.PipelineJob(\n",
        "    display_name=\"model-build\",\n",
        "    template_path=\"model_build.json\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        "    parameter_values={\n",
        "        \"dataset_id\": dataset_id,\n",
        "        \"display_name\": \"chicago\" + TIMESTAMP,\n",
        "        \"deploy_image\": DEPLOY_IMAGE,\n",
        "        \"bucket\": BUCKET_NAME,\n",
        "        \"project\": PROJECT_ID,\n",
        "        \"region\": REGION,\n",
        "    },\n",
        ")\n",
        "\n",
        "pipeline.run()\n",
        "\n",
        "! rm -f model_build.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view_pipleline_results:model_build"
      },
      "source": [
        "### View the model pipeline execution results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "view_pipleline_results:model_build"
      },
      "outputs": [],
      "source": [
        "print(\"model-build\")\n",
        "artifacts = print_pipeline_output(pipeline, \"model-build\")\n",
        "print(\"\\n\")\n",
        "print(\"model-upload\")\n",
        "artifacts = print_pipeline_output(pipeline, \"model-upload\")\n",
        "print(\"\\n\")\n",
        "\n",
        "output = !gsutil cat $artifacts\n",
        "output = json.loads(output[0])\n",
        "model_id = output[\"artifacts\"][\"model\"][\"artifacts\"][0][\"metadata\"][\"resourceName\"]\n",
        "print(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "formalize_training_pipeline_intro"
      },
      "source": [
        "## Formalizing training pipeline introduction\n",
        "\n",
        "The training pipeline consists of training the model.\n",
        "\n",
        "### Train the model task\n",
        "\n",
        "- Retrieve the model architecture.\n",
        "- If warmup training:\n",
        "    - Warmup the model.\n",
        "    - Save model weights back to the base model architecture.\n",
        "- If full training:\n",
        "    - Retrieve the hyperparameters from the baseline model\n",
        "    - Train the model\n",
        "    - Evaluate the model\n",
        "    - Save the model artifacts to the specified Cloud Storage location.\n",
        "\n",
        "### Triggers\n",
        "\n",
        "Within the CI/CD process, the training pipeline is triggered for one or more of the following example reasons, while not exhaustive:\n",
        "\n",
        "- If the data pipeline is re-executed.\n",
        "- If the model pipeline is re-executed.\n",
        "- Code changes to the model training task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "construct_training_package"
      },
      "source": [
        "### Construct the training package\n",
        "\n",
        "#### Package layout\n",
        "\n",
        "Before you start training, you will look at how a Python package is assembled for a custom training job. When unarchived, the package contains the following directory/file layout.\n",
        "\n",
        "- PKG-INFO\n",
        "- README.md\n",
        "- setup.cfg\n",
        "- setup.py\n",
        "- trainer\n",
        "  - \\_\\_init\\_\\_.py\n",
        "  - task.py\n",
        "  - other Python scripts\n",
        "\n",
        "The files `setup.cfg` and `setup.py` are the instructions for installing the package into the operating environment of the Docker image.\n",
        "\n",
        "The file `trainer/task.py` is the Python script for executing the custom training job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "construct_training_package"
      },
      "outputs": [],
      "source": [
        "# Make folder for Python training script\n",
        "! rm -rf custom\n",
        "! mkdir custom\n",
        "\n",
        "# Add package information\n",
        "! touch custom/README.md\n",
        "\n",
        "setup_cfg = \"[egg_info]\\n\\ntag_build =\\n\\ntag_date = 0\"\n",
        "! echo \"$setup_cfg\" > custom/setup.cfg\n",
        "\n",
        "setup_py = \"import setuptools\\n\\nsetuptools.setup(\\n\\n    install_requires=[\\n\\n        'google-cloud-aiplatform',\\n\\n        'cloudml-hypertune',\\n\\n        'tensorflow_datasets==1.3.0',\\n\\n        'tensorflow_data_validation==1.2',\\n\\n    ],\\n\\n    packages=setuptools.find_packages())\"\n",
        "! echo \"$setup_py\" > custom/setup.py\n",
        "\n",
        "pkg_info = \"Metadata-Version: 1.0\\n\\nName: Chicago Taxi tabular binary classification\\n\\nVersion: 0.0.0\\n\\nSummary: Demostration training script\\n\\nHome-page: www.google.com\\n\\nAuthor: Google\\n\\nAuthor-email: cdpe@google.com\\n\\nLicense: Public\\n\\nDescription: Demo\\n\\nPlatform: Vertex AI\"\n",
        "! echo \"$pkg_info\" > custom/PKG-INFO\n",
        "\n",
        "# Make the training subfolder\n",
        "! mkdir custom/trainer\n",
        "! touch custom/trainer/__init__.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "read_tfrecords_func"
      },
      "source": [
        "### Load the transformed data into a tf.data.Dataset\n",
        "\n",
        "Next, you load the gzip TFRecords on Cloud Storage storage into a `tf.data.Dataset` generator. These functions are re-used when training the custom model using `Vertex Training`, so you save them to the python training package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "read_tfrecords_func"
      },
      "outputs": [],
      "source": [
        "%%writefile custom/trainer/data.py\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def _gzip_reader_fn(filenames):\n",
        "    \"\"\"Small utility returning a record reader that can read gzip'ed files.\"\"\"\n",
        "    return tf.data.TFRecordDataset(filenames, compression_type=\"GZIP\")\n",
        "\n",
        "\n",
        "def get_dataset(file_pattern, feature_spec, label_column, batch_size=200):\n",
        "    \"\"\"Generates features and label for tuning/training.\n",
        "    Args:\n",
        "      file_pattern: input tfrecord file pattern.\n",
        "      feature_spec: a dictionary of feature specifications.\n",
        "      batch_size: representing the number of consecutive elements of returned\n",
        "        dataset to combine in a single batch\n",
        "    Returns:\n",
        "      A dataset that contains (features, indices) tuple where features is a\n",
        "        dictionary of Tensors, and indices is a single Tensor of label indices.\n",
        "    \"\"\"\n",
        "\n",
        "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
        "        file_pattern=file_pattern,\n",
        "        batch_size=batch_size,\n",
        "        features=feature_spec,\n",
        "        label_key=label_column,\n",
        "        reader=_gzip_reader_fn,\n",
        "        num_epochs=1,\n",
        "        drop_final_batch=True,\n",
        "    )\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train_model_func"
      },
      "source": [
        "## Develop and test the training scripts\n",
        "\n",
        "When experimenting, one typically develops and tests the training package locally, before moving to training in the cloud.\n",
        "\n",
        "### Create training script\n",
        "\n",
        "Next, you write the Python script for compiling and training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_model_func"
      },
      "outputs": [],
      "source": [
        "%%writefile custom/trainer/train.py\n",
        "\n",
        "from trainer import data\n",
        "import tensorflow as tf\n",
        "import logging\n",
        "from hypertune import HyperTune\n",
        "\n",
        "def compile(model, hyperparams):\n",
        "    ''' Compile the model '''\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=hyperparams[\"learning_rate\"])\n",
        "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "    metrics = [tf.keras.metrics.BinaryAccuracy(name=\"accuracy\")]\n",
        "\n",
        "    model.compile(optimizer=optimizer,loss=loss, metrics=metrics)\n",
        "    return model\n",
        "\n",
        "def warmup(\n",
        "    model,\n",
        "    hyperparams,\n",
        "    train_data_dir,\n",
        "    label_column,\n",
        "    transformed_feature_spec\n",
        "):\n",
        "    ''' Warmup the initialized model weights '''\n",
        "\n",
        "    train_dataset = data.get_dataset(\n",
        "        train_data_dir,\n",
        "        transformed_feature_spec,\n",
        "        label_column,\n",
        "        batch_size=hyperparams[\"batch_size\"],\n",
        "    )\n",
        "\n",
        "    lr_inc = (hyperparams['end_learning_rate'] - hyperparams['start_learning_rate']) / hyperparams['num_epochs']\n",
        "\n",
        "    def scheduler(epoch, lr):\n",
        "        if epoch == 0:\n",
        "            return hyperparams['start_learning_rate']\n",
        "        return lr + lr_inc\n",
        "\n",
        "\n",
        "    callbacks = [tf.keras.callbacks.LearningRateScheduler(scheduler)]\n",
        "\n",
        "    logging.info(\"Model warmup started...\")\n",
        "    history = model.fit(\n",
        "            train_dataset,\n",
        "            epochs=hyperparams[\"num_epochs\"],\n",
        "            steps_per_epoch=hyperparams[\"steps\"],\n",
        "            callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    logging.info(\"Model warmup completed.\")\n",
        "    return history\n",
        "\n",
        "\n",
        "def train(\n",
        "    model,\n",
        "    hyperparams,\n",
        "    train_data_dir,\n",
        "    val_data_dir,\n",
        "    label_column,\n",
        "    transformed_feature_spec,\n",
        "    log_dir,\n",
        "    tuning=False\n",
        "):\n",
        "    ''' Train the model '''\n",
        "\n",
        "    train_dataset = data.get_dataset(\n",
        "        train_data_dir,\n",
        "        transformed_feature_spec,\n",
        "        label_column,\n",
        "        batch_size=hyperparams[\"batch_size\"],\n",
        "    )\n",
        "\n",
        "    val_dataset = data.get_dataset(\n",
        "        val_data_dir,\n",
        "        transformed_feature_spec,\n",
        "        label_column,\n",
        "        batch_size=hyperparams[\"batch_size\"],\n",
        "    )\n",
        "\n",
        "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=hyperparams[\"early_stop\"][\"monitor\"], patience=hyperparams[\"early_stop\"][\"patience\"], restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    callbacks = [early_stop]\n",
        "\n",
        "    if log_dir:\n",
        "        tensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
        "\n",
        "        callbacks = callbacks.append(tensorboard)\n",
        "\n",
        "    if tuning:\n",
        "        # Instantiate the HyperTune reporting object\n",
        "        hpt = HyperTune()\n",
        "\n",
        "        # Reporting callback\n",
        "        class HPTCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "            def on_epoch_end(self, epoch, logs=None):\n",
        "                hpt.report_hyperparameter_tuning_metric(\n",
        "                    hyperparameter_metric_tag='val_loss',\n",
        "                    metric_value=logs['val_loss'],\n",
        "                    global_step=epoch\n",
        "                )\n",
        "\n",
        "        if not callbacks:\n",
        "            callbacks = []\n",
        "        callbacks.append(HPTCallback())\n",
        "\n",
        "    logging.info(\"Model training started...\")\n",
        "    history = model.fit(\n",
        "            train_dataset,\n",
        "            epochs=hyperparams[\"num_epochs\"],\n",
        "            validation_data=val_dataset,\n",
        "            callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    logging.info(\"Model training completed.\")\n",
        "    return history\n",
        "\n",
        "def evaluate(\n",
        "    model,\n",
        "    hyperparams,\n",
        "    test_data_dir,\n",
        "    label_column,\n",
        "    transformed_feature_spec\n",
        "):\n",
        "    logging.info(\"Model evaluation started...\")\n",
        "    test_dataset = data.get_dataset(\n",
        "        test_data_dir,\n",
        "        transformed_feature_spec,\n",
        "        label_column,\n",
        "        hyperparams[\"batch_size\"],\n",
        "    )\n",
        "\n",
        "    evaluation_metrics = model.evaluate(test_dataset)\n",
        "    logging.info(\"Model evaluation completed.\")\n",
        "\n",
        "    return evaluation_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "serving_function:chicago"
      },
      "source": [
        "## Add a serving function\n",
        "\n",
        "Next, you add a serving function to your model for online and batch prediction. This allows prediction requests to be sent in raw format (unpreprocessed), either as a serialized TF.Example or JSONL object. The serving function will then preprocess the prediction request into the transformed format expected by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "serving_function:chicago"
      },
      "outputs": [],
      "source": [
        "%%writefile custom/trainer/serving.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_data_validation as tfdv\n",
        "import tensorflow_transform as tft\n",
        "import logging\n",
        "\n",
        "def _get_serve_features_fn(model, tft_output):\n",
        "    \"\"\"Returns a function that accept a dictionary of features and applies TFT.\"\"\"\n",
        "\n",
        "    model.tft_layer = tft_output.transform_features_layer()\n",
        "\n",
        "    @tf.function\n",
        "    def serve_features_fn(raw_features):\n",
        "        \"\"\"Returns the output to be used in the serving signature.\"\"\"\n",
        "\n",
        "        transformed_features = model.tft_layer(raw_features)\n",
        "        probabilities = model(transformed_features)\n",
        "        return {\"scores\": probabilities}\n",
        "\n",
        "\n",
        "    return serve_features_fn\n",
        "\n",
        "def _get_serve_tf_examples_fn(model, tft_output, feature_spec):\n",
        "    \"\"\"Returns a function that parses a serialized tf.Example and applies TFT.\"\"\"\n",
        "\n",
        "    model.tft_layer = tft_output.transform_features_layer()\n",
        "\n",
        "    @tf.function\n",
        "    def serve_tf_examples_fn(serialized_tf_examples):\n",
        "        \"\"\"Returns the output to be used in the serving signature.\"\"\"\n",
        "        for key in list(feature_spec.keys()):\n",
        "            if key not in features:\n",
        "                feature_spec.pop(key)\n",
        "\n",
        "        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
        "\n",
        "        transformed_features = model.tft_layer(parsed_features)\n",
        "        probabilities = model(transformed_features)\n",
        "        return {\"scores\": probabilities}\n",
        "\n",
        "    return serve_tf_examples_fn\n",
        "\n",
        "def construct_serving_model(\n",
        "    model, serving_model_dir, metadata\n",
        "):\n",
        "    global features\n",
        "\n",
        "    schema_location = metadata['schema']\n",
        "    features = metadata['numeric_features'] + metadata['categorical_features'] + metadata['embedding_features']\n",
        "    print(\"FEATURES\", features)\n",
        "    tft_output_dir = metadata[\"transform_artifacts_dir\"]\n",
        "\n",
        "    schema = tfdv.load_schema_text(schema_location)\n",
        "    feature_spec = tft.tf_metadata.schema_utils.schema_as_feature_spec(schema).feature_spec\n",
        "\n",
        "    tft_output = tft.TFTransformOutput(tft_output_dir)\n",
        "\n",
        "    # Drop features that were not used in training\n",
        "    features_input_signature = {\n",
        "        feature_name: tf.TensorSpec(\n",
        "            shape=(None, 1), dtype=spec.dtype, name=feature_name\n",
        "        )\n",
        "        for feature_name, spec in feature_spec.items()\n",
        "        if feature_name in features\n",
        "    }\n",
        "\n",
        "    signatures = {\n",
        "        \"serving_default\": _get_serve_features_fn(\n",
        "            model, tft_output\n",
        "        ).get_concrete_function(features_input_signature),\n",
        "        \"serving_tf_example\": _get_serve_tf_examples_fn(\n",
        "            model, tft_output, feature_spec\n",
        "        ).get_concrete_function(\n",
        "            tf.TensorSpec(shape=[None], dtype=tf.string, name=\"examples\")\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    logging.info(\"Model saving started...\")\n",
        "    model.save(serving_model_dir, signatures=signatures)\n",
        "    logging.info(\"Model saving completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_model_get"
      },
      "source": [
        "### Retrieve model from Vertex AI\n",
        "\n",
        "Next, create the Python script to retrieve your experimental model from Vertex AI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_model_get"
      },
      "outputs": [],
      "source": [
        "%%writefile custom/trainer/model.py\n",
        "\n",
        "import google.cloud.aiplatform as aip\n",
        "\n",
        "def get(model_id):\n",
        "    model = aip.Model(model_id)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_task_py"
      },
      "source": [
        "### Create the task script for the Python training package\n",
        "\n",
        "Next, you create the `task.py` script for driving the training package. Some noteable steps include:\n",
        "\n",
        "- Command-line arguments:\n",
        "    - `model-id`: The resource ID of the `Model` resource you built during experimenting. This is the untrained model architecture.\n",
        "    - `dataset-id`: The resource ID of the `Dataset` resource to use for training.\n",
        "    - `experiment`: The name of the experiment.\n",
        "    - `run`: The name of the run within this experiment.\n",
        "    - `tensorboard-logdir`: The logging directory for Vertex AI Tensorboard.\n",
        "\n",
        "\n",
        "- `get_data()`:\n",
        "    - Loads the Dataset resource into memory.\n",
        "    - Obtains the user metadata from the Dataset resource.\n",
        "    - From the metadata, obtain location of transformed data, transformation function and name of label column\n",
        "\n",
        "\n",
        "- `get_model()`:\n",
        "    - Loads the Model resource into memory.\n",
        "    - Obtains location of model artifacts of the model architecture.\n",
        "    - Loads the model architecture.\n",
        "    - Compiles the model.\n",
        "\n",
        "\n",
        "- `warmup_model()`:\n",
        "   - Warms up the initialized model weights\n",
        "\n",
        "\n",
        "- `train_model()`:\n",
        "    - Train the model.\n",
        "\n",
        "\n",
        "- `evaluate_model()`:\n",
        "    - Evaluates the model.\n",
        "    - Saves evaluation metrics to Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_task_py"
      },
      "outputs": [],
      "source": [
        "%%writefile custom/trainer/task.py\n",
        "import os\n",
        "import argparse\n",
        "import logging\n",
        "import json\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "import google.cloud.aiplatform as aip\n",
        "\n",
        "from trainer import data\n",
        "from trainer import model as model_\n",
        "from trainer import train\n",
        "try:\n",
        "    from trainer import serving\n",
        "except:\n",
        "    pass\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--model-dir', dest='model_dir',\n",
        "                    default=os.getenv('AIP_MODEL_DIR'), type=str, help='Model dir.')\n",
        "parser.add_argument('--model-id', dest='model_id',\n",
        "                    default=None, type=str, help='Vertex Model ID.')\n",
        "parser.add_argument('--dataset-id', dest='dataset_id',\n",
        "                    default=None, type=str, help='Vertex Dataset ID.')\n",
        "parser.add_argument('--lr', dest='lr',\n",
        "                    default=0.001, type=float,\n",
        "                    help='Learning rate.')\n",
        "parser.add_argument('--start_lr', dest='start_lr',\n",
        "                    default=0.0001, type=float,\n",
        "                    help='Starting learning rate.')\n",
        "parser.add_argument('--epochs', dest='epochs',\n",
        "                    default=20, type=int,\n",
        "                    help='Number of epochs.')\n",
        "parser.add_argument('--steps', dest='steps',\n",
        "                    default=200, type=int,\n",
        "                    help='Number of steps per epoch.')\n",
        "parser.add_argument('--batch_size', dest='batch_size',\n",
        "                    default=16, type=int,\n",
        "                    help='Batch size.')\n",
        "parser.add_argument('--distribute', dest='distribute', type=str, default='single',\n",
        "                    help='distributed training strategy')\n",
        "parser.add_argument('--tensorboard-log-dir', dest='tensorboard_log_dir',\n",
        "                    default=os.getenv('AIP_TENSORBOARD_LOG_DIR'), type=str,\n",
        "                    help='Output file for tensorboard logs')\n",
        "parser.add_argument('--experiment', dest='experiment',\n",
        "                    default=None, type=str,\n",
        "                    help='Name of experiment')\n",
        "parser.add_argument('--project', dest='project',\n",
        "                    default=None, type=str,\n",
        "                    help='Name of project')\n",
        "parser.add_argument('--run', dest='run',\n",
        "                    default=None, type=str,\n",
        "                    help='Name of run in experiment')\n",
        "parser.add_argument('--evaluate', dest='evaluate',\n",
        "                    default=False, type=bool,\n",
        "                    help='Whether to perform evaluation')\n",
        "parser.add_argument('--serving', dest='serving',\n",
        "                    default=False, type=bool,\n",
        "                    help='Whether to attach the serving function')\n",
        "parser.add_argument('--tuning', dest='tuning',\n",
        "                    default=False, type=bool,\n",
        "                    help='Whether to perform hyperparameter tuning')\n",
        "parser.add_argument('--warmup', dest='warmup',\n",
        "                    default=False, type=bool,\n",
        "                    help='Whether to perform warmup weight initialization')\n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "logging.getLogger().setLevel(logging.INFO)\n",
        "logging.info('DEVICES'  + str(device_lib.list_local_devices()))\n",
        "\n",
        "# Single Machine, single compute device\n",
        "if args.distribute == 'single':\n",
        "    if tf.test.is_gpu_available():\n",
        "        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
        "    else:\n",
        "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
        "    logging.info(\"Single device training\")\n",
        "# Single Machine, multiple compute device\n",
        "elif args.distribute == 'mirrored':\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "    logging.info(\"Mirrored Strategy distributed training\")\n",
        "# Multi Machine, multiple compute device\n",
        "elif args.distribute == 'multiworker':\n",
        "    strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
        "    logging.info(\"Multi-worker Strategy distributed training\")\n",
        "    logging.info('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', 'Not found')))\n",
        "logging.info('num_replicas_in_sync = {}'.format(strategy.num_replicas_in_sync))\n",
        "\n",
        "# Initialize the run for this experiment\n",
        "if args.experiment:\n",
        "    logging.info(\"Initialize experiment: {}\".format(args.experiment))\n",
        "    aip.init(experiment=args.experiment, project=args.project)\n",
        "    aip.start_run(args.run)\n",
        "\n",
        "metadata = {}\n",
        "\n",
        "def get_data():\n",
        "    ''' Get the preprocessed training data '''\n",
        "    global train_data_file_pattern, val_data_file_pattern, test_data_file_pattern\n",
        "    global label_column, transform_feature_spec, metadata\n",
        "\n",
        "    dataset = aip.TabularDataset(args.dataset_id)\n",
        "    METADATA = 'gs://' + dataset.labels['user_metadata'] + \"/metadata.jsonl\"\n",
        "\n",
        "    with tf.io.gfile.GFile(METADATA, \"r\") as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    TRANSFORMED_DATA_PREFIX = metadata['transformed_data_prefix']\n",
        "    label_column = metadata['label_column']\n",
        "\n",
        "    train_data_file_pattern = TRANSFORMED_DATA_PREFIX + '/train/data-*.gz'\n",
        "    val_data_file_pattern = TRANSFORMED_DATA_PREFIX + '/val/data-*.gz'\n",
        "    test_data_file_pattern = TRANSFORMED_DATA_PREFIX + '/test/data-*.gz'\n",
        "\n",
        "    TRANSFORM_ARTIFACTS_DIR = metadata['transform_artifacts_dir']\n",
        "    tft_output = tft.TFTransformOutput(TRANSFORM_ARTIFACTS_DIR)\n",
        "    transform_feature_spec = tft_output.transformed_feature_spec()\n",
        "\n",
        "def get_model():\n",
        "    ''' Get the untrained model architecture '''\n",
        "    global model_artifacts\n",
        "\n",
        "    vertex_model = model_.get(args.model_id)\n",
        "    model_artifacts = vertex_model.gca_resource.artifact_uri\n",
        "    model = tf.keras.models.load_model(model_artifacts)\n",
        "\n",
        "    # Compile the model\n",
        "    hyperparams = {}\n",
        "    hyperparams[\"learning_rate\"] = args.lr\n",
        "    if args.experiment:\n",
        "        aip.log_params(hyperparams)\n",
        "\n",
        "    metadata.update(hyperparams)\n",
        "    with tf.io.gfile.GFile(os.path.join(args.model_dir, \"metrics.txt\"), \"w\") as f:\n",
        "        f.write(json.dumps(metadata))\n",
        "\n",
        "    train.compile(model, hyperparams)\n",
        "    return model\n",
        "\n",
        "def warmup_model(model):\n",
        "    ''' Warmup the initialized model weights '''\n",
        "    warmupparams = {}\n",
        "    warmupparams[\"num_epochs\"] = args.epochs\n",
        "    warmupparams[\"batch_size\"] = args.batch_size\n",
        "    warmupparams[\"steps\"] = args.steps\n",
        "    warmupparams[\"start_learning_rate\"] = args.start_lr\n",
        "    warmupparams[\"end_learning_rate\"] = args.lr\n",
        "\n",
        "    train.warmup(model, warmupparams, train_data_file_pattern, label_column, transform_feature_spec)\n",
        "    return model\n",
        "\n",
        "def train_model(model):\n",
        "    ''' Train the model '''\n",
        "    trainparams = {}\n",
        "    trainparams[\"num_epochs\"] = args.epochs\n",
        "    trainparams[\"batch_size\"] = args.batch_size\n",
        "    trainparams[\"early_stop\"] = {\"monitor\": \"val_loss\", \"patience\": 5}\n",
        "    if args.experiment:\n",
        "        aip.log_params(trainparams)\n",
        "\n",
        "    metadata.update(trainparams)\n",
        "    with tf.io.gfile.GFile(os.path.join(args.model_dir, \"metrics.txt\"), \"w\") as f:\n",
        "        f.write(json.dumps(metadata))\n",
        "\n",
        "    train.train(model, trainparams, train_data_file_pattern, val_data_file_pattern, label_column, transform_feature_spec, args.tensorboard_log_dir, args.tuning)\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model):\n",
        "    ''' Evaluate the model '''\n",
        "    evalparams = {}\n",
        "    evalparams[\"batch_size\"] = args.batch_size\n",
        "    metrics = train.evaluate(model, evalparams, test_data_file_pattern, label_column, transform_feature_spec)\n",
        "\n",
        "    metadata.update({'metrics': metrics})\n",
        "    with tf.io.gfile.GFile(os.path.join(args.model_dir, \"metrics.txt\"), \"w\") as f:\n",
        "        f.write(json.dumps(metadata))\n",
        "\n",
        "get_data()\n",
        "with strategy.scope():\n",
        "    model = get_model()\n",
        "\n",
        "if args.warmup:\n",
        "    model = warmup_model(model)\n",
        "else:\n",
        "    model = train_model(model)\n",
        "\n",
        "if args.evaluate:\n",
        "    evaluate_model(model)\n",
        "\n",
        "if args.serving:\n",
        "    logging.info('Save serving model to: ' + args.model_dir)\n",
        "    serving.construct_serving_model(\n",
        "        model=model,\n",
        "        serving_model_dir=args.model_dir,\n",
        "        metadata=metadata\n",
        "    )\n",
        "elif args.warmup:\n",
        "    logging.info('Save warmed up model to: ' + model_artifacts)\n",
        "    model.save(model_artifacts)\n",
        "else:\n",
        "    logging.info('Save trained model to: ' + args.model_dir)\n",
        "    model.save(args.model_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_package_locally:id"
      },
      "source": [
        "### Test training package locally\n",
        "\n",
        "Next, test your completed training package locally with just a few epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_package_locally:id"
      },
      "outputs": [],
      "source": [
        "DATASET_ID = dataset_id\n",
        "MODEL_ID = model_id\n",
        "!cd custom; python3 -m trainer.task --model-id={MODEL_ID} --dataset-id={DATASET_ID} --experiment='chicago' --run='test' --project={PROJECT_ID} --epochs=5 --model-dir=/tmp --evaluate=True --serving=True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tarball_training_script"
      },
      "source": [
        "#### Store training script on your Cloud Storage bucket\n",
        "\n",
        "Next, you package the training folder into a compressed tar ball, and then store it in your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tarball_training_script"
      },
      "outputs": [],
      "source": [
        "! rm -f custom.tar custom.tar.gz\n",
        "! tar cvf custom.tar custom\n",
        "! gzip custom.tar\n",
        "! gsutil cp custom.tar.gz $BUCKET_NAME/trainer_chicago.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_custom_pipeline:lbn"
      },
      "source": [
        "## Construct custom training pipeline\n",
        "\n",
        "In the example below, you construct a pipeline for training a custom model using pre-built Google Cloud Pipeline Components for Vertex AI Training, as follows:\n",
        "\n",
        "\n",
        "1. Pipeline arguments, specify the locations of:\n",
        "    - `dataset_id`: The full resource path name to the corresponding `Vertex AI Dataset` training.\n",
        "    - `model_id`: The full resource path name to the corresponding `Vertex AI Model` model architecture.\n",
        "    - `python_package`: The custom training Python package.\n",
        "    - `python_module`: The entry module in the package to execute.\n",
        "    - `args`: The command line arguments to the custom training Python script.\n",
        "    - `container_uri`: The training container.\n",
        "    - `model_serving_container_image_uri`: The serving container.\n",
        "    - `machine_type`: The machine VM image for training.\n",
        "    - `replica_count`: The number of machine VMs for training.\n",
        "    - `accelerator_type`: The type of accelerator, if any, for training.\n",
        "    - `accelerator_count`: The number of accelerators for training.\n",
        "    - `tensorboard`:The full resource path to the `Vertex AI Tensorboard` instance.\n",
        "    - `warmup`: Warmup the initialization of the base model architecture weights.\n",
        "    - `bucket`: The Cloud Storage location of the trained model artifacts.\n",
        "    - `service_account`: The service account for executing the pipeline components.\n",
        "    - `project`: The project for executing the pipeline components.\n",
        "    - `label`: Metadata relating to the trained model to store in the corresponding trained `Vertex AI Model` resource.\n",
        "\n",
        "\n",
        "2. Use the prebuilt component `CustomPythonPackageTrainingJobRunOp` to train a custom model and upload the custom model as a Vertex AI Model resource, where:\n",
        "    - The display name for the dataset is passed into the pipeline.\n",
        "    - The dataset is the output from the `TabularDatasetCreateOp`.\n",
        "    - The python package, command line argument are passed into the pipeline.\n",
        "    - The training and serving containers are specified in the pipeline definition.\n",
        "    - The component returns the model resource as `outputs[\"model\"]`.\n",
        "\n",
        "\n",
        "3. Use the prebuilt component `EndpointCreateOp` to create a `Vertex AI Endpoint` to deploy the trained model to, where:\n",
        "    - Since the component has no dependencies on other components, by default it would be executed in parallel with the model training.\n",
        "    - The `after(training_op)` is added to serialize its execution, so its only executed if the training operation completes successfully.\n",
        "     - The component returns the endpoint resource as `outputs[\"endpoint\"]`.\n",
        "\n",
        "\n",
        "4. Use the prebuilt component `ModelDeployOp` to deploy the trained `Vertex AI Model` to, where:\n",
        "    - The display name for the dataset is passed into the pipeline.\n",
        "    - The model is the output from the `CustomPythonPackageTrainingJobRunOp`.\n",
        "    - The endpoint is the output from the `EndpointCreateOp`.\n",
        "\n",
        "*Note:* Since each component is executed as a graph node in its own execution context, you pass the parameter `project` for each component op, in constrast to doing a `aip.init(project=project)` if this was a Python script calling the SDK methods directly within the same execution context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_custom_pipeline:lbn"
      },
      "outputs": [],
      "source": [
        "@dsl.pipeline(\n",
        "    name=\"chicago-custom-training\",\n",
        "    description=\"Custom tabular binary classification training\",\n",
        ")\n",
        "def pipeline(\n",
        "    display_name: str,\n",
        "    dataset_id: str,\n",
        "    model_id: str,\n",
        "    python_package: str,\n",
        "    python_module: str,\n",
        "    args: str,\n",
        "    container_uri: str,\n",
        "    machine_type: str,\n",
        "    bucket: str,\n",
        "    model_serving_container_image_uri: str = \"\",\n",
        "    tensorboard: str = \"\",\n",
        "    service_account: str = \"\",\n",
        "    label: str = str({\"candidate_model\": \"1\"}).replace(\"'\", '\"'),\n",
        "    replica_count: int = 1,\n",
        "    accelerator_type: str = \"\",\n",
        "    accelerator_count: int = 0,\n",
        "    warmup: str = \"False\",\n",
        "    project: str = PROJECT_ID,\n",
        "    region: str = REGION,\n",
        "):\n",
        "    from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
        "    from google_cloud_pipeline_components.v1.endpoint import (EndpointCreateOp,\n",
        "                                                              ModelDeployOp)\n",
        "\n",
        "    with dsl.Condition(warmup == \"True\", name=\"warmup-model\"):\n",
        "\n",
        "        warmup_op = gcc_aip.CustomPythonPackageTrainingJobRunOp(\n",
        "            project=project,\n",
        "            display_name=display_name,\n",
        "            # Warmup Training\n",
        "            python_package_gcs_uri=python_package,\n",
        "            python_module_name=python_module,\n",
        "            container_uri=container_uri,\n",
        "            staging_bucket=bucket,\n",
        "            args=args,\n",
        "            replica_count=replica_count,\n",
        "            machine_type=machine_type,\n",
        "            accelerator_type=accelerator_type,\n",
        "            accelerator_count=accelerator_count,\n",
        "        )\n",
        "\n",
        "    with dsl.Condition(warmup == \"False\", name=\"train-model\"):\n",
        "\n",
        "        training_op = gcc_aip.CustomPythonPackageTrainingJobRunOp(\n",
        "            project=project,\n",
        "            display_name=display_name,\n",
        "            # Full Training\n",
        "            python_package_gcs_uri=python_package,\n",
        "            python_module_name=python_module,\n",
        "            container_uri=container_uri,\n",
        "            staging_bucket=bucket,\n",
        "            args=args,\n",
        "            replica_count=replica_count,\n",
        "            machine_type=machine_type,\n",
        "            accelerator_type=accelerator_type,\n",
        "            accelerator_count=accelerator_count,\n",
        "            tensorboard=tensorboard,\n",
        "            service_account=service_account,\n",
        "            # Serving - As part of this operation, the model is registered to Vertex AI\n",
        "            model_serving_container_image_uri=model_serving_container_image_uri,\n",
        "            model_display_name=display_name,\n",
        "            labels=label,\n",
        "        )\n",
        "\n",
        "        endpoint_op = EndpointCreateOp(\n",
        "            project=project,\n",
        "            location=region,\n",
        "            display_name=display_name,\n",
        "        ).after(training_op)\n",
        "\n",
        "        deploy_op = ModelDeployOp(\n",
        "            model=training_op.outputs[\"model\"],\n",
        "            endpoint=endpoint_op.outputs[\"endpoint\"],\n",
        "            dedicated_resources_min_replica_count=1,\n",
        "            dedicated_resources_max_replica_count=1,\n",
        "            dedicated_resources_machine_type=\"n1-standard-4\",\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_pipeline:model_warmup"
      },
      "source": [
        "### Compile and execute the model warmup condition of the pipeline\n",
        "\n",
        "Next, you compile the pipeline and then execute it. The pipeline takes the following parameters, which are passed as the dictionary `parameter_values`:\n",
        "\n",
        "- `dataset_id`: The full resource name of the corresponding Vertex AI Dataset.\n",
        "- `model_id`: The full resource name of the corresponding Vertex AI Model architecture.\n",
        "- `display_name`: The display name for the trained Vertex AI Model resource.\n",
        "- `python_package`: The Python package for the custom warmup training job.\n",
        "- `python_module`: The Python module in the package to execute.\n",
        "- `args`: The command line arguments to pass to the Python module.\n",
        "- `container_uri`: The training container image.\n",
        "- `machine_type`: The VM for executing the training job.\n",
        "- `replica_count`: The number of virtual machines -- if doing distributed multi-machine training.\n",
        "- `accelerator_type`: The type of HW accelerators -- if any.\n",
        "- `accelerator_count`: The number of HW accelerators -- if any.\n",
        "- `bucket`: The Cloud Storage location to store the model artifacts.\n",
        "- `tensorboard`: The full resource name of a Vertex AI Tensorboard.\n",
        "- `service_account`: The service account for the Tensorboard instance.\n",
        "- `project`: The project ID.\n",
        "- `region`: The region.\n",
        "\n",
        "*Note*: This portion of the pipeline does not create a new `Vertex AI Model` resource, but instead updates the weight of the model artifacts of the existing `Vertex AI Model` base architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_pipeline:model_warmup"
      },
      "outputs": [],
      "source": [
        "PIPELINE_ROOT = \"{}/pipeline_root/model-train\".format(BUCKET_NAME)\n",
        "\n",
        "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"model_train.json\")\n",
        "\n",
        "pipeline = aip.PipelineJob(\n",
        "    display_name=\"model-warmup\",\n",
        "    template_path=\"model_train.json\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        "    parameter_values={\n",
        "        \"dataset_id\": dataset_id,\n",
        "        \"model_id\": model_id,\n",
        "        \"display_name\": \"chicago\" + TIMESTAMP,\n",
        "        \"python_package\": f\"{BUCKET_NAME}/trainer_chicago.tar.gz\",\n",
        "        \"python_module\": \"trainer.task\",\n",
        "        \"args\": [\n",
        "            \"--dataset-id\",\n",
        "            dataset_id,\n",
        "            \"--model-id\",\n",
        "            model_id,\n",
        "            \"--epochs\",\n",
        "            str(5),\n",
        "            \"--batch_size\",\n",
        "            str(16),\n",
        "            \"--steps\",\n",
        "            str(200),\n",
        "            \"--lr\",\n",
        "            baseline_metrics[\"learning_rate\"],\n",
        "            \"--start_lr\",\n",
        "            0.0001,\n",
        "            \"--warmup\",\n",
        "            True,\n",
        "            \"--project\",\n",
        "            PROJECT_ID,\n",
        "        ],\n",
        "        \"container_uri\": TRAIN_IMAGE,\n",
        "        \"machine_type\": TRAIN_COMPUTE,\n",
        "        \"replica_count\": 1,\n",
        "        \"accelerator_type\": TRAIN_GPU.name,\n",
        "        \"accelerator_count\": TRAIN_NGPU,\n",
        "        \"bucket\": BUCKET_NAME,\n",
        "        \"project\": PROJECT_ID,\n",
        "        \"region\": REGION,\n",
        "        \"warmup\": \"True\",\n",
        "    },\n",
        ")\n",
        "\n",
        "pipeline.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_tensorboard_instance"
      },
      "source": [
        "### Create a Vertex AI TensorBoard instance\n",
        "\n",
        "Create a Vertex AI TensorBoard instance to use TensorBoard in conjunction with Vertex AI Training for custom model training.\n",
        "\n",
        "Learn more about [Get started with Vertex AI TensorBoard](https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-overview)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_tensorboard_instance"
      },
      "outputs": [],
      "source": [
        "TENSORBOARD_DISPLAY_NAME = \"chicago_\" + TIMESTAMP\n",
        "tensorboard = aip.Tensorboard.create(display_name=TENSORBOARD_DISPLAY_NAME)\n",
        "tensorboard_resource_name = tensorboard.gca_resource.name\n",
        "print(\"TensorBoard resource name:\", tensorboard_resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_pipeline:model_train"
      },
      "source": [
        "### Compile and execute the model training pipeline\n",
        "\n",
        "Next, you compile the pipeline and then execute it. The pipeline takes the following parameters, which are passed as the dictionary `parameter_values`:\n",
        "\n",
        "- `dataset_id`: The full resource name of the corresponding Vertex AI Dataset.\n",
        "- `model_id`: The full resource name of the corresponding Vertex AI Model architecture.\n",
        "- `display_name`: The display name for the trained Vertex AI Model resource.\n",
        "- `python_package`: The Python package for the custom training job.\n",
        "- `python_module`: The Python module in the package to execute.\n",
        "- `args`: The command line arguments to pass to the Python module.\n",
        "    - *Note*: The pipeline uses the hyperparameters from the baseline model. Alternatively, one could use the hyperparameters from the current blessed model, or repeat hyperparameter tuning.\n",
        "- `container_uri`: The training container image.\n",
        "- `model_serving_container_image_uri`: The associated deployment container image.\n",
        "- `machine_type`: The VM for executing the training job.\n",
        "- `replica_count`: The number of virtual machines -- if doing distributed multi-machine training.\n",
        "- `accelerator_type`: The type of HW accelerators -- if any.\n",
        "- `accelerator_count`: The number of HW accelerators -- if any.\n",
        "- `bucket`: The Cloud Storage location to store the model artifacts.\n",
        "- `tensorboard`: The full resource name of a Vertex AI Tensorboard.\n",
        "- `service_account`: The service account for the Tensorboard instance.\n",
        "- `project`: The project ID.\n",
        "- `region`: The region."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_pipeline:model_train"
      },
      "outputs": [],
      "source": [
        "PIPELINE_ROOT = \"{}/pipeline_root/model-train\".format(BUCKET_NAME)\n",
        "\n",
        "pipeline = aip.PipelineJob(\n",
        "    display_name=\"model-train\",\n",
        "    template_path=\"model_train.json\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        "    parameter_values={\n",
        "        \"dataset_id\": dataset_id,\n",
        "        \"model_id\": model_id,\n",
        "        \"display_name\": \"chicago\" + TIMESTAMP,\n",
        "        \"python_package\": f\"{BUCKET_NAME}/trainer_chicago.tar.gz\",\n",
        "        \"python_module\": \"trainer.task\",\n",
        "        \"args\": [\n",
        "            \"--dataset-id\",\n",
        "            dataset_id,\n",
        "            \"--model-id\",\n",
        "            model_id,\n",
        "            \"--experiment\",\n",
        "            \"chicago\" + TIMESTAMP,\n",
        "            \"--run\",\n",
        "            \"retrain\",\n",
        "            \"--epochs\",\n",
        "            str(int(baseline_metrics[\"num_epochs\"])),\n",
        "            \"--batch_size\",\n",
        "            str(int(baseline_metrics[\"batch_size\"])),\n",
        "            \"--lr\",\n",
        "            baseline_metrics[\"learning_rate\"],\n",
        "            \"--evaluate\",\n",
        "            \"True\",\n",
        "            \"--serving\",\n",
        "            \"True\",\n",
        "            \"--project\",\n",
        "            PROJECT_ID,\n",
        "        ],\n",
        "        \"container_uri\": TRAIN_IMAGE,\n",
        "        \"tensorboard\": tensorboard.gca_resource.name,\n",
        "        \"service_account\": SERVICE_ACCOUNT,\n",
        "        \"model_serving_container_image_uri\": DEPLOY_IMAGE,\n",
        "        \"machine_type\": TRAIN_COMPUTE,\n",
        "        \"replica_count\": 1,\n",
        "        \"accelerator_type\": TRAIN_GPU.name,\n",
        "        \"accelerator_count\": TRAIN_NGPU,\n",
        "        \"bucket\": BUCKET_NAME,\n",
        "        \"project\": PROJECT_ID,\n",
        "        \"region\": REGION,\n",
        "    },\n",
        ")\n",
        "\n",
        "pipeline.run()\n",
        "\n",
        "! rm -rf model_train.json custom_tar.gz custom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view_pipleline_results:model_train"
      },
      "source": [
        "### View the training pipeline results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "view_pipleline_results:model_train"
      },
      "outputs": [],
      "source": [
        "print(\"custompythonpackagetrainingjob-run-2\")\n",
        "artifacts = print_pipeline_output(pipeline, \"custompythonpackagetrainingjob-run-2\")\n",
        "print(\"\\n\")\n",
        "output = !gsutil cat $artifacts\n",
        "output = json.loads(output[0])\n",
        "model_id = output[\"artifacts\"][\"model\"][\"artifacts\"][0][\"metadata\"][\"resourceName\"]\n",
        "print(\"\\n\")\n",
        "print(model_id)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"endpoint-create\")\n",
        "artifacts = print_pipeline_output(pipeline, \"endpoint-create\")\n",
        "print(\"\\n\")\n",
        "print(\"model-deploy\")\n",
        "artifacts = print_pipeline_output(pipeline, \"model-deploy\")\n",
        "print(\"\\n\")\n",
        "print(\"endpoint-create\")\n",
        "artifacts = print_pipeline_output(pipeline, \"endpoint-create\")\n",
        "print(\"\\n\")\n",
        "print(\"model-deploy\")\n",
        "artifacts = print_pipeline_output(pipeline, \"model-deploy\")\n",
        "print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "get_experiment:init"
      },
      "source": [
        "### Get the experiment results\n",
        "\n",
        "Next, you use the experiment name as a parameter to the method `get_experiment_df()` to get the results of the experiment as a pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "get_experiment:init"
      },
      "outputs": [],
      "source": [
        "EXPERIMENT_NAME = \"chicago\" + TIMESTAMP\n",
        "\n",
        "aip.init(experiment=EXPERIMENT_NAME)\n",
        "experiment_df = aip.get_experiment_df()\n",
        "experiment_df = experiment_df[experiment_df.experiment_name == EXPERIMENT_NAME]\n",
        "experiment_df.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "get_metrics_trained_model"
      },
      "source": [
        "#### Get the evaluation metrics of the trained model\n",
        "\n",
        "Now that the model is trained, get and display the evaluation metric results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "get_metrics_trained_model"
      },
      "outputs": [],
      "source": [
        "model = aip.Model(model_id)\n",
        "model_artifacts = model.gca_resource.artifact_uri\n",
        "\n",
        "!gsutil cat {model_artifacts}/metrics.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup:stage3"
      },
      "outputs": [],
      "source": [
        "delete_all = False\n",
        "\n",
        "if delete_all:\n",
        "    # Delete the dataset using the Vertex dataset object\n",
        "    try:\n",
        "        if \"dataset\" in globals():\n",
        "            dataset.delete()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    # Delete the model using the Vertex model object\n",
        "    try:\n",
        "        if \"model\" in globals():\n",
        "            model.delete()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    # delete the BQ table\n",
        "    # delete the pipeline\n",
        "\n",
        "    if \"BUCKET_NAME\" in globals():\n",
        "        ! gsutil rm -r $BUCKET_NAME"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "mlops_formalization.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "copyright"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff60de67fa8d"
      },
      "source": [
        "This notebook is a revised version of an unpublished notebook from Juan Acevedo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title:generic,gcp"
      },
      "source": [
        "# E2E ML on GCP: MLOps stage 3 : formalization: get started with TFX pipelines\n",
        "\n",
        "\n",
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/get_started_with_tfx_pipeline.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/get_started_with_tfx_pipeline.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/ml_ops/stage3/get_started_with_tfx_pipeline.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>                                                                                               \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview:mlops"
      },
      "source": [
        "## Overview\n",
        "\n",
        "\n",
        "This tutorial demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. This tutorial covers stage 3 : formalization: get started with TFX and Vertex AI Pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "objective:mlops,stage4,get_started_vertex_model_evaluation"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to use TensorFlow Extended (TFX) with `Vertex AI Pipelines`.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services:\n",
        "\n",
        "- `Vertex AI Pipelines`\n",
        "- `Vertex AI Training`\n",
        "- `Google Cloud Pipeline Components`\n",
        "- `Dataflow`\n",
        "\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Create a TFX e2e pipeline.\n",
        "- Execute the pipeline locally.\n",
        "- Execute the pipeline on Google Cloud using `Vertex AI Training`\n",
        "- Execute the pipeline using `Vertex AI Pipelines`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset:bank,lbn"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset used for this tutorial is the [CIFAR10 dataset](https://www.tensorflow.org/datasets/catalog/cifar10) from [TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/overview). The version of the dataset is built into TensorFlow. The trained model predicts which type of class an image is from ten classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, or truck."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "costs"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_mlops"
      },
      "source": [
        "## Installations\n",
        "\n",
        "Install the packages required for executing this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_mlops"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Vertex AI Workbench Notebook product has specific requirements\n",
        "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
        "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
        "    \"/opt/deeplearning/metadata/env_version\"\n",
        ")\n",
        "\n",
        "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_WORKBENCH_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\"\n",
        "\n",
        "! pip3 install --upgrade google-cloud-aiplatform $USER_FLAG -q\n",
        "! pip3 install --upgrade google-cloud-pipeline-components $USER_FLAG -q\n",
        "! pip3 install --upgrade tfx[kfp] $USER_FLAG -q\n",
        "! pip3 install --upgrade tensorflow $USER_FLAG -q\n",
        "! pip3 install --upgrade tensorflow-hub $USER_FLAG -q\n",
        "! pip3 install --upgrade apache-beam[gcp] $USER_FLAG -q\n",
        "! pip3 install -U tensorflow-io {USER_FLAG} -q\n",
        "! pip3 install -U tensorflow-estimator==2.6.0 {USER_FLAG} -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "restart"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "restart"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "before_you_begin"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### GPU runtime\n",
        "\n",
        "*Make sure you're running this notebook in a GPU runtime if you have that option. In Colab, select* **Runtime > Change Runtime Type > GPU**\n",
        "\n",
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "2. [Make sure that billing is enabled for your project.](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
        "\n",
        "3. Enable the APIs necessary to execute this notebook -- see cell below.\n",
        "\n",
        "4. If you are running this notebook locally, you will need to install the [Cloud SDK]((https://cloud.google.com/sdk)).\n",
        "\n",
        "5. Enter your project ID in the cell below. Then run the  cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "619529337e6d"
      },
      "outputs": [],
      "source": [
        "! gcloud services enable compute.googleapis.com         \\\n",
        "                       containerregistry.googleapis.com  \\\n",
        "                       aiplatform.googleapis.com  \\\n",
        "                       cloudbuild.googleapis.com \\\n",
        "                       cloudfunctions.googleapis.com \\\n",
        "                       dataflow.googleapis.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "project_id"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_project_id"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_project_id"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID:\", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_gcloud_project_id"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
        "\n",
        "- Americas: `us-central1`\n",
        "- Europe: `europe-west4`\n",
        "- Asia Pacific: `asia-east1`\n",
        "\n",
        "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
        "\n",
        "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "region"
      },
      "outputs": [],
      "source": [
        "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
        "\n",
        "if REGION == \"[your-region]\":\n",
        "    REGION = \"us-central1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "timestamp"
      },
      "source": [
        "#### Timestamp\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "timestamp"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcp_authenticate"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using Vertex AI Workbench Notebooks**, your environment is already authenticated. Skip this step.\n",
        "\n",
        "**If you are using Colab**, run the cell below and follow the instructions when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "In the Cloud Console, go to the [Create service account key](https://console.cloud.google.com/apis/credentials/serviceaccountkey) page.\n",
        "\n",
        "**Click Create service account**.\n",
        "\n",
        "In the **Service account name** field, enter a name, and click **Create**.\n",
        "\n",
        "In the **Grant this service account access to project** section, click the Role drop-down list. Type \"Vertex\" into the filter box, and select **Vertex Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "\n",
        "Click Create. A JSON file that contains your key downloads to your local environment.\n",
        "\n",
        "Enter the path to your service account key as the GOOGLE_APPLICATION_CREDENTIALS variable in the cell below and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcp_authenticate"
      },
      "outputs": [],
      "source": [
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# If on Vertex AI Workbench, then don't execute this code\n",
        "IS_COLAB = False\n",
        "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
        "    \"DL_ANACONDA_HOME\"\n",
        "):\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        IS_COLAB = True\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bucket:mbsdk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "When you initialize the Vertex SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bucket"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_bucket"
      },
      "outputs": [],
      "source": [
        "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
        "    BUCKET_NAME = PROJECT_ID + \"aip-\" + TIMESTAMP\n",
        "    BUCKET_URI = \"gs://\" + BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_bucket"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "validate_bucket"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "validate_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "set_service_account"
      },
      "source": [
        "#### Service Account\n",
        "\n",
        "**If you don't know your service account**, try to get your service account using `gcloud` command by executing the second cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_service_account"
      },
      "outputs": [],
      "source": [
        "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_service_account"
      },
      "outputs": [],
      "source": [
        "if (\n",
        "    SERVICE_ACCOUNT == \"\"\n",
        "    or SERVICE_ACCOUNT is None\n",
        "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
        "):\n",
        "    # Get your service account from gcloud\n",
        "    if not IS_COLAB:\n",
        "        shell_output = !gcloud auth list 2>/dev/null\n",
        "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
        "\n",
        "    if IS_COLAB:\n",
        "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
        "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "\n",
        "    print(\"Service Account:\", SERVICE_ACCOUNT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "set_service_account:pipelines"
      },
      "source": [
        "#### Set service account access for Vertex AI Pipelines\n",
        "\n",
        "Run the following commands to grant your service account access to read and write pipeline artifacts in the bucket that you created in the previous step -- you only need to run these once per service account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_service_account:pipelines"
      },
      "outputs": [],
      "source": [
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI\n",
        "\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/dataflow.admin $BUCKET_URI\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/dataflow.worker $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_vars"
      },
      "source": [
        "### Set up variables\n",
        "\n",
        "Next, set up some variables used throughout the tutorial.\n",
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_kfp"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from os import listdir\n",
        "\n",
        "import google.cloud.aiplatform as aiplatform\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from kfp import dsl\n",
        "from kfp.v2 import compiler\n",
        "from kfp.v2.dsl import component\n",
        "from tfx import v1 as tfx\n",
        "from tfx.components import (BulkInferrer, Evaluator, ExampleValidator,\n",
        "                            ImportExampleGen, InfraValidator, Pusher,\n",
        "                            SchemaGen, StatisticsGen, Trainer, Transform,\n",
        "                            Tuner)\n",
        "from tfx.dsl.components.common import resolver\n",
        "from tfx.dsl.experimental import latest_blessed_model_resolver\n",
        "from tfx.orchestration import metadata, pipeline\n",
        "from tfx.proto import (bulk_inferrer_pb2, example_gen_pb2, pusher_pb2,\n",
        "                       trainer_pb2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "outputs": [],
      "source": [
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "accelerators:prediction,mbsdk"
      },
      "source": [
        "#### Set hardware accelerators\n",
        "\n",
        "You can set hardware accelerators for training and prediction.\n",
        "\n",
        "Set the variables `TRAIN_GPU/TRAIN_NGPU` and `DEPLOY_GPU/DEPLOY_NGPU` to use a container image supporting a GPU and the number of GPUs allocated to the virtual machine (VM) instance. For example, to use a GPU container image with 4 Nvidia Telsa K80 GPUs allocated to each VM, you would specify:\n",
        "\n",
        "    (aip.AcceleratorType.NVIDIA_TESLA_K80, 4)\n",
        "\n",
        "\n",
        "Otherwise specify `(None, None)` to use a container image to run on a CPU.\n",
        "\n",
        "Learn more about [hardware accelerator support for your region](https://cloud.google.com/vertex-ai/docs/general/locations#accelerators).\n",
        "\n",
        "*Note*: TF releases before 2.3 for GPU support will fail to load the custom model in this tutorial. It is a known issue and fixed in TF 2.3. This is caused by static graph ops that are generated in the serving function. If you encounter this issue on your own custom models, use a container image for TF 2.3 with GPU support."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "accelerators:prediction,mbsdk"
      },
      "outputs": [],
      "source": [
        "if os.getenv(\"IS_TESTING_TRAIN_GPU\"):\n",
        "    TRAIN_GPU, TRAIN_NGPU = (\n",
        "        aip.gapic.AcceleratorType.NVIDIA_TESLA_K80,\n",
        "        int(os.getenv(\"IS_TESTING_TRAIN_GPU\")),\n",
        "    )\n",
        "else:\n",
        "    TRAIN_GPU, TRAIN_NGPU = (None, None)\n",
        "\n",
        "if os.getenv(\"IS_TESTING_DEPLOY_GPU\"):\n",
        "    DEPLOY_GPU, DEPLOY_NGPU = (\n",
        "        aip.gapic.AcceleratorType.NVIDIA_TESLA_K80,\n",
        "        int(os.getenv(\"IS_TESTING_DEPLOY_GPU\")),\n",
        "    )\n",
        "else:\n",
        "    DEPLOY_GPU, DEPLOY_NGPU = (None, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "container:prediction"
      },
      "source": [
        "#### Set pre-built containers\n",
        "\n",
        "Set the pre-built Docker container image for training and prediction.\n",
        "\n",
        "\n",
        "For the latest list, see [Pre-built containers for training](https://cloud.google.com/ai-platform-unified/docs/training/pre-built-containers).\n",
        "\n",
        "\n",
        "For the latest list, see [Pre-built containers for prediction](https://cloud.google.com/ai-platform-unified/docs/predictions/pre-built-containers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "container:prediction"
      },
      "outputs": [],
      "source": [
        "if os.getenv(\"IS_TESTING_TF\"):\n",
        "    TF = os.getenv(\"IS_TESTING_TF\")\n",
        "else:\n",
        "    TF = \"2.5\".replace(\".\", \"-\")\n",
        "\n",
        "if TF[0] == \"2\":\n",
        "    if TRAIN_GPU:\n",
        "        TRAIN_VERSION = \"tf-gpu.{}\".format(TF)\n",
        "    else:\n",
        "        TRAIN_VERSION = \"tf-cpu.{}\".format(TF)\n",
        "    if DEPLOY_GPU:\n",
        "        DEPLOY_VERSION = \"tf2-gpu.{}\".format(TF)\n",
        "    else:\n",
        "        DEPLOY_VERSION = \"tf2-cpu.{}\".format(TF)\n",
        "else:\n",
        "    if TRAIN_GPU:\n",
        "        TRAIN_VERSION = \"tf-gpu.{}\".format(TF)\n",
        "    else:\n",
        "        TRAIN_VERSION = \"tf-cpu.{}\".format(TF)\n",
        "    if DEPLOY_GPU:\n",
        "        DEPLOY_VERSION = \"tf-gpu.{}\".format(TF)\n",
        "    else:\n",
        "        DEPLOY_VERSION = \"tf-cpu.{}\".format(TF)\n",
        "\n",
        "TRAIN_IMAGE = \"{}-docker.pkg.dev/vertex-ai/training/{}:latest\".format(\n",
        "    REGION.split(\"-\")[0], TRAIN_VERSION\n",
        ")\n",
        "DEPLOY_IMAGE = \"{}-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(\n",
        "    REGION.split(\"-\")[0], DEPLOY_VERSION\n",
        ")\n",
        "\n",
        "print(\"Training:\", TRAIN_IMAGE, TRAIN_GPU, TRAIN_NGPU)\n",
        "print(\"Deployment:\", DEPLOY_IMAGE, DEPLOY_GPU, DEPLOY_NGPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "machine:prediction"
      },
      "source": [
        "#### Set machine type\n",
        "\n",
        "Next, set the machine type to use for training and prediction.\n",
        "\n",
        "- Set the variables `TRAIN_COMPUTE` and `DEPLOY_COMPUTE` to configure  the compute resources for the VMs you will use for for training and prediction.\n",
        " - `machine type`\n",
        "     - `n1-standard`: 3.75GB of memory per vCPU.\n",
        "     - `n1-highmem`: 6.5GB of memory per vCPU\n",
        "     - `n1-highcpu`: 0.9 GB of memory per vCPU\n",
        " - `vCPUs`: number of \\[2, 4, 8, 16, 32, 64, 96 \\]\n",
        "\n",
        "*Note: The following is not supported for training:*\n",
        "\n",
        " - `standard`: 2 vCPUs\n",
        " - `highcpu`: 2, 4 and 8 vCPUs\n",
        "\n",
        "*Note: You may also use n2 and e2 machine types for training and deployment, but they do not support GPUs*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "machine:prediction"
      },
      "outputs": [],
      "source": [
        "if os.getenv(\"IS_TESTING_TRAIN_MACHINE\"):\n",
        "    MACHINE_TYPE = os.getenv(\"IS_TESTING_TRAIN_MACHINE\")\n",
        "else:\n",
        "    MACHINE_TYPE = \"n1-standard\"\n",
        "\n",
        "VCPU = \"4\"\n",
        "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
        "print(\"Train machine type\", TRAIN_COMPUTE)\n",
        "\n",
        "if os.getenv(\"IS_TESTING_DEPLOY_MACHINE\"):\n",
        "    MACHINE_TYPE = os.getenv(\"IS_TESTING_DEPLOY_MACHINE\")\n",
        "else:\n",
        "    MACHINE_TYPE = \"n1-standard\"\n",
        "\n",
        "VCPU = \"4\"\n",
        "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
        "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro:model_eval,automl"
      },
      "source": [
        "## Introduction to TensorFlow Extended (TFX)\n",
        "\n",
        "A component is an implementation of an ML task that you can use as a step in your TFX pipeline. TFX provides several [standard components](https://www.tensorflow.org/tfx/guide#tfx_standard_components) that you can use in your pipelines. If these components do not meet your needs, you can build [custom components](https://www.tensorflow.org/tfx/guide/understanding_custom_components).\n",
        "\n",
        "The outputs of steps in a TFX pipeline are artifacts. Subsequent steps in your workflow may use these artifacts as inputs. In this way, TFX lets you transfter data between workflow steps.\n",
        "\n",
        "This tutorial will cover the following standard TFX components.\n",
        "\n",
        "- ExampleGen\n",
        "- StatisticsGen\n",
        "- SchemaGen\n",
        "- ExampleValidator\n",
        "- Transform\n",
        "- Trainer\n",
        "- Tuner\n",
        "- Evaluator\n",
        "- InfraValidator\n",
        "- Pusher\n",
        "- BulkInferer\n",
        "\n",
        "<img src=\"https://g3doc.corp.google.com/cloud/sales/teams/sales_aiml_northam/g3doc/tfx-pipelines/img/tfx-components.png\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_file:u_dataset,csv"
      },
      "source": [
        "#### Location of Cloud Storage training data.\n",
        "\n",
        "Next, you download a subset of the CIFAR-10 dataset as TFRecords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_file:flowers,csv,icn"
      },
      "outputs": [],
      "source": [
        "! rm -rf custom\n",
        "! mkdir custom\n",
        "\n",
        "! wget https://github.com/tensorflow/tfx/raw/master/tfx/examples/cifar10/data/test/cifar10_test.tfrecord -P custom/data/test/\n",
        "! wget https://github.com/tensorflow/tfx/raw/master/tfx/examples/cifar10/data/train/cifar10_train.tfrecord -P custom/data/train/\n",
        "! wget https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/cifar10/data/labels.txt -P custom/data/\n",
        "\n",
        "DATA_ROOT = \"custom/data\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d355a7ff607c"
      },
      "source": [
        "### Overview of ExampleGen component\n",
        "\n",
        "The `ExampleGen` component ingests data into TFX pipelines. In consumes external files/services to generate examples which will be read by other TFX components. It also provides consistent and configurable partition, and shuffles the dataset for ML best practice.\n",
        "\n",
        "* Consumes: Data from external data sources such as CSV, TFRecord, Avro, Parquet and BigQuery.\n",
        "\n",
        "* Emits: Artifact of tf.Example records, tf.SequenceExample records, or proto format, depending on the payload format.\n",
        "\n",
        "In this example, you call `ImportExampleGen()` with the following parameters:\n",
        "\n",
        "- `input_base`: (optional) The location of the TFRecord dataset. Default is None.\n",
        "- `input_config`: (optional) How the dataset is laid out at the input_base. Default is None. If unset, the files under input_base will be treated as a single split.\n",
        "    - `splits`: How the dataset is split.\n",
        "    \n",
        "Additional parameters you may set:\n",
        "\n",
        "- `output_config`: (optional) The output configuration. Default is None. If unset, default splits will be 'train' and 'eval' with size 2:1.\n",
        "- `range_config`: (optional) Specifies the range of span values to consider. Default is None. If unset, driver will default to searching for latest span with no restrictions.\n",
        "- `payload_format`: (optional) Payload format of input data. Should be one of example_gen_pb2.PayloadFormat enum. Default is `example_gen_pb2.FORMAT_TF_EXAMPLE`.\n",
        "    \n",
        "The output from this call is a compiled component that can be executed in the context of a pipeline.\n",
        "\n",
        "Learn more about [ImportExampleGen](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/components/ImportExampleGen)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "330ff0d4fe80"
      },
      "outputs": [],
      "source": [
        "input_config = example_gen_pb2.Input(\n",
        "    splits=[\n",
        "        example_gen_pb2.Input.Split(name=\"train\", pattern=\"train/*\"),\n",
        "        example_gen_pb2.Input.Split(name=\"eval\", pattern=\"test/*\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "example_gen = ImportExampleGen(input_base=DATA_ROOT, input_config=input_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "827d93e8e92b"
      },
      "source": [
        "### Overview of the StatisticsGen component\n",
        "\n",
        "The `StatisticsGen` component generates features statistics over both training and serving data, which can be used by other pipeline components. `StatisticsGen` uses Apache Beam to scale to large datasets.\n",
        "\n",
        "* Consumes: Dataset artifact created by an ExampleGen pipeline component.\n",
        "\n",
        "* Emits: Dataset statistics artifact.\n",
        "\n",
        "In this example, you call `StatisticsGen()` with the following parameters:\n",
        "\n",
        "- `examples`: The dataset artifact from which to produce the dataset statistics artifact, i.e., `example_gen.outputs['examples']`.\n",
        "\n",
        "Additional parameters you may set:\n",
        "\n",
        "- `schema`: (optional) A schema channel to use for automatically configuring the value of `stats_options` passed to TensorFlow Data Validation (TFDV) library.\n",
        "- `stats_options`: (optional) \tThe StatsOptions instance to configure optional TFDV behavior. When `stats_options.schema` is set, it will be used instead of the schema channel input. \n",
        "- `exlude_splits`: List of names of splits to exclude from inferring the schema. Default is None.\n",
        "\n",
        "The output from this call is a compiled component that can be executed in the context of a pipeline.\n",
        "\n",
        "Learn more about [StatisticsGen](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/components/StatisticsGen)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b76e3e77cd85"
      },
      "outputs": [],
      "source": [
        "statistics_gen = StatisticsGen(examples=example_gen.outputs[\"examples\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b9b9d373a29"
      },
      "source": [
        "### Overview of the SchemaGen component\n",
        "\n",
        "Some of the TFX components use a schema description of the input data. The schema is an instance of `schema.proto`. It can specify data types for feature values, whether a feature has to be present in all examples, allowed value ranges, and other properties. A `SchemaGen` pipeline component will automatically generate a schema by inferring types, categories, and ranges from the training data.\n",
        "\n",
        "* Consumes: Dataset statistics artifact from a `StatisticsGen` component.\n",
        "\n",
        "* Emits: Dataset schema artifact.\n",
        "\n",
        "In this example, you call `SchemaGen()` with the following parameters:\n",
        "\n",
        "- `statistics`: The dataset statistics artifact from which to produce a dataset schema artifact, i.e., `statistics_gen.outputs['statistics']`.\n",
        "- `infer_feature_shape`: (optonal) Whether to infer the feature shape. Default to True.\n",
        "\n",
        "Additional parameters you may set:\n",
        "\n",
        "- `exlude_splits`: (optional) List of names of splits to exclude from inferring the schema. Default is None.\n",
        "\n",
        "The output from this call is a compiled component that can be executed in the context of a pipeline.\n",
        "\n",
        "Learn more about [SchemaGen](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/components/SchemaGen)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "489383b45b7f"
      },
      "outputs": [],
      "source": [
        "schema_gen = SchemaGen(\n",
        "    statistics=statistics_gen.outputs[\"statistics\"], infer_feature_shape=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68ba05ecfad8"
      },
      "source": [
        "### Overview of the ExampleValidator component\n",
        "\n",
        "The `ExampleValidator` component identifies anomalies in training and serving data. It can detect different classes of anomalies in the data. For example it can:\n",
        "\n",
        "* Perform validity checks by comparing data statistics against a schema that codifies expectations of the user.\n",
        "\n",
        "* Detect training-serving skew by comparing training and serving data.\n",
        "\n",
        "* Detect data drift by looking at a series of data.\n",
        "\n",
        "The ExampleValidator component:\n",
        "\n",
        "* Consumes: A schema artifact from a `SchemaGen` component and statistics artifact from a `StatisticsGen` component.\n",
        "\n",
        "* Emits: Validation artifact.\n",
        "\n",
        "In this example, you call `ExampleValidator()` with the following parameters:\n",
        "\n",
        "- `statistics`: The dataset statistics artifact from `StatisticsGen` component.\n",
        "- `schema`: The dataset schema artifact from `SchemaGen` component.\n",
        "\n",
        "Additional parameters you may set:\n",
        "\n",
        "- `exlude_splits`: (optional) List of names of splits to exclude from validating. Default is None.\n",
        "\n",
        "The output from this call is a compiled component that can be executed in the context of a pipeline.\n",
        "\n",
        "Learn more about [ExampleValidator](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/components/ExampleValidator)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d03cf3921c44"
      },
      "outputs": [],
      "source": [
        "example_validator = ExampleValidator(\n",
        "    statistics=statistics_gen.outputs[\"statistics\"], schema=schema_gen.outputs[\"schema\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "076347205e0f"
      },
      "source": [
        "### Overview of the Transform component\n",
        "\n",
        "The `Transform` component performs feature engineering on `tf.Examples` emitted from an `ExampleGen` component, using a data schema created by a `SchemaGen` component, and emits both a SavedModel as well as statistics on both pre-transform and post-transform data. When executed, the SavedModel will accept `tf.Examples` emmited from an `ExampleGen` component and emit the transformed feature data.\n",
        "\n",
        "* Consumes: `tf.Examples` from an `ExampleGen` component, and a dataset schema from a `SchemaGen` component.\n",
        "\n",
        "* Emits: A SavedModel for a `Trainer` component, pre-transform and post-transform statistics.\n",
        "\n",
        "In this example, you call `Transform()` with the following parameters:\n",
        "\n",
        "- `examples`: Dataset examples from a `ExampleGen` component.\n",
        "- `schema`: A dataset schema from a `SchemaGen` component.\n",
        "- `module_file`: The file path to a python module file, from which the `preprocessing_fn` function will be loaded. Exactly one of `module_file` or `preprocessing_fn` must be supplied.\n",
        "\n",
        "Additional parameters you may set:\n",
        "\n",
        "- `preprocessing_fn`: (optional) The path to python function that implements a `preprocessing_fn`. \n",
        "- `splits_config`: (optional) Specifies splits that should be analyzed and transformed. Defailts to None.\n",
        "- `analyze_cache`: (optional) When provided, Transform will try use the cached calculation if possible. Defaults to None.\n",
        "- `materialize`: (optional) If True, write transformed examples as an output. Defaults to True.\n",
        "- `disable_analyzer_cache`: (optional) If False, Transform will use input cache if provided and write cache output. If True, analyzer_cache must not be provided. Defaults to False.\n",
        "- `force_tf_compat_v1`: (optional) If True and/or TF2 behaviors are disabled Transform will use Tensorflow in compat.v1 mode irrespective of installed version of Tensorflow. Defaults to False. \n",
        "- `custom_config`: (optional) A dictionary which contains additional parameters that will be passed to preprocessing_fn. Defaults to None.\n",
        "- `disable_statistics`: (optional) If True, do not invoke TFDV to compute pre-transform and post-transform statistics. When statistics are computed, they will will be stored in the pre_transform_feature_stats/ and post_transform_feature_stats/ subfolders of the transform_graph export. Defaults to False.\n",
        "- `stats_options_updater_fn`: (optional) The path to a python function that implements a 'stats_options_updater_fn'. See 'module_file' for expected signature of the function. 'stats_options_updater_fn' cannot be defined if 'module_file' is specified. Defaults to None.\n",
        "\n",
        "The output from this call is a compiled component that can be executed in the context of a pipeline.\n",
        "\n",
        "Learn more about [Transform](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/components/Transform)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98cd5003d3e0"
      },
      "outputs": [],
      "source": [
        "TRANSFORM_MODULE = \"custom/transform.py\"  # implements preprocessing_fn\n",
        "\n",
        "transform = Transform(\n",
        "    examples=example_gen.outputs[\"examples\"],\n",
        "    schema=schema_gen.outputs[\"schema\"],\n",
        "    module_file=TRANSFORM_MODULE,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eed3462a2ff"
      },
      "source": [
        "#### Write the transform module\n",
        "\n",
        "Next, write the transform module for preprocessing the examples from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d271daa422f"
      },
      "outputs": [],
      "source": [
        "%%writefile custom/transform.py\n",
        "import tensorflow as tf\n",
        "\n",
        "_IMAGE_KEY = 'image'\n",
        "_LABEL_KEY = 'label'\n",
        "\n",
        "def _transformed_name(key):\n",
        "    # This makes it easier for continuous training since mobilenet's input layer is input_1\n",
        "    if key == _IMAGE_KEY:\n",
        "        return 'input_1'\n",
        "    else:\n",
        "        return key + '_xf'\n",
        "\n",
        "\n",
        "# TFX Transform will call this function.\n",
        "def preprocessing_fn(inputs):\n",
        "    \"\"\"tf.transform's callback function for preprocessing inputs.\n",
        "\n",
        "      Args:\n",
        "        inputs: map from feature keys to raw not-yet-transformed features.\n",
        "\n",
        "      Returns:\n",
        "        Map from string feature key to transformed feature operations.\n",
        "    \"\"\"\n",
        "    outputs = {}\n",
        "\n",
        "    # tf.io.decode_png function cannot be applied on a batch of data.\n",
        "    # We have to use tf.map_fn\n",
        "    image_features = tf.map_fn(\n",
        "        lambda x: tf.io.decode_png(x[0], channels=3),\n",
        "        inputs[_IMAGE_KEY],\n",
        "        dtype=tf.uint8\n",
        "    )\n",
        "    image_features = tf.image.resize(image_features, [32, 32])\n",
        "    image_features = tf.keras.applications.mobilenet.preprocess_input(image_features)\n",
        "\n",
        "    outputs[_transformed_name(_IMAGE_KEY)] = image_features\n",
        "    # Do not apply label transformation as it will result in wrong evaluation.\n",
        "    outputs[_transformed_name(_LABEL_KEY)] = inputs[_LABEL_KEY]\n",
        "\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33034d5b9250"
      },
      "source": [
        "### Overview of the Tuner component\n",
        "\n",
        "The `Tuner` component tunes the hyperparameters for the model.\n",
        "\n",
        "* Consumes:\n",
        "\n",
        "    - `tf.Examples` used for training and evaluation.\n",
        "    - A user provided module file (or module fn) that defines the tuning logic, including model definition, hyperparameter search space, objective, etc.\n",
        "    - Protobuf definition of training and evaluation arguments.\n",
        "    - (Optional) Protobuf definition of tuning arguments.\n",
        "    - (Optional) Transform graph produced by an upstream Transform component.\n",
        "    - (Optional) A data schema created by a SchemaGen pipeline component and optionally altered by the developer.\n",
        "\n",
        "* Emits: the best hyperparameter results artifact.\n",
        "\n",
        "In this example, you call `Tuner()` with the following parameters:\n",
        "\n",
        "- `examples`: The training/evaluation examples from `ExampleGen`.\n",
        "- `module_file`: (optional) A path to python module file containing UDF tuner definition. The module_file must implement a function named tuner_fn at its top level. The function must have the following signature. def tuner_fn(fn_args: FnArgs) -> TunerFnResult: Exactly one of 'module_file' or `tuner_fn` must be supplied.\n",
        "- `transform_graph`: The input transform graph if present. This is used when transformed examples are provided.\n",
        "- `train_args`: (optional) A trainer_pb2.TrainArgs instance, containing args used for training. Currently only splits and num_steps are available. Default behavior (when splits is empty) is train on train split.\n",
        "- `eval_args`: (optional) A trainer_pb2.EvalArgs instance, containing args used for eval. Currently only splits and num_steps are available. Default behavior (when splits is empty) is evaluate on eval split.\n",
        "\n",
        "Additional parameters you may set:\n",
        "\n",
        "- `schema`: (optional) The schema for the training and evaluation data. This is used when raw examples are provided.\n",
        "- `base_model`: (optional) The model that will be used for training. This can be used for warmstart, transfer learning or model ensembling.\n",
        "- `tuner_fn`: (optional) A python path to UDF model definition function. See `module_file` for the required signature of the UDF. Exactly one of `module_file` or `tuner_fn` must be supplied.\n",
        "- `tune_args`: (optional) A `trainer_pb2.TrainArgs` instance, containing args used for training. Currently only splits and num_steps are available. Default behavior (when splits is empty) is train on train split.\n",
        "- `custom_config`: (optional) A dictionary which contains addtional training job parameters that will be passed into user module.\n",
        "\n",
        "The output from this call is a compiled component that can be executed in the context of a pipeline.\n",
        "\n",
        "Learn more about [Tuner](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/components/Tuner)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cbd1bc0a01b"
      },
      "outputs": [],
      "source": [
        "from tfx.proto import trainer_pb2\n",
        "\n",
        "TUNER_MODULE = \"custom/tuner.py\"  # implements tuner_fn\n",
        "\n",
        "tuner = Tuner(\n",
        "    module_file=TUNER_MODULE,\n",
        "    examples=transform.outputs[\"transformed_examples\"],\n",
        "    transform_graph=transform.outputs[\"transform_graph\"],\n",
        "    train_args=trainer_pb2.TrainArgs(num_steps=20),\n",
        "    eval_args=trainer_pb2.EvalArgs(num_steps=5),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25989f83d0fd"
      },
      "source": [
        "#### Write the tuner module\n",
        "\n",
        "Next, write the tuner module for hyperparameter tuning the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f64ab38b0b87"
      },
      "outputs": [],
      "source": [
        "%%writefile custom/tuner.py\n",
        "\n",
        "# not implemented in this tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aef586c464af"
      },
      "source": [
        "### Overview of the Trainer component\n",
        "\n",
        "The `Trainer` TFX pipeline component trains a TensorFlow model.\n",
        "\n",
        "* Consumes:\n",
        "\n",
        "    - `tf.Examples` used for training and eval\n",
        "    - A user provided module file that defines the trainer logic\n",
        "    - Protobuf definition of train args and eval args\n",
        "    - (Optional) A data schema created by a SchemaGen pipeline component\n",
        "    - (Optional) Transform graph produced by an upstream Transform component\n",
        "    - (Optional) Pre-trained models used for scenarios such as warmstart\n",
        "    - (Optional) Hyperparameters, which will be passed to use module function.\n",
        "\n",
        "* Emits: At least one model for inference/serving (typically in SavedModelFormat) and optionally another model for eval (typlically an EvalSavedModel)\n",
        "\n",
        "In this example, you call `Trainer()` with the following parameters:\n",
        "\n",
        "- `module_file`: (optional) The file path to a python module file, from which the `run_fn` function will be loaded. Exactly one of `module_file` or `run_fn` must be supplied.\n",
        "- `examples`: (optional) The source of examples used in training (required). May be raw or transformed.\n",
        "- `transform_graph`: (optional) The input transform graph if present.\n",
        "- `schema`: (optional) The schema for training and evaluation data.\n",
        "- `train_args`: (optional) A `proto.TrainArgs` instance, containing args used for training Currently only splits and num_steps are available. Default behavior (when splits is empty) is train on train split.\n",
        "- `eval_args`: (optional) A `proto.EvalArgs` instance, containing args used for evaluation. Currently only splits and num_steps are available. Default behavior (when splits is empty) is evaluate on eval split.\n",
        "\n",
        "Additional parameters you may set:\n",
        "\n",
        "- `run_fn`: (optional) A python path to UDF model definition function for generic trainer. See `module_file` for details. Exactly one of `module_file` or `run_fn` must be supplied if Trainer uses GenericExecutor (default).\n",
        "- `trainer_fn`: (optional) A python path to UDF model definition function for estimator based trainer. See `module_file` for the required signature of the UDF. Exactly one of `module_file` or `trainer_fn` must be supplied if `Trainer` uses Estimator based Executor.\n",
        "- `hyperparameters`: (optional) The hyperparameters for training module. \n",
        "- `base_model`: (optional) The model that will be used for training. This can be used for warmstart, transfer learning or model ensembling.\n",
        "- `custom_config`: (optional) A dict which contains addtional training job parameters that will be passed into user module.\n",
        "\n",
        "The output from this call is a compiled component that can be executed in the context of a pipeline.\n",
        "\n",
        "Learn more about [Trainer](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/components/Trainer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cca0d08fe749"
      },
      "outputs": [],
      "source": [
        "TRAINER_MODULE = \"custom/train.py\"  # implements run_fn\n",
        "\n",
        "trainer = Trainer(\n",
        "    module_file=TRAINER_MODULE,\n",
        "    examples=transform.outputs[\"transformed_examples\"],\n",
        "    transform_graph=transform.outputs[\"transform_graph\"],\n",
        "    schema=schema_gen.outputs[\"schema\"],\n",
        "    # not implemented in this tutorial:\n",
        "    # hyperparameters=tuner.outputs['best_hyperparameters'],\n",
        "    # This will be passed to `run_fn`.\n",
        "    train_args=trainer_pb2.TrainArgs(num_steps=100),\n",
        "    eval_args=trainer_pb2.EvalArgs(num_steps=5),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8acedc7a4939"
      },
      "source": [
        "#### Write the trainer module\n",
        "\n",
        "Next, write the trainer module for training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52eef6ce61a1"
      },
      "outputs": [],
      "source": [
        "%%writefile custom/train.py\n",
        "import os\n",
        "from typing import List, Text\n",
        "import absl\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "from tensorflow.keras.regularizers import L2\n",
        "\n",
        "from tfx.components.trainer.fn_args_utils import DataAccessor\n",
        "from tfx.components.trainer.fn_args_utils import FnArgs\n",
        "from tfx.components.trainer.rewriting import converters\n",
        "from tfx.components.trainer.rewriting import rewriter\n",
        "from tfx.components.trainer.rewriting import rewriter_factory\n",
        "from tfx.dsl.io import fileio\n",
        "from tfx_bsl.tfxio import dataset_options\n",
        "\n",
        "# When training on the whole dataset use following constants instead.\n",
        "# This setting should give ~91% accuracy on the whole test set\n",
        "# _TRAIN_DATA_SIZE = 50000\n",
        "# _EVAL_DATA_SIZE = 10000\n",
        "# _TRAIN_BATCH_SIZE = 64\n",
        "# _EVAL_BATCH_SIZE = 64\n",
        "# _CLASSIFIER_LEARNING_RATE = 3e-4\n",
        "# _FINETUNE_LEARNING_RATE = 5e-5\n",
        "# _CLASSIFIER_EPOCHS = 12\n",
        "\n",
        "_TRAIN_DATA_SIZE = 1024\n",
        "_EVAL_DATA_SIZE = 1024\n",
        "_TRAIN_BATCH_SIZE = 32\n",
        "_EVAL_BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-3\n",
        "FINETUNE_LEARNING_RATE = 7e-6\n",
        "EPOCHS = 30\n",
        "\n",
        "\n",
        "_IMAGE_KEY = 'image'\n",
        "_LABEL_KEY = 'label'\n",
        "\n",
        "def _transformed_name(key):\n",
        "    # This makes it easier for continuous training since mobilenet's input layer is input_1\n",
        "    if key == _IMAGE_KEY:\n",
        "        return 'input_1'\n",
        "    else:\n",
        "        return key + '_xf'\n",
        "    \n",
        "def _get_serve_image_fn(model, tf_transform_output):\n",
        "  \"\"\"Returns a function that feeds the input tensor into the model.\"\"\"\n",
        "  model.tft_layer = tf_transform_output.transform_features_layer()\n",
        "  @tf.function\n",
        "  def serve_image_fn(serialized_tf_examples):\n",
        "\n",
        "    feature_spec = tf_transform_output.raw_feature_spec()\n",
        "    feature_spec.pop(_LABEL_KEY)\n",
        "    parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
        "\n",
        "    transformed_features = model.tft_layer(parsed_features)\n",
        "    #del transformed_features[_transformed_name(_IMAGE_KEY)]\n",
        "\n",
        "    return model(transformed_features)\n",
        "\n",
        "  return serve_image_fn\n",
        "\n",
        "\n",
        "def _image_augmentation(image_features):\n",
        "  \"\"\"Perform image augmentation on batches of images .\n",
        "\n",
        "  Args:\n",
        "    image_features: a batch of image features\n",
        "\n",
        "  Returns:\n",
        "    The augmented image features\n",
        "  \"\"\"\n",
        "  batch_size = tf.shape(image_features)[0]\n",
        "  image_features = tf.image.random_flip_left_right(image_features)\n",
        "  image_features = tf.image.resize_with_crop_or_pad(image_features, 36, 36)\n",
        "  image_features = tf.image.random_crop(image_features,\n",
        "                                        (batch_size, 32, 32, 3))\n",
        "  return image_features\n",
        "\n",
        "\n",
        "def _data_augmentation(feature_dict):\n",
        "  \"\"\"Perform data augmentation on batches of data.\n",
        "\n",
        "  Args:\n",
        "    feature_dict: a dict containing features of samples\n",
        "\n",
        "  Returns:\n",
        "    The feature dict with augmented features\n",
        "  \"\"\"\n",
        "  image_features = feature_dict[_transformed_name(_IMAGE_KEY)]\n",
        "  image_features = _image_augmentation(image_features)\n",
        "  feature_dict[_transformed_name(_IMAGE_KEY)] = image_features\n",
        "  return feature_dict\n",
        "\n",
        "\n",
        "def _input_fn(file_pattern: List[Text],\n",
        "              data_accessor: DataAccessor,\n",
        "              tf_transform_output: tft.TFTransformOutput,\n",
        "              is_train: bool = False,\n",
        "              batch_size: int = 200) -> tf.data.Dataset:\n",
        "  \"\"\"Generates features and label for tuning/training.\n",
        "\n",
        "  Args:\n",
        "    file_pattern: List of paths or patterns of input tfrecord files.\n",
        "    data_accessor: DataAccessor for converting input to RecordBatch.\n",
        "    tf_transform_output: A TFTransformOutput.\n",
        "    is_train: Whether the input dataset is train split or not.\n",
        "    batch_size: representing the number of consecutive elements of returned\n",
        "      dataset to combine in a single batch\n",
        "\n",
        "  Returns:\n",
        "    A dataset that contains (features, indices) tuple where features is a\n",
        "      dictionary of Tensors, and indices is a single Tensor of label indices.\n",
        "  \"\"\"\n",
        "  dataset = data_accessor.tf_dataset_factory(\n",
        "      file_pattern,\n",
        "      dataset_options.TensorFlowDatasetOptions(\n",
        "          batch_size=batch_size, label_key=_transformed_name(_LABEL_KEY)),\n",
        "      tf_transform_output.transformed_metadata.schema)\n",
        "  # Apply data augmentation. We have to do data augmentation here because\n",
        "  # we need to apply data agumentation on-the-fly during training. If we put\n",
        "  # it in Transform, it will only be applied once on the whole dataset, which\n",
        "  # will lose the point of data augmentation.\n",
        "  if is_train:\n",
        "    dataset = dataset.map(lambda x, y: (_data_augmentation(x), y))\n",
        "\n",
        "  return dataset\n",
        "\n",
        "\n",
        "def _build_keras_model() -> tf.keras.Model:\n",
        "  \"\"\"Creates a Image classification model with MobileNet backbone.\n",
        "\n",
        "  Returns:\n",
        "    The image classification Keras Model and the backbone MobileNet model\n",
        "  \"\"\"\n",
        "  # We create a MobileNet model with weights pre-trained on ImageNet.\n",
        "  # We remove the top classification layer of the MobileNet, which was\n",
        "  # used for classifying ImageNet objects. We will add our own classification\n",
        "  # layer for CIFAR10 later. We use average pooling at the last convolution\n",
        "  # layer to get a 1D vector for classification, which is consistent with the\n",
        "  # origin MobileNet setup\n",
        "  base_model = tf.keras.applications.MobileNet(\n",
        "      input_shape=(32, 32, 3),\n",
        "      include_top=False,\n",
        "      weights='imagenet',\n",
        "      pooling='avg')\n",
        "  base_model.input_spec = None\n",
        "\n",
        "  # freeze the layers of the base model\n",
        "  base_model.trainable = False\n",
        "\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.InputLayer(\n",
        "          input_shape=(32, 32, 3), name=_transformed_name(_IMAGE_KEY)),\n",
        "      base_model,\n",
        "      tf.keras.layers.Dense(10, activation='softmax', kernel_regularizer=L2(0.001))\n",
        "  ])\n",
        "\n",
        "  model.compile(\n",
        "      loss='sparse_categorical_crossentropy',\n",
        "      optimizer=tf.keras.optimizers.RMSprop(lr=LEARNING_RATE),\n",
        "      metrics=['sparse_categorical_accuracy'])\n",
        "  model.summary(print_fn=absl.logging.info)\n",
        "\n",
        "  return model, base_model\n",
        "\n",
        "# TFX Trainer will call this function.\n",
        "def run_fn(fn_args: FnArgs):\n",
        "    \"\"\"Train the model based on given args.\n",
        "\n",
        "    Args:\n",
        "    fn_args: Holds args used to train the model as name/value pairs.\n",
        "\n",
        "    Raises:\n",
        "    ValueError: if invalid inputs.\n",
        "    \"\"\"\n",
        "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
        "\n",
        "    strategy = tf.distribute.MirroredStrategy(devices=['device:GPU:0'])\n",
        "\n",
        "    baseline_path = fn_args.base_model\n",
        "    if baseline_path is not None:\n",
        "        with strategy.scope():\n",
        "            model = tf.keras.models.load_model(os.path.join(baseline_path))\n",
        "            # rename input layer to match transform name\n",
        "            #model.layers[0].layers[0]._name = _transformed_name(_IMAGE_KEY)\n",
        "    else:\n",
        "        with strategy.scope():\n",
        "            model, base_model = _build_keras_model()\n",
        "\n",
        "    train_dataset = _input_fn(\n",
        "        fn_args.train_files,\n",
        "        fn_args.data_accessor,\n",
        "        tf_transform_output,\n",
        "        is_train=True,\n",
        "        batch_size=_TRAIN_BATCH_SIZE)\n",
        "    eval_dataset = _input_fn(\n",
        "        fn_args.eval_files,\n",
        "        fn_args.data_accessor,\n",
        "        tf_transform_output,\n",
        "        is_train=False,\n",
        "        batch_size=_EVAL_BATCH_SIZE)\n",
        "\n",
        "    absl.logging.info('TensorBoard logging to {}'.format(fn_args.model_run_dir))\n",
        "    # Write logs to path\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "        log_dir=fn_args.model_run_dir, update_freq='batch')\n",
        "\n",
        "    # Our training regime has two phases: we first freeze the backbone and train\n",
        "    # the newly added classifier only, then unfreeze part of the backbone and\n",
        "    # fine-tune with classifier jointly.\n",
        "    steps_per_epoch = int(_TRAIN_DATA_SIZE / _TRAIN_BATCH_SIZE)\n",
        "\n",
        "    absl.logging.info('Start training the top classifier')\n",
        "    model.fit(\n",
        "        train_dataset,\n",
        "        epochs=EPOCHS,\n",
        "        steps_per_epoch=fn_args.train_steps, #steps_per_epoch,\n",
        "        validation_data=eval_dataset,\n",
        "        validation_steps=fn_args.eval_steps,\n",
        "        callbacks=[tensorboard_callback])\n",
        "\n",
        "    # only if first training, finetune.\n",
        "    if baseline_path is None:\n",
        "        absl.logging.info('Start fine-tuning the model')\n",
        "        # Unfreeze the top MobileNet layers and do joint fine-tuning\n",
        "        base_model.trainable = True\n",
        "\n",
        "        # We need to recompile the model because layer properties have changed\n",
        "        model.compile(\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            optimizer=tf.keras.optimizers.RMSprop(lr=FINETUNE_LEARNING_RATE),\n",
        "            metrics=['sparse_categorical_accuracy'])\n",
        "        model.summary(print_fn=absl.logging.info)\n",
        "\n",
        "        model.fit(\n",
        "            train_dataset,\n",
        "            initial_epoch=EPOCHS,\n",
        "            epochs=EPOCHS + 20,\n",
        "            steps_per_epoch=steps_per_epoch,\n",
        "            validation_data=eval_dataset,\n",
        "            validation_steps=fn_args.eval_steps,\n",
        "            callbacks=[tensorboard_callback])\n",
        "\n",
        "    signatures = {\n",
        "      'serving_default':\n",
        "          _get_serve_image_fn(model,tf_transform_output).get_concrete_function(\n",
        "              tf.TensorSpec(\n",
        "                  shape=[None],\n",
        "                  dtype=tf.string,\n",
        "                  name=_IMAGE_KEY))\n",
        "    }\n",
        "\n",
        "    temp_saving_model_dir = os.path.join(fn_args.serving_model_dir)\n",
        "    model.save(temp_saving_model_dir, save_format='tf', signatures=signatures)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35b30bdee44d"
      },
      "source": [
        "### Overview of the Evaluator component\n",
        "\n",
        "The Evaluator component performs deep analysis on the training results for your models, to help you understand how your model performs on subsets of your data. The Evaluator also helps you validate your exported models, ensuring that they are \"good enough\" to be pushed to production.\n",
        "\n",
        "* Consumes:\n",
        "\n",
        "    - An eval split from `ExampleGen`\n",
        "\n",
        "    - A trained model from `Trainer`\n",
        "\n",
        "    - (Optional) A previously blessed model\n",
        "\n",
        "* Emits:\n",
        "\n",
        "    - Analysis results to ML Metadata\n",
        "\n",
        "    - Validation results to ML Metadata\n",
        "\n",
        "In this example, you call `Evaluator()` with the following parameters:\n",
        "\n",
        "- `examples`: The source of examples used for evaluation (required).\n",
        "- `model`: (optional) The model produced by the `Trainer` component.\n",
        "- `eval_config`: (optional) Instance of tfma.EvalConfig containg configuration settings for running the evaluation. \n",
        "\n",
        "Additional parameters you may set:\n",
        "\n",
        "- `baseline_model`: (optional) The baseline model for model diff and model validation purpose.\n",
        "- `example_splits`: (optional) Names of splits on which the metrics are computed. Default behavior (when example_splits is set to None or Empty) is using the 'eval' split.\n",
        "- `schema`: (optional) The schema for TFXIO.\n",
        "- `module_file`: (optional) A path to python module file containing UDFs for Evaluator customization. \n",
        "- `module_path`: (optional) A python path to the custom module that contains the UDFs. See 'module_file' for the required signature of UDFs.\n",
        "\n",
        "The output from this call is a compiled component that can be executed in the context of a pipeline.\n",
        "\n",
        "Learn more about [Evaluator](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/components/Evaluator)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff87103d6411"
      },
      "outputs": [],
      "source": [
        "import tensorflow_model_analysis as tfma\n",
        "\n",
        "LOWER_BOUND_VALIDATION = 0.55  # the metric threshold to validate the model.\n",
        "\n",
        "eval_config = tfma.EvalConfig(\n",
        "    model_specs=[tfma.ModelSpec(label_key=\"label\")],\n",
        "    slicing_specs=[tfma.SlicingSpec()],\n",
        "    metrics_specs=[\n",
        "        tfma.MetricsSpec(\n",
        "            metrics=[\n",
        "                tfma.MetricConfig(\n",
        "                    class_name=\"SparseCategoricalAccuracy\",\n",
        "                    threshold=tfma.MetricThreshold(\n",
        "                        value_threshold=tfma.GenericValueThreshold(\n",
        "                            lower_bound={\"value\": LOWER_BOUND_VALIDATION}\n",
        "                        ),\n",
        "                        # Change threshold will be ignored if there is no\n",
        "                        # baseline model resolved from MLMD (first run).\n",
        "                        change_threshold=tfma.GenericChangeThreshold(\n",
        "                            direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
        "                            absolute={\"value\": -1e-3},\n",
        "                        ),\n",
        "                    ),\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "    ],\n",
        ")\n",
        "\n",
        "evaluator = Evaluator(\n",
        "    examples=example_gen.outputs[\"examples\"],\n",
        "    model=trainer.outputs[\"model\"],\n",
        "    # baseline_model=model_resolver.outputs['model'],\n",
        "    eval_config=eval_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c9f5ec19abc"
      },
      "source": [
        "### Overview of the InfraValidator component\n",
        "\n",
        "`InfraValidator` is a TFX component that is used as an early warning layer before pushing a model into production. The name \"infra\" validator came from the fact that it is validating the model in the actual model serving \"infrastructure\". If `Evaluator` is to guarantee the performance of the model, `InfraValidator` is to guarantee the model is mechanically fine and prevents bad models from being pushed.\n",
        "\n",
        "* Consumes: a trained model in SavedModel format from the `Trainer `component\n",
        "\n",
        "* Emits: infra validation result artifact\n",
        "\n",
        "In this example, you call `InfraValidator()` with the following parameters:\n",
        "\n",
        "- `model`: The model produced by the `Trainer` component.\n",
        "- `serving_spec`: A ServingSpec configuration about serving binary and test platform config to launch model server for validation. \n",
        "\n",
        "Additional parameters you may set:\n",
        "\n",
        "- `examples`: (optional) The source of examples used for validating the infrastructure.\n",
        "- `request_spec`: (optional)\n",
        "- `validation_spec`: (optional)\n",
        "\n",
        "The output from this call is a compiled component that can be executed in the context of a pipeline.\n",
        "\n",
        "Learn more about [InfraValidator](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/components/InfraValidator)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27d3f7213a2d"
      },
      "outputs": [],
      "source": [
        "# Not implemented in this tutorial\n",
        "try:\n",
        "    infra_validator = InfraValidator(\n",
        "        model=trainer.outputs[\"model\"], serving_spec=tfx.proto.ServingSpec(...)\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01f024bc41b3"
      },
      "source": [
        "### Overview of the Pusher component\n",
        "\n",
        "The `Pusher` component is used to push a validated model to a deployment target during model training or re-training. Before the deployment, Pusher relies on one or more blessings from other validation components to decide whether to push the model or not.\n",
        "\n",
        "* Consumes:\n",
        "\n",
        "A trained model from the `Trainer` component\n",
        "(Optional but recommended) InfraValidator blesses the model if the model is mechanically servable in a production environment\n",
        "\n",
        "* Emits: the same trained model along with versioning metadata\n",
        "\n",
        "In this example, you call `Pusher()` with the following parameters:\n",
        "\n",
        "- `model`: (optional) The model artifact from the `Trainer` component.\n",
        "- `model_blessing`: (optional) The model evaluation artifact from the `Evaluator` component.\n",
        "- `push_destination`: (optional) A pusher_pb2.PushDestination instance, providing info for tensorflow serving to load models.\n",
        "\n",
        "Additional parameters you may set:\n",
        "\n",
        "- `infra_blessing`: The infrastructure validation artifact from `InfraValidator`.\n",
        "- `custom_config`: A dictionary which contains the deployment job parameters to be passed to Cloud platforms.\n",
        "\n",
        "The output from this call is a compiled component that can be executed in the context of a pipeline.\n",
        "\n",
        "Learn more about [Pusher](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/components/Pusher)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77541d723882"
      },
      "outputs": [],
      "source": [
        "LOCAL_ROOT = \"./\"\n",
        "_pipeline_name = \"cifar10\"\n",
        "serving_model_dir_lite = os.path.join(LOCAL_ROOT, \"serving_model_lite\", _pipeline_name)\n",
        "\n",
        "\n",
        "pusher = Pusher(\n",
        "    model=trainer.outputs[\"model\"],\n",
        "    model_blessing=evaluator.outputs[\"blessing\"],\n",
        "    push_destination=pusher_pb2.PushDestination(\n",
        "        filesystem=pusher_pb2.PushDestination.Filesystem(\n",
        "            base_directory=serving_model_dir_lite\n",
        "        )\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb8455989226"
      },
      "source": [
        "### Overview of the BulkInferrer component\n",
        "\n",
        "The `BulkInferrer` TFX component performs batch inference on unlabeled data. The generated InferenceResult contains the original features and the prediction results.\n",
        "\n",
        "* Consumes:\n",
        "\n",
        "    - A trained model in SavedModel format\n",
        "    - Unlabelled tf.Examples that contain features\n",
        "    - (Optional) Validation results from Evaluator component\n",
        "\n",
        "\n",
        "* Emits: The inference (prediction) results\n",
        "\n",
        "In this example, you call `BulkInferrer()` with the following parameters:\n",
        "\n",
        "- `examples`: The examples for inference, usually from an instance of `ExampleGen`.\n",
        "- `model`: (optional) The model artifact from the `Trainer` component.\n",
        "- `model_blessing`: (optional) The model evaluation artifact from the `Evaluator` component.\n",
        "- `data_spec`: (optional) A `bulk_inferrer_pb2.DataSpec` instance that describes data selection.\n",
        "- `model_spec`: (optional) A `bulk_inferrer_pb2.ModelSpec` instance that describes model specification.\n",
        "\n",
        "Additional parameters you may set:\n",
        "\n",
        "- `output_example_spec`: (optional) A bulk_inferrer_pb2.OutputExampleSpec instance, specify if you want BulkInferrer to output examples instead of inference result.\n",
        "\n",
        "The output from this call is a compiled component that can be executed in the context of a pipeline.\n",
        "\n",
        "Learn more about [BulkInferrer](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/components/BulkInferrer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f23923f800dc"
      },
      "outputs": [],
      "source": [
        "bulk_inferrer = BulkInferrer(\n",
        "    examples=example_gen.outputs[\"examples\"],\n",
        "    model=trainer.outputs[\"model\"],\n",
        "    model_blessing=evaluator.outputs[\"blessing\"],\n",
        "    data_spec=bulk_inferrer_pb2.DataSpec(),\n",
        "    model_spec=bulk_inferrer_pb2.ModelSpec(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27f08fa3c1fa"
      },
      "source": [
        "## Define the TFX pipeline\n",
        "\n",
        "Next, you define the TFX pipeline. In this tutorial, the pipeline consists of the following steps:\n",
        "\n",
        "- `example_gen`: Generate tf.Examples for training and evaluation.\n",
        "- `statistics_gen`: Generate the dataset statistiscs artifact.\n",
        "- `schema_gen`: Generate the dataset schema artifact.\n",
        "- `example_validator`: Analyze the dataset examples for anomalies.\n",
        "- `transform`: Preprocess the dataset examples.\n",
        "- `trainer`: Train the model from the preprocessed dataset examples.\n",
        "- `evaluator`: Evaluate whether the model meets deployment criteria.\n",
        "- `pusher`: Deploy the model\n",
        "\n",
        "The pipeline DAG is defined (and compiled) by instantiating a `pipeline.Pipeline`, with the following parameters:\n",
        "\n",
        "- `pipeline_name`: The display name for the pipeline.\n",
        "- `pipeline_root`: The storage location for writing the artifacts from each pipeline step.\n",
        "- `components`: The ordered list of pipeline steps.\n",
        "- `enable_cache`: Whether to reuse cached results from previous pipeline run. Defaults to False.\n",
        "- `metadata_connection_config`: Settings for storing metadata information from the pipeline steps.\n",
        "- `beam_pipeline_args`: The parameters for the Apache Beam job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e63385b25e56"
      },
      "outputs": [],
      "source": [
        "LOCAL_ROOT = \"./\"\n",
        "\n",
        "_pipeline_name = \"cifar10\"\n",
        "_pipeline_root = f\"{LOCAL_ROOT}/pipelines/{_pipeline_name}\"\n",
        "_metadata_path = os.path.join(LOCAL_ROOT, \"metadata\", _pipeline_name, \"metadata.db\")\n",
        "\n",
        "_beam_pipeline_args = [\n",
        "    \"--direct_running_mode=multi_processing\",\n",
        "    \"--direct_num_workers=0\",\n",
        "]\n",
        "\n",
        "\n",
        "components = [\n",
        "    example_gen,\n",
        "    statistics_gen,\n",
        "    schema_gen,\n",
        "    example_validator,\n",
        "    transform,\n",
        "    trainer,\n",
        "    evaluator,\n",
        "    pusher,\n",
        "]\n",
        "\n",
        "tfx_pipeline = pipeline.Pipeline(\n",
        "    pipeline_name=_pipeline_name,\n",
        "    pipeline_root=_pipeline_root,\n",
        "    components=components,\n",
        "    enable_cache=True,\n",
        "    metadata_connection_config=metadata.sqlite_metadata_connection_config(\n",
        "        _metadata_path\n",
        "    ),\n",
        "    beam_pipeline_args=_beam_pipeline_args,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a91272c2e69"
      },
      "source": [
        "### Execute the TFX pipeline\n",
        "\n",
        "Next, you execute the pipeline. In this tutorial, you use the `Apahe BeamDagRunner` to execute the TFX pipeline DAG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02d24161019e"
      },
      "outputs": [],
      "source": [
        "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
        "\n",
        "BeamDagRunner().run(tfx_pipeline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da680575c5f8"
      },
      "source": [
        "#### Visualizing artifact output from ImportExampleGen\n",
        "\n",
        "Next, you view the output from `ImportExampleGen`, which consists of compressed archived files of the image data from the CIFAR-10 dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9afbe7f861ee"
      },
      "outputs": [],
      "source": [
        "path = f\"{_pipeline_root}/ImportExampleGen/examples\"\n",
        "examples_run = os.path.join(path, [f for f in listdir(path)][0])\n",
        "train_examples_filepath = os.path.join(examples_run, \"Split-train\")\n",
        "eval_examples_filepath = os.path.join(examples_run, \"Split-eval\")\n",
        "\n",
        "print(\"Training TFRecords\")\n",
        "! ls {train_examples_filepath}\n",
        "print(\"Evaluation TFRecords\")\n",
        "! ls {eval_examples_filepath}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9669e5d95752"
      },
      "source": [
        "#### Visualizing artifact output from StatisticsGen\n",
        "\n",
        "You can visualize some of the StatisticsGen Artifact and compare our train and eval datasets. This visualization tool is very useful to give us insights about the data such as data distribution and possible data skew."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1c660799aa86"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from os import listdir\n",
        "\n",
        "import tensorflow_data_validation as tfdv\n",
        "\n",
        "path = f\"{_pipeline_root}/StatisticsGen/statistics/\"\n",
        "stats_run = os.path.join(path, [f for f in listdir(path)][0])\n",
        "train_stats_filepath = os.path.join(stats_run, \"Split-train\", \"FeatureStats.pb\")\n",
        "eval_stats_filepath = os.path.join(stats_run, \"Split-eval\", \"FeatureStats.pb\")\n",
        "\n",
        "train_stats = tfdv.load_stats_binary(train_stats_filepath)\n",
        "eval_stats = tfdv.load_stats_binary(eval_stats_filepath)\n",
        "\n",
        "tfdv.visualize_statistics(\n",
        "    train_stats, rhs_statistics=eval_stats, lhs_name=\"train\", rhs_name=\"eval\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f7d13aeaad3"
      },
      "source": [
        "#### Visualizing artifact output from SchemaGen\n",
        "\n",
        "Next, you view the output from `SchemaGen`, which is in protobuf format stored as a plain text file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50a108bffbe1"
      },
      "outputs": [],
      "source": [
        "path = f\"{_pipeline_root}/SchemaGen/schema/\"\n",
        "schema_run = os.path.join(path, [f for f in listdir(path)][0])\n",
        "\n",
        "! cat {schema_run}/schema.pbtxt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd86f4568431"
      },
      "source": [
        "#### Visualizing artifact output from ExampleValidator\n",
        "\n",
        "Next, you view the output from `ExampleValidator`, which is in protobuf format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a652731d39a4"
      },
      "outputs": [],
      "source": [
        "path = f\"{_pipeline_root}/ExampleValidator/anomalies/\"\n",
        "example_validator_run = os.path.join(path, [f for f in listdir(path)][0])\n",
        "\n",
        "train_example_validator_filepath = os.path.join(\n",
        "    example_validator_run, \"Split-train\", \"SchemaDiff.pb\"\n",
        ")\n",
        "eval_example_validator_filepath = os.path.join(\n",
        "    example_validator_run, \"Split-eval\", \"SchemaDiff.pb\"\n",
        ")\n",
        "\n",
        "! ls {train_example_validator_filepath}\n",
        "! ls {eval_example_validator_filepath}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80f92a721248"
      },
      "source": [
        "#### Visualizing artifact output from Transform\n",
        "\n",
        "Next, you view the output from `Transform`, which contains the following subfolders:\n",
        "\n",
        "- `pre_transform_schema`\n",
        "- `pre_transform_stats`\n",
        "- `post_transform_anomalies`\n",
        "- `post_transform_schema`\n",
        "- `post_transform_stats`\n",
        "- `transform_graph`\n",
        "- `transform_examples`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c6554279b23"
      },
      "outputs": [],
      "source": [
        "path = f\"{_pipeline_root}/Transform/transform_graph/\"\n",
        "transform_graph_run = os.path.join(path, [f for f in listdir(path)][0])\n",
        "\n",
        "print(\"Transform Graph\")\n",
        "! ls {transform_graph_run}\n",
        "\n",
        "path = f\"{_pipeline_root}/Transform/transformed_examples/\"\n",
        "transformed_examples_run = os.path.join(path, [f for f in listdir(path)][0])\n",
        "\n",
        "transformed_examples_train_filepath = os.path.join(\n",
        "    transformed_examples_run, \"Split-train\"\n",
        ")\n",
        "transformed_examples_eval_filepath = os.path.join(\n",
        "    transformed_examples_run, \"Split-eval\"\n",
        ")\n",
        "\n",
        "print(\"Transformed Examples\")\n",
        "! ls {transformed_examples_train_filepath} {transformed_examples_eval_filepath}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89b3383908ef"
      },
      "source": [
        "#### Visualizing artifact output from Trainer\n",
        "\n",
        "Next, you view the output from `Trainer`, whichn consists of the trained model artifacts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6f8bdc20ae0"
      },
      "outputs": [],
      "source": [
        "path = f\"{_pipeline_root}/Trainer/model/\"\n",
        "trainer_run = os.path.join(path, [f for f in listdir(path)][0])\n",
        "\n",
        "model_artifacts = os.path.join(trainer_run, \"Format-Serving\")\n",
        "\n",
        "! ls {model_artifacts}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "199dedb59424"
      },
      "source": [
        "#### Visualizing artifact output from Evaluator\n",
        "\n",
        "Next, you view the output from `Evaluator`, which consists of two parts: `evaluation` and `blessing`.\n",
        "\n",
        "The `evaluation` consists of the validation results stored as TFRecords.\n",
        "\n",
        "The `blessing` consists of a single (empty) plain text file, whose name is either:\n",
        "\n",
        "- `BLESSED`: Model passed the validation requirements.\n",
        "- `NOT_BLESSED`: Model did not pass the validation requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ea07943d1c98"
      },
      "outputs": [],
      "source": [
        "path = f\"{_pipeline_root}/Evaluator/evaluation/\"\n",
        "evaluation_run = os.path.join(path, [f for f in listdir(path)][0])\n",
        "\n",
        "path = f\"{_pipeline_root}/Evaluator/blessing/\"\n",
        "blessing_run = os.path.join(path, [f for f in listdir(path)][0])\n",
        "\n",
        "print(\"Evaluation\")\n",
        "! ls {evaluation_run}\n",
        "print(\"Blessing\")\n",
        "! ls {blessing_run}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc6767513fc1"
      },
      "source": [
        "#### Visualizing artifact output from Pusher\n",
        "\n",
        "Next, you view the output from `Pusher`. If the model passed evaluation, then the output will contain the pushed model artifacts:\n",
        "\n",
        "- `saved_model.pb`\n",
        "- `keras_metadata.pb`\n",
        "- `assets`\n",
        "- `variables`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "241f31420b20"
      },
      "outputs": [],
      "source": [
        "path = f\"{_pipeline_root}/Pusher/pushed_model/\"\n",
        "pusher_run = os.path.join(path, [f for f in listdir(path)][0])\n",
        "\n",
        "! ls {pusher_run}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94ef2fe2809f"
      },
      "source": [
        "#### Delete local temporary files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35b44dcae42c"
      },
      "outputs": [],
      "source": [
        "! rm -rf metadata pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2191ae638a10"
      },
      "source": [
        "## Running the TFX pipeline as a `Vertex AI CustomJob`\n",
        "\n",
        "In this section of the tutorial, you run your pre-existing TFX pipeline in Google Cloud using the `Vertex AI CustomJob`. The `CustomJob` service provides you the ability to run any Python package on Google Cloud, and be able to track the job under **Training->Custom Jobs**.\n",
        "\n",
        "You perform the following steps:\n",
        "\n",
        "1. Place the TFX pipeline code into a Python script.\n",
        "2. Create a Python package containing:\n",
        "    - Python scripts (TFX pipeline, transform/trainer/tune scripts)\n",
        "    - setup and requirements\n",
        "    - Compress and tar the package and copy to your Cloud Storage bucket.\n",
        "3. Copy the training data to your Cloud Storage bucket.\n",
        "4. Setup the worker pool specification.\n",
        "5. Create and run the `CustomJob`.\n",
        "\n",
        "### Recommendation\n",
        "\n",
        "Using a `CustomJob` for executing TFX pipelines on Google Cloud is recommended for smaller jobs -- vs. converting to and executing as a `Vertex AI Pipeline`.\n",
        "\n",
        "The way Vertex AI pipelines allocates resources is not well-suited for small jobs. When running on Vertex AI pipelines, you are running dataflow jobs for ingesting data, creating statistics, schema and transformations, and running training jobs as custom training jobs in Vertex AI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a355dd0a9f60"
      },
      "source": [
        "### Write the TFX pipeline as a Python script\n",
        "\n",
        "Next, you write your TFX pipeline code as a Python script -- which will be ran by the `CustomJob`.\n",
        "\n",
        "*Note:* sqlite3 cannot open a database on a GCS style path (i.e., gs://). You use GCSFuse to make the GCS bucket appear as a network mounted filesystem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b45f0544ed5"
      },
      "outputs": [],
      "source": [
        "! mkdir custom/trainer\n",
        "! gsutil cp custom/transform.py {BUCKET_URI}/transform.py\n",
        "! gsutil cp custom/train.py {BUCKET_URI}/train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8286c0c99143"
      },
      "outputs": [],
      "source": [
        "content = f\"\"\"\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from tfx import v1 as tfx\n",
        "from tfx.components import (ImportExampleGen, \n",
        "                            StatisticsGen, \n",
        "                            SchemaGen, \n",
        "                            ExampleValidator, \n",
        "                            Transform,\n",
        "                            Trainer,\n",
        "                            Tuner,\n",
        "                            Evaluator,\n",
        "                            InfraValidator,\n",
        "                            Pusher,\n",
        "                            BulkInferrer\n",
        "                           )\n",
        "from tfx.dsl.components.common import resolver\n",
        "from tfx.orchestration import pipeline\n",
        "from tfx.orchestration import metadata\n",
        "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
        "\n",
        "from tfx.proto import example_gen_pb2\n",
        "from tfx.proto import trainer_pb2\n",
        "from tfx.proto import pusher_pb2\n",
        "from tfx.proto import bulk_inferrer_pb2\n",
        "\n",
        "import tensorflow_model_analysis as tfma\n",
        "\n",
        "DATA_ROOT = \"{BUCKET_URI}/data\"\n",
        "LOCAL_ROOT = \"{BUCKET_URI}\"\n",
        "\n",
        "input_config = example_gen_pb2.Input(splits=[\n",
        "    example_gen_pb2.Input.Split(name='train', pattern='train/*'),\n",
        "    example_gen_pb2.Input.Split(name='eval', pattern='test/*')\n",
        "])\n",
        "\n",
        "example_gen = ImportExampleGen(\n",
        "    input_base=DATA_ROOT, \n",
        "    input_config=input_config\n",
        ")\n",
        "\n",
        "statistics_gen = StatisticsGen(\n",
        "    examples=example_gen.outputs['examples']\n",
        ")\n",
        "\n",
        "schema_gen = SchemaGen(\n",
        "    statistics=statistics_gen.outputs['statistics'], \n",
        "    infer_feature_shape=True\n",
        ")\n",
        "\n",
        "example_validator = ExampleValidator(\n",
        "      statistics=statistics_gen.outputs['statistics'],\n",
        "      schema=schema_gen.outputs['schema']\n",
        ")\n",
        "\n",
        "TRANSFORM_MODULE = '{BUCKET_URI}/transform.py'  # implements preprocessing_fn\n",
        "\n",
        "transform = Transform(\n",
        "      examples=example_gen.outputs['examples'],\n",
        "      schema=schema_gen.outputs['schema'],\n",
        "      module_file=TRANSFORM_MODULE\n",
        ")\n",
        "\n",
        "TRAINER_MODULE = '{BUCKET_URI}/train.py'  # implements run_fn\n",
        "\n",
        "trainer = Trainer(\n",
        "    module_file=TRAINER_MODULE,   \n",
        "    examples=transform.outputs['transformed_examples'],\n",
        "    transform_graph=transform.outputs['transform_graph'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    \n",
        "    # not implemented in this tutorial: \n",
        "    # hyperparameters=tuner.outputs['best_hyperparameters'],\n",
        "    \n",
        "    # This will be passed to `run_fn`.\n",
        "    train_args=trainer_pb2.TrainArgs(num_steps=100),\n",
        "    eval_args=trainer_pb2.EvalArgs(num_steps=5)\n",
        ")\n",
        "\n",
        "LOWER_BOUND_VALIDATION = 0.55  # the metric threshold to validate the model.\n",
        "\n",
        "eval_config = tfma.EvalConfig(\n",
        "      model_specs=[tfma.ModelSpec(label_key='label')],\n",
        "      slicing_specs=[tfma.SlicingSpec()],\n",
        "      metrics_specs=[\n",
        "          tfma.MetricsSpec(metrics=[\n",
        "              tfma.MetricConfig(\n",
        "                  class_name='SparseCategoricalAccuracy',\n",
        "                  threshold=tfma.MetricThreshold(\n",
        "                      value_threshold=tfma.GenericValueThreshold(\n",
        "                          lower_bound={{'value': LOWER_BOUND_VALIDATION}}),\n",
        "                      # Change threshold will be ignored if there is no\n",
        "                      # baseline model resolved from MLMD (first run).\n",
        "                      change_threshold=tfma.GenericChangeThreshold(\n",
        "                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
        "                          absolute={{'value': -1e-3}})))\n",
        "          ])\n",
        "      ])\n",
        "\n",
        "evaluator = Evaluator(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    model=trainer.outputs['model'],\n",
        "    #baseline_model=model_resolver.outputs['model'],\n",
        "    eval_config=eval_config\n",
        ")\n",
        "\n",
        "_pipeline_name = 'cifar10'\n",
        "serving_model_dir_lite = os.path.join(LOCAL_ROOT, 'serving_model_lite', _pipeline_name)\n",
        "\n",
        "gs_prefix = 'gs://'\n",
        "gcsfuse_prefix = '/gcs/'\n",
        "if serving_model_dir_lite.startswith(gs_prefix):\n",
        "    serving_model_dir_lite = serving_model_dir_lite.replace(gs_prefix, gcsfuse_prefix)\n",
        "    dirpath = os.path.split(serving_model_dir_lite)[0]\n",
        "    if not os.path.isdir(dirpath):\n",
        "        os.makedirs(dirpath)\n",
        "\n",
        "\n",
        "pusher = Pusher(\n",
        "    model=trainer.outputs['model'],\n",
        "    model_blessing=evaluator.outputs['blessing'],\n",
        "    push_destination=pusher_pb2.PushDestination(\n",
        "        filesystem=pusher_pb2.PushDestination.Filesystem(\n",
        "            base_directory=serving_model_dir_lite\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "_pipeline_root = LOCAL_ROOT + \"/pipelines/\" + _pipeline_name\n",
        "_metadata_path = os.path.join(LOCAL_ROOT, 'metadata', _pipeline_name, 'metadata.db')\n",
        "\n",
        "if _metadata_path.startswith(gs_prefix):\n",
        "    _metadata_path = _metadata_path.replace(gs_prefix, gcsfuse_prefix)\n",
        "    dirpath = os.path.split(_metadata_path)[0]\n",
        "    if not os.path.isdir(dirpath):\n",
        "        os.makedirs(dirpath)\n",
        "if _pipeline_root.startswith(gs_prefix):\n",
        "    _pipeline_root = _pipeline_root.replace(gs_prefix, gcsfuse_prefix)\n",
        "    dirpath = os.path.split(_pipeline_root)[0]\n",
        "    if not os.path.isdir(dirpath):\n",
        "        os.makedirs(dirpath)\n",
        "\n",
        "_beam_pipeline_args = [\n",
        "    '--direct_running_mode=multi_processing',\n",
        "    '--direct_num_workers=0',\n",
        "]\n",
        "\n",
        "\n",
        "components = [\n",
        "    example_gen, \n",
        "    statistics_gen, \n",
        "    schema_gen,\n",
        "    example_validator,\n",
        "    transform,\n",
        "    trainer,\n",
        "    evaluator,\n",
        "    pusher\n",
        "]\n",
        "\n",
        "pipeline = pipeline.Pipeline(\n",
        "    pipeline_name=_pipeline_name,\n",
        "    pipeline_root=_pipeline_root,\n",
        "    components=components,\n",
        "    enable_cache=False,\n",
        "    metadata_connection_config=metadata.sqlite_metadata_connection_config(_metadata_path),\n",
        "    beam_pipeline_args=_beam_pipeline_args\n",
        ")\n",
        "\n",
        "BeamDagRunner().run(pipeline)\n",
        "\"\"\"\n",
        "\n",
        "with open(\"custom/trainer/tfx_pipeline.py\", \"w\") as f:\n",
        "    f.write(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "examine_training_package"
      },
      "source": [
        "### Examine the TFX pipeline package\n",
        "\n",
        "#### Package layout\n",
        "\n",
        "Before you start the custom job for your TFX pipeline, you will look at how a Python package is assembled for a custom job. When unarchived, the package contains the following directory/file layout.\n",
        "\n",
        "- PKG-INFO\n",
        "- README.md\n",
        "- setup.cfg\n",
        "- setup.py\n",
        "- trainer\n",
        "  - \\_\\_init\\_\\_.py\n",
        "  - tfx_pipeline.py\n",
        "  - transform.py\n",
        "  - tuner.py\n",
        "  - train.py\n",
        "\n",
        "The files `setup.cfg` and `setup.py` are the instructions for installing the package into the operating environment of the Docker image.\n",
        "\n",
        "The file `trainer/task.py` is the Python script for executing the custom hyperparameter tuning job. *Note*, when we referred to it in the worker pool specification, we replace the directory slash with a dot (`trainer.task`) and dropped the file suffix (`.py`).\n",
        "\n",
        "#### Package Assembly\n",
        "\n",
        "In the following cells, you will assemble the TFX pipeline package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "examine_training_package"
      },
      "outputs": [],
      "source": [
        "# Add package information\n",
        "! touch custom/README.md\n",
        "\n",
        "setup_cfg = \"[egg_info]\\n\\ntag_build =\\n\\ntag_date = 0\"\n",
        "! echo \"$setup_cfg\" > custom/setup.cfg\n",
        "\n",
        "setup_py = \"import setuptools\\n\\nsetuptools.setup(\\n\\n    install_requires=[\\n\\n        'tensorflow==2.5.0',\\n\\n        'tensorflow_hub',\\n\\n    'tfx',\\n\\n],\\n\\n    packages=setuptools.find_packages())\"\n",
        "! echo \"$setup_py\" > custom/setup.py\n",
        "\n",
        "pkg_info = \"Metadata-Version: 1.0\\n\\nName: Boston Housing tabular regression\\n\\nVersion: 0.0.0\\n\\nSummary: Demostration hyperparameter tuning script\\n\\nHome-page: www.google.com\\n\\nAuthor: Google\\n\\nAuthor-email: aferlitsch@google.com\\n\\nLicense: Public\\n\\nDescription: Demo\\n\\nPlatform: Vertex\"\n",
        "! echo \"$pkg_info\" > custom/PKG-INFO\n",
        "\n",
        "! touch custom/trainer/__init__.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eeb35b3c503"
      },
      "source": [
        "#### Copy Python package and data to your bucket\n",
        "\n",
        "Next, compressed and archive the TFX pipeline package, and copy both the package and the data to your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07cffa05a051"
      },
      "outputs": [],
      "source": [
        "! rm -f custom.tar custom.tar.gz\n",
        "! tar cvf custom.tar custom\n",
        "! gzip custom.tar\n",
        "! gsutil cp custom.tar.gz $BUCKET_URI/trainer.tar.gz\n",
        "\n",
        "! gsutil cp -r custom/data $BUCKET_URI/data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train_custom_job_machine_specification"
      },
      "source": [
        "### Prepare your machine specification\n",
        "\n",
        "Now define the machine specification for your custom hyperparameter tuning job. This tells Vertex what type of machine instance to provision for the hyperparameter tuning.\n",
        "  - `machine_type`: The type of GCP instance to provision -- e.g., n1-standard-8.\n",
        "  - `accelerator_type`: The type, if any, of hardware accelerator. In this tutorial if you previously set the variable `TRAIN_GPU != None`, you are using a GPU; otherwise you will use a CPU.\n",
        "  - `accelerator_count`: The number of accelerators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7e10f949d31"
      },
      "outputs": [],
      "source": [
        "if TRAIN_GPU:\n",
        "    machine_spec = {\n",
        "        \"machine_type\": TRAIN_COMPUTE,\n",
        "        \"accelerator_type\": TRAIN_GPU,\n",
        "        \"accelerator_count\": TRAIN_NGPU,\n",
        "    }\n",
        "else:\n",
        "    machine_spec = {\"machine_type\": TRAIN_COMPUTE, \"accelerator_count\": 0}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train_custom_job_disk_specification"
      },
      "source": [
        "### Prepare your disk specification\n",
        "\n",
        "(optional) Now define the disk specification for your custom hyperparameter tuning job. This tells Vertex what type and size of disk to provision in each machine instance for the hyperparameter tuning.\n",
        "\n",
        "  - `boot_disk_type`: Either SSD or Standard. SSD is faster, and Standard is less expensive. Defaults to SSD.\n",
        "  - `boot_disk_size_gb`: Size of disk in GB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91e1638595fc"
      },
      "outputs": [],
      "source": [
        "DISK_TYPE = \"pd-ssd\"  # [ pd-ssd, pd-standard]\n",
        "DISK_SIZE = 200  # GB\n",
        "\n",
        "disk_spec = {\"boot_disk_type\": DISK_TYPE, \"boot_disk_size_gb\": DISK_SIZE}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train_custom_job_worker_pool_specification:prebuilt_container"
      },
      "source": [
        "### Define the worker pool specification\n",
        "\n",
        "Next, you define the worker pool specification for your custom hyperparameter tuning job. The worker pool specification will consist of the following:\n",
        "\n",
        "- `replica_count`: The number of instances to provision of this machine type.\n",
        "- `machine_spec`: The hardware specification.\n",
        "- `disk_spec` : (optional) The disk storage specification.\n",
        "\n",
        "- `python_package`: The Python training package to install on the VM instance(s) and which Python module to invoke, along with command line arguments for the Python module.\n",
        "\n",
        "Let's dive deeper now into the python package specification:\n",
        "\n",
        "-`executor_image_spec`: This is the docker image which is configured for your custom hyperparameter tuning job.\n",
        "\n",
        "-`package_uris`: This is a list of the locations (URIs) of your python training packages to install on the provisioned instance. The locations need to be in a Cloud Storage bucket. These can be either individual python files or a zip (archive) of an entire package. In the later case, the job service will unzip (unarchive) the contents into the docker image.\n",
        "\n",
        "-`python_module`: The Python module (script) to invoke for running the custom hyperparameter tuning job. In this example, you will be invoking `trainer.task.py` -- note that it was not neccessary to append the `.py` suffix.\n",
        "\n",
        "-`args`: The command line arguments to pass to the corresponding Pythom module. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74f8f39aca06"
      },
      "outputs": [],
      "source": [
        "CMDARGS = []\n",
        "\n",
        "worker_pool_spec = [\n",
        "    {\n",
        "        \"replica_count\": 1,\n",
        "        \"machine_spec\": machine_spec,\n",
        "        \"disk_spec\": disk_spec,\n",
        "        \"python_package_spec\": {\n",
        "            \"executor_image_uri\": TRAIN_IMAGE,\n",
        "            \"package_uris\": [BUCKET_URI + \"/trainer.tar.gz\"],\n",
        "            \"python_module\": \"trainer.tfx_pipeline\",\n",
        "            \"args\": CMDARGS,\n",
        "        },\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_custom_job:mbsdk"
      },
      "source": [
        "## Create a custom job\n",
        "\n",
        "Use the class `CustomJob` to create a custom job, such as for hyperparameter tuning, with the following parameters:\n",
        "\n",
        "- `display_name`: A human readable name for the custom job.\n",
        "- `worker_pool_specs`: The specification for the corresponding VM instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_custom_job:mbsdk"
      },
      "outputs": [],
      "source": [
        "job = aiplatform.CustomJob(\n",
        "    display_name=\"tfx_\" + TIMESTAMP, worker_pool_specs=worker_pool_spec\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a0cf39d7156"
      },
      "source": [
        "### Execute the custom job\n",
        "\n",
        "Next, you execute the `CustomJob` using the `run()` method, with the following parameters:\n",
        "\n",
        "- `service_account`: The service account to execute the job.\n",
        "- `sync`: Whether to run the job asynchronously or block until completion (True)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09c4408c80d8"
      },
      "outputs": [],
      "source": [
        "job.run(service_account=SERVICE_ACCOUNT, sync=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48ddf8fa533a"
      },
      "source": [
        "### Delete the custom job\n",
        "\n",
        "The method 'delete()' will delete the custom job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44e5886dfbb8"
      },
      "outputs": [],
      "source": [
        "job.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af9bc5f289b0"
      },
      "source": [
        "#### Verify the metadata database was created\n",
        "\n",
        "Next, verify that the metadata.db database file exists where you specified it to be written in your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dffcb705d70"
      },
      "outputs": [],
      "source": [
        "! gsutil ls {BUCKET_URI}/metadata/cifar10/metadata.db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dae5c129ebd"
      },
      "source": [
        "#### Visualizing artifact output from ImportExampleGen\n",
        "\n",
        "Next, you view the output from `ImportExampleGen`, which consists of compressed archived files of the image data from the CIFAR-10 dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c62317dc8c57"
      },
      "outputs": [],
      "source": [
        "_pipeline_root = f\"{BUCKET_URI}/pipelines/cifar10\"\n",
        "\n",
        "path = f\"{_pipeline_root}/ImportExampleGen/examples\"\n",
        "dirs = ! gsutil ls {path}\n",
        "examples_run = dirs[-1]\n",
        "\n",
        "train_examples_filepath = os.path.join(examples_run, \"Split-train\")\n",
        "eval_examples_filepath = os.path.join(examples_run, \"Split-eval\")\n",
        "\n",
        "print(\"Training TFRecords\")\n",
        "! gsutil ls {train_examples_filepath}\n",
        "print(\"Evaluation TFRecords\")\n",
        "! gsutil ls {eval_examples_filepath}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "857acd7e98ce"
      },
      "source": [
        "#### Visualizing artifact output from StatisticsGen\n",
        "\n",
        "You can visualize some of the StatisticsGen Artifact and compare our train and eval datasets. This visualization tool is very useful to give us insights about the data such as data distribution and possible data skew."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "983989c70cf6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from os import listdir\n",
        "\n",
        "import tensorflow_data_validation as tfdv\n",
        "\n",
        "path = f\"{_pipeline_root}/StatisticsGen/statistics/\"\n",
        "dirs = ! gsutil ls {path}\n",
        "stats_run = dirs[-1]\n",
        "train_stats_filepath = os.path.join(stats_run, \"Split-train\", \"FeatureStats.pb\")\n",
        "eval_stats_filepath = os.path.join(stats_run, \"Split-eval\", \"FeatureStats.pb\")\n",
        "\n",
        "train_stats = tfdv.load_stats_binary(train_stats_filepath)\n",
        "eval_stats = tfdv.load_stats_binary(eval_stats_filepath)\n",
        "\n",
        "tfdv.visualize_statistics(\n",
        "    train_stats, rhs_statistics=eval_stats, lhs_name=\"train\", rhs_name=\"eval\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72d9a3f29236"
      },
      "source": [
        "#### Visualizing artifact output from SchemaGen\n",
        "\n",
        "Next, you view the output from `SchemaGen`, which is in protobuf format stored as a plain text file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f498be762133"
      },
      "outputs": [],
      "source": [
        "path = f\"{_pipeline_root}/SchemaGen/schema/\"\n",
        "dirs = ! gsutil ls {path}\n",
        "schema_run = dirs[-1]\n",
        "\n",
        "! gsutil cat {schema_run}schema.pbtxt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12c24b8abc50"
      },
      "source": [
        "#### Visualizing artifact output from ExampleValidator\n",
        "\n",
        "Next, you view the output from `ExampleValidator`, which is in protobuf format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fc70c932d21"
      },
      "outputs": [],
      "source": [
        "path = f\"{_pipeline_root}/ExampleValidator/anomalies/\"\n",
        "dirs = ! gsutil ls {path}\n",
        "example_validator_run = dirs[-1]\n",
        "\n",
        "train_example_validator_filepath = os.path.join(\n",
        "    example_validator_run, \"Split-train\", \"SchemaDiff.pb\"\n",
        ")\n",
        "eval_example_validator_filepath = os.path.join(\n",
        "    example_validator_run, \"Split-eval\", \"SchemaDiff.pb\"\n",
        ")\n",
        "\n",
        "! gsutil ls {train_example_validator_filepath}\n",
        "! gsutil ls {eval_example_validator_filepath}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f7a19b38322"
      },
      "source": [
        "#### Visualizing artifact output from Transform\n",
        "\n",
        "Next, you view the output from `Transform`, which contains the following subfolders:\n",
        "\n",
        "- `pre_transform_schema`\n",
        "- `pre_transform_stats`\n",
        "- `post_transform_anomalies`\n",
        "- `post_transform_schema`\n",
        "- `post_transform_stats`\n",
        "- `transform_graph`\n",
        "- `transform_examples`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0438c17332c2"
      },
      "outputs": [],
      "source": [
        "path = f\"{_pipeline_root}/Transform/transform_graph/\"\n",
        "dirs = ! gsutil ls {path}\n",
        "transform_graph_run = dirs[-1]\n",
        "\n",
        "print(\"Transform Graph\")\n",
        "! gsutil ls {transform_graph_run}\n",
        "\n",
        "path = f\"{_pipeline_root}/Transform/transformed_examples/\"\n",
        "dirs = ! gsutil ls {path}\n",
        "transformed_examples_run = dirs[-1]\n",
        "\n",
        "transformed_examples_train_filepath = os.path.join(\n",
        "    transformed_examples_run, \"Split-train\"\n",
        ")\n",
        "transformed_examples_eval_filepath = os.path.join(\n",
        "    transformed_examples_run, \"Split-eval\"\n",
        ")\n",
        "\n",
        "print(\"Transformed Examples\")\n",
        "! gsutil ls {transformed_examples_train_filepath} {transformed_examples_eval_filepath}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "521ca994ba84"
      },
      "source": [
        "#### Visualizing artifact output from Trainer\n",
        "\n",
        "Next, you view the output from `Trainer`, whichn consists of the trained model artifacts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac1b9cababbb"
      },
      "outputs": [],
      "source": [
        "path = f\"{_pipeline_root}/Trainer/model/\"\n",
        "dirs = ! gsutil ls {path}\n",
        "trainer_run = dirs[-1]\n",
        "model_artifacts = os.path.join(trainer_run, \"Format-Serving\")\n",
        "\n",
        "! gsutil ls {model_artifacts}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2d593f6cb06"
      },
      "source": [
        "#### Visualizing artifact output from Evaluator\n",
        "\n",
        "Next, you view the output from `Evaluator`, which consists of two parts: `evaluation` and `blessing`.\n",
        "\n",
        "The `evaluation` consists of the validation results stored as TFRecords.\n",
        "\n",
        "The `blessing` consists of a single (empty) plain text file, whose name is either:\n",
        "\n",
        "- `BLESSED`: Model passed the validation requirements.\n",
        "- `NOT_BLESSED`: Model did not pass the validation requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deeea4e5c828"
      },
      "outputs": [],
      "source": [
        "path = f\"{_pipeline_root}/Evaluator/evaluation/\"\n",
        "dirs = ! gsutil ls {path}\n",
        "evaluation_run = dirs[-1]\n",
        "\n",
        "path = f\"{_pipeline_root}/Evaluator/blessing/\"\n",
        "dirs = ! gsutil ls {path}\n",
        "blessing_run = dirs[-1]\n",
        "\n",
        "print(\"Evaluation\")\n",
        "! gsutil ls {evaluation_run}\n",
        "print(\"Blessing\")\n",
        "! gsutil ls {blessing_run}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33753261f3f0"
      },
      "source": [
        "#### Visualizing artifact output from Pusher\n",
        "\n",
        "Next, you view the output from `Pusher`. If the model passed evaluation, then the output will contain the pushed model artifacts:\n",
        "\n",
        "- `saved_model.pb`\n",
        "- `keras_metadata.pb`\n",
        "- `assets`\n",
        "- `variables`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd2eb179e365"
      },
      "outputs": [],
      "source": [
        "path = f\"{_pipeline_root}/Pusher/pushed_model/\"\n",
        "dirs = ! gsutil ls {path}\n",
        "pusher_run = dirs[-1]\n",
        "\n",
        "! gsutil ls {pusher_run}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b773e8d2bd2"
      },
      "source": [
        "## Execute TFX pipeline as `Vertex AI Pipeline`\n",
        "\n",
        "The Vertex AI pipeline will run in the cloud and deploy a model to a Vertex AI endpoint. All metadata will be stored in the Vertex AI Metadata store.\n",
        "\n",
        "With a few modifications to our code this pipeline can be deployed in Vertex.\n",
        "\n",
        "- Define constants with cloud paths instead of local ones (done previously as CustomJob)\n",
        "- Copy the data to our GCS bucket (done previously as CustomJob)\n",
        "- Copy the Transform and Trainer module to our GCS bucket (done previously as CustomJob)\n",
        "- Create Service Account Key (done previously as CustomJob)\n",
        "- Change our Beam arguments to use the DataflowRunner\n",
        "- Change the Trainer and Pusher components to use google_cloud_ai_platform extensions.\n",
        "\n",
        "### Recommendation\n",
        "\n",
        "Using `Vertex AI Pipelines` for executing TFX pipelines is recommended for larger jobs -- vs. executing as a `CustomJob`\n",
        "\n",
        "The way Vertex AI pipelines allocates resources is best suited for large jobs. When running on Vertex AI pipelines, you are running dataflow jobs for ingesting data, creating statistics, schema and transformations, and running training jobs as custom training jobs in Vertex AI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f646bcd1c53"
      },
      "source": [
        "### Update Apache Beam arguments to use Dataflow\n",
        "\n",
        "`Dataflow` provides serverless large-scale preprocessing of data for Apache Beam. You enable `Dataflow` as a runner by setting the parameter `--runner` to `DataflowRunner`. \n",
        "\n",
        "Additional parameters set:\n",
        "\n",
        "- `project`: Your project ID.\n",
        "- `region`: Your region (location).\n",
        "- `service_account`: Your service account.\n",
        "- `machine_type`: The VM to provision.\n",
        "- `disk_size_gb`: The amount of disk space for the provisioned VM.\n",
        "- `temp_location`: The Cloud Storage location for Apache Beam to write temporary resources.\n",
        "\n",
        "Learn more about [Dataflow](https://cloud.google.com/dataflow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6beb188468a1"
      },
      "outputs": [],
      "source": [
        "_beam_pipeline_args = [\n",
        "    \"--runner=DataflowRunner\",\n",
        "    f\"--project={PROJECT_ID}\",\n",
        "    f\"--temp_location={BUCKET_NAME}/tmp/\",\n",
        "    f\"--region={REGION}\",\n",
        "    \"--disk_size_gb=50\",\n",
        "    f\"--machine-type={TRAIN_COMPUTE}\",\n",
        "    f\"--service_account_email=vertexai-test@{PROJECT_ID}.iam.gserviceaccount.com\",\n",
        "    \"--experiments=use_runner_v2\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60fcdcd7c3a1"
      },
      "source": [
        "### Update the Trainer and Pusher components\n",
        "\n",
        "Next, you update the Trainer and Pusher components to use TFX extensions for `Vertex AI`.\n",
        "\n",
        "#### Update the Trainer component\n",
        "\n",
        "You update the `Trainer` component call as follows:\n",
        "\n",
        "- Replace `custom_config` with a `Vertex AI` job specification, which contains the `worker_pool_specs`.\n",
        "- Replace the `tfx.components.Trainer` with the `Vertex AI` extension `tfx.extensions.google_cloud_ai_platform.Trainer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eef258d90157"
      },
      "outputs": [],
      "source": [
        "vertex_job_spec = {\n",
        "    \"project\": PROJECT_ID,\n",
        "    \"worker_pool_specs\": [\n",
        "        {\n",
        "            \"machine_spec\": {\n",
        "                \"machine_type\": TRAIN_COMPUTE,\n",
        "                \"accelerator_type\": TRAIN_GPU,\n",
        "                \"accelerator_count\": TRAIN_NGPU,\n",
        "            },\n",
        "            \"replica_count\": 1,\n",
        "            \"container_spec\": {\"image_uri\": TRAIN_IMAGE},\n",
        "        }\n",
        "    ],\n",
        "}\n",
        "\n",
        "train_custom_config = {\n",
        "    tfx.extensions.google_cloud_ai_platform.TRAINING_ARGS_KEY: vertex_job_spec,\n",
        "    tfx.extensions.google_cloud_ai_platform.VERTEX_REGION_KEY: REGION,  # must be us-central1 for vertexai\n",
        "    tfx.extensions.google_cloud_ai_platform.ENABLE_VERTEX_KEY: True,\n",
        "}\n",
        "\n",
        "DATA_ROOT = os.path.join(BUCKET_URI, \"data\")\n",
        "labels_path = os.path.join(DATA_ROOT, \"labels.txt\")\n",
        "\n",
        "train_custom_config[\"labels_path\"] = labels_path\n",
        "\n",
        "TRAINER_MODULE = os.path.join(BUCKET_URI, \"train.py\")\n",
        "\n",
        "trainer = tfx.extensions.google_cloud_ai_platform.Trainer(\n",
        "    module_file=TRAINER_MODULE,\n",
        "    examples=transform.outputs[\"transformed_examples\"],\n",
        "    transform_graph=transform.outputs[\"transform_graph\"],\n",
        "    schema=schema_gen.outputs[\"schema\"],\n",
        "    # base_model=model_resolver.outputs['model'],\n",
        "    train_args=trainer_pb2.TrainArgs(num_steps=160),\n",
        "    eval_args=trainer_pb2.EvalArgs(num_steps=4),\n",
        "    custom_config=train_custom_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eea2f4704469"
      },
      "source": [
        "#### Update the Pusher component\n",
        "\n",
        "You update the `Pusher` component call as follows:\n",
        "\n",
        "- Update the `custom_config` with a `Vertex AI` serving specification.\n",
        "- Replace the `tfx.components.Pusher` with the `Vertex AI` extension `tfx.extensions.google_cloud_ai_platform.Pusher`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "291bc55873dd"
      },
      "outputs": [],
      "source": [
        "vertex_serving_spec = {\n",
        "    \"project_id\": PROJECT_ID,\n",
        "    \"endpoint_name\": \"vertex-pipeline-cifar10\",\n",
        "    \"machine_type\": DEPLOY_COMPUTE,\n",
        "}\n",
        "\n",
        "pusher = tfx.extensions.google_cloud_ai_platform.Pusher(\n",
        "    model=trainer.outputs[\"model\"],\n",
        "    model_blessing=evaluator.outputs[\"blessing\"],\n",
        "    custom_config={\n",
        "        tfx.extensions.google_cloud_ai_platform.ENABLE_VERTEX_KEY: True,\n",
        "        tfx.extensions.google_cloud_ai_platform.VERTEX_REGION_KEY: REGION,\n",
        "        tfx.extensions.google_cloud_ai_platform.VERTEX_CONTAINER_IMAGE_URI_KEY: DEPLOY_IMAGE,\n",
        "        tfx.extensions.google_cloud_ai_platform.SERVING_ARGS_KEY: vertex_serving_spec,\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90d26c151301"
      },
      "source": [
        "### Update the TFX pipeline to run as a `Vertex AI Pipeline`\n",
        "\n",
        "Finally, you will update your TFX pipeline code to run as a `Vertex AI Pipeline`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f66b49ce69a8"
      },
      "outputs": [],
      "source": [
        "content = f\"\"\"\n",
        "import os\n",
        "import logging\n",
        "import absl\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from tfx import v1 as tfx\n",
        "from tfx.components import (ImportExampleGen, \n",
        "                            StatisticsGen, \n",
        "                            SchemaGen, \n",
        "                            ExampleValidator, \n",
        "                            Transform,\n",
        "                            Trainer,\n",
        "                            Tuner,\n",
        "                            Evaluator,\n",
        "                            InfraValidator,\n",
        "                            Pusher,\n",
        "                            #BulkInferrer\n",
        "                           )\n",
        "from tfx.dsl.components.common import resolver\n",
        "from tfx.orchestration import pipeline\n",
        "from tfx.orchestration import metadata\n",
        "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
        "\n",
        "from tfx.proto import example_gen_pb2\n",
        "from tfx.proto import trainer_pb2\n",
        "from tfx.proto import pusher_pb2\n",
        "from tfx.proto import bulk_inferrer_pb2\n",
        "\n",
        "import tensorflow_model_analysis as tfma\n",
        "\n",
        "DATA_ROOT = \"{BUCKET_URI}/data\"\n",
        "LOCAL_ROOT = \"{BUCKET_URI}\"\n",
        "\n",
        "def create_pipeline():\n",
        "    input_config = example_gen_pb2.Input(splits=[\n",
        "        example_gen_pb2.Input.Split(name='train', pattern='train/*'),\n",
        "        example_gen_pb2.Input.Split(name='eval', pattern='test/*')\n",
        "    ])\n",
        "\n",
        "    example_gen = ImportExampleGen(\n",
        "        input_base=DATA_ROOT, \n",
        "        input_config=input_config\n",
        "    )\n",
        "\n",
        "    statistics_gen = StatisticsGen(\n",
        "        examples=example_gen.outputs['examples']\n",
        "    )\n",
        "\n",
        "    schema_gen = SchemaGen(\n",
        "        statistics=statistics_gen.outputs['statistics'], \n",
        "        infer_feature_shape=True\n",
        "    )\n",
        "\n",
        "    example_validator = ExampleValidator(\n",
        "          statistics=statistics_gen.outputs['statistics'],\n",
        "          schema=schema_gen.outputs['schema']\n",
        "    )\n",
        "\n",
        "    TRANSFORM_MODULE = '{BUCKET_URI}/transform.py'  # implements preprocessing_fn\n",
        "\n",
        "    transform = Transform(\n",
        "          examples=example_gen.outputs['examples'],\n",
        "          schema=schema_gen.outputs['schema'],\n",
        "          module_file=TRANSFORM_MODULE,\n",
        "          force_tf_compat_v1=True\n",
        "    )\n",
        "\n",
        "    TRAINER_MODULE = '{BUCKET_URI}/train.py'  # implements run_fn\n",
        "\n",
        "    vertex_job_spec = {{\n",
        "          'project': '{PROJECT_ID}',\n",
        "          'worker_pool_specs' : [\n",
        "              {{\n",
        "                'machine_spec': {{\n",
        "                  'machine_type' : '{TRAIN_COMPUTE}',\n",
        "                  'accelerator_type' : {TRAIN_GPU},  \n",
        "                  'accelerator_count' : {TRAIN_NGPU}\n",
        "                }},\n",
        "                'replica_count' : 1,\n",
        "                'container_spec' : {{\n",
        "                  'image_uri' : '{TRAIN_IMAGE}'\n",
        "                }}\n",
        "              }}\n",
        "          ]\n",
        "    }}\n",
        "\n",
        "    train_custom_config = {{\n",
        "      tfx.extensions.google_cloud_ai_platform.TRAINING_ARGS_KEY : vertex_job_spec,\n",
        "      tfx.extensions.google_cloud_ai_platform.VERTEX_REGION_KEY : '{REGION}', \n",
        "      tfx.extensions.google_cloud_ai_platform.ENABLE_VERTEX_KEY : True\n",
        "    }}\n",
        "\n",
        "    labels_path = os.path.join(DATA_ROOT, 'labels.txt')\n",
        "\n",
        "    train_custom_config['labels_path'] = labels_path\n",
        "\n",
        "    trainer = Trainer(\n",
        "        module_file=TRAINER_MODULE,   \n",
        "        examples=transform.outputs['transformed_examples'],\n",
        "        transform_graph=transform.outputs['transform_graph'],\n",
        "        schema=schema_gen.outputs['schema'],\n",
        "\n",
        "        # not implemented in this tutorial: \n",
        "        # hyperparameters=tuner.outputs['best_hyperparameters'],\n",
        "\n",
        "        # This will be passed to `run_fn`.\n",
        "        train_args=trainer_pb2.TrainArgs(num_steps=100),\n",
        "        eval_args=trainer_pb2.EvalArgs(num_steps=5)\n",
        "    )\n",
        "\n",
        "    LOWER_BOUND_VALIDATION = 0.55  # the metric threshold to validate the model.\n",
        "\n",
        "    eval_config = tfma.EvalConfig(\n",
        "          model_specs=[tfma.ModelSpec(label_key='label')],\n",
        "          slicing_specs=[tfma.SlicingSpec()],\n",
        "          metrics_specs=[\n",
        "              tfma.MetricsSpec(metrics=[\n",
        "                  tfma.MetricConfig(\n",
        "                      class_name='SparseCategoricalAccuracy',\n",
        "                      threshold=tfma.MetricThreshold(\n",
        "                          value_threshold=tfma.GenericValueThreshold(\n",
        "                              lower_bound={{'value': LOWER_BOUND_VALIDATION}}),\n",
        "                          # Change threshold will be ignored if there is no\n",
        "                          # baseline model resolved from MLMD (first run).\n",
        "                          change_threshold=tfma.GenericChangeThreshold(\n",
        "                              direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
        "                              absolute={{'value': -1e-3}})))\n",
        "              ])\n",
        "          ])\n",
        "\n",
        "    evaluator = Evaluator(\n",
        "        examples=example_gen.outputs['examples'],\n",
        "        model=trainer.outputs['model'],\n",
        "        #baseline_model=model_resolver.outputs['model'],\n",
        "        eval_config=eval_config\n",
        "    )\n",
        "\n",
        "    _pipeline_name = 'cifar10'\n",
        "\n",
        "    vertex_serving_spec = {{\n",
        "        'project_id' : '{PROJECT_ID}',\n",
        "          'endpoint_name' : 'vertex-pipeline-cifar10',\n",
        "          'machine_type' : '{DEPLOY_COMPUTE}'\n",
        "      }}\n",
        "\n",
        "    pusher = tfx.extensions.google_cloud_ai_platform.Pusher(\n",
        "      model=trainer.outputs['model'],\n",
        "      model_blessing=evaluator.outputs['blessing'],\n",
        "      custom_config={{\n",
        "        tfx.extensions.google_cloud_ai_platform.ENABLE_VERTEX_KEY: True,\n",
        "        tfx.extensions.google_cloud_ai_platform.VERTEX_REGION_KEY: '{REGION}',\n",
        "        tfx.extensions.google_cloud_ai_platform.VERTEX_CONTAINER_IMAGE_URI_KEY: '{DEPLOY_IMAGE}',\n",
        "        tfx.extensions.google_cloud_ai_platform.SERVING_ARGS_KEY: vertex_serving_spec\n",
        "      }}\n",
        "    )\n",
        "\n",
        "    _pipeline_root = LOCAL_ROOT + \"/pipelines/\" + _pipeline_name\n",
        "    _metadata_path = os.path.join(LOCAL_ROOT, 'metadata', _pipeline_name, 'metadata.db')\n",
        "\n",
        "    gs_prefix = 'gs://'\n",
        "    gcsfuse_prefix = '/gcs/'\n",
        "    try:\n",
        "        if _metadata_path.startswith(gs_prefix):\n",
        "            _metadata_path = _metadata_path.replace(gs_prefix, gcsfuse_prefix)\n",
        "            dirpath = os.path.split(_metadata_path)[0]\n",
        "            if not os.path.isdir(dirpath):\n",
        "                os.makedirs(dirpath)\n",
        "        if _pipeline_root.startswith(gs_prefix):\n",
        "            _pipeline_root = _pipeline_root.replace(gs_prefix, gcsfuse_prefix)\n",
        "            dirpath = os.path.split(_pipeline_root)[0]\n",
        "            if not os.path.isdir(dirpath):\n",
        "                os.makedirs(dirpath)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    _beam_pipeline_args = [\n",
        "        '--runner=DataflowRunner',\n",
        "        \"--project={PROJECT_ID}\",\n",
        "        \"--temp_location={BUCKET_URI}/tmp/\",\n",
        "        \"--region={REGION}\",\n",
        "        '--disk_size_gb=50',\n",
        "        '--machine-type={TRAIN_COMPUTE}',\n",
        "        \"--service_account_email={SERVICE_ACCOUNT}\",\n",
        "        '--experiments=use_runner_v2'\n",
        "    ]\n",
        "\n",
        "    components = [\n",
        "        example_gen, \n",
        "        statistics_gen, \n",
        "        schema_gen,\n",
        "        example_validator,\n",
        "        transform,\n",
        "        trainer,\n",
        "        evaluator,\n",
        "        pusher\n",
        "    ]\n",
        "\n",
        "    tfx_pipeline = pipeline.Pipeline(\n",
        "        pipeline_name=_pipeline_name,\n",
        "        pipeline_root=_pipeline_root,\n",
        "        components=components,\n",
        "        enable_cache=False,\n",
        "        # metadata_connection_config=metadata.sqlite_metadata_connection_config(_metadata_path),  # Vertex AI tracks metadata in ML Metadata\n",
        "        beam_pipeline_args=_beam_pipeline_args\n",
        "    )\n",
        "    \n",
        "    return tfx_pipeline\n",
        "\n",
        "# To run this pipeline from the python CLI:\n",
        "#   $python cifar_pipeline_native_keras.py\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  loggers = [logging.getLogger(name) for name in logging.root.manager.loggerDict]\n",
        "  for logger in loggers:\n",
        "    logger.setLevel(logging.INFO)\n",
        "  logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "  absl.logging.set_verbosity(absl.logging.FATAL)\n",
        "\n",
        "  runner = tfx.orchestration.experimental.KubeflowV2DagRunner(\n",
        "    config=tfx.orchestration.experimental.KubeflowV2DagRunnerConfig(),\n",
        "    output_filename=\"cifar10-pipeline.json\"\n",
        "  )\n",
        "\n",
        "  runner.run(\n",
        "      create_pipeline()\n",
        "  )\n",
        "\"\"\"\n",
        "\n",
        "with open(\"custom/trainer/tfx_pipeline.py\", \"w\") as f:\n",
        "    f.write(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b93adb8a94a"
      },
      "source": [
        "### Generate the JSON pipeline file\n",
        "\n",
        "Next, execute the script locally to generate the KFP pipeline JSON definition.\n",
        "\n",
        "*Note*: This won't actually excute the pipeline -- just generates the JSON file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5c4aeb0e985"
      },
      "outputs": [],
      "source": [
        "! python3 custom/trainer/tfx_pipeline.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e427bc52232"
      },
      "source": [
        "### Execute the `Vertex AI Pipeline`\n",
        "\n",
        "Now that you have a compiled KFP JSON pipeline definition, you execute it as a `Vertex AI Pipeline` job.\n",
        "\n",
        "The output will contain a link with See your Pipeline job here. The link will take you to the pipeline definition that is running in `Vertex AI`, and will look like the image below.\n",
        "\n",
        "<img src='https://g3doc.corp.google.com/cloud/sales/teams/sales_aiml_northam/g3doc/tfx-pipelines/img/vertex-ai-pipeline.png'/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3c502fc7e41"
      },
      "outputs": [],
      "source": [
        "PIPELINE_ROOT = f\"{BUCKET_URI}/pipeline_root/cifar10\"\n",
        "\n",
        "job = aiplatform.PipelineJob(\n",
        "    display_name=\"cifar10-tfx\",\n",
        "    template_path=\"cifar10-pipeline.json\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        "    enable_caching=True,\n",
        ")\n",
        "\n",
        "job.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view_pipleline_results:bqml"
      },
      "source": [
        "### View the pipeline results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "view_pipleline_results:bqml"
      },
      "outputs": [],
      "source": [
        "PROJECT_NUMBER = job.gca_resource.name.split(\"/\")[1]\n",
        "print(PROJECT_NUMBER)\n",
        "\n",
        "\n",
        "def print_pipeline_output(job, output_task_name):\n",
        "    JOB_ID = job.name\n",
        "    print(JOB_ID)\n",
        "    artifact = \"\"\n",
        "    for _ in range(len(job.gca_resource.job_detail.task_details)):\n",
        "        TASK_ID = job.gca_resource.job_detail.task_details[_].task_id\n",
        "        EXECUTE_OUTPUT = (\n",
        "            PIPELINE_ROOT\n",
        "            + \"/\"\n",
        "            + PROJECT_NUMBER\n",
        "            + \"/\"\n",
        "            + JOB_ID\n",
        "            + \"/\"\n",
        "            + output_task_name\n",
        "            + \"_\"\n",
        "            + str(TASK_ID)\n",
        "            + \"/executor_output.json\"\n",
        "        )\n",
        "        GCP_RESOURCES = (\n",
        "            PIPELINE_ROOT\n",
        "            + \"/\"\n",
        "            + PROJECT_NUMBER\n",
        "            + \"/\"\n",
        "            + JOB_ID\n",
        "            + \"/\"\n",
        "            + output_task_name\n",
        "            + \"_\"\n",
        "            + str(TASK_ID)\n",
        "            + \"/gcp_resources\"\n",
        "        )\n",
        "        EVALUATION_METRICS = (\n",
        "            PIPELINE_ROOT\n",
        "            + \"/\"\n",
        "            + PROJECT_NUMBER\n",
        "            + \"/\"\n",
        "            + JOB_ID\n",
        "            + \"/\"\n",
        "            + output_task_name\n",
        "            + \"_\"\n",
        "            + str(TASK_ID)\n",
        "            + \"/evaluation_metrics\"\n",
        "        )\n",
        "        # Check if file exists, 0 is success\n",
        "        !gsutil -q stat $EXECUTE_OUTPUT\n",
        "        if _exit_code == 0:\n",
        "            ! gsutil cat $EXECUTE_OUTPUT\n",
        "            artifact = EXECUTE_OUTPUT\n",
        "            break\n",
        "        !gsutil -q stat $GCP_RESOURCES\n",
        "        if _exit_code == 0:\n",
        "            ! gsutil cat $GCP_RESOURCES\n",
        "            artifact = GCP_RESOURCES\n",
        "            break\n",
        "        !gsutil -q stat $EVALUATION_METRICS\n",
        "        if _exit_code == 0:\n",
        "            ! gsutil cat $EVALUATION_METRICS\n",
        "            artifact = EVALUATION_METRICS\n",
        "            break\n",
        "\n",
        "    return artifact\n",
        "\n",
        "\n",
        "print(\"ImportExampleGen\")\n",
        "artifacts = print_pipeline_output(job, \"ImportExampleGen\")\n",
        "print(\"\\n\\n\")\n",
        "print(\"StatisticsGen\")\n",
        "artifacts = print_pipeline_output(job, \"StatisticsGen\")\n",
        "print(\"\\n\\n\")\n",
        "print(\"SchemaGen\")\n",
        "metrics = print_pipeline_output(job, \"SchemaGen\")\n",
        "print(\"\\n\\n\")\n",
        "print(\"ExampleValidator\")\n",
        "artifacts = print_pipeline_output(job, \"ExampleValidator\")\n",
        "print(\"\\n\\n\")\n",
        "print(\"Transform\")\n",
        "artifacts = print_pipeline_output(job, \"Transform\")\n",
        "print(\"\\n\\n\")\n",
        "print(\"Trainer\")\n",
        "artifacts = print_pipeline_output(job, \"Trainer\")\n",
        "print(\"\\n\\n\")\n",
        "print(\"Evaluator\")\n",
        "artifacts = print_pipeline_output(job, \"Evaluator\")\n",
        "print(\"\\n\\n\")\n",
        "print(\"Pusher\")\n",
        "artifacts = print_pipeline_output(job, \"Pusher\")\n",
        "print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f431a9e6f025"
      },
      "source": [
        "### Delete the pipeline job\n",
        "\n",
        "The method 'delete()' will delete the pipeline job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00bf554abbc6"
      },
      "outputs": [],
      "source": [
        "job.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cleanup:mbsdk"
      },
      "source": [
        "# Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup:mbsdk"
      },
      "outputs": [],
      "source": [
        "delete_bucket = False\n",
        "\n",
        "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
        "    ! gsutil rm -r $BUCKET_URI\n",
        "\n",
        "! rm -rf custom custom.tar.gz"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "get_started_with_tfx_pipeline.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

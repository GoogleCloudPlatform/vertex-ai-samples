{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title:generic,gcp"
   },
   "source": [
    "# E2E ML on GCP: MLOps stage 3 : formalization: get started with BigQuery and TFDV pipeline components\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/get_started_with_bq_tfdv_pipeline_components.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/get_started_with_bq_tfdv_pipeline_components.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:mlops"
   },
   "source": [
    "## Overview\n",
    "\n",
    "\n",
    "This tutorial demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. This tutorial covers stage 3 : formalization: get started with BigQuery and TFDV pipeline components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:gsod,lrg"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset used for this tutorial is the GSOD dataset from [BigQuery public datasets](https://cloud.google.com/bigquery/public-data). The version of the dataset you use only the fields year, month and day to predict the value of mean daily temperature (mean_temp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:mlops,stage3,get_started_bq_tfdv_pipeline_components"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you learn how to use build lightweight Python components for BigQuery and Tensorflow Data Validation.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services:\n",
    "\n",
    "- `Vertex AI Pipelines`\n",
    "- `Vertex AI Datasets`\n",
    "- `BigQuery`\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Build and execute a pipeline component for creating a Vertex AI Tabular Dataset from a BigQuery table.\n",
    "- Build and execute a pipeline component for generating TFDV statistics and schema from a Vertex AI Tabular Dataset.\n",
    "- Execute a Vertex AI pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costs \n",
    "\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_mlops"
   },
   "source": [
    "## Installations\n",
    "\n",
    "Install *one time* the packages for executing the MLOps notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "install_mlops",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.8.0-cp39-cp39-manylinux2010_x86_64.whl (497.6 MB)\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Using cached tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Using cached wrapt-1.14.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "Collecting flatbuffers>=1.12\n",
      "  Using cached flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Using cached h5py-3.6.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/yy/lib/python3.9/site-packages (from tensorflow) (61.2.0)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.24.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "Collecting absl-py>=0.4.0\n",
      "  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Collecting keras<2.9,>=2.8.0rc0\n",
      "  Using cached keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Collecting libclang>=9.0.1\n",
      "  Using cached libclang-13.0.0-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting typing-extensions>=3.6.6\n",
      "  Using cached typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/envs/yy/lib/python3.9/site-packages (from tensorflow) (1.22.3)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/envs/yy/lib/python3.9/site-packages (from tensorflow) (3.20.0rc2)\n",
      "Collecting gast>=0.2.1\n",
      "  Using cached gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/envs/yy/lib/python3.9/site-packages (from tensorflow) (1.44.0)\n",
      "Collecting tensorboard<2.9,>=2.8\n",
      "  Using cached tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-2.1.0-py3-none-any.whl (224 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.27.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/envs/yy/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.6.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (5.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/envs/yy/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/yy/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Using cached importlib_metadata-4.11.3-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/yy/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/yy/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/yy/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.3)\n",
      "Collecting zipp>=0.5\n",
      "  Using cached zipp-3.7.0-py3-none-any.whl (5.3 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/envs/yy/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: tf-estimator-nightly, termcolor, tensorboard-plugin-wit, libclang, keras, flatbuffers, zipp, wrapt, werkzeug, typing-extensions, tensorflow-io-gcs-filesystem, tensorboard-data-server, opt-einsum, oauthlib, keras-preprocessing, h5py, google-pasta, gast, astunparse, absl-py, requests-oauthlib, importlib-metadata, markdown, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 flatbuffers-2.0 gast-0.5.3 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 h5py-3.6.0 importlib-metadata-4.11.3 keras-2.8.0 keras-preprocessing-1.1.2 libclang-13.0.0 markdown-3.3.6 oauthlib-3.2.0 opt-einsum-3.3.0 requests-oauthlib-1.3.1 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tensorflow-io-gcs-filesystem-0.24.0 termcolor-1.1.0 tf-estimator-nightly-2.8.0.dev2021122109 typing-extensions-4.1.1 werkzeug-2.1.0 wrapt-1.14.0 zipp-3.7.0\n",
      "Requirement already satisfied: google-cloud-aiplatform[tensorboard] in /opt/conda/envs/yy/lib/python3.9/site-packages (1.11.0)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from google-cloud-aiplatform[tensorboard]) (2.2.1)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from google-cloud-aiplatform[tensorboard]) (2.34.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/envs/yy/lib/python3.9/site-packages (from google-cloud-aiplatform[tensorboard]) (21.3)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/envs/yy/lib/python3.9/site-packages (from google-cloud-aiplatform[tensorboard]) (2.7.1)\n",
      "Requirement already satisfied: proto-plus>=1.15.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from google-cloud-aiplatform[tensorboard]) (1.20.3)\n",
      "Collecting tensorflow<=2.7.0,>=2.3.0\n",
      "  Using cached tensorflow-2.7.0-cp39-cp39-manylinux2010_x86_64.whl (489.7 MB)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (3.20.0rc2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (2.27.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (1.56.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (2.6.2)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/envs/yy/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (1.44.0)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/envs/yy/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (1.44.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/envs/yy/lib/python3.9/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform[tensorboard]) (2.2.3)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform[tensorboard]) (2.3.2)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/envs/yy/lib/python3.9/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform[tensorboard]) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/yy/lib/python3.9/site-packages (from packaging>=14.3->google-cloud-aiplatform[tensorboard]) (3.0.7)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/envs/yy/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /opt/conda/envs/yy/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (1.22.3)\n",
      "Collecting keras<2.8,>=2.7.0rc0\n",
      "  Using cached keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/envs/yy/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (1.1.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (1.14.0)\n",
      "Collecting gast<0.5.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /opt/conda/envs/yy/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (0.24.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (0.37.1)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /opt/conda/envs/yy/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (2.8.0)\n",
      "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
      "  Using cached tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /opt/conda/envs/yy/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (13.0.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (3.6.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (1.0.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/envs/yy/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/envs/yy/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/yy/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (5.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/envs/yy/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (4.8)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform[tensorboard]) (1.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/yy/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/yy/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/yy/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (3.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/yy/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (3.3.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/envs/yy/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (2.1.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/envs/yy/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (0.4.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (61.2.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (0.6.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/envs/yy/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (4.11.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/envs/yy/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/yy/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (3.7.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (3.2.0)\n",
      "Installing collected packages: tensorflow-estimator, keras, gast, tensorflow\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.8.0\n",
      "    Uninstalling keras-2.8.0:\n",
      "      Successfully uninstalled keras-2.8.0\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.5.3\n",
      "    Uninstalling gast-0.5.3:\n",
      "      Successfully uninstalled gast-0.5.3\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.8.0\n",
      "    Uninstalling tensorflow-2.8.0:\n",
      "      Successfully uninstalled tensorflow-2.8.0\n",
      "Successfully installed gast-0.4.0 keras-2.7.0 tensorflow-2.7.0 tensorflow-estimator-2.7.0\n",
      "Collecting kfp\n",
      "  Downloading kfp-1.8.12.tar.gz (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.2/301.2 KB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py<2,>=0.9 in /opt/conda/envs/yy/lib/python3.9/site-packages (from kfp) (1.0.0)\n",
      "Collecting PyYAML<6,>=5.3\n",
      "  Using cached PyYAML-5.4.1-cp39-cp39-manylinux1_x86_64.whl (630 kB)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/envs/yy/lib/python3.9/site-packages (from kfp) (2.7.1)\n",
      "Collecting google-cloud-storage<2,>=1.20.0\n",
      "  Using cached google_cloud_storage-1.44.0-py2.py3-none-any.whl (106 kB)\n",
      "Collecting kubernetes<19,>=8.0.0\n",
      "  Using cached kubernetes-18.20.0-py2.py3-none-any.whl (1.6 MB)\n",
      "Collecting google-api-python-client<2,>=1.7.8\n",
      "  Using cached google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
      "Collecting google-auth<2,>=1.6.1\n",
      "  Using cached google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "Collecting requests-toolbelt<1,>=0.8.0\n",
      "  Using cached requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "Collecting cloudpickle<3,>=2.0.0\n",
      "  Using cached cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
      "Collecting kfp-server-api<2.0.0,>=1.1.2\n",
      "  Using cached kfp_server_api-1.8.1-py3-none-any.whl\n",
      "Collecting jsonschema<4,>=3.0.1\n",
      "  Using cached jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/envs/yy/lib/python3.9/site-packages (from kfp) (0.8.9)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /opt/conda/envs/yy/lib/python3.9/site-packages (from kfp) (8.1.1)\n",
      "Collecting Deprecated<2,>=1.2.7\n",
      "  Using cached Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting strip-hints<1,>=0.1.8\n",
      "  Using cached strip_hints-0.1.10-py2.py3-none-any.whl\n",
      "Collecting docstring-parser<1,>=0.7.3\n",
      "  Using cached docstring_parser-0.13-py3-none-any.whl\n",
      "Collecting kfp-pipeline-spec<0.2.0,>=0.1.14\n",
      "  Downloading kfp_pipeline_spec-0.1.14-py3-none-any.whl (18 kB)\n",
      "Collecting fire<1,>=0.3.1\n",
      "  Using cached fire-0.4.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from kfp) (3.20.0rc2)\n",
      "Collecting uritemplate<4,>=3.0.1\n",
      "  Using cached uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Collecting pydantic<2,>=1.8.2\n",
      "  Using cached pydantic-1.9.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "Collecting typer<1.0,>=0.3.2\n",
      "  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: six in /opt/conda/envs/yy/lib/python3.9/site-packages (from absl-py<2,>=0.9->kfp) (1.16.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/envs/yy/lib/python3.9/site-packages (from Deprecated<2,>=1.2.7->kfp) (1.14.0)\n",
      "Requirement already satisfied: termcolor in /opt/conda/envs/yy/lib/python3.9/site-packages (from fire<1,>=0.3.1->kfp) (1.1.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (2.27.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.56.0)\n",
      "Collecting google-auth-httplib2>=0.0.3\n",
      "  Using cached google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from google-api-python-client<2,>=1.7.8->kfp) (0.20.4)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/yy/lib/python3.9/site-packages (from google-auth<2,>=1.6.1->kfp) (0.2.8)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from google-auth<2,>=1.6.1->kfp) (61.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/envs/yy/lib/python3.9/site-packages (from google-auth<2,>=1.6.1->kfp) (4.8)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from google-cloud-storage<2,>=1.20.0->kfp) (2.3.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from google-cloud-storage<2,>=1.20.0->kfp) (2.2.3)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from jsonschema<4,>=3.0.1->kfp) (0.18.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from jsonschema<4,>=3.0.1->kfp) (21.4.0)\n",
      "Requirement already satisfied: urllib3>=1.15 in /opt/conda/envs/yy/lib/python3.9/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (1.26.9)\n",
      "Requirement already satisfied: certifi in /opt/conda/envs/yy/lib/python3.9/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2021.10.8)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/envs/yy/lib/python3.9/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2.8.2)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/envs/yy/lib/python3.9/site-packages (from kubernetes<19,>=8.0.0->kfp) (1.3.1)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0\n",
      "  Downloading websocket_client-1.3.2-py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/yy/lib/python3.9/site-packages (from pydantic<2,>=1.8.2->kfp) (4.1.1)\n",
      "Requirement already satisfied: wheel in /opt/conda/envs/yy/lib/python3.9/site-packages (from strip-hints<1,>=0.1.8->kfp) (0.37.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/envs/yy/lib/python3.9/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client<2,>=1.7.8->kfp) (3.0.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/envs/yy/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/yy/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (3.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/envs/yy/lib/python3.9/site-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp) (3.2.0)\n",
      "Building wheels for collected packages: kfp\n",
      "  Building wheel for kfp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp: filename=kfp-1.8.12-py3-none-any.whl size=419048 sha256=169cbae580abfdc58ecc094297302bdaaa713e24bf6b738bce832cc83d8cc1d8\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/e0/a3/8b/897768051e54a03d4ad9d8211e100fc4cc5439d20743d8ab25\n",
      "Successfully built kfp\n",
      "Installing collected packages: websocket-client, uritemplate, typer, strip-hints, PyYAML, pydantic, kfp-pipeline-spec, jsonschema, fire, docstring-parser, Deprecated, cloudpickle, cachetools, requests-toolbelt, kfp-server-api, google-auth, kubernetes, google-auth-httplib2, google-api-python-client, google-cloud-storage, kfp\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n",
      "      Successfully uninstalled PyYAML-6.0\n",
      "  Attempting uninstall: jsonschema\n",
      "    Found existing installation: jsonschema 4.4.0\n",
      "    Uninstalling jsonschema-4.4.0:\n",
      "      Successfully uninstalled jsonschema-4.4.0\n",
      "  Attempting uninstall: cachetools\n",
      "    Found existing installation: cachetools 5.0.0\n",
      "    Uninstalling cachetools-5.0.0:\n",
      "      Successfully uninstalled cachetools-5.0.0\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.6.2\n",
      "    Uninstalling google-auth-2.6.2:\n",
      "      Successfully uninstalled google-auth-2.6.2\n",
      "  Attempting uninstall: google-cloud-storage\n",
      "    Found existing installation: google-cloud-storage 2.2.1\n",
      "    Uninstalling google-cloud-storage-2.2.1:\n",
      "      Successfully uninstalled google-cloud-storage-2.2.1\n",
      "Successfully installed Deprecated-1.2.13 PyYAML-5.4.1 cachetools-4.2.4 cloudpickle-2.0.0 docstring-parser-0.13 fire-0.4.0 google-api-python-client-1.12.11 google-auth-1.35.0 google-auth-httplib2-0.1.0 google-cloud-storage-1.44.0 jsonschema-3.2.0 kfp-1.8.12 kfp-pipeline-spec-0.1.14 kfp-server-api-1.8.1 kubernetes-18.20.0 pydantic-1.9.0 requests-toolbelt-0.9.1 strip-hints-0.1.10 typer-0.4.1 uritemplate-3.0.1 websocket-client-1.3.2\n"
     ]
    }
   ],
   "source": [
    "! pip3 install -U tensorflow $USER_FLAG\n",
    "! pip3 install --upgrade google-cloud-aiplatform[tensorboard] $USER_FLAG\n",
    "! pip3 install --upgrade kfp $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "restart"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_id"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "autoset_project_id"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: vertex-ai-dev\n"
     ]
    }
   ],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
    "\n",
    "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type:\"string\"}\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "timestamp"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "timestamp"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "When you initialize the Vertex SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "autoset_bucket"
   },
   "outputs": [],
   "source": [
    "if BUCKET_URI == \"\" or BUCKET_URI is None or BUCKET_URI == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_URI = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "create_bucket"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://vertex-ai-devaip-20220404064524/...\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validate_bucket"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "validate_bucket"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account"
   },
   "source": [
    "#### Service Account\n",
    "\n",
    "**If you don't know your service account**, try to get your service account using `gcloud` command by executing the second cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "set_service_account"
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "autoset_service_account"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service Account: 931647533046-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud auth list 2>/dev/null\n",
    "    SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account:pipelines"
   },
   "source": [
    "#### Set service account access for Vertex AI Pipelines\n",
    "\n",
    "Run the following commands to grant your service account access to read and write pipeline artifacts in the bucket that you created in the previous step -- you only need to run these once per service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "set_service_account:pipelines"
   },
   "outputs": [],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### Set up variables\n",
    "\n",
    "Next, set up some variables used throughout the tutorial.\n",
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_tf"
   },
   "source": [
    "#### Import TensorFlow\n",
    "\n",
    "Import the TensorFlow package into your Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "import_tf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-04 06:45:46.558956: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-04 06:45:46.559002: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "import_kfp:namedtuple"
   },
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq_dataflow_components_intro"
   },
   "source": [
    "## Pipeline components with BigQuery and Dataflow\n",
    "\n",
    "### An anatomy of a pipeline component\n",
    "\n",
    "Let's dive a bit into how pipeline components are executed. First, each component is containerized. That is, each component has its own:\n",
    "\n",
    "- container image\n",
    "- installation requirements\n",
    "- (optional) hardware requirements\n",
    "\n",
    "The above affects the amount of time/resources required to provision the component. For example, if each component in the pipeline had a different machine requirement, then a machine would have to be provisioned for each component. Even if the machine type is the same, if each component had a different container image, then a new container image would have to be provisioned for each component.\n",
    "\n",
    "In otherwords, the efficiency of the pipeline is affected by the amount of provisioning.\n",
    "\n",
    "Additionally, since each component runs in a container with its own memory space, there are performance issues relating to the amount of data moved across the container boundaries -- i.e., marshalling. To marshall data, the data has to be serialized and written to a volume storage, where the next component can access and de-serialize the data. Simple data types like integers, floats, strings, small dictionaries can be efficiently marshalled. You want to avoid though marshalling large memory objects.\n",
    "\n",
    "### Construction of data pipeline components\n",
    "\n",
    "Both BigQuery and Dataflow deal with data, and more importantly large amounts of data. As a result, you need to carefully consider the construction of the pipeline, so that you are not marshalling large amounts of in-memory data.\n",
    "\n",
    "For example, consider a task that consists of reading a million records into an in-memory pandas dataframe, and then the in-memory data is processed for statistics. You could write this as two components: one component creates the dataframe, and the other processes it. Sounds good, nice and modular and the first component is likely reusable. Bad choice though.\n",
    "\n",
    "If you did construct the components this way, the first component would have to write the dataframe to a disk, and the second component would then have to read it back from disk. Very inefficient. If you need a large in-memory object, one should only create it in the same component where it is used. In this example, one would create a single component to create and process the dataframe.\n",
    "\n",
    "Let's now consider Vertex AI resources like datasets, models and endpoints. These resources have a physical manifestation which may include a combination of data and binary files. The Vertex AI resource object is not the actual files, but a in-memory wrapper. The resource object consists of properties and method, and file data is not read into memory until a property/method needs it.\n",
    "\n",
    "Thus, for efficiency purposes, Vertex AI was designed with reference identifiers. One can load these resource wrappers via the resource identifier. Thus, when creating or otherwise referencing resource objects between components, one passes the resource identifier(s) between components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_file:u_dataset,bq"
   },
   "source": [
    "#### Location of BigQuery training data.\n",
    "\n",
    "Now set the variable `IMPORT_FILE` to the location of the data table in BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "import_file:gsod,bq,lrg"
   },
   "outputs": [],
   "source": [
    "IMPORT_FILE = \"bq://bigquery-public-data.samples.gsod\"\n",
    "BQ_TABLE = \"bigquery-public-data.samples.gsod\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_dataset_component:bq"
   },
   "source": [
    "### BigQuery components\n",
    "\n",
    "First, you build a component `create_dataset_bq` to create a Vertex AI dataset from a BigQuery table. The component will return the resource identifier for the created Vertex AI dataset. Next, you build two downstream components:\n",
    "\n",
    "    - `get_dataset_source`: Using the returned resource identifier, load the dataset resource object and get/return the dataset input source.\n",
    "    - `get_column_names`: Using the returned resource identifier, load the dataset resource object and get/return the dataset column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "create_dataset_component:bq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/yy/lib/python3.9/site-packages/kfp/v2/compiler/compiler.py:1278: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/931647533046/locations/us-central1/pipelineJobs/dataset-bq-20220404064558\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/931647533046/locations/us-central1/pipelineJobs/dataset-bq-20220404064558')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/dataset-bq-20220404064558?project=931647533046\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/931647533046/locations/us-central1/pipelineJobs/dataset-bq-20220404064558 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/931647533046/locations/us-central1/pipelineJobs/dataset-bq-20220404064558 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/931647533046/locations/us-central1/pipelineJobs/dataset-bq-20220404064558 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/931647533046/locations/us-central1/pipelineJobs/dataset-bq-20220404064558 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/931647533046/locations/us-central1/pipelineJobs/dataset-bq-20220404064558 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob run completed. Resource name: projects/931647533046/locations/us-central1/pipelineJobs/dataset-bq-20220404064558\n"
     ]
    }
   ],
   "source": [
    "@component(packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def create_dataset_bq(bq_table: str, display_name: str, project: str) -> str:\n",
    "    import google.cloud.aiplatform as aip\n",
    "\n",
    "    dataset = aip.TabularDataset.create(\n",
    "        display_name=display_name, bq_source=\"bq://\" + bq_table, project=project\n",
    "    )\n",
    "\n",
    "    return dataset.resource_name\n",
    "\n",
    "\n",
    "@component(packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def get_dataset_source(dataset_id: str) -> str:\n",
    "    import google.cloud.aiplatform as aip\n",
    "\n",
    "    dataset = aip.TabularDataset(dataset_id)\n",
    "    if \"gcsSource\" in dataset.gca_resource.metadata[\"inputConfig\"].keys():\n",
    "        files = dataset.gca_resource.metadata[\"inputConfig\"][\"gcsSource\"][\"uri\"]\n",
    "        return list(files)\n",
    "    else:\n",
    "        bq = dataset.gca_resource.metadata[\"inputConfig\"][\"bigquerySource\"][\"uri\"]\n",
    "        return bq\n",
    "\n",
    "\n",
    "@component(packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def get_column_names(dataset_id: str) -> list:\n",
    "    import google.cloud.aiplatform as aip\n",
    "\n",
    "    dataset = aip.TabularDataset(dataset_id)\n",
    "    return dataset.column_names\n",
    "\n",
    "\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/dataset_bq\".format(BUCKET_URI)\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"dataset-bq\",\n",
    "    description=\"Vertex Dataset from BQ Table\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "def pipeline(\n",
    "    bq_table: str = BQ_TABLE, display_name: str = \"example\", project: str = PROJECT_ID\n",
    "):\n",
    "    create_op = create_dataset_bq(bq_table, display_name, project)\n",
    "\n",
    "    _ = get_dataset_source(create_op.output)\n",
    "\n",
    "    _ = get_column_names(create_op.output)\n",
    "\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"dataset_bq.json\")\n",
    "\n",
    "pipeline = aip.PipelineJob(\n",
    "    display_name=\"dataset_bq\",\n",
    "    template_path=\"dataset_bq.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "pipeline.run()\n",
    "\n",
    "! rm dataset_bq.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view_pipeline_results:dataset_bq"
   },
   "source": [
    "### View the pipeline execution results\n",
    "\n",
    "Next, view the results -- i.e., artifacts that are passed by each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "view_pipeline_results:dataset_bq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "931647533046\n",
      "create_dataset_bq\n",
      "dataset-bq-20220404064558\n",
      "{\"parameters\": {\"Output\": {\"stringValue\": \"projects/931647533046/locations/us-central1/datasets/7386831376701456384\"}}}\n",
      "\n",
      "\n",
      "get_dataset_source\n",
      "dataset-bq-20220404064558\n",
      "{\"parameters\": {\"Output\": {\"stringValue\": \"bq://bigquery-public-data.samples.gsod\"}}}\n",
      "\n",
      "\n",
      "get_column_names\n",
      "dataset-bq-20220404064558\n",
      "{\"parameters\": {\"Output\": {\"stringValue\": \"[\\\"num_mean_visibility_samples\\\", \\\"max_temperature\\\", \\\"mean_temp\\\", \\\"max_sustained_wind_speed\\\", \\\"day\\\", \\\"max_gust_wind_speed\\\", \\\"snow\\\", \\\"year\\\", \\\"month\\\", \\\"hail\\\", \\\"mean_visibility\\\", \\\"rain\\\", \\\"fog\\\", \\\"mean_station_pressure\\\", \\\"min_temperature\\\", \\\"max_temperature_explicit\\\", \\\"num_mean_dew_point_samples\\\", \\\"mean_dew_point\\\", \\\"tornado\\\", \\\"wban_number\\\", \\\"station_number\\\", \\\"num_mean_station_pressure_samples\\\", \\\"num_mean_wind_speed_samples\\\", \\\"mean_wind_speed\\\", \\\"num_mean_sealevel_pressure_samples\\\", \\\"thunder\\\", \\\"min_temperature_explicit\\\", \\\"snow_depth\\\", \\\"num_mean_temp_samples\\\", \\\"total_precipitation\\\", \\\"mean_sealevel_pressure\\\"]\"}}}"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "PROJECT_NUMBER = pipeline.gca_resource.name.split(\"/\")[1]\n",
    "print(PROJECT_NUMBER)\n",
    "\n",
    "\n",
    "def print_pipeline_output(job, output_task_name):\n",
    "    JOB_ID = job.name\n",
    "    print(JOB_ID)\n",
    "    for _ in range(len(job.gca_resource.job_detail.task_details)):\n",
    "        TASK_ID = job.gca_resource.job_detail.task_details[_].task_id\n",
    "        EXECUTE_OUTPUT = (\n",
    "            PIPELINE_ROOT\n",
    "            + \"/\"\n",
    "            + PROJECT_NUMBER\n",
    "            + \"/\"\n",
    "            + JOB_ID\n",
    "            + \"/\"\n",
    "            + output_task_name\n",
    "            + \"_\"\n",
    "            + str(TASK_ID)\n",
    "            + \"/executor_output.json\"\n",
    "        )\n",
    "        GCP_RESOURCES = (\n",
    "            PIPELINE_ROOT\n",
    "            + \"/\"\n",
    "            + PROJECT_NUMBER\n",
    "            + \"/\"\n",
    "            + JOB_ID\n",
    "            + \"/\"\n",
    "            + output_task_name\n",
    "            + \"_\"\n",
    "            + str(TASK_ID)\n",
    "            + \"/gcp_resources\"\n",
    "        )\n",
    "        if tf.io.gfile.exists(EXECUTE_OUTPUT):\n",
    "            ! gsutil cat $EXECUTE_OUTPUT\n",
    "            break\n",
    "        elif tf.io.gfile.exists(GCP_RESOURCES):\n",
    "            ! gsutil cat $GCP_RESOURCES\n",
    "            break\n",
    "\n",
    "    return EXECUTE_OUTPUT\n",
    "\n",
    "\n",
    "print(\"create_dataset_bq\")\n",
    "artifacts = print_pipeline_output(pipeline, \"create-dataset-bq\")\n",
    "output = !gsutil cat $artifacts\n",
    "val = json.loads(output[0])\n",
    "dataset_id = val[\"parameters\"][\"Output\"][\"stringValue\"]\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"get_dataset_source\")\n",
    "artifacts = print_pipeline_output(pipeline, \"get-dataset-source\")\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"get_column_names\")\n",
    "artifacts = print_pipeline_output(pipeline, \"get-column-names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "delete_pipeline"
   },
   "source": [
    "### Delete a pipeline job\n",
    "\n",
    "After a pipeline job is completed, you can delete the pipeline job with the method `delete()`.  Prior to completion, a pipeline job can be canceled with the method `cancel()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "delete_pipeline"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.base:Deleting PipelineJob : projects/931647533046/locations/us-central1/pipelineJobs/dataset-bq-20220404064558\n",
      "INFO:google.cloud.aiplatform.base:Delete PipelineJob  backing LRO: projects/931647533046/locations/us-central1/operations/3212036922046152704\n",
      "INFO:google.cloud.aiplatform.base:PipelineJob deleted. . Resource name: projects/931647533046/locations/us-central1/pipelineJobs/dataset-bq-20220404064558\n"
     ]
    }
   ],
   "source": [
    "pipeline.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_statistics_pipeline"
   },
   "source": [
    "### Build TFDV component for dataset statistics\n",
    "\n",
    "Next, you build a component that will use the Tensorflow Data Validation package to produce dataset statistics and schema from the Vertex AI dataset you created, with the following parameters:\n",
    "\n",
    "- `dataset_id`: The resource ID of the Vertex AI dataset.\n",
    "- `label`: The label column for the dataset.\n",
    "- `bucket`: The bucket to write the statistics and schema data\n",
    "\n",
    "The statistics and schema are large memory objects that may be reused downstream by other components. For this purpose, the component directly writes the data to a Cloud Storage bucket, and then returns the Cloud Storage locations of the statistics and schema file as output artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "create_statistics_pipeline"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/931647533046/locations/us-central1/pipelineJobs/dataset-stats-20220404064938\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/931647533046/locations/us-central1/pipelineJobs/dataset-stats-20220404064938')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/dataset-stats-20220404064938?project=931647533046\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/931647533046/locations/us-central1/pipelineJobs/dataset-stats-20220404064938 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/931647533046/locations/us-central1/pipelineJobs/dataset-stats-20220404064938 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/931647533046/locations/us-central1/pipelineJobs/dataset-stats-20220404064938 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/931647533046/locations/us-central1/pipelineJobs/dataset-stats-20220404064938 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob run completed. Resource name: projects/931647533046/locations/us-central1/pipelineJobs/dataset-stats-20220404064938\n"
     ]
    }
   ],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-aiplatform\",\n",
    "        \"google-cloud-bigquery\",\n",
    "        \"tensorflow-data-validation==1.2\",\n",
    "        \"tensorflow==2.5\",\n",
    "    ]\n",
    ")\n",
    "def statistics(\n",
    "    dataset_id: str, label: str, bucket: str\n",
    ") -> NamedTuple(\"Outputs\", [(\"stats\", str), (\"schema\", str)]):  # Return parameters\n",
    "    import google.cloud.aiplatform as aip\n",
    "    import tensorflow_data_validation as tfdv\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    dataset = aip.TabularDataset(dataset_id)\n",
    "    if \"gcsSource\" in dataset.gca_resource.metadata[\"inputConfig\"].keys():\n",
    "        files = dataset.gca_resource.metadata[\"inputConfig\"][\"gcsSource\"][\"uri\"]\n",
    "        files = list(files)\n",
    "        stats = tfdv.generate_statistics_from_csv(\n",
    "            data_location=files[0],\n",
    "            stats_options=tfdv.StatsOptions(label_feature=label, num_top_values=50),\n",
    "        )\n",
    "    else:\n",
    "        bq = dataset.gca_resource.metadata[\"inputConfig\"][\"bigquerySource\"][\"uri\"]\n",
    "        bq_table = bq[5:]\n",
    "        table = bigquery.TableReference.from_string(bq_table)\n",
    "        bqclient = bigquery.Client()\n",
    "        rows = bqclient.list_rows(\n",
    "            table,\n",
    "            selected_fields=[\n",
    "                bigquery.SchemaField(\"station_number\", \"STRING\"),\n",
    "                bigquery.SchemaField(\"year\", \"INTEGER\"),\n",
    "                bigquery.SchemaField(\"month\", \"INTEGER\"),\n",
    "                bigquery.SchemaField(\"day\", \"INTEGER\"),\n",
    "                bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n",
    "            ],\n",
    "            max_results=10000,\n",
    "        )\n",
    "        dataframe = rows.to_dataframe()\n",
    "        stats = tfdv.generate_statistics_from_dataframe(\n",
    "            dataframe=dataframe,\n",
    "            stats_options=tfdv.StatsOptions(label_feature=label, num_top_values=50),\n",
    "        )\n",
    "\n",
    "    stats_file = bucket + \"/stats.txt\"\n",
    "    tfdv.write_stats_text(output_path=stats_file, stats=stats)\n",
    "\n",
    "    schema = tfdv.infer_schema(statistics=stats)\n",
    "\n",
    "    schema_file = bucket + \"/schema.txt\"\n",
    "    tfdv.write_schema_text(output_path=schema_file, schema=schema)\n",
    "\n",
    "    return (stats_file, schema_file)\n",
    "\n",
    "\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/dataset_stats\".format(BUCKET_URI)\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"dataset-stats\", description=\"Dataset statistics\", pipeline_root=PIPELINE_ROOT\n",
    ")\n",
    "def pipeline(dataset_id: str, label: str, bucket: str):\n",
    "\n",
    "    _ = statistics(dataset_id, label, bucket)\n",
    "\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"dataset_stats.json\")\n",
    "\n",
    "pipeline = aip.PipelineJob(\n",
    "    display_name=\"dataset_stats\",\n",
    "    template_path=\"dataset_stats.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\n",
    "        \"dataset_id\": dataset_id,\n",
    "        \"label\": \"mean_temp\",\n",
    "        \"bucket\": BUCKET_URI,\n",
    "    },\n",
    ")\n",
    "\n",
    "pipeline.run()\n",
    "\n",
    "!rm -f dataset_stats.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view_pipeline_results:statistics"
   },
   "source": [
    "### View the pipeline execution results\n",
    "\n",
    "Next, view the results -- i.e., the location of the statistics and schema artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "view_pipeline_results:statistics"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset-stats-20220404064938\n",
      "{\"parameters\": {\"stats\": {\"stringValue\": \"gs://vertex-ai-devaip-20220404064524/stats.txt\"}, \"schema\": {\"stringValue\": \"gs://vertex-ai-devaip-20220404064524/schema.txt\"}}}"
     ]
    }
   ],
   "source": [
    "artifacts = print_pipeline_output(pipeline, \"statistics\")\n",
    "output = !gsutil cat $artifacts\n",
    "val = json.loads(output[0])\n",
    "schema_location = val[\"parameters\"][\"schema\"][\"stringValue\"]\n",
    "stats_location = val[\"parameters\"][\"stats\"][\"stringValue\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "delete_pipeline"
   },
   "source": [
    "### Delete a pipeline job\n",
    "\n",
    "After a pipeline job is completed, you can delete the pipeline job with the method `delete()`.  Prior to completion, a pipeline job can be canceled with the method `cancel()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "delete_pipeline"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.base:Deleting PipelineJob : projects/931647533046/locations/us-central1/pipelineJobs/dataset-stats-20220404064938\n",
      "INFO:google.cloud.aiplatform.base:Delete PipelineJob  backing LRO: projects/931647533046/locations/us-central1/operations/7281039185375395840\n",
      "INFO:google.cloud.aiplatform.base:PipelineJob deleted. . Resource name: projects/931647533046/locations/us-central1/pipelineJobs/dataset-stats-20220404064938\n"
     ]
    }
   ],
   "source": [
    "pipeline.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "source": [
    "# Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n",
    "- Vertex AI dataset\n",
    "- Cloud Storage Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "cleanup:mbsdk",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.base:Deleting TabularDataset : projects/931647533046/locations/us-central1/datasets/7386831376701456384\n",
      "INFO:google.cloud.aiplatform.base:Delete TabularDataset  backing LRO: projects/931647533046/locations/us-central1/operations/2527489778685837312\n",
      "INFO:google.cloud.aiplatform.base:TabularDataset deleted. . Resource name: projects/931647533046/locations/us-central1/datasets/7386831376701456384\n"
     ]
    }
   ],
   "source": [
    "# Set this to true only if you'd like to delete your bucket\n",
    "delete_bucket = False\n",
    "\n",
    "# Create reference to Vertex AI dataset created in pipeline\n",
    "dataset = aip.TabularDataset(dataset_id)\n",
    "\n",
    "# delete Vertex AI dataset\n",
    "dataset.delete()\n",
    "\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "get_started_with_bq_tfdv_pipeline_components.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-env-yy-py",
   "name": "common-cpu.m89",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m89"
  },
  "kernelspec": {
   "display_name": "Python [conda env:yy]",
   "language": "python",
   "name": "conda-env-yy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title:generic,gcp",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "# E2E ML on GCP: MLOps stage 3 : formalization: get started with Kubeflow Pipelines\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/tree/master/notebooks/official/automl/ml_ops_stage3/get_started_with_kubeflow_pipelines.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/ai/platform/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/tree/master/notebooks/official/automl/ml_ops_stage3/get_started_with_kubeflow_pipelines.ipynb\">\n",
    "      Open in Google Cloud Notebooks\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:mlops",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Overview\n",
    "\n",
    "\n",
    "This tutorial demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. This tutorial covers stage 3 : formalization: get started with Kubeflow Pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:mlops,stage3,get_started_kubeflow_pipelines",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you learn how to use `Kubeflow Pipelines`.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services:\n",
    "\n",
    "- `Vertex AI Pipelines`\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Building KFP lightweight Python function components.\n",
    "- Assembling and compiling KFP components into a pipeline.\n",
    "- Executing a KFP pipeline using Vertex AI Pipelines.\n",
    "- Building sequential, parallel, multiple output components.\n",
    "- Building control flow into pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_mlops",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Installations\n",
    "\n",
    "Install *one time* the packages for executing the MLOps notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_mlops",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "ONCE_ONLY = False\n",
    "if ONCE_ONLY:\n",
    "    ! pip3 install -U tensorflow==2.5 $USER_FLAG\n",
    "    ! pip3 install -U tensorflow-data-validation==1.2 $USER_FLAG\n",
    "    ! pip3 install -U tensorflow-transform==1.2 $USER_FLAG\n",
    "    ! pip3 install -U tensorflow-io==0.18 $USER_FLAG\n",
    "    ! pip3 install --upgrade google-cloud-aiplatform[tensorboard] $USER_FLAG\n",
    "    ! pip3 install --upgrade google-cloud-bigquery $USER_FLAG\n",
    "    ! pip3 install --upgrade google-cloud-logging $USER_FLAG\n",
    "    ! pip3 install --upgrade apache-beam[gcp] $USER_FLAG\n",
    "    ! pip3 install --upgrade pyarrow $USER_FLAG\n",
    "    ! pip3 install --upgrade cloudml-hypertune $USER_FLAG\n",
    "    ! pip3 install --upgrade kfp $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "restart",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_gcloud_project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
    "\n",
    "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "region",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "REGION = 'us-central1'  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "timestamp",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "timestamp",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "When you initialize the Vertex SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validate_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "validate_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Service Account\n",
    "\n",
    "**If you don't know your service account**, try to get your service account using `gcloud` command by executing the second cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_service_account",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_service_account",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "if SERVICE_ACCOUNT == \"\" or SERVICE_ACCOUNT is None or SERVICE_ACCOUNT == \"[your-service-account]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud auth list 2>/dev/null\n",
    "    SERVICE_ACCOUNT = shell_output[2].strip()\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account:pipelines",
    "repo": "snippets_pipelines.ipynb"
   },
   "source": [
    "#### Set service account access for Vertex AI Pipelines\n",
    "\n",
    "Run the following commands to grant your service account access to read and write pipeline artifacts in the bucket that you created in the previous step -- you only need to run these once per service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_service_account:pipelines",
    "repo": "snippets_pipelines.ipynb"
   },
   "outputs": [],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_NAME\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Set up variables\n",
    "\n",
    "Next, set up some variables used throughout the tutorial.\n",
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_aip:mbsdk",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_tf",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Import TensorFlow\n",
    "\n",
    "Import the TensorFlow package into your Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_tf",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_kfp:namedtuple",
    "repo": "snippets_pipelines.ipynb"
   },
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pipelines_intro",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Pipeline introduction\n",
    "\n",
    "Vertex AI Pipelines lets you orchestrate your machine learning (ML) workflows in a serverless manner. Pipelines are re-usable, and their executions and artifact generation can be tracked by Vertex AI Experiments and Vertex AI ML Metadata. With pipelines, you do the following:\n",
    "\n",
    "    1. Design the pipeline workflow.\n",
    "    2. Compile the pipeline.\n",
    "    3. Schedule execution (or run now) the pipeline.\n",
    "    4. Get the pipeline results.\n",
    "\n",
    "Pipelines are designed using language specific domain specific language (DSL). Vertex AI Pipelines support both KFP DSL and TFX DSL for designing pipelines.\n",
    "\n",
    "In addition to designing components, you can use a wide variety of pre-built Google Cloud Pipeline Components for Vertex AI services.\n",
    "\n",
    "Learn more about [Building a pipeline](https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pipelines_intro:helloworld",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Basic pipeline introduction\n",
    "\n",
    "This demonstrates the basics of constructing and executing a pipeline. You do the following:\n",
    "\n",
    "1. Design a simple Python function based component to output the input string.\n",
    "2. Construct a pipeline that uses the component.\n",
    "2. Compile the pipeline.\n",
    "3. Execute the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "design_component:helloworld",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Design hello world component\n",
    "\n",
    "To create a KFP component from a Python function, you add the KFP DSL decorator `@component` to the function. In this example, the decorator takes the following parameters:\n",
    "\n",
    "- `output_component_file`: (optional) write the component description to a YAML file such that the component is portable.\n",
    "- `base_image`: (optional): The interpreter for executing the Python function. By default it is Python 3.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "design_component:helloworld",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "@component(output_component_file=\"hello_world.yaml\", base_image=\"python:3.9\")\n",
    "def hello_world(text: str) -> str:\n",
    "    print(text)\n",
    "    return text\n",
    "\n",
    "! cat hello_world.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "design_pipeline:helloworld",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Design the hello world pipeline\n",
    "\n",
    "Next, you design the pipeline for running the hello world component. A pipeline is specified as a Python function with the KFP DSL decorator `@dsl.component`, with the following parameters:\n",
    "\n",
    "- `name`: Name of the pipeline.\n",
    "- `description`: Description of the pipeline.\n",
    "- `pipeline_root`: The artifact repository where KFP stores a pipelineâ€™s artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "design_pipeline:helloworld",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "PIPELINE_ROOT = \"{}/pipeline_root/hello_world\".format(BUCKET_NAME)\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"hello-world\",\n",
    "    description=\"A simple intro pipeline\",\n",
    "    pipeline_root=PIPELINE_ROOT\n",
    ")\n",
    "def pipeline(text: str = \"hi there\"):\n",
    "    hello_world_task = hello_world(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "compile_pipeline:helloworld",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Compile the hello world pipeline\n",
    "\n",
    "Once the design of the pipeline is completed, the next step is to compile it. The pipeline definition is compiled into a JSON formatted file, which is transportable and can be interpreted by both KFP and Vertex AI Pipelines.\n",
    "\n",
    "You compile the pipeline with the method Compiler().compile(), with the following parameters:\n",
    "\n",
    "- `pipeline_func`: The corresponding DSL function that defines the pipeline.\n",
    "- `package_path`: The JSON file to write the transportable compiled pipeline to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "compile_pipeline:helloworld",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"hello_world.json\"\n",
    ")\n",
    "\n",
    "! cat hello_world.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_pipeline:helloworld",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Execute the hello world pipeline\n",
    "\n",
    "Now that the pipeline is compiled, you can execute by:\n",
    "\n",
    "- Create a Vertex AI PipelineJob, with the following parameters:\n",
    "    - `display_name`: The human readable name for the job.\n",
    "    - `template_path`: Thee compiled JSON pipeline definition.\n",
    "    - `pipeline_root`: Where to write output artifacts to.\n",
    "\n",
    "Click on the generated link below `INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:` to see your run in the Cloud Console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_pipeline:helloworld",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "pipeline = aip.PipelineJob(\n",
    "    display_name=\"hello_world\",\n",
    "    template_path=\"hello_world.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "\n",
    "pipeline.run()\n",
    "\n",
    "! rm hello_world.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view_pipeline_results:helloworld",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### View the hello world pipeline execution results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "view_pipeline_results:helloworld",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "PROJECT_NUMBER = pipeline.gca_resource.name.split('/')[1]\n",
    "print(PROJECT_NUMBER)\n",
    "\n",
    "def print_pipeline_output(job, output_task_name):\n",
    "    JOB_ID = job.name\n",
    "    print(JOB_ID)\n",
    "    for _ in range(len(job.gca_resource.job_detail.task_details)):\n",
    "        TASK_ID = job.gca_resource.job_detail.task_details[_].task_id\n",
    "        EXECUTE_OUTPUT = PIPELINE_ROOT + '/' + PROJECT_NUMBER + '/' + JOB_ID + '/' + output_task_name + '_' + str(TASK_ID) + '/executor_output.json'\n",
    "        if tf.io.gfile.exists(EXECUTE_OUTPUT):\n",
    "            ! gsutil cat $EXECUTE_OUTPUT\n",
    "            break\n",
    "\n",
    "    return EXECUTE_OUTPUT\n",
    "\n",
    "print_pipeline_output(pipeline, 'hello-world')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "delete_pipeline",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "### Delete a pipeline job\n",
    "\n",
    "After a pipeline job is completed, you can delete the pipeline job with the method `delete()`.  Prior to completion, a pipeline job can be canceled with the method `cancel()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "delete_pipeline",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [],
   "source": [
    "pipeline.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_component:helloworld",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Load a component from YAML definition\n",
    "\n",
    "By storing the component definition, you can share and resuse the component by loading the component from its corresponding YAML file definition:\n",
    "\n",
    "    hello_world_op = components.load_component_from_file('./hello_world.yaml').\n",
    "\n",
    "You can also use the load_component_from_url method, if your component YAML file is stored online, such as if in a git repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_component:helloworld",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "from kfp import components\n",
    "\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/hello_world-v2\".format(BUCKET_NAME)\n",
    "\n",
    "hello_world_op = components.load_component_from_file('./hello_world.yaml')\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"hello-world-v2\",\n",
    "    description=\"A simple intro pipeline\",\n",
    "    pipeline_root=PIPELINE_ROOT\n",
    ")\n",
    "def pipeline(text: str = \"hi there\"):\n",
    "    hellow_world_task = hello_world_op(text)\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"hello_world-v2.json\"\n",
    ")\n",
    "\n",
    "pipeline = aip.PipelineJob(\n",
    "    display_name=\"hello_world-v2\",\n",
    "    template_path=\"hello_world-v2.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "\n",
    "pipeline.run()\n",
    "\n",
    "! rm hello_world-v2.json hello_world.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "delete_pipeline",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "### Delete a pipeline job\n",
    "\n",
    "After a pipeline job is completed, you can delete the pipeline job with the method `delete()`.  Prior to completion, a pipeline job can be canceled with the method `cancel()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "delete_pipeline",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [],
   "source": [
    "pipeline.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_pipeline:numpy_array",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Package dependencies\n",
    "\n",
    "Each component is assembled and executed within its own container. If a component has a dependency on one or more Python packages, you specify installing the packages with the parameter `packages_to_install`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_pipeline:numpy_array",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"numpy\"])\n",
    "def numpy_mean(values: list) -> float:\n",
    "    import numpy as np\n",
    "    return np.mean(values)\n",
    "\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/numpy_mean\".format(BUCKET_NAME)\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"numpy\",\n",
    "    description=\"A simple intro pipeline\",\n",
    "    pipeline_root=PIPELINE_ROOT\n",
    ")\n",
    "def pipeline(values: list = [2,3]):\n",
    "    numpy_task = numpy_mean(values)\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"numpy_mean.json\"\n",
    ")\n",
    "\n",
    "pipeline = aip.PipelineJob(\n",
    "    display_name=\"numpy_mean\",\n",
    "    template_path=\"numpy_mean.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "\n",
    "pipeline.run()\n",
    "\n",
    "print_pipeline_output(pipeline, 'numpy-mean')\n",
    "\n",
    "! rm numpy_mean.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "delete_pipeline",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "### Delete a pipeline job\n",
    "\n",
    "After a pipeline job is completed, you can delete the pipeline job with the method `delete()`.  Prior to completion, a pipeline job can be canceled with the method `cancel()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "delete_pipeline",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [],
   "source": [
    "pipeline.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_pipeline:add_div2",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Sequential tasks in pipeline\n",
    "\n",
    "Next, you design and execute a pipeline with sequential tasks. In this example, the first task adds two integers and the second tasks divides the result (output) of the add task by 2.\n",
    "\n",
    "*Note:* The output from the add task is referenced by the property `output`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_pipeline:add_div2",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "PIPELINE_ROOT = \"{}/pipeline_root/add_div2\".format(BUCKET_NAME)\n",
    "\n",
    "@component(output_component_file=\"add.yaml\", base_image=\"python:3.9\")\n",
    "def add(v1: int, v2: int) -> int:\n",
    "    return v1 + v2\n",
    "\n",
    "@component(output_component_file=\"div2.yaml\", base_image=\"python:3.9\")\n",
    "def div_by_2(v: int) -> int:\n",
    "    return v // 2\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"add-div2\",\n",
    "    description=\"A simple intro pipeline\",\n",
    "    pipeline_root=PIPELINE_ROOT\n",
    ")\n",
    "def pipeline(v1: int = 4, v2: int = 5):\n",
    "    add_task = add(v1, v2)\n",
    "    div2_task = div_by_2(add_task.output)\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"add_div2.json\"\n",
    ")\n",
    "\n",
    "pipeline = aip.PipelineJob(\n",
    "    display_name=\"add_div2\",\n",
    "    template_path=\"add_div2.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "\n",
    "pipeline.run()\n",
    "\n",
    "print_pipeline_output(pipeline, 'div-by-2')\n",
    "\n",
    "! rm add.yaml div2.yaml add_div2.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "delete_pipeline",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "### Delete a pipeline job\n",
    "\n",
    "After a pipeline job is completed, you can delete the pipeline job with the method `delete()`.  Prior to completion, a pipeline job can be canceled with the method `cancel()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "delete_pipeline",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [],
   "source": [
    "pipeline.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_pipeline:multi_output",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Multiple output pipeline\n",
    "\n",
    "Next, you design and execute a pipeline where a first component has multiple outputs, which are then used as inputs to the next component. To distinquish between the outputs, when used as inputs to the next component, you do:\n",
    "\n",
    "1. Set the function return type to `NamedTuple`.\n",
    "2. In NamedTuple, specify a name and type for each output, in the specified order.\n",
    "3. In subsequent component, refer to the named output when using it as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_pipeline:multi_output",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "PIPELINE_ROOT = \"{}/pipeline_root/multi_output\".format(BUCKET_NAME)\n",
    "\n",
    "@component()\n",
    "def multi_output(\n",
    "    text1: str,\n",
    "    text2: str\n",
    ") -> NamedTuple(\n",
    "    \"Outputs\",\n",
    "    [\n",
    "        (\"output_1\", str),  # Return parameters\n",
    "        (\"output_2\", str),\n",
    "    ],\n",
    "):\n",
    "    output_1 = text1 + ' '\n",
    "    output_2 = text2\n",
    "    return (output_1, output_2)\n",
    "\n",
    "@component()\n",
    "def concat(\n",
    "    text1: str,\n",
    "    text2: str\n",
    ") -> str:\n",
    "    return text1 + text2\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"multi-output\",\n",
    "    description=\"A simple intro pipeline\",\n",
    "    pipeline_root=PIPELINE_ROOT\n",
    ")\n",
    "def pipeline(text1: str = \"hello\", text2: str = \"world\"):\n",
    "    multi_output_task = multi_output(text1, text2)\n",
    "    concat_task = concat(\n",
    "        multi_output_task.outputs[\"output_1\"],\n",
    "        multi_output_task.outputs[\"output_2\"],\n",
    "    )\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"multi_output.json\"\n",
    ")\n",
    "\n",
    "pipeline = aip.PipelineJob(\n",
    "    display_name=\"multi-output\",\n",
    "    template_path=\"multi_output.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "\n",
    "pipeline.run()\n",
    "\n",
    "print_pipeline_output(pipeline, 'concat')\n",
    "\n",
    "! rm multi_output.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "delete_pipeline",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "### Delete a pipeline job\n",
    "\n",
    "After a pipeline job is completed, you can delete the pipeline job with the method `delete()`.  Prior to completion, a pipeline job can be canceled with the method `cancel()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "delete_pipeline",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [],
   "source": [
    "pipeline.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_pipeline:parallel",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Parallel tasks in component\n",
    "\n",
    "Next, you design and execute a pipeline with parallel tasks. In this example, one parallel task adds up a list of integers and another substracts them. Note that the compiler knows these two tasks can be ran in parallel, because their input is not dependent on the output of the other task.\n",
    "\n",
    "Finally, the add task waits on the two parallel tasks to complete, and then adds together the two outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_pipeline:parallel",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "PIPELINE_ROOT = \"{}/pipeline_root/parallel\".format(BUCKET_NAME)\n",
    "\n",
    "@component()\n",
    "def add_list(values: list) -> int:\n",
    "    ret = 0\n",
    "    for value in values:\n",
    "        ret += 1\n",
    "    return ret\n",
    "\n",
    "@component()\n",
    "def sub_list(values: list) -> int:\n",
    "    ret = 0\n",
    "    for value in values:\n",
    "        ret -= 1\n",
    "    return ret\n",
    "\n",
    "@component()\n",
    "def add(value1: int, value2: int) -> int:\n",
    "    return value1 + value2\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"parallel\",\n",
    "    description=\"A simple intro pipeline\",\n",
    "    pipeline_root=PIPELINE_ROOT\n",
    ")\n",
    "def pipeline(values: list = [1, 2, 3]):\n",
    "    add_list_task = add_list(values)\n",
    "    sub_list_task = sub_list(values)\n",
    "    add_task = add(add_list_task.output, sub_list_task.output)\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"parallel.json\"\n",
    ")\n",
    "\n",
    "pipeline = aip.PipelineJob(\n",
    "    display_name=\"parallel\",\n",
    "    template_path=\"parallel.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "\n",
    "pipeline.run()\n",
    "\n",
    "print_pipeline_output(pipeline, 'add')\n",
    "\n",
    "! rm parallel.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "delete_pipeline",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "### Delete a pipeline job\n",
    "\n",
    "After a pipeline job is completed, you can delete the pipeline job with the method `delete()`.  Prior to completion, a pipeline job can be canceled with the method `cancel()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "delete_pipeline",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [],
   "source": [
    "pipeline.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfp_controlflow",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Control flow in pipeline\n",
    "\n",
    "While Python control statements, e.g., if/else, for, can be used in a component, they cannot be used in the pipeline function. Each task in the pipeline function runs as a node in a graph. Thus a control flow statement also has to run as a graph node. To support this, KFP provides a set of DSL statements that implement control flow as a graph node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_pipeline:parallelfor",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### dsl.ParallelFor\n",
    "\n",
    "The statement `dsl.ParallelFor()` implements a for loop, where each iteration in the for loop runs in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_pipeline:parallelfor",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "PIPELINE_ROOT = \"{}/pipeline_root/parallel_for\".format(BUCKET_NAME)\n",
    "\n",
    "@component()\n",
    "def double(val: int) -> int:\n",
    "    return val * 2\n",
    "\n",
    "@component\n",
    "def echo (val: int) -> int:\n",
    "    return val\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"parallel-for\",\n",
    "    description=\"A simple intro pipeline\",\n",
    "    pipeline_root=PIPELINE_ROOT\n",
    ")\n",
    "def pipeline(values: list = [1, 2, 3]):\n",
    "    with dsl.ParallelFor(values) as item:\n",
    "        output = double(item).output\n",
    "        echo_task = echo(output)\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"parallel_for.json\"\n",
    ")\n",
    "\n",
    "pipeline = aip.PipelineJob(\n",
    "    display_name=\"parallel-for\",\n",
    "    template_path=\"parallel_for.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "\n",
    "pipeline.run()\n",
    "\n",
    "print_pipeline_output(pipeline, 'echo')\n",
    "\n",
    "! rm parallel_for.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "delete_pipeline",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "### Delete a pipeline job\n",
    "\n",
    "After a pipeline job is completed, you can delete the pipeline job with the method `delete()`.  Prior to completion, a pipeline job can be canceled with the method `cancel()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "delete_pipeline",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [],
   "source": [
    "pipeline.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_pipeline:condition",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### dsl.Condition\n",
    "\n",
    "The statement `dsl.Condition()` implements an `if` statement. There is no support for an `else` or `elif` statement. You use a separate `dsl.Condition()` for each value you want to test for. For example, if the output from a task is `True` or `False`, you will have two `dsl.Condition()` statements, one for True and one for False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_pipeline:condition",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "@component()\n",
    "def flip() -> int:\n",
    "    import random\n",
    "    return random.randint(0, 1)\n",
    "\n",
    "@component()\n",
    "def heads() -> bool:\n",
    "    print(\"heads\")\n",
    "    return True\n",
    "\n",
    "@component()\n",
    "def tails() -> bool:\n",
    "    print(\"tails\")\n",
    "    return False\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"condition\",\n",
    "    description=\"A simple intro pipeline\",\n",
    "    pipeline_root=PIPELINE_ROOT\n",
    ")\n",
    "def pipeline():\n",
    "    flip_task = flip()\n",
    "    with dsl.Condition(flip_task.output == 1, name=\"true_clause\"):\n",
    "        task = heads()\n",
    "    with dsl.Condition(flip_task.output == 0, name=\"false_clause\"):\n",
    "        task = tails()\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"condition.json\"\n",
    ")\n",
    "\n",
    "pipeline = aip.PipelineJob(\n",
    "    display_name=\"condition\",\n",
    "    template_path=\"condition.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "\n",
    "pipeline.run()\n",
    "\n",
    "print_pipeline_output(pipeline, 'flip')\n",
    "\n",
    "! rm condition.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "delete_pipeline",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "### Delete a pipeline job\n",
    "\n",
    "After a pipeline job is completed, you can delete the pipeline job with the method `delete()`.  Prior to completion, a pipeline job can be canceled with the method `cancel()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "delete_pipeline",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [],
   "source": [
    "pipeline.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pipeline_errata",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Errata\n",
    "\n",
    "### Caching in pipeline components\n",
    "\n",
    "When running a pipeline with Vertex AI Pipelines, the outcome state of each task is cached. With caching, if the pipeline is ran again, and the compiled definition of the task and state has not changed, the cached output will be used instead of running the task again.\n",
    "\n",
    "To override caching, i.e., forceable run the task, you set the parameter `enable_caching` to `False` when creating the Vertex AI Pipeline job.\n",
    "\n",
    "```\n",
    "pipeline = aip.PipelineJob(\n",
    "    display_name=\"example\",\n",
    "    template_path=\"example.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching=False\n",
    ")\n",
    "```\n",
    "\n",
    "### Asynchronous execution of pipeline\n",
    "\n",
    "When running a pipeline with the method `run()`, the pipeline is ran synchronously. To run asynchronously, you use the method `submit()`. Once the job has started, your Python script can continue to execute. Then when you need to block execution using the method `wait()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:mbsdk",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "# Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n",
    "- Dataset\n",
    "- Pipeline\n",
    "- Model\n",
    "- Endpoint\n",
    "- AutoML Training Job\n",
    "- Batch Job\n",
    "- Custom Job\n",
    "- Hyperparameter Tuning Job\n",
    "- Cloud Storage Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup:mbsdk",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [],
   "source": [
    "delete_all = True\n",
    "\n",
    "if delete_all:\n",
    "    # Delete the dataset using the Vertex dataset object\n",
    "    try:\n",
    "        if 'dataset' in globals():\n",
    "            dataset.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the model using the Vertex model object\n",
    "    try:\n",
    "        if 'model' in globals():\n",
    "            model.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the endpoint using the Vertex endpoint object\n",
    "    try:\n",
    "        if 'endpoint' in globals():\n",
    "            endpoint.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the AutoML or Pipeline training job\n",
    "    try:\n",
    "        if 'dag' in globals():\n",
    "            dag.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the custom training job\n",
    "    try:\n",
    "        if 'job' in globals():\n",
    "            job.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the batch prediction job using the Vertex batch prediction object\n",
    "    try:\n",
    "        if 'batch_predict_job' in globals():\n",
    "            batch_predict_job.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the hyperparameter tuning job using the Vertex hyperparameter tuning object\n",
    "    try:\n",
    "        if 'hpt_job' in globals():\n",
    "            hpt_job.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    if 'BUCKET_NAME' in globals():\n",
    "        ! gsutil rm -r $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m71"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vars": {
   "AIP": "AI Platform",
   "AUTOML": "AutoML",
   "AUTOML_URL": "https://cloud.google.com/vertex-ai/docs/start/automl-users",
   "BQ": "BigQuery",
   "COLAB": "Colab",
   "DOCKER": "Docker",
   "EDGE": "Edge",
   "EMAIL": "cdpe@gmail.com",
   "GAPIC": "client library",
   "GAPICP": "SDK",
   "GCONSOLE": "Cloud Console",
   "GCP": "Google Cloud",
   "GCS": "Cloud Storage",
   "GNOTEBOOK": "Google Cloud Notebook",
   "NOTEBOOK": "ml_ops_stage3/get_started_with_kubeflow_pipelines.ipynb",
   "REGION": "us-central1",
   "REPO": "vertex-ai-samples/tree/master/notebooks/official/automl",
   "SDKP": "SDK for Python",
   "STAGE": "3 : formalization: get started with Kubeflow Pipelines",
   "TENSORFLOW": "TensorFlow",
   "TFLite": "TFLite",
   "TFServing": "TF Serving",
   "TITLE": "E2E ML on GCP: MLOps stage 3 : formalization: get started with Kubeflow Pipelines",
   "VERTEX": "Vertex AI",
   "VERTEX_AI": "Vertex AI",
   "YEAR": "2021",
   "null": "null",
   "uCAIP": "Vertex"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

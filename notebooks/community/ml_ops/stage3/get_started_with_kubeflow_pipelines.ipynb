{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "copyright"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title:generic,gcp"
      },
      "source": [
        "# E2E ML on GCP: MLOps stage 3 : formalization: get started with Kubeflow Pipelines\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/get_started_with_kubeflow_pipelines.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "    <td>\n",
        "        <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/get_started_with_kubeflow_pipelines.ipynb\">\n",
        "        <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\\\" alt=\"Colab logo\"> Run in Colab\n",
        "        </a>\n",
        "  </td>\n",
        "    \n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/get_started_with_kubeflow_pipelines.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "<br/><br/><br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview:mlops"
      },
      "source": [
        "## Overview\n",
        "\n",
        "\n",
        "This tutorial demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. This tutorial covers stage 3 : formalization: get started with Kubeflow Pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "objective:mlops,stage3,get_started_kubeflow_pipelines"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to use `Kubeflow Pipelines`(KFP).\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services:\n",
        "\n",
        "- `Vertex AI Pipelines`\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Building KFP lightweight Python function components.\n",
        "- Assembling and compiling KFP components into a pipeline.\n",
        "- Executing a KFP pipeline using Vertex AI Pipelines.\n",
        "- Loading component and pipeline definitions from a source code repository.\n",
        "- Building sequential, parallel, multiple output components.\n",
        "- Building control flow into pipelines.\n",
        "\n",
        "### Costs\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "- Vertex AI\n",
        "- Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage pricing](https://cloud.google.com/storage/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_mlops"
      },
      "source": [
        "## Installations\n",
        "\n",
        "Install the following packages for executing this MLOps notebooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_mlops"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\"\n",
        "    \n",
        "! pip3 install tensorflow-io==0.18 $USER_FLAG -q\n",
        "! pip3 install --upgrade google-cloud-aiplatform \\\n",
        "                        pyarrow \\\n",
        "                        kfp $USER_FLAG -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "restart"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "restart"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "project_id"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_project_id"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_project_id"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID:\", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_gcloud_project_id"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
        "\n",
        "- Americas: `us-central1`\n",
        "- Europe: `europe-west4`\n",
        "- Asia Pacific: `asia-east1`\n",
        "\n",
        "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
        "\n",
        "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "region"
      },
      "outputs": [],
      "source": [
        "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
        "\n",
        "if REGION == \"[your-region]\":\n",
        "    REGION = \"us-central1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "timestamp"
      },
      "source": [
        "#### Timestamp\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "timestamp"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5627478895e"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using Google Cloud Notebooks**, your environment is already authenticated. Skip this step.\n",
        "\n",
        "**If you are using Colab**, run the cell below and follow the instructions when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "In the Cloud Console, go to the [Create service account key](https://console.cloud.google.com/apis/credentials/serviceaccountkey) page.\n",
        "\n",
        "1. **Click Create service account**.\n",
        "\n",
        "2. In the **Service account name** field, enter a name, and click **Create**.\n",
        "\n",
        "3. In the **Grant this service account access to project** section, click the Role drop-down list. Type \"Vertex\" into the filter box, and select **Vertex Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "\n",
        "4. Click Create. A JSON file that contains your key downloads to your local environment.\n",
        "\n",
        "5. Enter the path to your service account key as the GOOGLE_APPLICATION_CREDENTIALS variable in the cell below and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49ee8894d674"
      },
      "outputs": [],
      "source": [
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# If on Google Cloud Notebook, then don't execute this code\n",
        "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bucket:mbsdk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "When you initialize the Vertex SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bucket"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
        "BUCKET_URI = \"gs://{}\".format(BUCKET_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_bucket"
      },
      "outputs": [],
      "source": [
        "if BUCKET_URI == \"\" or BUCKET_URI is None or BUCKET_URI == \"gs://[your-bucket-name]\":\n",
        "    BUCKET_URI = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_bucket"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "validate_bucket"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "validate_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "set_service_account"
      },
      "source": [
        "#### Service Account\n",
        "\n",
        "You use a service account to create Vertex AI Pipeline jobs. If you do not want to use your project's Compute Engine service account, set `SERVICE_ACCOUNT` to another service account ID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_service_account"
      },
      "outputs": [],
      "source": [
        "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_service_account"
      },
      "outputs": [],
      "source": [
        "if (\n",
        "    SERVICE_ACCOUNT == \"\"\n",
        "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
        "    or SERVICE_ACCOUNT is None\n",
        "):\n",
        "    shell_output = ! gcloud projects describe $PROJECT_ID | sed -nre 's:.*projectNumber\\: (.*):\\1:p'\n",
        "    SERVICE_ACCOUNT = (\n",
        "        shell_output[0].replace(\"'\", \"\") + \"-compute@developer.gserviceaccount.com\"\n",
        "    )\n",
        "\n",
        "print(\"Service Account:\", SERVICE_ACCOUNT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "set_service_account:pipelines"
      },
      "source": [
        "#### Set service account access for Vertex AI Pipelines\n",
        "\n",
        "Run the following commands to grant your service account access to read and write pipeline artifacts in the bucket that you created in the previous step. You only need to run this step once per service account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_service_account:pipelines"
      },
      "outputs": [],
      "source": [
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
        "\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_vars"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "from typing import NamedTuple\n",
        "\n",
        "import google.cloud.aiplatform as aiplatform\n",
        "import tensorflow as tf\n",
        "from kfp import dsl\n",
        "from kfp.v2 import compiler\n",
        "from kfp.v2.dsl import component"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pipelines_intro"
      },
      "source": [
        "## Pipeline introduction\n",
        "\n",
        "Vertex AI Pipelines lets you orchestrate your machine learning (ML) workflows in a serverless manner. Pipelines are re-usable, and their executions and artifact generation can be tracked by Vertex AI Experiments and Vertex AI ML Metadata. With pipelines, you do the following:\n",
        "\n",
        "    1. Design the pipeline workflow.\n",
        "    2. Compile the pipeline.\n",
        "    3. Schedule pipeline execution (or run now).\n",
        "    4. Get the pipeline results.\n",
        "\n",
        "Pipelines are designed using domain specific language (DSL). Vertex AI Pipelines support both KFP DSL and TFX DSL for designing pipelines.\n",
        "\n",
        "In addition to designing components, you can use a wide variety of pre-built Google Cloud Pipeline Components for Vertex AI services.\n",
        "\n",
        "Learn more about [Building a pipeline](https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pipelines_intro:helloworld"
      },
      "source": [
        "## Basic pipeline\n",
        "\n",
        "This step demonstrates the basics of constructing and executing a pipeline. You do the following:\n",
        "\n",
        "1. Design a simple Python function based component to output the input string.\n",
        "2. Construct a pipeline that uses the component.\n",
        "2. Compile the pipeline.\n",
        "3. Execute the pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "design_component:helloworld"
      },
      "source": [
        "### Design hello world component\n",
        "\n",
        "To create a KFP component from a Python function, you add the KFP DSL decorator `@component` to the function. In this example, the decorator takes the following parameters:\n",
        "\n",
        "- `output_component_file`(optional): write the component description to a YAML file such that the component is portable.\n",
        "- `base_image`(optional): The interpreter for executing the Python function. By default it is Python 3.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "design_component:helloworld"
      },
      "outputs": [],
      "source": [
        "@component(output_component_file=\"hello_world.yaml\", base_image=\"python:3.9\")\n",
        "def hello_world(text: str) -> str:\n",
        "    print(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "! cat hello_world.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "design_pipeline:helloworld"
      },
      "source": [
        "### Design the hello world pipeline\n",
        "\n",
        "Next, you design the pipeline for running the hello world component. A pipeline is specified as a Python function with the KFP DSL decorator `@dsl.component`, with the following parameters:\n",
        "\n",
        "- `name`: Name of the pipeline.\n",
        "- `description`: Description of the pipeline.\n",
        "- `pipeline_root`: The artifact repository where KFP stores a pipeline’s artifacts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "design_pipeline:helloworld"
      },
      "outputs": [],
      "source": [
        "PIPELINE_ROOT = \"{}/pipeline_root/hello_world\".format(BUCKET_URI)\n",
        "\n",
        "\n",
        "@dsl.pipeline(\n",
        "    name=\"hello-world\",\n",
        "    description=\"A simple intro pipeline\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        ")\n",
        "def pipeline(text: str = \"hi there\"):\n",
        "    hello_world_task = hello_world(text)\n",
        "    return hello_world_task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "compile_pipeline:helloworld"
      },
      "source": [
        "### Compile the hello world pipeline\n",
        "\n",
        "Once the design of the pipeline is completed, the next step is to compile it. The pipeline definition is compiled into a JSON formatted file, which is transportable and can be interpreted by both KFP and Vertex AI Pipelines.\n",
        "\n",
        "Compile the pipeline with the Compiler().compile() method using the following parameters:\n",
        "\n",
        "- `pipeline_func`: The corresponding DSL function that defines the pipeline.\n",
        "- `package_path`: The JSON file to write the transportable compiled pipeline to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "compile_pipeline:helloworld"
      },
      "outputs": [],
      "source": [
        "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"hello_world.json\")\n",
        "\n",
        "! cat hello_world.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_pipeline:helloworld"
      },
      "source": [
        "### Execute the hello world pipeline\n",
        "\n",
        "Now that the pipeline is compiled, you can execute it by:\n",
        "\n",
        "- Creating a Vertex AI PipelineJob with the following parameters:\n",
        "    - `display_name`: The human readable name for the job.\n",
        "    - `template_path`: The compiled JSON pipeline definition.\n",
        "    - `pipeline_root`: Where to write output artifacts to.\n",
        "\n",
        "Click on the generated link below `INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:` to see your job run in the Cloud Console."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_pipeline:helloworld"
      },
      "outputs": [],
      "source": [
        "pipeline = aiplatform.PipelineJob(\n",
        "    display_name=\"hello_world\",\n",
        "    template_path=\"hello_world.json\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        ")\n",
        "\n",
        "pipeline.run()\n",
        "\n",
        "! rm hello_world.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view_pipeline_results:helloworld"
      },
      "source": [
        "### View the hello world pipeline execution results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "view_pipeline_results:helloworld"
      },
      "outputs": [],
      "source": [
        "PROJECT_NUMBER = pipeline.gca_resource.name.split(\"/\")[1]\n",
        "print(PROJECT_NUMBER)\n",
        "\n",
        "\n",
        "def print_pipeline_output(job, output_task_name):\n",
        "    JOB_ID = job.name\n",
        "    print(JOB_ID)\n",
        "    for _ in range(len(job.gca_resource.job_detail.task_details)):\n",
        "        TASK_ID = job.gca_resource.job_detail.task_details[_].task_id\n",
        "        EXECUTE_OUTPUT = (\n",
        "            PIPELINE_ROOT\n",
        "            + \"/\"\n",
        "            + PROJECT_NUMBER\n",
        "            + \"/\"\n",
        "            + JOB_ID\n",
        "            + \"/\"\n",
        "            + output_task_name\n",
        "            + \"_\"\n",
        "            + str(TASK_ID)\n",
        "            + \"/executor_output.json\"\n",
        "        )\n",
        "        GCP_RESOURCES = (\n",
        "            PIPELINE_ROOT\n",
        "            + \"/\"\n",
        "            + PROJECT_NUMBER\n",
        "            + \"/\"\n",
        "            + JOB_ID\n",
        "            + \"/\"\n",
        "            + output_task_name\n",
        "            + \"_\"\n",
        "            + str(TASK_ID)\n",
        "            + \"/gcp_resources\"\n",
        "        )\n",
        "        EVAL_METRICS = (\n",
        "            PIPELINE_ROOT\n",
        "            + \"/\"\n",
        "            + PROJECT_NUMBER\n",
        "            + \"/\"\n",
        "            + JOB_ID\n",
        "            + \"/\"\n",
        "            + output_task_name\n",
        "            + \"_\"\n",
        "            + str(TASK_ID)\n",
        "            + \"/evaluation_metrics\"\n",
        "        )\n",
        "        if tf.io.gfile.exists(EXECUTE_OUTPUT):\n",
        "            ! gsutil cat $EXECUTE_OUTPUT\n",
        "            return EXECUTE_OUTPUT\n",
        "        elif tf.io.gfile.exists(GCP_RESOURCES):\n",
        "            ! gsutil cat $GCP_RESOURCES\n",
        "            return GCP_RESOURCES\n",
        "        elif tf.io.gfile.exists(EVAL_METRICS):\n",
        "            ! gsutil cat $EVAL_METRICS\n",
        "            return EVAL_METRICS\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "print_pipeline_output(pipeline, \"hello-world\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "delete_pipeline"
      },
      "source": [
        "### Delete a pipeline job\n",
        "\n",
        "After a pipeline job is completed, you can delete the pipeline job with the `delete()` method. Prior to completion, a pipeline job can be canceled with the method `cancel()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "delete_pipeline"
      },
      "outputs": [],
      "source": [
        "pipeline.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_component:helloworld"
      },
      "source": [
        "### Load a component from YAML definition\n",
        "\n",
        "By storing the component definition, you can share and resuse the component by loading the component from its corresponding YAML file definition:\n",
        "\n",
        "    hello_world_op = components.load_component_from_file('./hello_world.yaml').\n",
        "\n",
        "You can also use the `load_component_from_url` method, if your component YAML file is stored online, such as in a git repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_component:helloworld"
      },
      "outputs": [],
      "source": [
        "from kfp import components\n",
        "\n",
        "PIPELINE_ROOT = \"{}/pipeline_root/hello_world-v2\".format(BUCKET_URI)\n",
        "\n",
        "hello_world_op = components.load_component_from_file(\"./hello_world.yaml\")\n",
        "\n",
        "\n",
        "@dsl.pipeline(\n",
        "    name=\"hello-world-v2\",\n",
        "    description=\"A simple intro pipeline\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        ")\n",
        "def pipeline(text: str = \"hi there\"):\n",
        "    hello_world_task = hello_world_op(text)\n",
        "    return hello_world_task\n",
        "\n",
        "\n",
        "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"hello_world-v2.json\")\n",
        "\n",
        "pipeline = aiplatform.PipelineJob(\n",
        "    display_name=\"hello_world-v2\",\n",
        "    template_path=\"hello_world-v2.json\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        ")\n",
        "\n",
        "pipeline.run()\n",
        "\n",
        "! rm hello_world-v2.json hello_world.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "delete_pipeline"
      },
      "source": [
        "### Delete a pipeline job\n",
        "\n",
        "After a pipeline job is completed, you can delete the pipeline job with the `delete()` method. Prior to completion, a pipeline job can be canceled with the method `cancel()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "delete_pipeline"
      },
      "outputs": [],
      "source": [
        "pipeline.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_component_pipeline_git:helloworld"
      },
      "source": [
        "### Loading components and pipeline YAML definitions from source control\n",
        "\n",
        "By storing the component and pipeline definitions in a source repository, like Github, you can version control your components and pipelines, as follows:\n",
        "\n",
        "- Use the method `load_component_from_url()`.\n",
        "\n",
        "- Pull the raw file format version from the repo. For github, that will be in the form of:\n",
        "\n",
        "    https://raw.githubusercontent.com/....\n",
        "\n",
        "- Specify the version of the component/pipeline. For github, that will be the branch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_component_pipeline_git:helloworld"
      },
      "outputs": [],
      "source": [
        "VERSION = \"main\"\n",
        "hello_world_op = components.load_component_from_url(\n",
        "    f\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/{VERSION}/notebooks/community/ml_ops/stage3/src/hello_world.yaml\"\n",
        ")\n",
        "\n",
        "! wget https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/{VERSION}/notebooks/community/ml_ops/stage3/src/hello_world.json -O hello_git_example.json\n",
        "\n",
        "pipeline = aiplatform.PipelineJob(\n",
        "    display_name=\"hello_world-git\",\n",
        "    template_path=\"hello_git_example.json\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        ")\n",
        "\n",
        "pipeline.run()\n",
        "\n",
        "! rm -f hello_git_example.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "delete_pipeline"
      },
      "source": [
        "### Delete a pipeline job\n",
        "\n",
        "After a pipeline job is completed, you can delete the pipeline job with the `delete()` method. Prior to completion, a pipeline job can be canceled with the method `cancel()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "delete_pipeline"
      },
      "outputs": [],
      "source": [
        "pipeline.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_pipeline:numpy_array"
      },
      "source": [
        "### Package dependencies\n",
        "\n",
        "Each component is assembled and executed within its own container. If a component has a dependency on one or more Python packages, you specify installing the packages with the parameter `packages_to_install`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_pipeline:numpy_array"
      },
      "outputs": [],
      "source": [
        "@component(packages_to_install=[\"numpy\"])\n",
        "def numpy_mean(values: list) -> float:\n",
        "    import numpy as np\n",
        "\n",
        "    return np.mean(values)\n",
        "\n",
        "\n",
        "PIPELINE_ROOT = \"{}/pipeline_root/numpy_mean\".format(BUCKET_URI)\n",
        "\n",
        "\n",
        "@dsl.pipeline(\n",
        "    name=\"numpy\", description=\"A simple intro pipeline\", pipeline_root=PIPELINE_ROOT\n",
        ")\n",
        "def pipeline(values: list = [2, 3]):\n",
        "    numpy_task = numpy_mean(values)\n",
        "    return numpy_task\n",
        "\n",
        "\n",
        "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"numpy_mean.json\")\n",
        "\n",
        "pipeline = aiplatform.PipelineJob(\n",
        "    display_name=\"numpy_mean\",\n",
        "    template_path=\"numpy_mean.json\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        ")\n",
        "\n",
        "pipeline.run()\n",
        "\n",
        "print_pipeline_output(pipeline, \"numpy-mean\")\n",
        "\n",
        "! rm numpy_mean.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "delete_pipeline"
      },
      "source": [
        "### Delete a pipeline job\n",
        "\n",
        "After a pipeline job is completed, you can delete the pipeline job with the `delete()` method. Prior to completion, a pipeline job can be canceled with the method `cancel()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "delete_pipeline"
      },
      "outputs": [],
      "source": [
        "pipeline.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_pipeline:add_div2"
      },
      "source": [
        "## Sequential tasks in pipeline\n",
        "\n",
        "Next, you design and execute a pipeline with sequential tasks. In this example, the first task adds two integers and the second tasks divides the result (output) of the add task by 2.\n",
        "\n",
        "*Note:* The output from the add task is referenced by the property `output`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_pipeline:add_div2"
      },
      "outputs": [],
      "source": [
        "PIPELINE_ROOT = \"{}/pipeline_root/add_div2\".format(BUCKET_URI)\n",
        "\n",
        "\n",
        "@component(output_component_file=\"add.yaml\", base_image=\"python:3.9\")\n",
        "def add(v1: int, v2: int) -> int:\n",
        "    return v1 + v2\n",
        "\n",
        "\n",
        "@component(output_component_file=\"div2.yaml\", base_image=\"python:3.9\")\n",
        "def div_by_2(v: int) -> int:\n",
        "    return v // 2\n",
        "\n",
        "\n",
        "@dsl.pipeline(\n",
        "    name=\"add-div2\", description=\"A simple intro pipeline\", pipeline_root=PIPELINE_ROOT\n",
        ")\n",
        "def pipeline(v1: int = 4, v2: int = 5):\n",
        "    add_task = add(v1, v2)\n",
        "    div2_task = div_by_2(add_task.output)\n",
        "    return div2_task\n",
        "\n",
        "\n",
        "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"add_div2.json\")\n",
        "\n",
        "pipeline = aiplatform.PipelineJob(\n",
        "    display_name=\"add_div2\",\n",
        "    template_path=\"add_div2.json\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        ")\n",
        "\n",
        "pipeline.run()\n",
        "\n",
        "print_pipeline_output(pipeline, \"div-by-2\")\n",
        "\n",
        "! rm add.yaml div2.yaml add_div2.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "delete_pipeline"
      },
      "source": [
        "### Delete a pipeline job\n",
        "\n",
        "After a pipeline job is completed, you can delete the pipeline job with the `delete()` method. Prior to completion, a pipeline job can be canceled with the method `cancel()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "delete_pipeline"
      },
      "outputs": [],
      "source": [
        "pipeline.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_pipeline:multi_output"
      },
      "source": [
        "### Multiple output pipeline\n",
        "\n",
        "Next, you design and execute a pipeline where a first component has multiple outputs, which are then used as inputs to the next component. To distinguish between the outputs, when used as inputs to the next component, you follow:\n",
        "\n",
        "1. Set the function return type to `NamedTuple`.\n",
        "2. In NamedTuple, specify a name and type for each output, in the specified order.\n",
        "3. In subsequent component, refer to the named output when using it as input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_pipeline:multi_output"
      },
      "outputs": [],
      "source": [
        "PIPELINE_ROOT = \"{}/pipeline_root/multi_output\".format(BUCKET_URI)\n",
        "\n",
        "\n",
        "@component()\n",
        "def multi_output(\n",
        "    text1: str, text2: str\n",
        ") -> NamedTuple(\n",
        "    \"Outputs\",\n",
        "    [\n",
        "        (\"output_1\", str),  # Return parameters\n",
        "        (\"output_2\", str),\n",
        "    ],\n",
        "):\n",
        "    output_1 = text1 + \" \"\n",
        "    output_2 = text2\n",
        "    return (output_1, output_2)\n",
        "\n",
        "\n",
        "@component()\n",
        "def concat(text1: str, text2: str) -> str:\n",
        "    return text1 + text2\n",
        "\n",
        "\n",
        "@dsl.pipeline(\n",
        "    name=\"multi-output\",\n",
        "    description=\"A simple intro pipeline\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        ")\n",
        "def pipeline(text1: str = \"hello\", text2: str = \"world\"):\n",
        "    multi_output_task = multi_output(text1, text2)\n",
        "    concat_task = concat(\n",
        "        multi_output_task.outputs[\"output_1\"],\n",
        "        multi_output_task.outputs[\"output_2\"],\n",
        "    )\n",
        "    return concat_task\n",
        "\n",
        "\n",
        "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"multi_output.json\")\n",
        "\n",
        "pipeline = aiplatform.PipelineJob(\n",
        "    display_name=\"multi-output\",\n",
        "    template_path=\"multi_output.json\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        ")\n",
        "\n",
        "pipeline.run()\n",
        "\n",
        "print_pipeline_output(pipeline, \"concat\")\n",
        "\n",
        "! rm multi_output.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "delete_pipeline"
      },
      "source": [
        "### Delete a pipeline job\n",
        "\n",
        "After a pipeline job is completed, you can delete the pipeline job with the `delete()` method. Prior to completion, a pipeline job can be canceled with the method `cancel()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "delete_pipeline"
      },
      "outputs": [],
      "source": [
        "pipeline.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_pipeline:parallel"
      },
      "source": [
        "## Parallel tasks in component\n",
        "\n",
        "Next, you design and execute a pipeline with parallel tasks. In this example, one parallel task adds up a list of integers and another substracts them. Note that the compiler knows these two tasks can be run in parallel, because their input is not dependent on the output of the other task.\n",
        "\n",
        "Finally, the `add_int` task waits on the two parallel tasks to complete, and then adds together the two outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_pipeline:parallel"
      },
      "outputs": [],
      "source": [
        "PIPELINE_ROOT = \"{}/pipeline_root/parallel\".format(BUCKET_URI)\n",
        "\n",
        "\n",
        "@component()\n",
        "def add_list(values: list) -> int:\n",
        "    ret = 0\n",
        "    for value in values:\n",
        "        ret = value + ret\n",
        "    return ret\n",
        "\n",
        "\n",
        "@component()\n",
        "def sub_list(values: list) -> int:\n",
        "    ret = 0\n",
        "    for value in values:\n",
        "        ret = value - ret\n",
        "    return ret\n",
        "\n",
        "\n",
        "@component()\n",
        "def add_int(value1: int, value2: int) -> int:\n",
        "    return value1 + value2\n",
        "\n",
        "\n",
        "@dsl.pipeline(\n",
        "    name=\"parallel\", description=\"A simple intro pipeline\", pipeline_root=PIPELINE_ROOT\n",
        ")\n",
        "def pipeline(values: list = [1, 2, 3]):\n",
        "    add_list_task = add_list(values)\n",
        "    sub_list_task = sub_list(values)\n",
        "    add_task = add_int(add_list_task.output, sub_list_task.output)\n",
        "    return add_task\n",
        "\n",
        "\n",
        "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"parallel.json\")\n",
        "\n",
        "pipeline = aiplatform.PipelineJob(\n",
        "    display_name=\"parallel\",\n",
        "    template_path=\"parallel.json\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        ")\n",
        "\n",
        "pipeline.run()\n",
        "\n",
        "print_pipeline_output(pipeline, \"add\")\n",
        "\n",
        "! rm parallel.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "delete_pipeline"
      },
      "source": [
        "### Delete a pipeline job\n",
        "\n",
        "After a pipeline job is completed, you can delete the pipeline job with the `delete()` method. Prior to completion, a pipeline job can be canceled with the method `cancel()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "delete_pipeline"
      },
      "outputs": [],
      "source": [
        "pipeline.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfp_controlflow"
      },
      "source": [
        "## Control flow in pipeline\n",
        "\n",
        "While Python control statements(e.g., if/else, for) can be used in a component, they cannot be used in a pipeline function. Each task in a pipeline function runs as a node in a graph. Thus a control flow statement also has to run as a graph node. To support this, KFP provides a set of DSL statements that implement control flow as a graph node."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_pipeline:parallelfor"
      },
      "source": [
        "### dsl.ParallelFor\n",
        "\n",
        "The statement `dsl.ParallelFor()` implements a `for` loop, where each iteration in the `for` loop runs in parallel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_pipeline:parallelfor"
      },
      "outputs": [],
      "source": [
        "PIPELINE_ROOT = \"{}/pipeline_root/parallel_for\".format(BUCKET_URI)\n",
        "\n",
        "\n",
        "@component()\n",
        "def double(val: int) -> int:\n",
        "    return val * 2\n",
        "\n",
        "\n",
        "@component\n",
        "def echo(val: int) -> int:\n",
        "    return val\n",
        "\n",
        "\n",
        "@dsl.pipeline(\n",
        "    name=\"parallel-for\",\n",
        "    description=\"A simple intro pipeline\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        ")\n",
        "def pipeline(values: list = [1, 2, 3]):\n",
        "    with dsl.ParallelFor(values) as item:\n",
        "        output = double(item).output\n",
        "        echo_task = echo(output)\n",
        "    return echo_task\n",
        "\n",
        "\n",
        "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"parallel_for.json\")\n",
        "\n",
        "pipeline = aiplatform.PipelineJob(\n",
        "    display_name=\"parallel-for\",\n",
        "    template_path=\"parallel_for.json\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        ")\n",
        "\n",
        "pipeline.run()\n",
        "\n",
        "print_pipeline_output(pipeline, \"echo\")\n",
        "\n",
        "! rm parallel_for.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "delete_pipeline"
      },
      "source": [
        "### Delete a pipeline job\n",
        "\n",
        "After a pipeline job is completed, you can delete the pipeline job with the `delete()` method. Prior to completion, a pipeline job can be canceled with the method `cancel()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "delete_pipeline"
      },
      "outputs": [],
      "source": [
        "pipeline.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_pipeline:condition"
      },
      "source": [
        "### dsl.Condition\n",
        "\n",
        "The statement `dsl.Condition()` implements an `if` statement. There is no support for an `else` or `elif` statement. You use a separate `dsl.Condition()` for each value you want to test for. For example, if the output from a task is `1` or `0`, you will have two `dsl.Condition()` statements, one for 1 and one for 0.\n",
        "\n",
        "The condition in `dsl.Condition()` is evaluated at run-time, not compile time. As such it is not Python code anymore.  The condition is of type `ConditionOperator`. This operator has three parts:\n",
        "\n",
        "1. PipelineParam or task output\n",
        "2. == or !=\n",
        "3. string or integer value\n",
        "\n",
        "A `dsl.Condition()` can be named using the `name` parameter while defining the condition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_pipeline:condition"
      },
      "outputs": [],
      "source": [
        "@component()\n",
        "def flip() -> int:\n",
        "    import random\n",
        "\n",
        "    return random.randint(0, 1)\n",
        "\n",
        "\n",
        "@component()\n",
        "def heads() -> bool:\n",
        "    print(\"heads\")\n",
        "    return True\n",
        "\n",
        "\n",
        "@component()\n",
        "def tails() -> bool:\n",
        "    print(\"tails\")\n",
        "    return False\n",
        "\n",
        "\n",
        "@dsl.pipeline(\n",
        "    name=\"condition\", description=\"A simple intro pipeline\", pipeline_root=PIPELINE_ROOT\n",
        ")\n",
        "def pipeline():\n",
        "    flip_task = flip()\n",
        "    with dsl.Condition(flip_task.output == 1, name=\"true_clause\"):\n",
        "        task = heads()\n",
        "    with dsl.Condition(flip_task.output == 0, name=\"false_clause\"):\n",
        "        task = tails()\n",
        "    return task\n",
        "\n",
        "\n",
        "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"condition.json\")\n",
        "\n",
        "pipeline = aiplatform.PipelineJob(\n",
        "    display_name=\"condition\",\n",
        "    template_path=\"condition.json\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        ")\n",
        "\n",
        "pipeline.run()\n",
        "\n",
        "print_pipeline_output(pipeline, \"flip\")\n",
        "\n",
        "! rm condition.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "delete_pipeline"
      },
      "source": [
        "### Delete a pipeline job\n",
        "\n",
        "After a pipeline job is completed, you can delete the pipeline job with the `delete()` method. Prior to completion, a pipeline job can be canceled with the method `cancel()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "delete_pipeline"
      },
      "outputs": [],
      "source": [
        "pipeline.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pipeline_errata"
      },
      "source": [
        "### Caching in pipeline components\n",
        "\n",
        "When running a pipeline with Vertex AI Pipelines, the outcome state of each task is cached. With caching, if the pipeline is run again, and the compiled definition of the task and state has not changed, the cached output will be used instead of running the task again.\n",
        "\n",
        "To override caching, i.e., force run the task, you set the parameter `enable_caching` to `False` when creating the Vertex AI Pipeline job.\n",
        "\n",
        "```\n",
        "pipeline = aiplatform.PipelineJob(\n",
        "    display_name=\"example\",\n",
        "    template_path=\"example.json\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        "    enable_caching=False\n",
        ")\n",
        "```\n",
        "\n",
        "### Asynchronous execution of pipeline\n",
        "\n",
        "When running a pipeline with the method `run()`, the pipeline is run synchronously. To run asynchronously, you use the method `submit()`. Once the job has started, your Python script can continue to execute. To block execution, you can use the method `wait()`.\n",
        "\n",
        "### Setting machine resources for pipeline steps\n",
        "\n",
        "By default, Vertex AI Pipelines automatically finds the best matching machine type to run the component. You can override and specify the machine resources on a per component basis, when you invoke the component in a pipeline, as follows:\n",
        "\n",
        "```\n",
        "@dsl.pipeline(name='my-pipeline')\n",
        "def pipeline():\n",
        "  task = taskOp().\n",
        "    set_cpu_limit('CPU_LIMIT').\n",
        "    set_memory_limit('MEMORY_LIMIT').\n",
        "    add_node_selector_constraint(SELECTOR_CONSTRAINT).\n",
        "    set_gpu_limit(GPU_LIMIT))\n",
        "```\n",
        "\n",
        "Learn more about [Specifying machine types in pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/machine-types)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cleanup:mbsdk"
      },
      "source": [
        "# Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:\n",
        "\n",
        "### Cloud Storage Bucket\n",
        "\n",
        "Set `delete_bucket` to True to delete the Cloud storage bucket used in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup:mbsdk"
      },
      "outputs": [],
      "source": [
        "delete_bucket = False\n",
        "\n",
        "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
        "    ! gsutil rm -r $BUCKET_URI"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "get_started_with_kubeflow_pipelines.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

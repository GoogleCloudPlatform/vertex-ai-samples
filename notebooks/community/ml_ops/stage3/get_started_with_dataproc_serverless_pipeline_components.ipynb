{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "copyright"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title:generic,gcp"
      },
      "source": [
        "# E2E ML on GCP: MLOps stage 3 : formalization: get started with Datproc Serverless pipeline components\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/get_started_with_dataproc_serverless_pipeline_components.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "    <td>\n",
        "        <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/get_started_with_dataproc_serverless_pipeline_components.ipynb\">\n",
        "        <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\\\" alt=\"Colab logo\"> Run in Colab\n",
        "        </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/get_started_with_dataproc_serverless_pipeline_components.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "<br/><br/><br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview:mlops"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. This tutorial covers stage 3 : formalization: get started with Dataproc Serverless pipeline components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "objective:mlops,stage3,get_started_dataflow_pipeline_components"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to use prebuilt `Google Cloud Pipeline Components` for `Dataproc Serverless` service. The documentation for the components can be found [here](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.0/google_cloud_pipeline_components.experimental.dataproc.html).\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services:\n",
        "\n",
        "- `Vertex AI Pipelines`\n",
        "- `Google Cloud Pipeline Components`\n",
        "- `Dataproc Serverless`\n",
        "\n",
        "An example pipeline is provided for each Dataproc Serverless component, which includes:\n",
        "- `DataprocPySparkBatchOp` for running PySpark batch workloads.\n",
        "- `DataprocSparkBatchOp` for running Spark batch workloads.\n",
        "- `DataprocSparkSqlBatchOp` for running Spark SQL batch workloads.\n",
        "- `DataprocSparkRBatchOp` for running SparkR batch workloads.\n",
        "\n",
        "### Costs\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "- Vertex AI\n",
        "- Dataproc Serverless\n",
        "- Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Dataproc Serverless pricing](https://cloud.google.com/dataproc-serverless/pricing) and [Cloud Storage pricing](https://cloud.google.com/storage/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bucket:mbsdk"
      },
      "source": [
        "### Before you begin\n",
        "\n",
        "**Before proceeding, you should complete the following pre-requisites:**\n",
        "\n",
        "* [Configure your project for Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/configure-project).\n",
        "\n",
        "* [Enable the Dataproc API](https://console.cloud.google.com/flows/enableapi?apiid=dataproc.googleleapis.com) in your project.\n",
        "\n",
        "* Ensure your project meets the networking requirements detailed in [Dataproc Serverless for Spark network configuration](https://cloud.google.com/dataproc-serverless/docs/concepts/network)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_mlops"
      },
      "source": [
        "## Installations\n",
        "\n",
        "Install the following packages for executing this MLOps notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_mlops"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\"\n",
        "\n",
        "# Install the required packages\n",
        "! pip3 install tensorflow-io $USER_FLAG -q\n",
        "! pip3 install --upgrade google-cloud-pipeline-components kfp $USER_FLAG -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "restart"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "restart"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "project_id"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_project_id"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_project_id"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID:\", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_gcloud_project_id"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you. The notebook will configure the Dataproc Serverless components to run in the same region.\n",
        "\n",
        "- Americas: `us-central1`\n",
        "- Europe: `europe-west4`\n",
        "- Asia Pacific: `asia-east1`\n",
        "\n",
        "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
        "\n",
        "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "region"
      },
      "outputs": [],
      "source": [
        "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
        "\n",
        "if REGION == \"[your-region]\":\n",
        "    REGION = \"us-central1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "timestamp"
      },
      "source": [
        "#### Timestamp\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "timestamp"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c38be665ca50"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using Google Cloud Notebooks**, your environment is already authenticated. Skip this step.\n",
        "\n",
        "**If you are using Colab**, run the cell below and follow the instructions when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "In the Cloud Console, go to the [Create service account key](https://console.cloud.google.com/apis/credentials/serviceaccountkey) page.\n",
        "\n",
        "1. **Click Create service account**.\n",
        "\n",
        "2. In the **Service account name** field, enter a name, and click **Create**.\n",
        "\n",
        "3. In the **Grant this service account access to project** section, click the Role drop-down list. Type \"Vertex\" into the filter box, and select **Vertex Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "\n",
        "4. Click Create. A JSON file that contains your key downloads to your local environment.\n",
        "\n",
        "5. Enter the path to your service account key as the GOOGLE_APPLICATION_CREDENTIALS variable in the cell below and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0953a00668e"
      },
      "outputs": [],
      "source": [
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# If on Google Cloud Notebook, then don't execute this code\n",
        "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bucket:mbsdk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "When you initialize the Vertex SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bucket"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_bucket"
      },
      "outputs": [],
      "source": [
        "if BUCKET_URI == \"\" or BUCKET_URI is None or BUCKET_URI == \"gs://[your-bucket-name]\":\n",
        "    BUCKET_URI = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_bucket"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "validate_bucket"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "validate_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "set_service_account"
      },
      "source": [
        "#### Service Account\n",
        "\n",
        "You use a service account to create the Vertex AI Pipeline job. If you do not want to use your project's Compute Engine service account, set `SERVICE_ACCOUNT` to another service account ID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ac0515b2a89"
      },
      "outputs": [],
      "source": [
        "SERVICE_ACCOUNT = \"[your-service-account]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_service_account"
      },
      "outputs": [],
      "source": [
        "if (\n",
        "    SERVICE_ACCOUNT == \"\"\n",
        "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
        "    or SERVICE_ACCOUNT is None\n",
        "):\n",
        "    shell_output = ! gcloud projects describe $PROJECT_ID | sed -nre 's:.*projectNumber\\: (.*):\\1:p'\n",
        "    SERVICE_ACCOUNT = (\n",
        "        shell_output[0].replace(\"'\", \"\") + \"-compute@developer.gserviceaccount.com\"\n",
        "    )\n",
        "\n",
        "print(\"Service Account:\", SERVICE_ACCOUNT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff4ebb4b3758"
      },
      "source": [
        "Run the following commands to grant your project's Compute Engine service account access to read and write pipeline artifacts in the bucket that you created in the previous step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_service_account:pipelines"
      },
      "outputs": [],
      "source": [
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
        "\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbf89ab9b953"
      },
      "source": [
        "#### Grant Dataproc roles to the Service Account\n",
        "\n",
        "**Note: You can skip this section if you're using your project's Compute Engine default service account, and if that account has been granted the project `Editor` role already.**\n",
        "\n",
        "For simplicity, this notebook uses the same service account for Vertex AI Pipelines jobs and Dataproc Serverless workloads. Dataproc Serverless provides IAM roles to create and run workloads:\n",
        "\n",
        "* The `Dataproc Editor` role grants the necessary IAM permissions to create a workload. The role should be granted to user and service accounts that create Dataproc batch workloads.\n",
        "* The `Dataproc Worker` role grants the necessary IAM permissions to run a workload. The role should be granted to the service account that runs the batch workload.\n",
        "\n",
        "You may not need to grant the `Dataproc Editor` and `Dataproc Worker` roles if your service account has already been granted with the  permissions required to create and run Dataproc batch workloads. For example, the Compute Engine default service account may already be granted the project `Editor` role. The `Editor` role provides sufficient permissions to both create and run Dataproc workloads.\n",
        "\n",
        "**Note:** The following cells will fail if the account used by `gcloud` has not been granted permissions to modify IAM policies. In this case, you can set IAM policies for the service account by accessing the [IAM page in the Cloud Console](https://console.cloud.google.com/iam-admin/iam) with a user account that has been granted the required permissions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eef3257460b4"
      },
      "source": [
        "Set `GRANT_DATAPROC_EDITOR_ROLE` to `True` if you wish to grant the `Dataproc Editor` role to your service account. The `Dataproc Editor` role grants the IAM permissions needed to create Dataproc Serverless workloads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "642069aed38d"
      },
      "outputs": [],
      "source": [
        "GRANT_DATAPROC_EDITOR_ROLE = False\n",
        "\n",
        "if GRANT_DATAPROC_EDITOR_ROLE:\n",
        "    ! gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
        "        --member=\"serviceAccount:$SERVICE_ACCOUNT\" \\\n",
        "        --role=\"roles/dataproc.editor\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e4afaabbe9e"
      },
      "source": [
        "Set `GRANT_DATAPROC_WORKER_ROLE` to `True` if you wish to grant the `Dataproc Worker` role to your service account. The `Dataproc Worker` role grants the IAM permissions needed to run Dataproc Serverless workloads.\n",
        "\n",
        "*Note*: The following cell will fail if the account used by `gcloud` has not been granted permissions to modify IAM policies. In this case, you can set IAM policies for the service account by accessing the [IAM page in the Cloud Console](https://console.cloud.google.com/iam-admin/iam) with a user account that has been granted the required permissions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2c3cc811fc4"
      },
      "outputs": [],
      "source": [
        "GRANT_DATAPROC_WORKER_ROLE = False\n",
        "\n",
        "if GRANT_DATAPROC_WORKER_ROLE:\n",
        "    ! gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
        "        --member=\"serviceAccount:$SERVICE_ACCOUNT\" \\\n",
        "        --role=\"roles/dataproc.worker\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_vars"
      },
      "source": [
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "import google.cloud.aiplatform as aiplatform\n",
        "from kfp import dsl\n",
        "from kfp.v2 import compiler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "writefile:wc.py"
      },
      "source": [
        "## Running a PySpark workload\n",
        "\n",
        "This section shows you how to create a PySpark batch workload from Vertex AI Pipelines. The pipeline uses `DataprocPySparkBatchOp` to run a Python script that counts the frequency of words used in Shakespeare.\n",
        "\n",
        "### Write the PySpark word count program.\n",
        "\n",
        "First, you write a Python script that uses PySpark to perform a simple word count. The code is written to a local file called `wordcount.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "writefile:wc.py"
      },
      "outputs": [],
      "source": [
        "%%writefile wordcount.py\n",
        "#\n",
        "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
        "# contributor license agreements.  See the NOTICE file distributed with\n",
        "# this work for additional information regarding copyright ownership.\n",
        "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
        "# (the \"License\"); you may not use this file except in compliance with\n",
        "# the License.  You may obtain a copy of the License at\n",
        "#\n",
        "#    http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "#\n",
        "\n",
        "\"\"\"A PySpark program that counts the number of words in Shakespeare.\"\"\"\n",
        "\n",
        "import argparse\n",
        "import sys\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "def run(argv=None):\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--input',\n",
        "                        dest='input',\n",
        "                        default=' ',\n",
        "                        help='Input file to process.')\n",
        "    parser.add_argument('--output',\n",
        "                        dest='output',\n",
        "                        default='gs://YOUR_OUTPUT_BUCKET/AND_OUTPUT_PREFIX',\n",
        "                        help='Output file to write results to.')\n",
        "    \n",
        "    known_args, _ = parser.parse_known_args(argv)\n",
        "    \n",
        "    spark = SparkSession\\\n",
        "            .builder\\\n",
        "            .appName(\"wordcount\")\\\n",
        "            .getOrCreate()\n",
        "    \n",
        "    sc = spark.sparkContext    \n",
        "    words = sc.textFile(known_args.input).flatMap(lambda line: line.split(\" \"))\n",
        "    wordCounts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)\n",
        "    wordCounts.saveAsTextFile(known_args.output)\n",
        "    \n",
        "    spark.stop()\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    run(sys.argv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "copy_to_gcs:wc"
      },
      "source": [
        "### Copy python module to Cloud Storage\n",
        "\n",
        "Next, you copy `wordcount.py` to a Cloud Storage bucket.\n",
        "\n",
        "Additionally, you set the Cloud Storage locations for the input and output of the script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "copy_to_gcs:wc"
      },
      "outputs": [],
      "source": [
        "GCS_WC_PY = BUCKET_URI + \"/wordcount.py\"\n",
        "! gsutil cp wordcount.py $GCS_WC_PY\n",
        "\n",
        "GCS_WC_OUT = BUCKET_URI + \"/wc_out/\"\n",
        "GCS_WC_IN = \"gs://dataproc-datasets-us-central1/shakespeare/all-lines.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_dataflow_pipeline:wc"
      },
      "source": [
        "### Create and execute the pipeline job\n",
        "\n",
        "In this example, the `DataprocPySparkBatchOp` component takes the following parameters:\n",
        "\n",
        "- `batch_id`: The batch ID to use for the Dataproc Batch workload.\n",
        "- `project_id`: The project ID.\n",
        "- `location`: The region.\n",
        "- `main_python_file_uri`: The URI of the main Python file.\n",
        "- `service_account`: The service account that runs the workload.\n",
        "- `args`: The arguments to pass to the PySpark program.\n",
        "\n",
        "Learn more about the [Dataproc Serverless PySpark batch component](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.0/google_cloud_pipeline_components.experimental.dataproc.html#google_cloud_pipeline_components.experimental.dataproc.DataprocPySparkBatchOp)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_dataflow_pipeline:wc"
      },
      "outputs": [],
      "source": [
        "PIPELINE_ROOT = \"{}/pipeline_root/dataproc_pyspark\".format(BUCKET_URI)\n",
        "BATCH_ID = \"wordcount-pyspark-\" + TIMESTAMP\n",
        "ARGS = [\n",
        "    \"--input\",\n",
        "    GCS_WC_IN,\n",
        "    \"--output\",\n",
        "    GCS_WC_OUT,\n",
        "]\n",
        "\n",
        "\n",
        "@dsl.pipeline(\n",
        "    name=\"dataproc-pyspark\",\n",
        "    description=\"An exmaple pipeline that uses DataprocPySparkBatchOp for running a PySpark batch workload.\",\n",
        ")\n",
        "def pipeline(\n",
        "    batch_id: str = BATCH_ID,\n",
        "    project_id: str = PROJECT_ID,\n",
        "    location: str = REGION,\n",
        "    main_python_file_uri: str = GCS_WC_PY,\n",
        "    service_account: str = SERVICE_ACCOUNT,\n",
        "    args: list = ARGS,\n",
        "):\n",
        "    from google_cloud_pipeline_components.experimental.dataproc import \\\n",
        "        DataprocPySparkBatchOp\n",
        "\n",
        "    _ = DataprocPySparkBatchOp(\n",
        "        project=project_id,\n",
        "        location=location,\n",
        "        batch_id=batch_id,\n",
        "        main_python_file_uri=main_python_file_uri,\n",
        "        service_account=service_account,\n",
        "        args=args,\n",
        "    )\n",
        "\n",
        "\n",
        "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"pipeline.json\")\n",
        "\n",
        "pipeline = aiplatform.PipelineJob(\n",
        "    display_name=\"pipeline\",\n",
        "    template_path=\"pipeline.json\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        "    enable_caching=False,\n",
        ")\n",
        "\n",
        "pipeline.run()\n",
        "\n",
        "! gsutil cat {GCS_WC_OUT}* | head -n10\n",
        "\n",
        "! rm -f pipeline.json wordcount.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view_pipeline_results:automl,icn"
      },
      "source": [
        "### View Dataproc pipeline results\n",
        "\n",
        "Finally, you will view the artifact outputs of each task in the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ffb3048fa7b"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "PROJECT_NUMBER = pipeline.gca_resource.name.split(\"/\")[1]\n",
        "print(PROJECT_NUMBER)\n",
        "\n",
        "\n",
        "def print_pipeline_output(job, output_task_name):\n",
        "    JOB_ID = job.name\n",
        "    print(JOB_ID)\n",
        "    for _ in range(len(job.gca_resource.job_detail.task_details)):\n",
        "        TASK_ID = job.gca_resource.job_detail.task_details[_].task_id\n",
        "        EXECUTE_OUTPUT = (\n",
        "            PIPELINE_ROOT\n",
        "            + \"/\"\n",
        "            + PROJECT_NUMBER\n",
        "            + \"/\"\n",
        "            + JOB_ID\n",
        "            + \"/\"\n",
        "            + output_task_name\n",
        "            + \"_\"\n",
        "            + str(TASK_ID)\n",
        "            + \"/executor_output.json\"\n",
        "        )\n",
        "        GCP_RESOURCES = (\n",
        "            PIPELINE_ROOT\n",
        "            + \"/\"\n",
        "            + PROJECT_NUMBER\n",
        "            + \"/\"\n",
        "            + JOB_ID\n",
        "            + \"/\"\n",
        "            + output_task_name\n",
        "            + \"_\"\n",
        "            + str(TASK_ID)\n",
        "            + \"/gcp_resources\"\n",
        "        )\n",
        "        EVAL_METRICS = (\n",
        "            PIPELINE_ROOT\n",
        "            + \"/\"\n",
        "            + PROJECT_NUMBER\n",
        "            + \"/\"\n",
        "            + JOB_ID\n",
        "            + \"/\"\n",
        "            + output_task_name\n",
        "            + \"_\"\n",
        "            + str(TASK_ID)\n",
        "            + \"/evaluation_metrics\"\n",
        "        )\n",
        "        if tf.io.gfile.exists(EXECUTE_OUTPUT):\n",
        "            print(EXECUTE_OUTPUT, \"EXECUTE_OUTPUT\")\n",
        "            ! gsutil cat $EXECUTE_OUTPUT\n",
        "            return EXECUTE_OUTPUT\n",
        "        elif tf.io.gfile.exists(GCP_RESOURCES):\n",
        "            ! gsutil cat $GCP_RESOURCES\n",
        "            return GCP_RESOURCES\n",
        "        elif tf.io.gfile.exists(EVAL_METRICS):\n",
        "            ! gsutil cat $EVAL_METRICS\n",
        "            return EVAL_METRICS\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "print(\"dataproc-create-pyspark-batch\")\n",
        "artifacts = print_pipeline_output(pipeline, \"dataproc-create-pyspark-batch\")\n",
        "print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "delete_pipeline"
      },
      "source": [
        "### Delete the pipeline job\n",
        "\n",
        "After a pipeline job is completed, you can delete the pipeline job with the method `delete()`.  Prior to completion, a pipeline job can be canceled with the method `cancel()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "delete_pipeline"
      },
      "outputs": [],
      "source": [
        "pipeline.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b88bebd6e4f"
      },
      "source": [
        "### Delete the batch\n",
        "\n",
        "You can delete the created batch in Dataproc serverless using the following `gcloud` command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1c21bcd1293e"
      },
      "outputs": [],
      "source": [
        "! gcloud dataproc batches delete $BATCH_ID --region=$REGION --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e23a029082cb"
      },
      "source": [
        "## Running a Spark workload\n",
        "This section shows you how to create a Spark batch workload from Vertex Pipelines. The pipeline uses the `DataprocSparkBatchOp` component to run the `JavaWordCount` example that is pre-installed in the Dataproc Serverless default container image.\n",
        "\n",
        "[View the source code](https://github.com/apache/spark/blob/master/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java) for the `JavaWordCount` example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11d844ef843d"
      },
      "source": [
        "### Create and execute the pipeline job\n",
        "\n",
        "In this example, the `DataprocSparkBatchOp` component takes the following parameters:\n",
        "\n",
        "- `batch_id`: The batch ID to use for the Dataproc Batch workload.\n",
        "- `project_id`: The project ID.\n",
        "- `location`: The region.\n",
        "- `main_class`: The main class.\n",
        "- `jar_file_uris`: The URIs of any required JARs to include in the executor and driver CLASSPATH.\n",
        "- `service_account`: The service account that runs the workload.\n",
        "- `args`: The arguments to pass to the Spark program.\n",
        "\n",
        "Learn more about the [Dataproc Serverless Spark batch component](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.0/google_cloud_pipeline_components.experimental.dataproc.html#google_cloud_pipeline_components.experimental.dataproc.DataprocSparkBatchOp)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecebf4d2ef08"
      },
      "outputs": [],
      "source": [
        "PIPELINE_ROOT = \"{}/pipeline_root/dataproc_spark\".format(BUCKET_URI)\n",
        "BATCH_ID = \"wordcount-spark-\" + TIMESTAMP\n",
        "\n",
        "MAIN_CLASS = \"org.apache.spark.examples.JavaWordCount\"\n",
        "JAR_FILE_URIS = [\"file:///usr/lib/spark/examples/jars/spark-examples.jar\"]\n",
        "ARGS = [\"gs://dataproc-datasets-us-central1/shakespeare/all-lines.txt\"]\n",
        "\n",
        "\n",
        "@dsl.pipeline(\n",
        "    name=\"dataproc-spark-wc\",\n",
        "    description=\"An example pipeline that uses DataprocSparkBatchOp to run a Spark batch workload.\",\n",
        ")\n",
        "def pipeline(\n",
        "    batch_id: str = BATCH_ID,\n",
        "    project_id: str = PROJECT_ID,\n",
        "    location: str = REGION,\n",
        "    main_class: str = MAIN_CLASS,\n",
        "    jar_file_uris: list = JAR_FILE_URIS,\n",
        "    service_account: str = SERVICE_ACCOUNT,\n",
        "    args: list = ARGS,\n",
        "):\n",
        "    from google_cloud_pipeline_components.experimental.dataproc import \\\n",
        "        DataprocSparkBatchOp\n",
        "\n",
        "    _ = DataprocSparkBatchOp(\n",
        "        project=project_id,\n",
        "        location=location,\n",
        "        batch_id=batch_id,\n",
        "        main_class=main_class,\n",
        "        jar_file_uris=jar_file_uris,\n",
        "        service_account=service_account,\n",
        "        args=args,\n",
        "    )\n",
        "\n",
        "\n",
        "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"pipeline.json\")\n",
        "\n",
        "pipeline = aiplatform.PipelineJob(\n",
        "    display_name=\"pipeline\",\n",
        "    template_path=\"pipeline.json\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        "    enable_caching=False,\n",
        ")\n",
        "\n",
        "pipeline.run()\n",
        "\n",
        "! rm -f pipeline.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01e9788d6e6b"
      },
      "source": [
        "The `JavaWordCount` example prints the results to standard output, which is captured in Cloud Logging. \n",
        "\n",
        "When the pipeline finishes running, you can inspect the logging output of the Dataproc batch workload using the Cloud Console. Run the following cell to generate a link to the batch workload in the Cloud Console:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f68589209f4c"
      },
      "outputs": [],
      "source": [
        "from IPython.core.display import HTML, display\n",
        "\n",
        "display(\n",
        "    HTML(\n",
        "        f\"\"\"<a href=\"https://console.cloud.google.com/dataproc/batches/{REGION}/{BATCH_ID}/monitoring?project={PROJECT_ID}\">Link to Dataproc Batch workload.</a>\"\"\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view_pipeline_results:automl,icn"
      },
      "source": [
        "### View Dataproc pipeline results\n",
        "\n",
        "Finally, you will view the artifact outputs of each task in the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6b2f50d2fd1"
      },
      "outputs": [],
      "source": [
        "PROJECT_NUMBER = pipeline.gca_resource.name.split(\"/\")[1]\n",
        "print(PROJECT_NUMBER)\n",
        "\n",
        "print(\"dataproc-create-spark-batch\")\n",
        "artifacts = print_pipeline_output(pipeline, \"dataproc-create-spark-batch\")\n",
        "print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4f62545d631"
      },
      "source": [
        "### Delete the pipeline job\n",
        "\n",
        "After a pipeline job is completed, you can delete the pipeline job with the method `delete()`.  Prior to completion, a pipeline job can be canceled with the method `cancel()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dad89ebf819"
      },
      "outputs": [],
      "source": [
        "pipeline.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a204485faa5"
      },
      "source": [
        "### Delete the batch\n",
        "\n",
        "You can delete the created batch in Dataproc serverless using the following `gcloud` command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3347ac71dcca"
      },
      "outputs": [],
      "source": [
        "! gcloud dataproc batches delete $BATCH_ID --region=$REGION --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b8acd182060"
      },
      "source": [
        "## Running a Spark SQL workload\n",
        "This section shows you how to create a Spark SQL batch workload from Vertex Pipelines. The pipeline uses `DataprocSparkSqlBatchOp` component to run Spark SQL queries on a public sample dataset. The sample dataset is provided by the US Social Security Adminstration and contains approximately 7 MB of data about popular baby names."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bec2db36f1f4"
      },
      "source": [
        "Download and extract the baby names zip file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfdca5d2c2d1"
      },
      "outputs": [],
      "source": [
        "! curl -OL https://www.ssa.gov/OACT/babynames/names.zip && unzip -o names.zip -d babynames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e7586c2b275"
      },
      "source": [
        "The data is in CSV format. Each row contains values for `name`, `gender`, and `count` in that order. Run the next cell to inspect the sample data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "807ac94124ea"
      },
      "outputs": [],
      "source": [
        "! head babynames/yob2010.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "256d5e5fb5a4"
      },
      "source": [
        "Copy the sample data to Cloud Storage:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "976c835eb92d"
      },
      "outputs": [],
      "source": [
        "! gsutil -m cp babynames/*.txt $BUCKET_URI/babynames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03f72425b84e"
      },
      "source": [
        "Next, write the SQL queries to execute to a file. The queries in the file perform the following:\n",
        "\n",
        "- Creates an external table called `babynames_2010` that uses one of the sample files that you uploaded to Cloud Storage (`yob2010.txt`). The `yob2010.txt` file contains baby names from the year 2010.\n",
        "- Creates an external table called `top_2010`, which is populated using a `SELECT` statement that queries popular female names. The table data is stored as CSV files in Cloud Storage. \n",
        "\n",
        "The values for `bucket-name`, `output-location`, and `gender` will be provided at runtime using `DataprocSparkSqlBatchOp` component parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c56506e15791"
      },
      "outputs": [],
      "source": [
        "%%writefile top_names.sql\n",
        "\n",
        "CREATE TABLE babynames_2010 (name STRING, gender STRING, count INT)\n",
        "    USING CSV LOCATION '${bucket-name}/babynames/yob2010.txt';\n",
        "\n",
        "CREATE TABLE top_2010 \n",
        "    USING CSV LOCATION '${output-location}'\n",
        "AS\n",
        "    SELECT name, count \n",
        "        FROM babynames_2010 \n",
        "    WHERE gender = '${gender}' ORDER BY count DESC LIMIT ${max-results};"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb07db26ae1e"
      },
      "source": [
        "Copy the query file to Cloud Storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "309cd1ec3ce6"
      },
      "outputs": [],
      "source": [
        "! gsutil cp top_names.sql $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c523daee53d"
      },
      "source": [
        "### Create and execute the pipeline job\n",
        "\n",
        "In this example, the `DataprocSparkSqlBatchOp` component takes the following parameters:\n",
        "\n",
        "- `batch_id`: The batch ID to use for the Dataproc Batch workload.\n",
        "- `project_id`: The project ID.\n",
        "- `location`: The region.\n",
        "- `query_file_uri`: The URI of the file containing the SQL queries.\n",
        "- `query_variables`: The mapping of query variable names to values (equivalent to the Spark SQL command  `SET name=\"value\";`).\n",
        "- `service_account`: The service account that runs the workload.\n",
        "\n",
        "Learn more about the [Dataproc Serverless Spark SQL batch component](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.0/google_cloud_pipeline_components.experimental.dataproc.html#google_cloud_pipeline_components.experimental.dataproc.DataprocSparkSqlBatchOp)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e231252f38d3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PIPELINE_ROOT = \"{}/pipeline_root/dataproc_spark_sql\".format(BUCKET_URI)\n",
        "BATCH_ID = \"top-names-spark-sql-\" + TIMESTAMP\n",
        "\n",
        "QUERY_FILE_URI = os.path.join(BUCKET_URI, \"top_names.sql\")\n",
        "OUTPUT_LOCATION = os.path.join(BUCKET_URI, \"top_2010_names_f\")\n",
        "QUERY_VARIABLES = {\n",
        "    \"bucket-name\": BUCKET_URI,\n",
        "    \"output-location\": OUTPUT_LOCATION,\n",
        "    \"max-results\": \"50\",\n",
        "    \"gender\": \"F\",\n",
        "}\n",
        "\n",
        "\n",
        "@dsl.pipeline(\n",
        "    name=\"dataproc-spark-sql-top-names\",\n",
        "    description=\"An example pipeline that uses DataprocSparkSqlBatchOp to run Spark SQL queries.\",\n",
        ")\n",
        "def pipeline(\n",
        "    batch_id: str = BATCH_ID,\n",
        "    project_id: str = PROJECT_ID,\n",
        "    location: str = REGION,\n",
        "    query_file_uri: str = QUERY_FILE_URI,\n",
        "    query_variables: dict = QUERY_VARIABLES,\n",
        "    service_account: str = SERVICE_ACCOUNT,\n",
        "):\n",
        "    from google_cloud_pipeline_components.experimental.dataproc import \\\n",
        "        DataprocSparkSqlBatchOp\n",
        "\n",
        "    _ = DataprocSparkSqlBatchOp(\n",
        "        project=project_id,\n",
        "        location=location,\n",
        "        batch_id=batch_id,\n",
        "        query_file_uri=query_file_uri,\n",
        "        query_variables=query_variables,\n",
        "        service_account=service_account,\n",
        "    )\n",
        "\n",
        "\n",
        "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"pipeline.json\")\n",
        "\n",
        "pipeline = aiplatform.PipelineJob(\n",
        "    display_name=\"pipeline\",\n",
        "    template_path=\"pipeline.json\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        "    enable_caching=False,\n",
        ")\n",
        "\n",
        "pipeline.run()\n",
        "\n",
        "! gsutil cat $OUTPUT_LOCATION/*.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view_pipeline_results:automl,icn"
      },
      "source": [
        "### View Dataproc pipeline results\n",
        "\n",
        "Finally, you will view the artifact outputs of each task in the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08743ab3e702"
      },
      "outputs": [],
      "source": [
        "PROJECT_NUMBER = pipeline.gca_resource.name.split(\"/\")[1]\n",
        "print(PROJECT_NUMBER)\n",
        "\n",
        "print(\"dataproc-create-spark-sql-batch\")\n",
        "artifacts = print_pipeline_output(pipeline, \"dataproc-create-spark-sql-batch\")\n",
        "print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac01bfa0f7cc"
      },
      "source": [
        "### Delete the pipeline job\n",
        "\n",
        "After a pipeline job is completed, you can delete the pipeline job with the method `delete()`.  Prior to completion, a pipeline job can be canceled with the method `cancel()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e24a24370a8a"
      },
      "outputs": [],
      "source": [
        "pipeline.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "261ead937f4c"
      },
      "source": [
        "### Delete the batch\n",
        "\n",
        "You can delete the created batch in Dataproc serverless using the following `gcloud` command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01f828745b4e"
      },
      "outputs": [],
      "source": [
        "! gcloud dataproc batches delete $BATCH_ID --region=$REGION --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a92aede948a5"
      },
      "source": [
        "## Running a SparkR workload\n",
        "This section shows you how to create a SparkR batch workload from Vertex AI Pipelines. The pipeline uses `DataprocSparkRBatchOp` component to run a simple R script  that counts the frequency of words used in Shakespeare.\n",
        "\n",
        "### Write the PySpark word count program.\n",
        "\n",
        "First, you write a R script that performs a simple word count. The program code is written to a local file called `wordcount.R`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8ff08c17510"
      },
      "outputs": [],
      "source": [
        "%%writefile wordcount.R\n",
        "\n",
        "library(SparkR)\n",
        "\n",
        "sparkR.session(appName = \"SparkR-wordcount\")\n",
        "\n",
        "args <- commandArgs(trailing = TRUE)\n",
        "inputFile <- args[[1]]\n",
        "outputFile <- args[[2]]\n",
        "\n",
        "lines <- read.text(inputFile)\n",
        "filtered <- selectExpr(lines, \"regexp_replace(value, '[\\\",.?:!]', '') as filtered\")\n",
        "words <- selectExpr(filtered, \"explode(split(filtered, ' ')) as word\")\n",
        "wordCounts <- count(groupBy(words, \"word\"))\n",
        "\n",
        "write.df(wordCounts, outputFile, \"com.databricks.spark.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff878c2624dd"
      },
      "source": [
        "### Copy the R script to Cloud Storage\n",
        "\n",
        "Next, you copy `wordcount.R` to your Cloud Storage bucket.\n",
        "\n",
        "Additionally, you set the Cloud Storage locations for the input and output of the script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4559787ec09"
      },
      "outputs": [],
      "source": [
        "GCS_WC_R = BUCKET_URI + \"/wordcount.R\"\n",
        "! gsutil cp wordcount.R $GCS_WC_R\n",
        "\n",
        "GCS_WC_R_OUT = BUCKET_URI + \"/wc_r_out\"\n",
        "GCS_WC_R_IN = \"gs://dataproc-datasets-us-central1/shakespeare/all-lines.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "308ac040884d"
      },
      "source": [
        "### Create and execute the pipeline job\n",
        "\n",
        "In this example, the `DataprocSparkRBatchOp` component takes the following parameters:\n",
        "\n",
        "- `batch_id`: The batch ID to use for the Dataproc Batch workload.\n",
        "- `project_id`: The project ID.\n",
        "- `location`: The region.\n",
        "- `main_r_file_uri`: The URI of the main R file.\n",
        "- `service_account`: The service account that runs the workload.\n",
        "- `args`: The arguments to pass to the Spark program.\n",
        "\n",
        "Learn more about the [Dataproc Serverless SparkR batch component](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.0/google_cloud_pipeline_components.experimental.dataproc.html#google_cloud_pipeline_components.experimental.dataproc.DataprocSparkRBatchOp)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5374a36fe572"
      },
      "outputs": [],
      "source": [
        "PIPELINE_ROOT = \"{}/pipeline_root/dataproc_sparkr\".format(BUCKET_URI)\n",
        "BATCH_ID = \"wordcount-sparkr-\" + TIMESTAMP\n",
        "\n",
        "# [input-file, output-file]\n",
        "ARGS = [GCS_WC_R_IN, GCS_WC_R_OUT]\n",
        "\n",
        "\n",
        "@dsl.pipeline(\n",
        "    name=\"dataproc-sparkr-wc\",\n",
        "    description=\"An example pipeline that uses DataprocSparkRBatchOp to run a SparkR batch workload.\",\n",
        ")\n",
        "def pipeline(\n",
        "    batch_id: str = BATCH_ID,\n",
        "    project_id: str = PROJECT_ID,\n",
        "    location: str = REGION,\n",
        "    main_r_file_uri: str = GCS_WC_R,\n",
        "    service_account: str = SERVICE_ACCOUNT,\n",
        "    args: list = ARGS,\n",
        "):\n",
        "    from google_cloud_pipeline_components.experimental.dataproc import \\\n",
        "        DataprocSparkRBatchOp\n",
        "\n",
        "    _ = DataprocSparkRBatchOp(\n",
        "        project=project_id,\n",
        "        location=location,\n",
        "        batch_id=batch_id,\n",
        "        main_r_file_uri=main_r_file_uri,\n",
        "        args=args,\n",
        "    )\n",
        "\n",
        "\n",
        "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"pipeline.json\")\n",
        "\n",
        "pipeline = aiplatform.PipelineJob(\n",
        "    display_name=\"pipeline\",\n",
        "    template_path=\"pipeline.json\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        "    enable_caching=False,\n",
        ")\n",
        "\n",
        "pipeline.run()\n",
        "\n",
        "# Print the first 1KB of the CSV output:\n",
        "\n",
        "! gsutil cat -r 0-1024 $GCS_WC_R_OUT/*.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view_pipeline_results:automl,icn"
      },
      "source": [
        "### View Dataproc pipeline results\n",
        "\n",
        "Finally, you will view the artifact outputs of each task in the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e6abaec00fe"
      },
      "outputs": [],
      "source": [
        "PROJECT_NUMBER = pipeline.gca_resource.name.split(\"/\")[1]\n",
        "print(PROJECT_NUMBER)\n",
        "\n",
        "print(\"dataproc-create-spark-r-batch\")\n",
        "artifacts = print_pipeline_output(pipeline, \"dataproc-create-spark-r-batch\")\n",
        "print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01631d2c16f3"
      },
      "source": [
        "### Delete the pipeline job\n",
        "\n",
        "After a pipeline job is completed, you can delete the pipeline job with the method `delete()`.  Prior to completion, a pipeline job can be canceled with the method `cancel()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "896b86a19d91"
      },
      "outputs": [],
      "source": [
        "pipeline.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40234ba85e4d"
      },
      "source": [
        "### Delete the batch\n",
        "\n",
        "You can delete the created batch in Dataproc serverless using the following `gcloud` command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9f5ad4e43a8"
      },
      "outputs": [],
      "source": [
        "! gcloud dataproc batches delete $BATCH_ID --region=$REGION --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e0ce492628d"
      },
      "source": [
        "# Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial.\n",
        "\n",
        "### Delete Cloud Storage bucket\n",
        "Set `delete_bucket` to `True` to delete the Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup:mbsdk"
      },
      "outputs": [],
      "source": [
        "delete_bucket = False\n",
        "\n",
        "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
        "    # Delete the Cloud storage bucket\n",
        "    ! gsutil rm -r $BUCKET_URI"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "get_started_with_dataproc_serverless_pipeline_components.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "copyright"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title:generic,gcp"
      },
      "source": [
        "# E2E ML on GCP: MLOps stage 3 : formalization: get started with Dataflow pipeline components\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/get_started_with_dataflow_pipeline_components.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/ai/platform/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/get_started_with_dataflow_pipeline_components.ipynb\">\n",
        "      Open in Google Cloud Notebooks\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "<br/><br/><br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview:mlops"
      },
      "source": [
        "## Overview\n",
        "\n",
        "\n",
        "This tutorial demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. This tutorial covers stage 3 : formalization: get started with Dataflow pipeline components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset:gsod,lrg"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset used for this tutorial is the GSOD dataset from [BigQuery public datasets](https://cloud.google.com/bigquery/public-data). The version of the dataset you use only the fields year, month and day to predict the value of mean daily temperature (mean_temp)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "objective:mlops,stage3,get_started_dataflow_pipeline_components"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to use prebuilt `Google Cloud Pipeline Components` for `Dataflow`.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services:\n",
        "\n",
        "- `Vertex AI Pipelines`\n",
        "- `Google Cloud Pipeline Components`\n",
        "- `Dataflow`\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Build an Apache Beam data pipeline.\n",
        "- Encapsulate the Apache Beam data pipeline with a Dataflow component in a Vertex AI pipeline.\n",
        "- Execute a Vertex AI pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_mlops"
      },
      "source": [
        "## Installations\n",
        "\n",
        "Install *one time* the packages for executing the MLOps notebooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_mlops"
      },
      "outputs": [],
      "source": [
        "ONCE_ONLY = False\n",
        "if ONCE_ONLY:\n",
        "    ! pip3 install -U tensorflow==2.5 $USER_FLAG\n",
        "    ! pip3 install -U tensorflow-data-validation==1.2 $USER_FLAG\n",
        "    ! pip3 install -U tensorflow-transform==1.2 $USER_FLAG\n",
        "    ! pip3 install -U tensorflow-io==0.18 $USER_FLAG\n",
        "    ! pip3 install --upgrade google-cloud-aiplatform[tensorboard] $USER_FLAG\n",
        "    ! pip3 install --upgrade google-cloud-pipeline-components $USER_FLAG\n",
        "    ! pip3 install --upgrade google-cloud-bigquery $USER_FLAG\n",
        "    ! pip3 install --upgrade google-cloud-logging $USER_FLAG\n",
        "    ! pip3 install --upgrade apache-beam[gcp] $USER_FLAG\n",
        "    ! pip3 install --upgrade pyarrow $USER_FLAG\n",
        "    ! pip3 install --upgrade cloudml-hypertune $USER_FLAG\n",
        "    ! pip3 install --upgrade kfp $USER_FLAG\n",
        "    ! pip3 install --upgrade torchvision $USER_FLAG\n",
        "    ! pip3 install --upgrade rpy2 $USER_FLAG\n",
        "    ! pip3 install --upgrade python-tabulate $USER_FLAG\n",
        "    ! pip3 install -U opencv-python-headless==4.5.2.52 $USER_FLAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "restart"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "restart"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "project_id"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_project_id"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_project_id"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID:\", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_gcloud_project_id"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
        "\n",
        "- Americas: `us-central1`\n",
        "- Europe: `europe-west4`\n",
        "- Asia Pacific: `asia-east1`\n",
        "\n",
        "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
        "\n",
        "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "region"
      },
      "outputs": [],
      "source": [
        "REGION = \"us-central1\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "timestamp"
      },
      "source": [
        "#### Timestamp\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "timestamp"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bucket:mbsdk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "When you initialize the Vertex SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bucket"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_bucket"
      },
      "outputs": [],
      "source": [
        "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
        "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_bucket"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "validate_bucket"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "validate_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "set_service_account"
      },
      "source": [
        "#### Service Account\n",
        "\n",
        "**If you don't know your service account**, try to get your service account using `gcloud` command by executing the second cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_service_account"
      },
      "outputs": [],
      "source": [
        "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_service_account"
      },
      "outputs": [],
      "source": [
        "if (\n",
        "    SERVICE_ACCOUNT == \"\"\n",
        "    or SERVICE_ACCOUNT is None\n",
        "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
        "):\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = !gcloud auth list 2>/dev/null\n",
        "    SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
        "    print(\"Service Account:\", SERVICE_ACCOUNT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "set_service_account:pipelines"
      },
      "source": [
        "#### Set service account access for Vertex AI Pipelines\n",
        "\n",
        "Run the following commands to grant your service account access to read and write pipeline artifacts in the bucket that you created in the previous step -- you only need to run these once per service account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_service_account:pipelines"
      },
      "outputs": [],
      "source": [
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_NAME\n",
        "\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_vars"
      },
      "source": [
        "### Set up variables\n",
        "\n",
        "Next, set up some variables used throughout the tutorial.\n",
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "import google.cloud.aiplatform as aip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_kfp"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "from kfp import dsl\n",
        "from kfp.v2 import compiler\n",
        "from kfp.v2.dsl import component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_gcpc:dataflow"
      },
      "outputs": [],
      "source": [
        "from google_cloud_pipeline_components.v1.dataflow import DataflowPythonJobOp\n",
        "from google_cloud_pipeline_components.v1.wait_gcp_resources import \\\n",
        "    WaitGcpResourcesOp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "writefile:wc.py"
      },
      "source": [
        "### Write the Apache Beam pipeline module\n",
        "\n",
        "First, you write the Python module for the Dataflow pipeline. Since it is a module, you additional add the `if __name__ == '__main__':` entry point and use `argparse` to pass command line arguments to the module.\n",
        "\n",
        "This module implements the Apache Beam word count example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "writefile:wc.py"
      },
      "outputs": [],
      "source": [
        "%%writefile wc.py\n",
        "#\n",
        "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
        "# contributor license agreements.  See the NOTICE file distributed with\n",
        "# this work for additional information regarding copyright ownership.\n",
        "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
        "# (the \"License\"); you may not use this file except in compliance with\n",
        "# the License.  You may obtain a copy of the License at\n",
        "#\n",
        "#    http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "#\n",
        "\n",
        "\"\"\"A minimalist word-counting workflow that counts words in Shakespeare.\n",
        "\n",
        "This is the first in a series of successively more detailed 'word count'\n",
        "examples.\n",
        "\n",
        "Next, see the wordcount pipeline, then the wordcount_debugging pipeline, for\n",
        "more detailed examples that introduce additional concepts.\n",
        "\n",
        "Concepts:\n",
        "\n",
        "1. Reading data from text files\n",
        "2. Specifying 'inline' transforms\n",
        "3. Counting a PCollection\n",
        "4. Writing data to Cloud Storage as text files\n",
        "\n",
        "To execute this pipeline locally, first edit the code to specify the output\n",
        "location. Output location could be a local file path or an output prefix\n",
        "on GCS. (Only update the output location marked with the first CHANGE comment.)\n",
        "\n",
        "To execute this pipeline remotely, first edit the code to set your project ID,\n",
        "runner type, the staging location, the temp location, and the output location.\n",
        "The specified GCS bucket(s) must already exist. (Update all the places marked\n",
        "with a CHANGE comment.)\n",
        "\n",
        "Then, run the pipeline as described in the README. It will be deployed and run\n",
        "using the Google Cloud Dataflow Service. No args are required to run the\n",
        "pipeline. You can see the results in your output bucket in the GCS browser.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import argparse\n",
        "import logging\n",
        "import re\n",
        "\n",
        "from past.builtins import unicode\n",
        "\n",
        "import apache_beam as beam\n",
        "from apache_beam.io import ReadFromText\n",
        "from apache_beam.io import WriteToText\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "from apache_beam.options.pipeline_options import SetupOptions\n",
        "\n",
        "\n",
        "def run(argv=None):\n",
        "  \"\"\"Main entry point; defines and runs the wordcount pipeline.\"\"\"\n",
        "\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument('--input',\n",
        "                      dest='input',\n",
        "                      default='gs://dataflow-samples/shakespeare/kinglear.txt',\n",
        "                      help='Input file to process.')\n",
        "  parser.add_argument('--output',\n",
        "                      dest='output',\n",
        "                      # CHANGE 1/5: The Google Cloud Storage path is required\n",
        "                      # for outputting the results.\n",
        "                      default='gs://YOUR_OUTPUT_BUCKET/AND_OUTPUT_PREFIX',\n",
        "                      help='Output file to write results to.')\n",
        "  known_args, pipeline_args = parser.parse_known_args(argv)\n",
        "  # pipeline_args.extend([\n",
        "  #     # CHANGE 2/5: (OPTIONAL) Change this to DataflowRunner to\n",
        "  #     # run your pipeline on the Google Cloud Dataflow Service.\n",
        "  #     '--runner=DirectRunner',\n",
        "  #     # CHANGE 3/5: Your project ID is required in order to run your pipeline on\n",
        "  #     # the Google Cloud Dataflow Service.\n",
        "  #     '--project=SET_YOUR_PROJECT_ID_HERE',\n",
        "  #     # CHANGE 4/5: Your Google Cloud Storage path is required for staging local\n",
        "  #     # files.\n",
        "  #     '--staging_location=gs://YOUR_BUCKET_NAME/AND_STAGING_DIRECTORY',\n",
        "  #     # CHANGE 5/5: Your Google Cloud Storage path is required for temporary\n",
        "  #     # files.\n",
        "  #     '--temp_location=gs://YOUR_BUCKET_NAME/AND_TEMP_DIRECTORY',\n",
        "  #     '--job_name=your-wordcount-job',\n",
        "  # ])\n",
        "\n",
        "  # We use the save_main_session option because one or more DoFn's in this\n",
        "  # workflow rely on global context (e.g., a module imported at module level).\n",
        "  pipeline_options = PipelineOptions(pipeline_args)\n",
        "  pipeline_options.view_as(SetupOptions).save_main_session = True\n",
        "  with beam.Pipeline(options=pipeline_options) as p:\n",
        "\n",
        "    # Read the text file[pattern] into a PCollection.\n",
        "    lines = p | ReadFromText(known_args.input)\n",
        "\n",
        "    # Count the occurrences of each word.\n",
        "    counts = (\n",
        "        lines\n",
        "        | 'Split' >> (beam.FlatMap(lambda x: re.findall(r'[A-Za-z\\']+', x))\n",
        "                      .with_output_types(unicode))\n",
        "        | 'PairWithOne' >> beam.Map(lambda x: (x, 1))\n",
        "        | 'GroupAndSum' >> beam.CombinePerKey(sum))\n",
        "\n",
        "    # Format the counts into a PCollection of strings.\n",
        "    def format_result(word_count):\n",
        "      (word, count) = word_count\n",
        "      return '%s: %s' % (word, count)\n",
        "\n",
        "    output = counts | 'Format' >> beam.Map(format_result)\n",
        "\n",
        "    # Write the output using a \"Write\" transform that has side effects.\n",
        "    # pylint: disable=expression-not-assigned\n",
        "    output | WriteToText(known_args.output)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  logging.getLogger().setLevel(logging.INFO)\n",
        "  run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "writefile:requirements,wc"
      },
      "source": [
        "### Write the requirements (installs) for the Apache Beam pipeline module\n",
        "\n",
        "Next, create the `requirements.txt` file to specify Python modules that are required to be installed for executing the Apache Beam pipeline module -- in this case, `apache-beam` is required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "writefile:requirements,wc"
      },
      "outputs": [],
      "source": [
        "%%writefile requirements.txt\n",
        "apache-beam\n",
        "future"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "copy_to_gcs:wc"
      },
      "source": [
        "### Copy python module and requirements file to Cloud Storage\n",
        "\n",
        "Next, you copy the Python module and requirements file to your Cloud Storage bucket.\n",
        "\n",
        "Additional, you set the Cloud Storage location for the output of the Apache Beam word count pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "copy_to_gcs:wc"
      },
      "outputs": [],
      "source": [
        "GCS_WC_PY = BUCKET_NAME + \"/wc.py\"\n",
        "! gsutil cp wc.py $GCS_WC_PY\n",
        "GCS_REQUIREMENTS_TXT = BUCKET_NAME + \"/requirements.txt\"\n",
        "! gsutil cp requirements.txt $GCS_REQUIREMENTS_TXT\n",
        "\n",
        "GCS_WC_OUT = BUCKET_NAME + \"/wc_out.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_dataflow_pipeline:wc"
      },
      "source": [
        "### Create and execute the pipeline job\n",
        "\n",
        "In this example, the `DataflowPythonJobOp` component takes the following parameters:\n",
        "\n",
        "- `project_id`: The project ID.\n",
        "- `location`: The region.\n",
        "- `python_module_path`: The Cloud Storage location of the Apache Beam pipeline.\n",
        "- `temp_location`: The Cloud Storage temporary file workspace for the Apache Beam pipeline.\n",
        "- `requirements_file_path`: The required Python modules to install.\n",
        "- `args`: The arguments to pass to the Apache Beam pipeline.\n",
        "\n",
        "Learn more about [Google Cloud Pipeline Component for Dataflow](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-0.2.0/google_cloud_pipeline_components.experimental.dataflow.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_dataflow_pipeline:wc"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "PIPELINE_ROOT = \"{}/pipeline_root/dataflow_wc\".format(BUCKET_NAME)\n",
        "\n",
        "\n",
        "@dsl.pipeline(name=\"dataflow-wc\", description=\"Dataflow word count component pipeline\")\n",
        "def pipeline(\n",
        "    python_file_path: str = GCS_WC_PY,\n",
        "    project_id: str = PROJECT_ID,\n",
        "    location: str = REGION,\n",
        "    staging_dir: str = PIPELINE_ROOT,\n",
        "    args: list = [\"--output\", GCS_WC_OUT, \"--runner\", \"DataflowRunner\"],\n",
        "    requirements_file_path: str = GCS_REQUIREMENTS_TXT,\n",
        "):\n",
        "\n",
        "    dataflow_python_op = DataflowPythonJobOp(\n",
        "        project=project_id,\n",
        "        location=location,\n",
        "        python_module_path=python_file_path,\n",
        "        temp_location=staging_dir,\n",
        "        requirements_file_path=requirements_file_path,\n",
        "        args=args,\n",
        "    )\n",
        "\n",
        "    dataflow_wait_op = WaitGcpResourcesOp(\n",
        "        gcp_resources=dataflow_python_op.outputs[\"gcp_resources\"]\n",
        "    )\n",
        "\n",
        "\n",
        "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"dataflow_wc.json\")\n",
        "\n",
        "pipeline = aip.PipelineJob(\n",
        "    display_name=\"dataflow_wc\",\n",
        "    template_path=\"dataflow_wc.json\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        "    enable_caching=False,\n",
        ")\n",
        "\n",
        "pipeline.run()\n",
        "\n",
        "! gsutil cat {GCS_WC_OUT}* | head -n10\n",
        "\n",
        "! rm -f dataflow_wc.json wc.py requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "delete_pipeline"
      },
      "source": [
        "### Delete a pipeline job\n",
        "\n",
        "After a pipeline job is completed, you can delete the pipeline job with the method `delete()`.  Prior to completion, a pipeline job can be canceled with the method `cancel()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "delete_pipeline"
      },
      "outputs": [],
      "source": [
        "pipeline.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "writefile:split.py"
      },
      "source": [
        "### Write the Apache Beam pipeline module\n",
        "\n",
        "Next, you write the Python module for the Apache Beam pipeline. This module implements the a dataset split task into training and test data, and writes the split dataset as CSV files to a Cloud Storage bucket. In this example, the Python module will recieve some arguments for the pipeline from the command-line, which will be passed by the Dataflow pipeline component.\n",
        "\n",
        "*Note:* The Dataflow prebuilt component implicitly adds Dataflow-specific command-line arguments, such as `project`, `location`, `runner`, and `temp_location`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "writefile:split.py"
      },
      "outputs": [],
      "source": [
        "%%writefile split.py\n",
        "\n",
        "import argparse\n",
        "import logging\n",
        "import tensorflow_transform.beam as tft_beam\n",
        "\n",
        "\n",
        "from past.builtins import unicode\n",
        "\n",
        "import apache_beam as beam\n",
        "from apache_beam.io import ReadFromText\n",
        "from apache_beam.io import WriteToText\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "from apache_beam.options.pipeline_options import SetupOptions\n",
        "\n",
        "def run(argv=None):\n",
        "  \"\"\"Main entry point; defines and runs the wordcount pipeline.\"\"\"\n",
        "\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument('--bq_table',\n",
        "                      dest='bq_table')\n",
        "  parser.add_argument('--bucket',\n",
        "                      dest='bucket')\n",
        "  args, pipeline_args = parser.parse_known_args(argv)\n",
        "  logging.info(\"ARGS\")\n",
        "  logging.info(args)\n",
        "  logging.info(\"PIPELINE ARGS\")\n",
        "  logging.info(pipeline_args)\n",
        "  for i in range(0, len(pipeline_args), 2):\n",
        "        if \"--temp_location\" == pipeline_args[i]:\n",
        "            temp_location = pipeline_args[i+1]\n",
        "        elif \"--project\" == pipeline_args[i]:\n",
        "            project = pipeline_args[i+1]\n",
        "\n",
        "  exported_train = args.bucket + '/exported_data/train'\n",
        "  exported_eval  = args.bucket + '/exported_data/eval'\n",
        "\n",
        "\n",
        "  pipeline_options = PipelineOptions(pipeline_args)\n",
        "  pipeline_options.view_as(SetupOptions).save_main_session = True\n",
        "  with beam.Pipeline(options=pipeline_options) as pipeline:\n",
        "    with tft_beam.Context(temp_location):\n",
        "\n",
        "        raw_data_query = \"SELECT {0},{1} FROM {2} LIMIT 500\".format(\"CAST(station_number as STRING) AS station_number,year,month,day\",\"mean_temp\", args.bq_table)\n",
        "\n",
        "        def parse_bq_record(bq_record):\n",
        "            \"\"\"Parses a bq_record to a dictionary.\"\"\"\n",
        "            output = {}\n",
        "            for key in bq_record:\n",
        "                output[key] = [bq_record[key]]\n",
        "            return output\n",
        "\n",
        "        def split_dataset(bq_row, num_partitions, ratio):\n",
        "            \"\"\"Returns a partition number for a given bq_row.\"\"\"\n",
        "            import json\n",
        "\n",
        "            assert num_partitions == len(ratio)\n",
        "            bucket = sum(map(ord, json.dumps(bq_row))) % sum(ratio)\n",
        "            total = 0\n",
        "            for i, part in enumerate(ratio):\n",
        "                total += part\n",
        "                if bucket < total:\n",
        "                    return i\n",
        "            return len(ratio) - 1\n",
        "\n",
        "        # Read raw BigQuery data.\n",
        "        raw_train_data, raw_eval_data = (\n",
        "            pipeline\n",
        "            | \"Read Raw Data\"\n",
        "            >> beam.io.ReadFromBigQuery(\n",
        "                query=raw_data_query,\n",
        "                project=project,\n",
        "                use_standard_sql=True,\n",
        "            )\n",
        "            | \"Parse Data\" >> beam.Map(parse_bq_record)\n",
        "            | \"Split\" >> beam.Partition(split_dataset, 2, ratio=[8, 2])\n",
        "        )\n",
        "\n",
        "        # Write raw train data to GCS .\n",
        "        _ = raw_train_data | \"Write Raw Train Data\" >> beam.io.WriteToText(\n",
        "            file_path_prefix=exported_train, file_name_suffix=\".csv\"\n",
        "        )\n",
        "\n",
        "        # Write raw eval data to GCS .\n",
        "        _ = raw_eval_data | \"Write Raw Eval Data\" >> beam.io.WriteToText(\n",
        "            file_path_prefix=exported_eval, file_name_suffix=\".csv\"\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  logging.getLogger().setLevel(logging.INFO)\n",
        "  run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "writefile:requirements,split"
      },
      "source": [
        "### Write the requirements (installs) for the Apache Beam pipeline module\n",
        "\n",
        "Next, create the `requirements.txt` file to specify Python modules that are required to be installed for executing the Apache Beam pipeline module -- in this case, `apache-beam` and `tensorflow-transform` are required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "writefile:requirements,split"
      },
      "outputs": [],
      "source": [
        "%%writefile requirements.txt\n",
        "apache-beam\n",
        "tensorflow-transform==1.2.0\n",
        "future"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "writefile:setup,split"
      },
      "source": [
        "### Write the setup.py (installs) for the Dataflow workers\n",
        "\n",
        "Next, create the `setup.py` file to specify Python modules that are required to be installed for executing the Dataflow workers -- in this case, `tensorflow-transform` is required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "writefile:setup,split"
      },
      "outputs": [],
      "source": [
        "%%writefile setup.py\n",
        "import setuptools\n",
        "\n",
        "REQUIRED_PACKAGES = [\n",
        "    'tensorflow-transform==1.2.0',\n",
        "    'future'\n",
        "]\n",
        "PACKAGE_NAME = 'my_package'\n",
        "PACKAGE_VERSION = '0.0.1'\n",
        "setuptools.setup(\n",
        "    name=PACKAGE_NAME,\n",
        "    version=PACKAGE_VERSION,\n",
        "    description='Demo for split transformation',\n",
        "    install_requires=REQUIRED_PACKAGES,\n",
        "    author=\"cdpe@google.com\",\n",
        "    packages=setuptools.find_packages()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "copy_to_gcs:split"
      },
      "source": [
        "### Copy python module and requirements file to Cloud Storage\n",
        "\n",
        "Next, you copy the Python module, requirements and setup file to your Cloud Storage bucket.\n",
        "\n",
        "Additional, you set the Cloud Storage location for the output of the Apache Beam dataset split pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "copy_to_gcs:split"
      },
      "outputs": [],
      "source": [
        "GCS_SPLIT_PY = BUCKET_NAME + \"/split.py\"\n",
        "! gsutil cp split.py $GCS_SPLIT_PY\n",
        "GCS_REQUIREMENTS_TXT = BUCKET_NAME + \"/requirements.txt\"\n",
        "! gsutil cp requirements.txt $GCS_REQUIREMENTS_TXT\n",
        "GCS_SETUP_PY = BUCKET_NAME + \"/setup.py\"\n",
        "! gsutil cp setup.py $GCS_SETUP_PY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_file:u_dataset,bq"
      },
      "source": [
        "#### Location of BigQuery training data.\n",
        "\n",
        "Now set the variable `IMPORT_FILE` to the location of the data table in BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_file:gsod,bq,lrg"
      },
      "outputs": [],
      "source": [
        "IMPORT_FILE = \"bq://bigquery-public-data.samples.gsod\"\n",
        "BQ_TABLE = \"bigquery-public-data.samples.gsod\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_dataflow_pipeline:split"
      },
      "source": [
        "### Create and execute the pipeline job\n",
        "\n",
        "In this example, the `DataflowPythonJobOp` component takes the following parameters:\n",
        "\n",
        "- `project_id`: The project ID.\n",
        "- `location`: The region.\n",
        "- `python_module_path`: The Cloud Storage location of the Apache Beam pipeline.\n",
        "- `temp_location`: The Cloud Storage temporary file workspace for the Apache Beam pipeline.\n",
        "- `requirements_file_path`: The required Python modules to install.\n",
        "- `args`: The arguments to pass to the Apache Beam pipeline.\n",
        "\n",
        "Learn more about [Google Cloud Pipeline Component for Dataflow](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-0.2.0/google_cloud_pipeline_components.experimental.dataflow.html)\n",
        "\n",
        "Additional, you add `--runner=DataflowRunner` to the input args, to tell the component to use Dataflow instead of DirectRunner for the Apache Beam job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_dataflow_pipeline:split"
      },
      "outputs": [],
      "source": [
        "PIPELINE_ROOT = \"{}/pipeline_root/dataflow_split\".format(BUCKET_NAME)\n",
        "\n",
        "\n",
        "@dsl.pipeline(name=\"dataflow-split\", description=\"Dataflow split dataset\")\n",
        "def pipeline(\n",
        "    python_file_path: str = GCS_SPLIT_PY,\n",
        "    project_id: str = PROJECT_ID,\n",
        "    location: str = REGION,\n",
        "    staging_dir: str = PIPELINE_ROOT,\n",
        "    args: list = [\n",
        "        \"--bucket\",\n",
        "        BUCKET_NAME,\n",
        "        \"--bq_table\",\n",
        "        BQ_TABLE,\n",
        "        \"--runner\",\n",
        "        \"DataflowRunner\",\n",
        "        \"--setup_file\",\n",
        "        GCS_SETUP_PY,\n",
        "    ],\n",
        "    requirements_file_path: str = GCS_REQUIREMENTS_TXT,\n",
        "):\n",
        "    # DataflowPythonJobOp.component_spec.implementation.container.image = \"gcr.io/ml-pipeline/google-cloud-pipeline-components:v0.2.0_dataflow_logs_fix\"\n",
        "    dataflow_python_op = DataflowPythonJobOp(\n",
        "        project=project_id,\n",
        "        location=location,\n",
        "        python_module_path=python_file_path,\n",
        "        temp_location=staging_dir,\n",
        "        requirements_file_path=requirements_file_path,\n",
        "        args=args,\n",
        "    )\n",
        "\n",
        "    dataflow_wait_op = WaitGcpResourcesOp(\n",
        "        gcp_resources=dataflow_python_op.outputs[\"gcp_resources\"]\n",
        "    )\n",
        "\n",
        "\n",
        "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"dataflow_split.json\")\n",
        "\n",
        "pipeline = aip.PipelineJob(\n",
        "    display_name=\"dataflow_split\",\n",
        "    template_path=\"dataflow_split.json\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        "    enable_caching=False,\n",
        ")\n",
        "\n",
        "pipeline.run()\n",
        "\n",
        "! gsutil ls {BUCKET_NAME}/exported_data\n",
        "\n",
        "! rm -f dataflow_split.json split.py requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "delete_pipeline"
      },
      "source": [
        "### Delete a pipeline job\n",
        "\n",
        "After a pipeline job is completed, you can delete the pipeline job with the method `delete()`.  Prior to completion, a pipeline job can be canceled with the method `cancel()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "delete_pipeline"
      },
      "outputs": [],
      "source": [
        "pipeline.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cleanup:mbsdk"
      },
      "source": [
        "# Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:\n",
        "\n",
        "- Dataset\n",
        "- Pipeline\n",
        "- Model\n",
        "- Endpoint\n",
        "- AutoML Training Job\n",
        "- Batch Job\n",
        "- Custom Job\n",
        "- Hyperparameter Tuning Job\n",
        "- Cloud Storage Bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup:mbsdk"
      },
      "outputs": [],
      "source": [
        "delete_all = True\n",
        "\n",
        "if delete_all:\n",
        "    # Delete the dataset using the Vertex dataset object\n",
        "    try:\n",
        "        if \"dataset\" in globals():\n",
        "            dataset.delete()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    # Delete the model using the Vertex model object\n",
        "    try:\n",
        "        if \"model\" in globals():\n",
        "            model.delete()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    # Delete the endpoint using the Vertex endpoint object\n",
        "    try:\n",
        "        if \"endpoint\" in globals():\n",
        "            endpoint.undeploy_all()\n",
        "            endpoint.delete()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    # Delete the AutoML or Pipeline training job\n",
        "    try:\n",
        "        if \"dag\" in globals():\n",
        "            dag.delete()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    # Delete the custom training job\n",
        "    try:\n",
        "        if \"job\" in globals():\n",
        "            job.delete()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    # Delete the batch prediction job using the Vertex batch prediction object\n",
        "    try:\n",
        "        if \"batch_predict_job\" in globals():\n",
        "            batch_predict_job.delete()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    # Delete the hyperparameter tuning job using the Vertex hyperparameter tuning object\n",
        "    try:\n",
        "        if \"hpt_job\" in globals():\n",
        "            hpt_job.delete()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    if \"BUCKET_NAME\" in globals():\n",
        "        ! gsutil rm -r $BUCKET_NAME"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "get_started_with_dataflow_pipeline_components.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

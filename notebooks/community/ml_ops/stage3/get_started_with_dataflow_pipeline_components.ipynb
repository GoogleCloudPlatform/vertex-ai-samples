{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "copyright"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title:generic,gcp"
      },
      "source": [
        "# E2E ML on GCP: MLOps stage 3 : formalization: get started with Dataflow pipeline components\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/get_started_with_dataflow_pipeline_components.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/ai/platform/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/get_started_with_dataflow_pipeline_components.ipynb\">\n",
        "<img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> \n",
        "        Colab logo Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/get_started_with_dataflow_pipeline_components.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>   \n",
        "    \n",
        "</table>\n",
        "<br/><br/><br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview:mlops"
      },
      "source": [
        "## Overview\n",
        "\n",
        "\n",
        "This tutorial demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. This tutorial covers stage 3 : formalization: get started with Dataflow pipeline components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset:gsod,lrg"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset used for this tutorial is the GSOD dataset from [BigQuery public datasets](https://cloud.google.com/bigquery/public-data). The version of the dataset you use only consists of the fields `year`, `month` and `day` to predict the value of mean daily temperature (`mean_temp`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "objective:mlops,stage3,get_started_dataflow_pipeline_components"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to use prebuilt `Google Cloud Pipeline Components` for `Dataflow`.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services:\n",
        "\n",
        "- `Vertex AI Pipelines`\n",
        "- `Google Cloud Pipeline Components`\n",
        "- `Dataflow`\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Build an Apache Beam data pipeline.\n",
        "- Encapsulate the Apache Beam data pipeline with a Dataflow component in a Vertex AI pipeline.\n",
        "- Execute a Vertex AI pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset:gsod,lrg"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset used for this tutorial is the GSOD dataset from [BigQuery public datasets](https://cloud.google.com/bigquery/public-data). The version of the dataset you use only the fields year, month and day to predict the value of mean daily temperature (mean_temp)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c997d8d92ce"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "* Dataflow\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), and [Dataflow pricing](https://cloud.google.com/dataflow/pricing)\n",
        "and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_mlops"
      },
      "source": [
        "## Installations\n",
        "\n",
        "Install *one time* the packages for executing the MLOps notebooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_mlops"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\"\n",
        "\n",
        "! pip3 install -U tensorflow $USER_FLAG\n",
        "! pip3 install -U tensorflow-data-validation $USER_FLAG\n",
        "! pip3 install -U tensorflow-transform $USER_FLAG\n",
        "! pip3 install -U tensorflow-io $USER_FLAG\n",
        "! pip3 install --upgrade google-cloud-aiplatform[tensorboard] $USER_FLAG\n",
        "! pip3 install --upgrade google-cloud-pipeline-components $USER_FLAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "restart"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "restart"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb082379ed5b"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "1. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). {TODO: Update the APIs needed for your tutorial. Edit the API names, and update the link to append the API IDs, separating each one with a comma. For example, container.googleapis.com,cloudbuild.googleapis.com}\n",
        "\n",
        "1. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
        "\n",
        "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "project_id"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_project_id"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_project_id"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID:\", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "250cb8c648d5"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
        "\n",
        "- Americas: `us-central1`\n",
        "- Europe: `europe-west4`\n",
        "- Asia Pacific: `asia-east1`\n",
        "\n",
        "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
        "\n",
        "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "region"
      },
      "outputs": [],
      "source": [
        "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
        "\n",
        "if REGION == \"[your-region]\":\n",
        "    REGION = \"us-central1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "timestamp"
      },
      "source": [
        "#### UUID\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a uuid for each instance session, and append it onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "timestamp"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "\n",
        "# Generate a uuid of length 8\n",
        "def generate_uuid():\n",
        "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=8))\n",
        "\n",
        "\n",
        "UUID = generate_uuid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "927085b84a07"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using Google Cloud Notebooks**, your environment is already authenticated. Skip this step.\n",
        "\n",
        "**If you are using Colab**, run the cell below and follow the instructions when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "In the Cloud Console, go to the [Create service account key](https://console.cloud.google.com/apis/credentials/serviceaccountkey) page.\n",
        "\n",
        "**Click Create service account**.\n",
        "\n",
        "In the **Service account name** field, enter a name, and click **Create**.\n",
        "\n",
        "In the **Grant this service account access to project** section, click the Role drop-down list. Type \"Vertex\" into the filter box, and select **Vertex Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "\n",
        "Click Create. A JSON file that contains your key downloads to your local environment.\n",
        "\n",
        "Enter the path to your service account key as the GOOGLE_APPLICATION_CREDENTIALS variable in the cell below and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89788a802687"
      },
      "outputs": [],
      "source": [
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# If on Google Cloud Notebook, then don't execute this code\n",
        "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40ed98f5cc48"
      },
      "source": [
        "#### If you are using Google Cloud Notebooks, set the project using gcloud config."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fde1a355f1e9"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        ! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bucket:mbsdk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "When you initialize the Vertex SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bucket"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_bucket"
      },
      "outputs": [],
      "source": [
        "if BUCKET_URI == \"\" or BUCKET_URI is None or BUCKET_URI == \"gs://[your-bucket-name]\":\n",
        "    BUCKET_URI = \"gs://\" + PROJECT_ID + \"aip-\" + UUID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_bucket"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "validate_bucket"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "validate_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "set_service_account"
      },
      "source": [
        "#### Service Account\n",
        "\n",
        "**If you don't know your service account**, try to get your service account using `gcloud` command by executing the second cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_service_account"
      },
      "outputs": [],
      "source": [
        "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_service_account"
      },
      "outputs": [],
      "source": [
        "if (\n",
        "    SERVICE_ACCOUNT == \"\"\n",
        "    or SERVICE_ACCOUNT is None\n",
        "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
        "):\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = !gcloud auth list 2>/dev/null\n",
        "    SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
        "    print(\"Service Account:\", SERVICE_ACCOUNT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "set_service_account:pipelines"
      },
      "source": [
        "#### Set service account access for Vertex AI Pipelines\n",
        "\n",
        "Run the following commands to grant your service account access to read and write pipeline artifacts in the bucket that you created in the previous step -- you only need to run these once per service account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_service_account:pipelines"
      },
      "outputs": [],
      "source": [
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
        "\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_vars"
      },
      "source": [
        "### Set up variables\n",
        "\n",
        "Next, set up some variables used throughout the tutorial.\n",
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "import google.cloud.aiplatform as aiplatform\n",
        "from google_cloud_pipeline_components.v1.dataflow import DataflowPythonJobOp\n",
        "from google_cloud_pipeline_components.v1.wait_gcp_resources import \\\n",
        "    WaitGcpResourcesOp\n",
        "from kfp import dsl\n",
        "from kfp.v2 import compiler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "writefile:wc.py"
      },
      "source": [
        "### Write the Apache Beam pipeline module\n",
        "\n",
        "First, you write the Python module for the Dataflow pipeline. Since it is a module, you additional add the `if __name__ == '__main__':` entry point and use `argparse` to pass command line arguments to the module.\n",
        "\n",
        "This module implements the Apache Beam word count example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "writefile:wc.py"
      },
      "outputs": [],
      "source": [
        "%%writefile wc.py\n",
        "#\n",
        "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
        "# contributor license agreements.  See the NOTICE file distributed with\n",
        "# this work for additional information regarding copyright ownership.\n",
        "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
        "# (the \"License\"); you may not use this file except in compliance with\n",
        "# the License.  You may obtain a copy of the License at\n",
        "#\n",
        "#    http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "#\n",
        "\n",
        "\"\"\"A minimalist word-counting workflow that counts words in Shakespeare.\n",
        "\n",
        "This is the first in a series of successively more detailed 'word count'\n",
        "examples.\n",
        "\n",
        "Next, see the wordcount pipeline, then the wordcount_debugging pipeline, for\n",
        "more detailed examples that introduce additional concepts.\n",
        "\n",
        "Concepts:\n",
        "\n",
        "1. Reading data from text files\n",
        "2. Specifying 'inline' transforms\n",
        "3. Counting a PCollection\n",
        "4. Writing data to Cloud Storage as text files\n",
        "\n",
        "To execute this pipeline locally, first edit the code to specify the output\n",
        "location. Output location could be a local file path or an output prefix\n",
        "on GCS. (Only update the output location marked with the first CHANGE comment.)\n",
        "\n",
        "To execute this pipeline remotely, first edit the code to set your project ID,\n",
        "runner type, the staging location, the temp location, and the output location.\n",
        "The specified GCS bucket(s) must already exist. (Update all the places marked\n",
        "with a CHANGE comment.)\n",
        "\n",
        "Then, run the pipeline as described in the README. It will be deployed and run\n",
        "using the Google Cloud Dataflow Service. No args are required to run the\n",
        "pipeline. You can see the results in your output bucket in the GCS browser.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import argparse\n",
        "import logging\n",
        "import re\n",
        "\n",
        "from past.builtins import unicode\n",
        "\n",
        "import apache_beam as beam\n",
        "from apache_beam.io import ReadFromText\n",
        "from apache_beam.io import WriteToText\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "from apache_beam.options.pipeline_options import SetupOptions\n",
        "\n",
        "\n",
        "def run(argv=None):\n",
        "  \"\"\"Main entry point; defines and runs the wordcount pipeline.\"\"\"\n",
        "\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument('--input',\n",
        "                      dest='input',\n",
        "                      default='gs://dataflow-samples/shakespeare/kinglear.txt',\n",
        "                      help='Input file to process.')\n",
        "  parser.add_argument('--output',\n",
        "                      dest='output',\n",
        "                      # CHANGE 1/5: The Google Cloud Storage path is required\n",
        "                      # for outputting the results.\n",
        "                      default='gs://YOUR_OUTPUT_BUCKET/AND_OUTPUT_PREFIX',\n",
        "                      help='Output file to write results to.')\n",
        "  known_args, pipeline_args = parser.parse_known_args(argv)\n",
        "  # pipeline_args.extend([\n",
        "  #     # CHANGE 2/5: (OPTIONAL) Change this to DataflowRunner to\n",
        "  #     # run your pipeline on the Google Cloud Dataflow Service.\n",
        "  #     '--runner=DirectRunner',\n",
        "  #     # CHANGE 3/5: Your project ID is required in order to run your pipeline on\n",
        "  #     # the Google Cloud Dataflow Service.\n",
        "  #     '--project=SET_YOUR_PROJECT_ID_HERE',\n",
        "  #     # CHANGE 4/5: Your Google Cloud Storage path is required for staging local\n",
        "  #     # files.\n",
        "  #     '--staging_location=gs://YOUR_BUCKET_NAME/AND_STAGING_DIRECTORY',\n",
        "  #     # CHANGE 5/5: Your Google Cloud Storage path is required for temporary\n",
        "  #     # files.\n",
        "  #     '--temp_location=gs://YOUR_BUCKET_NAME/AND_TEMP_DIRECTORY',\n",
        "  #     '--job_name=your-wordcount-job',\n",
        "  # ])\n",
        "\n",
        "  # We use the save_main_session option because one or more DoFn's in this\n",
        "  # workflow rely on global context (e.g., a module imported at module level).\n",
        "  pipeline_options = PipelineOptions(pipeline_args)\n",
        "  pipeline_options.view_as(SetupOptions).save_main_session = True\n",
        "  with beam.Pipeline(options=pipeline_options) as p:\n",
        "\n",
        "    # Read the text file[pattern] into a PCollection.\n",
        "    lines = p | ReadFromText(known_args.input)\n",
        "\n",
        "    # Count the occurrences of each word.\n",
        "    counts = (\n",
        "        lines\n",
        "        | 'Split' >> (beam.FlatMap(lambda x: re.findall(r'[A-Za-z\\']+', x))\n",
        "                      .with_output_types(unicode))\n",
        "        | 'PairWithOne' >> beam.Map(lambda x: (x, 1))\n",
        "        | 'GroupAndSum' >> beam.CombinePerKey(sum))\n",
        "\n",
        "    # Format the counts into a PCollection of strings.\n",
        "    def format_result(word_count):\n",
        "      (word, count) = word_count\n",
        "      return '%s: %s' % (word, count)\n",
        "\n",
        "    output = counts | 'Format' >> beam.Map(format_result)\n",
        "\n",
        "    # Write the output using a \"Write\" transform that has side effects.\n",
        "    # pylint: disable=expression-not-assigned\n",
        "    output | WriteToText(known_args.output)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  logging.getLogger().setLevel(logging.INFO)\n",
        "  run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "writefile:requirements,wc"
      },
      "source": [
        "### Write the requirements (installs) for the Apache Beam pipeline module\n",
        "\n",
        "Next, create the `requirements.txt` file to specify Python modules that are required to be installed for executing the Apache Beam pipeline module -- in this case, `apache-beam` is required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "writefile:requirements,wc"
      },
      "outputs": [],
      "source": [
        "%%writefile requirements.txt\n",
        "apache-beam\n",
        "future"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "copy_to_gcs:wc"
      },
      "source": [
        "### Copy python module and requirements file to Cloud Storage\n",
        "\n",
        "Next, you copy the Python module and requirements file to your Cloud Storage bucket.\n",
        "\n",
        "Additional, you set the Cloud Storage location for the output of the Apache Beam word count pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "copy_to_gcs:wc"
      },
      "outputs": [],
      "source": [
        "GCS_WC_PY = BUCKET_URI + \"/wc.py\"\n",
        "! gsutil cp wc.py $GCS_WC_PY\n",
        "GCS_REQUIREMENTS_TXT = BUCKET_URI + \"/requirements.txt\"\n",
        "! gsutil cp requirements.txt $GCS_REQUIREMENTS_TXT\n",
        "\n",
        "GCS_WC_OUT = BUCKET_URI + \"/wc_out.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_dataflow_pipeline:wc"
      },
      "source": [
        "### Create and execute the pipeline job\n",
        "\n",
        "In this example, the `DataflowPythonJobOp` component takes the following parameters:\n",
        "\n",
        "- `project_id`: The project ID.\n",
        "- `location`: The region.\n",
        "- `python_module_path`: The Cloud Storage location of the Apache Beam pipeline.\n",
        "- `temp_location`: The Cloud Storage temporary file workspace for the Apache Beam pipeline.\n",
        "- `requirements_file_path`: The required Python modules to install.\n",
        "- `args`: The arguments to pass to the Apache Beam pipeline.\n",
        "\n",
        "Learn more about [Google Cloud Pipeline Component for Dataflow](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-0.2.0/google_cloud_pipeline_components.experimental.dataflow.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_dataflow_pipeline:wc"
      },
      "outputs": [],
      "source": [
        "PIPELINE_ROOT = \"{}/pipeline_root/dataflow_wc\".format(BUCKET_URI)\n",
        "\n",
        "\n",
        "@dsl.pipeline(name=\"dataflow-wc\", description=\"Dataflow word count component pipeline\")\n",
        "def pipeline(\n",
        "    python_file_path: str = GCS_WC_PY,\n",
        "    project_id: str = PROJECT_ID,\n",
        "    location: str = REGION,\n",
        "    staging_dir: str = PIPELINE_ROOT,\n",
        "    args: list = [\"--output\", GCS_WC_OUT, \"--runner\", \"DataflowRunner\"],\n",
        "    requirements_file_path: str = GCS_REQUIREMENTS_TXT,\n",
        "):\n",
        "\n",
        "    dataflow_python_op = DataflowPythonJobOp(\n",
        "        project=project_id,\n",
        "        location=location,\n",
        "        python_module_path=python_file_path,\n",
        "        temp_location=staging_dir,\n",
        "        requirements_file_path=requirements_file_path,\n",
        "        args=args,\n",
        "    )\n",
        "\n",
        "    _ = WaitGcpResourcesOp(gcp_resources=dataflow_python_op.outputs[\"gcp_resources\"])\n",
        "\n",
        "\n",
        "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"dataflow_wc.json\")\n",
        "\n",
        "pipeline = aiplatform.PipelineJob(\n",
        "    display_name=\"dataflow_wc\",\n",
        "    template_path=\"dataflow_wc.json\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        "    enable_caching=False,\n",
        ")\n",
        "\n",
        "pipeline.run()\n",
        "\n",
        "! gsutil cat {GCS_WC_OUT}* | head -n10\n",
        "\n",
        "! rm -f dataflow_wc.json wc.py requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "delete_pipeline"
      },
      "source": [
        "### Delete the pipeline job\n",
        "\n",
        "After a pipeline job is completed, you can delete the pipeline job with the method `delete()`.  Prior to completion, a pipeline job can be canceled with the method `cancel()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "delete_pipeline"
      },
      "outputs": [],
      "source": [
        "pipeline.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "writefile:split.py"
      },
      "source": [
        "### Write the Apache Beam pipeline module\n",
        "\n",
        "Next, you write the Python module for the Apache Beam pipeline. This module implements the a dataset split task into training and test data, and writes the split dataset as CSV files to a Cloud Storage bucket. In this example, the Python module will recieve some arguments for the pipeline from the command-line, which will be passed by the Dataflow pipeline component.\n",
        "\n",
        "*Note:* The Dataflow prebuilt component implicitly adds Dataflow-specific command-line arguments, such as `project`, `location`, `runner`, and `temp_location`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "writefile:split.py"
      },
      "outputs": [],
      "source": [
        "%%writefile split.py\n",
        "\n",
        "import argparse\n",
        "import logging\n",
        "import tensorflow_transform.beam as tft_beam\n",
        "\n",
        "\n",
        "from past.builtins import unicode\n",
        "\n",
        "import apache_beam as beam\n",
        "from apache_beam.io import ReadFromText\n",
        "from apache_beam.io import WriteToText\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "from apache_beam.options.pipeline_options import SetupOptions\n",
        "\n",
        "def run(argv=None):\n",
        "    \"\"\"Main entry point; defines and runs the wordcount pipeline.\"\"\"\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--bq_table',\n",
        "                      dest='bq_table')\n",
        "    parser.add_argument('--bucket',\n",
        "                      dest='bucket')\n",
        "    args, pipeline_args = parser.parse_known_args(argv)\n",
        "    logging.info(\"ARGS\")\n",
        "    logging.info(args)\n",
        "    logging.info(\"PIPELINE ARGS\")\n",
        "    logging.info(pipeline_args)\n",
        "    for i in range(0, len(pipeline_args), 2):\n",
        "        if \"--temp_location\" == pipeline_args[i]:\n",
        "            temp_location = pipeline_args[i+1]\n",
        "        elif \"--project\" == pipeline_args[i]:\n",
        "            project = pipeline_args[i+1]\n",
        "\n",
        "    exported_train = args.bucket + '/exported_data/train'\n",
        "    exported_eval  = args.bucket + '/exported_data/eval'\n",
        "\n",
        "    pipeline_options = PipelineOptions(pipeline_args)\n",
        "    pipeline_options.view_as(SetupOptions).save_main_session = True\n",
        "    with beam.Pipeline(options=pipeline_options) as pipeline:\n",
        "        with tft_beam.Context(temp_location):\n",
        "            raw_data_query = \"SELECT {0},{1} FROM {2} LIMIT 500\".format(\"CAST(station_number as STRING) AS station_number,year,month,day\",\"mean_temp\", args.bq_table)\n",
        "\n",
        "            def parse_bq_record(bq_record):\n",
        "                \"\"\"Parses a bq_record to a dictionary.\"\"\"\n",
        "                output = {}\n",
        "                for key in bq_record:\n",
        "                    output[key] = [bq_record[key]]\n",
        "                return output\n",
        "\n",
        "            def split_dataset(bq_row, num_partitions, ratio):\n",
        "                \"\"\"Returns a partition number for a given bq_row.\"\"\"\n",
        "                import json\n",
        "\n",
        "                assert num_partitions == len(ratio)\n",
        "                bucket = sum(map(ord, json.dumps(bq_row))) % sum(ratio)\n",
        "                total = 0\n",
        "                for i, part in enumerate(ratio):\n",
        "                    total += part\n",
        "                    if bucket < total:\n",
        "                        return i\n",
        "                return len(ratio) - 1\n",
        "\n",
        "            # Read raw BigQuery data.\n",
        "            raw_train_data, raw_eval_data = (\n",
        "                pipeline\n",
        "                | \"Read Raw Data\"\n",
        "                >> beam.io.ReadFromBigQuery(\n",
        "                    query=raw_data_query,\n",
        "                    project=project,\n",
        "                    use_standard_sql=True,\n",
        "                )\n",
        "                | \"Parse Data\" >> beam.Map(parse_bq_record)\n",
        "                | \"Split\" >> beam.Partition(split_dataset, 2, ratio=[8, 2])\n",
        "            )\n",
        "\n",
        "            # Write raw train data to GCS .\n",
        "            _ = raw_train_data | \"Write Raw Train Data\" >> beam.io.WriteToText(\n",
        "                file_path_prefix=exported_train, file_name_suffix=\".csv\"\n",
        "            )\n",
        "\n",
        "            # Write raw eval data to GCS .\n",
        "            _ = raw_eval_data | \"Write Raw Eval Data\" >> beam.io.WriteToText(\n",
        "                file_path_prefix=exported_eval, file_name_suffix=\".csv\"\n",
        "            )\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "    run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "writefile:requirements,split"
      },
      "source": [
        "### Write the requirements (installs) for the Apache Beam pipeline module\n",
        "\n",
        "Next, create the `requirements.txt` file to specify Python modules that are required to be installed for executing the Apache Beam pipeline module -- in this case, `apache-beam` and `tensorflow-transform` are required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "writefile:requirements,split"
      },
      "outputs": [],
      "source": [
        "%%writefile requirements.txt\n",
        "apache-beam\n",
        "tensorflow-transform==1.2.0\n",
        "future"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "writefile:setup,split"
      },
      "source": [
        "### Write the setup.py (installs) for the Dataflow workers\n",
        "\n",
        "Next, create the `setup.py` file to specify Python modules that are required to be installed for executing the Dataflow workers -- in this case, `tensorflow-transform` is required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "writefile:setup,split"
      },
      "outputs": [],
      "source": [
        "%%writefile setup.py\n",
        "import setuptools\n",
        "\n",
        "REQUIRED_PACKAGES = [\n",
        "    'tensorflow-transform==1.2.0',\n",
        "    'future'\n",
        "]\n",
        "PACKAGE_NAME = 'my_package'\n",
        "PACKAGE_VERSION = '0.0.1'\n",
        "setuptools.setup(\n",
        "    name=PACKAGE_NAME,\n",
        "    version=PACKAGE_VERSION,\n",
        "    description='Demo for split transformation',\n",
        "    install_requires=REQUIRED_PACKAGES,\n",
        "    author=\"cdpe@google.com\",\n",
        "    packages=setuptools.find_packages()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "copy_to_gcs:split"
      },
      "source": [
        "### Copy python module and requirements file to Cloud Storage\n",
        "\n",
        "Next, you copy the Python module, requirements and setup file to your Cloud Storage bucket.\n",
        "\n",
        "Additional, you set the Cloud Storage location for the output of the Apache Beam dataset split pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "copy_to_gcs:split"
      },
      "outputs": [],
      "source": [
        "GCS_SPLIT_PY = BUCKET_URI + \"/split.py\"\n",
        "! gsutil cp split.py $GCS_SPLIT_PY\n",
        "GCS_REQUIREMENTS_TXT = BUCKET_URI + \"/requirements.txt\"\n",
        "! gsutil cp requirements.txt $GCS_REQUIREMENTS_TXT\n",
        "GCS_SETUP_PY = BUCKET_URI + \"/setup.py\"\n",
        "! gsutil cp setup.py $GCS_SETUP_PY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_file:u_dataset,bq"
      },
      "source": [
        "#### Location of BigQuery training data.\n",
        "\n",
        "Now set the variable `IMPORT_FILE` to the location of the data table in BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_file:gsod,bq,lrg"
      },
      "outputs": [],
      "source": [
        "IMPORT_FILE = \"bq://bigquery-public-data.samples.gsod\"\n",
        "BQ_TABLE = \"bigquery-public-data.samples.gsod\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_dataflow_pipeline:split"
      },
      "source": [
        "### Create and execute the pipeline job\n",
        "\n",
        "In this example, the `DataflowPythonJobOp` component takes the following parameters:\n",
        "\n",
        "- `project_id`: The project ID.\n",
        "- `location`: The region.\n",
        "- `python_module_path`: The Cloud Storage location of the Apache Beam pipeline.\n",
        "- `temp_location`: The Cloud Storage temporary file workspace for the Apache Beam pipeline.\n",
        "- `requirements_file_path`: The required Python modules to install.\n",
        "- `args`: The arguments to pass to the Apache Beam pipeline.\n",
        "\n",
        "Learn more about [Google Cloud Pipeline Component for Dataflow](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-0.2.0/google_cloud_pipeline_components.experimental.dataflow.html)\n",
        "\n",
        "Additional, you add `--runner=DataflowRunner` to the input args, to tell the component to use Dataflow instead of DirectRunner for the Apache Beam job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_dataflow_pipeline:split"
      },
      "outputs": [],
      "source": [
        "PIPELINE_ROOT = \"{}/pipeline_root/dataflow_split\".format(BUCKET_URI)\n",
        "\n",
        "\n",
        "@dsl.pipeline(name=\"dataflow-split\", description=\"Dataflow split dataset\")\n",
        "def pipeline(\n",
        "    python_file_path: str = GCS_SPLIT_PY,\n",
        "    project_id: str = PROJECT_ID,\n",
        "    location: str = REGION,\n",
        "    staging_dir: str = PIPELINE_ROOT,\n",
        "    args: list = [\n",
        "        \"--bucket\",\n",
        "        BUCKET_URI,\n",
        "        \"--bq_table\",\n",
        "        BQ_TABLE,\n",
        "        \"--runner\",\n",
        "        \"DataflowRunner\",\n",
        "        \"--setup_file\",\n",
        "        GCS_SETUP_PY,\n",
        "    ],\n",
        "    requirements_file_path: str = GCS_REQUIREMENTS_TXT,\n",
        "):\n",
        "    # DataflowPythonJobOp.component_spec.implementation.container.image = \"gcr.io/ml-pipeline/google-cloud-pipeline-components:v0.2.0_dataflow_logs_fix\"\n",
        "    dataflow_python_op = DataflowPythonJobOp(\n",
        "        project=project_id,\n",
        "        location=location,\n",
        "        python_module_path=python_file_path,\n",
        "        temp_location=staging_dir,\n",
        "        requirements_file_path=requirements_file_path,\n",
        "        args=args,\n",
        "    )\n",
        "\n",
        "    _ = WaitGcpResourcesOp(gcp_resources=dataflow_python_op.outputs[\"gcp_resources\"])\n",
        "\n",
        "\n",
        "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"dataflow_split.json\")\n",
        "\n",
        "pipeline = aiplatform.PipelineJob(\n",
        "    display_name=\"dataflow_split\",\n",
        "    template_path=\"dataflow_split.json\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        "    enable_caching=False,\n",
        ")\n",
        "\n",
        "pipeline.run()\n",
        "\n",
        "! gsutil ls {BUCKET_URI}/exported_data\n",
        "\n",
        "! rm -f dataflow_split.json split.py requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "delete_pipeline"
      },
      "source": [
        "### Delete the pipeline job\n",
        "\n",
        "After a pipeline job is completed, you can delete the pipeline job with the method `delete()`.  Prior to completion, a pipeline job can be canceled with the method `cancel()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "delete_pipeline"
      },
      "outputs": [],
      "source": [
        "pipeline.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cleanup:mbsdk"
      },
      "source": [
        "# Clean up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:\n",
        "\n",
        "- Pipeline Job\n",
        "- Cloud Storage Bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup:mbsdk"
      },
      "outputs": [],
      "source": [
        "# Warning: Setting this to true will delete everything in your bucket\n",
        "delete_bucket = False\n",
        "\n",
        "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
        "    ! gsutil rm -r $BUCKET_URI"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "get_started_with_dataflow_pipeline_components.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

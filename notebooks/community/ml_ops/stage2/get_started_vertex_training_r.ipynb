{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "copyright"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title:generic,gcp"
      },
      "source": [
        "# E2E ML on GCP: MLOps stage 2 : experimentation: get started with Vertex AI Training for R\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage2/get_started_vertex_training_r.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage2/get_started_vertex_training_r.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "<br/><br/><br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview:mlops"
      },
      "source": [
        "## Overview\n",
        "\n",
        "\n",
        "This tutorial demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. This tutorial covers stage 2 : experimentation: get started with Vertex AI Training for R. Please note that this notebook should be ran only in R notebook image (e.g., R4.1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset:r,iris,lcn"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset used for this tutorial is the Iris dataset built into the R package. This dataset does not require any feature engineering. The trained model predicts the type of Iris flower species from a class of three species: setosa, virginica, or versicolor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "objective:mlops,stage2,get_started_vertex_training_r"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to use `Vertex AI Training` for training a R custom model.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services:\n",
        "\n",
        "- `Vertex AI Training`\n",
        "- `Vertex AI Model` resource\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Locally train an R model in a notebook using %%R magic commands\n",
        "- Create a deployment image with trained R model and serving functions.\n",
        "- Test the deployment image locally.\n",
        "- Create a `Vertex AI Model` resource for the deployment image with embedded R model.\n",
        "- Deploy the deployment image with embedded R model to a `Vertex AI Endpoint` resource.\n",
        "- Test the deployment image with embedded R model.\n",
        "- Create a R-to-Python training package.\n",
        "- Create a training image for training the model.\n",
        "- Train a R model using `Vertex AI Trainingh` service with the R-to-Python training package."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c997d8d92ce"
      },
      "source": [
        "### Costs \n",
        "\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_mlops"
      },
      "source": [
        "## Installations\n",
        "\n",
        "Install *one time* the packages for executing the MLOps notebooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fd00fa70a2a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_mlops"
      },
      "outputs": [],
      "source": [
        "! pip3 install --upgrade google-cloud-aiplatform[tensorboard] $USER_FLAG\n",
        "! pip3 install --upgrade rpy2 $USER_FLAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "restart"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "restart"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e3cab0cc491"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be929e7b4d76"
      },
      "source": [
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "1. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). {TODO: Update the APIs needed for your tutorial. Edit the API names, and update the link to append the API IDs, separating each one with a comma. For example, container.googleapis.com,cloudbuild.googleapis.com}\n",
        "\n",
        "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
        "\n",
        "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "project_id"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_project_id"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_project_id"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID:\", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_gcloud_project_id"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
        "\n",
        "- Americas: `us-central1`\n",
        "- Europe: `europe-west4`\n",
        "- Asia Pacific: `asia-east1`\n",
        "\n",
        "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
        "\n",
        "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "region"
      },
      "outputs": [],
      "source": [
        "REGION = \"[your-region]\"  # @param {type:\"string\"}\n",
        "if REGION == \"[your-region]\":\n",
        "    REGION = \"us-central1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "timestamp"
      },
      "source": [
        "#### Timestamp\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "timestamp"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bucket:mbsdk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "When you initialize the Vertex AI SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bucket"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_bucket"
      },
      "outputs": [],
      "source": [
        "if BUCKET_URI == \"\" or BUCKET_URI is None or BUCKET_URI == \"gs://[your-bucket-name]\":\n",
        "    BUCKET_URI = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_bucket"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "validate_bucket"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "validate_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_vars"
      },
      "source": [
        "### Set up variables\n",
        "\n",
        "Next, set up some variables used throughout the tutorial.\n",
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "import traceback\n",
        "\n",
        "import google.cloud.aiplatform as aip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "accelerators:training,cpu,prediction,cpu,mbsdk"
      },
      "source": [
        "#### Set hardware accelerators\n",
        "\n",
        "You can set hardware accelerators for training and prediction.\n",
        "\n",
        "Set the variables `TRAIN_GPU/TRAIN_NGPU` and `DEPLOY_GPU/DEPLOY_NGPU` to use a container image supporting a GPU and the number of GPUs allocated to the virtual machine (VM) instance. For example, to use a GPU container image with 4 Nvidia Telsa K80 GPUs allocated to each VM, you would specify:\n",
        "\n",
        "    (aip.AcceleratorType.NVIDIA_TESLA_K80, 4)\n",
        "\n",
        "\n",
        "Otherwise specify `(None, None)` to use a container image to run on a CPU.\n",
        "\n",
        "Learn more about [hardware accelerator support for your region](https://cloud.google.com/vertex-ai/docs/general/locations#accelerators).\n",
        "\n",
        "*Note*: TF releases before 2.3 for GPU support will fail to load the custom model in this tutorial. It is a known issue and fixed in TF 2.3. This is caused by static graph ops that are generated in the serving function. If you encounter this issue on your own custom models, use a container image for TF 2.3 with GPU support."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "accelerators:training,cpu,prediction,cpu,mbsdk"
      },
      "outputs": [],
      "source": [
        "if os.getenv(\"IS_TESTING_TRAIN_GPU\"):\n",
        "    TRAIN_GPU, TRAIN_NGPU = (\n",
        "        aip.gapic.AcceleratorType.NVIDIA_TESLA_K80,\n",
        "        int(os.getenv(\"IS_TESTING_TRAIN_GPU\")),\n",
        "    )\n",
        "else:\n",
        "    TRAIN_GPU, TRAIN_NGPU = (None, None)\n",
        "\n",
        "if os.getenv(\"IS_TESTING_DEPLOY_GPU\"):\n",
        "    DEPLOY_GPU, DEPLOY_NGPU = (\n",
        "        aip.gapic.AcceleratorType.NVIDIA_TESLA_K80,\n",
        "        int(os.getenv(\"IS_TESTING_DEPLOY_GPU\")),\n",
        "    )\n",
        "else:\n",
        "    DEPLOY_GPU, DEPLOY_NGPU = (None, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "machine:training,prediction"
      },
      "source": [
        "#### Set machine type\n",
        "\n",
        "Next, set the machine type to use for training and prediction.\n",
        "\n",
        "- Set the variables `TRAIN_COMPUTE` and `DEPLOY_COMPUTE` to configure  the compute resources for the VMs you will use for for training and prediction.\n",
        " - `machine type`\n",
        "     - `n1-standard`: 3.75GB of memory per vCPU.\n",
        "     - `n1-highmem`: 6.5GB of memory per vCPU\n",
        "     - `n1-highcpu`: 0.9 GB of memory per vCPU\n",
        " - `vCPUs`: number of \\[2, 4, 8, 16, 32, 64, 96 \\]\n",
        "\n",
        "*Note: The following is not supported for training:*\n",
        "\n",
        " - `standard`: 2 vCPUs\n",
        " - `highcpu`: 2, 4 and 8 vCPUs\n",
        "\n",
        "*Note: You may also use n2 and e2 machine types for training and deployment, but they do not support GPUs*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "machine:training,prediction"
      },
      "outputs": [],
      "source": [
        "if os.getenv(\"IS_TESTING_TRAIN_MACHINE\"):\n",
        "    MACHINE_TYPE = os.getenv(\"IS_TESTING_TRAIN_MACHINE\")\n",
        "else:\n",
        "    MACHINE_TYPE = \"n1-standard\"\n",
        "\n",
        "VCPU = \"4\"\n",
        "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
        "print(\"Train machine type\", TRAIN_COMPUTE)\n",
        "\n",
        "if os.getenv(\"IS_TESTING_DEPLOY_MACHINE\"):\n",
        "    MACHINE_TYPE = os.getenv(\"IS_TESTING_DEPLOY_MACHINE\")\n",
        "else:\n",
        "    MACHINE_TYPE = \"n1-standard\"\n",
        "\n",
        "VCPU = \"4\"\n",
        "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
        "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_intro"
      },
      "source": [
        "## Introduction to R training\n",
        "\n",
        "### Training a R model\n",
        "\n",
        "You can either train your model locally, or use the `Vertex AI Training` service. In the later case, you would install the R-to-Python interpreter `rpy2` into your training instance (e.g., setup.py) and execute the R training script using the R-to-Python interpreter `rpy2`.\n",
        "\n",
        "\n",
        "### Deploying a R model\n",
        "\n",
        "Deploying a R model on `Vertex AI Prediction` service requires to use a custom container that serves online predictions. In this tutorial, you deploy a container running plumber R package to serve predictions from trained model artifacts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_ipython"
      },
      "source": [
        "### Load the R notebook interpreter\n",
        "\n",
        "Loading the module `rpy2.ipython` will add support for %R and %RR magic cells in your notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_ipython"
      },
      "outputs": [],
      "source": [
        "%load_ext rpy2.ipython"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_install:randomForest"
      },
      "source": [
        "### Install and import some additional R packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_install:randomForest"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "\n",
        "install.packages(c(\"randomForest\", \"plumber\"), repos = \"http://cran.us.r-project.org\")\n",
        "\n",
        "library(ggplot2)\n",
        "library(randomForest)\n",
        "library(plumber)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quick_peek:r"
      },
      "source": [
        "#### Quick peek at your data\n",
        "\n",
        "This tutorial uses a version of the iris dataset that is built into the R package.\n",
        "\n",
        "Start by doing a quick peek at the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quick_peek:r"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "\n",
        "head(iris)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "custom_dir"
      },
      "outputs": [],
      "source": [
        "# Make folder for R\n",
        "! rm -rf deploy custom\n",
        "! mkdir deploy custom custom/trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "locally_train:r,iris"
      },
      "source": [
        "### Locally train an R model\n",
        "\n",
        "First, you locally train the R model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "locally_train:r,iris"
      },
      "outputs": [],
      "source": [
        "%%R\n",
        "\n",
        "# train model\n",
        "model = randomForest(Species ~ ., data = iris)\n",
        "# save model\n",
        "save(model, file = \"deploy/model.RData\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "serving_script_r"
      },
      "source": [
        "### Serving script\n",
        "\n",
        "You create a R script for serving predictions. This script does the following:\n",
        "\n",
        "- Extracts prediction request values from the HTTP body of the input request.\n",
        "- Construct a prediction request for the R model.\n",
        "- Submit the prediction request to the R model.\n",
        "- Returns the predicted results.\n",
        "\n",
        "*Note*: Plumber makes use of comment “annotations” above functions to define the web service. When you feed the file into Plumber, you’ll get a runnable web service that other systems can interact with over a network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "serving_script_r"
      },
      "outputs": [],
      "source": [
        "%%writefile deploy/serving.R\n",
        "# serving.R\n",
        "\n",
        "library(\"randomForest\")\n",
        "\n",
        "#* Health check\n",
        "#* @get /ping\n",
        "#* @serializer unboxedJSON\n",
        "function() {\n",
        "    list(status = \"OK\")\n",
        "}\n",
        "\n",
        "#* @apiTitle flower classifier\n",
        "#* @param petal_length\n",
        "#* @param petal_width\n",
        "#* @param sepal_length\n",
        "#* @param sepal_width\n",
        "#* @post /classify\n",
        "function (req)\n",
        "{\n",
        "    instances <- as.data.frame(jsonlite::fromJSON(req$postBody))\n",
        "    results <- list()\n",
        "\n",
        "    load(\"./model.RData\")\n",
        "\n",
        "    for(i in 1:nrow(instances)) {       # for-loop over columns\n",
        "        petal_length <- instances[i, \"instances.petal_length\"]\n",
        "        petal_width <- instances[i, \"instances.petal_width\"]\n",
        "        sepal_length <- instances[i, \"instances.sepal_length\"]\n",
        "        sepal_width <- instances[i, \"instances.sepal_width\"]\n",
        "        test = c(sepal_length, sepal_width, petal_length, petal_width)\n",
        "        test = sapply(test, as.numeric)\n",
        "        test = data.frame(matrix(test, ncol = 4))\n",
        "        colnames(test) = colnames(iris[, 1:4])\n",
        "        results <- append(results, predict(model, test))\n",
        "    }\n",
        "\n",
        "    list(predictions = results)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "start_server_r"
      },
      "source": [
        "#### Script for running the R server\n",
        "\n",
        "Next, you create a file that runs the server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "start_server_r"
      },
      "outputs": [],
      "source": [
        "%%writefile deploy/startServer.R\n",
        "library(plumber)\n",
        "pr <- plumb(\"serving.R\")\n",
        "pr$run(host = \"0.0.0.0\", port = 7080)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "docker_write,prediction,r"
      },
      "source": [
        "### Make R container for prediction\n",
        "\n",
        "Currently, Vertex AI does not have a prefined container for making predictions with a deployed R model. No problem, you can assemble your own custom container. For this tutorial, you construct a deployment container from a Docker image as follows:\n",
        "\n",
        "    - Set the base image supplied by RStudio (rstudio/plumber).\n",
        "    - Package the model artifacts and serving scripts into a Docker image.\n",
        "    - Start the serving script on port 7080."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "docker_write,prediction,r"
      },
      "outputs": [],
      "source": [
        "%%writefile deploy/Dockerfile\n",
        "\n",
        "FROM rstudio/plumber\n",
        "\n",
        "# install random forest\n",
        "RUN R -e 'install.packages(c(\"randomForest\"), repos = \"https://cran.rstudio.com/\")'\n",
        "\n",
        "# Copy model and script\n",
        "RUN mkdir /app\n",
        "COPY model.RData /app\n",
        "COPY serving.R /app\n",
        "COPY startServer.R /app\n",
        "WORKDIR /app\n",
        "\n",
        "# plumber & run server\n",
        "EXPOSE 7080\n",
        "\n",
        "ENTRYPOINT [\"R\", \"-f\", \"/app/startServer.R\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "docker_push,prediction,r"
      },
      "outputs": [],
      "source": [
        "DEPLOY_IMAGE = f\"gcr.io/{PROJECT_ID}/r-predict-iris\"\n",
        "print(DEPLOY_IMAGE)\n",
        "\n",
        "! docker build --tag=$DEPLOY_IMAGE ./deploy\n",
        "\n",
        "! docker push $DEPLOY_IMAGE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_docker_locally:launch,r"
      },
      "source": [
        "### Locally test the Docker image\n",
        "\n",
        "Next, you locally test the Docker image you created for serving predictions.\n",
        "\n",
        "#### Launch the serving binary\n",
        "\n",
        "First, you launch the serving binary, listening on port 7080, and then ping it as a health check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_docker_locally:launch,r"
      },
      "outputs": [],
      "source": [
        "! docker stop local_iris 2>/dev/null\n",
        "! docker run -t -d --rm -p 7080:7080 --name=local_iris $DEPLOY_IMAGE\n",
        "! sleep 10\n",
        "! curl http://localhost:7080/ping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_docker_locally:predict,r"
      },
      "source": [
        "#### Send predicton request\n",
        "\n",
        "Next, you send a prediction request to the serving binary you locally launched. Afterwards, you will shutdown the launched serving binary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_docker_locally:predict,r"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "cat > ./deploy/instances.json <<END\n",
        "{\n",
        "  \"instances\": [{\n",
        "      \"sepal_width\": 1,\n",
        "      \"sepal_length\": 2,\n",
        "      \"petal_width\": 3,\n",
        "      \"petal_length\": 1\n",
        "    },\n",
        "    {\n",
        "      \"sepal_width\": 4,\n",
        "      \"sepal_length\": 2,\n",
        "      \"petal_width\": 1,\n",
        "      \"petal_length\": 1\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "END\n",
        "\n",
        "curl -s -X POST \\\n",
        "  -H \"Content-Type: application/json; charset=utf-8\" \\\n",
        "  -d @./deploy/instances.json \\\n",
        "  http://localhost:7080/classify\n",
        "\n",
        "docker stop local_iris 1>/dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upload_model,r"
      },
      "source": [
        "### Upload a R model to a `Vertex AI Model` resource\n",
        "\n",
        "Next you upload the R model to a Vertex AI Model resource, with the following parameters:\n",
        "\n",
        "- `display_name`: A human readable name for the model resource.\n",
        "- `serving_container_image_uri`: The deployment image that contains the serving binary and R model.\n",
        "- `serving_container_predict_route`: The URI endpoint for prediction requests.\n",
        "- `serving_container_health_route`: The URI endpoint for health ping.\n",
        "- `serving_container_ports`: A list of ports to listen on for prediction/health requests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upload_model,r"
      },
      "outputs": [],
      "source": [
        "DISPLAY_NAME = \"iris_\" + TIMESTAMP\n",
        "health_route = \"/ping\"\n",
        "predict_route = \"/classify\"\n",
        "serving_container_ports = [7080]\n",
        "\n",
        "model = aip.Model.upload(\n",
        "    display_name=DISPLAY_NAME,\n",
        "    serving_container_image_uri=DEPLOY_IMAGE,\n",
        "    serving_container_predict_route=predict_route,\n",
        "    serving_container_health_route=health_route,\n",
        "    serving_container_ports=serving_container_ports,\n",
        ")\n",
        "\n",
        "model.wait()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deploy_model:mbsdk,all"
      },
      "source": [
        "## Deploy the model\n",
        "\n",
        "Next, deploy your model for online prediction. To deploy the model, you invoke the `deploy` method, with the following parameters:\n",
        "\n",
        "- `deployed_model_display_name`: A human readable name for the deployed model.\n",
        "- `traffic_split`: Percent of traffic at the endpoint that goes to this model, which is specified as a dictionary of one or more key/value pairs.\n",
        "If only one model, then specify as { \"0\": 100 }, where \"0\" refers to this model being uploaded and 100 means 100% of the traffic.\n",
        "If there are existing models on the endpoint, for which the traffic will be split, then use model_id to specify as { \"0\": percent, model_id: percent, ... }, where model_id is the model id of an existing model to the deployed endpoint. The percents must add up to 100.\n",
        "- `machine_type`: The type of machine to use for training.\n",
        "- `accelerator_type`: The hardware accelerator type.\n",
        "- `accelerator_count`: The number of accelerators to attach to a worker replica.\n",
        "- `starting_replica_count`: The number of compute instances to initially provision.\n",
        "- `max_replica_count`: The maximum number of compute instances to scale to. In this tutorial, only one instance is provisioned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deploy_model:mbsdk,all"
      },
      "outputs": [],
      "source": [
        "DEPLOYED_NAME = \"iris-\" + TIMESTAMP\n",
        "\n",
        "TRAFFIC_SPLIT = {\"0\": 100}\n",
        "\n",
        "MIN_NODES = 1\n",
        "MAX_NODES = 1\n",
        "\n",
        "if DEPLOY_GPU:\n",
        "    endpoint = model.deploy(\n",
        "        deployed_model_display_name=DEPLOYED_NAME,\n",
        "        traffic_split=TRAFFIC_SPLIT,\n",
        "        machine_type=DEPLOY_COMPUTE,\n",
        "        accelerator_type=DEPLOY_GPU.name,\n",
        "        accelerator_count=DEPLOY_NGPU,\n",
        "        min_replica_count=MIN_NODES,\n",
        "        max_replica_count=MAX_NODES,\n",
        "    )\n",
        "else:\n",
        "    endpoint = model.deploy(\n",
        "        deployed_model_display_name=DEPLOYED_NAME,\n",
        "        traffic_split=TRAFFIC_SPLIT,\n",
        "        machine_type=DEPLOY_COMPUTE,\n",
        "        min_replica_count=MIN_NODES,\n",
        "        max_replica_count=MAX_NODES,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_deployed_model:iris"
      },
      "source": [
        "### Make test prediction\n",
        "\n",
        "Next, you test the deployed model by sending synthentic data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_deployed_model:iris"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    INSTANCES = [\n",
        "        {\"sepal_width\": 1, \"sepal_length\": 2, \"petal_width\": 3, \"petal_length\": 1},\n",
        "        {\"sepal_width\": 4, \"sepal_length\": 2, \"petal_width\": 1, \"petal_length\": 1},\n",
        "    ]\n",
        "\n",
        "    prediction = endpoint.predict(instances=INSTANCES)\n",
        "\n",
        "    print(prediction)\n",
        "except:\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "undeploy_model:mbsdk"
      },
      "source": [
        "## Undeploy the model\n",
        "\n",
        "When you are done doing predictions, you undeploy the model from the `Endpoint` resouce. This deprovisions all compute resources and ends billing for the deployed model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "undeploy_model:mbsdk"
      },
      "outputs": [],
      "source": [
        "endpoint.undeploy_all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "endpoint_delete:mbsdk"
      },
      "source": [
        "#### Delete the endpoint\n",
        "\n",
        "The method 'delete()' will delete the endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "endpoint_delete:mbsdk"
      },
      "outputs": [],
      "source": [
        "endpoint.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_delete:mbsdk"
      },
      "source": [
        "#### Delete the model\n",
        "\n",
        "The method 'delete()' will delete the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model_delete:mbsdk"
      },
      "outputs": [],
      "source": [
        "model.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taskpy_contents:iris,r"
      },
      "source": [
        "### Create the task script for the Python/R training package\n",
        "\n",
        "Next, you create the `task.py` script for driving the training package. Some noteable steps include:\n",
        "\n",
        "\n",
        "- Command-line arguments:\n",
        "    - `model-dir`: The location to save the trained model. When using Vertex AI custom training, the location will be specified in the environment variable: `AIP_MODEL_DIR`.\n",
        "\n",
        "\n",
        "- Training:\n",
        "    - Uses R-to-Python interpreter to run the R training script.\n",
        "\n",
        "\n",
        "- Model artifact saving:\n",
        "    - Saves the model artifacts at the Cloud Storage location specified by `model-dir`.\n",
        "    - *Note*: GCSFuse (`/gcs`) is used to do filesystem operations on Cloud Storage buckets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taskpy_contents:iris,r"
      },
      "outputs": [],
      "source": [
        "%%writefile custom/trainer/task.py\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import rpy2\n",
        "import argparse\n",
        "import logging\n",
        "\n",
        "# import rpy2's package module\n",
        "import rpy2.robjects.packages as rpackages\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--model-dir', dest='model_dir',\n",
        "                    default=os.getenv('AIP_MODEL_DIR'), type=str, help='Model dir.')\n",
        "args = parser.parse_args()\n",
        "\n",
        "logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "# import R's utility package\n",
        "randomForest = rpackages.importr('randomForest')\n",
        "\n",
        "# train the model\n",
        "logging.info(\"Model training started ...\")\n",
        "rpy2.robjects.r('''\n",
        "    # train model\n",
        "    model = randomForest(Species ~ ., data = iris)\n",
        "    # save model\n",
        "    save(model, file = \"model.RData\")\n",
        "'''\n",
        ")\n",
        "logging.info(\"Model training completed ...\")\n",
        "\n",
        "# GCSFuse conversion\n",
        "gs_prefix = 'gs://'\n",
        "gcsfuse_prefix = '/gcs/'\n",
        "if args.model_dir.startswith(gs_prefix):\n",
        "    args.model_dir = args.model_dir.replace(gs_prefix, gcsfuse_prefix)\n",
        "    dirpath = os.path.split(args.model_dir)[0]\n",
        "    if not os.path.isdir(dirpath):\n",
        "        os.makedirs(dirpath)\n",
        "\n",
        "# Upload the saved model file to Cloud Storage\n",
        "gcs_model_path = os.path.join(args.model_dir, 'model.RData')\n",
        "logging.info(\"Saving model artifacts to {}\". format(gcs_model_path))\n",
        "with open(\"model.RData\", \"rb\") as f:\n",
        "    data = f.read()\n",
        "with open(gcs_model_path, \"wb\") as f:\n",
        "    f.write(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "docker_write,training,r"
      },
      "source": [
        "### Make R container for training\n",
        "\n",
        "Currently, Vertex AI does not have a prefined container for training an R model. No problem, you can assemble your own custom container. For this tutorial, you construct a deployment container from a Docker image as follows:\n",
        "\n",
        "    - Set the base image to a TensorFlow Deep Learning image\n",
        "    - Install R-to-Python package and other R tools\n",
        "    - Install cloud storage package\n",
        "    - Package the model artifacts and serving scripts into a Docker image.\n",
        "    - Set the entry point in the container to run the training package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "docker_write,training,r"
      },
      "outputs": [],
      "source": [
        "%%writefile custom/Dockerfile\n",
        "\n",
        "FROM gcr.io/deeplearning-platform-release/tf2-cpu.2-3\n",
        "\n",
        "RUN apt-get update && apt-get install -y --no-install-recommends build-essential r-base r-cran-randomforest python3.6 python3-pip python3-setuptools python3-dev\n",
        "RUN pip install google-cloud-storage\n",
        "RUN pip install rpy2\n",
        "\n",
        "# install random forest\n",
        "RUN R -e 'install.packages(c(\"randomForest\"), repos = \"https://cran.rstudio.com/\")'\n",
        "\n",
        "WORKDIR /\n",
        "\n",
        "# Copies the trainer code to the docker image.\n",
        "COPY trainer /trainer\n",
        "\n",
        "# Sets up the entry point to invoke the trainer.\n",
        "ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "docker_push,training,r"
      },
      "outputs": [],
      "source": [
        "TRAIN_IMAGE = f\"gcr.io/{PROJECT_ID}/r-train-iris\"\n",
        "print(TRAIN_IMAGE)\n",
        "\n",
        "! docker build --tag=$TRAIN_IMAGE ./custom\n",
        "\n",
        "! docker push $TRAIN_IMAGE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_custom_container_training_job:mbsdk,no_model"
      },
      "source": [
        "### Create and run custom training job\n",
        "\n",
        "\n",
        "To train a custom model, you perform two steps: 1) create a custom training job, and 2) run the job.\n",
        "\n",
        "#### Create custom training job\n",
        "\n",
        "A custom training job is created with the `CustomTrainingJob` class, with the following parameters:\n",
        "\n",
        "- `display_name`: The human readable name for the custom training job.\n",
        "- `container_uri`: The training container image.\n",
        "\n",
        "- `command`: The command (e.g., interpreter) and script to invokee within the container.\n",
        "\n",
        "*Note:* The interpreter and script to invoke is overridable within the container (i.e., ENTRYPOINT)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_custom_container_training_job:mbsdk,no_model"
      },
      "outputs": [],
      "source": [
        "job = aip.CustomContainerTrainingJob(\n",
        "    display_name=\"iris_\" + TIMESTAMP,\n",
        "    container_uri=TRAIN_IMAGE,\n",
        "    command=[\"python3\", \"trainer/task.py\"],\n",
        ")\n",
        "\n",
        "print(job)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_custom_container_training_job:r"
      },
      "source": [
        "#### Run the custom container training job\n",
        "\n",
        "Next, you run the custom job to start the training job by invoking the method `run()`. The parameters are the same as when running a CustomTrainingJob."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_custom_container_training_job:r"
      },
      "outputs": [],
      "source": [
        "CMDARGS = [\"--model-dir=\" + BUCKET_URI]\n",
        "\n",
        "job.run(args=CMDARGS, replica_count=1, machine_type=TRAIN_COMPUTE, sync=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "delete_job"
      },
      "source": [
        "### Delete a custom training job\n",
        "\n",
        "After a training job is completed, you can delete the training job with the method `delete()`.  Prior to completion, a training job can be canceled with the method `cancel()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "delete_job"
      },
      "outputs": [],
      "source": [
        "job.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cleanup:mbsdk"
      },
      "source": [
        "# Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:\n",
        "\n",
        "- Model (Already deleted in previous cells)\n",
        "- Endpoint (Already deleted in previous cells)\n",
        "- Custom Job (Already deleted in previous cells)\n",
        "- Cloud Storage Bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup:mbsdk"
      },
      "outputs": [],
      "source": [
        "if os.getenv(\"IS_TESTING\"):\n",
        "    ! gsutil rm -r $BUCKET_URI"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "get_started_vertex_training_r.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

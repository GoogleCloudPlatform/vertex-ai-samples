{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title:generic,gcp"
   },
   "source": [
    "# E2E ML on GCP: MLOps stage 2 : experimentation: get started with Vertex AI Training for R\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage2/get_started_vertex_training_r.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage2/get_started_vertex_training_r.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:mlops"
   },
   "source": [
    "## Overview\n",
    "\n",
    "\n",
    "This tutorial demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. This tutorial covers stage 2 : experimentation: get started with Vertex AI Training for R. Please note that this notebook should be ran only in R notebook image (e.g., R4.1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:r,iris,lcn"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset used for this tutorial is the Iris dataset built into the R package. This dataset does not require any feature engineering. The trained model predicts the type of Iris flower species from a class of three species: setosa, virginica, or versicolor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:mlops,stage2,get_started_vertex_training_r"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you learn how to use `Vertex AI Training` for training a R custom model.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services:\n",
    "\n",
    "- `Vertex AI Training`\n",
    "- `Vertex AI Model` resource\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Locally train an R model in a notebook using %%R magic commands\n",
    "- Create a deployment image with trained R model and serving functions.\n",
    "- Test the deployment image locally.\n",
    "- Create a `Vertex AI Model` resource for the deployment image with embedded R model.\n",
    "- Deploy the deployment image with embedded R model to a `Vertex AI Endpoint` resource.\n",
    "- Test the deployment image with embedded R model.\n",
    "- Create a R-to-Python training package.\n",
    "- Create a training image for training the model.\n",
    "- Train a R model using `Vertex AI Trainingh` service with the R-to-Python training package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costs \n",
    "\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_mlops"
   },
   "source": [
    "## Installations\n",
    "\n",
    "Install *one time* the packages for executing the MLOps notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "id": "install_mlops",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform[tensorboard] in /opt/conda/envs/nnn/lib/python3.9/site-packages (1.11.0)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from google-cloud-aiplatform[tensorboard]) (2.7.1)\n",
      "Requirement already satisfied: proto-plus>=1.15.0 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from google-cloud-aiplatform[tensorboard]) (1.20.3)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from google-cloud-aiplatform[tensorboard]) (2.34.2)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from google-cloud-aiplatform[tensorboard]) (21.3)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from google-cloud-aiplatform[tensorboard]) (2.2.1)\n",
      "Requirement already satisfied: tensorflow<=2.7.0,>=2.3.0 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from google-cloud-aiplatform[tensorboard]) (2.7.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (2.27.1)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (2.6.2)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (3.20.0rc2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (1.56.0)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (1.44.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (1.44.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform[tensorboard]) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform[tensorboard]) (2.8.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform[tensorboard]) (2.3.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from packaging>=14.3->google-cloud-aiplatform[tensorboard]) (3.0.7)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (2.7.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (3.6.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (0.37.1)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (1.22.3)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (0.24.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (4.1.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (0.2.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (2.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (1.16.0)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (0.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (1.14.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (1.0.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (3.3.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (13.0.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (1.1.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (2.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (2.7.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (5.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (4.8)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform[tensorboard]) (1.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (2.0.12)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (1.8.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (60.10.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (2.0.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (3.3.6)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (4.11.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (3.7.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (3.2.0)\n",
      "Requirement already satisfied: rpy2 in /opt/conda/envs/nnn/lib/python3.9/site-packages (3.4.5)\n",
      "Requirement already satisfied: cffi>=1.10.0 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from rpy2) (1.15.0)\n",
      "Requirement already satisfied: tzlocal in /opt/conda/envs/nnn/lib/python3.9/site-packages (from rpy2) (4.1)\n",
      "Requirement already satisfied: pytz in /opt/conda/envs/nnn/lib/python3.9/site-packages (from rpy2) (2022.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from rpy2) (3.1.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/envs/nnn/lib/python3.9/site-packages (from cffi>=1.10.0->rpy2) (2.21)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/nnn/lib/python3.9/site-packages (from jinja2->rpy2) (2.1.1)\n",
      "Requirement already satisfied: pytz-deprecation-shim in /opt/conda/envs/nnn/lib/python3.9/site-packages (from tzlocal->rpy2) (0.1.0.post0)\n",
      "Requirement already satisfied: tzdata in /opt/conda/envs/nnn/lib/python3.9/site-packages (from pytz-deprecation-shim->tzlocal->rpy2) (2022.1)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install --upgrade google-cloud-aiplatform[tensorboard] $USER_FLAG\n",
    "! pip3 install --upgrade rpy2 $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "restart"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). {TODO: Update the APIs needed for your tutorial. Edit the API names, and update the link to append the API IDs, separating each one with a comma. For example, container.googleapis.com,cloudbuild.googleapis.com}\n",
    "\n",
    "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    "\n",
    "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_id"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "autoset_project_id"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: vertex-ai-dev\n"
     ]
    }
   ],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
    "\n",
    "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type:\"string\"}\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "timestamp"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "timestamp"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "When you initialize the Vertex AI SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "autoset_bucket"
   },
   "outputs": [],
   "source": [
    "if BUCKET_URI == \"\" or BUCKET_URI is None or BUCKET_URI == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_URI = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "create_bucket"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://vertex-ai-devaip-20220328065343/...\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validate_bucket"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "validate_bucket"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### Set up variables\n",
    "\n",
    "Next, set up some variables used throughout the tutorial.\n",
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "accelerators:training,cpu,prediction,cpu,mbsdk"
   },
   "source": [
    "#### Set hardware accelerators\n",
    "\n",
    "You can set hardware accelerators for training and prediction.\n",
    "\n",
    "Set the variables `TRAIN_GPU/TRAIN_NGPU` and `DEPLOY_GPU/DEPLOY_NGPU` to use a container image supporting a GPU and the number of GPUs allocated to the virtual machine (VM) instance. For example, to use a GPU container image with 4 Nvidia Telsa K80 GPUs allocated to each VM, you would specify:\n",
    "\n",
    "    (aip.AcceleratorType.NVIDIA_TESLA_K80, 4)\n",
    "\n",
    "\n",
    "Otherwise specify `(None, None)` to use a container image to run on a CPU.\n",
    "\n",
    "Learn more about [hardware accelerator support for your region](https://cloud.google.com/vertex-ai/docs/general/locations#accelerators).\n",
    "\n",
    "*Note*: TF releases before 2.3 for GPU support will fail to load the custom model in this tutorial. It is a known issue and fixed in TF 2.3. This is caused by static graph ops that are generated in the serving function. If you encounter this issue on your own custom models, use a container image for TF 2.3 with GPU support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "accelerators:training,cpu,prediction,cpu,mbsdk"
   },
   "outputs": [],
   "source": [
    "if os.getenv(\"IS_TESTING_TRAIN_GPU\"):\n",
    "    TRAIN_GPU, TRAIN_NGPU = (\n",
    "        aip.gapic.AcceleratorType.NVIDIA_TESLA_K80,\n",
    "        int(os.getenv(\"IS_TESTING_TRAIN_GPU\")),\n",
    "    )\n",
    "else:\n",
    "    TRAIN_GPU, TRAIN_NGPU = (None, None)\n",
    "\n",
    "if os.getenv(\"IS_TESTING_DEPLOY_GPU\"):\n",
    "    DEPLOY_GPU, DEPLOY_NGPU = (\n",
    "        aip.gapic.AcceleratorType.NVIDIA_TESLA_K80,\n",
    "        int(os.getenv(\"IS_TESTING_DEPLOY_GPU\")),\n",
    "    )\n",
    "else:\n",
    "    DEPLOY_GPU, DEPLOY_NGPU = (None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "machine:training,prediction"
   },
   "source": [
    "#### Set machine type\n",
    "\n",
    "Next, set the machine type to use for training and prediction.\n",
    "\n",
    "- Set the variables `TRAIN_COMPUTE` and `DEPLOY_COMPUTE` to configure  the compute resources for the VMs you will use for for training and prediction.\n",
    " - `machine type`\n",
    "     - `n1-standard`: 3.75GB of memory per vCPU.\n",
    "     - `n1-highmem`: 6.5GB of memory per vCPU\n",
    "     - `n1-highcpu`: 0.9 GB of memory per vCPU\n",
    " - `vCPUs`: number of \\[2, 4, 8, 16, 32, 64, 96 \\]\n",
    "\n",
    "*Note: The following is not supported for training:*\n",
    "\n",
    " - `standard`: 2 vCPUs\n",
    " - `highcpu`: 2, 4 and 8 vCPUs\n",
    "\n",
    "*Note: You may also use n2 and e2 machine types for training and deployment, but they do not support GPUs*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "machine:training,prediction"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train machine type n1-standard-4\n",
      "Deploy machine type n1-standard-4\n"
     ]
    }
   ],
   "source": [
    "if os.getenv(\"IS_TESTING_TRAIN_MACHINE\"):\n",
    "    MACHINE_TYPE = os.getenv(\"IS_TESTING_TRAIN_MACHINE\")\n",
    "else:\n",
    "    MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Train machine type\", TRAIN_COMPUTE)\n",
    "\n",
    "if os.getenv(\"IS_TESTING_DEPLOY_MACHINE\"):\n",
    "    MACHINE_TYPE = os.getenv(\"IS_TESTING_DEPLOY_MACHINE\")\n",
    "else:\n",
    "    MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_intro"
   },
   "source": [
    "## Introduction to R training\n",
    "\n",
    "### Training a R model\n",
    "\n",
    "You can either train your model locally, or use the `Vertex AI Training` service. In the later case, you would install the R-to-Python interpreter `rpy2` into your training instance (e.g., setup.py) and execute the R training script using the R-to-Python interpreter `rpy2`.\n",
    "\n",
    "\n",
    "### Deploying a R model\n",
    "\n",
    "Deploying a R model on `Vertex AI Prediction` service requires to use a custom container that serves online predictions. In this tutorial, you deploy a container running plumber R package to serve predictions from trained model artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_ipython"
   },
   "source": [
    "### Load the R notebook interpreter\n",
    "\n",
    "Loading the module `rpy2.ipython` will add support for %R and %RR magic cells in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "r_ipython"
   },
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_install:randomForest"
   },
   "source": [
    "### Install and import some additional R packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "id": "r_install:randomForest",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: Installing packages into ‘/home/jupyter/.R/library’\n",
      "(as ‘lib’ is unspecified)\n",
      "\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: trying URL 'http://cran.us.r-project.org/src/contrib/randomForest_4.7-1.tar.gz'\n",
      "\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: Content type 'application/x-gzip'\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]:  length 80852 bytes (78 KB)\n",
      "\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: \n",
      "\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: downloaded 78 KB\n",
      "\n",
      "\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: trying URL 'http://cran.us.r-project.org/src/contrib/plumber_1.1.0.tar.gz'\n",
      "\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: Content type 'application/x-gzip'\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]:  length 1050958 bytes (1.0 MB)\n",
      "\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: \n",
      "\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: downloaded 1.0 MB\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* installing *source* package ‘randomForest’ ...\n",
      "** package ‘randomForest’ successfully unpacked and MD5 sums checked\n",
      "** using staged installation\n",
      "** libs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -I\"/usr/share/R/include\" -DNDEBUG      -fpic  -g -O2 -fdebug-prefix-map=/home/jranke/git/r-backports/buster/r-base-4.1.3=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c classTree.c -o classTree.o\n",
      "gcc -I\"/usr/share/R/include\" -DNDEBUG      -fpic  -g -O2 -fdebug-prefix-map=/home/jranke/git/r-backports/buster/r-base-4.1.3=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c init.c -o init.o\n",
      "gcc -I\"/usr/share/R/include\" -DNDEBUG      -fpic  -g -O2 -fdebug-prefix-map=/home/jranke/git/r-backports/buster/r-base-4.1.3=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c regTree.c -o regTree.o\n",
      "gcc -I\"/usr/share/R/include\" -DNDEBUG      -fpic  -g -O2 -fdebug-prefix-map=/home/jranke/git/r-backports/buster/r-base-4.1.3=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c regrf.c -o regrf.o\n",
      "gcc -I\"/usr/share/R/include\" -DNDEBUG      -fpic  -g -O2 -fdebug-prefix-map=/home/jranke/git/r-backports/buster/r-base-4.1.3=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c rf.c -o rf.o\n",
      "gfortran -fno-optimize-sibling-calls  -fpic  -g -O2 -fdebug-prefix-map=/home/jranke/git/r-backports/buster/r-base-4.1.3=. -fstack-protector-strong  -c rfsub.f -o rfsub.o\n",
      "gcc -I\"/usr/share/R/include\" -DNDEBUG      -fpic  -g -O2 -fdebug-prefix-map=/home/jranke/git/r-backports/buster/r-base-4.1.3=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c rfutils.c -o rfutils.o\n",
      "gcc -shared -L/usr/lib/R/lib -Wl,-z,relro -o randomForest.so classTree.o init.o regTree.o regrf.o rf.o rfsub.o rfutils.o -lgfortran -lm -lquadmath -L/usr/lib/R/lib -lR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "installing to /home/jupyter/.R/library/00LOCK-randomForest/00new/randomForest/libs\n",
      "** R\n",
      "** data\n",
      "** inst\n",
      "** byte-compile and prepare package for lazy loading\n",
      "** help\n",
      "*** installing help indices\n",
      "** building package indices\n",
      "** testing if installed package can be loaded from temporary location\n",
      "** checking absolute paths in shared objects and dynamic libraries\n",
      "** testing if installed package can be loaded from final location\n",
      "** testing if installed package keeps a record of temporary installation path\n",
      "* DONE (randomForest)\n",
      "* installing *source* package ‘plumber’ ...\n",
      "** package ‘plumber’ successfully unpacked and MD5 sums checked\n",
      "** using staged installation\n",
      "** R\n",
      "** inst\n",
      "** byte-compile and prepare package for lazy loading\n",
      "** help\n",
      "*** installing help indices\n",
      "*** copying figures\n",
      "** building package indices\n",
      "** testing if installed package can be loaded from temporary location\n",
      "** testing if installed package can be loaded from final location\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: \n",
      "\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: \n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: The downloaded source packages are in\n",
      "\t‘/tmp/RtmpiR7zil/downloaded_packages’\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: \n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "** testing if installed package keeps a record of temporary installation path\n",
      "* DONE (plumber)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: randomForest 4.7-1\n",
      "\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: Type rfNews() to see new features/changes/bug fixes.\n",
      "\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: \n",
      "Attaching package: ‘randomForest’\n",
      "\n",
      "\n",
      "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: The following object is masked from ‘package:ggplot2’:\n",
      "\n",
      "    margin\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "install.packages(c(\"randomForest\", \"plumber\"), repos = \"http://cran.us.r-project.org\")\n",
    "\n",
    "library(ggplot2)\n",
    "library(randomForest)\n",
    "library(plumber)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quick_peek:r"
   },
   "source": [
    "#### Quick peek at your data\n",
    "\n",
    "This tutorial uses a version of the iris dataset that is built into the R package.\n",
    "\n",
    "Start by doing a quick peek at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "quick_peek:r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n",
      "1          5.1         3.5          1.4         0.2  setosa\n",
      "2          4.9         3.0          1.4         0.2  setosa\n",
      "3          4.7         3.2          1.3         0.2  setosa\n",
      "4          4.6         3.1          1.5         0.2  setosa\n",
      "5          5.0         3.6          1.4         0.2  setosa\n",
      "6          5.4         3.9          1.7         0.4  setosa\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "head(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "custom_dir"
   },
   "outputs": [],
   "source": [
    "# Make folder for R\n",
    "! rm -rf deploy custom\n",
    "! mkdir deploy custom custom/trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "locally_train:r,iris"
   },
   "source": [
    "### Locally train an R model\n",
    "\n",
    "First, you locally train the R model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "locally_train:r,iris"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# train model\n",
    "model = randomForest(Species ~ ., data = iris)\n",
    "# save model\n",
    "save(model, file = \"deploy/model.RData\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "serving_script_r"
   },
   "source": [
    "### Serving script\n",
    "\n",
    "You create a R script for serving predictions. This script does the following:\n",
    "\n",
    "- Extracts prediction request values from the HTTP body of the input request.\n",
    "- Construct a prediction request for the R model.\n",
    "- Submit the prediction request to the R model.\n",
    "- Returns the predicted results.\n",
    "\n",
    "*Note*: Plumber makes use of comment “annotations” above functions to define the web service. When you feed the file into Plumber, you’ll get a runnable web service that other systems can interact with over a network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "serving_script_r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing deploy/serving.R\n"
     ]
    }
   ],
   "source": [
    "%%writefile deploy/serving.R\n",
    "# serving.R\n",
    "\n",
    "library(\"randomForest\")\n",
    "\n",
    "#* Health check\n",
    "#* @get /ping\n",
    "#* @serializer unboxedJSON\n",
    "function() {\n",
    "    list(status = \"OK\")\n",
    "}\n",
    "\n",
    "#* @apiTitle flower classifier\n",
    "#* @param petal_length\n",
    "#* @param petal_width\n",
    "#* @param sepal_length\n",
    "#* @param sepal_width\n",
    "#* @post /classify\n",
    "function (req)\n",
    "{\n",
    "    instances <- as.data.frame(jsonlite::fromJSON(req$postBody))\n",
    "    results <- list()\n",
    "\n",
    "    load(\"./model.RData\")\n",
    "\n",
    "    for(i in 1:nrow(instances)) {       # for-loop over columns\n",
    "        petal_length <- instances[i, \"instances.petal_length\"]\n",
    "        petal_width <- instances[i, \"instances.petal_width\"]\n",
    "        sepal_length <- instances[i, \"instances.sepal_length\"]\n",
    "        sepal_width <- instances[i, \"instances.sepal_width\"]\n",
    "        test = c(sepal_length, sepal_width, petal_length, petal_width)\n",
    "        test = sapply(test, as.numeric)\n",
    "        test = data.frame(matrix(test, ncol = 4))\n",
    "        colnames(test) = colnames(iris[, 1:4])\n",
    "        results <- append(results, predict(model, test))\n",
    "    }\n",
    "\n",
    "    list(predictions = results)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "start_server_r"
   },
   "source": [
    "#### Script for running the R server\n",
    "\n",
    "Next, you create a file that runs the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "start_server_r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing deploy/startServer.R\n"
     ]
    }
   ],
   "source": [
    "%%writefile deploy/startServer.R\n",
    "library(plumber)\n",
    "pr <- plumb(\"serving.R\")\n",
    "pr$run(host = \"0.0.0.0\", port = 7080)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "docker_write,prediction,r"
   },
   "source": [
    "### Make R container for prediction\n",
    "\n",
    "Currently, Vertex AI does not have a prefined container for making predictions with a deployed R model. No problem, you can assemble your own custom container. For this tutorial, you construct a deployment container from a Docker image as follows:\n",
    "\n",
    "    - Set the base image supplied by RStudio (rstudio/plumber).\n",
    "    - Package the model artifacts and serving scripts into a Docker image.\n",
    "    - Start the serving script on port 7080."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "docker_write,prediction,r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing deploy/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile deploy/Dockerfile\n",
    "\n",
    "FROM rstudio/plumber\n",
    "\n",
    "# install random forest\n",
    "RUN R -e 'install.packages(c(\"randomForest\"), repos = \"https://cran.rstudio.com/\")'\n",
    "\n",
    "# Copy model and script\n",
    "RUN mkdir /app\n",
    "COPY model.RData /app\n",
    "COPY serving.R /app\n",
    "COPY startServer.R /app\n",
    "WORKDIR /app\n",
    "\n",
    "# plumber & run server\n",
    "EXPOSE 7080\n",
    "\n",
    "ENTRYPOINT [\"R\", \"-f\", \"/app/startServer.R\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "id": "docker_push,prediction,r",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcr.io/vertex-ai-dev/r-predict-iris\n",
      "Sending build context to Docker daemon   38.4kB\n",
      "Step 1/9 : FROM rstudio/plumber\n",
      " ---> 8856194b5963\n",
      "Step 2/9 : RUN R -e 'install.packages(c(\"randomForest\"), repos = \"https://cran.rstudio.com/\")'\n",
      " ---> Using cache\n",
      " ---> 0e2d8893b1a2\n",
      "Step 3/9 : RUN mkdir /app\n",
      " ---> Using cache\n",
      " ---> bea8a622ee61\n",
      "Step 4/9 : COPY model.RData /app\n",
      " ---> 56eea5973a7b\n",
      "Step 5/9 : COPY serving.R /app\n",
      " ---> 847d6db9402e\n",
      "Step 6/9 : COPY startServer.R /app\n",
      " ---> 4c0b2e9dd69f\n",
      "Step 7/9 : WORKDIR /app\n",
      " ---> Running in 802421e7c397\n",
      "Removing intermediate container 802421e7c397\n",
      " ---> aa14a03e6fac\n",
      "Step 8/9 : EXPOSE 7080\n",
      " ---> Running in 0cadd76256e8\n",
      "Removing intermediate container 0cadd76256e8\n",
      " ---> a315cfb6b9b2\n",
      "Step 9/9 : ENTRYPOINT [\"R\", \"-f\", \"/app/startServer.R\"]\n",
      " ---> Running in 096284ce10e6\n",
      "Removing intermediate container 096284ce10e6\n",
      " ---> e6bbda68ccb8\n",
      "Successfully built e6bbda68ccb8\n",
      "Successfully tagged gcr.io/vertex-ai-dev/r-predict-iris:latest\n",
      "Using default tag: latest\n",
      "The push refers to repository [gcr.io/vertex-ai-dev/r-predict-iris]\n",
      "\n",
      "\u001b[1B723b64af: Preparing \n",
      "\u001b[1Bc61d3572: Preparing \n",
      "\u001b[1B42067cb9: Preparing \n",
      "\u001b[1Bba22c179: Preparing \n",
      "\u001b[1B587e6be4: Preparing \n",
      "\u001b[1B9fd4bd74: Preparing \n",
      "\u001b[1Beceb2333: Preparing \n",
      "\u001b[1B3ba7fe6a: Preparing \n",
      "\u001b[1B280ac02d: Preparing \n",
      "\u001b[1B6e81c28d: Preparing \n",
      "\u001b[1B6bcdfc29: Preparing \n",
      "\u001b[1B8312f000: Preparing \n",
      "\u001b[1Bb559763c: Preparing \n",
      "\u001b[12B2067cb9: Pushed lready exists 5kB\u001b[14A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[14A\u001b[2K\u001b[13A\u001b[2K\u001b[12A\u001b[2Klatest: digest: sha256:45eedc46a8fcb8eb71f89e24e1927312fe9045d66244c7ea8fe2af657236b318 size: 3247\n"
     ]
    }
   ],
   "source": [
    "DEPLOY_IMAGE = f\"gcr.io/{PROJECT_ID}/r-predict-iris\"\n",
    "print(DEPLOY_IMAGE)\n",
    "\n",
    "! docker build --tag=$DEPLOY_IMAGE ./deploy\n",
    "\n",
    "! docker push $DEPLOY_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test_docker_locally:launch,r"
   },
   "source": [
    "### Locally test the Docker image\n",
    "\n",
    "Next, you locally test the Docker image you created for serving predictions.\n",
    "\n",
    "#### Launch the serving binary\n",
    "\n",
    "First, you launch the serving binary, listening on port 7080, and then ping it as a health check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "test_docker_locally:launch,r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c0797f38d01b721ba180a95ae523dc44779abae59bf947a2a36e8aaaf5b38b27\n",
      "{\"status\":\"OK\"}"
     ]
    }
   ],
   "source": [
    "! docker stop local_iris 2>/dev/null\n",
    "! docker run -t -d --rm -p 7080:7080 --name=local_iris $DEPLOY_IMAGE\n",
    "! sleep 10\n",
    "! curl http://localhost:7080/ping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test_docker_locally:predict,r"
   },
   "source": [
    "#### Send predicton request\n",
    "\n",
    "Next, you send a prediction request to the serving binary you locally launched. Afterwards, you will shutdown the launched serving binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "test_docker_locally:predict,r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"predictions\":[\"setosa\",\"setosa\"]}"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cat > ./deploy/instances.json <<END\n",
    "{\n",
    "  \"instances\": [{\n",
    "      \"sepal_width\": 1,\n",
    "      \"sepal_length\": 2,\n",
    "      \"petal_width\": 3,\n",
    "      \"petal_length\": 1\n",
    "    },\n",
    "    {\n",
    "      \"sepal_width\": 4,\n",
    "      \"sepal_length\": 2,\n",
    "      \"petal_width\": 1,\n",
    "      \"petal_length\": 1\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "END\n",
    "\n",
    "curl -s -X POST \\\n",
    "  -H \"Content-Type: application/json; charset=utf-8\" \\\n",
    "  -d @./deploy/instances.json \\\n",
    "  http://localhost:7080/classify\n",
    "\n",
    "docker stop local_iris 1>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_model,r"
   },
   "source": [
    "### Upload a R model to a `Vertex AI Model` resource\n",
    "\n",
    "Next you upload the R model to a Vertex AI Model resource, with the following parameters:\n",
    "\n",
    "- `display_name`: A human readable name for the model resource.\n",
    "- `serving_container_image_uri`: The deployment image that contains the serving binary and R model.\n",
    "- `serving_container_predict_route`: The URI endpoint for prediction requests.\n",
    "- `serving_container_health_route`: The URI endpoint for health ping.\n",
    "- `serving_container_ports`: A list of ports to listen on for prediction/health requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "upload_model,r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Model\n",
      "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/931647533046/locations/us-central1/models/8967955485722411008/operations/5283252945985470464\n",
      "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/931647533046/locations/us-central1/models/8967955485722411008\n",
      "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n",
      "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/931647533046/locations/us-central1/models/8967955485722411008')\n"
     ]
    }
   ],
   "source": [
    "DISPLAY_NAME = \"iris_\" + TIMESTAMP\n",
    "health_route = \"/ping\"\n",
    "predict_route = \"/classify\"\n",
    "serving_container_ports = [7080]\n",
    "\n",
    "model = aip.Model.upload(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    serving_container_image_uri=DEPLOY_IMAGE,\n",
    "    serving_container_predict_route=predict_route,\n",
    "    serving_container_health_route=health_route,\n",
    "    serving_container_ports=serving_container_ports,\n",
    ")\n",
    "\n",
    "model.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deploy_model:mbsdk,all"
   },
   "source": [
    "## Deploy the model\n",
    "\n",
    "Next, deploy your model for online prediction. To deploy the model, you invoke the `deploy` method, with the following parameters:\n",
    "\n",
    "- `deployed_model_display_name`: A human readable name for the deployed model.\n",
    "- `traffic_split`: Percent of traffic at the endpoint that goes to this model, which is specified as a dictionary of one or more key/value pairs.\n",
    "If only one model, then specify as { \"0\": 100 }, where \"0\" refers to this model being uploaded and 100 means 100% of the traffic.\n",
    "If there are existing models on the endpoint, for which the traffic will be split, then use model_id to specify as { \"0\": percent, model_id: percent, ... }, where model_id is the model id of an existing model to the deployed endpoint. The percents must add up to 100.\n",
    "- `machine_type`: The type of machine to use for training.\n",
    "- `accelerator_type`: The hardware accelerator type.\n",
    "- `accelerator_count`: The number of accelerators to attach to a worker replica.\n",
    "- `starting_replica_count`: The number of compute instances to initially provision.\n",
    "- `max_replica_count`: The maximum number of compute instances to scale to. In this tutorial, only one instance is provisioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "deploy_model:mbsdk,all"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
      "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/931647533046/locations/us-central1/endpoints/3748024032855851008/operations/7172512989667393536\n",
      "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/931647533046/locations/us-central1/endpoints/3748024032855851008\n",
      "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
      "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/931647533046/locations/us-central1/endpoints/3748024032855851008')\n",
      "INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/931647533046/locations/us-central1/endpoints/3748024032855851008\n",
      "INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/931647533046/locations/us-central1/endpoints/3748024032855851008/operations/1774948836263854080\n",
      "INFO:google.cloud.aiplatform.models:Endpoint model deployed. Resource name: projects/931647533046/locations/us-central1/endpoints/3748024032855851008\n"
     ]
    }
   ],
   "source": [
    "DEPLOYED_NAME = \"iris-\" + TIMESTAMP\n",
    "\n",
    "TRAFFIC_SPLIT = {\"0\": 100}\n",
    "\n",
    "MIN_NODES = 1\n",
    "MAX_NODES = 1\n",
    "\n",
    "if DEPLOY_GPU:\n",
    "    endpoint = model.deploy(\n",
    "        deployed_model_display_name=DEPLOYED_NAME,\n",
    "        traffic_split=TRAFFIC_SPLIT,\n",
    "        machine_type=DEPLOY_COMPUTE,\n",
    "        accelerator_type=DEPLOY_GPU.name,\n",
    "        accelerator_count=DEPLOY_NGPU,\n",
    "        min_replica_count=MIN_NODES,\n",
    "        max_replica_count=MAX_NODES,\n",
    "    )\n",
    "else:\n",
    "    endpoint = model.deploy(\n",
    "        deployed_model_display_name=DEPLOYED_NAME,\n",
    "        traffic_split=TRAFFIC_SPLIT,\n",
    "        machine_type=DEPLOY_COMPUTE,\n",
    "        min_replica_count=MIN_NODES,\n",
    "        max_replica_count=MAX_NODES,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test_deployed_model:iris"
   },
   "source": [
    "### Make test prediction\n",
    "\n",
    "Next, you test the deployed model by sending synthentic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "test_deployed_model:iris"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/nnn/lib/python3.9/site-packages/google/api_core/grpc_helpers.py\", line 66, in error_remapped_callable\n",
      "    return callable_(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/nnn/lib/python3.9/site-packages/grpc/_channel.py\", line 946, in __call__\n",
      "    return _end_unary_response_blocking(state, call, False, None)\n",
      "  File \"/opt/conda/envs/nnn/lib/python3.9/site-packages/grpc/_channel.py\", line 849, in _end_unary_response_blocking\n",
      "    raise _InactiveRpcError(state)\n",
      "grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n",
      "\tstatus = StatusCode.INTERNAL\n",
      "\tdetails = \"Received RST_STREAM with error code 2\"\n",
      "\tdebug_error_string = \"{\"created\":\"@1648451320.112401100\",\"description\":\"Error received from peer ipv4:74.125.195.95:443\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":903,\"grpc_message\":\"Received RST_STREAM with error code 2\",\"grpc_status\":13}\"\n",
      ">\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_10876/2026989027.py\", line 7, in <cell line: 1>\n",
      "    prediction = endpoint.predict(instances=INSTANCES)\n",
      "  File \"/opt/conda/envs/nnn/lib/python3.9/site-packages/google/cloud/aiplatform/models.py\", line 1162, in predict\n",
      "    prediction_response = self._prediction_client.predict(\n",
      "  File \"/opt/conda/envs/nnn/lib/python3.9/site-packages/google/cloud/aiplatform_v1/services/prediction_service/client.py\", line 547, in predict\n",
      "    response = rpc(request, retry=retry, timeout=timeout, metadata=metadata,)\n",
      "  File \"/opt/conda/envs/nnn/lib/python3.9/site-packages/google/api_core/gapic_v1/method.py\", line 154, in __call__\n",
      "    return wrapped_func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/nnn/lib/python3.9/site-packages/google/api_core/grpc_helpers.py\", line 68, in error_remapped_callable\n",
      "    raise exceptions.from_grpc_error(exc) from exc\n",
      "google.api_core.exceptions.InternalServerError: 500 Received RST_STREAM with error code 2\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    INSTANCES = [\n",
    "        {\"sepal_width\": 1, \"sepal_length\": 2, \"petal_width\": 3, \"petal_length\": 1},\n",
    "        {\"sepal_width\": 4, \"sepal_length\": 2, \"petal_width\": 1, \"petal_length\": 1},\n",
    "    ]\n",
    "\n",
    "    prediction = endpoint.predict(instances=INSTANCES)\n",
    "\n",
    "    print(prediction)\n",
    "except:\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "undeploy_model:mbsdk"
   },
   "source": [
    "## Undeploy the model\n",
    "\n",
    "When you are done doing predictions, you undeploy the model from the `Endpoint` resouce. This deprovisions all compute resources and ends billing for the deployed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "undeploy_model:mbsdk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Undeploying Endpoint model: projects/931647533046/locations/us-central1/endpoints/3748024032855851008\n",
      "INFO:google.cloud.aiplatform.models:Undeploy Endpoint model backing LRO: projects/931647533046/locations/us-central1/endpoints/3748024032855851008/operations/2567582370681061376\n",
      "INFO:google.cloud.aiplatform.models:Endpoint model undeployed. Resource name: projects/931647533046/locations/us-central1/endpoints/3748024032855851008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.models.Endpoint object at 0x7fe63c0d1490> \n",
       "resource name: projects/931647533046/locations/us-central1/endpoints/3748024032855851008"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.undeploy_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "endpoint_delete:mbsdk"
   },
   "source": [
    "#### Delete the endpoint\n",
    "\n",
    "The method 'delete()' will delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "endpoint_delete:mbsdk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.base:Deleting Endpoint : projects/931647533046/locations/us-central1/endpoints/3748024032855851008\n",
      "INFO:google.cloud.aiplatform.base:Delete Endpoint  backing LRO: projects/931647533046/locations/us-central1/operations/6877527214074626048\n",
      "INFO:google.cloud.aiplatform.base:Endpoint deleted. . Resource name: projects/931647533046/locations/us-central1/endpoints/3748024032855851008\n"
     ]
    }
   ],
   "source": [
    "endpoint.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_delete:mbsdk"
   },
   "source": [
    "#### Delete the model\n",
    "\n",
    "The method 'delete()' will delete the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "model_delete:mbsdk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.base:Deleting Model : projects/931647533046/locations/us-central1/models/8967955485722411008\n",
      "INFO:google.cloud.aiplatform.base:Delete Model  backing LRO: projects/931647533046/locations/us-central1/operations/2977409936771776512\n",
      "INFO:google.cloud.aiplatform.base:Model deleted. . Resource name: projects/931647533046/locations/us-central1/models/8967955485722411008\n"
     ]
    }
   ],
   "source": [
    "model.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taskpy_contents:iris,r"
   },
   "source": [
    "### Create the task script for the Python/R training package\n",
    "\n",
    "Next, you create the `task.py` script for driving the training package. Some noteable steps include:\n",
    "\n",
    "\n",
    "- Command-line arguments:\n",
    "    - `model-dir`: The location to save the trained model. When using Vertex AI custom training, the location will be specified in the environment variable: `AIP_MODEL_DIR`.\n",
    "\n",
    "\n",
    "- Training:\n",
    "    - Uses R-to-Python interpreter to run the R training script.\n",
    "\n",
    "\n",
    "- Model artifact saving:\n",
    "    - Saves the model artifacts at the Cloud Storage location specified by `model-dir`.\n",
    "    - *Note*: GCSFuse (`/gcs`) is used to do filesystem operations on Cloud Storage buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "taskpy_contents:iris,r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom/trainer/task.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import rpy2\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "# import rpy2's package module\n",
    "import rpy2.robjects.packages as rpackages\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model-dir', dest='model_dir',\n",
    "                    default=os.getenv('AIP_MODEL_DIR'), type=str, help='Model dir.')\n",
    "args = parser.parse_args()\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "# import R's utility package\n",
    "randomForest = rpackages.importr('randomForest')\n",
    "\n",
    "# train the model\n",
    "logging.info(\"Model training started ...\")\n",
    "rpy2.robjects.r('''\n",
    "    # train model\n",
    "    model = randomForest(Species ~ ., data = iris)\n",
    "    # save model\n",
    "    save(model, file = \"model.RData\")\n",
    "'''\n",
    ")\n",
    "logging.info(\"Model training completed ...\")\n",
    "\n",
    "# GCSFuse conversion\n",
    "gs_prefix = 'gs://'\n",
    "gcsfuse_prefix = '/gcs/'\n",
    "if args.model_dir.startswith(gs_prefix):\n",
    "    args.model_dir = args.model_dir.replace(gs_prefix, gcsfuse_prefix)\n",
    "    dirpath = os.path.split(args.model_dir)[0]\n",
    "    if not os.path.isdir(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "\n",
    "# Upload the saved model file to Cloud Storage\n",
    "gcs_model_path = os.path.join(args.model_dir, 'model.RData')\n",
    "logging.info(\"Saving model artifacts to {}\". format(gcs_model_path))\n",
    "with open(\"model.RData\", \"rb\") as f:\n",
    "    data = f.read()\n",
    "with open(gcs_model_path, \"wb\") as f:\n",
    "    f.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "docker_write,training,r"
   },
   "source": [
    "### Make R container for training\n",
    "\n",
    "Currently, Vertex AI does not have a prefined container for training an R model. No problem, you can assemble your own custom container. For this tutorial, you construct a deployment container from a Docker image as follows:\n",
    "\n",
    "    - Set the base image to a TensorFlow Deep Learning image\n",
    "    - Install R-to-Python package and other R tools\n",
    "    - Install cloud storage package\n",
    "    - Package the model artifacts and serving scripts into a Docker image.\n",
    "    - Set the entry point in the container to run the training package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "docker_write,training,r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/tf2-cpu.2-3\n",
    "\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends build-essential r-base r-cran-randomforest python3.6 python3-pip python3-setuptools python3-dev\n",
    "RUN pip install google-cloud-storage\n",
    "RUN pip install rpy2\n",
    "\n",
    "# install random forest\n",
    "RUN R -e 'install.packages(c(\"randomForest\"), repos = \"https://cran.rstudio.com/\")'\n",
    "\n",
    "WORKDIR /\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY trainer /trainer\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "id": "docker_push,training,r",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcr.io/vertex-ai-dev/r-train-iris\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0328 07:09:00.777252891   10876 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon   5.12kB\n",
      "Step 1/8 : FROM gcr.io/deeplearning-platform-release/tf2-cpu.2-3\n",
      " ---> 1fdfb6e767fe\n",
      "Step 2/8 : RUN apt-get update && apt-get install -y --no-install-recommends build-essential r-base r-cran-randomforest python3.6 python3-pip python3-setuptools python3-dev\n",
      " ---> Using cache\n",
      " ---> 7847be457bb3\n",
      "Step 3/8 : RUN pip install google-cloud-storage\n",
      " ---> Using cache\n",
      " ---> b56971fb492f\n",
      "Step 4/8 : RUN pip install rpy2\n",
      " ---> Using cache\n",
      " ---> 081c4bc3e340\n",
      "Step 5/8 : RUN R -e 'install.packages(c(\"randomForest\"), repos = \"https://cran.rstudio.com/\")'\n",
      " ---> Using cache\n",
      " ---> 95aade764ec4\n",
      "Step 6/8 : WORKDIR /\n",
      " ---> Using cache\n",
      " ---> e665efacfc42\n",
      "Step 7/8 : COPY trainer /trainer\n",
      " ---> Using cache\n",
      " ---> f6423c298c7d\n",
      "Step 8/8 : ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]\n",
      " ---> Using cache\n",
      " ---> 463f2d00f67d\n",
      "Successfully built 463f2d00f67d\n",
      "Successfully tagged gcr.io/vertex-ai-dev/r-train-iris:latest\n",
      "Using default tag: latest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0328 07:09:05.925141476   10876 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [gcr.io/vertex-ai-dev/r-train-iris]\n",
      "\n",
      "\u001b[1Ba4cf1322: Preparing \n",
      "\u001b[1B7561b1f8: Preparing \n",
      "\u001b[1Bf2372bb6: Preparing \n",
      "\u001b[1Be62e08f4: Preparing \n",
      "\u001b[1Bcb84710f: Preparing \n",
      "\u001b[1Bdf428d24: Preparing \n",
      "\u001b[1B7aa87923: Preparing \n",
      "\u001b[1Baf295f89: Preparing \n",
      "\u001b[1B91940b32: Preparing \n",
      "\u001b[1B95a574c8: Preparing \n",
      "\u001b[1B10151b48: Preparing \n",
      "\u001b[1Bc089358e: Preparing \n",
      "\u001b[1B9b36546a: Preparing \n",
      "\u001b[1B82ce8d0b: Preparing \n",
      "\u001b[1B467ac3a5: Preparing \n",
      "\u001b[1B91c31559: Preparing \n",
      "\u001b[1Bae11254c: Preparing \n",
      "\u001b[1B2bcbe281: Preparing \n",
      "\u001b[1B4c112e39: Preparing \n",
      "\u001b[1B048fd290: Preparing \n",
      "\u001b[1Bbf18a086: Preparing \n",
      "\u001b[1B7a45d8d8: Preparing \n",
      "\u001b[1B6651fb01: Preparing \n",
      "\u001b[1Bd5cafaa0: Preparing \n",
      "\u001b[1B5fa9d77e: Layer already exists \u001b[21A\u001b[2K\u001b[20A\u001b[2K\u001b[17A\u001b[2K\u001b[14A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2Klatest: digest: sha256:516a2339b2b597283c61b09365ff89e797d3c917c775b33d875b377616a1d653 size: 5550\n"
     ]
    }
   ],
   "source": [
    "TRAIN_IMAGE = f\"gcr.io/{PROJECT_ID}/r-train-iris\"\n",
    "print(TRAIN_IMAGE)\n",
    "\n",
    "! docker build --tag=$TRAIN_IMAGE ./custom\n",
    "\n",
    "! docker push $TRAIN_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_custom_container_training_job:mbsdk,no_model"
   },
   "source": [
    "### Create and run custom training job\n",
    "\n",
    "\n",
    "To train a custom model, you perform two steps: 1) create a custom training job, and 2) run the job.\n",
    "\n",
    "#### Create custom training job\n",
    "\n",
    "A custom training job is created with the `CustomTrainingJob` class, with the following parameters:\n",
    "\n",
    "- `display_name`: The human readable name for the custom training job.\n",
    "- `container_uri`: The training container image.\n",
    "\n",
    "- `command`: The command (e.g., interpreter) and script to invokee within the container.\n",
    "\n",
    "*Note:* The interpreter and script to invoke is overridable within the container (i.e., ENTRYPOINT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "create_custom_container_training_job:mbsdk,no_model"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.aiplatform.training_jobs.CustomContainerTrainingJob object at 0x7fe64ca06bb0>\n"
     ]
    }
   ],
   "source": [
    "job = aip.CustomContainerTrainingJob(\n",
    "    display_name=\"iris_\" + TIMESTAMP,\n",
    "    container_uri=TRAIN_IMAGE,\n",
    "    command=[\"python3\", \"trainer/task.py\"],\n",
    ")\n",
    "\n",
    "print(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_custom_container_training_job:r"
   },
   "source": [
    "#### Run the custom container training job\n",
    "\n",
    "Next, you run the custom job to start the training job by invoking the method `run()`. The parameters are the same as when running a CustomTrainingJob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "run_custom_container_training_job:r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.training_jobs:Training Output directory:\n",
      "gs://vertex-ai-devaip-20220328065343/aiplatform-custom-training-2022-03-28-07:09:17.638 \n",
      "INFO:google.cloud.aiplatform.training_jobs:View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/5457523271245234176?project=931647533046\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/5457523271245234176 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/9050467786073047040?project=931647533046\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/5457523271245234176 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/5457523271245234176 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/5457523271245234176 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob run completed. Resource name: projects/931647533046/locations/us-central1/trainingPipelines/5457523271245234176\n",
      "WARNING:google.cloud.aiplatform.training_jobs:Training did not produce a Managed Model returning None. Training Pipeline projects/931647533046/locations/us-central1/trainingPipelines/5457523271245234176 is not configured to upload a Model. Create the Training Pipeline with model_serving_container_image_uri and model_display_name passed in. Ensure that your training script saves to model to os.environ['AIP_MODEL_DIR'].\n"
     ]
    }
   ],
   "source": [
    "CMDARGS = [\"--model-dir=\" + BUCKET_URI]\n",
    "\n",
    "job.run(args=CMDARGS, replica_count=1, machine_type=TRAIN_COMPUTE, sync=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "delete_job"
   },
   "source": [
    "### Delete a custom training job\n",
    "\n",
    "After a training job is completed, you can delete the training job with the method `delete()`.  Prior to completion, a training job can be canceled with the method `cancel()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "delete_job"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.base:Deleting CustomContainerTrainingJob : projects/931647533046/locations/us-central1/trainingPipelines/5457523271245234176\n",
      "INFO:google.cloud.aiplatform.base:Delete CustomContainerTrainingJob  backing LRO: projects/931647533046/locations/us-central1/operations/5751627307232002048\n",
      "INFO:google.cloud.aiplatform.base:CustomContainerTrainingJob deleted. . Resource name: projects/931647533046/locations/us-central1/trainingPipelines/5457523271245234176\n"
     ]
    }
   ],
   "source": [
    "job.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "source": [
    "# Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n",
    "- Model (Already deleted in previous cells)\n",
    "- Endpoint (Already deleted in previous cells)\n",
    "- Custom Job (Already deleted in previous cells)\n",
    "- Cloud Storage Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0328 07:12:37.352891561   10876 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing gs://vertex-ai-devaip-20220328065343/model.RData#1648451485820377...\n",
      "/ [1 objects]                                                                   \n",
      "Operation completed over 1 objects.                                              \n",
      "Removing gs://vertex-ai-devaip-20220328065343/...\n"
     ]
    }
   ],
   "source": [
    "if os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "get_started_vertex_training_r.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m89",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m89"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

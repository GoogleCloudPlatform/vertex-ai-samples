{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "copyright"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title:generic,gcp"
   },
   "source": [
    "# E2E ML on GCP: MLOps stage 2 : experimentation: get started with Vertex Distributed Training\n",
    "\n",
    "<table align=\"left\">\n",
    "      <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/ml_ops/stage2/get_started_vertex_distributed_training.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage2/get_started_vertex_distributed_training.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage2/get_started_vertex_distributed_training.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:mlops"
   },
   "source": [
    "## Overview\n",
    "\n",
    "\n",
    "This tutorial demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. This tutorial covers stage 2 : experimentation: get started with Vertex Distributed Training. Please note: There are incompatibilities between Colab and Docker and the Docker section may not work until resolved by the platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:custom,boston,lrg"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset used for this tutorial is the [Boston Housing Prices dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html). The version of the dataset you will use in this tutorial is built into TensorFlow. The trained model predicts the median price of a house in units of 1K USD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:mlops,stage2,get_started_vertex_distributed_training"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you learn how to use `Vertex AI Distributed Training` for when training with `Vertex AI`.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services:\n",
    "\n",
    "- `Vertex AI Distributed Training`\n",
    "- `Vertex AI Reduction Server`\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- `MirroredStrategy`: Train on a single VM with multiple GPUs.\n",
    "- `MultiWorkerMirroredStrategy`: Train on multiple VMs with automatic setup of replicas.\n",
    "- `MultiWorkerMirroredStrategy`: Train on multiple VMs with fine grain control of replicas.\n",
    "- `ReductionServer`: Train on multiple VMS and sync updates across VMS with `Vertex AI Reduction Server`.\n",
    "- `TPUTraining`: Train with multiple Cloud TPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "recommendation:mlops,stage2,vertex,distributed_training"
   },
   "source": [
    "### Costs\n",
    " \n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "Vertex AI\n",
    "Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/),\n",
    "        to generate a cost estimate based on your projected usage.\n",
    "### Recommendations\n",
    "\n",
    "When doing E2E MLOps on Google Cloud, the following are best practices for when to use Vertex AI Distributed Training:\n",
    "\n",
    "**Single VM / Single Device (OneDeviceStrategy)**\n",
    "\n",
    "You are experimenting and the total training data and number of model parameters is small.\n",
    "\n",
    "If the number of model parameters is very small, you may not get much benefit from a GPU and may consider using the VM's CPU.\n",
    "\n",
    "**Single VM / Multiple Compute Devices (MirroredStrategy)**\n",
    "\n",
    "The number of model parameters is very large, but the total training data is small.\n",
    "\n",
    "**Multiple VM / Multiple Compute Devices (MultiWorkerMirroredStrategy)**\n",
    "\n",
    "The number of model parameters is very large and the total training data is very large.\n",
    "\n",
    "**ReductionServer**\n",
    "\n",
    "While training across a large number of VMs and the model parameters updates to sync is very large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkYpRvOQyVYb"
   },
   "source": [
    "### Install additional packages\n",
    "\n",
    "Install the latest version of Vertex SDK for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xs_Kt8RcyXTC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TjOXHg2VyajN",
    "outputId": "14f59e85-b4e2-476b-efe4-534056b984a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-aiplatform\n",
      "  Downloading google_cloud_aiplatform-1.12.1-py2.py3-none-any.whl (1.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8 MB 12.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.7/dist-packages (from google-cloud-aiplatform) (1.31.5)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
      "  Downloading google_cloud_resource_manager-1.4.1-py2.py3-none-any.whl (402 kB)\n",
      "\u001b[K     |████████████████████████████████| 402 kB 36.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-aiplatform) (21.3)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-aiplatform) (1.21.0)\n",
      "Collecting proto-plus>=1.15.0\n",
      "  Downloading proto_plus-1.20.3-py3-none-any.whl (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 3.0 MB/s \n",
      "\u001b[?25hCollecting google-cloud-storage<3.0.0dev,>=1.32.0\n",
      "  Downloading google_cloud_storage-2.3.0-py2.py3-none-any.whl (107 kB)\n",
      "\u001b[K     |████████████████████████████████| 107 kB 46.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: google-auth<2.0dev,>=1.25.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (1.35.0)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (1.15.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (57.4.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (1.56.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (2.23.0)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (2022.1)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (3.17.3)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (1.44.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (0.2.8)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.0.3)\n",
      "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (0.4.1)\n",
      "Collecting grpc-google-iam-v1<0.13dev,>=0.12.3\n",
      "  Downloading grpc_google_iam_v1-0.12.4-py2.py3-none-any.whl (26 kB)\n",
      "Collecting google-cloud-storage<3.0.0dev,>=1.32.0\n",
      "  Downloading google_cloud_storage-2.2.1-py2.py3-none-any.whl (107 kB)\n",
      "\u001b[K     |████████████████████████████████| 107 kB 10.3 MB/s \n",
      "\u001b[?25hCollecting google-cloud-core<2.0dev,>=1.0.3\n",
      "  Downloading google_cloud_core-1.7.2-py2.py3-none-any.whl (28 kB)\n",
      "Collecting google-cloud-storage<3.0.0dev,>=1.32.0\n",
      "  Downloading google_cloud_storage-2.2.0-py2.py3-none-any.whl (107 kB)\n",
      "\u001b[K     |████████████████████████████████| 107 kB 47.0 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-2.1.0-py2.py3-none-any.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 49.4 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-2.0.0-py2.py3-none-any.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 35.3 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.44.0-py2.py3-none-any.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 40.4 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.43.0-py2.py3-none-any.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 40.1 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.42.3-py2.py3-none-any.whl (105 kB)\n",
      "\u001b[K     |████████████████████████████████| 105 kB 42.6 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.42.2-py2.py3-none-any.whl (105 kB)\n",
      "\u001b[K     |████████████████████████████████| 105 kB 44.8 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.42.1-py2.py3-none-any.whl (105 kB)\n",
      "\u001b[K     |████████████████████████████████| 105 kB 46.2 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.42.0-py2.py3-none-any.whl (105 kB)\n",
      "\u001b[K     |████████████████████████████████| 105 kB 10.1 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.41.1-py2.py3-none-any.whl (105 kB)\n",
      "\u001b[K     |████████████████████████████████| 105 kB 10.8 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.41.0-py2.py3-none-any.whl (104 kB)\n",
      "\u001b[K     |████████████████████████████████| 104 kB 47.1 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.40.0-py2.py3-none-any.whl (104 kB)\n",
      "\u001b[K     |████████████████████████████████| 104 kB 43.6 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.39.0-py2.py3-none-any.whl (103 kB)\n",
      "\u001b[K     |████████████████████████████████| 103 kB 31.5 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.38.0-py2.py3-none-any.whl (103 kB)\n",
      "\u001b[K     |████████████████████████████████| 103 kB 45.9 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.37.1-py2.py3-none-any.whl (103 kB)\n",
      "\u001b[K     |████████████████████████████████| 103 kB 43.7 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.37.0-py2.py3-none-any.whl (103 kB)\n",
      "\u001b[K     |████████████████████████████████| 103 kB 38.1 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.36.2-py2.py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 4.7 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.36.1-py2.py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 4.7 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.36.0-py2.py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 3.1 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.35.1-py2.py3-none-any.whl (96 kB)\n",
      "\u001b[K     |████████████████████████████████| 96 kB 3.9 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.35.0-py2.py3-none-any.whl (96 kB)\n",
      "\u001b[K     |████████████████████████████████| 96 kB 1.2 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.34.0-py2.py3-none-any.whl (96 kB)\n",
      "\u001b[K     |████████████████████████████████| 96 kB 3.1 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.33.0-py2.py3-none-any.whl (92 kB)\n",
      "\u001b[K     |████████████████████████████████| 92 kB 7.2 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_storage-1.32.0-py2.py3-none-any.whl (92 kB)\n",
      "\u001b[K     |████████████████████████████████| 92 kB 6.6 MB/s \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of google-cloud-resource-manager to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
      "  Downloading google_cloud_resource_manager-1.4.0-py2.py3-none-any.whl (402 kB)\n",
      "\u001b[K     |████████████████████████████████| 402 kB 37.0 MB/s \n",
      "\u001b[?25h  Downloading google_cloud_resource_manager-1.3.3-py2.py3-none-any.whl (286 kB)\n",
      "\u001b[K     |████████████████████████████████| 286 kB 37.8 MB/s \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of google-cloud-core to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting google-cloud-core<2.0dev,>=1.0.3\n",
      "  Downloading google_cloud_core-1.7.1-py2.py3-none-any.whl (28 kB)\n",
      "INFO: pip is looking at multiple versions of google-cloud-resource-manager to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading google_cloud_core-1.7.0-py2.py3-none-any.whl (28 kB)\n",
      "  Downloading google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
      "  Downloading google_cloud_core-1.5.0-py2.py3-none-any.whl (27 kB)\n",
      "  Downloading google_cloud_core-1.4.4-py2.py3-none-any.whl (27 kB)\n",
      "  Downloading google_cloud_core-1.4.3-py2.py3-none-any.whl (27 kB)\n",
      "INFO: pip is looking at multiple versions of google-cloud-core to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading google_cloud_core-1.4.2-py2.py3-none-any.whl (26 kB)\n",
      "  Downloading google_cloud_core-1.4.1-py2.py3-none-any.whl (26 kB)\n",
      "  Downloading google_cloud_core-1.4.0-py2.py3-none-any.whl (26 kB)\n",
      "  Downloading google_cloud_core-1.3.0-py2.py3-none-any.whl (26 kB)\n",
      "  Downloading google_cloud_core-1.2.0-py2.py3-none-any.whl (26 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
      "  Downloading google_cloud_core-1.1.0-py2.py3-none-any.whl (26 kB)\n",
      "  Downloading google_cloud_core-1.0.3-py2.py3-none-any.whl (26 kB)\n",
      "INFO: pip is looking at multiple versions of google-cloud-bigquery to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting google-cloud-bigquery<3.0.0dev,>=1.15.0\n",
      "  Downloading google_cloud_bigquery-2.34.3-py2.py3-none-any.whl (206 kB)\n",
      "\u001b[K     |████████████████████████████████| 206 kB 48.1 MB/s \n",
      "\u001b[?25hCollecting google-cloud-core<3.0.0dev,>=1.4.1\n",
      "  Downloading google_cloud_core-2.3.0-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
      "Collecting google-resumable-media<3.0dev,>=0.6.0\n",
      "  Downloading google_resumable_media-2.3.2-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 4.3 MB/s \n",
      "\u001b[?25hCollecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.3.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-cloud-aiplatform) (3.0.8)\n",
      "Collecting protobuf>=3.12.0\n",
      "  Downloading protobuf-3.20.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 51.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (3.0.4)\n",
      "Installing collected packages: protobuf, google-crc32c, proto-plus, grpc-google-iam-v1, google-resumable-media, google-cloud-core, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, google-cloud-aiplatform\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.17.3\n",
      "    Uninstalling protobuf-3.17.3:\n",
      "      Successfully uninstalled protobuf-3.17.3\n",
      "  Attempting uninstall: google-resumable-media\n",
      "    Found existing installation: google-resumable-media 0.4.1\n",
      "    Uninstalling google-resumable-media-0.4.1:\n",
      "      Successfully uninstalled google-resumable-media-0.4.1\n",
      "  Attempting uninstall: google-cloud-core\n",
      "    Found existing installation: google-cloud-core 1.0.3\n",
      "    Uninstalling google-cloud-core-1.0.3:\n",
      "      Successfully uninstalled google-cloud-core-1.0.3\n",
      "  Attempting uninstall: google-cloud-storage\n",
      "    Found existing installation: google-cloud-storage 1.18.1\n",
      "    Uninstalling google-cloud-storage-1.18.1:\n",
      "      Successfully uninstalled google-cloud-storage-1.18.1\n",
      "  Attempting uninstall: google-cloud-bigquery\n",
      "    Found existing installation: google-cloud-bigquery 1.21.0\n",
      "    Uninstalling google-cloud-bigquery-1.21.0:\n",
      "      Successfully uninstalled google-cloud-bigquery-1.21.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
      "pandas-gbq 0.13.3 requires google-cloud-bigquery[bqstorage,pandas]<2.0.0dev,>=1.11.1, but you have google-cloud-bigquery 2.34.3 which is incompatible.\n",
      "google-cloud-translate 1.5.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.3.0 which is incompatible.\n",
      "google-cloud-firestore 1.7.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.3.0 which is incompatible.\n",
      "google-cloud-datastore 1.8.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.3.0 which is incompatible.\u001b[0m\n",
      "Successfully installed google-cloud-aiplatform-1.12.1 google-cloud-bigquery-2.34.3 google-cloud-core-2.3.0 google-cloud-resource-manager-1.4.1 google-cloud-storage-2.3.0 google-crc32c-1.3.0 google-resumable-media-2.3.2 grpc-google-iam-v1-0.12.4 proto-plus-1.20.3 protobuf-3.20.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "google"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "! pip3 install  --upgrade google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQhwq1iozAxh"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zo3YFZXLzCRJ"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_id"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "autoset_project_id"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "set_gcloud_project_id",
    "outputId": "35c7fb4b-1454-4cc7-8d9a-f45abc235a5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
    "\n",
    "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qohAA9fJulvP"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "timestamp"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8NKwwe7aulvQ"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "poKeKYG8ulvQ"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "**If you are using Google Cloud Notebooks**, your environment is already\n",
    "authenticated. Skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIpJGzF9ulvQ"
   },
   "source": [
    "**If you are using Colab**, run the cell below and follow the instructions\n",
    "when prompted to authenticate your account via oAuth.\n",
    "\n",
    "**Otherwise**, follow these steps:\n",
    "\n",
    "1. In the Cloud Console, go to the [**Create service account key**\n",
    "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
    "\n",
    "2. Click **Create service account**.\n",
    "\n",
    "3. In the **Service account name** field, enter a name, and\n",
    "   click **Create**.\n",
    "\n",
    "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
    "into the filter box, and select\n",
    "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
    "\n",
    "5. Click *Create*. A JSON file that contains your key downloads to your\n",
    "local environment.\n",
    "\n",
    "6. Enter the path to your service account key as the\n",
    "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Vh6KDXB5ulvQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# If on Google Cloud Notebooks, then don't execute this code\n",
    "IS_COLAB = False\n",
    "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        IS_COLAB = True\n",
    "        \n",
    "        from google.colab import auth as google_auth\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "When you initialize the Vertex SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "autoset_bucket"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Moosy2rOulvR",
    "outputId": "91d56423-02f0-4499-bcb1-d4a3a56730f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://vertex-ai-devaip-20220502143626/...\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validate_bucket"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "56irx2CvulvS"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### Set up variables\n",
    "\n",
    "Next, set up some variables used throughout the tutorial.\n",
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "wbvYPSTDulvS"
   },
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "accelerators:training,prediction,ngpu,mbsdk"
   },
   "source": [
    "#### Set hardware accelerators\n",
    "\n",
    "You can set hardware accelerators for training and prediction.\n",
    "\n",
    "Set the variables `TRAIN_GPU/TRAIN_NGPU` and `DEPLOY_GPU/DEPLOY_NGPU` to use a container image supporting a GPU and the number of GPUs allocated to the virtual machine (VM) instance. For example, to use a GPU container image with 4 Nvidia Telsa K80 GPUs allocated to each VM, you would specify:\n",
    "\n",
    "    (aip.AcceleratorType.NVIDIA_TESLA_K80, 4)\n",
    "\n",
    "\n",
    "Otherwise specify `(None, None)` to use a container image to run on a CPU.\n",
    "\n",
    "Learn more about [hardware accelerator support for your region](https://cloud.google.com/vertex-ai/docs/general/locations#accelerators).\n",
    "\n",
    "*Note*: TF releases before 2.3 for GPU support will fail to load the custom model in this tutorial. It is a known issue and fixed in TF 2.3. This is caused by static graph ops that are generated in the serving function. If you encounter this issue on your own custom models, use a container image for TF 2.3 with GPU support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "PryARdnoulvT"
   },
   "outputs": [],
   "source": [
    "if os.getenv(\"IS_TESTING_TRAIN_GPU\"):\n",
    "    TRAIN_GPU, TRAIN_NGPU = (\n",
    "        aip.gapic.AcceleratorType.NVIDIA_TESLA_K80,\n",
    "        int(os.getenv(\"IS_TESTING_TRAIN_GPU\")),\n",
    "    )\n",
    "else:\n",
    "    TRAIN_GPU, TRAIN_NGPU = (aip.gapic.AcceleratorType.NVIDIA_TESLA_K80, 4)\n",
    "\n",
    "if os.getenv(\"IS_TESTING_DEPLOY_GPU\"):\n",
    "    DEPLOY_GPU, DEPLOY_NGPU = (\n",
    "        aip.gapic.AcceleratorType.NVIDIA_TESLA_K80,\n",
    "        int(os.getenv(\"IS_TESTING_DEPLOY_GPU\")),\n",
    "    )\n",
    "else:\n",
    "    DEPLOY_GPU, DEPLOY_NGPU = (None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "container:training,prediction"
   },
   "source": [
    "#### Set pre-built containers\n",
    "\n",
    "Set the pre-built Docker container image for training and prediction.\n",
    "\n",
    "\n",
    "For the latest list, see [Pre-built containers for training](https://cloud.google.com/ai-platform-unified/docs/training/pre-built-containers).\n",
    "\n",
    "\n",
    "For the latest list, see [Pre-built containers for prediction](https://cloud.google.com/ai-platform-unified/docs/predictions/pre-built-containers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LhhUFw2nulvT",
    "outputId": "f904b43b-f1c6-43a4-e102-90160497032f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-1:latest AcceleratorType.NVIDIA_TESLA_K80 4\n",
      "Deployment: us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-1:latest None None\n"
     ]
    }
   ],
   "source": [
    "if os.getenv(\"IS_TESTING_TF\"):\n",
    "    TF = os.getenv(\"IS_TESTING_TF\")\n",
    "else:\n",
    "    TF = \"2.1\".replace(\".\", \"-\")\n",
    "\n",
    "if TF[0] == \"2\":\n",
    "    if TRAIN_GPU:\n",
    "        TRAIN_VERSION = \"tf-gpu.{}\".format(TF)\n",
    "    else:\n",
    "        TRAIN_VERSION = \"tf-cpu.{}\".format(TF)\n",
    "    if DEPLOY_GPU:\n",
    "        DEPLOY_VERSION = \"tf2-gpu.{}\".format(TF)\n",
    "    else:\n",
    "        DEPLOY_VERSION = \"tf2-cpu.{}\".format(TF)\n",
    "else:\n",
    "    if TRAIN_GPU:\n",
    "        TRAIN_VERSION = \"tf-gpu.{}\".format(TF)\n",
    "    else:\n",
    "        TRAIN_VERSION = \"tf-cpu.{}\".format(TF)\n",
    "    if DEPLOY_GPU:\n",
    "        DEPLOY_VERSION = \"tf-gpu.{}\".format(TF)\n",
    "    else:\n",
    "        DEPLOY_VERSION = \"tf-cpu.{}\".format(TF)\n",
    "\n",
    "TRAIN_IMAGE = \"{}-docker.pkg.dev/vertex-ai/training/{}:latest\".format(\n",
    "    REGION.split(\"-\")[0], TRAIN_VERSION\n",
    ")\n",
    "DEPLOY_IMAGE = \"{}-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(\n",
    "    REGION.split(\"-\")[0], DEPLOY_VERSION\n",
    ")\n",
    "\n",
    "print(\"Training:\", TRAIN_IMAGE, TRAIN_GPU, TRAIN_NGPU)\n",
    "print(\"Deployment:\", DEPLOY_IMAGE, DEPLOY_GPU, DEPLOY_NGPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "machine:training"
   },
   "source": [
    "#### Set machine type\n",
    "\n",
    "Next, set the machine type to use for training.\n",
    "\n",
    "- Set the variable `TRAIN_COMPUTE` to configure  the compute resources for the VMs you will use for for training.\n",
    " - `machine type`\n",
    "     - `n1-standard`: 3.75GB of memory per vCPU.\n",
    "     - `n1-highmem`: 6.5GB of memory per vCPU\n",
    "     - `n1-highcpu`: 0.9 GB of memory per vCPU\n",
    " - `vCPUs`: number of \\[2, 4, 8, 16, 32, 64, 96 \\]\n",
    "\n",
    "*Note: The following is not supported for training:*\n",
    "\n",
    " - `standard`: 2 vCPUs\n",
    " - `highcpu`: 2, 4 and 8 vCPUs\n",
    "\n",
    "*Note: You may also use n2 and e2 machine types for training and deployment, but they do not support GPUs*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vytMaukeulvT",
    "outputId": "9f1a7a63-7315-4c48-fe0a-16dcf1973c4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train machine type n1-standard-4\n"
     ]
    }
   ],
   "source": [
    "if os.getenv(\"IS_TESTING_TRAIN_MACHINE\"):\n",
    "    MACHINE_TYPE = os.getenv(\"IS_TESTING_TRAIN_MACHINE\")\n",
    "else:\n",
    "    MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Train machine type\", TRAIN_COMPUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mirrored_intro"
   },
   "source": [
    "## Mirrored Strategy\n",
    "\n",
    "When training on a single VM, one can either train was a single compute device or with multiple compute devices on the same VM. With Vertex AI Distributed Training you can specify both the number of compute devices for the VM instance and type of compute devices: CPU, GPU.\n",
    "\n",
    "Vertex AI Distributed Training supports `tf.distribute.MirroredStrategy' for TensorFlow models. To enable training across multiple compute devices on the same VM, you do the following additional steps in your Python training script:\n",
    "\n",
    "1. Set the tf.distribute.MirrorStrategy\n",
    "2. Compile the model within the scope of tf.distribute.MirrorStrategy. *Note:* Tells MirroredStrategy which variables to mirror across your compute devices.\n",
    "3. Increase the batch size for each compute device to num_devices * batch size.\n",
    "\n",
    "During transitions, the distribution of batches will be synchronized as well as the updates to the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_custom_pp_training_job:mbsdk"
   },
   "source": [
    "### Create and run custom training job\n",
    "\n",
    "\n",
    "To train a custom model, you perform two steps: 1) create a custom training job, and 2) run the job.\n",
    "\n",
    "#### Create custom training job\n",
    "\n",
    "A custom training job is created with the `CustomTrainingJob` class, with the following parameters:\n",
    "\n",
    "- `display_name`: The human readable name for the custom training job.\n",
    "- `container_uri`: The training container image.\n",
    "\n",
    "- `python_package_gcs_uri`: The location of the Python training package as a tarball.\n",
    "- `python_module_name`: The relative path to the training script in the Python package.\n",
    "- `model_serving_container_uri`: The container image for deploying the model.\n",
    "\n",
    "*Note:* There is no requirements parameter. You specify any requirements in the `setup.py` script in your Python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "mhw34XoOulvU"
   },
   "outputs": [],
   "source": [
    "DISPLAY_NAME = \"boston_\" + TIMESTAMP\n",
    "\n",
    "job = aip.CustomPythonPackageTrainingJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    python_package_gcs_uri=f\"{BUCKET_NAME}/trainer_boston.tar.gz\",\n",
    "    python_module_name=\"trainer.task\",\n",
    "    container_uri=TRAIN_IMAGE,\n",
    "    model_serving_container_image_uri=DEPLOY_IMAGE,\n",
    "    project=PROJECT_ID,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "examine_training_package"
   },
   "source": [
    "### Examine the training package\n",
    "\n",
    "#### Package layout\n",
    "\n",
    "Before you start the training, you will look at how a Python package is assembled for a custom training job. When unarchived, the package contains the following directory/file layout.\n",
    "\n",
    "- PKG-INFO\n",
    "- README.md\n",
    "- setup.cfg\n",
    "- setup.py\n",
    "- trainer\n",
    "  - \\_\\_init\\_\\_.py\n",
    "  - task.py\n",
    "\n",
    "The files `setup.cfg` and `setup.py` are the instructions for installing the package into the operating environment of the Docker image.\n",
    "\n",
    "The file `trainer/task.py` is the Python script for executing the custom training job. *Note*, when we referred to it in the worker pool specification, we replace the directory slash with a dot (`trainer.task`) and dropped the file suffix (`.py`).\n",
    "\n",
    "#### Package Assembly\n",
    "\n",
    "In the following cells, you will assemble the training package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "IAaZpZyyulvU"
   },
   "outputs": [],
   "source": [
    "# Make folder for Python training script\n",
    "! rm -rf custom\n",
    "! mkdir custom\n",
    "\n",
    "# Add package information\n",
    "! touch custom/README.md\n",
    "\n",
    "setup_cfg = \"[egg_info]\\n\\ntag_build =\\n\\ntag_date = 0\"\n",
    "! echo \"$setup_cfg\" > custom/setup.cfg\n",
    "\n",
    "setup_py = \"import setuptools\\n\\nsetuptools.setup(\\n\\n    install_requires=[\\n\\n        'tensorflow==2.5.0',\\n\\n        'tensorflow_datasets==1.3.0',\\n\\n    ],\\n\\n    packages=setuptools.find_packages())\"\n",
    "! echo \"$setup_py\" > custom/setup.py\n",
    "\n",
    "pkg_info = \"Metadata-Version: 1.0\\n\\nName: Boston Housing cloud\\n\\nVersion: 0.0.0\\n\\nSummary: Demostration training script\\n\\nHome-page: www.google.com\\n\\nAuthor: Google\\n\\nAuthor-email: aferlitsch@google.com\\n\\nLicense: Public\\n\\nDescription: Demo\\n\\nPlatform: Vertex\"\n",
    "! echo \"$pkg_info\" > custom/PKG-INFO\n",
    "\n",
    "# Make the training subfolder\n",
    "! mkdir custom/trainer\n",
    "! touch custom/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taskpy_contents:mirrored,boston"
   },
   "source": [
    "#### Task.py contents\n",
    "\n",
    "In the next cell, you write the contents of the training script task.py. I won't go into detail, it's just there for you to browse. In summary:\n",
    "\n",
    "- Get the directory where to save the model artifacts from the command line (`--model_dir`), and if not specified, then from the environment variable `AIP_MODEL_DIR`.\n",
    "- Loads Boston Housing dataset from TF.Keras builtin datasets\n",
    "- Builds a simple deep neural network model using TF.Keras model API.\n",
    "- Compiles the model (`compile()`).\n",
    "- Sets a training distribution strategy according to the argument `args.distribute`.\n",
    "- Trains the model (`fit()`) with epochs specified by `args.epochs`.\n",
    "- Saves the trained model (`save(args.model_dir)`) to the specified model directory.\n",
    "- Saves the maximum value for each feature `f.write(str(params))` to the specified parameters file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zKzddzl6ulvV",
    "outputId": "125a21b9-7405-4c3a-c047-c98576559922"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom/trainer/task.py\n",
    "# Single, Mirrored and MultiWorker Distributed Training\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model-dir', dest='model_dir',\n",
    "                    default=os.getenv('AIP_MODEL_DIR'), type=str, help='Model dir.')\n",
    "parser.add_argument('--lr', dest='lr',\n",
    "                    default=0.001, type=float,\n",
    "                    help='Learning rate.')\n",
    "parser.add_argument('--epochs', dest='epochs',\n",
    "                    default=10, type=int,\n",
    "                    help='Number of epochs.')\n",
    "parser.add_argument('--steps', dest='steps',\n",
    "                    default=100, type=int,\n",
    "                    help='Number of steps per epoch.')\n",
    "parser.add_argument('--batch_size', dest='batch_size',\n",
    "                    default=16, type=int,\n",
    "                    help='Size of a batch.')\n",
    "parser.add_argument('--distribute', dest='distribute', type=str, default='single',\n",
    "                    help='distributed training strategy')\n",
    "parser.add_argument('--param-file', dest='param_file',\n",
    "                    default='/tmp/param.txt', type=str,\n",
    "                    help='Output file for parameters')\n",
    "args = parser.parse_args()\n",
    "\n",
    "logging.info('DEVICES'  + str(device_lib.list_local_devices()))\n",
    "\n",
    "# Single Machine, single compute device\n",
    "if args.distribute == 'single':\n",
    "    if tf.test.is_gpu_available():\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "    else:\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "    logging.info(\"Single device training\")\n",
    "# Single Machine, multiple compute device\n",
    "elif args.distribute == 'mirrored':\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    logging.info(\"Mirrored Strategy distributed training\")\n",
    "# Multi Machine, multiple compute device\n",
    "elif args.distribute == 'multiworker':\n",
    "    strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "    logging.info(\"Multi-worker Strategy distributed training\")\n",
    "    logging.info('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', 'Not found')))\n",
    "    # Single Machine, multiple TPU devices\n",
    "elif args.distribute == 'tpu':\n",
    "    cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=\"local\")\n",
    "    tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
    "    tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
    "    strategy = tf.distribute.TPUStrategy(cluster_resolver)\n",
    "    print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
    "\n",
    "logging.info('num_replicas_in_sync = {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "def _is_chief(task_type, task_id):\n",
    "    ''' Check for primary if multiworker training\n",
    "    '''\n",
    "    return (task_type == 'chief') or (task_type == 'worker' and task_id == 0) or task_type is None\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    # Scaling Boston Housing data features\n",
    "    def scale(feature):\n",
    "        max = np.max(feature)\n",
    "        feature = (feature / max).astype(np.float)\n",
    "        return feature, max\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.boston_housing.load_data(\n",
    "        path=\"boston_housing.npz\", test_split=0.2, seed=113\n",
    "    )\n",
    "\n",
    "    params = []\n",
    "    for _ in range(13):\n",
    "        x_train[_], max = scale(x_train[_])\n",
    "        x_test[_], _ = scale(x_test[_])\n",
    "    params.append(max)\n",
    "\n",
    "    # store the normalization (max) value for each feature\n",
    "    with tf.io.gfile.GFile(args.param_file, 'w') as f:\n",
    "        f.write(str(params))\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def get_model():\n",
    "    model = tf.keras.Sequential([\n",
    "          tf.keras.layers.Dense(128, activation='relu', input_shape=(13,)),\n",
    "          tf.keras.layers.Dense(128, activation='relu'),\n",
    "          tf.keras.layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "          loss='mse',\n",
    "          optimizer=tf.keras.optimizers.RMSprop(learning_rate=args.lr)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train(model, x_train, y_train):\n",
    "    NUM_WORKERS = strategy.num_replicas_in_sync\n",
    "    # Here the batch size scales up by number of workers since\n",
    "    # `tf.data.Dataset.batch` expects the global batch size.\n",
    "    GLOBAL_BATCH_SIZE = args.batch_size * NUM_WORKERS\n",
    "\n",
    "    model.fit(x_train, y_train, epochs=args.epochs, batch_size=GLOBAL_BATCH_SIZE)\n",
    "\n",
    "    if args.distribute == 'multiworker':\n",
    "        task_type, task_id = (strategy.cluster_resolver.task_type,\n",
    "                              strategy.cluster_resolver.task_id)\n",
    "    else:\n",
    "        task_type, task_id = None, None\n",
    "\n",
    "    if args.distribute==\"tpu\":\n",
    "        save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
    "        model.save(args.model_dir, options=save_locally)\n",
    "    # single, mirrored or primary for multiworker\n",
    "    elif _is_chief(task_type, task_id):\n",
    "        model.save(args.model_dir)\n",
    "    # non-primary workers for multi-workers\n",
    "    else:\n",
    "        # each worker saves their model instance to a unique temp location\n",
    "        worker_dir = args.model_dir + '/workertemp_' + str(task_id)\n",
    "        tf.io.gfile.makedirs(worker_dir)\n",
    "        model.save(worker_dir)\n",
    "\n",
    "with strategy.scope():\n",
    "    # Creation of dataset, and model building/compiling need to be within\n",
    "    # `strategy.scope()`.\n",
    "    model = get_model()\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = get_data()\n",
    "\n",
    "train(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tarball_training_script"
   },
   "source": [
    "#### Store training script on your Cloud Storage bucket\n",
    "\n",
    "Next, you package the training folder into a compressed tar ball, and then store it in your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LFUHioqTulvV",
    "outputId": "f1179e96-1462-45da-b1c6-c4b6dc5a1c33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom/\n",
      "custom/trainer/\n",
      "custom/trainer/task.py\n",
      "custom/trainer/__init__.py\n",
      "custom/setup.py\n",
      "custom/README.md\n",
      "custom/PKG-INFO\n",
      "custom/setup.cfg\n",
      "Copying file://custom.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  2.4 KiB/  2.4 KiB]                                                \n",
      "Operation completed over 1 objects/2.4 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "! rm -f custom.tar custom.tar.gz\n",
    "! tar cvf custom.tar custom\n",
    "! gzip custom.tar\n",
    "! gsutil cp custom.tar.gz $BUCKET_NAME/trainer_boston.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_custom_pp_training_job:mirrored"
   },
   "source": [
    "#### Run the custom Python package training job\n",
    "\n",
    "Next, you run the custom job to start the training job by invoking the method `run()`. The parameters are the same as when running a CustomTrainingJob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LnUX0UkvulvV",
    "outputId": "246ced5e-17c3-4a86-8c02-d743054c5daf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Output directory:\n",
      "gs://vertex-ai-devaip-20220502143626 \n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/5460718452035551232?project=931647533046\n",
      "View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/3651907475134742528?project=931647533046\n",
      "CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/5460718452035551232 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/5460718452035551232 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/5460718452035551232 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/5460718452035551232 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/5460718452035551232 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/5460718452035551232 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob run completed. Resource name: projects/931647533046/locations/us-central1/trainingPipelines/5460718452035551232\n",
      "Model available at projects/931647533046/locations/us-central1/models/1916233262934523904\n"
     ]
    }
   ],
   "source": [
    "MODEL_DIR = BUCKET_NAME\n",
    "\n",
    "CMDARGS = [\"--epochs=5\", \"--batch_size=16\", \"--distribute=mirrored\"]\n",
    "\n",
    "model = job.run(\n",
    "    model_display_name=\"boston_\" + TIMESTAMP,\n",
    "    args=CMDARGS,\n",
    "    replica_count=1,\n",
    "    machine_type=TRAIN_COMPUTE,\n",
    "    accelerator_type=TRAIN_GPU.name,\n",
    "    accelerator_count=TRAIN_NGPU,\n",
    "    base_output_dir=MODEL_DIR,\n",
    "    sync=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "delete_job"
   },
   "source": [
    "### Delete a custom training job\n",
    "\n",
    "After a training job is completed, you can delete the training job with the method `delete()`.  Prior to completion, a training job can be canceled with the method `cancel()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iUWHFpPoulvW",
    "outputId": "15eff600-af84-468e-a819-e64d77406055"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting CustomPythonPackageTrainingJob : projects/931647533046/locations/us-central1/trainingPipelines/5460718452035551232\n",
      "Delete CustomPythonPackageTrainingJob  backing LRO: projects/931647533046/locations/us-central1/operations/1908310800620060672\n",
      "CustomPythonPackageTrainingJob deleted. . Resource name: projects/931647533046/locations/us-central1/trainingPipelines/5460718452035551232\n"
     ]
    }
   ],
   "source": [
    "job.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_delete:mbsdk"
   },
   "source": [
    "#### Delete the model\n",
    "\n",
    "The method 'delete()' will delete the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-0gqCUTEulvW",
    "outputId": "081393f0-de60-4035-a411-7f62b84aa506"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting Model : projects/931647533046/locations/us-central1/models/1916233262934523904\n",
      "Delete Model  backing LRO: projects/931647533046/locations/us-central1/operations/4286211403871682560\n",
      "Model deleted. . Resource name: projects/931647533046/locations/us-central1/models/1916233262934523904\n"
     ]
    }
   ],
   "source": [
    "model.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "multiworker_intro"
   },
   "source": [
    "## Multi-Worker Mirrored Strategy\n",
    "\n",
    "With Vertex AI Distributed Training you can train with multiple VM instances\n",
    "\n",
    "Vertex AI Distributed Training supports `tf.distribute.MultiWorkerMirroredStrategy' for TensorFlow and PyTorch models. To enable training across multiple VMS, you do the following additional steps in your Python training script:\n",
    "\n",
    "1. All the additional steps for MirroredStrategy, except that MultiWorkerStrategy is set in place of MirroredStrategy.\n",
    "2. Setup the worker pools.\n",
    "3. Alter the saving of the model so that the non-primary workers save their model instance to a unique temporary directory each.\n",
    "\n",
    "*Note:* You do not need to construct the TF_CONFIG environment variable. It is automatically constructed by Vertex AI Distributed Training.\n",
    "\n",
    "Learn more about [Distributed Training](https://cloud.google.com/vertex-ai/docs/training/distributed-training)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "worker_pools"
   },
   "source": [
    "### Worker pools\n",
    "\n",
    "If you run a distributed training job with Vertex AI, you specify multiple machines (nodes) in a training cluster. The training service allocates the resources for the machine types you specify. Your running job on a given node is called a replica. A group of replicas with the same configuration is called a worker pool.\n",
    "\n",
    "Each replica in the training cluster is given a single role or task in distributed training. For example:\n",
    "\n",
    "- **Primary replica**: Exactly one replica is designated the primary replica. This task manages the others and reports status for the job as a whole.\n",
    "\n",
    "- **Worker(s)**: One or more replicas may be designated as workers. These replicas do their portion of the work as you designate in your job configuration.\n",
    "\n",
    "- Parameter server(s): If supported by your ML framework, one or more replicas may be designated as parameter servers. These replicas store model parameters and coordinate shared model state between the workers.\n",
    "\n",
    "Evaluator(s): If supported by your ML framework, one or more replicas may be designated as evaluators. These replicas can be used to evaluate your model. If you are using TensorFlow, note that TensorFlow generally expects that you use no more than one evaluator.\n",
    "\n",
    "To configure a distributed training job, define your list of worker pools (workerPoolSpecs[]), designating one WorkerPoolSpec for each type of task:\n",
    "\n",
    "*Note:* The worker pool is order dependent (0..3):\n",
    "\n",
    "**workerPoolSpecs[0]**: Primary, chief, scheduler, or \"master\"\n",
    "\n",
    "**workerPoolSpecs[1]**: Secondary, replicas, workers\n",
    "\n",
    "**workerPoolSpecs[2]**: Parameter servers, Reduction Server\n",
    "\n",
    "**workerPoolSpecs[2]**: Evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "multiworker_methods"
   },
   "source": [
    "### Distributed training options for Multi-Worker Mirrored Strategy\n",
    "\n",
    "How you setup the worker pools is dependent on the Vertex AI method you use for training.\n",
    "\n",
    "**CustomTrainingJob** / **CustomContainerTrainingJob** / **CustomPythonPackageTrainingJob**\n",
    "\n",
    "The `replica_count` includes the primary and secondary (replica_count-1), and share the same machine type and accelerators.\n",
    "\n",
    "You cannot specify a parameter server or evaluation node.\n",
    "\n",
    "**CustomJob**\n",
    "\n",
    "You specify a `worker_pool_spec`, where you can specify detailed settings for each of the four worker pools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXvPN8P6ulvX"
   },
   "source": [
    "### Create and run custom training job\n",
    "\n",
    "\n",
    "To train a custom model, you perform two steps: 1) create a custom training job, and 2) run the job.\n",
    "\n",
    "#### Create custom training job\n",
    "\n",
    "A custom training job is created with the `CustomTrainingJob` class, with the following parameters:\n",
    "\n",
    "- `display_name`: The human readable name for the custom training job.\n",
    "- `container_uri`: The training container image.\n",
    "\n",
    "- `python_package_gcs_uri`: The location of the Python training package as a tarball.\n",
    "- `python_module_name`: The relative path to the training script in the Python package.\n",
    "- `model_serving_container_uri`: The container image for deploying the model.\n",
    "\n",
    "*Note:* There is no requirements parameter. You specify any requirements in the `setup.py` script in your Python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "kYcFsVSEulvX"
   },
   "outputs": [],
   "source": [
    "DISPLAY_NAME = \"boston_\" + TIMESTAMP\n",
    "\n",
    "job = aip.CustomPythonPackageTrainingJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    python_package_gcs_uri=f\"{BUCKET_NAME}/trainer_boston.tar.gz\",\n",
    "    python_module_name=\"trainer.task\",\n",
    "    container_uri=TRAIN_IMAGE,\n",
    "    model_serving_container_image_uri=DEPLOY_IMAGE,\n",
    "    project=PROJECT_ID,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_custom_pp_training_job:multiworker"
   },
   "source": [
    "#### Run the custom Python package training job\n",
    "\n",
    "Next, you run the custom job to start the training job by invoking the method `run()`. The parameters are the same as when running a CustomTrainingJob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GHRxPU32ulvX",
    "outputId": "7e1688f1-dc0b-47e5-d588-807394626b23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Output directory:\n",
      "gs://vertex-ai-devaip-20220502143626 \n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/563053857270136832?project=931647533046\n",
      "View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/8263593493562130432?project=931647533046\n",
      "CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/563053857270136832 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/563053857270136832 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/563053857270136832 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/563053857270136832 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/563053857270136832 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/563053857270136832 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "Training failed with:\n",
      "code: 3\n",
      "message: \"The replica workerpool0-0 exited with a non-zero status of 1. Termination reason: Error. \\nTraceback (most recent call last):\\n  [...]\\n  File \\\"/root/.local/lib/python3.7/site-packages/tensorflow/python/training/tracking/util.py\\\", line 1219, in save\\n    file_prefix_tensor, object_graph_tensor, options)\\n  File \\\"/root/.local/lib/python3.7/site-packages/tensorflow/python/training/tracking/util.py\\\", line 1164, in _save_cached_when_graph_building\\n    save_op = saver.save(file_prefix, options=options)\\n  File \\\"/root/.local/lib/python3.7/site-packages/tensorflow/python/training/saving/functional_saver.py\\\", line 300, in save\\n    return save_fn()\\n  File \\\"/root/.local/lib/python3.7/site-packages/tensorflow/python/training/saving/functional_saver.py\\\", line 287, in save_fn\\n    sharded_prefixes, file_prefix, delete_old_dirs=True)\\n  File \\\"/root/.local/lib/python3.7/site-packages/tensorflow/python/ops/gen_io_ops.py\\\", line 504, in merge_v2_checkpoints\\n    delete_old_dirs=delete_old_dirs, name=name, ctx=_ctx)\\n  File \\\"/root/.local/lib/python3.7/site-packages/tensorflow/python/ops/gen_io_ops.py\\\", line 528, in merge_v2_checkpoints_eager_fallback\\n    attrs=_attrs, ctx=ctx, name=name)\\n  File \\\"/root/.local/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\\\", line 60, in quick_execute\\n    inputs, attrs, num_outputs)\\ntensorflow.python.framework.errors_impl.NotFoundError: Unable to read file (gs://vertex-ai-devaip-20220502143626/model/variables/variables_temp/part-00000-of-00001.index). Perhaps the file is corrupt or was produced by a newer version of TensorFlow with format changes (failed to seek to header entry): Error executing an HTTP request: HTTP response code 404 with body \\'<?xml version=\\'1.0\\' encoding=\\'UTF-8\\'?><Error><Code>NoSuchKey</Code><Message>The specified key does not exist.</Message><Details>No such object: vertex-ai-devaip-20220502143626/model/variables/variables_temp/part-00000-of-00001.index</Details></Error>\\'\\n\\t when reading gs://vertex-ai-devaip-20220502143626/model/variables/variables_temp/part-00000-of-00001.index [Op:MergeV2Checkpoints]\\n\\nTo find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=931647533046&resource=ml_job%2Fjob_id%2F8263593493562130432&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%228263593493562130432%22\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_DIR = BUCKET_NAME\n",
    "\n",
    "CMDARGS = [\"--epochs=5\", \"--batch_size=16\", \"--distribute=multiworker\"]\n",
    "\n",
    "try:\n",
    "    model = job.run(\n",
    "        model_display_name=\"boston_\" + TIMESTAMP,\n",
    "        args=CMDARGS,\n",
    "        replica_count=4,\n",
    "        machine_type=TRAIN_COMPUTE,\n",
    "        accelerator_type=TRAIN_GPU.name,\n",
    "        accelerator_count=TRAIN_NGPU,\n",
    "        base_output_dir=MODEL_DIR,\n",
    "        sync=True,\n",
    "    )\n",
    "except Exception as e:\n",
    "    # may fail duing model.save() -- seems to be some issue when merging checkpoints from the workers\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92D_hbuVulvX"
   },
   "source": [
    "### Delete a custom training job\n",
    "\n",
    "After a training job is completed, you can delete the training job with the method `delete()`.  Prior to completion, a training job can be canceled with the method `cancel()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CqrfWkB3ulvX",
    "outputId": "65d48550-4be0-42a0-b9ea-aad017c3e123"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting CustomPythonPackageTrainingJob : projects/931647533046/locations/us-central1/trainingPipelines/563053857270136832\n",
      "Delete CustomPythonPackageTrainingJob  backing LRO: projects/931647533046/locations/us-central1/operations/4739949066329260032\n",
      "CustomPythonPackageTrainingJob deleted. . Resource name: projects/931647533046/locations/us-central1/trainingPipelines/563053857270136832\n"
     ]
    }
   ],
   "source": [
    "job.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "customjob_intro:multiworker"
   },
   "source": [
    "### Multiworker distributed training with CustomJob\n",
    "\n",
    "Multiworker distributed training with `CustomJob` has the advantages of fine detail control of the primary replica and optionally specifying worker pools for parameter server and evaluators. Creating a `CustomJob` includes the following steps:\n",
    "\n",
    "\n",
    "1. Specify individual details for each worker pool.\n",
    "2. Embed training package into Docker image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_docker_container:training"
   },
   "source": [
    "### Create a Docker file\n",
    "\n",
    "To use your own custom training container, you build a Docker file and embed into the container your training scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "write_docker_file:training,multiworker"
   },
   "source": [
    "#### Write the Docker file contents\n",
    "\n",
    "Your first step in containerizing your code is to create a Docker file. In your Docker you’ll include all the commands needed to run your container image. It’ll install all the libraries you’re using and set up the entry point for your training code.\n",
    "\n",
    "1. Install a pre-defined container image from TensorFlow repository for deep learning images.\n",
    "2. Copies in the Python training code, to be shown subsequently.\n",
    "3. Sets the entry into the Python training script as `trainer/task.py`. Note, the `.py` is dropped in the ENTRYPOINT command, as it is implied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pGI2viDAulvY",
    "outputId": "fbf205a7-1f50-4008-fac6-7dcd7776c89b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/tf2-gpu.2-5\n",
    "\n",
    "WORKDIR /\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY trainer /trainer\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "name_container:training"
   },
   "source": [
    "#### Build the container locally\n",
    "\n",
    "Next, you will provide a name for your customer container that you will use when you submit it to the Google Container Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "7P8cdlFtulvY"
   },
   "outputs": [],
   "source": [
    "TRAIN_IMAGE = \"gcr.io/\" + PROJECT_ID + \"/boston:v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "build_container:training"
   },
   "source": [
    "Next, build the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jmw5cakNulvY",
    "outputId": "3b44f939-8e62-4ef4-a020-c9be13425c5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preconfiguring packages ...\n",
      "Selecting previously unselected package libnfnetlink0:amd64.\n",
      "(Reading database ... 155202 files and directories currently installed.)\n",
      "Preparing to unpack .../00-libnfnetlink0_1.0.1-3_amd64.deb ...\n",
      "Unpacking libnfnetlink0:amd64 (1.0.1-3) ...\n",
      "Selecting previously unselected package pigz.\n",
      "Preparing to unpack .../01-pigz_2.4-1_amd64.deb ...\n",
      "Unpacking pigz (2.4-1) ...\n",
      "Selecting previously unselected package libmnl0:amd64.\n",
      "Preparing to unpack .../02-libmnl0_1.0.4-2_amd64.deb ...\n",
      "Unpacking libmnl0:amd64 (1.0.4-2) ...\n",
      "Selecting previously unselected package iproute2.\n",
      "Preparing to unpack .../03-iproute2_4.15.0-2ubuntu1.3_amd64.deb ...\n",
      "Unpacking iproute2 (4.15.0-2ubuntu1.3) ...\n",
      "Selecting previously unselected package libatm1:amd64.\n",
      "Preparing to unpack .../04-libatm1_1%3a2.5.1-2build1_amd64.deb ...\n",
      "Unpacking libatm1:amd64 (1:2.5.1-2build1) ...\n",
      "Selecting previously unselected package libxtables12:amd64.\n",
      "Preparing to unpack .../05-libxtables12_1.6.1-2ubuntu2_amd64.deb ...\n",
      "Unpacking libxtables12:amd64 (1.6.1-2ubuntu2) ...\n",
      "Selecting previously unselected package apparmor.\n",
      "Preparing to unpack .../06-apparmor_2.12-4ubuntu5.1_amd64.deb ...\n",
      "Unpacking apparmor (2.12-4ubuntu5.1) ...\n",
      "Selecting previously unselected package libip6tc0:amd64.\n",
      "Preparing to unpack .../07-libip6tc0_1.6.1-2ubuntu2_amd64.deb ...\n",
      "Unpacking libip6tc0:amd64 (1.6.1-2ubuntu2) ...\n",
      "Selecting previously unselected package libiptc0:amd64.\n",
      "Preparing to unpack .../08-libiptc0_1.6.1-2ubuntu2_amd64.deb ...\n",
      "Unpacking libiptc0:amd64 (1.6.1-2ubuntu2) ...\n",
      "Selecting previously unselected package libnetfilter-conntrack3:amd64.\n",
      "Preparing to unpack .../09-libnetfilter-conntrack3_1.0.6-2_amd64.deb ...\n",
      "Unpacking libnetfilter-conntrack3:amd64 (1.0.6-2) ...\n",
      "Selecting previously unselected package iptables.\n",
      "Preparing to unpack .../10-iptables_1.6.1-2ubuntu2_amd64.deb ...\n",
      "Unpacking iptables (1.6.1-2ubuntu2) ...\n",
      "Selecting previously unselected package bridge-utils.\n",
      "Preparing to unpack .../11-bridge-utils_1.5-15ubuntu1_amd64.deb ...\n",
      "Unpacking bridge-utils (1.5-15ubuntu1) ...\n",
      "Selecting previously unselected package runc.\n",
      "Preparing to unpack .../12-runc_1.0.1-0ubuntu2~18.04.1_amd64.deb ...\n",
      "Unpacking runc (1.0.1-0ubuntu2~18.04.1) ...\n",
      "Selecting previously unselected package containerd.\n",
      "Preparing to unpack .../13-containerd_1.5.5-0ubuntu3~18.04.2_amd64.deb ...\n",
      "Unpacking containerd (1.5.5-0ubuntu3~18.04.2) ...\n",
      "Selecting previously unselected package dns-root-data.\n",
      "Preparing to unpack .../14-dns-root-data_2018013001_all.deb ...\n",
      "Unpacking dns-root-data (2018013001) ...\n",
      "Selecting previously unselected package dnsmasq-base.\n",
      "Preparing to unpack .../15-dnsmasq-base_2.79-1ubuntu0.5_amd64.deb ...\n",
      "Unpacking dnsmasq-base (2.79-1ubuntu0.5) ...\n",
      "Selecting previously unselected package docker.io.\n",
      "Preparing to unpack .../16-docker.io_20.10.7-0ubuntu5~18.04.3_amd64.deb ...\n",
      "Unpacking docker.io (20.10.7-0ubuntu5~18.04.3) ...\n",
      "Selecting previously unselected package netcat-traditional.\n",
      "Preparing to unpack .../17-netcat-traditional_1.10-41.1_amd64.deb ...\n",
      "Unpacking netcat-traditional (1.10-41.1) ...\n",
      "Selecting previously unselected package netcat.\n",
      "Preparing to unpack .../18-netcat_1.10-41.1_all.deb ...\n",
      "Unpacking netcat (1.10-41.1) ...\n",
      "Selecting previously unselected package ubuntu-fan.\n",
      "Preparing to unpack .../19-ubuntu-fan_0.12.10_all.deb ...\n",
      "Unpacking ubuntu-fan (0.12.10) ...\n",
      "Setting up runc (1.0.1-0ubuntu2~18.04.1) ...\n",
      "Setting up dns-root-data (2018013001) ...\n",
      "Setting up netcat-traditional (1.10-41.1) ...\n",
      "update-alternatives: using /bin/nc.traditional to provide /bin/nc (nc) in auto mode\n",
      "Setting up libiptc0:amd64 (1.6.1-2ubuntu2) ...\n",
      "Setting up apparmor (2.12-4ubuntu5.1) ...\n",
      "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
      "Setting up containerd (1.5.5-0ubuntu3~18.04.2) ...\n",
      "Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service → /lib/systemd/system/containerd.service.\n",
      "Setting up bridge-utils (1.5-15ubuntu1) ...\n",
      "Setting up libatm1:amd64 (1:2.5.1-2build1) ...\n",
      "Setting up libxtables12:amd64 (1.6.1-2ubuntu2) ...\n",
      "Setting up netcat (1.10-41.1) ...\n",
      "Setting up libnfnetlink0:amd64 (1.0.1-3) ...\n",
      "Setting up libmnl0:amd64 (1.0.4-2) ...\n",
      "Setting up pigz (2.4-1) ...\n",
      "Setting up libip6tc0:amd64 (1.6.1-2ubuntu2) ...\n",
      "Setting up libnetfilter-conntrack3:amd64 (1.0.6-2) ...\n",
      "Setting up iproute2 (4.15.0-2ubuntu1.3) ...\n",
      "Setting up iptables (1.6.1-2ubuntu2) ...\n",
      "Setting up dnsmasq-base (2.79-1ubuntu0.5) ...\n",
      "Setting up docker.io (20.10.7-0ubuntu5~18.04.3) ...\n",
      "Adding group `docker' (GID 109) ...\n",
      "Done.\n",
      "Created symlink /etc/systemd/system/multi-user.target.wants/docker.service → /lib/systemd/system/docker.service.\n",
      "Created symlink /etc/systemd/system/sockets.target.wants/docker.socket → /lib/systemd/system/docker.socket.\n",
      "invoke-rc.d: unknown initscript, /etc/init.d/docker not found.\n",
      "invoke-rc.d: could not determine current runlevel\n",
      "Setting up ubuntu-fan (0.12.10) ...\n",
      "Created symlink /etc/systemd/system/multi-user.target.wants/ubuntu-fan.service → /lib/systemd/system/ubuntu-fan.service.\n",
      "invoke-rc.d: could not determine current runlevel\n",
      "invoke-rc.d: policy-rc.d denied execution of start.\n",
      "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
      "Processing triggers for dbus (1.12.2-1ubuntu1.2) ...\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
      "\n",
      "Processing triggers for systemd (237-3ubuntu10.53) ...\n"
     ]
    }
   ],
   "source": [
    "if not IS_COLAB:\n",
    "    ! docker build custom -t $TRAIN_IMAGE\n",
    "else:\n",
    "    # install docker daemon\n",
    "    ! apt-get -qq install docker.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test_container:training"
   },
   "source": [
    "#### Test the container locally\n",
    "\n",
    "Run the container within your notebook instance to ensure it’s working correctly. You will run it for 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "jJGLjU-TulvZ"
   },
   "outputs": [],
   "source": [
    "if not IS_COLAB:\n",
    "    ! docker run $TRAIN_IMAGE --epochs=5 --model-dir=./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "register_container:training"
   },
   "source": [
    "#### Register the custom container\n",
    "\n",
    "When you’ve finished running the container locally, push it to Google Container Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "GAXGjae7ulvZ"
   },
   "outputs": [],
   "source": [
    "if not IS_COLAB:\n",
    "    ! docker push $TRAIN_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "worker_pool_primary"
   },
   "source": [
    "#### Primary worker pool\n",
    "\n",
    "The primary worker pool (index 0) coordinates the work done by all the other replicas. Set the replicaCount to 1. Since the worker is coordinating and not training, use a general purpose CPU, instead of a GPU.\n",
    "\n",
    "Learn more about [Machine Types for Training](https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "CEAnXBzCulvZ"
   },
   "outputs": [],
   "source": [
    "PRIMARY_COMPUTE = \"n2-highcpu-64\"\n",
    "\n",
    "MODEL_DIR = BUCKET_NAME\n",
    "\n",
    "CMDARGS = [\n",
    "    \"--model-dir=\" + MODEL_DIR,\n",
    "    \"--epochs=5\",\n",
    "    \"--batch_size=16\",\n",
    "    \"--distribute=multiworker\",\n",
    "]\n",
    "\n",
    "CONTAINER_SPEC = {\"image_uri\": TRAIN_IMAGE, \"command\": \"trainer.task\", \"args\": CMDARGS}\n",
    "\n",
    "PRIMARY_WORKER_POOL = {\n",
    "    \"replica_count\": 1,\n",
    "    \"machine_spec\": {\"machine_type\": PRIMARY_COMPUTE, \"accelerator_count\": 0},\n",
    "    \"container_spec\": CONTAINER_SPEC,\n",
    "}\n",
    "\n",
    "WORKER_POOL_SPECS = [PRIMARY_WORKER_POOL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "worker_pool_training"
   },
   "source": [
    "#### Training worker pool\n",
    "\n",
    "The secondary worker pool (index 1) performs model training. Each of the replicas will have an instance of the your training package installed on it.\n",
    "\n",
    "Each replica may have one (single device training) or multiple (mirrored) compute devices for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "6dchPSfNulvZ"
   },
   "outputs": [],
   "source": [
    "TRAIN_WORKER_POOL = {\n",
    "    \"replica_count\": 4,\n",
    "    \"machine_spec\": {\n",
    "        \"machine_type\": TRAIN_COMPUTE,\n",
    "        \"accelerator_count\": TRAIN_NGPU,\n",
    "        \"accelerator_type\": TRAIN_GPU,\n",
    "    },\n",
    "    \"container_spec\": CONTAINER_SPEC,\n",
    "}\n",
    "\n",
    "WORKER_POOL_SPECS.append(TRAIN_WORKER_POOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "custom_job:worker_pool"
   },
   "source": [
    "### Create CustomJob with worker pool specifications\n",
    "\n",
    "Next, you create a `CustomJob` for the multi-worker distributed training job:\n",
    "\n",
    "-`display_name`: The display name for the custom job.\n",
    "\n",
    "-`worker_pool_specs`: The detailed specifications for each worker pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "m2VgmqEOulva"
   },
   "outputs": [],
   "source": [
    "DISPLAY_NAME = \"boston_\" + TIMESTAMP\n",
    "\n",
    "job = aip.CustomJob(display_name=DISPLAY_NAME, worker_pool_specs=WORKER_POOL_SPECS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_custom_job:multiworker"
   },
   "source": [
    "### Run the CustomJob\n",
    "\n",
    "Next, you run the custom job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hg8vnI_Wulva",
    "outputId": "1d32ec26-4fa6-4ff7-8809-4d5037479b6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating CustomJob\n",
      "CustomJob created. Resource name: projects/931647533046/locations/us-central1/customJobs/3652523201646297088\n",
      "To use this CustomJob in another session:\n",
      "custom_job = aiplatform.CustomJob.get('projects/931647533046/locations/us-central1/customJobs/3652523201646297088')\n",
      "View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/3652523201646297088?project=931647533046\n",
      "CustomJob projects/931647533046/locations/us-central1/customJobs/3652523201646297088 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/931647533046/locations/us-central1/customJobs/3652523201646297088 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/931647533046/locations/us-central1/customJobs/3652523201646297088 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/931647533046/locations/us-central1/customJobs/3652523201646297088 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/931647533046/locations/us-central1/customJobs/3652523201646297088 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/931647533046/locations/us-central1/customJobs/3652523201646297088 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/931647533046/locations/us-central1/customJobs/3652523201646297088 current state:\n",
      "JobState.JOB_STATE_FAILED\n",
      "Job failed with:\n",
      "code: 3\n",
      "message: \"The replica workerpool0-0 exited with a non-zero status of 127. Termination reason: ContainerCannotRun. \\n Termination log message: OCI runtime create failed: container_linux.go:380: starting container process caused: exec: \\\"t\\\": executable file not found in $PATH: unknown\\nTo find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=931647533046&resource=ml_job%2Fjob_id%2F3652523201646297088&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%223652523201646297088%22\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    job.run(sync=True)\n",
    "except Exception as e:\n",
    "    # may fail in multi-worker to find startup script\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WT76Sc-culva"
   },
   "source": [
    "### Delete a custom training job\n",
    "\n",
    "After a training job is completed, you can delete the training job with the method `delete()`.  Prior to completion, a training job can be canceled with the method `cancel()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I_IxVfuDulva",
    "outputId": "faced87c-b654-477d-8d27-edb612aa3e7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting CustomJob : projects/931647533046/locations/us-central1/customJobs/3652523201646297088\n",
      "Delete CustomJob  backing LRO: projects/931647533046/locations/us-central1/operations/8950814717920673792\n",
      "CustomJob deleted. . Resource name: projects/931647533046/locations/us-central1/customJobs/3652523201646297088\n"
     ]
    }
   ],
   "source": [
    "job.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reduction_server_intro"
   },
   "source": [
    "## Reduction Server\n",
    "\n",
    "To speed up training of large models, many engineering teams are adopting distributed training using scale-out clusters of ML accelerators. However, distributed training at scale brings its own set of challenges. Specifically, limited network bandwidth between nodes makes optimizing performance of distributed training inherently difficult, particularly for large cluster configurations.\n",
    "\n",
    "Vertex AI Reduction Server optimizes bandwidth and latency of multi-node distributed training on NVIDIA GPUs for synchronous data parallel algorithms. Synchronous data parallelism is the foundation of many widely adopted distributed training frameworks, including TensorFlow’s MultiWorkerMirroredStrategy, Horovod, and PyTorch Distributed. By optimizing bandwidth usage and latency of the all-reduce collective operation used by these frameworks, Reduction Server can decrease both the time and cost of large training jobs.\n",
    "\n",
    "Learn more about [Optimizing training performance using Vertex Reduction Server](https://cloud.google.com/blog/topics/developers-practitioners/optimize-training-performance-reduction-server-vertex-ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "worker_pool_reduction_server"
   },
   "outputs": [],
   "source": [
    "reduction_server_count = 1\n",
    "reduction_server_machine_type = \"n1-highcpu-16\"\n",
    "reduction_server_image_uri = (\n",
    "    \"us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest\"\n",
    ")\n",
    "\n",
    "PARAMETER_POOL = {\n",
    "    \"replica_count\": reduction_server_count,\n",
    "    \"machine_spec\": {\n",
    "        \"machine_type\": reduction_server_machine_type,\n",
    "    },\n",
    "    \"container_spec\": {\"image_uri\": reduction_server_image_uri},\n",
    "}\n",
    "WORKER_POOL_SPECS.append(PARAMETER_POOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8Av8ATVulvb"
   },
   "source": [
    "### Create CustomJob with worker pool specifications\n",
    "\n",
    "Next, you create a `CustomJob` for the multi-worker distributed training job:\n",
    "\n",
    "-`display_name`: The display name for the custom job.\n",
    "\n",
    "-`worker_pool_specs`: The detailed specifications for each worker pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "TUWEP1Lmulvb"
   },
   "outputs": [],
   "source": [
    "DISPLAY_NAME = \"boston_\" + TIMESTAMP\n",
    "\n",
    "job = aip.CustomJob(display_name=DISPLAY_NAME, worker_pool_specs=WORKER_POOL_SPECS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_95FH8jeulvb"
   },
   "source": [
    "### Run the CustomJob\n",
    "\n",
    "Next, you run the custom job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IEbrY05Gulvb",
    "outputId": "0538e906-bd15-4b06-a373-23ea0595ecbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating CustomJob\n",
      "CustomJob created. Resource name: projects/931647533046/locations/us-central1/customJobs/6954787628415713280\n",
      "To use this CustomJob in another session:\n",
      "custom_job = aiplatform.CustomJob.get('projects/931647533046/locations/us-central1/customJobs/6954787628415713280')\n",
      "View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/6954787628415713280?project=931647533046\n",
      "CustomJob projects/931647533046/locations/us-central1/customJobs/6954787628415713280 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/931647533046/locations/us-central1/customJobs/6954787628415713280 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/931647533046/locations/us-central1/customJobs/6954787628415713280 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/931647533046/locations/us-central1/customJobs/6954787628415713280 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/931647533046/locations/us-central1/customJobs/6954787628415713280 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/931647533046/locations/us-central1/customJobs/6954787628415713280 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/931647533046/locations/us-central1/customJobs/6954787628415713280 current state:\n",
      "JobState.JOB_STATE_FAILED\n",
      "Job failed with:\n",
      "code: 3\n",
      "message: \"The replica workerpool0-0 exited with a non-zero status of 127. Termination reason: ContainerCannotRun. \\n Termination log message: OCI runtime create failed: container_linux.go:380: starting container process caused: exec: \\\"t\\\": executable file not found in $PATH: unknown\\nTo find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=931647533046&resource=ml_job%2Fjob_id%2F6954787628415713280&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%226954787628415713280%22\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    job.run(sync=True)\n",
    "except Exception as e:\n",
    "    # may fail in multi-worker to find startup script\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8R2Bnmwmulvb"
   },
   "source": [
    "### Delete a custom training job\n",
    "\n",
    "After a training job is completed, you can delete the training job with the method `delete()`.  Prior to completion, a training job can be canceled with the method `cancel()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s1geVE3Lulvb",
    "outputId": "11ab9a67-471d-40d2-e291-3daad0c98c4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting CustomJob : projects/931647533046/locations/us-central1/customJobs/6954787628415713280\n",
      "Delete CustomJob  backing LRO: projects/931647533046/locations/us-central1/operations/2087328885808037888\n",
      "CustomJob deleted. . Resource name: projects/931647533046/locations/us-central1/customJobs/6954787628415713280\n"
     ]
    }
   ],
   "source": [
    "job.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tpu_intro"
   },
   "source": [
    "## Cloud TPU Training\n",
    "\n",
    "To further speed up trainig, your organization can utilize Google's Cloud Tensor Processing Units (TPU) pods.\n",
    "\n",
    "Cloud TPU is the custom-designed machine learning ASIC that powers Google products like Translate, Photos, Search, Assistant, and Gmail. Cloud TPU is designed to run cutting-edge machine learning models with AI services on Google Cloud. And its custom high-speed network offers over 100 petaflops of performance in a single pod.\n",
    "\n",
    "Learn more about [Cloud TPU](https://cloud.google.com/tpu)\n",
    "\n",
    "*Note*: TPU VM Training is currently an opt-in feature. Your GCP project must first be added to the feature allowlist. Please email your project information(project id/number) to vertex-ai-tpu-vm-training-support@google.com for the allowlist. You will receive an email as soon as your project is ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "docker_write:tpu"
   },
   "source": [
    "### Write Docker file for TPU training\n",
    "\n",
    "Currently, there is no pre-built Vertex AI Docker image for training with TPUs. No problems, you can make your own, as follows:\n",
    "\n",
    "1. Create a vanilla Python 3 image (e.g., `python3:8`).\n",
    "2. Get and install the TPU library (`libtpu.so`).\n",
    "3. Copy in your training package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nQVPtknpulvb",
    "outputId": "e1cbd053-b007-4e55-a907-2094e56b7a85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting custom/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom/Dockerfile\n",
    "FROM python:3.8\n",
    "\n",
    "WORKDIR /\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY trainer /trainer\n",
    "\n",
    "RUN pip3 install tensorflow-datasets\n",
    "\n",
    "# Install TPU Tensorflow and dependencies.\n",
    "# libtpu.so must be under the '/lib' directory.\n",
    "RUN wget https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/libtpu/20210525/libtpu.so -O /lib/libtpu.so\n",
    "RUN chmod 777 /lib/libtpu.so\n",
    "\n",
    "RUN wget https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/tensorflow/20210525/tf_nightly-2.6.0-cp38-cp38-linux_x86_64.whl\n",
    "RUN pip3 install tf_nightly-2.6.0-cp38-cp38-linux_x86_64.whl\n",
    "RUN rm tf_nightly-2.6.0-cp38-cp38-linux_x86_64.whl\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "docker_push:tpu"
   },
   "source": [
    "### Build and push the Docker image to the Artifact Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "J_d_zEXUulvc"
   },
   "outputs": [],
   "source": [
    "TRAIN_IMAGE = \"gcr.io/\" + PROJECT_ID + \"/tpu-train:latest\"\n",
    "\n",
    "os.chdir(\"custom\")\n",
    "\n",
    "if not IS_COLAB:\n",
    "    ! docker build --quiet --tag={TRAIN_IMAGE} .\n",
    "    ! docker push {TRAIN_IMAGE}\n",
    "else:\n",
    "    # install docker daemon\n",
    "    ! apt-get -qq install docker.io\n",
    "\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "worker_pool_tpu"
   },
   "source": [
    "### TPU worker specification pool\n",
    "\n",
    "Next, you create the worker specification pool. For TPUs, you do:\n",
    "\n",
    "- Create only one worker pool (Primary).\n",
    "- Set the machine type to `cloud-tpu`.\n",
    "- Set the accelerator type to a `TPU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d514eU7lulvc",
    "outputId": "f586785d-e355-4e0f-8cc7-95630bf4dc70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpu\n",
      "{'container_spec': {'args': ['--epochs=20', '--steps=10000', '--distribute=tpu'], 'image_uri': 'gcr.io/vertex-ai-dev/tpu-train:latest'}, 'replica_count': 1, 'machine_spec': {'machine_type': 'cloud-tpu', 'accelerator_type': 7, 'accelerator_count': 8}}\n"
     ]
    }
   ],
   "source": [
    "# Use TPU Accelerators. Temporarily using numeric codes, until types are added to the SDK\n",
    "#   6 = TPU_V2\n",
    "#   7 = TPU_V3\n",
    "TRAIN_TPU, TRAIN_NTPU = (7, 8)\n",
    "TRAIN_COMPUTE = \"cloud-tpu\"\n",
    "\n",
    "\n",
    "if not TRAIN_NTPU or TRAIN_NTPU < 2:\n",
    "    TRAIN_STRATEGY = \"single\"\n",
    "else:\n",
    "    TRAIN_STRATEGY = \"tpu\"\n",
    "print(TRAIN_STRATEGY)\n",
    "\n",
    "EPOCHS = 20\n",
    "STEPS = 10000\n",
    "\n",
    "TRAINER_ARGS = [\n",
    "    \"--epochs=\" + str(EPOCHS),\n",
    "    \"--steps=\" + str(STEPS),\n",
    "    \"--distribute=\" + TRAIN_STRATEGY,\n",
    "]\n",
    "\n",
    "\n",
    "WORKER_POOL_SPECS = [\n",
    "    {\n",
    "        \"container_spec\": {\n",
    "            \"args\": TRAINER_ARGS,\n",
    "            \"image_uri\": TRAIN_IMAGE,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": TRAIN_COMPUTE,\n",
    "            \"accelerator_type\": TRAIN_TPU,\n",
    "            \"accelerator_count\": TRAIN_NTPU,\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "print(WORKER_POOL_SPECS[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RruSqNfrulvc"
   },
   "source": [
    "### Create CustomJob with worker pool specifications\n",
    "\n",
    "Next, you create a `CustomJob` for the multi-worker distributed training job:\n",
    "\n",
    "-`display_name`: The display name for the custom job.\n",
    "\n",
    "-`worker_pool_specs`: The detailed specifications for each worker pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "2QvSqbbHulvc"
   },
   "outputs": [],
   "source": [
    "DISPLAY_NAME = \"boston_\" + TIMESTAMP\n",
    "\n",
    "job = aip.CustomJob(display_name=DISPLAY_NAME, worker_pool_specs=WORKER_POOL_SPECS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iw4L3UIfulvd"
   },
   "source": [
    "### Run the CustomJob\n",
    "\n",
    "Next, you run the custom job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zmqCNS78ulvd",
    "outputId": "8755599a-b70b-4cb6-b5c8-f6d1360a74d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating CustomJob\n",
      "CustomJob created. Resource name: projects/931647533046/locations/us-central1/customJobs/7315075598605352960\n",
      "To use this CustomJob in another session:\n",
      "custom_job = aiplatform.CustomJob.get('projects/931647533046/locations/us-central1/customJobs/7315075598605352960')\n",
      "View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/7315075598605352960?project=931647533046\n",
      "CustomJob projects/931647533046/locations/us-central1/customJobs/7315075598605352960 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/931647533046/locations/us-central1/customJobs/7315075598605352960 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/931647533046/locations/us-central1/customJobs/7315075598605352960 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/931647533046/locations/us-central1/customJobs/7315075598605352960 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/931647533046/locations/us-central1/customJobs/7315075598605352960 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/931647533046/locations/us-central1/customJobs/7315075598605352960 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "CustomJob run completed. Resource name: projects/931647533046/locations/us-central1/customJobs/7315075598605352960\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    job.run(sync=True)\n",
    "except Exception as e:\n",
    "    # may fail in multi-worker to find startup script\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWZoH9QKulvd"
   },
   "source": [
    "### Delete a custom training job\n",
    "\n",
    "After a training job is completed, you can delete the training job with the method `delete()`.  Prior to completion, a training job can be canceled with the method `cancel()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lt8BJ4iBulvd",
    "outputId": "5c4b5b60-3ace-477d-96ed-d2ea056fe169"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting CustomJob : projects/931647533046/locations/us-central1/customJobs/7315075598605352960\n",
      "Delete CustomJob  backing LRO: projects/931647533046/locations/us-central1/operations/411989824426213376\n",
      "CustomJob deleted. . Resource name: projects/931647533046/locations/us-central1/customJobs/7315075598605352960\n"
     ]
    }
   ],
   "source": [
    "job.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup"
   },
   "source": [
    "# Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n",
    "\n",
    "- Cloud Storage Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "U98Wzc01ulvd"
   },
   "outputs": [],
   "source": [
    "# Set this to true only if you'd like to delete your bucket\n",
    "delete_bucket = False\n",
    "\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "get_started_vertex_distributed_training.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

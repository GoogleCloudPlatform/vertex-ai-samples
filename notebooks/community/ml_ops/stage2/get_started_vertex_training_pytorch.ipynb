{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title:generic,gcp"
   },
   "source": [
    "# E2E ML on GCP: MLOps stage 2 : experimentation: get started with Vertex Training for Pytorch\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage2/get_started_vertex_training_pytorch.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:mlops"
   },
   "source": [
    "## Overview\n",
    "\n",
    "\n",
    "This tutorial demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. This tutorial covers stage 2 : experimentation: get started with Vertex Training for Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:pytorch,cifar10,icn"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset used for this tutorial is the [CIFAR10 dataset](https://pytorch.org/vision/stable/datasets.html#cifar) from [Pytorch Datasets](https://pytorch.org/vision/stable/datasets.html). The version of the dataset you will use is built into TensorFlow. The trained model predicts which type of class an image is from ten classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, or truck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:mlops,stage2,get_started_vertex_training_pytorch"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you learn how to use `Vertex AI Training` for training a Pytorch custom model.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services:\n",
    "\n",
    "* `Vertex AI Training`\n",
    "* `Vertex AI Model` resource\n",
    "\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Single node training using a Python package.\n",
    "- Report accuracy when hyperparameter tuning.\n",
    "- Save the model artifacts to Cloud Storage using GCSFuse.\n",
    "- Create a `Vertex AI Model` resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Costs \n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up your local development environment\n",
    "\n",
    "**If you are using Colab or Google Cloud Notebooks**, your environment already meets\n",
    "all the requirements to run this notebook. You can skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
    "You need the following:\n",
    "\n",
    "* The Google Cloud SDK\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* Jupyter notebook running in a virtual environment with Python 3\n",
    "\n",
    "The Google Cloud guide to [Setting up a Python development\n",
    "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
    "installation guide](https://jupyter.org/install) provide detailed instructions\n",
    "for meeting these requirements. The following steps provide a condensed set of\n",
    "instructions:\n",
    "\n",
    "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "1. [Install\n",
    "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
    "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
    "\n",
    "1. To install Jupyter, run `pip3 install jupyter` on the\n",
    "command-line in a terminal shell.\n",
    "\n",
    "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
    "\n",
    "1. Open this notebook in the Jupyter Notebook Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_mlops"
   },
   "source": [
    "## Installations\n",
    "\n",
    "Install *one time* the packages for executing the MLOps notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "install_mlops"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform[tensorboard] in /usr/local/lib/python3.9/site-packages (1.11.0)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.9/site-packages (from google-cloud-aiplatform[tensorboard]) (2.2.1)\n",
      "Requirement already satisfied: proto-plus>=1.15.0 in /usr/local/lib/python3.9/site-packages (from google-cloud-aiplatform[tensorboard]) (1.20.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.9/site-packages (from google-cloud-aiplatform[tensorboard]) (21.3)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.9/site-packages (from google-cloud-aiplatform[tensorboard]) (2.7.1)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /usr/local/lib/python3.9/site-packages (from google-cloud-aiplatform[tensorboard]) (2.34.2)\n",
      "Collecting tensorflow<=2.7.0,>=2.3.0\n",
      "  Downloading tensorflow-2.7.0-cp39-cp39-manylinux2010_x86_64.whl (489.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 489.7 MB 5.3 kB/s  eta 0:00:01     |██████████████▌                 | 221.8 MB 61.2 MB/s eta 0:00:05MB/s eta 0:00:03     |████████████████████            | 306.1 MB 26.0 MB/s eta 0:00:08██████████████▉           | 318.9 MB 26.0 MB/s eta 0:00:07�█▎ | 463.7 MB 47.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (3.20.0rc2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in /usr/local/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (1.56.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (2.27.1)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (2.6.2)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (1.44.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (1.44.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (1.15.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (5.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (4.8)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /usr/local/lib/python3.9/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform[tensorboard]) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.9/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform[tensorboard]) (2.8.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.9/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform[tensorboard]) (2.3.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.9/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform[tensorboard]) (1.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/site-packages (from packaging>=14.3->google-cloud-aiplatform[tensorboard]) (3.0.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform[tensorboard]) (2021.10.8)\n",
      "Collecting keras<2.8,>=2.7.0rc0\n",
      "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 56.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast<0.5.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (1.22.3)\n",
      "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
      "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
      "\u001b[K     |████████████████████████████████| 463 kB 70.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 4.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting h5py>=2.9.0\n",
      "  Downloading h5py-3.6.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.5 MB 43.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.14.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 7.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard~=2.6\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 46.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting flatbuffers<3.0,>=1.12\n",
      "  Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.9/site-packages (from tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (0.36.2)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 6.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting typing-extensions>=3.6.6\n",
      "  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.21.0\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.24.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 49.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting libclang>=9.0.1\n",
      "  Downloading libclang-13.0.0-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.5 MB 23.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.4.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "\u001b[K     |████████████████████████████████| 126 kB 71.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 44.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 58.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow<=2.7.0,>=2.3.0->google-cloud-aiplatform[tensorboard]) (54.2.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 8.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.0.3-py3-none-any.whl (289 kB)\n",
      "\u001b[K     |████████████████████████████████| 289 kB 48.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Downloading importlib_metadata-4.11.3-py3-none-any.whl (18 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.7.0-py3-none-any.whl (5.3 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "\u001b[K     |████████████████████████████████| 151 kB 49.2 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=2bda752bbaafaf78c068f4c72e9cdba5d73996e594966678ad447a454a02b44b\n",
      "  Stored in directory: /root/.cache/pip/wheels/b6/0d/90/0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\n",
      "Successfully built termcolor\n",
      "Installing collected packages: zipp, oauthlib, requests-oauthlib, importlib-metadata, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, markdown, google-auth-oauthlib, absl-py, wrapt, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 flatbuffers-2.0 gast-0.4.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 h5py-3.6.0 importlib-metadata-4.11.3 keras-2.7.0 keras-preprocessing-1.1.2 libclang-13.0.0 markdown-3.3.6 oauthlib-3.2.0 opt-einsum-3.3.0 requests-oauthlib-1.3.1 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.7.0 tensorflow-estimator-2.7.0 tensorflow-io-gcs-filesystem-0.24.0 termcolor-1.1.0 typing-extensions-4.1.1 werkzeug-2.0.3 wrapt-1.14.0 zipp-3.7.0\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting google-cloud-pipeline-components\n",
      "  Downloading google_cloud_pipeline_components-1.0.1-py3-none-any.whl (372 kB)\n",
      "\u001b[K     |████████████████████████████████| 372 kB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting kfp<2.0.0,>=1.8.9\n",
      "  Downloading kfp-1.8.11.tar.gz (298 kB)\n",
      "\u001b[K     |████████████████████████████████| 298 kB 49.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-cloud-aiplatform>=1.4.3 in /usr/local/lib/python3.9/site-packages (from google-cloud-pipeline-components) (1.11.0)\n",
      "Collecting google-cloud-notebooks>=0.4.0\n",
      "  Downloading google_cloud_notebooks-1.2.1-py2.py3-none-any.whl (343 kB)\n",
      "\u001b[K     |████████████████████████████████| 343 kB 39.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.9/site-packages (from google-cloud-pipeline-components) (2.7.1)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.9/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (2.27.1)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.9/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (3.20.0rc2)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.9/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (2.6.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in /usr/local/lib/python3.9/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (1.56.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (5.0.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (1.15.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (0.2.8)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.9/site-packages (from google-cloud-aiplatform>=1.4.3->google-cloud-pipeline-components) (2.2.1)\n",
      "Requirement already satisfied: proto-plus>=1.15.0 in /usr/local/lib/python3.9/site-packages (from google-cloud-aiplatform>=1.4.3->google-cloud-pipeline-components) (1.20.3)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /usr/local/lib/python3.9/site-packages (from google-cloud-aiplatform>=1.4.3->google-cloud-pipeline-components) (2.34.2)\n",
      "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.9/site-packages (from google-cloud-aiplatform>=1.4.3->google-cloud-pipeline-components) (21.3)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.9/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (1.44.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.9/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (1.44.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.9/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.4.3->google-cloud-pipeline-components) (2.8.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.9/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.4.3->google-cloud-pipeline-components) (2.3.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /usr/local/lib/python3.9/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.4.3->google-cloud-pipeline-components) (2.2.3)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.9/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.4.3->google-cloud-pipeline-components) (1.3.0)\n",
      "Requirement already satisfied: absl-py<2,>=0.9 in /usr/local/lib/python3.9/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (1.0.0)\n",
      "Collecting PyYAML<6,>=5.3\n",
      "  Downloading PyYAML-5.4.1-cp39-cp39-manylinux1_x86_64.whl (630 kB)\n",
      "\u001b[K     |████████████████████████████████| 630 kB 45.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-storage<3.0.0dev,>=1.32.0\n",
      "  Downloading google_cloud_storage-1.44.0-py2.py3-none-any.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 52.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting kubernetes<19,>=8.0.0\n",
      "  Downloading kubernetes-18.20.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 44.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-api-python-client<2,>=1.7.8\n",
      "  Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 866 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<3.0dev,>=1.25.0\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "\u001b[K     |████████████████████████████████| 152 kB 44.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests-toolbelt<1,>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 3.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting cloudpickle<3,>=2.0.0\n",
      "  Downloading cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
      "Collecting kfp-server-api<2.0.0,>=1.1.2\n",
      "  Downloading kfp-server-api-1.8.1.tar.gz (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 3.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting jsonschema<4,>=3.0.1\n",
      "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 6.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tabulate<1,>=0.8.6 in /usr/local/lib/python3.9/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (0.8.9)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /usr/local/lib/python3.9/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (8.0.4)\n",
      "Collecting Deprecated<2,>=1.2.7\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting strip-hints<1,>=0.1.8\n",
      "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "Collecting docstring-parser<1,>=0.7.3\n",
      "  Downloading docstring_parser-0.13.tar.gz (23 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting kfp-pipeline-spec<0.2.0,>=0.1.13\n",
      "  Downloading kfp_pipeline_spec-0.1.13-py3-none-any.whl (18 kB)\n",
      "Collecting fire<1,>=0.3.1\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "\u001b[K     |████████████████████████████████| 87 kB 8.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting uritemplate<4,>=3.0.1\n",
      "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Collecting pydantic<2,>=1.8.2\n",
      "  Downloading pydantic-1.9.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.2 MB 37.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typer<1.0,>=0.3.2\n",
      "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.9/site-packages (from Deprecated<2,>=1.2.7->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (1.14.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.9/site-packages (from fire<1,>=0.3.1->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (1.1.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.9/site-packages (from google-api-python-client<2,>=1.7.8->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (0.20.4)\n",
      "Collecting google-auth-httplib2>=0.0.3\n",
      "  Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (54.2.0)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.9/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client<2,>=1.7.8->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (3.0.7)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/site-packages (from jsonschema<4,>=3.0.1->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (21.4.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.9/site-packages (from jsonschema<4,>=3.0.1->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (0.18.1)\n",
      "Requirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.9/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (1.26.9)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.9/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (2021.10.8)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.9/site-packages (from kubernetes<19,>=8.0.0->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (1.3.1)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.9/site-packages (from kubernetes<19,>=8.0.0->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (0.4.8)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/site-packages (from pydantic<2,>=1.8.2->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (4.1.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (3.3)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.9/site-packages (from strip-hints<1,>=0.1.8->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (0.36.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/site-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (3.2.0)\n",
      "Building wheels for collected packages: kfp, docstring-parser, fire, kfp-server-api, strip-hints\n",
      "  Building wheel for kfp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp: filename=kfp-1.8.11-py3-none-any.whl size=414450 sha256=69ddb1fdf3925f5b075ffa8151fddc41b4ae7b04685038d30cca9d97d113d4f3\n",
      "  Stored in directory: /root/.cache/pip/wheels/26/18/fa/9d382e244ad485d45fde65bfd896b9e4d2aaf2e44424767dc4\n",
      "  Building wheel for docstring-parser (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docstring-parser: filename=docstring_parser-0.13-py3-none-any.whl size=31866 sha256=1f426b5403d06a7b25521e93e8870a19cf9b386b88272776e33fbab4f6af4fed\n",
      "  Stored in directory: /root/.cache/pip/wheels/2d/55/01/b0e85099297d07c97ef2c7dcb025dcd6d62b477395019a408e\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115928 sha256=7a7505ffe21514847b60551c7d7411fff9f2bd0a216cc2d8268572e2ecdfa94f\n",
      "  Stored in directory: /root/.cache/pip/wheels/2a/93/86/8cd17bc6c40fb605c3ac549d0b860ef7e84ee5f67bf01a3287\n",
      "  Building wheel for kfp-server-api (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp-server-api: filename=kfp_server_api-1.8.1-py3-none-any.whl size=95548 sha256=1e1a5af1626410f0edb24260004c9448dc68d9129ce1daaacd0429390a8868a5\n",
      "  Stored in directory: /root/.cache/pip/wheels/9e/7f/2b/78d9aa8afb27e5398c21b695b5b253a9d089a6043a0c6e962f\n",
      "  Building wheel for strip-hints (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22279 sha256=9f1cc0fbf2223f438b4ff1b497b44f26e7baf9a75d8e2d9cd7e0f59f2f306c47\n",
      "  Stored in directory: /root/.cache/pip/wheels/eb/82/45/f4240f75b8cd041477e71032ea28652c913c0323f479032018\n",
      "Successfully built kfp docstring-parser fire kfp-server-api strip-hints\n",
      "Installing collected packages: cachetools, google-auth, uritemplate, PyYAML, google-auth-httplib2, typer, strip-hints, requests-toolbelt, pydantic, kubernetes, kfp-server-api, kfp-pipeline-spec, jsonschema, google-cloud-storage, google-api-python-client, fire, docstring-parser, Deprecated, cloudpickle, kfp, google-cloud-notebooks, google-cloud-pipeline-components\n",
      "  Attempting uninstall: cachetools\n",
      "    Found existing installation: cachetools 5.0.0\n",
      "    Uninstalling cachetools-5.0.0:\n",
      "      Successfully uninstalled cachetools-5.0.0\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.6.2\n",
      "    Uninstalling google-auth-2.6.2:\n",
      "      Successfully uninstalled google-auth-2.6.2\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n",
      "      Successfully uninstalled PyYAML-6.0\n",
      "  Attempting uninstall: jsonschema\n",
      "    Found existing installation: jsonschema 4.4.0\n",
      "    Uninstalling jsonschema-4.4.0:\n",
      "      Successfully uninstalled jsonschema-4.4.0\n",
      "  Attempting uninstall: google-cloud-storage\n",
      "    Found existing installation: google-cloud-storage 2.2.1\n",
      "    Uninstalling google-cloud-storage-2.2.1:\n",
      "      Successfully uninstalled google-cloud-storage-2.2.1\n",
      "Successfully installed Deprecated-1.2.13 PyYAML-5.4.1 cachetools-4.2.4 cloudpickle-2.0.0 docstring-parser-0.13 fire-0.4.0 google-api-python-client-1.12.11 google-auth-1.35.0 google-auth-httplib2-0.1.0 google-cloud-notebooks-1.2.1 google-cloud-pipeline-components-1.0.1 google-cloud-storage-1.44.0 jsonschema-3.2.0 kfp-1.8.11 kfp-pipeline-spec-0.1.13 kfp-server-api-1.8.1 kubernetes-18.20.0 pydantic-1.9.0 requests-toolbelt-0.9.1 strip-hints-0.1.10 typer-0.4.0 uritemplate-3.0.1\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Building wheels for collected packages: cloudml-hypertune\n",
      "  Building wheel for cloudml-hypertune (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3988 sha256=abf86ebe223fb4808354b3394cf6fe0160362d7f23f27a5304c1f7f7ff04ed1e\n",
      "  Stored in directory: /root/.cache/pip/wheels/92/28/87/5df079d0246bff34428d680be16fec0b3862b004893465228d\n",
      "Successfully built cloudml-hypertune\n",
      "Installing collected packages: cloudml-hypertune\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.12.0-cp39-cp39-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 21.0 MB 5.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/site-packages (from torchvision) (4.1.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/site-packages (from torchvision) (1.22.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/site-packages (from torchvision) (2.27.1)\n",
      "Collecting torch==1.11.0\n",
      "  Downloading torch-1.11.0-cp39-cp39-manylinux1_x86_64.whl (750.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 750.6 MB 3.2 kB/s  eta 0:00:01     |██████████████▏                 | 333.4 MB 47.1 MB/s eta 0:00:090:06██████████████▊           | 486.8 MB 46.9 MB/s eta 0:00:06        | 540.9 MB 59.8 MB/s eta 0:00:04     |████████████████████████        | 563.3 MB 32.0 MB/s eta 0:00:06     |██████████████████████████████▍ | 712.9 MB 30.8 MB/s eta 0:00:02��███████████▍| 737.2 MB 30.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/site-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests->torchvision) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/site-packages (from requests->torchvision) (2.0.12)\n",
      "Installing collected packages: torch, torchvision\n",
      "Successfully installed torch-1.11.0 torchvision-0.12.0\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install --upgrade google-cloud-aiplatform[tensorboard] $USER_FLAG\n",
    "! pip3 install --upgrade google-cloud-pipeline-components $USER_FLAG\n",
    "! pip3 install --upgrade cloudml-hypertune $USER_FLAG\n",
    "! pip3 install --upgrade torchvision $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "restart"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_id"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "autoset_project_id"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: vertex-ai-dev\n"
     ]
    }
   ],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "\n",
      "\n",
      "Updates are available for some Cloud SDK components.  To install them,\n",
      "please run:\n",
      "  $ gcloud components update\n",
      "\n",
      "\n",
      "\n",
      "To take a quick anonymous survey, run:\n",
      "  $ gcloud survey\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
    "\n",
    "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "timestamp"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "timestamp"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "When you initialize the Vertex SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "autoset_bucket"
   },
   "outputs": [],
   "source": [
    "if BUCKET_URI == \"\" or BUCKET_URI is None or BUCKET_URI == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_URI = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "create_bucket"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://vertex-ai-devaip-20220324060506/...\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validate_bucket"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "validate_bucket"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### Set up variables\n",
    "\n",
    "Next, set up some variables used throughout the tutorial.\n",
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "accelerators:training,mbsdk"
   },
   "source": [
    "#### Set hardware accelerators\n",
    "\n",
    "You can set hardware accelerators for training.\n",
    "\n",
    "Set the variable `TRAIN_GPU/TRAIN_NGPU` to use a container image supporting a GPU and the number of GPUs allocated to the virtual machine (VM) instance. For example, to use a GPU container image with 4 Nvidia Telsa K80 GPUs allocated to each VM, you would specify:\n",
    "\n",
    "    (aip.AcceleratorType.NVIDIA_TESLA_K80, 4)\n",
    "\n",
    "Otherwise specify `(None, None)` to use a container image to run on a CPU.\n",
    "\n",
    "Learn more [here](https://cloud.google.com/vertex-ai/docs/general/locations#accelerators) hardware accelerator support for your region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "accelerators:training,mbsdk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if os.getenv(\"IS_TESTING_TRAIN_GPU\"):\n",
    "    TRAIN_GPU, TRAIN_NGPU = (\n",
    "        aiplatform.gapic.AcceleratorType.NVIDIA_TESLA_K80,\n",
    "        int(os.getenv(\"IS_TESTING_TRAIN_GPU\")),\n",
    "    )\n",
    "else:\n",
    "    TRAIN_GPU, TRAIN_NGPU = (aiplatform.gapic.AcceleratorType.NVIDIA_TESLA_K80, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "container:training,pytorch"
   },
   "source": [
    "#### Set pre-built containers\n",
    "\n",
    "Set the pre-built Docker container image for training.\n",
    "\n",
    "- Set the variable `TF` to the TensorFlow version of the container image. For example, `2-1` would be version 2.1, and `1-15` would be version 1.15. The following list shows some of the pre-built images available:\n",
    "\n",
    "\n",
    "For the latest list, see [Pre-built containers for training](https://cloud.google.com/ai-platform-unified/docs/training/pre-built-containers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "container:training,pytorch"
   },
   "outputs": [],
   "source": [
    "if TRAIN_GPU:\n",
    "    TRAIN_VERSION = \"pytorch-gpu.1-9\"\n",
    "else:\n",
    "    TRAIN_VERSION = \"pytorch-xla.1-9\"\n",
    "\n",
    "TRAIN_IMAGE = \"{}-docker.pkg.dev/vertex-ai/training/{}:latest\".format(\n",
    "    REGION.split(\"-\")[0], TRAIN_VERSION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "machine:training"
   },
   "source": [
    "#### Set machine type\n",
    "\n",
    "Next, set the machine type to use for training.\n",
    "\n",
    "- Set the variable `TRAIN_COMPUTE` to configure  the compute resources for the VMs you will use for for training.\n",
    " - `machine type`\n",
    "     - `n1-standard`: 3.75GB of memory per vCPU.\n",
    "     - `n1-highmem`: 6.5GB of memory per vCPU\n",
    "     - `n1-highcpu`: 0.9 GB of memory per vCPU\n",
    " - `vCPUs`: number of \\[2, 4, 8, 16, 32, 64, 96 \\]\n",
    "\n",
    "*Note: The following is not supported for training:*\n",
    "\n",
    " - `standard`: 2 vCPUs\n",
    " - `highcpu`: 2, 4 and 8 vCPUs\n",
    "\n",
    "*Note: You may also use n2 and e2 machine types for training and deployment, but they do not support GPUs*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "machine:training"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train machine type n1-standard-4\n"
     ]
    }
   ],
   "source": [
    "if os.getenv(\"IS_TESTING_TRAIN_MACHINE\"):\n",
    "    MACHINE_TYPE = os.getenv(\"IS_TESTING_TRAIN_MACHINE\")\n",
    "else:\n",
    "    MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Train machine type\", TRAIN_COMPUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pytorch_intro"
   },
   "source": [
    "## Introduction to Pytorch training\n",
    "\n",
    "The Pytorch package supports both single node and distributed model training.\n",
    "\n",
    "Once you have trained a Pytorch model, you will want to save it at a Cloud Storage location, so it can subsequently be uploaded to a `Vertex AI Model` resource.\n",
    "The Pytorch package does not have support to save the model to a Cloud Storage location. Instead, you will do the following steps to save to a Cloud Storage location.\n",
    "\n",
    "1. Save the in-memory model to the local filesystem (e.g., model.pth).\n",
    "2. Use gsutil to copy the local copy to the specified Cloud Storage location.\n",
    "\n",
    "*Note*: You can do hyperparameter tuning with a Pytorch model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "examine_training_package:pytorch"
   },
   "source": [
    "### Examine the training package\n",
    "\n",
    "#### Package layout\n",
    "\n",
    "Before you start the training, you will look at how a Python package is assembled for a custom training job. When unarchived, the package contains the following directory/file layout.\n",
    "\n",
    "- PKG-INFO\n",
    "- README.md\n",
    "- setup.cfg\n",
    "- setup.py\n",
    "- trainer\n",
    "  - \\_\\_init\\_\\_.py\n",
    "  - task.py\n",
    "\n",
    "The files `setup.cfg` and `setup.py` are the instructions for installing the package into the operating environment of the Docker image.\n",
    "\n",
    "The file `trainer/task.py` is the Python script for executing the custom training job. *Note*, when we referred to it in the worker pool specification, we replace the directory slash with a dot (`trainer.task`) and dropped the file suffix (`.py`).\n",
    "\n",
    "#### Package Assembly\n",
    "\n",
    "In the following cells, you will assemble the training package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "examine_training_package:pytorch"
   },
   "outputs": [],
   "source": [
    "# Make folder for Python training script\n",
    "! rm -rf custom\n",
    "! mkdir custom\n",
    "\n",
    "# Add package information\n",
    "! touch custom/README.md\n",
    "\n",
    "#Instructions for installing package into environment of the docker image\n",
    "setup_cfg = \"[egg_info]\\n\\ntag_build =\\n\\ntag_date = 0\"\n",
    "! echo \"$setup_cfg\" > custom/setup.cfg\n",
    "\n",
    "setup_py = \"import setuptools\\n\\nsetuptools.setup(\\n\\n    install_requires=[\\n\\n        'cloudml-hypertune',\\n\\n    ],\\n\\n    packages=setuptools.find_packages())\"\n",
    "! echo \"$setup_py\" > custom/setup.py\n",
    "\n",
    "pkg_info = \"Metadata-Version: 1.0\\n\\nName: CIFAR10 image classification\\n\\nVersion: 0.0.0\\n\\nSummary: Demostration training script\\n\\nHome-page: www.google.com\\n\\nAuthor: Google\\n\\nAuthor-email: aferlitsch@google.com\\n\\nLicense: Public\\n\\nDescription: Demo\\n\\nPlatform: Vertex\"\n",
    "! echo \"$pkg_info\" > custom/PKG-INFO\n",
    "\n",
    "# Make the training subfolder\n",
    "! mkdir custom/trainer\n",
    "! touch custom/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taskpy_contents:cifar10,pytorch"
   },
   "source": [
    "### Create the task script for the Python training package\n",
    "\n",
    "Next, you create the `task.py` script for driving the training package. Some noteable steps include:\n",
    "\n",
    "- Command-line arguments:\n",
    "    - `model-dir`: The location to save the trained model. When using Vertex AI custom training, the location will be specified in the environment variable: `AIP_MODEL_DIR`,\n",
    "    - `batch_size`/`lr` : Hyperparameter tuning variables\n",
    "    - `distribute`: single node or distributed training.\n",
    "- Data preprocessing (`get_data()`):\n",
    "    - Download the dataset and split into training and test.\n",
    "- Model architecture (`getmodel()`):\n",
    "    - Get or build the model architecture.\n",
    "- Training (`train_model()`):\n",
    "    - Trains the model\n",
    "- Evaluation (`evaluate_model()`):\n",
    "    - Evaluates the model.\n",
    "    - If hyperparameter tuning, reports the metric for accuracy.\n",
    "- Model artifact saving\n",
    "    - Saves the model artifacts and evaluation metrics where the Cloud Storage location specified by `model-dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "taskpy_contents:cifar10,pytorch"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom/trainer/task.py\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import hypertune\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "#import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as distributed\n",
    "#import torch.optim\n",
    "#import torch.multiprocessing as mp\n",
    "import torch.utils.data as data\n",
    "#import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch ImageNet Training')\n",
    "parser.add_argument('--model-dir', dest='model_dir',\n",
    "                    default=os.getenv('AIP_MODEL_DIR'), type=str, help='Model dir.')\n",
    "parser.add_argument('--batch_size', dest='batch_size',\n",
    "                    type=int, default=16, help='Batch size')\n",
    "parser.add_argument('--epochs', dest='epochs',\n",
    "                    type=int, default=20, help='Number of epochs')\n",
    "parser.add_argument('--lr', dest='lr',\n",
    "                    type=int, default=20, help='Learning rate')\n",
    "parser.add_argument('--distribute', default=\"single\",\n",
    "                    type=str, help='Distributed training strategy')\n",
    "parser.add_argument('--checkpoints', default=False,\n",
    "                    type=bool, help='Whether to save checkpoints')\n",
    "args = parser.parse_args()\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "def distributed_is_initialized():\n",
    "    if args.distribute == \"mirror\":\n",
    "        if distributed.is_available() and distributed.is_initialized():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_data():\n",
    "\n",
    "    normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), normalize,]\n",
    "    )\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(root=\"./train\", transform=transform, train=True, download=True)\n",
    "    logging.info(train_dataset)\n",
    "    if distributed_is_initialized():\n",
    "        sampler = data.DistributedSampler(train_dataset)\n",
    "    else:\n",
    "        sampler = None\n",
    "    train_loader = data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=(sampler is None),\n",
    "        sampler=sampler,\n",
    "    )\n",
    "\n",
    "    test_dataset = datasets.CIFAR10(root=\"./test\", transform=transform, train=False, download=True)\n",
    "    logging.info(test_dataset)\n",
    "    sampler = None\n",
    "    test_loader = data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def get_model():\n",
    "    class Cifar10Model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.conv1 = nn.Conv2d(3, 16, 3)\n",
    "            self.pool = nn.MaxPool2d(2, 2)\n",
    "            self.conv2 = nn.Conv2d(16, 16, 5)\n",
    "            self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "            self.fc2 = nn.Linear(120, 84)\n",
    "            self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.pool(F.relu(self.conv1(x)))\n",
    "            x = self.pool(F.relu(self.conv2(x)))\n",
    "            x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "\n",
    "    logging.info(\"Get model architecture\")\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    gpu_id = \"0\" if torch.cuda.is_available() else None\n",
    "    logging.info(f\"Device: {device}\")\n",
    "\n",
    "    model = Cifar10Model()\n",
    "    model.to(device)\n",
    "\n",
    "    if distributed_is_initialized():\n",
    "        model = DistributedDataParallel(model)\n",
    "\n",
    "    loss = nn.CrossEntropyLoss().cuda(gpu_id)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    return model, loss, optimizer, device\n",
    "\n",
    "def train_model(model, loss_func, optimizer, train_loader, test_loader, is_chief, device):\n",
    "\n",
    "    class Average(object):\n",
    "        def __init__(self):\n",
    "            self.sum = 0\n",
    "            self.count = 0\n",
    "\n",
    "        def __str__(self):\n",
    "            return '{:.6f}'.format(self.average)\n",
    "\n",
    "        @property\n",
    "        def average(self):\n",
    "            return self.sum / self.count\n",
    "\n",
    "        def update(self, value, number):\n",
    "            self.sum += value * number\n",
    "            self.count += number\n",
    "\n",
    "    class Accuracy(object):\n",
    "        def __init__(self):\n",
    "            self.correct = 0\n",
    "            self.count = 0\n",
    "\n",
    "        def __str__(self):\n",
    "            return '{:.2f}%'.format(self.accuracy * 100)\n",
    "\n",
    "        @property\n",
    "        def accuracy(self):\n",
    "            return self.correct / self.count\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def update(self, output, target):\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct = pred.eq(target).sum().item()\n",
    "\n",
    "            self.correct += correct\n",
    "            self.count += output.size(0)\n",
    "\n",
    "    def train():\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        train_loss = Average()\n",
    "        train_acc = Accuracy()\n",
    "\n",
    "        for data, target in train_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            loss = loss_func(output, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.update(loss.item(), data.size(0))\n",
    "            train_acc.update(output, target)\n",
    "\n",
    "            return train_loss, train_acc\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(epoch):\n",
    "        model.eval()\n",
    "\n",
    "        test_loss = Average()\n",
    "        test_acc = Accuracy()\n",
    "\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "        loss = loss_func(output, target)\n",
    "\n",
    "        test_loss.update(loss.item(), data.size(0))\n",
    "        test_acc.update(output, target)\n",
    "\n",
    "        # report metric for hyperparameter tuning\n",
    "        hpt = hypertune.HyperTune()\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "            hyperparameter_metric_tag='accuracy',\n",
    "            metric_value=test_acc.accuracy,\n",
    "            global_step=epoch\n",
    "        )\n",
    "\n",
    "        return test_loss, test_acc\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "\n",
    "        logging.info('Epoch: {}, Training ...'.format(epoch))\n",
    "        train_loss, train_acc = train()\n",
    "\n",
    "        if is_chief:\n",
    "            test_loss, test_acc = evaluate(epoch)\n",
    "\n",
    "            if args.checkpoints:\n",
    "                torch.save(model.state_dict(), args.model_dir + f\"/{epoch}.chkpt\")\n",
    "\n",
    "            logging.info('Epoch: {}/{},'.format(epoch, args.epochs))\n",
    "            logging.info('train loss: {}, train acc: {},'.format(train_loss, train_acc))\n",
    "            logging.info('test loss: {}, test acc: {}.'.format(test_loss, test_acc))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "train_dataset, test_dataset = get_data()\n",
    "model, loss, optimizer, device = get_model()\n",
    "train_model(model, loss, optimizer, train_dataset, test_dataset, True, device)\n",
    "\n",
    "logging.info('start saving')\n",
    "# export model to gcs using GCSFuse\n",
    "logging.info(\"Exporting model artifacts ...\")\n",
    "gs_prefix = 'gs://'\n",
    "gcsfuse_prefix = '/gcs/'\n",
    "if args.model_dir.startswith(gs_prefix):\n",
    "    args.model_dir = args.model_dir.replace(gs_prefix, gcsfuse_prefix)\n",
    "    dirpath = os.path.split(args.model_dir)[0]\n",
    "    if not os.path.isdir(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "\n",
    "gcs_model_path = os.path.join(os.path.join(args.model_dir, 'model.pth'))\n",
    "torch.save(model.state_dict(), gcs_model_path)\n",
    "logging.info(f'Model is saved to {args.model_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test_package_locally"
   },
   "source": [
    "### Test training package locally\n",
    "\n",
    "Next, test your completed training package locally with just a few epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "test_package_locally,pytorch"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./train/cifar-10-python.tar.gz\n",
      "170499072it [00:03, 46602683.76it/s]                                            \n",
      "Extracting ./train/cifar-10-python.tar.gz to ./train\n",
      "INFO:root:Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./train\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
      "           )\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./test/cifar-10-python.tar.gz\n",
      "170499072it [00:03, 55817665.92it/s]                                            \n",
      "Extracting ./test/cifar-10-python.tar.gz to ./test\n",
      "INFO:root:Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./test\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
      "           )\n",
      "INFO:root:Get model architecture\n",
      "INFO:root:Device: cpu\n",
      "INFO:root:Epoch: 1, Training ...\n",
      "INFO:root:Epoch: 1/20,\n",
      "INFO:root:train loss: 2.312202, train acc: 6.25%,\n",
      "INFO:root:test loss: 95946222338048.000000, test acc: 12.50%.\n",
      "INFO:root:Epoch: 2, Training ...\n",
      "INFO:root:Epoch: 2/20,\n",
      "INFO:root:train loss: 56910116028416.000000, train acc: 0.00%,\n",
      "INFO:root:test loss: 172218941440.000000, test acc: 6.25%.\n",
      "INFO:root:Epoch: 3, Training ...\n",
      "INFO:root:Epoch: 3/20,\n",
      "INFO:root:train loss: 135523237888.000000, train acc: 12.50%,\n",
      "INFO:root:test loss: 157339615232.000000, test acc: 0.00%.\n",
      "INFO:root:Epoch: 4, Training ...\n",
      "INFO:root:Epoch: 4/20,\n",
      "INFO:root:train loss: 92636758016.000000, train acc: 18.75%,\n",
      "INFO:root:test loss: 1558775.250000, test acc: 18.75%.\n",
      "INFO:root:Epoch: 5, Training ...\n",
      "INFO:root:Epoch: 5/20,\n",
      "INFO:root:train loss: 1669308.625000, train acc: 12.50%,\n",
      "INFO:root:test loss: 990248.687500, test acc: 18.75%.\n",
      "INFO:root:Epoch: 6, Training ...\n",
      "INFO:root:Epoch: 6/20,\n",
      "INFO:root:train loss: 1482578.875000, train acc: 12.50%,\n",
      "INFO:root:test loss: 650190.812500, test acc: 18.75%.\n",
      "INFO:root:Epoch: 7, Training ...\n",
      "INFO:root:Epoch: 7/20,\n",
      "INFO:root:train loss: 915717.750000, train acc: 0.00%,\n",
      "INFO:root:test loss: 1076143.875000, test acc: 12.50%.\n",
      "INFO:root:Epoch: 8, Training ...\n",
      "INFO:root:Epoch: 8/20,\n",
      "INFO:root:train loss: 1238016.750000, train acc: 12.50%,\n",
      "INFO:root:test loss: 215230.625000, test acc: 18.75%.\n",
      "INFO:root:Epoch: 9, Training ...\n",
      "INFO:root:Epoch: 9/20,\n",
      "INFO:root:train loss: 288138.250000, train acc: 0.00%,\n",
      "INFO:root:test loss: 345287.343750, test acc: 6.25%.\n",
      "INFO:root:Epoch: 10, Training ...\n",
      "INFO:root:Epoch: 10/20,\n",
      "INFO:root:train loss: 413898.843750, train acc: 6.25%,\n",
      "INFO:root:test loss: 330525.343750, test acc: 0.00%.\n",
      "INFO:root:Epoch: 11, Training ...\n",
      "INFO:root:Epoch: 11/20,\n",
      "INFO:root:train loss: 367356.812500, train acc: 6.25%,\n",
      "INFO:root:test loss: 82077.500000, test acc: 18.75%.\n",
      "INFO:root:Epoch: 12, Training ...\n",
      "INFO:root:Epoch: 12/20,\n",
      "INFO:root:train loss: 121668.875000, train acc: 12.50%,\n",
      "INFO:root:test loss: 108.807159, test acc: 0.00%.\n",
      "INFO:root:Epoch: 13, Training ...\n",
      "INFO:root:Epoch: 13/20,\n",
      "INFO:root:train loss: 92.276505, train acc: 6.25%,\n",
      "INFO:root:test loss: 38654.687500, test acc: 18.75%.\n",
      "INFO:root:Epoch: 14, Training ...\n",
      "INFO:root:Epoch: 14/20,\n",
      "INFO:root:train loss: 81136.257812, train acc: 6.25%,\n",
      "INFO:root:test loss: 44104.312500, test acc: 0.00%.\n",
      "INFO:root:Epoch: 15, Training ...\n",
      "INFO:root:Epoch: 15/20,\n",
      "INFO:root:train loss: 57861.648438, train acc: 6.25%,\n",
      "INFO:root:test loss: 126.215034, test acc: 18.75%.\n",
      "INFO:root:Epoch: 16, Training ...\n",
      "INFO:root:Epoch: 16/20,\n",
      "INFO:root:train loss: 44273065984.000000, train acc: 6.25%,\n",
      "INFO:root:test loss: 138.963303, test acc: 0.00%.\n",
      "INFO:root:Epoch: 17, Training ...\n",
      "INFO:root:Epoch: 17/20,\n",
      "INFO:root:train loss: 118.323784, train acc: 6.25%,\n",
      "INFO:root:test loss: 136.643143, test acc: 0.00%.\n",
      "INFO:root:Epoch: 18, Training ...\n",
      "INFO:root:Epoch: 18/20,\n",
      "INFO:root:train loss: 123.828300, train acc: 0.00%,\n",
      "INFO:root:test loss: 127.257645, test acc: 0.00%.\n",
      "INFO:root:Epoch: 19, Training ...\n",
      "INFO:root:Epoch: 19/20,\n",
      "INFO:root:train loss: 130.439224, train acc: 12.50%,\n",
      "INFO:root:test loss: 123.825821, test acc: 18.75%.\n",
      "INFO:root:Epoch: 20, Training ...\n",
      "INFO:root:Epoch: 20/20,\n",
      "INFO:root:train loss: 114.869331, train acc: 0.00%,\n",
      "INFO:root:test loss: 225806.765625, test acc: 12.50%.\n",
      "INFO:root:start saving\n",
      "INFO:root:Exporting model artifacts ...\n",
      "INFO:root:Model is saved to custom\n"
     ]
    }
   ],
   "source": [
    "! python3 custom/trainer/task.py --model-dir=custom --distribute=mirror --checkpoints=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tarball_training_script"
   },
   "source": [
    "#### Store training script on your Cloud Storage bucket\n",
    "\n",
    "Next, you package the training folder into a compressed tar ball, and then store it in your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "tarball_training_script"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom/\n",
      "custom/20.chkpt\n",
      "custom/PKG-INFO\n",
      "custom/README.md\n",
      "custom/11.chkpt\n",
      "custom/8.chkpt\n",
      "custom/7.chkpt\n",
      "custom/9.chkpt\n",
      "custom/17.chkpt\n",
      "custom/10.chkpt\n",
      "custom/19.chkpt\n",
      "custom/setup.py\n",
      "custom/1.chkpt\n",
      "custom/14.chkpt\n",
      "custom/model.pth\n",
      "custom/4.chkpt\n",
      "custom/16.chkpt\n",
      "custom/15.chkpt\n",
      "custom/3.chkpt\n",
      "custom/setup.cfg\n",
      "custom/6.chkpt\n",
      "custom/trainer/\n",
      "custom/trainer/task.py\n",
      "custom/trainer/__init__.py\n",
      "custom/12.chkpt\n",
      "custom/5.chkpt\n",
      "custom/18.chkpt\n",
      "custom/2.chkpt\n",
      "custom/13.chkpt\n",
      "Copying file://custom.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  4.2 MiB/  4.2 MiB]                                                \n",
      "Operation completed over 1 objects/4.2 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "! rm -f custom.tar custom.tar.gz\n",
    "! tar cvf custom.tar custom\n",
    "! gzip custom.tar\n",
    "! gsutil cp custom.tar.gz $BUCKET_URI/trainer_cifar10.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "docker_write,prediction,pytorch"
   },
   "source": [
    "### Make Pytorch container for prediction\n",
    "\n",
    "Currently, Vertex AI does not have a predefined container for making predictions with a deployed Pytorch model. No problem, you can assemble your own custom container. Typically, one would base the container on the `Torch Server`. For demonstration purpose, you build a placeholder container (not complete) that includes the latest `Torch Server` image, and push it to the `Container Registry`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "docker_write,prediction,pytorch"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "FROM pytorch/torchserve:latest-cpu\n",
    "\n",
    "\n",
    "# run Torchserve HTTP serve to respond to prediction requests\n",
    "CMD [\"torchserve\", \\n     \"--start\", \\n     \"--ts-config=/home/model-server/config.properties\", \\n     \"--models\", \\n     \"$APP_NAME=$APP_NAME.mar\", \\n     \"--model-store\", \\n     \"/home/model-server/model-store\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "docker_push,prediction,pytorch"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcr.io/vertex-ai-dev/pytorch_predict_cifar10\n",
      "Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\n",
      "Using default tag: latest\n",
      "Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\n"
     ]
    }
   ],
   "source": [
    "APP_NAME = \"cifar10\"\n",
    "DEPLOY_IMAGE = f\"gcr.io/{PROJECT_ID}/pytorch_predict_{APP_NAME}\"\n",
    "print(DEPLOY_IMAGE)\n",
    "\n",
    "! docker build --tag=$DEPLOY_IMAGE ./\n",
    "\n",
    "! docker push $DEPLOY_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_custom_pp_training_job:mbsdk"
   },
   "source": [
    "### Create and run custom training job\n",
    "\n",
    "\n",
    "To train a custom model, you perform two steps: 1) create a custom training job, and 2) run the job.\n",
    "\n",
    "#### Create custom training job\n",
    "\n",
    "A custom training job is created with the `CustomTrainingJob` class, with the following parameters:\n",
    "\n",
    "- `display_name`: The human readable name for the custom training job.\n",
    "- `container_uri`: The training container image.\n",
    "\n",
    "- `python_package_gcs_uri`: The location of the Python training package as a tarball.\n",
    "- `python_module_name`: The relative path to the training script in the Python package.\n",
    "- `model_serving_container_uri`: The container image for deploying the model.\n",
    "\n",
    "*Note:* There is no requirements parameter. You specify any requirements in the `setup.py` script in your Python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "create_custom_pp_training_job:mbsdk"
   },
   "outputs": [],
   "source": [
    "DISPLAY_NAME = \"cifar10_\" + TIMESTAMP\n",
    "\n",
    "job = aiplatform.CustomPythonPackageTrainingJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    python_package_gcs_uri=f\"{BUCKET_URI}/trainer_cifar10.tar.gz\",\n",
    "    python_module_name=\"trainer.task\",\n",
    "    container_uri=TRAIN_IMAGE,\n",
    "    model_serving_container_image_uri=DEPLOY_IMAGE,\n",
    "    project=PROJECT_ID,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prepare_custom_cmdargs:cifar10,pytorch"
   },
   "source": [
    "### Prepare your command-line arguments\n",
    "\n",
    "Now define the command-line arguments for your custom training container:\n",
    "\n",
    "- `args`: The command-line arguments to pass to the executable that is set as the entry point into the container.\n",
    "  - `--model-dir` : For our demonstrations, we use this command-line argument to specify where to store the model artifacts.\n",
    "      - direct: You pass the Cloud Storage location as a command line argument to your training script (set variable `DIRECT = True`), or\n",
    "      - indirect: The service passes the Cloud Storage location as the environment variable `AIP_MODEL_DIR` to your training script (set variable `DIRECT = False`). In this case, you tell the service the model artifact location in the job specification.\n",
    "  - `--BLAH`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "prepare_custom_cmdargs:cifar10,pytorch"
   },
   "outputs": [],
   "source": [
    "MODEL_DIR = \"{}/{}\".format(BUCKET_URI, TIMESTAMP)\n",
    "\n",
    "DIRECT = False\n",
    "if DIRECT:\n",
    "    CMDARGS = [\"--model_dir=\" + MODEL_DIR]\n",
    "else:\n",
    "    CMDARGS = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_custom_job:mbsdk"
   },
   "source": [
    "#### Run the custom training job\n",
    "\n",
    "Next, you run the custom job to start the training job by invoking the method `run`, with the following parameters:\n",
    "\n",
    "- `model_display_name`: The human readable name for the `Model` resource.\n",
    "- `args`: The command-line arguments to pass to the training script.\n",
    "- `replica_count`: The number of compute instances for training (replica_count = 1 is single node training).\n",
    "- `machine_type`: The machine type for the compute instances.\n",
    "- `accelerator_type`: The hardware accelerator type.\n",
    "- `accelerator_count`: The number of accelerators to attach to a worker replica.\n",
    "- `base_output_dir`: The Cloud Storage location to write the model artifacts to.\n",
    "- `sync`: Whether to block until completion of the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "run_custom_job:mbsdk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.training_jobs:Training Output directory:\n",
      "gs://vertex-ai-devaip-20220324060506/20220324060506 \n",
      "INFO:google.cloud.aiplatform.training_jobs:View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/2483058445087932416?project=931647533046\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/2483058445087932416 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/6579504518646464512?project=931647533046\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/2483058445087932416 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/2483058445087932416 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/2483058445087932416 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/2483058445087932416 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/2483058445087932416 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomPythonPackageTrainingJob run completed. Resource name: projects/931647533046/locations/us-central1/trainingPipelines/2483058445087932416\n",
      "INFO:google.cloud.aiplatform.training_jobs:Model available at projects/931647533046/locations/us-central1/models/7500485694641405952\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_GPU:\n",
    "    model = job.run(\n",
    "        model_display_name=\"cifar10_\" + TIMESTAMP,\n",
    "        args=CMDARGS,\n",
    "        replica_count=1,\n",
    "        machine_type=TRAIN_COMPUTE,\n",
    "        accelerator_type=TRAIN_GPU.name,\n",
    "        accelerator_count=TRAIN_NGPU,\n",
    "        base_output_dir=MODEL_DIR,\n",
    "        sync=False,\n",
    "    )\n",
    "else:\n",
    "    model = job.run(\n",
    "        model_display_name=\"cifar10_\" + TIMESTAMP,\n",
    "        args=CMDARGS,\n",
    "        replica_count=1,\n",
    "        machine_type=TRAIN_COMPUTE,\n",
    "        base_output_dir=MODEL_DIR,\n",
    "        sync=False,\n",
    "    )\n",
    "\n",
    "model_path_to_deploy = MODEL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "list_job"
   },
   "source": [
    "### List a custom training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "list_job"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<google.cloud.aiplatform.training_jobs.CustomPythonPackageTrainingJob object at 0x7f7640151490> \n",
      "resource name: projects/931647533046/locations/us-central1/trainingPipelines/2483058445087932416]\n"
     ]
    }
   ],
   "source": [
    "_job = job.list(filter=f\"display_name={DISPLAY_NAME}\")\n",
    "print(_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "custom_job_wait:mbsdk"
   },
   "source": [
    "### Wait for completion of custom training job\n",
    "\n",
    "Next, wait for the custom training job to complete. Alternatively, one can set the parameter `sync` to `True` in the `run()` method to block until the custom training job is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "custom_job_wait:mbsdk"
   },
   "outputs": [],
   "source": [
    "model.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "delete_job"
   },
   "source": [
    "### Delete a custom training job\n",
    "\n",
    "After a training job is completed, you can delete the training job with the method `delete()`.  Prior to completion, a training job can be cancelled with the method `cancel()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "delete_job"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.base:Deleting CustomPythonPackageTrainingJob : projects/931647533046/locations/us-central1/trainingPipelines/2483058445087932416\n",
      "INFO:google.cloud.aiplatform.base:Delete CustomPythonPackageTrainingJob  backing LRO: projects/931647533046/locations/us-central1/operations/90708128743555072\n",
      "INFO:google.cloud.aiplatform.base:CustomPythonPackageTrainingJob deleted. . Resource name: projects/931647533046/locations/us-central1/trainingPipelines/2483058445087932416\n"
     ]
    }
   ],
   "source": [
    "job.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "source": [
    "# Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n",
    "- Model\n",
    "- Custom Job (Custom job deleted in previous cell)\n",
    "- Cloud Storage Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.base:Deleting Model : projects/931647533046/locations/us-central1/models/7500485694641405952\n"
     ]
    },
    {
     "ename": "NotFound",
     "evalue": "404 The Model \"projects/931647533046/locations/us-central1/models/7500485694641405952\" does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/google/api_core/grpc_helpers.py:66\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/grpc/_channel.py:946\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    944\u001b[0m state, call, \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(request, timeout, metadata, credentials,\n\u001b[1;32m    945\u001b[0m                               wait_for_ready, compression)\n\u001b[0;32m--> 946\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/grpc/_channel.py:849\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 849\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.NOT_FOUND\n\tdetails = \"The Model \"projects/931647533046/locations/us-central1/models/7500485694641405952\" does not exist.\"\n\tdebug_error_string = \"{\"created\":\"@1648127279.286991013\",\"description\":\"Error received from peer ipv4:172.217.212.95:443\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":903,\"grpc_message\":\"The Model \"projects/931647533046/locations/us-central1/models/7500485694641405952\" does not exist.\",\"grpc_status\":5}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Delete the model using the Vertex model object\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIS_TESTING\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      6\u001b[0m     get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m gsutil rm -r $BUCKET_URI\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/google/cloud/aiplatform/base.py:730\u001b[0m, in \u001b[0;36moptional_sync.<locals>.optional_run_in_thread.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m    729\u001b[0m         VertexAiResourceNounWithFutureManager\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 730\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;66;03m# callbacks to call within the Future (in same Thread)\u001b[39;00m\n\u001b[1;32m    733\u001b[0m internal_callbacks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/google/cloud/aiplatform/base.py:1148\u001b[0m, in \u001b[0;36mVertexAiResourceNounWithFutureManager.delete\u001b[0;34m(self, sync)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;124;03m\"\"\"Deletes this Vertex AI resource. WARNING: This deletion is\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;124;03mpermanent.\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;124;03m        be immediately returned and synced when the Future has completed.\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1147\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mlog_action_start_against_resource(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeleting\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1148\u001b[0m lro \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_delete_method\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresource_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1149\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mlog_action_started_against_resource_with_lro(\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDelete\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, lro\n\u001b[1;32m   1151\u001b[0m )\n\u001b[1;32m   1152\u001b[0m lro\u001b[38;5;241m.\u001b[39mresult()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/google/cloud/aiplatform_v1/services/model_service/client.py:1017\u001b[0m, in \u001b[0;36mModelServiceClient.delete_model\u001b[0;34m(self, request, name, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(metadata) \u001b[38;5;241m+\u001b[39m (\n\u001b[1;32m   1013\u001b[0m     gapic_v1\u001b[38;5;241m.\u001b[39mrouting_header\u001b[38;5;241m.\u001b[39mto_grpc_metadata(((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mname),)),\n\u001b[1;32m   1014\u001b[0m )\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m-> 1017\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;66;03m# Wrap the response in an operation future.\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m response \u001b[38;5;241m=\u001b[39m gac_operation\u001b[38;5;241m.\u001b[39mfrom_gapic(\n\u001b[1;32m   1021\u001b[0m     response,\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39moperations_client,\n\u001b[1;32m   1023\u001b[0m     empty_pb2\u001b[38;5;241m.\u001b[39mEmpty,\n\u001b[1;32m   1024\u001b[0m     metadata_type\u001b[38;5;241m=\u001b[39mgca_operation\u001b[38;5;241m.\u001b[39mDeleteOperationMetadata,\n\u001b[1;32m   1025\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/google/api_core/gapic_v1/method.py:154\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, *args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     metadata\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata)\n\u001b[1;32m    152\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m metadata\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/google/api_core/grpc_helpers.py:68\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mNotFound\u001b[0m: 404 The Model \"projects/931647533046/locations/us-central1/models/7500485694641405952\" does not exist."
     ]
    }
   ],
   "source": [
    "# Delete the model using the Vertex model object\n",
    "model.delete()\n",
    "\n",
    "\n",
    "if os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil rm -r $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "get_started_vertex_training_pytorch.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

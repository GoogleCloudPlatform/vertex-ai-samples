{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "copyright"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title:generic,gcp"
      },
      "source": [
        "# E2E ML on GCP: MLOps stage 2 : experimentation: get started with Vertex AI Feature Store\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage2/get_started_vertex_feature_store.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "    \n",
        "  <td>\n",
        "        <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage2/get_started_vertex_feature_store.ipynb\">\n",
        "        <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "        </a>\n",
        "  </td>\n",
        "    \n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/ml_ops/stage2/get_started_vertex_feature_store.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "    \n",
        "</table>\n",
        "<br/><br/><br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview:mlops"
      },
      "source": [
        "## Overview\n",
        "\n",
        "\n",
        "This tutorial demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. This tutorial covers stage 2 : experimentation: get started with Feature Store."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "objective:mlops,stage2,get_started_vertex_feature_store"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to use `Vertex AI Feature Store` when training and predicting with `Vertex AI`.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services:\n",
        "\n",
        "- `Vertex AI Feature Store`\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Creating a Vertex AI `Featurestore` resource.\n",
        "    - Creating `EntityType` resources for the `Featurestore` resource.\n",
        "    - Creating `Feature` resources for each `EntityType` resource.\n",
        "- Import feature values (entity data items) into `Featurestore` resource.\n",
        "    - From a Cloud Storage location.\n",
        "    - From a pandas DataFrame.\n",
        "- Perform online serving from a `Featurestore` resource.\n",
        "- Perform batch serving from a `Featurestore` resource."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset:movies,lbn,avro"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset used for this tutorial is the `Movie Recommendations` dataset. The version of the dataset you use in this tutorial is stored in a public Cloud Storage bucket, in Avro format.\n",
        "\n",
        "This dataset is used to predict whether a person watches a movie or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81c777b8ad32"
      },
      "source": [
        "### Costs\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "- Vertex AI\n",
        "- Cloud Storage\n",
        "- BigQuery\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing) and [BigQuery pricing](https://cloud.google.com/bigquery/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_mlops"
      },
      "source": [
        "## Installations\n",
        "\n",
        "Install the following packages for further running this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_mlops"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Vertex AI Workbench Notebook product has specific requirements\n",
        "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
        "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
        "    \"/opt/deeplearning/metadata/env_version\"\n",
        ")\n",
        "\n",
        "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_WORKBENCH_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\"\n",
        "\n",
        "# Install the dependecies\n",
        "! pip3 install --upgrade google-cloud-aiplatform google-cloud-bigquery pyarrow avro $USER_FLAG -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "restart"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "restart"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "project_id"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "1. [Enable the Vertex AI, Compute Engine, Cloud Storage and Cloud Logging APIs](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component,storage_component,logging).\n",
        "\n",
        "1. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
        "\n",
        "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "project_id"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_project_id"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_project_id"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID:\", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_gcloud_project_id"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
        "\n",
        "- Americas: `us-central1`\n",
        "- Europe: `europe-west4`\n",
        "- Asia Pacific: `asia-east1`\n",
        "\n",
        "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
        "\n",
        "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "region"
      },
      "outputs": [],
      "source": [
        "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
        "\n",
        "if REGION == \"[your-region]\":\n",
        "    REGION = \"us-central1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "timestamp"
      },
      "source": [
        "#### Timestamp\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "timestamp"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29b110b44457"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using Vertex AI Workbench Notebooks**, your environment is already authenticated. Skip this step.\n",
        "\n",
        "**If you are using Colab**, run the cell below and follow the instructions when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "In the Cloud Console, go to the [Create service account key](https://console.cloud.google.com/apis/credentials/serviceaccountkey) page.\n",
        "\n",
        "1. **Click Create service account**.\n",
        "\n",
        "2. In the **Service account name** field, enter a name, and click **Create**.\n",
        "\n",
        "3. In the **Grant this service account access to project** section, click the Role drop-down list. Type \"Vertex\" into the filter box, and select **Vertex Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "\n",
        "4. Click Create. A JSON file that contains your key downloads to your local environment.\n",
        "\n",
        "5. Enter the path to your service account key as the GOOGLE_APPLICATION_CREDENTIALS variable in the cell below and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89788a802687"
      },
      "outputs": [],
      "source": [
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# If on Vertex AI Workbench, then don't execute this code\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
        "    \"DL_ANACONDA_HOME\"\n",
        "):\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_vars"
      },
      "source": [
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "import google.cloud.aiplatform as aiplatform\n",
        "from google.cloud import bigquery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_bq"
      },
      "source": [
        "Initialize Vertex AI and BigQuery clients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_bq"
      },
      "outputs": [],
      "source": [
        "aiplatform.init(project=PROJECT_ID)\n",
        "bqclient = bigquery.Client(project=PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "featurestore_intro"
      },
      "source": [
        "## Introduction to Vertex AI Feature Store\n",
        "\n",
        "Let's assume you have a recommendation model that predicts a coupon to print on the back of a cash register receipt. Now, if that model was trained only on single transaction instances (what was bought and how much), then (in the past) you used an Apriori algorithm.\n",
        "\n",
        "But now we have historical data on the customer (say it's indexed by credit card number). Like total purchases to date, average purchase per transaction, frequency of purchase by product category, etc. We use this \"enriched data\" to train a recommender system.\n",
        "\n",
        "Now it's time to do a live prediction. You get a transaction from the cash register, but all it has is the credit card number and this transaction. It does not have the enriched data the model needs. During serving, the credit card number is used as an index to Feature Store to get the enriched data needed for the model.\n",
        "\n",
        "On the other hand, let's say the enriched data the model was trained on was timestamped on June 1st. The current transaction is from June 15th. Assume that the user has made other transactions between June 1st and 15th, and the enriched data has been continuously updated in Feature Store. But the model was trained on June 1st data. FeatureStore knows the version number and serves the June 1st version to the model (not the current June 15th). Otherwise, if you used June 15th data, you would have training-serving skew.\n",
        "\n",
        "Another problem here is the data drift. Things change and suddenly one day, everybody is buying toilet paper! There is a significant change in the distribution of existing enriched data from the distribution that the deployed model was trained on. FeatureStore can detect changes/thresholds in distribution changes and trigger a notification for retraining the model.\n",
        "\n",
        "Learn more about [Vertex AI Feature Store API](https://cloud.google.com/vertex-ai/docs/featurestore)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "featurestore_datamodel:movies"
      },
      "source": [
        "## Vertex AI Feature Store data model\n",
        "\n",
        "Vertex AI Feature Store organizes data with the following 3 important hierarchical concepts:\n",
        "\n",
        "        Featurestore -> EntityType -> Feature\n",
        "\n",
        "- `Featurestore`: the place to store your features.\n",
        "- `EntityType`: under a `Featurestore`, an `EntityType` describes an object to be modeled, real one or virtual one.\n",
        "- `Feature`: under an `EntityType`, a `Feature` describes an attribute of the `EntityType`.\n",
        "\n",
        "Learn more about [Vertex AI Feature Store data model](https://cloud.google.com/vertex-ai/docs/featurestore/concepts).\n",
        "\n",
        "In the movie prediction dataset, you create a `Featurestore` resource called movies. This `Featurestore` resource has 2 entity types:\n",
        "- `users`: The entity type has the `age`, `gender`, and `like genres` features.\n",
        "- `movies`: The entity type has the `genres` and `average rating` features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "featurestore_create"
      },
      "source": [
        "## Create a `Featurestore` resource\n",
        "\n",
        "First, you create a `Featurestore` for the dataset using the `Featurestore.create()` method, with the following parameters:\n",
        "\n",
        "- `featurestore_id`: The name of the feature store.\n",
        "- `online_store_fixed_node_count`: Configuration settings for online serving from the feature store.\n",
        "- `project`: The project ID.\n",
        "- `location`: The location (region)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "featurestore_create"
      },
      "outputs": [],
      "source": [
        "# Represents featurestore resource path.\n",
        "FEATURESTORE_NAME = \"movies_\" + TIMESTAMP\n",
        "\n",
        "featurestore = aiplatform.Featurestore.create(\n",
        "    featurestore_id=FEATURESTORE_NAME,\n",
        "    online_store_fixed_node_count=1,\n",
        "    project=PROJECT_ID,\n",
        "    location=REGION,\n",
        ")\n",
        "\n",
        "print(featurestore)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "featurestore_list"
      },
      "source": [
        "### List your `Featurestore` resources\n",
        "\n",
        "You can get a list of all your `Featurestore` resources in your project using the `Featurestore.list()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "featurestore_list"
      },
      "outputs": [],
      "source": [
        "for featurestore in aiplatform.Featurestore.list():\n",
        "    print(featurestore)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "featurestore_get"
      },
      "source": [
        "### Get a `Featurestore` resource\n",
        "\n",
        "You can get a specifed `Featurestore` resource in your project using the `Featurestore()` initializer, with the following parameters:\n",
        "\n",
        "- `featurestore_name`: The name for the `Featurestore` resource.\n",
        "- `project`: The project ID.\n",
        "- `location`: The location (region)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "featurestore_get"
      },
      "outputs": [],
      "source": [
        "featurestore = featurestore = aiplatform.Featurestore(\n",
        "    featurestore_name=FEATURESTORE_NAME, project=PROJECT_ID, location=REGION\n",
        ")\n",
        "print(featurestore)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "featurestore_create:entity_type"
      },
      "source": [
        "## Create entity types for your `Featurestore` resource\n",
        "\n",
        "Next, you create the `EntityType` resources for your `Featurestore` resource using the `create_entity_type()` method, with the following parameters:\n",
        "\n",
        "- `entity_type_id`: The name of the `EntityType` resource.\n",
        "- `description`:  A description of the entity type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "featurestore_create:entity_type"
      },
      "outputs": [],
      "source": [
        "for name, description in [(\"users\", \"Users descrip\"), (\"movies\", \"Movies descrip\")]:\n",
        "    entity_type = featurestore.create_entity_type(\n",
        "        entity_type_id=name, description=description\n",
        "    )\n",
        "    print(entity_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "featurestore_create:feature"
      },
      "source": [
        "### Add `Feature` resources for your `EntityType` resources\n",
        "\n",
        "Next, you create the `Feature` resources for each of the `EntityType` resources in your `Featurestore` resource using the `create_feature()` method, with the following parameters:\n",
        "\n",
        "- `feature_id`: The name of the `Feature` resource.\n",
        "- `description`: A description of the feature.\n",
        "- `value_type`: The data type for the feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "featurestore_create:feature,movies"
      },
      "outputs": [],
      "source": [
        "def create_features(featurestore_name, entity_name, features):\n",
        "    entity_type = aiplatform.EntityType(\n",
        "        entity_type_name=entity_name, featurestore_id=featurestore_name\n",
        "    )\n",
        "\n",
        "    for feature in features:\n",
        "        feature = entity_type.create_feature(\n",
        "            feature_id=feature[0], description=feature[1], value_type=feature[2]\n",
        "        )\n",
        "        print(feature)\n",
        "\n",
        "\n",
        "create_features(\n",
        "    FEATURESTORE_NAME,\n",
        "    \"users\",\n",
        "    [\n",
        "        (\"age\", \"Age descrip\", \"INT64\"),\n",
        "        (\"gender\", \"Gender descrip\", \"STRING\"),\n",
        "        (\"liked_genres\", \"Genres descrip\", \"STRING_ARRAY\"),\n",
        "    ],\n",
        ")\n",
        "\n",
        "create_features(\n",
        "    FEATURESTORE_NAME,\n",
        "    \"movies\",\n",
        "    [\n",
        "        (\"title\", \"Title descrip\", \"STRING\"),\n",
        "        (\"genres\", \"Genres descrip\", \"STRING\"),\n",
        "        (\"average_rating\", \"Ave descrip\", \"DOUBLE\"),\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "featurestore_list"
      },
      "source": [
        "### List your `Featurestore` resources\n",
        "\n",
        "You can get a list of all your `Featurestore` resources in your project using the `Featurestore.list()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "featurestore_list"
      },
      "outputs": [],
      "source": [
        "for featurestore in aiplatform.Featurestore.list():\n",
        "    print(featurestore)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "featurestore_list:filter"
      },
      "source": [
        "### Search `Feature` resources using a filter\n",
        "\n",
        "You can narrow your search of `Feature` resources using the method `list_features()` and specifying a `filter` string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "featurestore_list:filter"
      },
      "outputs": [],
      "source": [
        "# Search by data type\n",
        "features = []\n",
        "for entity_type in featurestore.list_entity_types():\n",
        "    features += entity_type.list_features(filter=\"value_type=DOUBLE\")\n",
        "print(\"By data type\")\n",
        "for feature in features:\n",
        "    print(features)\n",
        "\n",
        "# Search by data type\n",
        "features = []\n",
        "for entity_type in featurestore.list_entity_types():\n",
        "    _ = entity_type.list_features()\n",
        "    for feature in _:\n",
        "        if feature.name == \"title\":\n",
        "            print(type(feature))\n",
        "            features += [feature]\n",
        "\n",
        "print(\"By Name\")\n",
        "for feature in features:\n",
        "    print(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "featurestore_search:query"
      },
      "source": [
        "### Search `Feature` resources using a query\n",
        "\n",
        "You can narrow your search of `Feature` resources using the method `search()` and specifying a `query` filter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "featurestore_search:query"
      },
      "outputs": [],
      "source": [
        "features = aiplatform.Feature.search(query=\"value_type=DOUBLE\")\n",
        "print(\"By data type\")\n",
        "for feature in features:\n",
        "    print(features)\n",
        "\n",
        "aiplatform.Feature.search(query=\"feature_id=title\")\n",
        "print(\"By Name\")\n",
        "for feature in features:\n",
        "    print(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "277e9884cf37"
      },
      "source": [
        "Define paths to the feature data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_file:movies,lbn,avro"
      },
      "outputs": [],
      "source": [
        "IMPORT_FILE = (\n",
        "    \"gs://cloud-samples-data/vertex-ai/feature-store/datasets/movie_prediction.csv\"\n",
        ")\n",
        "FS_ENTITIES = {\n",
        "    \"users\": \"gs://cloud-samples-data/vertex-ai/feature-store/datasets/users.avro\",\n",
        "    \"movies\": \"gs://cloud-samples-data-us-central1/vertex-ai/feature-store/datasets/movies.avro\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "featurestore_import:movies,avro"
      },
      "source": [
        "## Import the feature data into your `Featurestore` resource\n",
        "\n",
        "Next, you import the feature data for your `Featurestore` resource. Once imported, you can use these feature values for online and offline (batch) serving.\n",
        "\n",
        "### Data layout\n",
        "\n",
        "Each imported `EntityType` resource data must have an ID. Also, each `EntityType` resource data item can optionally have a timestamp, sepecifying when the feature values were generated.\n",
        "\n",
        "When importing, specify the following in your request:\n",
        "\n",
        "- Data source format: BigQuery Table/Avro/CSV/Pandas Dataframe\n",
        "- Data source URL\n",
        "- Destination: featurestore/entity types/features to be imported\n",
        "\n",
        "The feature values for `Movie Recommendations` dataset are in Avro format. The Avro schemas are as follows:\n",
        "\n",
        "**Users entity**:\n",
        "\n",
        "```\n",
        "schema = {\n",
        "  \"type\": \"record\",\n",
        "  \"name\": \"User\",\n",
        "  \"fields\": [\n",
        "      {\n",
        "       \"name\":\"user_id\",\n",
        "       \"type\":[\"null\",\"string\"]\n",
        "      },\n",
        "      {\n",
        "       \"name\":\"age\",\n",
        "       \"type\":[\"null\",\"long\"]\n",
        "      },\n",
        "      {\n",
        "       \"name\":\"gender\",\n",
        "       \"type\":[\"null\",\"string\"]\n",
        "      },\n",
        "      {\n",
        "       \"name\":\"liked_genres\",\n",
        "       \"type\":{\"type\":\"array\",\"items\":\"string\"}\n",
        "      },\n",
        "      {\n",
        "       \"name\":\"update_time\",\n",
        "       \"type\":[\"null\",{\"type\":\"long\",\"logicalType\":\"timestamp-micros\"}]\n",
        "      },\n",
        "  ]\n",
        " }\n",
        " ```\n",
        "\n",
        "**Movies entity**:\n",
        "\n",
        "```\n",
        "schema = {\n",
        " \"type\": \"record\",\n",
        " \"name\": \"Movie\",\n",
        " \"fields\": [\n",
        "     {\n",
        "      \"name\":\"movie_id\",\n",
        "      \"type\":[\"null\",\"string\"]\n",
        "     },\n",
        "     {\n",
        "      \"name\":\"average_rating\",\n",
        "      \"type\":[\"null\",\"double\"]\n",
        "     },\n",
        "     {\n",
        "      \"name\":\"title\",\n",
        "      \"type\":[\"null\",\"string\"]\n",
        "     },\n",
        "     {\n",
        "      \"name\":\"genres\",\n",
        "      \"type\":[\"null\",\"string\"]\n",
        "     },\n",
        "     {\n",
        "      \"name\":\"update_time\",\n",
        "      \"type\":[\"null\",{\"type\":\"long\",\"logicalType\":\"timestamp-micros\"}]\n",
        "     },\n",
        " ]\n",
        "}\n",
        "```\n",
        "\n",
        "### Importing the feature values from Cloud Storage\n",
        "\n",
        "You import the feature values for the `EntityType` resources using the `ingest_from_gcs()` method, with the following parameters:\n",
        "\n",
        "- `entity_id_field`: The identifier name for the parent `EntityType` resource.\n",
        "- `feature_ids`: A list of identifier names for `Feature` resources' data to add to the `EntityType` resource.\n",
        "- `feature_time`: The field corresponding to the timestamp for the features being entered.\n",
        "- `gcs_source_type`: The format of the imported data. Must be CSV or Avro.\n",
        "- `gcs_source_uris`: A list of one or more Cloud Storage locations of the imported data files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "featurestore_import:movies,avro"
      },
      "outputs": [],
      "source": [
        "entity_type = featurestore.get_entity_type(\"users\")\n",
        "response = entity_type.ingest_from_gcs(\n",
        "    entity_id_field=\"user_id\",\n",
        "    feature_ids=[\"age\", \"gender\", \"liked_genres\"],\n",
        "    feature_time=\"update_time\",\n",
        "    gcs_source_type=\"avro\",\n",
        "    gcs_source_uris=[FS_ENTITIES[\"users\"]],\n",
        ")\n",
        "print(response)\n",
        "\n",
        "entity_type = featurestore.get_entity_type(\"movies\")\n",
        "response = entity_type.ingest_from_gcs(\n",
        "    entity_id_field=\"movie_id\",\n",
        "    feature_ids=[\"title\", \"genres\", \"average_rating\"],\n",
        "    feature_time=\"update_time\",\n",
        "    gcs_source_type=\"avro\",\n",
        "    gcs_source_uris=[FS_ENTITIES[\"movies\"]],\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "featurestore_delete_entities:movies"
      },
      "source": [
        "#### Delete the entity types and corresponding features and feature values\n",
        "\n",
        "Now, in preparation to repeat the process of importing feature values but from a dataframe this time, you delete the existing entity types, and the corresponding content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "featurestore_delete_entities:movies"
      },
      "outputs": [],
      "source": [
        "entity_type = featurestore.get_entity_type(\"users\")\n",
        "entity_type.delete(force=True)\n",
        "entity_type = featurestore.get_entity_type(\"movies\")\n",
        "entity_type.delete(force=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "featurestore_create:entity_type"
      },
      "source": [
        "## Create entity types for your `Featurestore` resource\n",
        "\n",
        "Next, you create the `EntityType` resources again for your `Featurestore` resource using the `create_entity_type()` method, with the following parameters:\n",
        "\n",
        "- `entity_type_id`: The name of the `EntityType` resource.\n",
        "- `description`:  A description of the entity type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "featurestore_create:entity_type"
      },
      "outputs": [],
      "source": [
        "for name, description in [(\"users\", \"Users descrip\"), (\"movies\", \"Movies descrip\")]:\n",
        "    entity_type = featurestore.create_entity_type(\n",
        "        entity_type_id=name, description=description\n",
        "    )\n",
        "    print(entity_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "featurestore_create:feature"
      },
      "source": [
        "### Add `Feature` resources for your `EntityType` resources\n",
        "\n",
        "Further, you create the `Feature` resources again for each of the `EntityType` resources in your `Featurestore` resource using the `create_feature()` method, with the following parameters:\n",
        "\n",
        "- `feature_id`: The name of the `Feature` resource.\n",
        "- `description`: A description of the feature.\n",
        "- `value_type`: The data type for the feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "featurestore_create:feature,movies"
      },
      "outputs": [],
      "source": [
        "def create_features(featurestore_name, entity_name, features):\n",
        "    entity_type = aiplatform.EntityType(\n",
        "        entity_type_name=entity_name, featurestore_id=featurestore_name\n",
        "    )\n",
        "\n",
        "    for feature in features:\n",
        "        feature = entity_type.create_feature(\n",
        "            feature_id=feature[0], description=feature[1], value_type=feature[2]\n",
        "        )\n",
        "        print(feature)\n",
        "\n",
        "\n",
        "create_features(\n",
        "    FEATURESTORE_NAME,\n",
        "    \"users\",\n",
        "    [\n",
        "        (\"age\", \"Age descrip\", \"INT64\"),\n",
        "        (\"gender\", \"Gender descrip\", \"STRING\"),\n",
        "        (\"liked_genres\", \"Genres descrip\", \"STRING_ARRAY\"),\n",
        "    ],\n",
        ")\n",
        "\n",
        "create_features(\n",
        "    FEATURESTORE_NAME,\n",
        "    \"movies\",\n",
        "    [\n",
        "        (\"title\", \"Title descrip\", \"STRING\"),\n",
        "        (\"genres\", \"Genres descrip\", \"STRING\"),\n",
        "        (\"average_rating\", \"Ave descrip\", \"DOUBLE\"),\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8715a3f719c8"
      },
      "source": [
        "Now, copy the `users` and `movies` data into avro files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_file:movies,lbn,df"
      },
      "outputs": [],
      "source": [
        "GCS_USERS_AVRO_URI = FS_ENTITIES[\"users\"]\n",
        "GCS_MOVIES_AVRO_URI = FS_ENTITIES[\"movies\"]\n",
        "\n",
        "USERS_AVRO_FN = \"users.avro\"\n",
        "MOVIES_AVRO_FN = \"movies.avro\"\n",
        "\n",
        "! gsutil cp $GCS_USERS_AVRO_URI $USERS_AVRO_FN\n",
        "! gsutil cp $GCS_MOVIES_AVRO_URI $MOVIES_AVRO_FN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_df_from_avro"
      },
      "source": [
        "#### Load Avro Files into pandas DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_df_from_avro"
      },
      "outputs": [],
      "source": [
        "from avro.datafile import DataFileReader\n",
        "from avro.io import DatumReader\n",
        "\n",
        "\n",
        "class AvroReader:\n",
        "    def __init__(self, data_file):\n",
        "        self.avro_reader = DataFileReader(open(data_file, \"rb\"), DatumReader())\n",
        "\n",
        "    def to_dataframe(self):\n",
        "        records = [record for record in self.avro_reader]\n",
        "        return pd.DataFrame.from_records(data=records)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "users_avro_reader = AvroReader(data_file=USERS_AVRO_FN)\n",
        "users_source_df = users_avro_reader.to_dataframe()\n",
        "print(users_source_df)\n",
        "\n",
        "movies_avro_reader = AvroReader(data_file=MOVIES_AVRO_FN)\n",
        "movies_source_df = movies_avro_reader.to_dataframe()\n",
        "print(movies_source_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "featurestore_import:movies,df"
      },
      "source": [
        "### Importing the feature values from DataFrame\n",
        "\n",
        "You import the feature values for the `EntityType` resources using the `ingest_from_df()` method, with the following parameters:\n",
        "\n",
        "- `entity_id_field`: The identifier name for the parent `EntityType` resource.\n",
        "- `feature_ids`: A list of identifier names for `Feature` resources' data to add to the `EntityType` resource.\n",
        "- `feature_time`: The field corresponding to the timestamp for the features being entered.\n",
        "- `df_source`: The DataFrame containing the imported feature values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "featurestore_import:movies,df"
      },
      "outputs": [],
      "source": [
        "entity_type = featurestore.get_entity_type(\"users\")\n",
        "entity_type.ingest_from_df(\n",
        "    feature_ids=[\"age\", \"gender\", \"liked_genres\"],\n",
        "    feature_time=\"update_time\",\n",
        "    df_source=users_source_df,\n",
        "    entity_id_field=\"user_id\",\n",
        ")\n",
        "\n",
        "entity_type = featurestore.get_entity_type(\"movies\")\n",
        "entity_type.ingest_from_df(\n",
        "    feature_ids=[\"average_rating\", \"title\", \"genres\"],\n",
        "    feature_time=\"update_time\",\n",
        "    df_source=movies_source_df,\n",
        "    entity_id_field=\"movie_id\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "featurestore_serving"
      },
      "source": [
        "## Vertex AI Feature Store serving\n",
        "\n",
        "The Vertex AI Feature Store service provides the following two services for serving features from a `Featurestore` resource:\n",
        "\n",
        "- Online serving - low-latency serving of small batches of features (prediction).\n",
        "\n",
        "- Batch serving - high-throughput serving of large batches of features (training and prediction)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "featurestore_serving:online,movies"
      },
      "outputs": [],
      "source": [
        "def serve_features(featurestore, entity_name, features, id):\n",
        "    entity_type = featurestore.get_entity_type(entity_name)\n",
        "    return entity_type.read(entity_ids=[id], feature_ids=features)\n",
        "\n",
        "\n",
        "features = serve_features(\n",
        "    featurestore, \"users\", [\"age\", \"gender\", \"liked_genres\"], \"alice\"\n",
        ")\n",
        "print(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "featurestore_serving:batch"
      },
      "source": [
        "## Batch Serving\n",
        "\n",
        "The Vertex AI Feature Store's batch serving service is optimized for serving large batches of features in real-time with high throughput, typically for training a model or batch prediction.\n",
        "\n",
        "One can batch serve to the following destinations:\n",
        "\n",
        "- BigQuery table\n",
        "- Cloud Storage location\n",
        "- Dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "featurestore_serving:batch,output,movies"
      },
      "source": [
        "### Output dataset\n",
        "\n",
        "For batch serving, you use a BigQuery table for the output. First, you must create this output destination table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "featurestore_serving:batch,output,movies"
      },
      "outputs": [],
      "source": [
        "# Output dataset\n",
        "DESTINATION_DATASET = f\"movies_predictions_{TIMESTAMP}\"\n",
        "\n",
        "# Output table.\n",
        "DESTINATION_TABLE = \"training_data\"  # @param {type:\"string\"}\n",
        "\n",
        "DESTINATION_TABLE_URI = f\"bq://{PROJECT_ID}.{DESTINATION_DATASET}.{DESTINATION_TABLE}\"\n",
        "\n",
        "dataset_id = f\"{PROJECT_ID}.{DESTINATION_DATASET}\"\n",
        "dataset = bigquery.Dataset(dataset_id)\n",
        "dataset = bqclient.create_dataset(dataset)\n",
        "print(\"Created dataset:\", dataset_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "featurestore_serving:batch,read,movies"
      },
      "source": [
        "### Batch Read Feature Values\n",
        "\n",
        "You batch serve entity data items to a BigQuery table using the `read_serve_to_bq()` method, with the following parameters:\n",
        "\n",
        "- `bq_destination_output_uri`: The destination BigQuery table to receive the served features.\n",
        "- `serving_feature_ids`: A dictionary of entity type and corresponding features to serve.\n",
        "- `read_instances_uri`: A Cloud Storage location to read the entity data items from.\n",
        "\n",
        "The output is stored in a BigQuery table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "featurestore_serving:batch,read,movies"
      },
      "outputs": [],
      "source": [
        "response = featurestore.batch_serve_to_bq(\n",
        "    bq_destination_output_uri=DESTINATION_TABLE_URI,\n",
        "    serving_feature_ids={\n",
        "        \"users\": [\"age\", \"gender\", \"liked_genres\"],\n",
        "        \"movies\": [\"average_rating\", \"genres\"],\n",
        "    },\n",
        "    read_instances_uri=IMPORT_FILE,\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "delete_bq_dataset"
      },
      "source": [
        "## Cleaning up\n",
        "### Delete a BigQuery dataset\n",
        "\n",
        "Use the method `delete_dataset()` to delete a BigQuery dataset along with all its tables, by setting the parameter `delete_contents` to `True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "delete_bq_dataset"
      },
      "outputs": [],
      "source": [
        "bqclient.delete_dataset(dataset, delete_contents=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "featurestore_delete"
      },
      "source": [
        "### Delete a `Featurestore` resource\n",
        "\n",
        "You can get a delete a specified `Featurestore` resource using the `delete()` method, with the following parameter:\n",
        "\n",
        "- `force`: A flag indicating whether to delete a non-empy `Featurestore` resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "featurestore_delete"
      },
      "outputs": [],
      "source": [
        "featurestore.delete(force=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "get_started_vertex_feature_store.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

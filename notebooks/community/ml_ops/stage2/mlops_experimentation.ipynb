{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "copyright"
      },
      "outputs": [],
      "source": [
        "# Copyright 2021 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title:generic,gcp"
      },
      "source": [
        "# E2E ML on GCP: MLOps stage 2 : experimentation\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage2/mlops_experimentation.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/ai/platform/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage2/mlops_experimentation.ipynb\">\n",
        "      Open in Google Cloud Notebooks\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "<br/><br/><br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview:mlops"
      },
      "source": [
        "## Overview\n",
        "\n",
        "\n",
        "This tutorial demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. This tutorial covers stage 2 : experimentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset:bq,chicago,lbn"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset used for this tutorial is the [Chicago Taxi](https://www.kaggle.com/chicago/chicago-taxi-trips-bq). The version of the dataset you will use in this tutorial is stored in a public BigQuery table. The trained model predicts whether someone would leave a tip for a taxi fare."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "objective:mlops,stage2,tabular"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you create a MLOps stage 2: experimentation process.\n",
        "\n",
        "This tutorial uses the following Vertex AI:\n",
        "\n",
        "- `Vertex AI Datasets`\n",
        "- `Vertex AI Models`\n",
        "- `Vertex AI AutoML`\n",
        "- `Vertex AI Training`\n",
        "- `Vertex AI TensorBoard`\n",
        "- `Vertex AI Vizier`\n",
        "- `Vertex AI Batch Prediction`\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Review the `Dataset` resource created during stage 1.\n",
        "- Train an AutoML tabular binary classifier model in the background.\n",
        "- Build the experimental model architecture.\n",
        "- Construct a custom training package for the `Dataset` resource.\n",
        "- Test the custom training package locally.\n",
        "- Test the custom training package in the cloud with Vertex AI Training.\n",
        "- Hyperparameter tune the model training with Vertex AI Vizier.\n",
        "- Train the custom model with Vertex AI Training.\n",
        "- Add a serving function for online/batch prediction to the custom model.\n",
        "- Test the custom model with the serving function.\n",
        "- Evaluate the custom model using Vertex AI Batch Prediction\n",
        "- Wait for the AutoML training job to complete.\n",
        "- Evaluate the AutoML model using Vertex AI Batch Prediction with the same evaluation slices as the custom model.\n",
        "- Set the evaluation results of the AutoML model as the baseline.\n",
        "- If the evaluation of the custom model is below baseline, continue to experiment with the custom model.\n",
        "- If the evaluation of the custom model is above baseline, save the model as the first best model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "recommendation:mlops,stage2,tabular"
      },
      "source": [
        "### Recommendations\n",
        "\n",
        "When doing E2E MLOps on Google Cloud for experimentation, the following best practices with structured (tabular) data are recommended:\n",
        "\n",
        " - Determine a baseline evaluation using AutoML.\n",
        " - Design and build a model architecture.\n",
        "     - Upload the untrained model architecture as a Vertex AI Model resource.\n",
        "\n",
        "\n",
        " - Construct a training package that can be ran locally and as a Vertex AI Training job.\n",
        "     - Decompose the training package into: data, model, train and task Python modules.\n",
        "     - Obtain the location of the transformed training data from the user metadata of the Vertex AI Dataset resource.\n",
        "     - Obtain the location of the model artifacts from the Vertex AI Model resource.\n",
        "     - Include in the training package initializing a Vertex AI Experiment and corresponding run.\n",
        "     - Log hyperparameters and training parameters for the experiment.\n",
        "     - Add callbacks for early stop, TensorBoard, and hyperparameter tuning, where hyperparameter tuning is a command-line option.\n",
        "\n",
        "\n",
        " - Test the training package locally with a small number of epochs.\n",
        " - Test the training package with Vertex AI Training.\n",
        " - Do hyperparameter tuning with Vertex AI Hyperparameter Tuning.\n",
        " - Do full training of the custom model with Vertex AI Training.\n",
        "     - Log the hyperparameter values for the experiment/run.\n",
        "\n",
        "\n",
        " - Evaluate the custom model.\n",
        "     - Single evaluation slice, same metrics as AutoML\n",
        "         - Add evaluation to the training package and return the results in a file in the Cloud Storage bucket used for training\n",
        "     - Custom evaluation slices, custom metrics\n",
        "         - Evaluate custom evaluation slices as a Vertex AI Batch Prediction for both AutoML and custom model\n",
        "         - Perform custom metrics on the results from the batch job\n",
        "\n",
        "\n",
        " - Compare custom model metrics against the AutoML baseline\n",
        "     - If less than baseline, then continue to experiment\n",
        "     - If greater then baseline, then upload model as the new baseline and save evaluation results with the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_mlops"
      },
      "source": [
        "## Installations\n",
        "\n",
        "Install *one time* the packages for executing the MLOps notebooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_mlops"
      },
      "outputs": [],
      "source": [
        "ONCE_ONLY = False\n",
        "if ONCE_ONLY:\n",
        "    ! pip3 install -U tensorflow==2.5 $USER_FLAG\n",
        "    ! pip3 install -U tensorflow-data-validation==1.2 $USER_FLAG\n",
        "    ! pip3 install -U tensorflow-transform==1.2 $USER_FLAG\n",
        "    ! pip3 install -U tensorflow-io==0.18 $USER_FLAG\n",
        "    ! pip3 install --upgrade google-cloud-aiplatform[tensorboard] $USER_FLAG\n",
        "    ! pip3 install --upgrade google-cloud-pipeline-components $USER_FLAG\n",
        "    ! pip3 install --upgrade google-cloud-bigquery $USER_FLAG\n",
        "    ! pip3 install --upgrade google-cloud-logging $USER_FLAG\n",
        "    ! pip3 install --upgrade apache-beam[gcp] $USER_FLAG\n",
        "    ! pip3 install --upgrade pyarrow $USER_FLAG\n",
        "    ! pip3 install --upgrade cloudml-hypertune $USER_FLAG\n",
        "    ! pip3 install --upgrade kfp $USER_FLAG\n",
        "    ! pip3 install --upgrade torchvision $USER_FLAG\n",
        "    ! pip3 install --upgrade rpy2 $USER_FLAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "restart"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "restart"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "project_id"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_project_id"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_project_id"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID:\", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_gcloud_project_id"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
        "\n",
        "- Americas: `us-central1`\n",
        "- Europe: `europe-west4`\n",
        "- Asia Pacific: `asia-east1`\n",
        "\n",
        "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
        "\n",
        "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "region"
      },
      "outputs": [],
      "source": [
        "REGION = \"us-central1\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "timestamp"
      },
      "source": [
        "#### Timestamp\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "timestamp"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bucket:mbsdk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "When you initialize the Vertex SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bucket"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_bucket"
      },
      "outputs": [],
      "source": [
        "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
        "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_bucket"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "validate_bucket"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "validate_bucket"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "set_service_account"
      },
      "source": [
        "#### Service Account\n",
        "\n",
        "**If you don't know your service account**, try to get your service account using `gcloud` command by executing the second cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_service_account"
      },
      "outputs": [],
      "source": [
        "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_service_account"
      },
      "outputs": [],
      "source": [
        "if (\n",
        "    SERVICE_ACCOUNT == \"\"\n",
        "    or SERVICE_ACCOUNT is None\n",
        "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
        "):\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = !gcloud auth list 2>/dev/null\n",
        "    SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
        "    print(\"Service Account:\", SERVICE_ACCOUNT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_vars"
      },
      "source": [
        "### Set up variables\n",
        "\n",
        "Next, set up some variables used throughout the tutorial.\n",
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_aip:mbsdk"
      },
      "outputs": [],
      "source": [
        "import google.cloud.aiplatform as aip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_tf"
      },
      "source": [
        "#### Import TensorFlow\n",
        "\n",
        "Import the TensorFlow package into your Python environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_tf"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_tft"
      },
      "source": [
        "#### Import TensorFlow Transform\n",
        "\n",
        "Import the TensorFlow Transform (TFT) package into your Python environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_tft"
      },
      "outputs": [],
      "source": [
        "import tensorflow_transform as tft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_tfdv"
      },
      "source": [
        "#### Import TensorFlow Data Validation\n",
        "\n",
        "Import the TensorFlow Data Validation (TFDV) package into your Python environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_tfdv"
      },
      "outputs": [],
      "source": [
        "import tensorflow_data_validation as tfdv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "outputs": [],
      "source": [
        "aip.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "accelerators:training,prediction,ngpu,mbsdk"
      },
      "source": [
        "#### Set hardware accelerators\n",
        "\n",
        "You can set hardware accelerators for training and prediction.\n",
        "\n",
        "Set the variables `TRAIN_GPU/TRAIN_NGPU` and `DEPLOY_GPU/DEPLOY_NGPU` to use a container image supporting a GPU and the number of GPUs allocated to the virtual machine (VM) instance. For example, to use a GPU container image with 4 Nvidia Telsa K80 GPUs allocated to each VM, you would specify:\n",
        "\n",
        "    (aip.AcceleratorType.NVIDIA_TESLA_K80, 4)\n",
        "\n",
        "\n",
        "Otherwise specify `(None, None)` to use a container image to run on a CPU.\n",
        "\n",
        "Learn more about [hardware accelerator support for your region](https://cloud.google.com/vertex-ai/docs/general/locations#accelerators).\n",
        "\n",
        "*Note*: TF releases before 2.3 for GPU support will fail to load the custom model in this tutorial. It is a known issue and fixed in TF 2.3. This is caused by static graph ops that are generated in the serving function. If you encounter this issue on your own custom models, use a container image for TF 2.3 with GPU support."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "accelerators:training,prediction,ngpu,mbsdk"
      },
      "outputs": [],
      "source": [
        "if os.getenv(\"IS_TESTING_TRAIN_GPU\"):\n",
        "    TRAIN_GPU, TRAIN_NGPU = (\n",
        "        aip.gapic.AcceleratorType.NVIDIA_TESLA_K80,\n",
        "        int(os.getenv(\"IS_TESTING_TRAIN_GPU\")),\n",
        "    )\n",
        "else:\n",
        "    TRAIN_GPU, TRAIN_NGPU = (aip.gapic.AcceleratorType.NVIDIA_TESLA_K80, 4)\n",
        "\n",
        "if os.getenv(\"IS_TESTING_DEPLOY_GPU\"):\n",
        "    DEPLOY_GPU, DEPLOY_NGPU = (\n",
        "        aip.gapic.AcceleratorType.NVIDIA_TESLA_K80,\n",
        "        int(os.getenv(\"IS_TESTING_DEPLOY_GPU\")),\n",
        "    )\n",
        "else:\n",
        "    DEPLOY_GPU, DEPLOY_NGPU = (None, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "container:training,prediction"
      },
      "source": [
        "#### Set pre-built containers\n",
        "\n",
        "Set the pre-built Docker container image for training and prediction.\n",
        "\n",
        "\n",
        "For the latest list, see [Pre-built containers for training](https://cloud.google.com/ai-platform-unified/docs/training/pre-built-containers).\n",
        "\n",
        "\n",
        "For the latest list, see [Pre-built containers for prediction](https://cloud.google.com/ai-platform-unified/docs/predictions/pre-built-containers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "container:training,prediction"
      },
      "outputs": [],
      "source": [
        "if os.getenv(\"IS_TESTING_TF\"):\n",
        "    TF = os.getenv(\"IS_TESTING_TF\")\n",
        "else:\n",
        "    TF = \"2.5\".replace(\".\", \"-\")\n",
        "\n",
        "if TF[0] == \"2\":\n",
        "    if TRAIN_GPU:\n",
        "        TRAIN_VERSION = \"tf-gpu.{}\".format(TF)\n",
        "    else:\n",
        "        TRAIN_VERSION = \"tf-cpu.{}\".format(TF)\n",
        "    if DEPLOY_GPU:\n",
        "        DEPLOY_VERSION = \"tf2-gpu.{}\".format(TF)\n",
        "    else:\n",
        "        DEPLOY_VERSION = \"tf2-cpu.{}\".format(TF)\n",
        "else:\n",
        "    if TRAIN_GPU:\n",
        "        TRAIN_VERSION = \"tf-gpu.{}\".format(TF)\n",
        "    else:\n",
        "        TRAIN_VERSION = \"tf-cpu.{}\".format(TF)\n",
        "    if DEPLOY_GPU:\n",
        "        DEPLOY_VERSION = \"tf-gpu.{}\".format(TF)\n",
        "    else:\n",
        "        DEPLOY_VERSION = \"tf-cpu.{}\".format(TF)\n",
        "\n",
        "TRAIN_IMAGE = \"{}-docker.pkg.dev/vertex-ai/training/{}:latest\".format(\n",
        "    REGION.split(\"-\")[0], TRAIN_VERSION\n",
        ")\n",
        "DEPLOY_IMAGE = \"{}-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(\n",
        "    REGION.split(\"-\")[0], DEPLOY_VERSION\n",
        ")\n",
        "\n",
        "print(\"Training:\", TRAIN_IMAGE, TRAIN_GPU, TRAIN_NGPU)\n",
        "print(\"Deployment:\", DEPLOY_IMAGE, DEPLOY_GPU, DEPLOY_NGPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "machine:training,prediction"
      },
      "source": [
        "#### Set machine type\n",
        "\n",
        "Next, set the machine type to use for training and prediction.\n",
        "\n",
        "- Set the variables `TRAIN_COMPUTE` and `DEPLOY_COMPUTE` to configure  the compute resources for the VMs you will use for for training and prediction.\n",
        " - `machine type`\n",
        "     - `n1-standard`: 3.75GB of memory per vCPU.\n",
        "     - `n1-highmem`: 6.5GB of memory per vCPU\n",
        "     - `n1-highcpu`: 0.9 GB of memory per vCPU\n",
        " - `vCPUs`: number of \\[2, 4, 8, 16, 32, 64, 96 \\]\n",
        "\n",
        "*Note: The following is not supported for training:*\n",
        "\n",
        " - `standard`: 2 vCPUs\n",
        " - `highcpu`: 2, 4 and 8 vCPUs\n",
        "\n",
        "*Note: You may also use n2 and e2 machine types for training and deployment, but they do not support GPUs*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "machine:training,prediction"
      },
      "outputs": [],
      "source": [
        "if os.getenv(\"IS_TESTING_TRAIN_MACHINE\"):\n",
        "    MACHINE_TYPE = os.getenv(\"IS_TESTING_TRAIN_MACHINE\")\n",
        "else:\n",
        "    MACHINE_TYPE = \"n1-standard\"\n",
        "\n",
        "VCPU = \"4\"\n",
        "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
        "print(\"Train machine type\", TRAIN_COMPUTE)\n",
        "\n",
        "if os.getenv(\"IS_TESTING_DEPLOY_MACHINE\"):\n",
        "    MACHINE_TYPE = os.getenv(\"IS_TESTING_DEPLOY_MACHINE\")\n",
        "else:\n",
        "    MACHINE_TYPE = \"n1-standard\"\n",
        "\n",
        "VCPU = \"4\"\n",
        "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
        "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "find_dataset:bq"
      },
      "source": [
        "### Retrieve the dataset from stage 1\n",
        "\n",
        "Next, retrieve the dataset you created during stage 1 with the helper function `find_dataset()`. This helper function finds all the datasets whose display name matches the specified prefix and import format (e.g., bq). Finally it sorts the matches by create time and returns the latest version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "find_dataset:bq"
      },
      "outputs": [],
      "source": [
        "def find_dataset(display_name_prefix, import_format):\n",
        "    matches = []\n",
        "    datasets = aip.TabularDataset.list()\n",
        "    for dataset in datasets:\n",
        "        if dataset.display_name.startswith(display_name_prefix):\n",
        "            try:\n",
        "                if (\n",
        "                    \"bq\" == import_format\n",
        "                    and dataset.to_dict()[\"metadata\"][\"inputConfig\"][\"bigquerySource\"]\n",
        "                ):\n",
        "                    matches.append(dataset)\n",
        "                if (\n",
        "                    \"csv\" == import_format\n",
        "                    and dataset.to_dict()[\"metadata\"][\"inputConfig\"][\"gcsSource\"]\n",
        "                ):\n",
        "                    matches.append(dataset)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    create_time = None\n",
        "    for match in matches:\n",
        "        if create_time is None or match.create_time > create_time:\n",
        "            create_time = match.create_time\n",
        "            dataset = match\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "dataset = find_dataset(\"Chicago Taxi\", \"bq\")\n",
        "\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_dataset_user_metadata"
      },
      "source": [
        "### Load dataset's user metadata\n",
        "\n",
        "Load the user metadata for the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_dataset_user_metadata"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "try:\n",
        "    with tf.io.gfile.GFile(\n",
        "        \"gs://\" + dataset.labels[\"user_metadata\"] + \"/metadata.jsonl\", \"r\"\n",
        "    ) as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    print(metadata)\n",
        "except:\n",
        "    print(\"no metadata\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_automl_pipeline:tabular,lbn"
      },
      "source": [
        "### Create and run training pipeline\n",
        "\n",
        "To train an AutoML model, you perform two steps: 1) create a training pipeline, and 2) run the pipeline.\n",
        "\n",
        "#### Create training pipeline\n",
        "\n",
        "An AutoML training pipeline is created with the `AutoMLTabularTrainingJob` class, with the following parameters:\n",
        "\n",
        "- `display_name`: The human readable name for the `TrainingJob` resource.\n",
        "- `optimization_prediction_type`: The type task to train the model for.\n",
        "  - `classification`: A tabuar classification model.\n",
        "  - `regression`: A tabular regression model.\n",
        "- `column_transformations`: (Optional): Transformations to apply to the input columns\n",
        "- `optimization_objective`: The optimization objective to minimize or maximize.\n",
        "  - binary classification:\n",
        "    - `minimize-log-loss`\n",
        "    - `maximize-au-roc`\n",
        "    - `maximize-au-prc`\n",
        "    - `maximize-precision-at-recall`\n",
        "    - `maximize-recall-at-precision`\n",
        "  - multi-class classification:\n",
        "    - `minimize-log-loss`\n",
        "  - regression:\n",
        "    - `minimize-rmse`\n",
        "    - `minimize-mae`\n",
        "    - `minimize-rmsle`\n",
        "\n",
        "The instantiated object is the DAG (directed acyclic graph) for the training pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_automl_pipeline:tabular,lbn"
      },
      "outputs": [],
      "source": [
        "dag = aip.AutoMLTabularTrainingJob(\n",
        "    display_name=\"chicago_\" + TIMESTAMP,\n",
        "    optimization_prediction_type=\"classification\",\n",
        "    optimization_objective=\"minimize-log-loss\",\n",
        ")\n",
        "\n",
        "print(dag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_automl_pipeline:async,tabular"
      },
      "source": [
        "#### Run the training pipeline\n",
        "\n",
        "Next, you run the DAG to start the training job by invoking the method `run`, with the following parameters:\n",
        "\n",
        "- `dataset`: The `Dataset` resource to train the model.\n",
        "- `model_display_name`: The human readable name for the trained model.\n",
        "- `training_fraction_split`: The percentage of the dataset to use for training.\n",
        "- `test_fraction_split`: The percentage of the dataset to use for test (holdout data).\n",
        "- `validation_fraction_split`: The percentage of the dataset to use for validation.\n",
        "- `target_column`: The name of the column to train as the label.\n",
        "- `budget_milli_node_hours`: (optional) Maximum training time specified in unit of millihours (1000 = hour).\n",
        "- `disable_early_stopping`: If `True`, training maybe completed before using the entire budget if the service believes it cannot further improve on the model objective measurements.\n",
        "\n",
        "The `run` method when completed returns the `Model` resource.\n",
        "\n",
        "The execution of the training pipeline will take upto 180 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_automl_pipeline:async,tabular"
      },
      "outputs": [],
      "source": [
        "async_model = dag.run(\n",
        "    dataset=dataset,\n",
        "    model_display_name=\"chicago_\" + TIMESTAMP,\n",
        "    training_fraction_split=0.8,\n",
        "    validation_fraction_split=0.1,\n",
        "    test_fraction_split=0.1,\n",
        "    budget_milli_node_hours=8000,\n",
        "    disable_early_stopping=False,\n",
        "    target_column=\"tip_bin\",\n",
        "    sync=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "start_experiment"
      },
      "source": [
        "### Create experiment for tracking training related metadata\n",
        "\n",
        "Setup tracking the parameters (configuration) and metrics (results) for each experiment:\n",
        "\n",
        "- `aip.init()` - Create an experiment instance\n",
        "- `aip.start_run()` - Track a specific run within the experiment.\n",
        "\n",
        "Learn more about [Introduction to Vertex AI ML Metadata](https://cloud.google.com/vertex-ai/docs/ml-metadata/introduction)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "start_experiment"
      },
      "outputs": [],
      "source": [
        "EXPERIMENT_NAME = \"chicago-\" + TIMESTAMP\n",
        "aip.init(experiment=EXPERIMENT_NAME)\n",
        "aip.start_run(\"run-1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_tensorboard_instance"
      },
      "source": [
        "### Create a Vertex AI TensorBoard instance\n",
        "\n",
        "Create a Vertex AI TensorBoard instance to use TensorBoard in conjunction with Vertex AI Training for custom model training.\n",
        "\n",
        "Learn more about [Get started with Vertex AI TensorBoard](https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-overview)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_tensorboard_instance"
      },
      "outputs": [],
      "source": [
        "TENSORBOARD_DISPLAY_NAME = \"chicago_\" + TIMESTAMP\n",
        "tensorboard = aip.Tensorboard.create(display_name=TENSORBOARD_DISPLAY_NAME)\n",
        "tensorboard_resource_name = tensorboard.gca_resource.name\n",
        "print(\"TensorBoard resource name:\", tensorboard_resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_input_layer:tabular"
      },
      "source": [
        "### Create the input layer for your custom model\n",
        "\n",
        "Next, you create the input layer for your custom tabular model, based on the data types of each feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_input_layer:tabular"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "\n",
        "def create_model_inputs(\n",
        "    numeric_features=None, categorical_features=None, embedding_features=None\n",
        "):\n",
        "    inputs = {}\n",
        "    for feature_name in numeric_features:\n",
        "        inputs[feature_name] = Input(name=feature_name, shape=[], dtype=tf.float32)\n",
        "    for feature_name in categorical_features:\n",
        "        inputs[feature_name] = Input(name=feature_name, shape=[], dtype=tf.int64)\n",
        "    for feature_name in embedding_features:\n",
        "        inputs[feature_name] = Input(name=feature_name, shape=[], dtype=tf.int64)\n",
        "\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "make_input_layer:tabular"
      },
      "outputs": [],
      "source": [
        "input_layers = create_model_inputs(\n",
        "    numeric_features=metadata[\"numeric_features\"],\n",
        "    categorical_features=metadata[\"categorical_features\"],\n",
        "    embedding_features=metadata[\"embedding_features\"],\n",
        ")\n",
        "\n",
        "print(input_layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_binary_classifier:tabular"
      },
      "source": [
        "### Create the binary classifier custom model\n",
        "\n",
        "Next, you create your binary classifier custom tabular model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_binary_classifier:tabular"
      },
      "outputs": [],
      "source": [
        "from math import sqrt\n",
        "\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.layers import (Activation, Concatenate, Dense, Embedding,\n",
        "                                     experimental)\n",
        "\n",
        "\n",
        "def create_binary_classifier(\n",
        "    input_layers,\n",
        "    tft_output,\n",
        "    metaparams,\n",
        "    numeric_features,\n",
        "    categorical_features,\n",
        "    embedding_features,\n",
        "):\n",
        "    layers = []\n",
        "    for feature_name in input_layers:\n",
        "        if feature_name in embedding_features:\n",
        "            vocab_size = tft_output.vocabulary_size_by_name(feature_name)\n",
        "            embedding_size = int(sqrt(vocab_size))\n",
        "            embedding_output = Embedding(\n",
        "                input_dim=vocab_size + 1,\n",
        "                output_dim=embedding_size,\n",
        "                name=f\"{feature_name}_embedding\",\n",
        "            )(input_layers[feature_name])\n",
        "            layers.append(embedding_output)\n",
        "        elif feature_name in categorical_features:\n",
        "            vocab_size = tft_output.vocabulary_size_by_name(feature_name)\n",
        "            onehot_layer = experimental.preprocessing.CategoryEncoding(\n",
        "                num_tokens=vocab_size,\n",
        "                output_mode=\"binary\",\n",
        "                name=f\"{feature_name}_onehot\",\n",
        "            )(input_layers[feature_name])\n",
        "            layers.append(onehot_layer)\n",
        "        elif feature_name in numeric_features:\n",
        "            numeric_layer = tf.expand_dims(input_layers[feature_name], -1)\n",
        "            layers.append(numeric_layer)\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    joined = Concatenate(name=\"combines_inputs\")(layers)\n",
        "    feedforward_output = Sequential(\n",
        "        [Dense(units, activation=\"relu\") for units in metaparams[\"hidden_units\"]],\n",
        "        name=\"feedforward_network\",\n",
        "    )(joined)\n",
        "    logits = Dense(units=1, name=\"logits\")(feedforward_output)\n",
        "    pred = Activation(\"sigmoid\")(logits)\n",
        "\n",
        "    model = Model(inputs=input_layers, outputs=[pred])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "make_binary_classifier:tabular"
      },
      "outputs": [],
      "source": [
        "TRANSFORM_ARTIFACTS_DIR = metadata[\"transform_artifacts_dir\"]\n",
        "tft_output = tft.TFTransformOutput(TRANSFORM_ARTIFACTS_DIR)\n",
        "\n",
        "metaparams = {\"hidden_units\": [128, 64]}\n",
        "aip.log_params(metaparams)\n",
        "\n",
        "model = create_binary_classifier(\n",
        "    input_layers,\n",
        "    tft_output,\n",
        "    metaparams,\n",
        "    numeric_features=metadata[\"numeric_features\"],\n",
        "    categorical_features=metadata[\"categorical_features\"],\n",
        "    embedding_features=metadata[\"embedding_features\"],\n",
        ")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualize_model"
      },
      "source": [
        "#### Visualize the model architecture\n",
        "\n",
        "Next, visualize the architecture of the custom model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualize_model"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(model, show_shapes=True, show_dtype=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_model:gcs"
      },
      "source": [
        "### Save model artifacts\n",
        "\n",
        "Next, save the model artifacts to your Cloud Storage bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_model:gcs"
      },
      "outputs": [],
      "source": [
        "MODEL_DIR = f\"{BUCKET_NAME}/base_model\"\n",
        "\n",
        "model.save(MODEL_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upload_model:vertex,base_model"
      },
      "source": [
        "### Upload the local model to a Vertex AI Model resource\n",
        "\n",
        "Next, you upload your local custom model artifacts to Vertex AI to convert into a managed Vertex AI Model resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upload_model:vertex,base_model"
      },
      "outputs": [],
      "source": [
        "vertex_custom_model = aip.Model.upload(\n",
        "    display_name=\"chicago_\" + TIMESTAMP,\n",
        "    artifact_uri=MODEL_DIR,\n",
        "    serving_container_image_uri=DEPLOY_IMAGE,\n",
        "    labels={\"base_model\": \"1\"},\n",
        "    sync=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "construct_training_package"
      },
      "source": [
        "### Construct the training package\n",
        "\n",
        "#### Package layout\n",
        "\n",
        "Before you start training, you will look at how a Python package is assembled for a custom training job. When unarchived, the package contains the following directory/file layout.\n",
        "\n",
        "- PKG-INFO\n",
        "- README.md\n",
        "- setup.cfg\n",
        "- setup.py\n",
        "- trainer\n",
        "  - \\_\\_init\\_\\_.py\n",
        "  - task.py\n",
        "  - other Python scripts\n",
        "\n",
        "The files `setup.cfg` and `setup.py` are the instructions for installing the package into the operating environment of the Docker image.\n",
        "\n",
        "The file `trainer/task.py` is the Python script for executing the custom training job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "construct_training_package"
      },
      "outputs": [],
      "source": [
        "# Make folder for Python training script\n",
        "! rm -rf custom\n",
        "! mkdir custom\n",
        "\n",
        "# Add package information\n",
        "! touch custom/README.md\n",
        "\n",
        "setup_cfg = \"[egg_info]\\n\\ntag_build =\\n\\ntag_date = 0\"\n",
        "! echo \"$setup_cfg\" > custom/setup.cfg\n",
        "\n",
        "setup_py = \"import setuptools\\n\\nsetuptools.setup(\\n\\n    install_requires=[\\n\\n        'google-cloud-aiplatform',\\n\\n        'cloudml-hypertune',\\n\\n        'tensorflow_datasets==1.3.0',\\n\\n        'tensorflow_data_validation==1.2',\\n\\n    ],\\n\\n    packages=setuptools.find_packages())\"\n",
        "! echo \"$setup_py\" > custom/setup.py\n",
        "\n",
        "pkg_info = \"Metadata-Version: 1.0\\n\\nName: Chicago Taxi tabular binary classifier\\n\\nVersion: 0.0.0\\n\\nSummary: Demostration training script\\n\\nHome-page: www.google.com\\n\\nAuthor: Google\\n\\nAuthor-email: cdpe@google.com\\n\\nLicense: Public\\n\\nDescription: Demo\\n\\nPlatform: Vertex AI\"\n",
        "! echo \"$pkg_info\" > custom/PKG-INFO\n",
        "\n",
        "# Make the training subfolder\n",
        "! mkdir custom/trainer\n",
        "! touch custom/trainer/__init__.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "transform_feature_spec"
      },
      "source": [
        "#### Get feature specification for the preprocessed data\n",
        "\n",
        "Next, create the feature specification for the preprocessed data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "transform_feature_spec"
      },
      "outputs": [],
      "source": [
        "transform_feature_spec = tft_output.transformed_feature_spec()\n",
        "print(transform_feature_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "read_tfrecords_func"
      },
      "source": [
        "### Load the transformed data into a tf.data.Dataset\n",
        "\n",
        "Next, you load the gzip TFRecords on Cloud Storage storage into a `tf.data.Dataset` generator. These functions are re-used when training the custom model using `Vertex Training`, so you save them to the python training package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "read_tfrecords_func"
      },
      "outputs": [],
      "source": [
        "%%writefile custom/trainer/data.py\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def _gzip_reader_fn(filenames):\n",
        "    \"\"\"Small utility returning a record reader that can read gzip'ed files.\"\"\"\n",
        "    return tf.data.TFRecordDataset(filenames, compression_type=\"GZIP\")\n",
        "\n",
        "\n",
        "def get_dataset(file_pattern, feature_spec, label_column, batch_size=200):\n",
        "    \"\"\"Generates features and label for tuning/training.\n",
        "    Args:\n",
        "      file_pattern: input tfrecord file pattern.\n",
        "      feature_spec: a dictionary of feature specifications.\n",
        "      batch_size: representing the number of consecutive elements of returned\n",
        "        dataset to combine in a single batch\n",
        "    Returns:\n",
        "      A dataset that contains (features, indices) tuple where features is a\n",
        "        dictionary of Tensors, and indices is a single Tensor of label indices.\n",
        "    \"\"\"\n",
        "\n",
        "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
        "        file_pattern=file_pattern,\n",
        "        batch_size=batch_size,\n",
        "        features=feature_spec,\n",
        "        label_key=label_column,\n",
        "        reader=_gzip_reader_fn,\n",
        "        num_epochs=1,\n",
        "        drop_final_batch=True,\n",
        "    )\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "read_tfrecords"
      },
      "outputs": [],
      "source": [
        "from custom.trainer import data\n",
        "\n",
        "TRANSFORMED_DATA_PREFIX = metadata[\"transformed_data_prefix\"]\n",
        "LABEL_COLUMN = metadata[\"label_column\"]\n",
        "\n",
        "train_data_file_pattern = TRANSFORMED_DATA_PREFIX + \"/train/data-*.gz\"\n",
        "val_data_file_pattern = TRANSFORMED_DATA_PREFIX + \"/val/data-*.gz\"\n",
        "test_data_file_pattern = TRANSFORMED_DATA_PREFIX + \"/test/data-*.gz\"\n",
        "\n",
        "for input_features, target in data.get_dataset(\n",
        "    train_data_file_pattern, transform_feature_spec, LABEL_COLUMN, batch_size=3\n",
        ").take(1):\n",
        "    for key in input_features:\n",
        "        print(\n",
        "            f\"{key} {input_features[key].dtype}: {input_features[key].numpy().tolist()}\"\n",
        "        )\n",
        "    print(f\"target: {target.numpy().tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_model_input"
      },
      "source": [
        "#### Test the model architecture with transformed input\n",
        "\n",
        "Next, test the model architecture with a sample of the transformed training input.\n",
        "\n",
        "*Note:* Since the model is untrained, the predictions should be random. Since this is a binary classifier, expect the predicted results ~0.5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_model_input"
      },
      "outputs": [],
      "source": [
        "model(input_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train_model_func"
      },
      "source": [
        "## Develop and test the training scripts\n",
        "\n",
        "When experimenting, one typically develops and tests the training package locally, before moving to training in the cloud.\n",
        "\n",
        "### Create training script\n",
        "\n",
        "Next, you write the Python script for compiling and training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_model_func"
      },
      "outputs": [],
      "source": [
        "%%writefile custom/trainer/train.py\n",
        "\n",
        "from trainer import data\n",
        "import tensorflow as tf\n",
        "import logging\n",
        "from hypertune import HyperTune\n",
        "\n",
        "def compile(model, hyperparams):\n",
        "    ''' Compile the model '''\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=hyperparams[\"learning_rate\"])\n",
        "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "    metrics = [tf.keras.metrics.BinaryAccuracy(name=\"accuracy\")]\n",
        "\n",
        "    model.compile(optimizer=optimizer,loss=loss, metrics=metrics)\n",
        "    return model\n",
        "\n",
        "def warmup(\n",
        "    model,\n",
        "    hyperparams,\n",
        "    train_data_dir,\n",
        "    label_column,\n",
        "    transformed_feature_spec\n",
        "):\n",
        "    ''' Warmup the initialized model weights '''\n",
        "\n",
        "    train_dataset = data.get_dataset(\n",
        "        train_data_dir,\n",
        "        transformed_feature_spec,\n",
        "        label_column,\n",
        "        batch_size=hyperparams[\"batch_size\"],\n",
        "    )\n",
        "\n",
        "    lr_inc = (hyperparams['end_learning_rate'] - hyperparams['start_learning_rate']) / hyperparams['num_epochs']\n",
        "\n",
        "    def scheduler(epoch, lr):\n",
        "        if epoch == 0:\n",
        "            return hyperparams['start_learning_rate']\n",
        "        return lr + lr_inc\n",
        "\n",
        "\n",
        "    callbacks = [tf.keras.callbacks.LearningRateScheduler(scheduler)]\n",
        "\n",
        "    logging.info(\"Model warmup started...\")\n",
        "    history = model.fit(\n",
        "            train_dataset,\n",
        "            epochs=hyperparams[\"num_epochs\"],\n",
        "            steps_per_epoch=hyperparams[\"steps\"],\n",
        "            callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    logging.info(\"Model warmup completed.\")\n",
        "    return history\n",
        "\n",
        "\n",
        "def train(\n",
        "    model,\n",
        "    hyperparams,\n",
        "    train_data_dir,\n",
        "    val_data_dir,\n",
        "    label_column,\n",
        "    transformed_feature_spec,\n",
        "    log_dir,\n",
        "    tuning=False\n",
        "):\n",
        "    ''' Train the model '''\n",
        "\n",
        "    train_dataset = data.get_dataset(\n",
        "        train_data_dir,\n",
        "        transformed_feature_spec,\n",
        "        label_column,\n",
        "        batch_size=hyperparams[\"batch_size\"],\n",
        "    )\n",
        "\n",
        "    val_dataset = data.get_dataset(\n",
        "        val_data_dir,\n",
        "        transformed_feature_spec,\n",
        "        label_column,\n",
        "        batch_size=hyperparams[\"batch_size\"],\n",
        "    )\n",
        "\n",
        "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=hyperparams[\"early_stop\"][\"monitor\"], patience=hyperparams[\"early_stop\"][\"patience\"], restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    callbacks = [early_stop]\n",
        "\n",
        "    if log_dir:\n",
        "        tensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
        "\n",
        "        callbacks = callbacks.append(tensorboard)\n",
        "\n",
        "    if tuning:\n",
        "        # Instantiate the HyperTune reporting object\n",
        "        hpt = HyperTune()\n",
        "\n",
        "        # Reporting callback\n",
        "        class HPTCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "            def on_epoch_end(self, epoch, logs=None):\n",
        "                hpt.report_hyperparameter_tuning_metric(\n",
        "                    hyperparameter_metric_tag='val_loss',\n",
        "                    metric_value=logs['val_loss'],\n",
        "                    global_step=epoch\n",
        "                )\n",
        "\n",
        "        if not callbacks:\n",
        "            callbacks = []\n",
        "        callbacks.append(HPTCallback())\n",
        "\n",
        "    logging.info(\"Model training started...\")\n",
        "    history = model.fit(\n",
        "            train_dataset,\n",
        "            epochs=hyperparams[\"num_epochs\"],\n",
        "            validation_data=val_dataset,\n",
        "            callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    logging.info(\"Model training completed.\")\n",
        "    return history\n",
        "\n",
        "def evaluate(\n",
        "    model,\n",
        "    hyperparams,\n",
        "    test_data_dir,\n",
        "    label_column,\n",
        "    transformed_feature_spec\n",
        "):\n",
        "    logging.info(\"Model evaluation started...\")\n",
        "    test_dataset = data.get_dataset(\n",
        "        test_data_dir,\n",
        "        transformed_feature_spec,\n",
        "        label_column,\n",
        "        hyperparams[\"batch_size\"],\n",
        "    )\n",
        "\n",
        "    evaluation_metrics = model.evaluate(test_dataset)\n",
        "    logging.info(\"Model evaluation completed.\")\n",
        "\n",
        "    return evaluation_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train_model_local"
      },
      "source": [
        "### Train the model locally\n",
        "\n",
        "Next, test the training package locally, by training with just a few epochs:\n",
        "\n",
        "- `num_epochs`: The number of epochs to pass to the training package.\n",
        "- `compile()`: Compile the model for training.\n",
        "- `warmup()`: Warmup the initialized model weights.\n",
        "- `train()`: Train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_model_local"
      },
      "outputs": [],
      "source": [
        "os.chdir(\"custom\")\n",
        "\n",
        "import logging\n",
        "\n",
        "from trainer import train\n",
        "\n",
        "TENSORBOARD_LOG_DIR = \"./logs\"\n",
        "\n",
        "logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "hyperparams = {}\n",
        "hyperparams[\"learning_rate\"] = 0.01\n",
        "aip.log_params(hyperparams)\n",
        "\n",
        "train.compile(model, hyperparams)\n",
        "\n",
        "warmupparams = {}\n",
        "warmupparams[\"start_learning_rate\"] = 0.0001\n",
        "warmupparams[\"end_learning_rate\"] = 0.01\n",
        "warmupparams[\"num_epochs\"] = 4\n",
        "warmupparams[\"batch_size\"] = 64\n",
        "warmupparams[\"steps\"] = 50\n",
        "aip.log_params(warmupparams)\n",
        "\n",
        "train.warmup(\n",
        "    model, warmupparams, train_data_file_pattern, LABEL_COLUMN, transform_feature_spec\n",
        ")\n",
        "\n",
        "trainparams = {}\n",
        "trainparams[\"num_epochs\"] = 5\n",
        "trainparams[\"batch_size\"] = 64\n",
        "trainparams[\"early_stop\"] = {\"monitor\": \"val_loss\", \"patience\": 5}\n",
        "aip.log_params(trainparams)\n",
        "\n",
        "train.train(\n",
        "    model,\n",
        "    trainparams,\n",
        "    train_data_file_pattern,\n",
        "    val_data_file_pattern,\n",
        "    LABEL_COLUMN,\n",
        "    transform_feature_spec,\n",
        "    TENSORBOARD_LOG_DIR,\n",
        ")\n",
        "\n",
        "os.chdir(\"..\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eval_model_local"
      },
      "source": [
        "### Evaluate the model locally\n",
        "\n",
        "Next, test the evaluation portion of the training package:\n",
        "\n",
        "\n",
        "- `evaluate()`: Evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eval_model_local"
      },
      "outputs": [],
      "source": [
        "os.chdir(\"custom\")\n",
        "\n",
        "from trainer import train\n",
        "\n",
        "evalparams = {}\n",
        "evalparams[\"batch_size\"] = 64\n",
        "\n",
        "metrics = {}\n",
        "metrics[\"loss\"], metrics[\"acc\"] = train.evaluate(\n",
        "    model, evalparams, test_data_file_pattern, LABEL_COLUMN, transform_feature_spec\n",
        ")\n",
        "print(\"ACC\", metrics[\"acc\"], \"LOSS\", metrics[\"loss\"])\n",
        "aip.log_metrics(metrics)\n",
        "\n",
        "os.chdir(\"..\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_model_get"
      },
      "source": [
        "### Retrieve model from Vertex AI\n",
        "\n",
        "Next, create the Python script to retrieve your experimental model from Vertex AI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_model_get"
      },
      "outputs": [],
      "source": [
        "%%writefile custom/trainer/model.py\n",
        "\n",
        "import google.cloud.aiplatform as aip\n",
        "\n",
        "def get(model_id):\n",
        "    model = aip.Model(model_id)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_task_py"
      },
      "source": [
        "### Create the task script for the Python training package\n",
        "\n",
        "Next, you create the `task.py` script for driving the training package. Some noteable steps include:\n",
        "\n",
        "- Command-line arguments:\n",
        "    - `model-id`: The resource ID of the `Model` resource you built during experimenting. This is the untrained model architecture.\n",
        "    - `dataset-id`: The resource ID of the `Dataset` resource to use for training.\n",
        "    - `experiment`: The name of the experiment.\n",
        "    - `run`: The name of the run within this experiment.\n",
        "    - `tensorboard-logdir`: The logging directory for Vertex AI Tensorboard.\n",
        "\n",
        "\n",
        "- `get_data()`:\n",
        "    - Loads the Dataset resource into memory.\n",
        "    - Obtains the user metadata from the Dataset resource.\n",
        "    - From the metadata, obtain location of transformed data, transformation function and name of label column\n",
        "\n",
        "\n",
        "- `get_model()`:\n",
        "    - Loads the Model resource into memory.\n",
        "    - Obtains location of model artifacts of the model architecture.\n",
        "    - Loads the model architecture.\n",
        "    - Compiles the model.\n",
        "\n",
        "\n",
        "- `warmup_model()`:\n",
        "   - Warms up the initialized model weights\n",
        "\n",
        "\n",
        "- `train_model()`:\n",
        "    - Train the model.\n",
        "\n",
        "\n",
        "- `evaluate_model()`:\n",
        "    - Evaluates the model.\n",
        "    - Saves evaluation metrics to Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_task_py"
      },
      "outputs": [],
      "source": [
        "%%writefile custom/trainer/task.py\n",
        "import os\n",
        "import argparse\n",
        "import logging\n",
        "import json\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "import google.cloud.aiplatform as aip\n",
        "\n",
        "from trainer import data\n",
        "from trainer import model as model_\n",
        "from trainer import train\n",
        "try:\n",
        "    from trainer import serving\n",
        "except:\n",
        "    pass\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--model-dir', dest='model_dir',\n",
        "                    default=os.getenv('AIP_MODEL_DIR'), type=str, help='Model dir.')\n",
        "parser.add_argument('--model-id', dest='model_id',\n",
        "                    default=None, type=str, help='Vertex Model ID.')\n",
        "parser.add_argument('--dataset-id', dest='dataset_id',\n",
        "                    default=None, type=str, help='Vertex Dataset ID.')\n",
        "parser.add_argument('--lr', dest='lr',\n",
        "                    default=0.001, type=float,\n",
        "                    help='Learning rate.')\n",
        "parser.add_argument('--start_lr', dest='start_lr',\n",
        "                    default=0.0001, type=float,\n",
        "                    help='Starting learning rate.')\n",
        "parser.add_argument('--epochs', dest='epochs',\n",
        "                    default=20, type=int,\n",
        "                    help='Number of epochs.')\n",
        "parser.add_argument('--steps', dest='steps',\n",
        "                    default=200, type=int,\n",
        "                    help='Number of steps per epoch.')\n",
        "parser.add_argument('--batch_size', dest='batch_size',\n",
        "                    default=16, type=int,\n",
        "                    help='Batch size.')\n",
        "parser.add_argument('--distribute', dest='distribute', type=str, default='single',\n",
        "                    help='distributed training strategy')\n",
        "parser.add_argument('--tensorboard-log-dir', dest='tensorboard_log_dir',\n",
        "                    default=os.getenv('AIP_TENSORBOARD_LOG_DIR'), type=str,\n",
        "                    help='Output file for tensorboard logs')\n",
        "parser.add_argument('--experiment', dest='experiment',\n",
        "                    default=None, type=str,\n",
        "                    help='Name of experiment')\n",
        "parser.add_argument('--project', dest='project',\n",
        "                    default=None, type=str,\n",
        "                    help='Name of project')\n",
        "parser.add_argument('--run', dest='run',\n",
        "                    default=None, type=str,\n",
        "                    help='Name of run in experiment')\n",
        "parser.add_argument('--evaluate', dest='evaluate',\n",
        "                    default=False, type=bool,\n",
        "                    help='Whether to perform evaluation')\n",
        "parser.add_argument('--serving', dest='serving',\n",
        "                    default=False, type=bool,\n",
        "                    help='Whether to attach the serving function')\n",
        "parser.add_argument('--tuning', dest='tuning',\n",
        "                    default=False, type=bool,\n",
        "                    help='Whether to perform hyperparameter tuning')\n",
        "parser.add_argument('--warmup', dest='warmup',\n",
        "                    default=False, type=bool,\n",
        "                    help='Whether to perform warmup weight initialization')\n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "logging.getLogger().setLevel(logging.INFO)\n",
        "logging.info('DEVICES'  + str(device_lib.list_local_devices()))\n",
        "\n",
        "# Single Machine, single compute device\n",
        "if args.distribute == 'single':\n",
        "    if tf.test.is_gpu_available():\n",
        "        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
        "    else:\n",
        "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
        "    logging.info(\"Single device training\")\n",
        "# Single Machine, multiple compute device\n",
        "elif args.distribute == 'mirrored':\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "    logging.info(\"Mirrored Strategy distributed training\")\n",
        "# Multi Machine, multiple compute device\n",
        "elif args.distribute == 'multiworker':\n",
        "    strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
        "    logging.info(\"Multi-worker Strategy distributed training\")\n",
        "    logging.info('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', 'Not found')))\n",
        "logging.info('num_replicas_in_sync = {}'.format(strategy.num_replicas_in_sync))\n",
        "\n",
        "# Initialize the run for this experiment\n",
        "if args.experiment:\n",
        "    logging.info(\"Initialize experiment: {}\".format(args.experiment))\n",
        "    aip.init(experiment=args.experiment, project=args.project)\n",
        "    aip.start_run(args.run)\n",
        "\n",
        "metadata = {}\n",
        "\n",
        "def get_data():\n",
        "    ''' Get the preprocessed training data '''\n",
        "    global train_data_file_pattern, val_data_file_pattern, test_data_file_pattern\n",
        "    global label_column, transform_feature_spec, metadata\n",
        "\n",
        "    dataset = aip.TabularDataset(args.dataset_id)\n",
        "    METADATA = 'gs://' + dataset.labels['user_metadata'] + \"/metadata.jsonl\"\n",
        "\n",
        "    with tf.io.gfile.GFile(METADATA, \"r\") as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    TRANSFORMED_DATA_PREFIX = metadata['transformed_data_prefix']\n",
        "    label_column = metadata['label_column']\n",
        "\n",
        "    train_data_file_pattern = TRANSFORMED_DATA_PREFIX + '/train/data-*.gz'\n",
        "    val_data_file_pattern = TRANSFORMED_DATA_PREFIX + '/val/data-*.gz'\n",
        "    test_data_file_pattern = TRANSFORMED_DATA_PREFIX + '/test/data-*.gz'\n",
        "\n",
        "    TRANSFORM_ARTIFACTS_DIR = metadata['transform_artifacts_dir']\n",
        "    tft_output = tft.TFTransformOutput(TRANSFORM_ARTIFACTS_DIR)\n",
        "    transform_feature_spec = tft_output.transformed_feature_spec()\n",
        "\n",
        "def get_model():\n",
        "    ''' Get the untrained model architecture '''\n",
        "    global model_artifacts\n",
        "\n",
        "    vertex_model = model_.get(args.model_id)\n",
        "    model_artifacts = vertex_model.gca_resource.artifact_uri\n",
        "    model = tf.keras.models.load_model(model_artifacts)\n",
        "\n",
        "    # Compile the model\n",
        "    hyperparams = {}\n",
        "    hyperparams[\"learning_rate\"] = args.lr\n",
        "    if args.experiment:\n",
        "        aip.log_params(hyperparams)\n",
        "\n",
        "    metadata.update(hyperparams)\n",
        "    with tf.io.gfile.GFile(os.path.join(args.model_dir, \"metrics.txt\"), \"w\") as f:\n",
        "        f.write(json.dumps(metadata))\n",
        "\n",
        "    train.compile(model, hyperparams)\n",
        "    return model\n",
        "\n",
        "def warmup_model(model):\n",
        "    ''' Warmup the initialized model weights '''\n",
        "    warmupparams = {}\n",
        "    warmupparams[\"num_epochs\"] = args.epochs\n",
        "    warmupparams[\"batch_size\"] = args.batch_size\n",
        "    warmupparams[\"steps\"] = args.steps\n",
        "    warmupparams[\"start_learning_rate\"] = args.start_lr\n",
        "    warmupparams[\"end_learning_rate\"] = args.lr\n",
        "\n",
        "    train.warmup(model, warmupparams, train_data_file_pattern, label_column, transform_feature_spec)\n",
        "    return model\n",
        "\n",
        "def train_model(model):\n",
        "    ''' Train the model '''\n",
        "    trainparams = {}\n",
        "    trainparams[\"num_epochs\"] = args.epochs\n",
        "    trainparams[\"batch_size\"] = args.batch_size\n",
        "    trainparams[\"early_stop\"] = {\"monitor\": \"val_loss\", \"patience\": 5}\n",
        "    if args.experiment:\n",
        "        aip.log_params(trainparams)\n",
        "\n",
        "    metadata.update(trainparams)\n",
        "    with tf.io.gfile.GFile(os.path.join(args.model_dir, \"metrics.txt\"), \"w\") as f:\n",
        "        f.write(json.dumps(metadata))\n",
        "\n",
        "    train.train(model, trainparams, train_data_file_pattern, val_data_file_pattern, label_column, transform_feature_spec, args.tensorboard_log_dir, args.tuning)\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model):\n",
        "    ''' Evaluate the model '''\n",
        "    evalparams = {}\n",
        "    evalparams[\"batch_size\"] = args.batch_size\n",
        "    metrics = train.evaluate(model, evalparams, test_data_file_pattern, label_column, transform_feature_spec)\n",
        "\n",
        "    metadata.update({'metrics': metrics})\n",
        "    with tf.io.gfile.GFile(os.path.join(args.model_dir, \"metrics.txt\"), \"w\") as f:\n",
        "        f.write(json.dumps(metadata))\n",
        "\n",
        "get_data()\n",
        "with strategy.scope():\n",
        "    model = get_model()\n",
        "\n",
        "if args.warmup:\n",
        "    model = warmup_model(model)\n",
        "else:\n",
        "    model = train_model(model)\n",
        "\n",
        "if args.evaluate:\n",
        "    evaluate_model(model)\n",
        "\n",
        "if args.serving:\n",
        "    logging.info('Save serving model to: ' + args.model_dir)\n",
        "    serving.construct_serving_model(\n",
        "        model=model,\n",
        "        serving_model_dir=args.model_dir,\n",
        "        metadata=metadata\n",
        "    )\n",
        "elif args.warmup:\n",
        "    logging.info('Save warmed up model to: ' + model_artifacts)\n",
        "    model.save(model_artifacts)\n",
        "else:\n",
        "    logging.info('Save trained model to: ' + args.model_dir)\n",
        "    model.save(args.model_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_package_locally"
      },
      "source": [
        "### Test training package locally\n",
        "\n",
        "Next, test your completed training package locally with just a few epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_package_locally"
      },
      "outputs": [],
      "source": [
        "DATASET_ID = dataset.resource_name\n",
        "MODEL_ID = vertex_custom_model.resource_name\n",
        "!cd custom; python3 -m trainer.task --model-id={MODEL_ID} --dataset-id={DATASET_ID} --experiment='chicago' --run='test' --project={PROJECT_ID} --epochs=5 --model-dir=/tmp --evaluate=True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "warmup_base_model"
      },
      "source": [
        "### Warmup training\n",
        "\n",
        "Now that you have tested the training scripts, you perform warmup training on the base model. Warmup training is used to stabilize the weight initialization. By doing so, each subsequent training and tuning of the model architecture will start with the same stabilized weight initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "warmup_base_model"
      },
      "outputs": [],
      "source": [
        "MODEL_DIR = f\"{BUCKET_NAME}/base_model\"\n",
        "\n",
        "!cd custom; python3 -m trainer.task --model-id={MODEL_ID} --dataset-id={DATASET_ID} --project={PROJECT_ID} --epochs=5 --steps=300 --batch_size=16 --lr=0.01 --start_lr=0.0001 --model-dir={MODEL_DIR} --warmup=True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mirrored_intro"
      },
      "source": [
        "## Mirrored Strategy\n",
        "\n",
        "When training on a single VM, one can either train was a single compute device or with multiple compute devices on the same VM. With Vertex AI Distributed Training you can specify both the number of compute devices for the VM instance and type of compute devices: CPU, GPU.\n",
        "\n",
        "Vertex AI Distributed Training supports `tf.distribute.MirroredStrategy' for TensorFlow models. To enable training across multiple compute devices on the same VM, you do the following additional steps in your Python training script:\n",
        "\n",
        "1. Set the tf.distribute.MirrorStrategy\n",
        "2. Compile the model within the scope of tf.distribute.MirrorStrategy. *Note:* Tells MirroredStrategy which variables to mirror across your compute devices.\n",
        "3. Increase the batch size for each compute device to num_devices * batch size.\n",
        "\n",
        "During transitions, the distribution of batches will be synchronized as well as the updates to the model parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_custom_pp_training_job:mbsdk"
      },
      "source": [
        "### Create and run custom training job\n",
        "\n",
        "\n",
        "To train a custom model, you perform two steps: 1) create a custom training job, and 2) run the job.\n",
        "\n",
        "#### Create custom training job\n",
        "\n",
        "A custom training job is created with the `CustomTrainingJob` class, with the following parameters:\n",
        "\n",
        "- `display_name`: The human readable name for the custom training job.\n",
        "- `container_uri`: The training container image.\n",
        "\n",
        "- `python_package_gcs_uri`: The location of the Python training package as a tarball.\n",
        "- `python_module_name`: The relative path to the training script in the Python package.\n",
        "- `model_serving_container_uri`: The container image for deploying the model.\n",
        "\n",
        "*Note:* There is no requirements parameter. You specify any requirements in the `setup.py` script in your Python package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_custom_pp_training_job:mbsdk"
      },
      "outputs": [],
      "source": [
        "DISPLAY_NAME = \"chicago_\" + TIMESTAMP\n",
        "\n",
        "job = aip.CustomPythonPackageTrainingJob(\n",
        "    display_name=DISPLAY_NAME,\n",
        "    python_package_gcs_uri=f\"{BUCKET_NAME}/trainer_chicago.tar.gz\",\n",
        "    python_module_name=\"trainer.task\",\n",
        "    container_uri=TRAIN_IMAGE,\n",
        "    model_serving_container_image_uri=DEPLOY_IMAGE,\n",
        "    project=PROJECT_ID,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup:trainer"
      },
      "outputs": [],
      "source": [
        "! rm -rf custom/logs\n",
        "! rm -rf custom/trainer/__pycache__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tarball_training_script"
      },
      "source": [
        "#### Store training script on your Cloud Storage bucket\n",
        "\n",
        "Next, you package the training folder into a compressed tar ball, and then store it in your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tarball_training_script"
      },
      "outputs": [],
      "source": [
        "! rm -f custom.tar custom.tar.gz\n",
        "! tar cvf custom.tar custom\n",
        "! gzip custom.tar\n",
        "! gsutil cp custom.tar.gz $BUCKET_NAME/trainer_chicago.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_custom_pp_training_job:test"
      },
      "source": [
        "#### Run the custom Python package training job\n",
        "\n",
        "Next, you run the custom job to start the training job by invoking the method `run()`. The parameters are the same as when running a CustomTrainingJob.\n",
        "\n",
        "*Note:* The parameter service_account is set so that the initializing experiment step `aip.init(experiment=\"...\")` has necessarily permission to access the Vertex AI Metadata Store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_custom_pp_training_job:test"
      },
      "outputs": [],
      "source": [
        "MODEL_DIR = BUCKET_NAME + \"/testing\"\n",
        "\n",
        "CMDARGS = [\n",
        "    \"--epochs=5\",\n",
        "    \"--batch_size=16\",\n",
        "    \"--distribute=mirrored\",\n",
        "    \"--experiment=chicago\",\n",
        "    \"--run=test\",\n",
        "    \"--project=\" + PROJECT_ID,\n",
        "    \"--model-id=\" + MODEL_ID,\n",
        "    \"--dataset-id=\" + DATASET_ID,\n",
        "]\n",
        "\n",
        "model = job.run(\n",
        "    model_display_name=\"chicago_\" + TIMESTAMP,\n",
        "    args=CMDARGS,\n",
        "    replica_count=1,\n",
        "    machine_type=TRAIN_COMPUTE,\n",
        "    accelerator_type=TRAIN_GPU.name,\n",
        "    accelerator_count=TRAIN_NGPU,\n",
        "    base_output_dir=MODEL_DIR,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    tensorboard=tensorboard_resource_name,\n",
        "    sync=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "delete_job"
      },
      "source": [
        "### Delete a custom training job\n",
        "\n",
        "After a training job is completed, you can delete the training job with the method `delete()`.  Prior to completion, a training job can be canceled with the method `cancel()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "delete_job"
      },
      "outputs": [],
      "source": [
        "job.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_delete:mbsdk"
      },
      "source": [
        "#### Delete the model\n",
        "\n",
        "The method 'delete()' will delete the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model_delete:mbsdk"
      },
      "outputs": [],
      "source": [
        "model.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp_tuning"
      },
      "source": [
        "## Hyperparameter tuning\n",
        "\n",
        "Next, you perform hyperparameter tuning with the training package. The training package has some additions that make the same package usable for both hyperparameter tuning, as well as local testing and full cloud training:\n",
        "\n",
        "- Command-Line:\n",
        "  - `tuning`: indicates to use the HyperTune service as a callback during training.\n",
        "\n",
        "\n",
        "- `train()`: If tuning is set, creates and adds a callback to HyperTune service."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train_custom_job_machine_specification"
      },
      "source": [
        "### Prepare your machine specification\n",
        "\n",
        "Now define the machine specification for your custom training job. This tells Vertex what type of machine instance to provision for the training.\n",
        "  - `machine_type`: The type of GCP instance to provision -- e.g., n1-standard-8.\n",
        "  - `accelerator_type`: The type, if any, of hardware accelerator. In this tutorial if you previously set the variable `TRAIN_GPU != None`, you are using a GPU; otherwise you will use a CPU.\n",
        "  - `accelerator_count`: The number of accelerators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_custom_job_machine_specification"
      },
      "outputs": [],
      "source": [
        "if TRAIN_GPU:\n",
        "    machine_spec = {\n",
        "        \"machine_type\": TRAIN_COMPUTE,\n",
        "        \"accelerator_type\": TRAIN_GPU,\n",
        "        \"accelerator_count\": TRAIN_NGPU,\n",
        "    }\n",
        "else:\n",
        "    machine_spec = {\"machine_type\": TRAIN_COMPUTE, \"accelerator_count\": 0}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train_custom_job_disk_specification"
      },
      "source": [
        "### Prepare your disk specification\n",
        "\n",
        "(optional) Now define the disk specification for your custom training job. This tells Vertex what type and size of disk to provision in each machine instance for the training.\n",
        "\n",
        "  - `boot_disk_type`: Either SSD or Standard. SSD is faster, and Standard is less expensive. Defaults to SSD.\n",
        "  - `boot_disk_size_gb`: Size of disk in GB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_custom_job_disk_specification"
      },
      "outputs": [],
      "source": [
        "DISK_TYPE = \"pd-ssd\"  # [ pd-ssd, pd-standard]\n",
        "DISK_SIZE = 200  # GB\n",
        "\n",
        "disk_spec = {\"boot_disk_type\": DISK_TYPE, \"boot_disk_size_gb\": DISK_SIZE}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "worker_pool_hpt"
      },
      "source": [
        "### Define worker pool specification for hyperparameter tuning job\n",
        "\n",
        "Next, define the worker pool specification. Note that we plan to tune the learning rate and batch size, so you do not pass them as command-line arguments (omitted). The Vertex AI Hyperparameter Tuning service will pick values for both learning rate and batch size during trials, which it will pass along as command-line arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "worker_pool_hpt"
      },
      "outputs": [],
      "source": [
        "CMDARGS = [\n",
        "    \"--epochs=5\",\n",
        "    \"--distribute=mirrored\",\n",
        "    # \"--experiment=chicago\",\n",
        "    # \"--run=tune\",\n",
        "    # \"--project=\" + PROJECT_ID,\n",
        "    \"--model-id=\" + MODEL_ID,\n",
        "    \"--dataset-id=\" + DATASET_ID,\n",
        "    \"--tuning=True\",\n",
        "]\n",
        "\n",
        "worker_pool_spec = [\n",
        "    {\n",
        "        \"replica_count\": 1,\n",
        "        \"machine_spec\": machine_spec,\n",
        "        \"disk_spec\": disk_spec,\n",
        "        \"python_package_spec\": {\n",
        "            \"executor_image_uri\": TRAIN_IMAGE,\n",
        "            \"package_uris\": [BUCKET_NAME + \"/trainer_chicago.tar.gz\"],\n",
        "            \"python_module\": \"trainer.task\",\n",
        "            \"args\": CMDARGS,\n",
        "        },\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_custom_job:mbsdk"
      },
      "source": [
        "## Create a custom job\n",
        "\n",
        "Use the class `CustomJob` to create a custom job, such as for hyperparameter tuning, with the following parameters:\n",
        "\n",
        "- `display_name`: A human readable name for the custom job.\n",
        "- `worker_pool_specs`: The specification for the corresponding VM instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_custom_job:mbsdk"
      },
      "outputs": [],
      "source": [
        "job = aip.CustomJob(\n",
        "    display_name=\"chicago_\" + TIMESTAMP, worker_pool_specs=worker_pool_spec\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_hpt_job:mbsdk"
      },
      "source": [
        "## Create a hyperparameter tuning job\n",
        "\n",
        "Use the class `HyperparameterTuningJob` to create a hyperparameter tuning job, with the following parameters:\n",
        "\n",
        "- `display_name`: A human readable name for the custom job.\n",
        "- `custom_job`: The worker pool spec from this custom job applies to the CustomJobs created in all the trials.\n",
        "- `metrics_spec`: The metrics to optimize. The dictionary key is the metric_id, which is reported by your training job, and the dictionary value is the optimization goal of the metric('minimize' or 'maximize').\n",
        "- `parameter_spec`: The parameters to optimize. The dictionary key is the metric_id, which is passed into your training job as a command line key word argument, and the dictionary value is the parameter specification of the metric.\n",
        "- `search_algorithm`: The search algorithm to use: `grid`, `random` and `None`. If `None` is specified, the `Vizier` service (Bayesian) is used.\n",
        "- `max_trial_count`: The maximum number of trials to perform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_hpt_job:stage2"
      },
      "outputs": [],
      "source": [
        "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
        "\n",
        "hpt_job = aip.HyperparameterTuningJob(\n",
        "    display_name=\"chicago_\" + TIMESTAMP,\n",
        "    custom_job=job,\n",
        "    metric_spec={\n",
        "        \"val_loss\": \"minimize\",\n",
        "    },\n",
        "    parameter_spec={\n",
        "        \"lr\": hpt.DoubleParameterSpec(min=0.001, max=0.1, scale=\"log\"),\n",
        "        \"batch_size\": hpt.DiscreteParameterSpec([16, 32, 64, 128, 256], scale=\"linear\"),\n",
        "    },\n",
        "    search_algorithm=None,\n",
        "    max_trial_count=8,\n",
        "    parallel_trial_count=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_hpt_job:mbsdk"
      },
      "source": [
        "## Run the hyperparameter tuning job\n",
        "\n",
        "Use the `run()` method to execute the hyperparameter tuning job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_hpt_job:mbsdk"
      },
      "outputs": [],
      "source": [
        "hpt_job.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "best_trial:mbsdk"
      },
      "source": [
        "### Best trial\n",
        "\n",
        "Now look at which trial was the best:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "best_trial:mbsdk"
      },
      "outputs": [],
      "source": [
        "best = (None, None, None, 0.0)\n",
        "for trial in hpt_job.trials:\n",
        "    # Keep track of the best outcome\n",
        "    if float(trial.final_measurement.metrics[0].value) > best[3]:\n",
        "        try:\n",
        "            best = (\n",
        "                trial.id,\n",
        "                float(trial.parameters[0].value),\n",
        "                float(trial.parameters[1].value),\n",
        "                float(trial.final_measurement.metrics[0].value),\n",
        "            )\n",
        "        except:\n",
        "            best = (\n",
        "                trial.id,\n",
        "                float(trial.parameters[0].value),\n",
        "                None,\n",
        "                float(trial.final_measurement.metrics[0].value),\n",
        "            )\n",
        "\n",
        "print(best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "delete_hpt_job"
      },
      "source": [
        "### Delete the hyperparameter tuning job\n",
        "\n",
        "The method 'delete()' will delete the hyperparameter tuning job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "delete_hpt_job"
      },
      "outputs": [],
      "source": [
        "hpt_job.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_hpt"
      },
      "source": [
        "### Save the best hyperparameter values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_hpt"
      },
      "outputs": [],
      "source": [
        "LR = best[2]\n",
        "BATCH_SIZE = int(best[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_custom_pp_training_job:mbsdk"
      },
      "source": [
        "### Create and run custom training job\n",
        "\n",
        "\n",
        "To train a custom model, you perform two steps: 1) create a custom training job, and 2) run the job.\n",
        "\n",
        "#### Create custom training job\n",
        "\n",
        "A custom training job is created with the `CustomTrainingJob` class, with the following parameters:\n",
        "\n",
        "- `display_name`: The human readable name for the custom training job.\n",
        "- `container_uri`: The training container image.\n",
        "\n",
        "- `python_package_gcs_uri`: The location of the Python training package as a tarball.\n",
        "- `python_module_name`: The relative path to the training script in the Python package.\n",
        "- `model_serving_container_uri`: The container image for deploying the model.\n",
        "\n",
        "*Note:* There is no requirements parameter. You specify any requirements in the `setup.py` script in your Python package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_custom_pp_training_job:mbsdk"
      },
      "outputs": [],
      "source": [
        "DISPLAY_NAME = \"chicago_\" + TIMESTAMP\n",
        "\n",
        "job = aip.CustomPythonPackageTrainingJob(\n",
        "    display_name=DISPLAY_NAME,\n",
        "    python_package_gcs_uri=f\"{BUCKET_NAME}/trainer_chicago.tar.gz\",\n",
        "    python_module_name=\"trainer.task\",\n",
        "    container_uri=TRAIN_IMAGE,\n",
        "    model_serving_container_image_uri=DEPLOY_IMAGE,\n",
        "    project=PROJECT_ID,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_custom_pp_training_job:full"
      },
      "source": [
        "#### Run the custom Python package training job\n",
        "\n",
        "Next, you run the custom job to start the training job by invoking the method `run()`. The parameters are the same as when running a CustomTrainingJob.\n",
        "\n",
        "*Note:* The parameter service_account is set so that the initializing experiment step `aip.init(experiment=\"...\")` has necessarily permission to access the Vertex AI Metadata Store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_custom_pp_training_job:full"
      },
      "outputs": [],
      "source": [
        "MODEL_DIR = BUCKET_NAME + \"/trained\"\n",
        "FULL_EPOCHS = 100\n",
        "\n",
        "CMDARGS = [\n",
        "    f\"--epochs={FULL_EPOCHS}\",\n",
        "    f\"--lr={LR}\",\n",
        "    f\"--batch_size={BATCH_SIZE}\",\n",
        "    \"--distribute=mirrored\",\n",
        "    \"--experiment=chicago\",\n",
        "    \"--run=full\",\n",
        "    \"--project=\" + PROJECT_ID,\n",
        "    \"--model-id=\" + MODEL_ID,\n",
        "    \"--dataset-id=\" + DATASET_ID,\n",
        "    \"--evaluate=True\",\n",
        "]\n",
        "\n",
        "model = job.run(\n",
        "    model_display_name=\"chicago_\" + TIMESTAMP,\n",
        "    args=CMDARGS,\n",
        "    replica_count=1,\n",
        "    machine_type=TRAIN_COMPUTE,\n",
        "    accelerator_type=TRAIN_GPU.name,\n",
        "    accelerator_count=TRAIN_NGPU,\n",
        "    base_output_dir=MODEL_DIR,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    tensorboard=tensorboard_resource_name,\n",
        "    sync=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "delete_job"
      },
      "source": [
        "### Delete a custom training job\n",
        "\n",
        "After a training job is completed, you can delete the training job with the method `delete()`.  Prior to completion, a training job can be canceled with the method `cancel()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "delete_job"
      },
      "outputs": [],
      "source": [
        "job.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "get_experiment"
      },
      "source": [
        "### Get the experiment results\n",
        "\n",
        "Next, you use the experiment name as a parameter to the method `get_experiment_df()` to get the results of the experiment as a pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "get_experiment"
      },
      "outputs": [],
      "source": [
        "EXPERIMENT_NAME = \"chicago\"\n",
        "\n",
        "experiment_df = aip.get_experiment_df()\n",
        "experiment_df = experiment_df[experiment_df.experiment_name == EXPERIMENT_NAME]\n",
        "experiment_df.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "review_builtin_metrics"
      },
      "source": [
        "## Review the custom model evaluation results\n",
        "\n",
        "Next, you review the evaluation metrics builtin into the training package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "review_builtin_metrics"
      },
      "outputs": [],
      "source": [
        "METRICS = MODEL_DIR + \"/model/metrics.txt\"\n",
        "! gsutil cat $METRICS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "delete_tensorboard"
      },
      "source": [
        "### Delete the TensorBoard instance\n",
        "\n",
        "Next, delete the TensorBoard instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "delete_tensorboard"
      },
      "outputs": [],
      "source": [
        "tensorboard.delete()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reload_model"
      },
      "outputs": [],
      "source": [
        "vertex_custom_model = model\n",
        "model = tf.keras.models.load_model(MODEL_DIR + \"/model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "serving_function:chicago"
      },
      "source": [
        "## Add a serving function\n",
        "\n",
        "Next, you add a serving function to your model for online and batch prediction. This allows prediction requests to be sent in raw format (unpreprocessed), either as a serialized TF.Example or JSONL object. The serving function will then preprocess the prediction request into the transformed format expected by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "serving_function:chicago"
      },
      "outputs": [],
      "source": [
        "%%writefile custom/trainer/serving.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_data_validation as tfdv\n",
        "import tensorflow_transform as tft\n",
        "import logging\n",
        "\n",
        "def _get_serve_features_fn(model, tft_output):\n",
        "    \"\"\"Returns a function that accept a dictionary of features and applies TFT.\"\"\"\n",
        "\n",
        "    model.tft_layer = tft_output.transform_features_layer()\n",
        "\n",
        "    @tf.function\n",
        "    def serve_features_fn(raw_features):\n",
        "        \"\"\"Returns the output to be used in the serving signature.\"\"\"\n",
        "\n",
        "        transformed_features = model.tft_layer(raw_features)\n",
        "        probabilities = model(transformed_features)\n",
        "        return {\"scores\": probabilities}\n",
        "\n",
        "\n",
        "    return serve_features_fn\n",
        "\n",
        "def _get_serve_tf_examples_fn(model, tft_output, feature_spec):\n",
        "    \"\"\"Returns a function that parses a serialized tf.Example and applies TFT.\"\"\"\n",
        "\n",
        "    model.tft_layer = tft_output.transform_features_layer()\n",
        "\n",
        "    @tf.function\n",
        "    def serve_tf_examples_fn(serialized_tf_examples):\n",
        "        \"\"\"Returns the output to be used in the serving signature.\"\"\"\n",
        "        for key in list(feature_spec.keys()):\n",
        "            if key not in features:\n",
        "                feature_spec.pop(key)\n",
        "\n",
        "        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
        "\n",
        "        transformed_features = model.tft_layer(parsed_features)\n",
        "        probabilities = model(transformed_features)\n",
        "        return {\"scores\": probabilities}\n",
        "\n",
        "    return serve_tf_examples_fn\n",
        "\n",
        "def construct_serving_model(\n",
        "    model, serving_model_dir, metadata\n",
        "):\n",
        "    global features\n",
        "\n",
        "    schema_location = metadata['schema']\n",
        "    features = metadata['numeric_features'] + metadata['categorical_features'] + metadata['embedding_features']\n",
        "    print(\"FEATURES\", features)\n",
        "    tft_output_dir = metadata[\"transform_artifacts_dir\"]\n",
        "\n",
        "    schema = tfdv.load_schema_text(schema_location)\n",
        "    feature_spec = tft.tf_metadata.schema_utils.schema_as_feature_spec(schema).feature_spec\n",
        "\n",
        "    tft_output = tft.TFTransformOutput(tft_output_dir)\n",
        "\n",
        "    # Drop features that were not used in training\n",
        "    features_input_signature = {\n",
        "        feature_name: tf.TensorSpec(\n",
        "            shape=(None, 1), dtype=spec.dtype, name=feature_name\n",
        "        )\n",
        "        for feature_name, spec in feature_spec.items()\n",
        "        if feature_name in features\n",
        "    }\n",
        "\n",
        "    signatures = {\n",
        "        \"serving_default\": _get_serve_features_fn(\n",
        "            model, tft_output\n",
        "        ).get_concrete_function(features_input_signature),\n",
        "        \"serving_tf_example\": _get_serve_tf_examples_fn(\n",
        "            model, tft_output, feature_spec\n",
        "        ).get_concrete_function(\n",
        "            tf.TensorSpec(shape=[None], dtype=tf.string, name=\"examples\")\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    logging.info(\"Model saving started...\")\n",
        "    model.save(serving_model_dir, signatures=signatures)\n",
        "    logging.info(\"Model saving completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "construct_serving_model"
      },
      "source": [
        "### Construct the serving model\n",
        "\n",
        "Now construct the serving model and store the serving model to your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "construct_serving_model"
      },
      "outputs": [],
      "source": [
        "os.chdir(\"custom\")\n",
        "\n",
        "from trainer import serving\n",
        "\n",
        "SERVING_MODEL_DIR = BUCKET_NAME + \"/serving_model\"\n",
        "\n",
        "serving.construct_serving_model(\n",
        "    model=model, serving_model_dir=SERVING_MODEL_DIR, metadata=metadata\n",
        ")\n",
        "\n",
        "serving_model = tf.keras.models.load_model(SERVING_MODEL_DIR)\n",
        "\n",
        "os.chdir(\"..\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_serving_model:tfrec"
      },
      "source": [
        "### Test the serving model locally with tf.Example data\n",
        "\n",
        "Next, test the layer interface in the serving model for tf.Example data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_serving_model:tfrec"
      },
      "outputs": [],
      "source": [
        "EXPORTED_TFREC_PREFIX = metadata[\"exported_tfrec_prefix\"]\n",
        "file_names = tf.data.TFRecordDataset.list_files(\n",
        "    EXPORTED_TFREC_PREFIX + \"/data-*.tfrecord\"\n",
        ")\n",
        "for batch in tf.data.TFRecordDataset(file_names).batch(3).take(1):\n",
        "    predictions = serving_model.signatures[\"serving_tf_example\"](batch)\n",
        "    for key in predictions:\n",
        "        print(f\"{key}: {predictions[key]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_serving_model:jsonl,chicago"
      },
      "source": [
        "### Test the serving model locally with JSONL data\n",
        "\n",
        "Next, test the layer interface in the serving model for JSONL data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_serving_model:jsonl,chicago"
      },
      "outputs": [],
      "source": [
        "schema = tfdv.load_schema_text(metadata[\"schema\"])\n",
        "feature_spec = tft.tf_metadata.schema_utils.schema_as_feature_spec(schema).feature_spec\n",
        "\n",
        "instance = {\n",
        "    \"dropoff_grid\": \"POINT(-87.6 41.9)\",\n",
        "    \"euclidean\": 2064.2696,\n",
        "    \"loc_cross\": \"\",\n",
        "    \"payment_type\": \"Credit Card\",\n",
        "    \"pickup_grid\": \"POINT(-87.6 41.9)\",\n",
        "    \"trip_miles\": 1.37,\n",
        "    \"trip_day\": 12,\n",
        "    \"trip_hour\": 6,\n",
        "    \"trip_month\": 2,\n",
        "    \"trip_day_of_week\": 4,\n",
        "    \"trip_seconds\": 555,\n",
        "}\n",
        "\n",
        "for feature_name in instance:\n",
        "    dtype = feature_spec[feature_name].dtype\n",
        "    instance[feature_name] = tf.constant([[instance[feature_name]]], dtype)\n",
        "\n",
        "predictions = serving_model.signatures[\"serving_default\"](**instance)\n",
        "for key in predictions:\n",
        "    print(f\"{key}: {predictions[key].numpy()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upload_serving_model:vertex,labels"
      },
      "source": [
        "### Upload the serving model to a Vertex AI Model resource\n",
        "\n",
        "Next, you upload your serving custom model artifacts to Vertex AI to convert into a managed Vertex AI Model resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upload_serving_model:vertex,labels"
      },
      "outputs": [],
      "source": [
        "vertex_serving_model = aip.Model.upload(\n",
        "    display_name=\"chicago_\" + TIMESTAMP,\n",
        "    artifact_uri=SERVING_MODEL_DIR,\n",
        "    serving_container_image_uri=DEPLOY_IMAGE,\n",
        "    labels={\"user_metadata\": BUCKET_NAME[5:]},\n",
        "    sync=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluate_serving_model"
      },
      "source": [
        "### Evaluate the serving model\n",
        "\n",
        "Next, evaluate the serving model with the evaluation (test) slices. For apples-to-apples comparison, you use the same evaluation slices for both the custom model and the AutoML model. Since your evaluation slices and metrics maybe custom, we recommend:\n",
        "\n",
        "- Send each evaluation slice as a Vertex AI Batch Prediction Job.\n",
        "- Use a custom evaluation script to evaluate the results from the batch prediction job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluate_serving_model"
      },
      "outputs": [],
      "source": [
        "SERVING_OUTPUT_DATA_DIR = BUCKET_NAME + \"/batch_eval\"\n",
        "EXPORTED_JSONL_PREFIX = metadata[\"exported_jsonl_prefix\"]\n",
        "\n",
        "MIN_NODES = 1\n",
        "MAX_NODES = 1\n",
        "\n",
        "job = vertex_serving_model.batch_predict(\n",
        "    instances_format=\"jsonl\",\n",
        "    predictions_format=\"jsonl\",\n",
        "    job_display_name=\"chicago_\" + TIMESTAMP,\n",
        "    gcs_source=EXPORTED_JSONL_PREFIX + \"*.jsonl\",\n",
        "    gcs_destination_prefix=SERVING_OUTPUT_DATA_DIR,\n",
        "    model_parameters=None,\n",
        "    machine_type=DEPLOY_COMPUTE,\n",
        "    accelerator_type=DEPLOY_GPU,\n",
        "    accelerator_count=DEPLOY_NGPU,\n",
        "    starting_replica_count=MIN_NODES,\n",
        "    max_replica_count=MAX_NODES,\n",
        "    sync=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "custom_eval_script"
      },
      "source": [
        "### Perform custom evaluation metrics\n",
        "\n",
        "After the batch job has completed, you input the results and target labels to your custom evaluation script. For demonstration purposes, we just display the results of the batch prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "custom_eval_script"
      },
      "outputs": [],
      "source": [
        "batch_dir = ! gsutil ls $SERVING_OUTPUT_DATA_DIR\n",
        "batch_dir = batch_dir[0]\n",
        "outputs = ! gsutil ls $batch_dir\n",
        "errors = outputs[0]\n",
        "results = outputs[1]\n",
        "print(\"errors\")\n",
        "! gsutil cat $errors\n",
        "print(\"results\")\n",
        "! gsutil cat $results | head -n10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_model:async"
      },
      "outputs": [],
      "source": [
        "model = async_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "automl_job_wait:mbsdk"
      },
      "source": [
        "### Wait for completion of AutoML training job\n",
        "\n",
        "Next, wait for the AutoML training job to complete. Alternatively, one can set the parameter `sync` to `True` in the `run()` method to block until the AutoML training job is completed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "automl_job_wait:mbsdk"
      },
      "outputs": [],
      "source": [
        "model.wait()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluate_the_model:mbsdk"
      },
      "source": [
        "## Review model evaluation scores\n",
        "After your model has finished training, you can review the evaluation scores for it.\n",
        "\n",
        "First, you need to get a reference to the new model. As with datasets, you can either use the reference to the model variable you created when you deployed the model or you can list all of the models in your project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluate_the_model:mbsdk"
      },
      "outputs": [],
      "source": [
        "# Get model resource ID\n",
        "models = aip.Model.list(filter=\"display_name=chicago_\" + TIMESTAMP)\n",
        "\n",
        "# Get a reference to the Model Service client\n",
        "client_options = {\"api_endpoint\": f\"{REGION}-aiplatform.googleapis.com\"}\n",
        "model_service_client = aip.gapic.ModelServiceClient(client_options=client_options)\n",
        "\n",
        "model_evaluations = model_service_client.list_model_evaluations(\n",
        "    parent=models[0].resource_name\n",
        ")\n",
        "model_evaluation = list(model_evaluations)[0]\n",
        "print(model_evaluation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "custom_vs_automl_compare"
      },
      "source": [
        "## Compare metric results with AutoML baseline\n",
        "\n",
        "Finally, you make a decision if the current experiment produces a custom model that is better than the AutoML baseline, as follows:\n",
        "    - Compare the evaluation results for each evaluation slice between the custom model and the AutoML model.\n",
        "    - Weight the results according to your business purposes.\n",
        "    - Add up the result and make a determination if the custom model is better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "store_model_metadata"
      },
      "source": [
        "### Store evaluation results for custom model\n",
        "\n",
        "Next, you use the labels field to store user metadata containing the custom metrics information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "store_model_metadata"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "metadata = {}\n",
        "metadata[\"train_eval_metrics\"] = METRICS\n",
        "metadata[\"custom_eval_metrics\"] = \"[you-fill-this-in]\"\n",
        "\n",
        "with tf.io.gfile.GFile(\"gs://\" + BUCKET_NAME[5:] + \"/metadata.jsonl\", \"w\") as f:\n",
        "    json.dump(metadata, f)\n",
        "\n",
        "!gsutil cat $BUCKET_NAME/metadata.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cleanup:mbsdk"
      },
      "source": [
        "# Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:\n",
        "\n",
        "- Dataset\n",
        "- Pipeline\n",
        "- Model\n",
        "- Endpoint\n",
        "- AutoML Training Job\n",
        "- Batch Job\n",
        "- Custom Job\n",
        "- Hyperparameter Tuning Job\n",
        "- Cloud Storage Bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup:stage2"
      },
      "outputs": [],
      "source": [
        "delete_all = False\n",
        "\n",
        "if delete_all:\n",
        "    # Delete the dataset using the Vertex dataset object\n",
        "    try:\n",
        "        if \"dataset\" in globals():\n",
        "            dataset.delete()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    # Delete the model using the Vertex model object\n",
        "    try:\n",
        "        if \"model\" in globals():\n",
        "            model.delete()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    if \"BUCKET_NAME\" in globals():\n",
        "        ! gsutil rm -r $BUCKET_NAME"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "mlops_experimentation.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

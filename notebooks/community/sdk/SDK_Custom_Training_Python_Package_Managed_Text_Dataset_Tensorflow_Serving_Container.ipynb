{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41aMjXA5rKoL"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ij53c3EzrKoS"
   },
   "source": [
    "# Vertex SDK for Python: Custom Training using Python Package, Managed Text Dataset, and TF-Serving Container Example\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>                                                                                               \n",
    "</table>\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to create a Custom Model using Custom Python Package Training, with a Vertex AI Dataset, and how to serve the model using Tensorflow-Serving Container for online prediction, and batch prediction. It will require you provide a bucket where the dataset will be stored.\n",
    "\n",
    "Note: You may incur charges for training, prediction, storage or usage of other GCP products in connection with testing this SDK.\n",
    "\n",
    "### Dataset\n",
    "#### Stack Overflow Data\n",
    "We download the stack overflow data from from  https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz and will create a Vertex AI managed text dataset. \n",
    "\n",
    "The Stack Overflow Data is licensed under the Creative Commons Attribution-ShareAlike 3.0 Unported License. To view a copy of this license, visit http://creativecommons.org/licenses/by-sa/3.0/ \n",
    "\n",
    "For more information about this dataset please visit: https://console.cloud.google.com/marketplace/details/stack-exchange/stack-overflow\n",
    "\n",
    "\n",
    "## Objective\n",
    "\n",
    "- Utility Functions to Download Data and Prepare CSV Files for Creating Vertex AI Managed Dataset\n",
    "- Download Data\n",
    "- Prepare CSV Files for Creating Managed Dataset\n",
    "- Create Custom Training Python Package\n",
    "- Create TensorFlow Serving Containe\n",
    "- Run Custom Python Package Training with Managed Text Dataset\n",
    "- Deploy a Model and Create an Endpoint on Vertex AI\n",
    "- Predict on the Endpoint\n",
    "- Batch Prediction Job on the Model\n",
    "## Costs \n",
    "\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up your local development environment\n",
    "\n",
    "**If you are using Colab or Google Cloud Notebooks**, your environment already meets\n",
    "all the requirements to run this notebook. You can skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
    "You need the following:\n",
    "\n",
    "* The Google Cloud SDK\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* Jupyter notebook running in a virtual environment with Python 3\n",
    "\n",
    "The Google Cloud guide to [Setting up a Python development\n",
    "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
    "installation guide](https://jupyter.org/install) provide detailed instructions\n",
    "for meeting these requirements. The following steps provide a condensed set of\n",
    "instructions:\n",
    "\n",
    "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "1. [Install\n",
    "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
    "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
    "\n",
    "1. To install Jupyter, run `pip3 install jupyter` on the\n",
    "command-line in a terminal shell.\n",
    "\n",
    "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
    "\n",
    "1. Open this notebook in the Jupyter Notebook Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOMNWzTbftDr"
   },
   "source": [
    "### Install additional packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "Be020jY-ftDv",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: google-cloud-aiplatform 1.12.1\n",
      "Uninstalling google-cloud-aiplatform-1.12.1:\n",
      "  Successfully uninstalled google-cloud-aiplatform-1.12.1\n",
      "Collecting google-cloud-aiplatform\n",
      "  Using cached google_cloud_aiplatform-1.12.1-py2.py3-none-any.whl (1.8 MB)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /opt/conda/envs/fer/lib/python3.9/site-packages (from google-cloud-aiplatform) (2.34.3)\n",
      "Requirement already satisfied: proto-plus>=1.15.0 in /opt/conda/envs/fer/lib/python3.9/site-packages (from google-cloud-aiplatform) (1.20.3)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/envs/fer/lib/python3.9/site-packages (from google-cloud-aiplatform) (2.7.2)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/envs/fer/lib/python3.9/site-packages (from google-cloud-aiplatform) (2.3.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/envs/fer/lib/python3.9/site-packages (from google-cloud-aiplatform) (1.4.1)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/envs/fer/lib/python3.9/site-packages (from google-cloud-aiplatform) (21.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in /opt/conda/envs/fer/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (1.56.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/envs/fer/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (2.6.6)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/envs/fer/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (3.20.1)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/envs/fer/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (2.27.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/envs/fer/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (1.44.0)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/envs/fer/lib/python3.9/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (1.44.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/envs/fer/lib/python3.9/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/envs/fer/lib/python3.9/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.3.0)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/envs/fer/lib/python3.9/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.3.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /opt/conda/envs/fer/lib/python3.9/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.12.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/fer/lib/python3.9/site-packages (from packaging>=14.3->google-cloud-aiplatform) (3.0.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/envs/fer/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (1.16.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/envs/fer/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (5.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/envs/fer/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/fer/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (0.2.8)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/envs/fer/lib/python3.9/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/fer/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/fer/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/fer/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/fer/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (1.26.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/envs/fer/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (0.4.8)\n",
      "Installing collected packages: google-cloud-aiplatform\n",
      "Successfully installed google-cloud-aiplatform-1.12.1\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.8.0-cp39-cp39-manylinux2010_x86_64.whl (497.6 MB)\n",
      "Collecting flatbuffers>=1.12\n",
      "  Using cached flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Using cached h5py-3.6.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
      "Collecting keras<2.9,>=2.8.0rc0\n",
      "  Using cached keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Using cached tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/envs/fer/lib/python3.9/site-packages (from tensorflow) (1.44.0)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.25.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/envs/fer/lib/python3.9/site-packages (from tensorflow) (3.20.1)\n",
      "Collecting gast>=0.2.1\n",
      "  Using cached gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Collecting typing-extensions>=3.6.6\n",
      "  Downloading typing_extensions-4.2.0-py3-none-any.whl (24 kB)\n",
      "Collecting libclang>=9.0.1\n",
      "  Using cached libclang-13.0.0-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/fer/lib/python3.9/site-packages (from tensorflow) (62.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/envs/fer/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting tensorboard<2.9,>=2.8\n",
      "  Using cached tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting absl-py>=0.4.0\n",
      "  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/envs/fer/lib/python3.9/site-packages (from tensorflow) (1.22.3)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting wrapt>=1.11.0\n",
      "  Using cached wrapt-1.14.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/envs/fer/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-2.1.1-py3-none-any.whl (224 kB)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/envs/fer/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.6.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/envs/fer/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.27.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/fer/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/envs/fer/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/envs/fer/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (5.0.0)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Using cached importlib_metadata-4.11.3-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/fer/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/fer/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/fer/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/fer/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.12)\n",
      "Collecting zipp>=0.5\n",
      "  Using cached zipp-3.8.0-py3-none-any.whl (5.4 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/envs/fer/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: tf-estimator-nightly, termcolor, tensorboard-plugin-wit, libclang, keras, flatbuffers, zipp, wrapt, werkzeug, typing-extensions, tensorflow-io-gcs-filesystem, tensorboard-data-server, opt-einsum, oauthlib, keras-preprocessing, h5py, google-pasta, gast, astunparse, absl-py, requests-oauthlib, importlib-metadata, markdown, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 flatbuffers-2.0 gast-0.5.3 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 h5py-3.6.0 importlib-metadata-4.11.3 keras-2.8.0 keras-preprocessing-1.1.2 libclang-13.0.0 markdown-3.3.6 oauthlib-3.2.0 opt-einsum-3.3.0 requests-oauthlib-1.3.1 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tensorflow-io-gcs-filesystem-0.25.0 termcolor-1.1.0 tf-estimator-nightly-2.8.0.dev2021122109 typing-extensions-4.2.0 werkzeug-2.1.1 wrapt-1.14.0 zipp-3.8.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip3 uninstall -y google-cloud-aiplatform\n",
    "!pip3 install google-cloud-aiplatform\n",
    "! pip3 install --upgrade tensorflow \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart the kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). {TODO: Update the APIs needed for your tutorial. Edit the API names, and update the link to append the API IDs, separating each one with a comma. For example, container.googleapis.com,cloudbuild.googleapis.com}\n",
    "\n",
    "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    "\n",
    "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, set your project ID here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append it onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "**If you are using Google Cloud Notebooks**, your environment is already\n",
    "authenticated. Skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you are using Colab**, run the cell below and follow the instructions\n",
    "when prompted to authenticate your account via oAuth.\n",
    "\n",
    "**Otherwise**, follow these steps:\n",
    "\n",
    "1. In the Cloud Console, go to the [**Create service account key**\n",
    "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
    "\n",
    "2. Click **Create service account**.\n",
    "\n",
    "3. In the **Service account name** field, enter a name, and\n",
    "   click **Create**.\n",
    "\n",
    "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
    "into the filter box, and select\n",
    "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
    "\n",
    "5. Click *Create*. A JSON file that contains your key downloads to your\n",
    "local environment.\n",
    "\n",
    "6. Enter the path to your service account key as the\n",
    "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# If on Google Cloud Notebooks, then don't execute this code\n",
    "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
    "Cloud Storage buckets.\n",
    "\n",
    "You may also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook. We suggest that you [choose a region where Vertex AI services are\n",
    "available](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_URI = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "REGION = \"[your-region]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BUCKET_URI == \"\" or BUCKET_URI is None or BUCKET_URI == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_URI = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "from google.cloud import storage\n",
    "from tensorflow.keras import utils\n",
    "from google.cloud import aiplatform\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHNj4P3vrKoV"
   },
   "source": [
    "### Set Your Application Name, Task Name, and Directories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "RnNL5St8rKoV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Name:      mbsdk_custom-py-pkg-training_keras-text-class-stack-overflow-tag\n",
      "Task Directory: ./mbsdk_custom-py-pkg-training_keras-text-class-stack-overflow-tag\n",
      "Data Directory: ./mbsdk_custom-py-pkg-training_keras-text-class-stack-overflow-tag/data\n"
     ]
    }
   ],
   "source": [
    "APP_NAME = \"keras-text-class-stack-overflow-tag\"\n",
    "TASK_TYPE = \"mbsdk_custom-py-pkg-training\"\n",
    "\n",
    "TASK_NAME = f\"{TASK_TYPE}_{APP_NAME}\"\n",
    "\n",
    "TASK_DIR = f\"./{TASK_NAME}\"\n",
    "DATA_DIR = f\"{TASK_DIR}/data\"\n",
    "\n",
    "print(f\"Task Name:      {TASK_NAME}\")\n",
    "print(f\"Task Directory: {TASK_DIR}\")\n",
    "print(f\"Data Directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hZmDcfLS13W"
   },
   "source": [
    "### Set a GCS Prefix\n",
    "\n",
    "If you want to centeralize all input and output files under the gcs location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "W_4gzaP0SyQd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket Name:    20220422091719\n",
      "GCS Prefix:     mbsdk_custom-py-pkg-training/keras-text-class-stack-overflow-tag\n"
     ]
    }
   ],
   "source": [
    "BUCKET_NAME = BUCKET_URI.split(\"gs://\")[1]\n",
    "GCS_PREFIX = f\"{TASK_TYPE}/{APP_NAME}\"\n",
    "\n",
    "print(f\"Bucket Name:    {BUCKET_NAME}\")\n",
    "print(f\"GCS Prefix:     {GCS_PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5T1d5uBoftDw"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJF047yNftDw"
   },
   "source": [
    "### Utility Functions to Download Data and Prepare CSV Files for Creating Vertex AI Managed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "9yOl-l_oftDx"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    destination_file_name = os.path.join(\"gs://\", bucket_name, destination_blob_name)\n",
    "\n",
    "    return destination_file_name\n",
    "\n",
    "\n",
    "def download_data(data_dir):\n",
    "    \"\"\"Download data.\"\"\"\n",
    "\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    url = \"https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz\"\n",
    "    dataset = utils.get_file(\n",
    "        \"stack_overflow_16k.tar.gz\",\n",
    "        url,\n",
    "        untar=True,\n",
    "        cache_dir=data_dir,\n",
    "        cache_subdir=\"\",\n",
    "    )\n",
    "    data_dir = os.path.join(os.path.dirname(dataset))\n",
    "\n",
    "    return data_dir\n",
    "\n",
    "\n",
    "def upload_train_data_to_gcs(train_data_dir, bucket_name, destination_blob_prefix):\n",
    "    \"\"\"Create CSV file using train data content.\"\"\"\n",
    "\n",
    "    train_data_dir = os.path.join(data_dir, \"train\")\n",
    "    train_data_fn = os.path.join(data_dir, \"train.csv\")\n",
    "\n",
    "    fp = open(train_data_fn, \"w\", encoding=\"utf8\")\n",
    "    writer = csv.writer(\n",
    "        fp, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_ALL, lineterminator=\"\\n\"\n",
    "    )\n",
    "\n",
    "    for root, _, files in os.walk(train_data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                class_name = root.split(\"/\")[-1]\n",
    "                file_fn = os.path.join(root, file)\n",
    "                with open(file_fn, \"r\") as f:\n",
    "                    content = f.readlines()\n",
    "                    lines = [x.strip().strip('\"') for x in content]\n",
    "                    writer.writerow((lines[0], class_name))\n",
    "\n",
    "    fp.close()\n",
    "\n",
    "    train_gcs_url = upload_blob(\n",
    "        bucket_name, train_data_fn, os.path.join(destination_blob_prefix, \"train.csv\")\n",
    "    )\n",
    "\n",
    "    return train_gcs_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3UDGt8MrKoY"
   },
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "Ks2ahWfUrKoa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is downloaded to: ./mbsdk_custom-py-pkg-training_keras-text-class-stack-overflow-tag/data\n"
     ]
    }
   ],
   "source": [
    "data_dir = download_data(DATA_DIR)\n",
    "print(f\"Data is downloaded to: {data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "l7AHtjj3rKoa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md  stack_overflow_16k.tar.gz  test  train  train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0422 14:02:13.035362029   12992 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    }
   ],
   "source": [
    "!ls $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "puroD_Wr-W4Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csharp\tjava  javascript  python\n"
     ]
    }
   ],
   "source": [
    "!ls $data_dir/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQNL5AjtrKoa"
   },
   "source": [
    "### Prepare CSV Files for Creating Managed Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ewrG6VArKob"
   },
   "source": [
    "#### Create CSV Files using Data Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "GvbpGr0ArKob"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data content is loaded to gs://20220422091719/mbsdk_custom-py-pkg-training/keras-text-class-stack-overflow-tag/data/train.csv\n"
     ]
    }
   ],
   "source": [
    "gcs_source_train_url = upload_train_data_to_gcs(\n",
    "    train_data_dir=os.path.join(data_dir, \"train\"),\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    destination_blob_prefix=f\"{GCS_PREFIX}/data\",\n",
    ")\n",
    "\n",
    "print(f\"Train data content is loaded to {gcs_source_train_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "Gg_OiKJUrKoc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0422 14:02:23.038895263   12992 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://20220422091719/mbsdk_custom-py-pkg-training/keras-text-class-stack-overflow-tag/data/test.json\n",
      "gs://20220422091719/mbsdk_custom-py-pkg-training/keras-text-class-stack-overflow-tag/data/train.csv\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://$BUCKET_NAME/$GCS_PREFIX/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzKx7Z2WrKoc"
   },
   "source": [
    "# Create Custom Training Python Package\n",
    "\n",
    "Before you can perform custom training with a pre-built container, you must create a [Python Source Distribution](https://docs.python.org/3/distutils/sourcedist.html) that contains your training application and upload it to a Cloud Storage bucket that your Google Cloud project can access.\n",
    "\n",
    "We will create a directory and write all of our package build artifacts into that folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "eBDjNpkLrKoc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0422 14:02:27.334142732   12992 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0422 14:02:27.467122012   12992 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    }
   ],
   "source": [
    "PYTHON_PACKAGE_APPLICATION_DIR = f\"{TASK_NAME}/trainer\"\n",
    "\n",
    "!mkdir -p $PYTHON_PACKAGE_APPLICATION_DIR\n",
    "!touch $PYTHON_PACKAGE_APPLICATION_DIR/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPSRvSvMrKod"
   },
   "source": [
    "### Write the Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "Zupq7mwzrKod"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mbsdk_custom-py-pkg-training_keras-text-class-stack-overflow-tag/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PYTHON_PACKAGE_APPLICATION_DIR}/task.py\n",
    "\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "VOCAB_SIZE = 10000\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "\n",
    "def str2bool(v):\n",
    "  if isinstance(v, bool):\n",
    "    return v\n",
    "  if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "    return True\n",
    "  elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "    return False\n",
    "  else:\n",
    "    raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "def build_model(num_classes, loss, optimizer, metrics, vectorize_layer):\n",
    "  # vocab_size is VOCAB_SIZE + 1 since 0 is used additionally for padding.\n",
    "  model = tf.keras.Sequential([\n",
    "      vectorize_layer,\n",
    "      layers.Embedding(VOCAB_SIZE + 1, 64, mask_zero=True),\n",
    "      layers.Conv1D(64, 5, padding=\"valid\", activation=\"relu\", strides=2),\n",
    "      layers.GlobalMaxPooling1D(),\n",
    "      layers.Dense(num_classes),\n",
    "      layers.Activation('softmax')\n",
    "  ])\n",
    "  model.compile(\n",
    "      loss=loss,\n",
    "      optimizer=optimizer,\n",
    "      metrics=metrics)\n",
    "\n",
    "  return model\n",
    "\n",
    "def get_string_labels(predicted_scores_batch, class_names):\n",
    "  predicted_labels = tf.argmax(predicted_scores_batch, axis=1)\n",
    "  predicted_labels = tf.gather(class_names, predicted_labels)\n",
    "  return predicted_labels\n",
    "\n",
    "def predict(export_model, class_names, inputs):\n",
    "  predicted_scores = export_model.predict(inputs)\n",
    "  predicted_labels = get_string_labels(predicted_scores, class_names)\n",
    "  return predicted_labels\n",
    "\n",
    "def parse_args():\n",
    "  parser = argparse.ArgumentParser(\n",
    "      description='Keras Text Classification on Stack Overflow Questions')\n",
    "  parser.add_argument(\n",
    "      '--epochs', default=25, type=int, help='number of training epochs')\n",
    "  parser.add_argument(\n",
    "      '--batch-size', default=16, type=int, help='mini-batch size')\n",
    "  parser.add_argument(\n",
    "      '--model-dir', default=os.getenv('AIP_MODEL_DIR'), type=str, help='model directory')\n",
    "  parser.add_argument(\n",
    "      '--data-dir', default='./data', type=str, help='data directory')\n",
    "  parser.add_argument(\n",
    "      '--test-run', default=False, type=str2bool, help='test run the training application, i.e. 1 epoch for training using sample dataset')\n",
    "  parser.add_argument(\n",
    "      '--model-version', default=1, type=int, help='model version')\n",
    "  args = parser.parse_args()\n",
    "  return args\n",
    "\n",
    "def load_aip_dataset(aip_data_uri_pattern, batch_size, class_names, test_run, shuffle=True, seed=42):\n",
    "\n",
    "  data_file_urls = list()\n",
    "  labels = list()\n",
    "\n",
    "  class_indices = dict(zip(class_names, range(len(class_names))))\n",
    "  num_classes = len(class_names)\n",
    "\n",
    "  for aip_data_uri in tqdm.tqdm(tf.io.gfile.glob(pattern=aip_data_uri_pattern)):\n",
    "    with tf.io.gfile.GFile(name=aip_data_uri, mode='r') as gfile:\n",
    "      for line in gfile.readlines():\n",
    "        line = json.loads(line)\n",
    "        data_file_urls.append(line['textContent'])\n",
    "        classification_annotation = line['classificationAnnotations'][0]\n",
    "        label = classification_annotation['displayName']\n",
    "        labels.append(class_indices[label])\n",
    "        if test_run:\n",
    "          break\n",
    "\n",
    "  data = list()\n",
    "  for data_file_url in tqdm.tqdm(data_file_urls):\n",
    "    with tf.io.gfile.GFile(name=data_file_url, mode='r') as gf:\n",
    "      txt = gf.read()\n",
    "      data.append(txt)\n",
    "\n",
    "  print(f' data files count: {len(data_file_urls)}')\n",
    "  print(f' data count: {len(data)}')\n",
    "  print(f' labels count: {len(labels)}')\n",
    "\n",
    "  dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "  label_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "  label_ds = label_ds.map(lambda x: tf.one_hot(x, num_classes))\n",
    "\n",
    "  dataset = tf.data.Dataset.zip((dataset, label_ds))\n",
    "\n",
    "  if shuffle:\n",
    "    # Shuffle locally at each iteration\n",
    "    dataset = dataset.shuffle(buffer_size=batch_size * 8, seed=seed)\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  # Users may need to reference `class_names`.\n",
    "  dataset.class_names = class_names\n",
    "\n",
    "  return dataset\n",
    "\n",
    "def main():\n",
    "\n",
    "  args = parse_args()\n",
    "\n",
    "  class_names = ['csharp', 'java', 'javascript', 'python']\n",
    "  class_indices = dict(zip(class_names, range(len(class_names))))\n",
    "  num_classes = len(class_names)\n",
    "  print(f' class names: {class_names}')\n",
    "  print(f' class indices: {class_indices}')\n",
    "  print(f' num classes: {num_classes}')\n",
    "\n",
    "  epochs = 1 if args.test_run else args.epochs\n",
    "\n",
    "  aip_model_dir = os.environ.get('AIP_MODEL_DIR')\n",
    "  aip_data_format = os.environ.get('AIP_DATA_FORMAT')\n",
    "  aip_training_data_uri = os.environ.get('AIP_TRAINING_DATA_URI')\n",
    "  aip_validation_data_uri = os.environ.get('AIP_VALIDATION_DATA_URI')\n",
    "  aip_test_data_uri = os.environ.get('AIP_TEST_DATA_URI')\n",
    "\n",
    "  print(f\"aip_model_dir: {aip_model_dir}\")\n",
    "  print(f\"aip_data_format: {aip_data_format}\")\n",
    "  print(f\"aip_training_data_uri: {aip_training_data_uri}\")\n",
    "  print(f\"aip_validation_data_uri: {aip_validation_data_uri}\")\n",
    "  print(f\"aip_test_data_uri: {aip_test_data_uri}\")\n",
    "\n",
    "  print('Loading AIP dataset')\n",
    "  train_ds = load_aip_dataset(\n",
    "      aip_training_data_uri, args.batch_size, class_names, args.test_run)\n",
    "  print('AIP training dataset is loaded')\n",
    "  val_ds = load_aip_dataset(\n",
    "      aip_validation_data_uri, 1, class_names, args.test_run)\n",
    "  print('AIP validation dataset is loaded')\n",
    "  test_ds = load_aip_dataset(\n",
    "      aip_test_data_uri, 1, class_names, args.test_run)\n",
    "  print('AIP test dataset is loaded')\n",
    "\n",
    "  vectorize_layer = TextVectorization(\n",
    "      max_tokens=VOCAB_SIZE,\n",
    "      output_mode='int',\n",
    "      output_sequence_length=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "  train_text = train_ds.map(lambda text, labels: text)\n",
    "  vectorize_layer.adapt(train_text)\n",
    "  print('The vectorize_layer is adapted')\n",
    "\n",
    "\n",
    "  print('Build model')\n",
    "  optimizer = 'adam'\n",
    "  metrics = ['accuracy']\n",
    "\n",
    "  model = build_model(\n",
    "      num_classes, losses.CategoricalCrossentropy(from_logits=True), optimizer, metrics, vectorize_layer)\n",
    "\n",
    "  history = model.fit(train_ds, validation_data=val_ds, epochs=epochs)\n",
    "  history = history.history\n",
    "\n",
    "  print('Training accuracy: {acc}, loss: {loss}'.format(\n",
    "      acc=history['accuracy'][-1], loss=history['loss'][-1]))\n",
    "  print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "      acc=history['val_accuracy'][-1], loss=history['val_loss'][-1]))\n",
    "\n",
    "  loss, accuracy = model.evaluate(test_ds)\n",
    "  print('Test accuracy: {acc}, loss: {loss}'.format(\n",
    "      acc=accuracy, loss=loss))\n",
    "\n",
    "  inputs = [\n",
    "      \"how do I extract keys from a dict into a list?\",  # python\n",
    "      \"debug public static void main(string[] args) {...}\",  # java\n",
    "  ]\n",
    "  predicted_labels = predict(model, class_names, inputs)\n",
    "  for input, label in zip(inputs, predicted_labels):\n",
    "    print(f'Question: {input}')\n",
    "    print(f'Predicted label: {label.numpy()}')\n",
    "\n",
    "  model_export_path = os.path.join(args.model_dir, str(args.model_version))\n",
    "  model.save(model_export_path)\n",
    "  print(f'Model version {args.model_version} is exported to {args.model_dir}')\n",
    "\n",
    "  loaded = tf.saved_model.load(model_export_path)\n",
    "  input_name = list(loaded.signatures['serving_default'].structured_input_signature[1].keys())[0]\n",
    "  print(f'Serving function input: {input_name}')\n",
    "\n",
    "  return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAhynoNBrKoh"
   },
   "source": [
    "### Build Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "kXSBQcqdrKof"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./mbsdk_custom-py-pkg-training_keras-text-class-stack-overflow-tag/setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TASK_DIR}/setup.py\n",
    "\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "setup(\n",
    "    name='trainer',\n",
    "    version='0.1',\n",
    "    packages=find_packages(),\n",
    "    install_requires=(),\n",
    "    include_package_data=True,\n",
    "    description='My training application.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "MclanW0UrKoh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data  setup.py\ttrainer\n"
     ]
    }
   ],
   "source": [
    "!ls $TASK_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "j19Nk2vzrKoh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0422 14:02:40.839893572   12992 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "writing trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to trainer.egg-info/dependency_links.txt\n",
      "writing top-level names to trainer.egg-info/top_level.txt\n",
      "reading manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n",
      "running check\n",
      "warning: check: missing required meta-data: url\n",
      "\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) should be supplied\n",
      "\n",
      "creating trainer-0.1\n",
      "creating trainer-0.1/trainer\n",
      "creating trainer-0.1/trainer.egg-info\n",
      "copying files to trainer-0.1...\n",
      "copying setup.py -> trainer-0.1\n",
      "copying trainer/__init__.py -> trainer-0.1/trainer\n",
      "copying trainer/task.py -> trainer-0.1/trainer\n",
      "copying trainer.egg-info/PKG-INFO -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/SOURCES.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/dependency_links.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/top_level.txt -> trainer-0.1/trainer.egg-info\n",
      "Writing trainer-0.1/setup.cfg\n",
      "Creating tar archive\n",
      "removing 'trainer-0.1' (and everything under it)\n"
     ]
    }
   ],
   "source": [
    "!cd $TASK_DIR && python3 setup.py sdist --formats=gztar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "9c1r_k69rKoi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 jupyter jupyter 3272 Apr 22 09:28 ./mbsdk_custom-py-pkg-training_keras-text-class-stack-overflow-tag/dist/trainer-0.1.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr $TASK_DIR/dist/trainer-0.1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyQv9AcNrKoi"
   },
   "source": [
    "### Upload the Package to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "9WjF8SFrrKoi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Training Python Package is uploaded to: gs://20220422091719/custom-training-python-package/keras-text-class-stack-overflow-tag/trainer-0.1.tar.gz\n"
     ]
    }
   ],
   "source": [
    "destination_blob_name = f\"custom-training-python-package/{APP_NAME}/trainer-0.1.tar.gz\"\n",
    "source_file_name = f\"{TASK_DIR}/dist/trainer-0.1.tar.gz\"\n",
    "\n",
    "python_package_gcs_uri = upload_blob(\n",
    "    BUCKET_NAME, source_file_name, destination_blob_name\n",
    ")\n",
    "python_module_name = \"trainer.task\"\n",
    "\n",
    "print(f\"Custom Training Python Package is uploaded to: {python_package_gcs_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWJpv4ejrWi0"
   },
   "source": [
    "# Create TensorFlow Serving Container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMOad4Z4aOgv"
   },
   "source": [
    "Download the TensorFlow Serving Docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "KC8e1yrHrVyA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest: Pulling from tensorflow/serving\n",
      "\n",
      "\u001b[1B7da0a88c: Pulling fs layer \n",
      "\u001b[1Ba5f15c7a: Pulling fs layer \n",
      "\u001b[1Bbe87baa1: Pulling fs layer \n",
      "\u001b[1B6208e893: Pulling fs layer \n",
      "\u001b[1Bf464ef73: Pulling fs layer \n",
      "\u001b[1Bbf86544b: Pulling fs layer \n",
      "\u001b[1BDigest: sha256:6651f4839e1124dbde75ee531825112af0a6b8ef082c88ab14ca53eb69a2e4bb7A\u001b[2K\u001b[7A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[3A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\n",
      "Status: Downloaded newer image for tensorflow/serving:latest\n",
      "docker.io/tensorflow/serving:latest\n"
     ]
    }
   ],
   "source": [
    "!docker pull tensorflow/serving:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtiNicSyaTk1"
   },
   "source": [
    "Create a tag for registering the image and register the image with Cloud Container Registry (gcr.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "sUfgSddcsRvw"
   },
   "outputs": [],
   "source": [
    "TF_SERVING_CONTAINER_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/tf-serving\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Bw_A7ynTsjTF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\n",
      "The push refers to repository [gcr.io/vertex-ai-dev/tf-serving]\n",
      "\n",
      "\u001b[1B23850a27: Preparing \n",
      "\u001b[1Ba33781cd: Preparing \n",
      "\u001b[1B89523b17: Preparing \n",
      "\u001b[1Bf28d5f3c: Preparing \n",
      "\u001b[1Bc6d2db45: Preparing \n",
      "\u001b[1Bbacb0351: Preparing \n",
      "\u001b[2Bbacb0351: Layer already exists \u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[2A\u001b[2Klatest: digest: sha256:6651f4839e1124dbde75ee531825112af0a6b8ef082c88ab14ca53eb69a2e4bb size: 1780\n"
     ]
    }
   ],
   "source": [
    "!docker tag tensorflow/serving $TF_SERVING_CONTAINER_IMAGE_URI\n",
    "!docker push $TF_SERVING_CONTAINER_IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rc_MIJwlrKoj"
   },
   "source": [
    "# Run Custom Python Package Training with Managed Text Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiNL0HSVrKoj"
   },
   "source": [
    "## Initialize Vertex SDK for Python\n",
    "\n",
    "Initialize the *client* for Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "J5MqvGtMrKoj"
   },
   "outputs": [],
   "source": [
    "\n",
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGT1uT-HrKoj"
   },
   "source": [
    "## Create a Dataset on Vertex AI\n",
    "We will now create a Vertex AI text dataset using the previously prepared csv files. Choose one of the options below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "lY566K2crKok"
   },
   "outputs": [],
   "source": [
    "dataset_display_name = f\"temp-{APP_NAME}-content\"\n",
    "gcs_source = gcs_source_train_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyLoUsx9rKok"
   },
   "source": [
    "#### Option 1: Create a Dataset with CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "h7QwC1ThrKok"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TextDataset\n",
      "Create TextDataset backing LRO: projects/931647533046/locations/us-central1/datasets/5386423901590913024/operations/8149640177178902528\n",
      "TextDataset created. Resource name: projects/931647533046/locations/us-central1/datasets/5386423901590913024\n",
      "To use this TextDataset in another session:\n",
      "ds = aiplatform.TextDataset('projects/931647533046/locations/us-central1/datasets/5386423901590913024')\n",
      "Importing TextDataset data: projects/931647533046/locations/us-central1/datasets/5386423901590913024\n",
      "Import TextDataset data backing LRO: projects/931647533046/locations/us-central1/datasets/5386423901590913024/operations/7433567836426993664\n"
     ]
    }
   ],
   "source": [
    "dataset = aiplatform.TextDataset.create(\n",
    "    display_name=dataset_display_name,\n",
    "    gcs_source=gcs_source,\n",
    "    import_schema_uri=aiplatform.schema.dataset.ioformat.text.single_label_classification,\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xksBj3M0rKok"
   },
   "source": [
    "#### Option 2: Create a Dataset, then Import CSV File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93w8LGlx7uWy"
   },
   "source": [
    "```\n",
    "dataset = aiplatform.TextDataset.create(\n",
    "    display_name=dataset_display_name,\n",
    ")\n",
    "dataset.import_data(\n",
    "    gcs_source=gcs_source, \n",
    "    import_schema_uri=aiplatform.schema.dataset.ioformat.text.single_label_classification,\n",
    "    sync=False\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncP9QwvkrKok"
   },
   "source": [
    "#### Option 3: Retrieve a Dataset on Vertex AI\n",
    "If you have previously created a Dataset on Vertex AI, you can retrieve the dataset using the `dataset_name`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmiQDoS_7ncz"
   },
   "source": [
    "```\n",
    "dataset_name = 'YOUR DATASET NAME'\n",
    "\n",
    "dataset = aiplatform.TextDataset(dataset_name)\n",
    "dataset.resource_name\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzLvubIrrKon"
   },
   "source": [
    "## Launch a Training Job and Create a Model on Vertex AI\n",
    "\n",
    "We will now train a model with the python package we just built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cyz86ixRYNtT"
   },
   "source": [
    "### Config a Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "9zk8SRuK1Xvi"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = APP_NAME\n",
    "PRE_BUILT_TRAINING_CONTAINER_IMAGE_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxc_yVxkbbbd"
   },
   "source": [
    "You will need to specify the python package that was built and uploaded to GCS, the module name of the python package, the pre-built training container image uri for training, and in this example, we are using TensorFlow serving container for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "uRGrFdxOftD1"
   },
   "outputs": [],
   "source": [
    "job = aiplatform.CustomPythonPackageTrainingJob(\n",
    "    display_name=f\"temp_{TASK_NAME}_tf-serving\",\n",
    "    python_package_gcs_uri=python_package_gcs_uri,\n",
    "    python_module_name=python_module_name,\n",
    "    container_uri=PRE_BUILT_TRAINING_CONTAINER_IMAGE_URI,\n",
    "    model_serving_container_image_uri=TF_SERVING_CONTAINER_IMAGE_URI,\n",
    "    model_serving_container_command=[\"/usr/bin/tensorflow_model_server\"],\n",
    "    model_serving_container_args=[\n",
    "        f\"--model_name={MODEL_NAME}\",\n",
    "        \"--model_base_path=$(AIP_STORAGE_URI)\",\n",
    "        \"--rest_api_port=8080\",\n",
    "        \"--port=8500\",\n",
    "        \"--file_system_poll_wait_seconds=31540000\",\n",
    "    ],\n",
    "    model_serving_container_predict_route=f\"/v1/models/{MODEL_NAME}:predict\",\n",
    "    model_serving_container_health_route=f\"/v1/models/{MODEL_NAME}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPfZGlZeTN72"
   },
   "source": [
    "### Run the Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "V6iY5wcerKon"
   },
   "outputs": [],
   "source": [
    "model = job.run(\n",
    "    dataset=dataset,\n",
    "    annotation_schema_uri=aiplatform.schema.dataset.annotation.text.classification,\n",
    "    args=[\"--epochs\", \"50\"],\n",
    "    replica_count=1,\n",
    "    model_display_name=f\"temp_{TASK_NAME}_tf-serving\",\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "InvalidArgument",
     "evalue": "400 List of found errors:\t1.Field: name; Message: Invalid Model resource name.\t [field_violations {\n  field: \"name\"\n  description: \"Invalid Model resource name.\"\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/fer/lib/python3.9/site-packages/google/api_core/grpc_helpers.py:57\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/conda/envs/fer/lib/python3.9/site-packages/grpc/_channel.py:946\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    944\u001b[0m state, call, \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(request, timeout, metadata, credentials,\n\u001b[1;32m    945\u001b[0m                               wait_for_ready, compression)\n\u001b[0;32m--> 946\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/fer/lib/python3.9/site-packages/grpc/_channel.py:849\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 849\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"List of found errors:\t1.Field: name; Message: Invalid Model resource name.\t\"\n\tdebug_error_string = \"{\"created\":\"@1650636275.029097904\",\"description\":\"Error received from peer ipv4:74.125.142.95:443\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":903,\"grpc_message\":\"List of found errors:\\t1.Field: name; Message: Invalid Model resource name.\\t\",\"grpc_status\":3}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [73]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m=\u001b[39m\u001b[43maiplatform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemp_mbsdk_custom-py-pkg-training_keras-text-class-stack-overflow-tag_tf-serving\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/fer/lib/python3.9/site-packages/google/cloud/aiplatform/models.py:1610\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, model_name, project, location, credentials)\u001b[0m\n\u001b[1;32m   1586\u001b[0m \u001b[38;5;124;03m\"\"\"Retrieves the model resource and instantiates its representation.\u001b[39;00m\n\u001b[1;32m   1587\u001b[0m \n\u001b[1;32m   1588\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1601\u001b[0m \u001b[38;5;124;03m        credentials set in aiplatform.init will be used.\u001b[39;00m\n\u001b[1;32m   1602\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m   1605\u001b[0m     project\u001b[38;5;241m=\u001b[39mproject,\n\u001b[1;32m   1606\u001b[0m     location\u001b[38;5;241m=\u001b[39mlocation,\n\u001b[1;32m   1607\u001b[0m     credentials\u001b[38;5;241m=\u001b[39mcredentials,\n\u001b[1;32m   1608\u001b[0m     resource_name\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[1;32m   1609\u001b[0m )\n\u001b[0;32m-> 1610\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_gca_resource\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/fer/lib/python3.9/site-packages/google/cloud/aiplatform/base.py:598\u001b[0m, in \u001b[0;36mVertexAiResourceNoun._get_gca_resource\u001b[0;34m(self, resource_name, parent_resource_name_fields)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;124;03m\"\"\"Returns GAPIC service representation of client class resource.\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \n\u001b[1;32m    580\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;124;03m        Should not include project and location.\u001b[39;00m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    587\u001b[0m resource_name \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mfull_resource_name(\n\u001b[1;32m    588\u001b[0m     resource_name\u001b[38;5;241m=\u001b[39mresource_name,\n\u001b[1;32m    589\u001b[0m     resource_noun\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resource_noun,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    595\u001b[0m     resource_id_validator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resource_id_validator,\n\u001b[1;32m    596\u001b[0m )\n\u001b[0;32m--> 598\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getter_method\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresource_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_DEFAULT_RETRY\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/fer/lib/python3.9/site-packages/google/cloud/aiplatform_v1/services/model_service/client.py:744\u001b[0m, in \u001b[0;36mModelServiceClient.get_model\u001b[0;34m(self, request, name, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    739\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(metadata) \u001b[38;5;241m+\u001b[39m (\n\u001b[1;32m    740\u001b[0m     gapic_v1\u001b[38;5;241m.\u001b[39mrouting_header\u001b[38;5;241m.\u001b[39mto_grpc_metadata(((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mname),)),\n\u001b[1;32m    741\u001b[0m )\n\u001b[1;32m    743\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 744\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/envs/fer/lib/python3.9/site-packages/google/api_core/gapic_v1/method.py:154\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, *args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     metadata\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata)\n\u001b[1;32m    152\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m metadata\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/fer/lib/python3.9/site-packages/google/api_core/retry.py:283\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    280\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    282\u001b[0m )\n\u001b[0;32m--> 283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_deadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/fer/lib/python3.9/site-packages/google/api_core/retry.py:190\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, deadline, on_error)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/conda/envs/fer/lib/python3.9/site-packages/google/api_core/grpc_helpers.py:59\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: 400 List of found errors:\t1.Field: name; Message: Invalid Model resource name.\t [field_violations {\n  field: \"name\"\n  description: \"Invalid Model resource name.\"\n}\n]"
     ]
    }
   ],
   "source": [
    "model=aiplatform.Model(model_name=\"temp_mbsdk_custom-py-pkg-training_keras-text-class-stack-overflow-tag_tf-serving\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = aiplatform.Model.list(\n",
    "            filter=f\"display_name=temp_mbsdk_custom-py-pkg-training_keras-text-class-stack-overflow-tag_tf-serving\", order_by=\"create_time\"\n",
    "        )\n",
    "\n",
    "model = models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.models.Model object at 0x7f75f43408e0> \n",
       "resource name: projects/931647533046/locations/us-central1/models/7533629373149085696"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "zV1PjANLrKoo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextDataset data imported. Resource name: projects/931647533046/locations/us-central1/datasets/5386423901590913024\n",
      "Training Output directory:\n",
      "gs://20220422091719/aiplatform-custom-training-2022-04-22-09:52:08.326 \n",
      "No dataset split provided. The service will use a default split.\n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/1034062648376819712?project=931647533046\n",
      "CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/1034062648376819712 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/7579182689643659264?project=931647533046\n",
      "CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/1034062648376819712 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/1034062648376819712 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/1034062648376819712 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/1034062648376819712 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/1034062648376819712 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/1034062648376819712 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/1034062648376819712 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/1034062648376819712 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/1034062648376819712 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob run completed. Resource name: projects/931647533046/locations/us-central1/trainingPipelines/1034062648376819712\n",
      "Model available at projects/931647533046/locations/us-central1/models/7533629373149085696\n"
     ]
    }
   ],
   "source": [
    "model.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XO0-zdXxrKoo"
   },
   "source": [
    "# Deploy a Model and Create an Endpoint on Vertex AI\n",
    "\n",
    "Deploy your model, then wait until the model FINISHES deployment before proceeding to prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "tEg2IDwPftD2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/931647533046/locations/us-central1/endpoints/7630478755370106880/operations/9077381700417224704\n",
      "Endpoint created. Resource name: projects/931647533046/locations/us-central1/endpoints/7630478755370106880\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/931647533046/locations/us-central1/endpoints/7630478755370106880')\n",
      "Deploying model to Endpoint : projects/931647533046/locations/us-central1/endpoints/7630478755370106880\n",
      "Deploy Endpoint model backing LRO: projects/931647533046/locations/us-central1/endpoints/7630478755370106880/operations/4429666884970872832\n"
     ]
    }
   ],
   "source": [
    "endpoint = model.deploy(machine_type=\"n1-standard-4\", sync=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "9AoIJdCHrKop"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint model deployed. Resource name: projects/931647533046/locations/us-central1/endpoints/7630478755370106880\n"
     ]
    }
   ],
   "source": [
    "endpoint.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z87MXJEDrKoq"
   },
   "source": [
    "## Predict on the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "vQXGCg8IrKoq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Indices: {'csharp': 0, 'java': 1, 'javascript': 2, 'python': 3}\n",
      "Class Maps:    {0: 'csharp', 1: 'java', 2: 'javascript', 3: 'python'}\n"
     ]
    }
   ],
   "source": [
    "class_names = [\"csharp\", \"java\", \"javascript\", \"python\"]\n",
    "\n",
    "class_ids = range(len(class_names))\n",
    "\n",
    "class_indices = dict(zip(class_names, class_ids))\n",
    "class_maps = dict(zip(class_ids, class_names))\n",
    "print(f\"Class Indices: {class_indices}\")\n",
    "print(f\"Class Maps:    {class_maps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Di8RtRxipm63"
   },
   "outputs": [],
   "source": [
    "text_inputs = [\n",
    "    \"how do I extract keys from a dict into a list?\",  # python\n",
    "    \"debug public static void main(string[] args) {...}\",  # java\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['how do I extract keys from a dict into a list?'], ['debug public static void main(string[] args) {...}']]\n"
     ]
    }
   ],
   "source": [
    "f=[[text] for text in text_inputs]\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "dPHYGAlvlTAj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: how do I extract keys from a dict into a list?\n",
      "Predicted Tag: python\n",
      "\n",
      "Question: debug public static void main(string[] args) {...}\n",
      "Predicted Tag: java\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = endpoint.predict(instances=text_inputs)\n",
    "for text, predicted_scores in zip(text_inputs, predictions.predictions):\n",
    "    class_id = np.argmax(predicted_scores)\n",
    "    class_name = class_maps[class_id]\n",
    "    print(f\"Question: {text}\")\n",
    "    print(f\"Predicted Tag: {class_name}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRkvzr2-LgA9"
   },
   "source": [
    "# Batch Prediction Job on the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "nlzRioF7UGFd"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def upload_test_data_to_gcs(test_data_dir, test_gcs_url):\n",
    "    \"\"\"Create JSON file using test data content.\"\"\"\n",
    "\n",
    "    input_name = \"text_vectorization_input\"\n",
    "\n",
    "    with tf.io.gfile.GFile(test_gcs_url, \"w\") as gf:\n",
    "\n",
    "        for root, _, files in os.walk(test_data_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(\".txt\"):\n",
    "                    file_fn = os.path.join(root, file)\n",
    "                    with open(file_fn, \"r\") as f:\n",
    "                        content = f.readlines()\n",
    "                        lines = [x.strip().strip('\"') for x in content]\n",
    "\n",
    "                        data = {input_name: lines[0]}\n",
    "                        gf.write(json.dumps(data))\n",
    "                        gf.write(\"\\n\")\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "C_sbOm-5ud5C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data content is loaded to gs://20220422091719/mbsdk_custom-py-pkg-training/keras-text-class-stack-overflow-tag/data/test.json\n"
     ]
    }
   ],
   "source": [
    "gcs_source_test_url = f\"gs://{BUCKET_NAME}/{GCS_PREFIX}/data/test.json\"\n",
    "upload_test_data_to_gcs(\n",
    "    test_data_dir=os.path.join(data_dir, \"test\"), test_gcs_url=gcs_source_test_url\n",
    ")\n",
    "\n",
    "print(f\"Test data content is loaded to {gcs_source_test_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_source_test_url=\"gs://20220422091719/mbsdk_custom-py-pkg-training/keras-text-class-stack-overflow-tag/data/test.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "QBMBk2WxLqBP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0422 13:07:08.787097997   12992 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://20220422091719/mbsdk_custom-py-pkg-training/keras-text-class-stack-overflow-tag/data/test.json\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $gcs_source_test_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "2JhoTiD5LuDW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BatchPredictionJob\n",
      "BatchPredictionJob created. Resource name: projects/931647533046/locations/us-central1/batchPredictionJobs/798960674079244288\n",
      "To use this BatchPredictionJob in another session:\n",
      "bpj = aiplatform.BatchPredictionJob('projects/931647533046/locations/us-central1/batchPredictionJobs/798960674079244288')\n",
      "View Batch Prediction Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/batch-predictions/798960674079244288?project=931647533046\n"
     ]
    }
   ],
   "source": [
    "batch_predict_job = model.batch_predict(\n",
    "    job_display_name=f\"temp_{TASK_NAME}_tf-serving\",\n",
    "    gcs_source=gcs_source_test_url,\n",
    "    gcs_destination_prefix=f\"gs://{BUCKET_NAME}/{GCS_PREFIX}/batch_prediction\",\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "retsI3LLls_W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchPredictionJob projects/931647533046/locations/us-central1/batchPredictionJobs/798960674079244288 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/931647533046/locations/us-central1/batchPredictionJobs/798960674079244288 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/931647533046/locations/us-central1/batchPredictionJobs/798960674079244288 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/931647533046/locations/us-central1/batchPredictionJobs/798960674079244288 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/931647533046/locations/us-central1/batchPredictionJobs/798960674079244288 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/931647533046/locations/us-central1/batchPredictionJobs/798960674079244288 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/931647533046/locations/us-central1/batchPredictionJobs/798960674079244288 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/931647533046/locations/us-central1/batchPredictionJobs/798960674079244288 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/931647533046/locations/us-central1/batchPredictionJobs/798960674079244288 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "BatchPredictionJob run completed. Resource name: projects/931647533046/locations/us-central1/batchPredictionJobs/798960674079244288\n"
     ]
    }
   ],
   "source": [
    "batch_predict_job.wait()\n",
    "bp_iter_outputs = batch_predict_job.iter_outputs()\n",
    "\n",
    "prediction_errors_stats = list()\n",
    "prediction_results = list()\n",
    "for blob in bp_iter_outputs:\n",
    "    if blob.name.split(\"/\")[-1].startswith(\"prediction.errors_stats\"):\n",
    "        prediction_errors_stats.append(blob.name)\n",
    "    if blob.name.split(\"/\")[-1].startswith(\"prediction.results\"):\n",
    "        prediction_results.append(blob.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "KmfK3Tzzlv9C"
   },
   "outputs": [],
   "source": [
    "tags = list()\n",
    "for prediction_result in prediction_results:\n",
    "    gfile_name = f\"gs://{bp_iter_outputs.bucket.name}/{prediction_result}\"\n",
    "    with tf.io.gfile.GFile(name=gfile_name, mode=\"r\") as gfile:\n",
    "        for line in gfile.readlines():\n",
    "            line = json.loads(line)\n",
    "            text = line[\"instance\"][\"text_vectorization_input\"][0]\n",
    "            prediction = line[\"prediction\"]\n",
    "            class_id = np.argmax(prediction)\n",
    "            class_name = class_maps[class_id]\n",
    "            tags.append([text, class_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "UcMQ-daLn_oh"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p</td>\n",
       "      <td>java</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  question   tag\n",
       "0        p  java"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tags_df = pd.DataFrame(tags, columns=[\"question\", \"tag\"])\n",
    "tags_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "QUV_PHjtoCpQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "java    1\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_df[\"tag\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_bucket = False\n",
    "\n",
    "# Delete the dataset using the Vertex dataset object\n",
    "dataset.delete()\n",
    "\n",
    "# Undeploy model from the endpoint\n",
    "endpoint.undeploy_all()\n",
    "\n",
    "# Delete the endpoint\n",
    "endpoint.delete()\n",
    "\n",
    "# Delete the model using the Vertex model object\n",
    "model.delete()\n",
    "\n",
    "# Delete the AutoML or Pipeline training job\n",
    "job.delete()\n",
    "\n",
    "# Delete the batch prediction job using the Vertex batch prediction object\n",
    "batch_predict_job.delete()\n",
    "\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "bcKMGrqDrKoS",
    "q3UDGt8MrKoY",
    "bQNL5AjtrKoa",
    "9ewrG6VArKob",
    "eiNL0HSVrKoj",
    "KGT1uT-HrKoj",
    "kyLoUsx9rKok",
    "xksBj3M0rKok",
    "ncP9QwvkrKok",
    "Cyz86ixRYNtT"
   ],
   "name": "AI_Platform_(Unified)_SDK_Custom_Training_Python_Package_Managed_Text_Dataset_Tensorflow_Serving_Container.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-env-fer-py",
   "name": "common-cpu.m89",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m89"
  },
  "kernelspec": {
   "display_name": "Python [conda env:fer]",
   "language": "python",
   "name": "conda-env-fer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "a6b56b1c7b76"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/sdk/SDK_BigQuery_Custom_Container_Training.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/sdk/SDK_BigQuery_Custom_Container_Training.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/sdk/SDK_BigQuery_Custom_Container_Training.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>                                                                                               \n",
        "</table>"
      ],
      "metadata": {
        "id": "et4hRnB9mrau"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Overview \n",
        "To use this Jupyter notebook, copy the notebook to a Google Cloud Notebooks instance and open it. You can run each step, or cell, and see its results. To run a cell, use Shift+Enter. Jupyter automatically displays the return value of the last line in each cell. For more information about running notebooks in Google Cloud Notebook, see the Google Cloud Notebook guide.. \n",
        "\n",
        "### Objective \n",
        "\n",
        "This notebook demonstrate how to create a Custom Model using Custom Container Training and a Big Query Dataset. It will require you provide a bucket where the dataset will be stored.\n",
        "\n",
        "Costs  \n",
        "This tutorial uses billable components of Google Cloud: \n",
        "\n",
        "Vertex AI\n",
        "Cloud Storage\n",
        "Learn about Vertex AI  pricing and Cloud Storage pricing, and use the Pricing Calculator to generate a cost estimate based on your projected usage."
      ],
      "metadata": {
        "id": "wLMxmUTwn1td"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up your local development environment\n",
        "\n",
        "**If you are using Colab or Google Cloud Notebooks**, your environment already meets\n",
        "all the requirements to run this notebook. You can skip this step."
      ],
      "metadata": {
        "id": "g5dkyDy1obku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
        "You need the following:\n",
        "\n",
        "* The Google Cloud SDK\n",
        "* Git\n",
        "* Python 3\n",
        "* virtualenv\n",
        "* Jupyter notebook running in a virtual environment with Python 3\n",
        "\n",
        "The Google Cloud guide to [Setting up a Python development\n",
        "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
        "installation guide](https://jupyter.org/install) provide detailed instructions\n",
        "for meeting these requirements. The following steps provide a condensed set of\n",
        "instructions:\n",
        "\n",
        "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
        "\n",
        "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
        "\n",
        "1. [Install\n",
        "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
        "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
        "\n",
        "1. To install Jupyter, run `pip3 install jupyter` on the\n",
        "command-line in a terminal shell.\n",
        "\n",
        "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
        "\n",
        "1. Open this notebook in the Jupyter Notebook Dashboard."
      ],
      "metadata": {
        "id": "gyja44LCozU_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpYscdzmf4Gu"
      },
      "source": [
        "# Ensure the following APIs are enabled:\n",
        "- [BigQuery](https://console.cloud.google.com/apis/library/bigquery.googleapis.com?q=BigQuery)\n",
        "- [Cloudbuild](https://console.cloud.google.com/apis/library/cloudbuild.googleapis.com?q=Cloudbuild)\n",
        "- [Container Registry](https://console.cloud.google.com/apis/library/containerregistry.googleapis.com?q=container%20registry)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOMNWzTbftDr"
      },
      "source": [
        "# Install Vertex AI SDK for Python\n",
        "\n",
        "\n",
        "After the SDK installation the kernel will be automatically restarted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Be020jY-ftDv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9da34519-ad6e-4cd1-f22f-6ef6767a4906"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping google-cloud-aiplatform as it is not installed.\u001b[0m\n",
            "Collecting google-cloud-aiplatform\n",
            "  Downloading google_cloud_aiplatform-1.12.1-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 15.4 MB/s \n",
            "\u001b[?25hCollecting google-cloud-storage<3.0.0dev,>=1.32.0\n",
            "  Downloading google_cloud_storage-2.3.0-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 49.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-aiplatform) (1.21.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-aiplatform) (21.3)\n",
            "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.7/dist-packages (from google-cloud-aiplatform) (1.31.5)\n",
            "Collecting proto-plus>=1.15.0\n",
            "  Downloading proto_plus-1.20.3-py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 3.2 MB/s \n",
            "\u001b[?25hCollecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
            "  Downloading google_cloud_resource_manager-1.4.1-py2.py3-none-any.whl (402 kB)\n",
            "\u001b[K     |████████████████████████████████| 402 kB 58.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (1.15.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (1.56.0)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=1.25.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (1.35.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (3.17.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (2022.1)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (2.23.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (1.44.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (0.2.8)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (0.4.1)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.0.3)\n",
            "Collecting grpc-google-iam-v1<0.13dev,>=0.12.3\n",
            "  Downloading grpc_google_iam_v1-0.12.4-py2.py3-none-any.whl (26 kB)\n",
            "Collecting google-cloud-storage<3.0.0dev,>=1.32.0\n",
            "  Downloading google_cloud_storage-2.2.1-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 49.3 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-2.2.0-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 60.9 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-2.1.0-py2.py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 56.0 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-2.0.0-py2.py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 69.7 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.44.0-py2.py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 50.5 MB/s \n",
            "\u001b[?25hCollecting google-cloud-core<2.0dev,>=1.0.3\n",
            "  Downloading google_cloud_core-1.7.2-py2.py3-none-any.whl (28 kB)\n",
            "Collecting google-cloud-storage<3.0.0dev,>=1.32.0\n",
            "  Downloading google_cloud_storage-1.43.0-py2.py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 54.7 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.42.3-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 41.0 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.42.2-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 55.4 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.42.1-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 50.0 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.42.0-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 52.1 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.41.1-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 71.8 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.41.0-py2.py3-none-any.whl (104 kB)\n",
            "\u001b[K     |████████████████████████████████| 104 kB 69.5 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.40.0-py2.py3-none-any.whl (104 kB)\n",
            "\u001b[K     |████████████████████████████████| 104 kB 70.1 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.39.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 68.9 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.38.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 60.5 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.37.1-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 62.9 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.37.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 61.1 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.36.2-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 4.9 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.36.1-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 5.2 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.36.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.0 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.35.1-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 5.1 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.35.0-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 5.2 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.34.0-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 4.9 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.33.0-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 9.4 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.32.0-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 9.2 MB/s \n",
            "\u001b[?25hINFO: pip is looking at multiple versions of google-cloud-resource-manager to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
            "  Downloading google_cloud_resource_manager-1.4.0-py2.py3-none-any.whl (402 kB)\n",
            "\u001b[K     |████████████████████████████████| 402 kB 53.8 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_resource_manager-1.3.3-py2.py3-none-any.whl (286 kB)\n",
            "\u001b[K     |████████████████████████████████| 286 kB 51.7 MB/s \n",
            "\u001b[?25hINFO: pip is looking at multiple versions of google-cloud-core to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting google-cloud-core<2.0dev,>=1.0.3\n",
            "  Downloading google_cloud_core-1.7.1-py2.py3-none-any.whl (28 kB)\n",
            "INFO: pip is looking at multiple versions of google-cloud-resource-manager to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading google_cloud_core-1.7.0-py2.py3-none-any.whl (28 kB)\n",
            "  Downloading google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
            "  Downloading google_cloud_core-1.5.0-py2.py3-none-any.whl (27 kB)\n",
            "  Downloading google_cloud_core-1.4.4-py2.py3-none-any.whl (27 kB)\n",
            "  Downloading google_cloud_core-1.4.3-py2.py3-none-any.whl (27 kB)\n",
            "INFO: pip is looking at multiple versions of google-cloud-core to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading google_cloud_core-1.4.2-py2.py3-none-any.whl (26 kB)\n",
            "  Downloading google_cloud_core-1.4.1-py2.py3-none-any.whl (26 kB)\n",
            "  Downloading google_cloud_core-1.4.0-py2.py3-none-any.whl (26 kB)\n",
            "  Downloading google_cloud_core-1.3.0-py2.py3-none-any.whl (26 kB)\n",
            "  Downloading google_cloud_core-1.2.0-py2.py3-none-any.whl (26 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
            "  Downloading google_cloud_core-1.1.0-py2.py3-none-any.whl (26 kB)\n",
            "  Downloading google_cloud_core-1.0.3-py2.py3-none-any.whl (26 kB)\n",
            "INFO: pip is looking at multiple versions of google-cloud-bigquery to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting google-cloud-bigquery<3.0.0dev,>=1.15.0\n",
            "  Downloading google_cloud_bigquery-2.34.3-py2.py3-none-any.whl (206 kB)\n",
            "\u001b[K     |████████████████████████████████| 206 kB 56.9 MB/s \n",
            "\u001b[?25hCollecting google-cloud-core<3.0.0dev,>=1.4.1\n",
            "  Downloading google_cloud_core-2.3.0-py2.py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
            "Collecting google-resumable-media<3.0dev,>=0.6.0\n",
            "  Downloading google_resumable_media-2.3.2-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting google-crc32c<2.0dev,>=1.0\n",
            "  Downloading google_crc32c-1.3.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38 kB)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-cloud-aiplatform) (3.0.8)\n",
            "Collecting protobuf>=3.12.0\n",
            "  Downloading protobuf-3.20.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 27.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform) (2.10)\n",
            "Installing collected packages: protobuf, google-crc32c, proto-plus, grpc-google-iam-v1, google-resumable-media, google-cloud-core, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, google-cloud-aiplatform\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "  Attempting uninstall: google-resumable-media\n",
            "    Found existing installation: google-resumable-media 0.4.1\n",
            "    Uninstalling google-resumable-media-0.4.1:\n",
            "      Successfully uninstalled google-resumable-media-0.4.1\n",
            "  Attempting uninstall: google-cloud-core\n",
            "    Found existing installation: google-cloud-core 1.0.3\n",
            "    Uninstalling google-cloud-core-1.0.3:\n",
            "      Successfully uninstalled google-cloud-core-1.0.3\n",
            "  Attempting uninstall: google-cloud-storage\n",
            "    Found existing installation: google-cloud-storage 1.18.1\n",
            "    Uninstalling google-cloud-storage-1.18.1:\n",
            "      Successfully uninstalled google-cloud-storage-1.18.1\n",
            "  Attempting uninstall: google-cloud-bigquery\n",
            "    Found existing installation: google-cloud-bigquery 1.21.0\n",
            "    Uninstalling google-cloud-bigquery-1.21.0:\n",
            "      Successfully uninstalled google-cloud-bigquery-1.21.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "pandas-gbq 0.13.3 requires google-cloud-bigquery[bqstorage,pandas]<2.0.0dev,>=1.11.1, but you have google-cloud-bigquery 2.34.3 which is incompatible.\n",
            "google-cloud-translate 1.5.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.3.0 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.3.0 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.3.0 which is incompatible.\u001b[0m\n",
            "Successfully installed google-cloud-aiplatform-1.12.1 google-cloud-bigquery-2.34.3 google-cloud-core-2.3.0 google-cloud-resource-manager-1.4.1 google-cloud-storage-2.3.0 google-crc32c-1.3.0 google-resumable-media-2.3.2 grpc-google-iam-v1-0.12.4 proto-plus-1.20.3 protobuf-3.20.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'restart': True, 'status': 'ok'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "!pip3 uninstall -y google-cloud-aiplatform\n",
        "!pip3 install google-cloud-aiplatform\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "181b681faf5c"
      },
      "source": [
        "### Enter your project and GCS bucket\n",
        "\n",
        "Enter your Project Id in the cell below. Then run the cell to make sure the Cloud SDK uses the right project for all the commands in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d61oYG3KftDw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27c8495f-122b-483b-af98-860b56e02c1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project ID:  \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"\"\n",
        "\n",
        "# Get your Google Cloud project ID from gcloud\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID: \", PROJECT_ID)\n",
        "    \n",
        "MY_STAGING_BUCKET = \"gs://YOUR BUCKET\"  # bucket should be in same region as ucaip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HRQg6eXiolk"
      },
      "source": [
        "Otherwise, set your project ID here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sg6AbQRviolk",
        "outputId": "e889836a-cc58-425f-e790-b57b6c7bd283"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project ID:  vertex-ai-dev\n"
          ]
        }
      ],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
        "    PROJECT_ID = \"[your-project-id]\"   # @param {type:\"string\"}\n",
        "    print(\"Project ID: \", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using Google Cloud Notebooks**, your environment is already\n",
        "authenticated. Skip this step"
      ],
      "metadata": {
        "id": "6x6CSodKjMmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If you are using Colab**, run the cell below and follow the instructions\n",
        "when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "1. In the Cloud Console, go to the [**Create service account key**\n",
        "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
        "\n",
        "2. Click **Create service account**.\n",
        "\n",
        "3. In the **Service account name** field, enter a name, and\n",
        "   click **Create**.\n",
        "\n",
        "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
        "into the filter box, and select\n",
        "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "\n",
        "5. Click *Create*. A JSON file that contains your key downloads to your\n",
        "local environment.\n",
        "\n",
        "6. Enter the path to your service account key as the\n",
        "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
      ],
      "metadata": {
        "id": "ZaQd5jNwjP_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# If on Google Cloud Notebooks, then don't execute this code\n",
        "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ],
      "metadata": {
        "id": "idpLDmlyjWyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Timestamp\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append it onto the name of resources you create in this tutorial."
      ],
      "metadata": {
        "id": "r2lr6-MVpXLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ],
      "metadata": {
        "id": "he2lcG3Jpdxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "When you submit a training job using the Cloud SDK, you upload a Python package\n",
        "containing your training code to a Cloud Storage bucket. Vertex AI runs\n",
        "the code from this package. In this tutorial, Vertex AI also saves the\n",
        "trained model that results from your job in the same bucket. Using this model artifact, you can then\n",
        "create Vertex AI model and endpoint resources in order to serve\n",
        "online predictions.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
        "Cloud Storage buckets.\n",
        "\n",
        "You may also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook. We suggest that you [choose a region where Vertex AI services are\n",
        "available](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions)."
      ],
      "metadata": {
        "id": "VxkQeJLyjgyr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GF076Vmoioll"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}\n",
        "REGION = \"[your-region]\"  # @param {type:\"string\"}\n",
        "\n",
        "if BUCKET_URI == \"\" or BUCKET_URI is None or BUCKET_URI == \"gs://[your-bucket-name]\":\n",
        "    BUCKET_URI = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP\n",
        "\n",
        "if REGION == \"[your-region]\":\n",
        "    REGION = \"us-central1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElTizrkXiolm"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ar8qPT6iolm",
        "outputId": "77e0e442-c0f5-4158-a8f0-4ac73d66854b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating gs://vertex-ai-devaip-20220428142746/...\n"
          ]
        }
      ],
      "source": [
        "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_Hq3rtPiolm"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KfwVmnIiolm"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5T1d5uBoftDw"
      },
      "source": [
        "# Copy Big Query Iris Dataset\n",
        "We will make a Big Query dataset and copy Big Query's public iris table to that dataset. For more information about this dataset please visit: https://archive.ics.uci.edu/ml/datasets/iris "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJF047yNftDw"
      },
      "source": [
        "### Make BQ Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yOl-l_oftDx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f85051f-5619-4ee5-ce47-9d0f13b45c29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BigQuery error in mk operation: Dataset 'vertex-ai-dev:ml_datasets' already\n",
            "exists.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
        "!bq mk {PROJECT_ID}:ml_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xn9TuBZAftDx"
      },
      "source": [
        "### Copy bigquery-public-data.ml_datasets.iris"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISFR8nFfftDx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0136599-e38e-455d-d70f-c6e95447bbc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table 'vertex-ai-dev:ml_datasets.iris' already exists, skipping\n"
          ]
        }
      ],
      "source": [
        "!bq cp -n --project_id={PROJECT_ID} bigquery-public-data:ml_datasets.iris {PROJECT_ID}:ml_datasets.iris "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IltwFqKIftDx"
      },
      "source": [
        "# Create Training Container\n",
        "We will create a directory and write all of our container build artifacts into that folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40BkhtMeftDy"
      },
      "outputs": [],
      "source": [
        "CONTAINER_ARTIFACTS_DIR = \"demo-container-artifacts\"\n",
        "\n",
        "!mkdir {CONTAINER_ARTIFACTS_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVeG-LPOftDy"
      },
      "source": [
        "### Create Cloudbuild YAML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kODuFZCftDy"
      },
      "outputs": [],
      "source": [
        "cloudbuild_yaml = \"\"\"steps:\n",
        "- name: 'gcr.io/cloud-builders/docker'\n",
        "  args: [ 'build', '-t', 'gcr.io/{PROJECT_ID}/test-custom-container', '.' ]\n",
        "images: ['gcr.io/{PROJECT_ID}/test-custom-container']\"\"\".format(\n",
        "    PROJECT_ID=PROJECT_ID\n",
        ")\n",
        "\n",
        "with open(f\"{CONTAINER_ARTIFACTS_DIR}/cloudbuild.yaml\", \"w\") as fp:\n",
        "    fp.write(cloudbuild_yaml)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQ_GUCtZftDz"
      },
      "source": [
        "### Write The Dockerfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rja_jo3rftDz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83326736-b28b-4d88-be10-d514d45e83b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing demo-container-artifacts/Dockerfile\n"
          ]
        }
      ],
      "source": [
        "%%writefile {CONTAINER_ARTIFACTS_DIR}/Dockerfile\n",
        "\n",
        "# Specifies base image and tag\n",
        "FROM gcr.io/google-appengine/python\n",
        "WORKDIR /root\n",
        "\n",
        "# Installs additional packages\n",
        "RUN pip3 install tensorflow tensorflow-io pyarrow\n",
        "\n",
        "# Copies the trainer code to the docker image.\n",
        "COPY test_script.py /root/test_script.py\n",
        "\n",
        "# Sets up the entry point to invoke the trainer.\n",
        "ENTRYPOINT [\"python3\", \"test_script.py\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dfrLShaftDz"
      },
      "source": [
        "### Write the entrypoint script to invoke trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0jSd8NWftDz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92d86286-3f8b-456a-c135-ac3326fe37f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing demo-container-artifacts/test_script.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {CONTAINER_ARTIFACTS_DIR}/test_script.py\n",
        "\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.framework import dtypes\n",
        "from tensorflow_io.bigquery import BigQueryClient\n",
        "from tensorflow_io.bigquery import BigQueryReadSession\n",
        "import tensorflow as tf\n",
        "from tensorflow import feature_column\n",
        "import os\n",
        "\n",
        "training_data_uri = os.environ[\"AIP_TRAINING_DATA_URI\"]\n",
        "validation_data_uri = os.environ[\"AIP_VALIDATION_DATA_URI\"]\n",
        "test_data_uri = os.environ[\"AIP_TEST_DATA_URI\"]\n",
        "data_format = os.environ[\"AIP_DATA_FORMAT\"]\n",
        "\n",
        "def caip_uri_to_fields(uri):\n",
        "    uri = uri[5:]\n",
        "    project, dataset, table = uri.split('.')\n",
        "    return project, dataset, table\n",
        "\n",
        "feature_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
        "\n",
        "target_name = 'species'\n",
        "\n",
        "def transform_row(row_dict):\n",
        "  # Trim all string tensors\n",
        "  trimmed_dict = { column:\n",
        "                  (tf.strings.strip(tensor) if tensor.dtype == 'string' else tensor) \n",
        "                  for (column,tensor) in row_dict.items()\n",
        "                  }\n",
        "  target = trimmed_dict.pop(target_name)\n",
        "\n",
        "  target_float = tf.cond(tf.equal(tf.strings.strip(target), 'versicolor'), \n",
        "                 lambda: tf.constant(1.0),\n",
        "                 lambda: tf.constant(0.0))\n",
        "  return (trimmed_dict, target_float)\n",
        "\n",
        "def read_bigquery(project, dataset, table):\n",
        "  tensorflow_io_bigquery_client = BigQueryClient()\n",
        "  read_session = tensorflow_io_bigquery_client.read_session(\n",
        "      \"projects/\" + project,\n",
        "      project, table, dataset,\n",
        "      feature_names + [target_name],\n",
        "      [dtypes.float64] * 4 + [dtypes.string],\n",
        "      requested_streams=2)\n",
        "\n",
        "  dataset = read_session.parallel_read_rows()\n",
        "  transformed_ds = dataset.map(transform_row)\n",
        "  return transformed_ds\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "training_ds = read_bigquery(*caip_uri_to_fields(training_data_uri)).shuffle(10).batch(BATCH_SIZE)\n",
        "eval_ds = read_bigquery(*caip_uri_to_fields(validation_data_uri)).batch(BATCH_SIZE)\n",
        "test_ds = read_bigquery(*caip_uri_to_fields(test_data_uri)).batch(BATCH_SIZE)\n",
        "\n",
        "feature_columns = []\n",
        "\n",
        "# numeric cols\n",
        "for header in feature_names:\n",
        "  feature_columns.append(feature_column.numeric_column(header))\n",
        "\n",
        "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
        "\n",
        "Dense = tf.keras.layers.Dense\n",
        "model = tf.keras.Sequential(\n",
        "  [\n",
        "    feature_layer,\n",
        "      Dense(16, activation=tf.nn.relu),\n",
        "      Dense(8, activation=tf.nn.relu),\n",
        "      Dense(4, activation=tf.nn.relu),\n",
        "      Dense(1, activation=tf.nn.sigmoid),\n",
        "  ])\n",
        "\n",
        "# Compile Keras model\n",
        "model.compile(\n",
        "    loss='binary_crossentropy', \n",
        "    metrics=['accuracy'],\n",
        "    optimizer='adam')\n",
        "\n",
        "model.fit(training_ds, epochs=5, validation_data=eval_ds)\n",
        "\n",
        "print(model.evaluate(test_ds))\n",
        "\n",
        "tf.saved_model.save(model, os.environ[\"AIP_MODEL_DIR\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LYlV4D2ftD0"
      },
      "source": [
        "### Build The Container"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tGdX7B_ftD1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23ef42d1-1ca2-4418-885d-c7fb0200e2bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating temporary tarball archive of 3 file(s) totalling 3.1 KiB before compression.\n",
            "Uploading tarball of [demo-container-artifacts] to [gs://vertex-ai-dev_cloudbuild/source/1651156805.847043-f9151b3b01084e72ac8c41bc235a520e.tgz]\n",
            "Created [https://cloudbuild.googleapis.com/v1/projects/vertex-ai-dev/locations/global/builds/11e75ab2-4805-4aa0-a501-89a7121a5e3d].\n",
            "Logs are available at [https://console.cloud.google.com/cloud-build/builds/11e75ab2-4805-4aa0-a501-89a7121a5e3d?project=931647533046].\n",
            " REMOTE BUILD OUTPUT\n",
            "starting build \"11e75ab2-4805-4aa0-a501-89a7121a5e3d\"\n",
            "\n",
            "FETCHSOURCE\n",
            "Fetching storage object: gs://vertex-ai-dev_cloudbuild/source/1651156805.847043-f9151b3b01084e72ac8c41bc235a520e.tgz#1651156806447310\n",
            "Copying gs://vertex-ai-dev_cloudbuild/source/1651156805.847043-f9151b3b01084e72ac8c41bc235a520e.tgz#1651156806447310...\n",
            "/ [1 files][  1.4 KiB/  1.4 KiB]                                                \n",
            "Operation completed over 1 objects/1.4 KiB.\n",
            "BUILD\n",
            "Already have image (with digest): gcr.io/cloud-builders/docker\n",
            "Sending build context to Docker daemon  6.656kB\n",
            "Step 1/5 : FROM gcr.io/google-appengine/python\n",
            "latest: Pulling from google-appengine/python\n",
            "Digest: sha256:c6480acd38ca4605e0b83f5196ab6fe8a8b59a0288a7b8216c42dbc45b5de8f6\n",
            "Status: Downloaded newer image for gcr.io/google-appengine/python:latest\n",
            " ---> ce284ab20159\n",
            "Step 2/5 : WORKDIR /root\n",
            " ---> Running in c243103e9a10\n",
            "Removing intermediate container c243103e9a10\n",
            " ---> 74b15f57b8ba\n",
            "Step 3/5 : RUN pip3 install tensorflow tensorflow-io pyarrow\n",
            " ---> Running in 74e7ea27d5bc\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.8.0-cp37-cp37m-manylinux2010_x86_64.whl (497.5 MB)\n",
            "Collecting tensorflow-io\n",
            "  Downloading tensorflow_io-0.25.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.4 MB)\n",
            "Collecting pyarrow\n",
            "  Downloading pyarrow-7.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "Collecting keras<2.9,>=2.8.0rc0\n",
            "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
            "Collecting termcolor>=1.1.0\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "Collecting opt-einsum>=2.3.2\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "Requirement already satisfied: setuptools in /opt/python3.7/lib/python3.7/site-packages (from tensorflow) (40.2.0)\n",
            "Collecting libclang>=9.0.1\n",
            "  Downloading libclang-14.0.1-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n",
            "Collecting typing-extensions>=3.6.6\n",
            "  Downloading typing_extensions-4.2.0-py3-none-any.whl (24 kB)\n",
            "Collecting absl-py>=0.4.0\n",
            "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
            "Collecting grpcio<2.0,>=1.24.3\n",
            "  Downloading grpcio-1.44.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "Collecting astunparse>=1.6.0\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Collecting gast>=0.2.1\n",
            "  Downloading gast-0.5.3-py3-none-any.whl (19 kB)\n",
            "Collecting flatbuffers>=1.12\n",
            "  Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.25.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
            "Collecting numpy>=1.20\n",
            "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "Collecting keras-preprocessing>=1.1.1\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "Collecting six>=1.12.0\n",
            "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "Collecting protobuf>=3.9.2\n",
            "  Downloading protobuf-3.20.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "Collecting wrapt>=1.11.0\n",
            "  Downloading wrapt-1.14.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (75 kB)\n",
            "Collecting google-pasta>=0.1.1\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Collecting h5py>=2.9.0\n",
            "  Downloading h5py-3.6.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n",
            "Collecting tensorboard<2.9,>=2.8\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/python3.7/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow) (0.31.1)\n",
            "Collecting cached-property\n",
            "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "Collecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Collecting werkzeug>=0.11.15\n",
            "  Downloading Werkzeug-2.1.1-py3-none-any.whl (224 kB)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-62.1.0-py3-none-any.whl (1.1 MB)\n",
            "Collecting google-auth<3,>=1.6.3\n",
            "  Downloading google_auth-2.6.6-py2.py3-none-any.whl (156 kB)\n",
            "Collecting requests<3,>=2.21.0\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "Collecting rsa<5,>=3.1.4\n",
            "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
            "Collecting cachetools<6.0,>=2.0.0\n",
            "  Downloading cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "Collecting requests-oauthlib>=0.7.0\n",
            "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Collecting importlib-metadata>=4.4\n",
            "  Downloading importlib_metadata-4.11.3-py3-none-any.whl (18 kB)\n",
            "Collecting zipp>=0.5\n",
            "  Downloading zipp-3.8.0-py3-none-any.whl (5.4 kB)\n",
            "Collecting pyasn1<0.5.0,>=0.4.6\n",
            "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Downloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "Collecting idna<4,>=2.5\n",
            "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
            "Collecting charset-normalizer~=2.0.0\n",
            "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
            "Collecting oauthlib>=3.0.0\n",
            "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
            "Building wheels for collected packages: termcolor\n",
            "  Building wheel for termcolor (setup.py): started\n",
            "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4001 sha256=99ceccf803c344aacbf047c41687c91e0cba87526cc381fbe32bb1f35af49dde\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
            "Successfully built termcolor\n",
            "Installing collected packages: urllib3, pyasn1, idna, charset-normalizer, certifi, zipp, typing-extensions, six, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, setuptools, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, cached-property, absl-py, wrapt, tf-estimator-nightly, termcolor, tensorflow-io-gcs-filesystem, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow-io, tensorflow, pyarrow\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 40.2.0\n",
            "    Uninstalling setuptools-40.2.0:\n",
            "      Successfully uninstalled setuptools-40.2.0\n",
            "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cached-property-1.5.2 cachetools-5.0.0 certifi-2021.10.8 charset-normalizer-2.0.12 flatbuffers-2.0 gast-0.5.3 google-auth-2.6.6 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.44.0 h5py-3.6.0 idna-3.3 importlib-metadata-4.11.3 keras-2.8.0 keras-preprocessing-1.1.2 libclang-14.0.1 markdown-3.3.6 numpy-1.21.6 oauthlib-3.2.0 opt-einsum-3.3.0 protobuf-3.20.1 pyarrow-7.0.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.27.1 requests-oauthlib-1.3.1 rsa-4.8 setuptools-62.1.0 six-1.16.0 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tensorflow-io-0.25.0 tensorflow-io-gcs-filesystem-0.25.0 termcolor-1.1.0 tf-estimator-nightly-2.8.0.dev2021122109 typing-extensions-4.2.0 urllib3-1.26.9 werkzeug-2.1.1 wrapt-1.14.0 zipp-3.8.0\n",
            "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "\u001b[0m\u001b[91mWARNING: You are using pip version 21.2.4; however, version 22.0.4 is available.\n",
            "You should consider upgrading via the '/opt/python3.7/bin/python3.7 -m pip install --upgrade pip' command.\n",
            "\u001b[0mRemoving intermediate container 74e7ea27d5bc\n",
            " ---> ce12a9ad4612\n",
            "Step 4/5 : COPY test_script.py /root/test_script.py\n",
            " ---> f12e325ed91e\n",
            "Step 5/5 : ENTRYPOINT [\"python3\", \"test_script.py\"]\n",
            " ---> Running in 74468af98191\n",
            "Removing intermediate container 74468af98191\n",
            " ---> b22cac9d28dd\n",
            "Successfully built b22cac9d28dd\n",
            "Successfully tagged gcr.io/vertex-ai-dev/test-custom-container:latest\n",
            "PUSH\n",
            "Pushing gcr.io/vertex-ai-dev/test-custom-container\n",
            "The push refers to repository [gcr.io/vertex-ai-dev/test-custom-container]\n",
            "1796c78509ed: Preparing\n",
            "2ea3e4ee148b: Preparing\n",
            "087d7553d285: Preparing\n",
            "16919ab89eca: Preparing\n",
            "74bcef7f7402: Preparing\n",
            "bc9e931c388e: Preparing\n",
            "20896f2c3dd8: Preparing\n",
            "7b80c69caf34: Preparing\n",
            "3bbec54fac0c: Preparing\n",
            "4006ffa4c683: Preparing\n",
            "844d958e8cbe: Preparing\n",
            "84ff92691f90: Preparing\n",
            "b49bce339f97: Preparing\n",
            "dcb7197db903: Preparing\n",
            "bc9e931c388e: Waiting\n",
            "20896f2c3dd8: Waiting\n",
            "7b80c69caf34: Waiting\n",
            "3bbec54fac0c: Waiting\n",
            "4006ffa4c683: Waiting\n",
            "844d958e8cbe: Waiting\n",
            "b49bce339f97: Waiting\n",
            "84ff92691f90: Waiting\n",
            "dcb7197db903: Waiting\n",
            "087d7553d285: Layer already exists\n",
            "16919ab89eca: Layer already exists\n",
            "74bcef7f7402: Layer already exists\n",
            "7b80c69caf34: Layer already exists\n",
            "20896f2c3dd8: Layer already exists\n",
            "bc9e931c388e: Layer already exists\n",
            "3bbec54fac0c: Layer already exists\n",
            "844d958e8cbe: Layer already exists\n",
            "84ff92691f90: Layer already exists\n",
            "4006ffa4c683: Layer already exists\n",
            "b49bce339f97: Layer already exists\n",
            "dcb7197db903: Layer already exists\n",
            "1796c78509ed: Pushed\n",
            "2ea3e4ee148b: Pushed\n",
            "latest: digest: sha256:22c02545cbe3accd31d4e6db0f4ea64823a4f4c5bfd0cbfdcc7f757ecebffd7f size: 3254\n",
            "DONE\n",
            "\n",
            "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                       IMAGES                                                STATUS\n",
            "11e75ab2-4805-4aa0-a501-89a7121a5e3d  2022-04-28T14:40:08+00:00  3M53S     gs://vertex-ai-dev_cloudbuild/source/1651156805.847043-f9151b3b01084e72ac8c41bc235a520e.tgz  gcr.io/vertex-ai-dev/test-custom-container (+1 more)  SUCCESS\n"
          ]
        }
      ],
      "source": [
        "!gcloud builds submit --project={PROJECT_ID} --config {CONTAINER_ARTIFACTS_DIR}/cloudbuild.yaml {CONTAINER_ARTIFACTS_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf0pugbvftD1",
        "tags": []
      },
      "source": [
        "# Run The Custom Container Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ee691569d8d"
      },
      "source": [
        "## Initialize The Vertex AI SDK for Python\n",
        "\n",
        "Initialize the *client* for Vertex AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vEEr62NUftD1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "b93bfe30-418f-4e8e-e993-f2f492c1131f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-4d5db9be8eca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maiplatform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0maiplatform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPROJECT_ID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstaging_bucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBUCKET_URI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'aiplatform' from 'google.cloud' (unknown location)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "736ddff8408b"
      },
      "source": [
        "# Create a Managed Tabular Dataset from Big Query Dataset\n",
        "\n",
        "This section will create a managed Tabular dataset from the iris Big Query table we copied above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBdOv6lWftD1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "213b5840-e7c0-468e-d3d7-5b9bedcb57c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating TabularDataset\n",
            "Create TabularDataset backing LRO: projects/931647533046/locations/us-central1/datasets/8641682007249649664/operations/523981680784965632\n",
            "TabularDataset created. Resource name: projects/931647533046/locations/us-central1/datasets/8641682007249649664\n",
            "To use this TabularDataset in another session:\n",
            "ds = aiplatform.TabularDataset('projects/931647533046/locations/us-central1/datasets/8641682007249649664')\n"
          ]
        }
      ],
      "source": [
        "ds = aiplatform.TabularDataset.create(\n",
        "    display_name=\"bq_iris_dataset\", bq_source=f\"bq://{PROJECT_ID}.ml_datasets.iris\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee242cc1f74c"
      },
      "source": [
        "# Launch The Training Job to Create a Model\n",
        "\n",
        "We will train a model with the container we built above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRGrFdxOftD1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7038b2c-240a-4ee5-f554-c8f04acbdd51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Output directory:\n",
            "gs://vertex-ai-devaip-20220428142746/aiplatform-custom-training-2022-04-28-14:44:30.360 \n",
            "No dataset split provided. The service will use a default split.\n",
            "View Training:\n",
            "https://console.cloud.google.com/ai/platform/locations/us-central1/training/1946107543617011712?project=931647533046\n",
            "CustomContainerTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/1946107543617011712 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "CustomContainerTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/1946107543617011712 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "CustomContainerTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/1946107543617011712 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "CustomContainerTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/1946107543617011712 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "CustomContainerTrainingJob projects/931647533046/locations/us-central1/trainingPipelines/1946107543617011712 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "View backing custom job:\n",
            "https://console.cloud.google.com/ai/platform/locations/us-central1/training/1591273151101140992?project=931647533046\n",
            "CustomContainerTrainingJob run completed. Resource name: projects/931647533046/locations/us-central1/trainingPipelines/1946107543617011712\n",
            "Model available at projects/931647533046/locations/us-central1/models/3478419383678664704\n"
          ]
        }
      ],
      "source": [
        "job = aiplatform.CustomContainerTrainingJob(\n",
        "    display_name=\"train-bq-iris\",\n",
        "    container_uri=f\"gcr.io/{PROJECT_ID}/test-custom-container:latest\",\n",
        "    model_serving_container_image_uri=\"gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-2:latest\",\n",
        ")\n",
        "model = job.run(\n",
        "    ds,\n",
        "    replica_count=1,\n",
        "    model_display_name=\"bq-iris-model\",\n",
        "    bigquery_destination=f\"bq://{PROJECT_ID}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7fa9b59f919"
      },
      "source": [
        "# Deploy The Model\n",
        "\n",
        "Deploy your model, then wait until the model FINISHES deployment before proceeding to prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEg2IDwPftD2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "758265b0-7cf4-498e-9c59-890b9edb6bc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Endpoint\n",
            "Create Endpoint backing LRO: projects/931647533046/locations/us-central1/endpoints/46979932831612928/operations/2320917932105793536\n",
            "Endpoint created. Resource name: projects/931647533046/locations/us-central1/endpoints/46979932831612928\n",
            "To use this Endpoint in another session:\n",
            "endpoint = aiplatform.Endpoint('projects/931647533046/locations/us-central1/endpoints/46979932831612928')\n",
            "Deploying model to Endpoint : projects/931647533046/locations/us-central1/endpoints/46979932831612928\n",
            "Deploy Endpoint model backing LRO: projects/931647533046/locations/us-central1/endpoints/46979932831612928/operations/6932603950533181440\n",
            "Endpoint model deployed. Resource name: projects/931647533046/locations/us-central1/endpoints/46979932831612928\n"
          ]
        }
      ],
      "source": [
        "endpoint = model.deploy(machine_type=\"n1-standard-4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dbd6c650a03",
        "tags": []
      },
      "source": [
        "# Make a prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKVhGB1PftD2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfbf5b4e-8437-48b8-c4aa-bab45c60af79"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Prediction(predictions=[[0.543015361]], deployed_model_id='8055171718668353536', explanations=None)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "endpoint.predict(\n",
        "    [{\"sepal_length\": 5.1, \"sepal_width\": 2.5, \"petal_length\": 3.0, \"petal_width\": 1.1}]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:"
      ],
      "metadata": {
        "id": "MaoIczP8qu--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Delete endpoint resource\n",
        "! gcloud ai endpoints delete $ENDPOINT_NAME --quiet --region $REGION_NAME\n",
        "\n",
        "# Delete Cloud Storage objects that were created\n",
        "! gsutil -m rm -r $JOB_DIR\n",
        "\n",
        "if os.getenv(\"IS_TESTING\"):\n",
        "! gsutil -m rm -r $BUCKET_URI  "
      ],
      "metadata": {
        "id": "-UaP-qoKqzc1"
      },
      "execution_count": 2,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SDK_BigQuery_Custom_Container_Training.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "environment": {
      "kernel": "python3",
      "name": "common-cpu.m91",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title:generic,gcp"
   },
   "source": [
    "# E2E ML on GCP: Get started with serving from Vertex AI Feature Store\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage6/get_started_vertex_feature_store_serving.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "    \n",
    "  <td>\n",
    "        <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage6/get_started_vertex_feature_store_serving.ipynb\">\n",
    "        <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "        </a>\n",
    "  </td>\n",
    "    \n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/ml_ops/stage6/get_started_vertex_feature_store_serving.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "    \n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:mlops"
   },
   "source": [
    "## Overview\n",
    "\n",
    "\n",
    "This tutorial demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. This tutorial covers stage 3 : serving: get started with serving from Feature Store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:mlops,stage2,get_started_vertex_feature_store"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you learn how to use `Vertex AI Feature Store` to train a model and subsequently to serve features when doing online and batch prediction.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services:\n",
    "\n",
    "- `Vertex AI Feature Store`\n",
    "- `Vertex AI Training`\n",
    "- `Vertex AI Prediction`\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Creating a Vertex AI `Featurestore` resource.\n",
    "    - Creating `EntityType` resources for the `Featurestore` resource.\n",
    "    - Creating `Feature` resources for each `EntityType` resource.\n",
    "- Import feature values (entity data items) into `Featurestore` resource.\n",
    "    - From a Cloud Storage location.\n",
    "    - From a pandas DataFrame.\n",
    "- Perform online prediction from a `Featurestore` resource.\n",
    "- Perform batch prediction from a `Featurestore` resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:movies,lbn,avro"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset used in this notebook consists of order items data since 2018 for an online ecommerce\n",
    "store. This dataset is publicly available at `bigquery-public-data.thelook_ecommerce.order_items`\n",
    "BigQuery table which can be accessed by pinning the bigquery-public-data project in BigQuery.\n",
    "\n",
    "The table consists of various fields related to each of the order items like the order_id, product_id,\n",
    "user_id, status, and price when it is created when it has been shipped, etc. Among these fields, the\n",
    "current notebook makes use of the following fields assuming their purpose is as described below :\n",
    "\n",
    "* user_id: The Id of the user.\n",
    "* product_id: The Id of the product.\n",
    "* created_at: When the user has placed the order.\n",
    "* status: The status of the order (Shipped, Processing, Cancelled, Returned, and Completed).\n",
    "\n",
    "The dataset is used to train a Recommender model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81c777b8ad32"
   },
   "source": [
    "### Costs\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "- Vertex AI\n",
    "- Cloud Storage\n",
    "- BigQuery\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing) and [BigQuery pricing](https://cloud.google.com/bigquery/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_mlops"
   },
   "source": [
    "## Installations\n",
    "\n",
    "Install the following packages to further running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_mlops"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "# Install the dependecies\n",
    "! pip3 install --upgrade google-cloud-aiplatform \\\n",
    "                         google-cloud-bigquery \\\n",
    "                         pyarrow \\\n",
    "                         pandas {USER_FLAG} --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "restart"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_id"
   },
   "source": [
    "## Before you begin\n",
    "\n",
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. [Enable the Vertex AI, Compute Engine, Cloud Storage and Cloud Logging APIs](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component,storage_component,logging).\n",
    "\n",
    "1. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    "\n",
    "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_id"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID with `gcloud` command below ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_project_id"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the default project ID in current enviornment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
    "\n",
    "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06571eb4063b"
   },
   "source": [
    "#### UUID\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a uuid for each instance session, and append it onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4e166d927e36"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29b110b44457"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "**If you are using Vertex AI Workbench Notebooks**, your environment is already authenticated. Skip this step.\n",
    "\n",
    "**If you are using Colab**, run the cell below and follow the instructions when prompted to authenticate your account via oAuth.\n",
    "\n",
    "**Otherwise**, follow these steps:\n",
    "\n",
    "In the Cloud Console, go to the [Create service account key](https://console.cloud.google.com/apis/credentials/serviceaccountkey) page.\n",
    "\n",
    "1. **Click Create service account**.\n",
    "\n",
    "2. In the **Service account name** field, enter a name, and click **Create**.\n",
    "\n",
    "3. In the **Grant this service account access to project** section, click the Role drop-down list. Type \"Vertex\" into the filter box, and select **Vertex Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
    "\n",
    "4. Click Create. A JSON file that contains your key downloads to your local environment.\n",
    "\n",
    "5. Enter the path to your service account key as the GOOGLE_APPLICATION_CREDENTIALS variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89788a802687"
   },
   "outputs": [],
   "source": [
    "# IMPORTANT - If you are using Vertex AI Workbench Notebooks, your environment is already authenticated. Skip this step.\n",
    "\n",
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If on Vertex AI Workbench, then don't execute this code\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
    "    \"DL_ANACONDA_HOME\"\n",
    "):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "When you initialize the Vertex AI SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_bucket"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
    "    BUCKET_NAME = PROJECT_ID + \"vai-\" + UUID\n",
    "    BUCKET_URI = \"gs://\" + BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_bucket"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validate_bucket"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "validate_bucket"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set bucket access for Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil uniformbucketlevelaccess set on {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "import google.cloud.aiplatform as aiplatform\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_bq"
   },
   "source": [
    "### Initialize Vertex AI and BigQuery clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_bq"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)\n",
    "bqclient = bigquery.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "container:training,prediction"
   },
   "source": [
    "#### Set pre-built containers\n",
    "\n",
    "Set the pre-built Docker container image for training and prediction.\n",
    "\n",
    "\n",
    "For the latest list, see [Pre-built containers for training](https://cloud.google.com/ai-platform-unified/docs/training/pre-built-containers).\n",
    "\n",
    "\n",
    "For the latest list, see [Pre-built containers for prediction](https://cloud.google.com/ai-platform-unified/docs/predictions/pre-built-containers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "container:training,prediction"
   },
   "outputs": [],
   "source": [
    "TF = \"2.8\".replace(\".\", \"-\")\n",
    "TRAIN_VERSION = \"tf-cpu.{}\".format(TF)\n",
    "DEPLOY_VERSION = \"tf2-cpu.{}\".format(TF)\n",
    "\n",
    "TRAIN_IMAGE = \"{}-docker.pkg.dev/vertex-ai/training/{}:latest\".format(\n",
    "    REGION.split(\"-\")[0], TRAIN_VERSION\n",
    ")\n",
    "DEPLOY_IMAGE = \"{}-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(\n",
    "    REGION.split(\"-\")[0], DEPLOY_VERSION\n",
    ")\n",
    "\n",
    "print(\"Training:\", TRAIN_IMAGE)\n",
    "print(\"Deployment:\", DEPLOY_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "machine:training"
   },
   "source": [
    "#### Set machine type\n",
    "\n",
    "Next, set the machine type to use for training.\n",
    "\n",
    "- Set the variables `TRAIN_COMPUTE`/`DEPLOY_COMPUTE` to configure  the compute resources for the VMs you will use for for training and prediction.\n",
    " - `machine type`\n",
    "     - `n1-standard`: 3.75GB of memory per vCPU.\n",
    "     - `n1-highmem`: 6.5GB of memory per vCPU\n",
    "     - `n1-highcpu`: 0.9 GB of memory per vCPU\n",
    " - `vCPUs`: number of \\[2, 4, 8, 16, 32, 64, 96 \\]\n",
    "\n",
    "*Note: The following is not supported for training:*\n",
    "\n",
    " - `standard`: 2 vCPUs\n",
    " - `highcpu`: 2, 4 and 8 vCPUs\n",
    "\n",
    "*Note: You may also use n2 and e2 machine types for training and deployment, but they do not support GPUs*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "machine:training"
   },
   "outputs": [],
   "source": [
    "MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Train machine type\", TRAIN_COMPUTE)\n",
    "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_intro"
   },
   "source": [
    "## Introduction to Vertex AI Feature Store\n",
    "\n",
    "Let's assume you have a recommendation model that predicts a coupon to print on the back of a cash register receipt. Now, if that model was trained only on single transaction instances (what was bought and how much), then (in the past) you use an Apriori algorithm.\n",
    "\n",
    "But now we have historical data on the customer (say it's indexed by credit card number). Like total purchases to date, average purchase per transaction, frequency of purchase by product category, etc. We use this \"enriched data\" to train a recommender system.\n",
    "\n",
    "Now it's time to do a live prediction. You get a transaction from the cash register, but all it has is the credit card number and this transaction. It does not have the enriched data the model needs. During serving, the credit card number is used as an index to Feature Store to get the enriched data needed for the model.\n",
    "\n",
    "On the other hand, let's say the enriched data the model was trained on was timestamped on June 1st. The current transaction is from June 15th. Assume that the user has made other transactions between June 1st and 15th, and the enriched data has been continuously updated in Feature Store. But the model was trained on June 1st data. Feature Store knows the version number and serves the June 1st version to the model (not the current June 15th). Otherwise, if you used June 15th data, you would have training-serving skew.\n",
    "\n",
    "Another problem here is the data drift. Things change and suddenly one day, everybody is buying toilet paper! There is a significant change in the distribution of existing enriched data from the distribution that the deployed model was trained on. Feature Store can detect changes/thresholds in distribution changes and trigger a notification for retraining the model.\n",
    "\n",
    "Learn more about [Vertex AI Feature Store API](https://cloud.google.com/vertex-ai/docs/featurestore)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_datamodel:movies"
   },
   "source": [
    "## Vertex AI Feature Store data model\n",
    "\n",
    "Vertex AI Feature Store organizes data with the following 3 important hierarchical concepts:\n",
    "\n",
    "        Featurestore -> EntityType -> Feature\n",
    "\n",
    "- `Featurestore`: the place to store your features.\n",
    "- `EntityType`: under a `Featurestore`, an `EntityType` describes an object to be modeled, real one or virtual one.\n",
    "- `Feature`: under an `EntityType`, a `Feature` describes an attribute of the `EntityType`.\n",
    "\n",
    "Learn more about [Vertex AI Feature Store data model](https://cloud.google.com/vertex-ai/docs/featurestore/concepts).\n",
    "\n",
    "In this ecommerce example, you will create a `Featurestore` resource called ecomm_recommendation. This `Featurestore` resource has 2 entity types: \n",
    "- `users`: The entity type has the `product_id`, and `rating` features.\n",
    "- `products`: The entity type has the `user_list` and `product_name` features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_create"
   },
   "source": [
    "## Create a `Featurestore` resource\n",
    "\n",
    "First, you create a `Featurestore` for the dataset using the `Featurestore.create()` method, with the following parameters:\n",
    "\n",
    "- `featurestore_id`: The name of the feature store.\n",
    "- `online_store_fixed_node_count`: Configuration settings for online serving from the feature store.\n",
    "- `project`: The project ID.\n",
    "- `location`: The location (region)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "featurestore_create"
   },
   "outputs": [],
   "source": [
    "# Represents featurestore resource path.\n",
    "FEATURESTORE_NAME = \"ecomm_recommendation\" + UUID\n",
    "\n",
    "featurestore = aiplatform.Featurestore.create(\n",
    "    featurestore_id=FEATURESTORE_NAME,\n",
    "    online_store_fixed_node_count=1,\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    ")\n",
    "\n",
    "print(featurestore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_get"
   },
   "source": [
    "### Get a `Featurestore` resource\n",
    "\n",
    "You can get a specifed `Featurestore` resource in your project using the `Featurestore()` initializer, with the following parameters:\n",
    "\n",
    "- `featurestore_name`: The name for the `Featurestore` resource.\n",
    "- `project`: The project ID.\n",
    "- `location`: The location (region)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "featurestore_get"
   },
   "outputs": [],
   "source": [
    "featurestore = aiplatform.Featurestore(\n",
    "    featurestore_name=FEATURESTORE_NAME, project=PROJECT_ID, location=REGION\n",
    ")\n",
    "print(featurestore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_create:entity_type"
   },
   "source": [
    "## Create entity types for your `Featurestore` resource\n",
    "\n",
    "Next, you create the `EntityType` resources for your `Featurestore` resource using the `create_entity_type()` method, with the following parameters:\n",
    "\n",
    "- `entity_type_id`: The name of the `EntityType` resource.\n",
    "- `description`:  A description of the entity type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "featurestore_create:entity_type"
   },
   "outputs": [],
   "source": [
    "for name, description in [\n",
    "    (\"users\", \"Description of the user\"),\n",
    "    (\"products\", \"Description of the product\"),\n",
    "]:\n",
    "    entity_type = featurestore.create_entity_type(\n",
    "        entity_type_id=name, description=description\n",
    "    )\n",
    "    print(entity_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_create:feature"
   },
   "source": [
    "### Add `Feature` resources for your `EntityType` resources\n",
    "\n",
    "Next, you create the `Feature` resources for each of the `EntityType` resources in your `Featurestore` resource using the `create_feature()` method, with the following parameters:\n",
    "\n",
    "- `feature_id`: The name of the `Feature` resource.\n",
    "- `description`: A description of the feature.\n",
    "- `value_type`: The data type for the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "featurestore_create:feature,movies"
   },
   "outputs": [],
   "source": [
    "def create_features(featurestore_name, entity_name, features):\n",
    "    entity_type = aiplatform.EntityType(\n",
    "        entity_type_name=entity_name, featurestore_id=featurestore_name\n",
    "    )\n",
    "\n",
    "    for feature in features:\n",
    "        feature = entity_type.create_feature(\n",
    "            feature_id=feature[0], description=feature[1], value_type=feature[2]\n",
    "        )\n",
    "        print(feature)\n",
    "\n",
    "\n",
    "create_features(\n",
    "    FEATURESTORE_NAME,\n",
    "    \"users\",\n",
    "    [\n",
    "        (\"product_id\", \"product description\", \"INT64\"),\n",
    "        (\"rating\", \"rating of the product\", \"DOUBLE\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "create_features(\n",
    "    FEATURESTORE_NAME,\n",
    "    \"products\",\n",
    "    [\n",
    "        (\"users_list\", \"List of user ids who bought product\", \"STRING_ARRAY\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform feature engineering on the dataset\n",
    "\n",
    "Next you perform feature engineering on the public BigQuery dataset and then import them into Feature Store.\n",
    "\n",
    "### Load the BigQuery dataset into a dataframe\n",
    "\n",
    "* Load the data from BigQuery into a pandas dataFrame.\n",
    "* Select the columns to use.\n",
    "    - user_id\n",
    "    - product_id\n",
    "    - created_at\n",
    "    - status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = \"\"\"\n",
    "SELECT\n",
    "    CAST(user_id AS STRING) AS user_id,\n",
    "    product_id,\n",
    "    created_at,\n",
    "    status\n",
    "FROM\n",
    "    `bigquery-public-data.thelook_ecommerce.order_items`\n",
    "\"\"\"\n",
    "\n",
    "df_bq_table = bqclient.query(query_string).result().to_dataframe()\n",
    "\n",
    "print(df_bq_table.shape)\n",
    "df_bq_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive a new column ratings\n",
    "\n",
    "Next, you add a new column for the ratings. Since the ratings are numerical, you derive them from the existing status column, as follows:\n",
    "\n",
    "- Map the status string values to a numerical range (0..4). \n",
    "- Normalize the values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the status to a rating\n",
    "rating_map = {\n",
    "    \"Cancelled\": 0,\n",
    "    \"Returned\": 1,\n",
    "    \"Processing\": 2,\n",
    "    \"Shipped\": 3,\n",
    "    \"Complete\": 4,\n",
    "}\n",
    "\n",
    "df_bq_table[\"rating\"] = df_bq_table[\"status\"].map(rating_map)\n",
    "print(df_bq_table.head())\n",
    "\n",
    "# Normalize the ratings\n",
    "min_rating = min(df_bq_table[\"rating\"])\n",
    "max_rating = max(df_bq_table[\"rating\"])\n",
    "\n",
    "df_bq_table[\"rating\"] = (\n",
    "    df_bq_table[\"rating\"]\n",
    "    .apply(lambda x: (x - min_rating) / (max_rating - min_rating))\n",
    "    .values\n",
    ")\n",
    "print(df_bq_table.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the dataset\n",
    "\n",
    "Next, filter the dataset to only users who bought products until last week, and then drop the column 'status'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAST_WEEK_DATE = datetime.now() - pd.to_timedelta(\"7day\")\n",
    "\n",
    "df_filtered = df_bq_table[\n",
    "    (df_bq_table[\"created_at\"] < PAST_WEEK_DATE.isoformat() + \"Z\")\n",
    "].reset_index()\n",
    "\n",
    "result = df_filtered.groupby([\"product_id\"])[\"user_id\"].apply(list).to_dict()\n",
    "\n",
    "df_prod_user_list = pd.DataFrame(result.items(), columns=[\"product_id\", \"users_list\"])\n",
    "df_prod_user_list[\"product_id\"] = df_prod_user_list[\"product_id\"].astype(\"string\")\n",
    "print(df_prod_user_list.head())\n",
    "\n",
    "df_bq_table.drop(\"status\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reimport preprocessed data into BigQuery\n",
    "\n",
    "#### Create destination table for preprocessed data.\n",
    "\n",
    "Next, you create a BigQuery dataset where you will subsequently add tables for the preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DESTINATION_DATASET = f\"product_recommendation_{UUID}\"\n",
    "\n",
    "USERS_SOURCE_TABLE_NAME = \"user_prod_rating_data\"\n",
    "USERS_SOURCE_TABLE_URI = (\n",
    "    f\"bq://{PROJECT_ID}.{DESTINATION_DATASET}.{USERS_SOURCE_TABLE_NAME}\"\n",
    ")\n",
    "\n",
    "PRODUCTS_SOURCE_TABLE_NAME = \"prod_users_list_data\"\n",
    "PRODUCTS_SOURCE_TABLE_URI = (\n",
    "    f\"bq://{PROJECT_ID}.{DESTINATION_DATASET}.{PRODUCTS_SOURCE_TABLE_NAME}\"\n",
    ")\n",
    "\n",
    "# Create destination dataset\n",
    "dataset_id = \"{}.{}\".format(PROJECT_ID, DESTINATION_DATASET)\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "dataset.location = REGION\n",
    "dataset = bqclient.create_dataset(dataset)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create table for filtered dataset\n",
    "\n",
    "Next, you create a table and load the filtered dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"user_id\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"product_id\", \"INT64\"),\n",
    "    bigquery.SchemaField(\"created_at\", \"TIMESTAMP\"),\n",
    "    bigquery.SchemaField(\"rating\", \"FLOAT\"),\n",
    "]\n",
    "\n",
    "table_id = f\"{PROJECT_ID}.{DESTINATION_DATASET}.{USERS_SOURCE_TABLE_NAME}\"\n",
    "table = bigquery.Table(table_id, schema=schema)\n",
    "bqclient.create_table(table, exists_ok=True)\n",
    "\n",
    "\n",
    "# Load data to BQ\n",
    "job = bqclient.load_table_from_dataframe(df_bq_table, table_id)\n",
    "print(job.errors, job.state)\n",
    "while job.running():\n",
    "    from time import sleep\n",
    "\n",
    "    sleep(30)\n",
    "    print(\"Running ...\")\n",
    "print(job.errors, job.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create table for the products user list.\n",
    "\n",
    "Create the new table for the products users list table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "# Create a table\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"product_id\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"users_list\", \"STRING\", \"REPEATED\"),\n",
    "]\n",
    "table_id = f\"{PROJECT_ID}.{DESTINATION_DATASET}.{PRODUCTS_SOURCE_TABLE_NAME}\"\n",
    "table = bigquery.Table(table_id, schema=schema)\n",
    "bqclient.create_table(table, exists_ok=True)\n",
    "\n",
    "# Load data to BQ\n",
    "job = bqclient.load_table_from_dataframe(df_prod_user_list, table_id)\n",
    "print(job.errors, job.state)\n",
    "while job.running():\n",
    "    sleep(30)\n",
    "    print(\"Running ...\")\n",
    "print(job.errors, job.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_import:movies,avro",
    "tags": []
   },
   "source": [
    "## Import the feature data into your `Featurestore` resource\n",
    "\n",
    "Next, you import the feature data for your `Featurestore` resource. Once imported, you can use these feature values for online and offline (batch) serving.\n",
    "\n",
    "### Data layout\n",
    "\n",
    "Each imported `EntityType` resource data must have an ID. Also, each `EntityType` resource data item can optionally have a timestamp, specifying when the feature values were generated.\n",
    "\n",
    "When importing, specify the following in your request:\n",
    "\n",
    "- Data source format: BigQuery Table/Avro/CSV/Pandas Dataframe\n",
    "- Data source URL\n",
    "- Destination: featurestore/entity types/features to be imported\n",
    "\n",
    "In this tutorial, the schema is:\n",
    "\n",
    "    For the Users entity:\n",
    "    schema = {\n",
    "        \"name\": \"users\",\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\":\"product_id\",\n",
    "                \"type\":[\"null\",\"integer\"]\n",
    "            },\n",
    "            {\n",
    "                \"name\":\"rating\",\n",
    "                \"type\":[\"null\",\"double\"]\n",
    "                },\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    For the Products entity:\n",
    "    schema = {\n",
    "        \"name\": \"products\",\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\":\"users_list\",\n",
    "                \"type\":[\"null\",\"string_array\"]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "### Importing the feature values from BigQuery\n",
    "\n",
    "You import the feature values for the `EntityType` resources using the `ingest_from_bq()` method, with the following parameters:\n",
    "\n",
    "- `entity_id_field`: The identifier name for the parent `EntityType` resource.\n",
    "- `feature_ids`: A list of identifier names for `Feature` resources' data to add to the `EntityType` resource.\n",
    "- `feature_time`: The field corresponding to the timestamp for the features being entered.\n",
    "- `bq_source_uri`: The BigQuery table to import data from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "featurestore_import:movies,avro"
   },
   "outputs": [],
   "source": [
    "entity_type = featurestore.get_entity_type(\"users\")\n",
    "response = entity_type.ingest_from_bq(\n",
    "    entity_id_field=\"user_id\",\n",
    "    feature_ids=[\"product_id\", \"rating\"],\n",
    "    feature_time=\"created_at\",\n",
    "    bq_source_uri=f\"bq://{PROJECT_ID}.{DESTINATION_DATASET}.{USERS_SOURCE_TABLE_NAME}\",\n",
    ")\n",
    "print(response)\n",
    "\n",
    "\n",
    "def past_6days():\n",
    "    return datetime.now() - timedelta(days=6)\n",
    "\n",
    "\n",
    "entity_type = featurestore.get_entity_type(\"products\")\n",
    "response = entity_type.ingest_from_bq(\n",
    "    entity_id_field=\"product_id\",\n",
    "    feature_ids=[\"users_list\"],\n",
    "    feature_time=past_6days(),\n",
    "    bq_source_uri=f\"bq://{PROJECT_ID}.{DESTINATION_DATASET}.{PRODUCTS_SOURCE_TABLE_NAME}\",\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_serving"
   },
   "source": [
    "## Vertex AI Feature Store serving\n",
    "\n",
    "The Vertex AI Feature Store service provides the following two services for serving features from a `Featurestore` resource:\n",
    "\n",
    "- Online serving - low-latency serving of small batches of features (prediction).\n",
    "\n",
    "- Batch serving - high-throughput serving of large batches of features (training and prediction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_serving:batch"
   },
   "source": [
    "## Batch Serving\n",
    "\n",
    "The Vertex AI Feature Store's batch serving service is optimized for serving large batches of features in real-time with high throughput, typically for training a model or batch prediction.\n",
    "\n",
    "One can batch serve to the following destinations:\n",
    "\n",
    "- BigQuery table\n",
    "- Cloud Storage location\n",
    "- Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_serving:batch,output,movies"
   },
   "source": [
    "### Output dataset\n",
    "\n",
    "In this notebook, you train a model using data from your feature store in CSV format from Google Cloud Storage\n",
    "\n",
    "### Use case\n",
    "\n",
    "**The task** is to prepare a dataset to train a model, which recommends products for a given user. To achieve this, you need 2 sets of input:\n",
    "\n",
    "*   Features: you already imported into the feature store.\n",
    "*   Labels: the ground-truth data recorded that is rating.\n",
    "\n",
    "To be more specific, the ground-truth observation is described in Table 1 and the desired dataset is described in Table 2. Each row in Table 2 is a result of joining the imported feature values from Vertex AI Feature Store according to the entity IDs and timestamps in Table 1. In this example,  the `product_id` and `rating` features from `users` are chosen to batch train. \n",
    "\n",
    "batch_serve_to_df method takes Table 1 as\n",
    "input for read_instances_df argument joins all required feature values from the feature store, and returns Table 2 for training.\n",
    "\n",
    "<h4 align=\"center\">Table 1. Ground-truth Data</h4>\n",
    "\n",
    "users | timestamp            \n",
    "----- | -------------------- \n",
    "87228 | 2022-07-01T00:00:00Z \n",
    "16173 | 2022-07-01T18:09:43Z \n",
    "...   | ...      | ...     \n",
    "\n",
    "\n",
    "<h4 align=\"center\">Table 2. Expected Training Data Generated by batch_serve_to_df (Positive Samples)</h4>\n",
    "\n",
    "feature_timestamp            | entity_type_users | product_id | rating |\n",
    "-------------------- | ----------------- | --------------- | ---------------- |\n",
    "2022-07-01T00:00:00Z | 87228 | 4567 | 0.5 |\n",
    "2022-07-01T00:00:00Z | 16173 | 5490 | 0.75 |\n",
    "... | ... | ... | ... | ...  \n",
    "\n",
    "#### Why timestamp?\n",
    "\n",
    "Note that there is a `timestamp` column in Table 2. This indicates the time when the ground-truth was observed. This is to avoid data inconsistency.\n",
    "\n",
    "For example, the 1st row of Table 2 indicates that id `87228` brought product on `2022-07-01T00:00:00Z`. The feature store keeps feature values for all timestamps but fetches feature values *only* at the given timestamp during batch serving.\n",
    "\n",
    "### Batch Serve To DataFrame\n",
    "\n",
    "Assemble the request which specifies the following info:\n",
    "\n",
    "*   Where is the label data, i.e., Table 1.\n",
    "*   Which features are read, i.e., the column names in Table 1.\n",
    "\n",
    "Next, you get the dataframe from the feature store using batch_serve_to_df and store it into a csv file that will be used for training the recommender model in Vertex AI.\n",
    "\n",
    "* Export the entityType Id (`users`) and `timestamp` columns as csv into the created GCS bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwNAh1Ysifal"
   },
   "outputs": [],
   "source": [
    "from datetime import timezone\n",
    "\n",
    "past_week_date = (datetime.now() - pd.to_timedelta(\"7day\")).isoformat() + \"Z\"\n",
    "df_sorted = df_bq_table.sort_values(\"created_at\", ascending=False, ignore_index=True)\n",
    "df_sorted.rename(columns={\"user_id\": \"users\"}, inplace=True)\n",
    "df_sorted = df_sorted[df_sorted[\"created_at\"] <= past_week_date].reset_index()\n",
    "df_sorted[\"created_at\"] = df_sorted[\"created_at\"].astype(str)\n",
    "df_sorted[\"timestamp\"] = df_sorted[\"created_at\"].map(\n",
    "    lambda x: datetime.fromisoformat(x).astimezone(timezone.utc)\n",
    ")\n",
    "df_batch = df_sorted[[\"users\", \"timestamp\"]]\n",
    "\n",
    "df_batch.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_serving:batch,read,movies"
   },
   "source": [
    "### Batch Read Feature Values\n",
    "\n",
    " You batch serve entity data items to a dataframe using the `batch_serve_to_df` method with the following parameters:\n",
    "\n",
    "- `serving_feature_ids`: A dictionary of entity type and corresponding features to serve.\n",
    "- `read_instances_uri`: A Cloud Storage location to read the entity data items from.\n",
    "\n",
    "The output is stored in a BigQuery table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "featurestore_serving:batch,read,movies"
   },
   "outputs": [],
   "source": [
    "batch_serve = featurestore.batch_serve_to_df(\n",
    "    serving_feature_ids={\"users\": [\"product_id\", \"rating\"]}, read_instances_df=df_batch\n",
    ")\n",
    "\n",
    "batch_serve.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export dataframe data to CSV\n",
    "\n",
    "Next, you export the dataframe data to a CSV file in Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_FILE = f\"{BUCKET_URI}/data.csv\"\n",
    "\n",
    "batch_serve.to_csv(CSV_FILE, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a recommender model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGBVQPxSifan"
   },
   "source": [
    "In this section, you train a custom model for recommending products for a given user with data from the `batch_serve_to_df` method.\n",
    "\n",
    "You create a custom trained model from a Python script in a Docker container using the Vertex AI SDK for Python, and then get a prediction from the deployed model by sending data.\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Train a Vertex AI custom `TrainingPipeline` to train a TensorFlow model.\n",
    "- Deploy the `Model` resource to a serving `Endpoint` resource.\n",
    "- Make a prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLruf4blifao"
   },
   "source": [
    "### Train a model\n",
    "\n",
    "There are two ways you can train a model using a container image:\n",
    "\n",
    "- **Use a Vertex AI pre-built container**. If you use a pre-built training container, you must additionally specify a Python package to install into the container image. This Python package contains your training code.\n",
    "\n",
    "- **Use your own custom container image**. If you use your own container, the container image must contain your training code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-l9xyOXifao"
   },
   "source": [
    "### Define the command args for the training script\n",
    "\n",
    "Prepare the command-line arguments to pass to your training script.\n",
    "- `args`: The command line arguments to pass to the corresponding Python module. In this example, they are:\n",
    "  - `\"--epochs=\" + EPOCHS`: The number of epochs for training.\n",
    "  - `\"--batch_size=\" + BATCH_SIZE`: The batch size for training.\n",
    "  - `\"--training_data=\" + GCS_PATH`: The path to the csv with training data from feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Ebg4UFWifap"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "CMDARGS = [\n",
    "    \"--epochs=\" + str(EPOCHS),\n",
    "    \"--batch_size=\" + str(BATCH_SIZE),\n",
    "    \"--training_data=\" + CSV_FILE,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKAkzoDjifap"
   },
   "source": [
    "#### Training script\n",
    "\n",
    "Next, you write the contents of the training script, `task.py`. In summary, the script does the following:\n",
    "\n",
    "- Loads the csv data from Google Cloud Storage.\n",
    "- Builds a model using TF.Keras model API.\n",
    "- Compiles the model (`compile()`).\n",
    "- Trains the model (`fit()`) with epochs and batch size according to the arguments `args.epochs` and `args.batch_size`\n",
    "- Gets the directory where to save the model artifacts from the environment variable `AIP_MODEL_DIR`. This variable is [set by the training service](https://cloud.google.com/vertex-ai/docs/training/code-requirements#environment-variables).\n",
    "- Saves the trained model to the model directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GXPR1CVgifap",
    "outputId": "b20a2b96-d654-4689-edaf-f77a03b0a0c7"
   },
   "outputs": [],
   "source": [
    "%%writefile task.py\n",
    "\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "# Read args\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--epochs', dest='epochs',\n",
    "                    default=10, type=int,\n",
    "                    help='Number of epochs.')\n",
    "parser.add_argument('--batch_size', dest='batch_size',\n",
    "                    default=10, type=int,\n",
    "                    help='Batch size.')\n",
    "parser.add_argument('--training_data', dest='training_data', type=str,\n",
    "                    help=\"URI of the training data in BQ\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "# Collect the arguments\n",
    "training_data_uri = args.training_data\n",
    "\n",
    "# Set up training variables\n",
    "LABEL_COLUMN = \"rating\"\n",
    "UNUSED_COLUMNS = [\"timestamp\",\"entity_type_users\",\"product_id\"]\n",
    "NA_VALUES = [\"NA\", \".\", \" \", \"\", \"null\", \"NaN\"]\n",
    "\n",
    "# # Possible categorical values\n",
    "RATING = [0,1,2,3,4]\n",
    "\n",
    "df_train = pd.read_csv(training_data_uri)\n",
    "\n",
    "# Remove NA values\n",
    "def clean_dataframe(df):\n",
    "    return df.replace(to_replace=NA_VALUES, value=np.NaN).dropna()\n",
    "\n",
    "df_train = clean_dataframe(df_train)\n",
    "\n",
    "user_ids = df_train[\"entity_type_users\"].unique().tolist()\n",
    "user2user_encoded = {x: i for i, x in enumerate(user_ids)}\n",
    "\n",
    "product_ids = df_train[\"product_id\"].unique().tolist()\n",
    "product2product_encoded = {x: i for i, x in enumerate(product_ids)}\n",
    "\n",
    "df_train[\"user\"] = df_train[\"entity_type_users\"].map(user2user_encoded)\n",
    "df_train[\"product\"] = df_train[\"product_id\"].map(product2product_encoded)\n",
    "NUM_USERS = len(user2user_encoded)\n",
    "NUM_PRODUCTS = len(product2product_encoded)\n",
    "\n",
    "\n",
    "def convert_dataframe_to_dataset(\n",
    "    df_train,\n",
    "):\n",
    "    NUMERIC_COLUMNS = [\"entity_type_users\",\"product_id\",\"rating\"]\n",
    "    df_train[NUMERIC_COLUMNS] = df_train[NUMERIC_COLUMNS].astype(\"float32\")\n",
    "    df_train = df_train.drop(columns=UNUSED_COLUMNS)\n",
    "\n",
    "    df_train_x, df_train_y = df_train, df_train.pop(LABEL_COLUMN)\n",
    "\n",
    "    y_train = np.asarray(df_train_y).astype(\"float32\")\n",
    "\n",
    "    # Convert to numpy representation\n",
    "    x_train = np.asarray(df_train_x)\n",
    "\n",
    "    dataset_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    return dataset_train\n",
    "\n",
    "# Create datasets\n",
    "dataset_train = convert_dataframe_to_dataset(df_train)\n",
    "\n",
    "# Shuffle train set\n",
    "dataset_train = dataset_train.shuffle(len(df_train))\n",
    "\n",
    "EMBEDDING_SIZE = 50\n",
    "class RecommenderNet(tf.keras.Model):\n",
    "        def __init__(self, num_users, num_products, embedding_size, **kwargs):\n",
    "            super(RecommenderNet, self).__init__(**kwargs)\n",
    "            self.num_users = num_users\n",
    "            self.num_products = num_products\n",
    "            self.embedding_size = embedding_size\n",
    "            self.user_embedding = tf.keras.layers.Embedding(\n",
    "                num_users,\n",
    "                embedding_size,\n",
    "                embeddings_initializer=\"he_normal\",\n",
    "                embeddings_regularizer=tf.keras.regularizers.l2(1e-6),\n",
    "            )\n",
    "            self.user_bias = tf.keras.layers.Embedding(num_users, 1)\n",
    "            self.product_embedding = tf.keras.layers.Embedding(\n",
    "                num_products,\n",
    "                embedding_size,\n",
    "                embeddings_initializer=\"he_normal\",\n",
    "                embeddings_regularizer=tf.keras.regularizers.l2(1e-6),\n",
    "            )\n",
    "            self.product_bias = tf.keras.layers.Embedding(num_products, 1)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            user_vector = self.user_embedding(inputs[:, 0])\n",
    "            user_bias = self.user_bias(inputs[:, 0])\n",
    "            product_vector = self.product_embedding(inputs[:, 1])\n",
    "            product_bias = self.product_bias(inputs[:, 1])\n",
    "            dot_user_product = tf.tensordot(user_vector, product_vector, 2)\n",
    "            # Add all the components (including bias)\n",
    "            x = dot_user_product + user_bias + product_bias\n",
    "            # The sigmoid activation forces the rating to between 0 and 1\n",
    "            return tf.nn.sigmoid(x)\n",
    "\n",
    "def create_model(num_users,num_products):\n",
    "    # Create model\n",
    "        model = RecommenderNet(num_users, num_products, EMBEDDING_SIZE)\n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        )\n",
    "        return model\n",
    "\n",
    "\n",
    "model = create_model(num_users=NUM_USERS,num_products=NUM_PRODUCTS)\n",
    "\n",
    "dataset_train = dataset_train.batch(args.batch_size)\n",
    "\n",
    "# Train the model\n",
    "model.fit(dataset_train, epochs=args.epochs)\n",
    "\n",
    "tf.saved_model.save(model, os.getenv(\"AIP_MODEL_DIR\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZJ0ic_oifaq"
   },
   "source": [
    "### Train the model\n",
    "\n",
    "Use the `CustomTrainingJob` class to define the `TrainingPipeline`. The class takes the following parameters:\n",
    "\n",
    "- `display_name`: The user-defined name of this training pipeline.\n",
    "- `script_path`: The local path to the training script.\n",
    "- `container_uri`: The URI of the training container image.\n",
    "- `requirements`: The list of Python package dependencies of the script.\n",
    "- `model_serving_container_image_uri`: The URI of a container that can serve predictions for your model  either a pre-built container or a custom container.\n",
    "\n",
    "Use the `run` function to start training. The function takes the following parameters:\n",
    "\n",
    "- `args`: The command line arguments to be passed to the Python script.\n",
    "- `replica_count`: The number of worker replicas.\n",
    "- `model_display_name`: The display name of the `Model` if the script produces a managed `Model`.\n",
    "- `machine_type`: The type of machine to use for training.\n",
    "- `accelerator_type`: The hardware accelerator type.\n",
    "- `accelerator_count`: The number of accelerators to attach to a worker replica.\n",
    "\n",
    "The `run` function creates a training pipeline that trains and creates a `Model` object. After the training pipeline completes, the `run` function returns the `Model` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxGdxjxkifaq",
    "outputId": "bb148c7a-81cd-4c59-a5d7-f8bff0d76af7"
   },
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "DEPLOYED_NAME = f\"product-recommender-{UUID}-{TIMESTAMP}\"\n",
    "\n",
    "job = aiplatform.CustomTrainingJob(\n",
    "    display_name=DEPLOYED_NAME,\n",
    "    script_path=\"task.py\",\n",
    "    container_uri=TRAIN_IMAGE,\n",
    "    requirements=[\"google-cloud-bigquery>=2.20.0\", \"db-dtypes\"],\n",
    "    model_serving_container_image_uri=DEPLOY_IMAGE,\n",
    ")\n",
    "\n",
    "# Start the training\n",
    "model = job.run(\n",
    "    model_display_name=DEPLOYED_NAME,\n",
    "    args=CMDARGS,\n",
    "    replica_count=1,\n",
    "    machine_type=TRAIN_COMPUTE,\n",
    "    accelerator_count=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model\n",
    "\n",
    "Next, you deploy the trained model to an `Endpoint`. You can do this by calling the `deploy` function on the `Model` resource. This will do two things:\n",
    "\n",
    "1. Create an `Endpoint` resource for deploying the `Model` resource.\n",
    "2. Deploy the `Model` resource to the `Endpoint` resource.\n",
    "\n",
    "\n",
    "The function takes the following parameters:\n",
    "\n",
    "- `deployed_model_display_name`: A human-readable name for the deployed model.\n",
    "- `traffic_split`: Percent of traffic at the endpoint that goes to this model, which is specified as a dictionary of one or more key/value pairs.\n",
    "   - If only one model, then specify `{ \"0\": 100 }`, where \"0\" refers to this model being uploaded and 100 means 100% of the traffic.\n",
    "   - If there are existing models on the endpoint, for which the traffic will be split, then use `model_id` to specify `{ \"0\": percent, model_id: percent, ... }`, where `model_id` is the ID of an existing `DeployedModel` on the endpoint. The percentages must add up to 100.\n",
    "- `machine_type`: The type of machine to use for training.\n",
    "- `accelerator_type`: The hardware accelerator type.\n",
    "- `accelerator_count`: The number of accelerators to attach to a worker replica.\n",
    "- `starting_replica_count`: The number of compute instances to initially provision.\n",
    "- `max_replica_count`: The maximum number of compute instances to scale to. In this tutorial, only one instance is provisioned.\n",
    "\n",
    "#### Traffic split\n",
    "\n",
    "The `traffic_split` parameter is specified as a Python dictionary. You can deploy more than one instance of your model to an endpoint, and then set the percentage of traffic that goes to each instance.\n",
    "\n",
    "You can use a traffic split to introduce a new model gradually into production. For example, if you had one existing model in production with 100% of the traffic, you could deploy a new model to the same endpoint, direct 10% of traffic to it, and reduce the original model's traffic to 90%. This allows you to monitor the new model's performance while minimizing the disruption to the majority of users.\n",
    "\n",
    "#### Compute instance scaling\n",
    "\n",
    "You can specify a single instance (or node) to serve your online prediction requests. This tutorial uses a single node, so the variables `MIN_NODES` and `MAX_NODES` are both set to `1`.\n",
    "\n",
    "If you want to use multiple nodes to serve your online prediction requests, set `MAX_NODES` to the maximum number of nodes you want to use. Vertex AI auto-scales the number of nodes used to serve your predictions, up to the maximum number you set. Refer to the [pricing page](https://cloud.google.com/vertex-ai/pricing#prediction-prices) to understand the costs of autoscaling with multiple nodes.\n",
    "\n",
    "#### Endpoint\n",
    "\n",
    "The method will block until the model is deployed and eventually return an `Endpoint` object. If this is the first time a model is deployed to the endpoint, it may take a few additional minutes to complete the provisioning of resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "DEPLOYED_NAME = f\"product-recommender-{UUID}-{TIMESTAMP}\"\n",
    "\n",
    "TRAFFIC_SPLIT = {\"0\": 100}\n",
    "\n",
    "MIN_NODES = 1\n",
    "MAX_NODES = 1\n",
    "\n",
    "endpoint = model.deploy(\n",
    "    deployed_model_display_name=DEPLOYED_NAME,\n",
    "    traffic_split=TRAFFIC_SPLIT,\n",
    "    machine_type=DEPLOY_COMPUTE,\n",
    "    min_replica_count=MIN_NODES,\n",
    "    max_replica_count=MAX_NODES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prediction\n",
    "Finally, you make a online prediction to your recommender model that was deployed to an endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the test item\n",
    "You use a test item from the test slice of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from google.cloud import storage\n",
    "\n",
    "# Set up training variables\n",
    "LABEL_COLUMN = \"rating\"\n",
    "UNUSED_COLUMNS = [\"timestamp\", \"entity_type_users\", \"product_id\"]\n",
    "NA_VALUES = [\"NA\", \".\", \" \", \"\", \"null\", \"NaN\"]\n",
    "\n",
    "# # Possible categorical values\n",
    "RATING = [0, 1, 2, 3, 4]\n",
    "\n",
    "df_test = pd.read_csv(CSV_FILE)\n",
    "\n",
    "# Remove NA values\n",
    "def clean_dataframe(df):\n",
    "    return df.replace(to_replace=NA_VALUES, value=np.NaN).dropna()\n",
    "\n",
    "\n",
    "df_test = clean_dataframe(df_test)\n",
    "\n",
    "user_ids = df_test[\"entity_type_users\"].unique().tolist()\n",
    "user2user_encoded = {x: i for i, x in enumerate(user_ids)}\n",
    "product_ids = df_test[\"product_id\"].unique().tolist()\n",
    "product_encoded2product = {i: x for i, x in enumerate(product_ids)}\n",
    "product2product_encoded = {x: i for i, x in enumerate(product_ids)}\n",
    "\n",
    "df_test[\"user\"] = df_test[\"entity_type_users\"].map(user2user_encoded)\n",
    "df_test[\"product\"] = df_test[\"product_id\"].map(product2product_encoded)\n",
    "\n",
    "sample = df_test.sample(1)\n",
    "user_id = sample[\"user\"].values[0]\n",
    "products_bought = sample[\"product\"].to_list()\n",
    "products_not_bought = (\n",
    "    df_test[~df_test[\"product\"].isin(products_bought)][\"product\"].unique().tolist()\n",
    ")\n",
    "\n",
    "instances_input = [[float(user_id), k] for k in products_not_bought]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send the prediction request\n",
    "Next, you make the prediction request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = endpoint.predict(instances=instances_input)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Top 10 products recommendation\n",
    "Based upon the ratings predicted by recommendation model, We selected top 10 products for the selected `user_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions_array = np.array(\n",
    "    [prediction.predictions[k][0] for k in range(len(prediction.predictions))]\n",
    ")\n",
    "top_rating_indices = predictions_array.argsort()[-10:][::-1]\n",
    "top_predictions = predictions_array[top_rating_indices]\n",
    "top_10_products = [\n",
    "    int(product_encoded2product.get(instances_input[k][1])) for k in top_rating_indices\n",
    "]\n",
    "print(top_10_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "delete_bq_dataset"
   },
   "source": [
    "## Cleaning up\n",
    "### Delete the BigQuery dataset\n",
    "\n",
    "Use the method `delete_dataset()` to delete a BigQuery dataset along with all its tables, by setting the parameter `delete_contents` to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "delete_bq_dataset"
   },
   "outputs": [],
   "source": [
    "DESTINATION_DATASET = f\"product_recommendation_{UUID}\"\n",
    "dataset_id = \"{}.{}\".format(PROJECT_ID, DESTINATION_DATASET)\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "bqclient.delete_dataset(dataset, delete_contents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_delete"
   },
   "source": [
    "### Delete a `Featurestore` resource\n",
    "\n",
    "You can get a delete a specified `Featurestore` resource using the `delete()` method, with the following parameter:\n",
    "\n",
    "- `force`: A flag indicating whether to delete a non-empy `Featurestore` resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "featurestore_delete"
   },
   "outputs": [],
   "source": [
    "featurestore.delete(force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the Vertex AI `Model` and `Endpoint`\n",
    "\n",
    "Next, undelpoy and delete the Vertex AI Model and Endpoint resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.undeploy_all()\n",
    "endpoint.delete()\n",
    "model.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Google Cloud Bucket Bucket\n",
    "Finally, you delete the Google Cloud Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil -m rm -r $BUCKET_URI\n",
    "! gsutil rb $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "get_started_vertex_feature_store.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

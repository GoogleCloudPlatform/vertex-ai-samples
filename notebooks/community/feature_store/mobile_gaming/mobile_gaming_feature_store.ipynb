{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/vertex-ai-samples/notebooks/community/feature_store/mobile_gaming_feature_store.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/inardini/vertex-ai-samples/blob/main/vertex-ai-samples/notebooks/community/feature_store/mobile_gaming_feature_store.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FZeBEwdXS4d"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Imagine you are a member of the Data Science team working on the same Mobile Gaming application reported in the [Churn prediction for game developers using Google Analytics 4 (GA4) and BigQuery ML](https://cloud.google.com/blog/topics/developers-practitioners/churn-prediction-game-developers-using-google-analytics-4-ga4-and-bigquery-ml) blog post. \n",
        "\n",
        "Your team successfully implemented a model that determines the likelihood of specific users returning to your app and consumes that insight to drive marketing incentives. As a result, the company consolidates its user base. \n",
        "\n",
        "Now, businesses want to use that information in real-time to monetize it by implementing a conditional ads system. In particular, each time a user plays with the app, they want to display ads depending on the customer demographic,  behavioral information and the resulting propensity of return. Of course, the new application should work with a minimum impact on the user experience. \n",
        "\n",
        "Given the business challenge, the team is required to design and build a possible serving system which needs to minimize real-time prediction serving latency. \n",
        "\n",
        "The assumptions are:\n",
        "\n",
        "1.   Predictions would be delivered synchronously\n",
        "2.   Scalability, support for multiple ML frameworks and security are essential.\n",
        "3.   Only demographic features (country, operating system and language) passed in real time.\n",
        "2.   The system would be able to handle behavioral features as static reference features calculated each 24h (offline batch feature engineering job). \n",
        "3.   It has to migrate training serving skew by a timestamp data model, a point-in-time lookups to avoid data leakage and a feature distribution monitoring service. \n",
        "\n",
        "Based on those assumptions, low read-latency lookup data store and a performing serving engine are needed. Indeed, about the data store, even if you can implement governance on BigQuery, it is still not optimized for singleton lookup operations. Also, the solution need a low overhead serving system that can seamlessly scale up and down based on requests.\n",
        "\n",
        "Last year, Google Cloud announced Vertex AI, a managed machine learning (ML) platform that allows data science teams to accelerate the deployment and maintenance of ML models. The platform is composed of several building blocks and two of them are Vertex AI Feature store and Vertex AI prediction. \n",
        "\n",
        "With Vertex AI Feature store, you have a managed service for low latency scalable feature serving. It also provides a centralized feature repository with easy APIs to search & discover features and feature monitoring capabilities to track drift and other quality issues. With Vertex AI Prediction, you will deploy models into production more easily with online serving via HTTP or batch prediction for bulk scoring. It offers a unified scalable framework to deploy custom models trained in TensorFlow, scikit or XGB, as well as BigQuery ML and AutoML models, and on a broad range of machine types and GPUs.\n",
        "\n",
        "Below the high level picture puts together once the team decides to go with Google Cloud:\n",
        "\n",
        "<img src=\"./assets/solution_overview_final.png\"/>\n",
        "\n",
        "In order:\n",
        "\n",
        "1.   Once you create historical features, they are ingested into Vertex AI Feature store\n",
        "\n",
        "2.   Then you can train and deploy the model using BigQuery (or AutoML)\n",
        "\n",
        "3.   Once the model is deployed, the ML serving engine will receive a prediction request passing entity ID and demographic attributes. \n",
        "\n",
        "4.   Features related to a specific entity will be retrieved from the Vertex AI Feature store and passed them as inputs to the model for online prediction.\n",
        "\n",
        "5.   The predictions will be returned back to the activation layer.\n",
        "\n",
        "\n",
        "### Dataset\n",
        "\n",
        "The dataset is the public sample export data from an actual mobile game app called \"Flood It!\" (Android, iOS)\n",
        "\n",
        "### Objective\n",
        "\n",
        "In the following notebook, you will learn the role of Vertex AI Feature Store in a scenario when the user's activities within the first 24 hours of first user engagement and the gaming platform would consume in order to offer conditional ads.\n",
        "\n",
        "**Notice that we assume that already know how to set up a Vertex AI Feature store. In case you are not, please check out [this detailed notebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/feature_store/gapic-feature-store.ipynb).**\n",
        "\n",
        "At the end, you will more confident about how Vertex AI Feature store\n",
        "\n",
        "1.   Provide a centralized feature repository with easy APIs to search & discover features and fetch them for training/serving. \n",
        "\n",
        "2.   Simplify deployments of models for Online Prediction, via low latency scalable feature serving.\n",
        "\n",
        "3.   Mitigate training serving skew and data leakage by performing point in time lookups to fetch historical data for training.\n",
        "\n",
        "### Costs \n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* BigQuery\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze4-nDLfK4pw"
      },
      "source": [
        "### Set up your local development environment\n",
        "\n",
        "**If you are using Colab or Google Cloud Notebooks**, your environment already meets\n",
        "all the requirements to run this notebook. You can skip this step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCuSR8GkAgzl"
      },
      "source": [
        "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
        "You need the following:\n",
        "\n",
        "* The Google Cloud SDK\n",
        "* Git\n",
        "* Python 3\n",
        "* virtualenv\n",
        "* Jupyter notebook running in a virtual environment with Python 3\n",
        "\n",
        "The Google Cloud guide to [Setting up a Python development\n",
        "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
        "installation guide](https://jupyter.org/install) provide detailed instructions\n",
        "for meeting these requirements. The following steps provide a condensed set of\n",
        "instructions:\n",
        "\n",
        "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
        "\n",
        "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
        "\n",
        "1. [Install\n",
        "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
        "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
        "\n",
        "1. To install Jupyter, run `pip3 install jupyter` on the\n",
        "command-line in a terminal shell.\n",
        "\n",
        "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
        "\n",
        "1. Open this notebook in the Jupyter Notebook Dashboard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7EUnXsZhAGF"
      },
      "source": [
        "### Install additional packages\n",
        "\n",
        "Install additional package dependencies not installed in your notebook environment, such as {XGBoost, AdaNet, or TensorFlow Hub TODO: Replace with relevant packages for the tutorial}. Use the latest major GA version of each package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b4ef9b72d43"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzEo6DeE2GOP"
      },
      "outputs": [],
      "source": [
        "! pip3 install --upgrade pip\n",
        "! pip3 install {USER_FLAG} --upgrade git+https://github.com/googleapis/python-aiplatform.git@main -q --no-warn-conflicts\n",
        "! pip3 install {USER_FLAG} --upgrade pandas==1.3.5 -q --no-warn-conflicts\n",
        "! pip3 install {USER_FLAG} --upgrade google-cloud-bigquery==2.24.0 -q --no-warn-conflicts\n",
        "! pip3 install {USER_FLAG} --upgrade tensorflow==2.8.0 -q --no-warn-conflicts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhq5zEbGg0XX"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzrelQZ22IZj"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWEdiXsJg0XY"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF1j6f9HApxa"
      },
      "source": [
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "1. [Enable the Vertex AI API and Compute Engine API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component). \n",
        "\n",
        "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
        "\n",
        "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WReHDGG5g0XY"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM1iC_MfAts1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"\"\n",
        "\n",
        "# Get your Google Cloud project ID from gcloud\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID: \", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJYoRfYng0XZ"
      },
      "source": [
        "Otherwise, set your project ID here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riG_qUokg0XZ"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
        "    PROJECT_ID = \"\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEjRdjxBuDsi"
      },
      "outputs": [],
      "source": [
        "!gcloud config set project '' #change it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06571eb4063b"
      },
      "source": [
        "#### Timestamp\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append it onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "697568e92bd6"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr--iN2kAylZ"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using Google Cloud Notebooks**, your environment is already\n",
        "authenticated. Skip this step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBCra4QMA2wR"
      },
      "source": [
        "**If you are using Colab**, run the cell below and follow the instructions\n",
        "when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "1. In the Cloud Console, go to the [**Create service account key**\n",
        "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
        "\n",
        "2. Click **Create service account**.\n",
        "\n",
        "3. In the **Service account name** field, enter a name, and\n",
        "   click **Create**.\n",
        "\n",
        "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
        "into the filter box, and select\n",
        "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "\n",
        "5. Click *Create*. A JSON file that contains your key downloads to your\n",
        "local environment.\n",
        "\n",
        "6. Enter the path to your service account key as the\n",
        "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# If on Google Cloud Notebooks, then don't execute this code\n",
        "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgPO1eR3CYjk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
        "Cloud Storage buckets.\n",
        "\n",
        "You may also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook. Make sure to [choose a region where Vertex AI services are\n",
        "available](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions). You may\n",
        "not use a Multi-Regional Storage bucket for training with Vertex AI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzGDU7TWdts_"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = \"\"  # @param {type:\"string\"}\n",
        "REGION = \"[your-region]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf221059d072"
      },
      "outputs": [],
      "source": [
        "if BUCKET_URI == \"\" or BUCKET_URI is None or BUCKET_URI == \"gs://[your-bucket-name]\":\n",
        "    BUCKET_URI = \"gs://\" + PROJECT_ID + \"-aip-\" + TIMESTAMP\n",
        "\n",
        "if REGION == \"[your-region]\":\n",
        "    REGION = \"us-central1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EcIXiGsCePi"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIq7R4HZCfIc"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "994afa65eaa2"
      },
      "source": [
        "Run the following cell to grant access to your Cloud Storage resources from Vertex AI Feature store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psP1rPU9TRnX"
      },
      "outputs": [],
      "source": [
        "! gsutil uniformbucketlevelaccess set on $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucvCsknMCims"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhOb7YnwClBb"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utO91mebwEmw"
      },
      "source": [
        "### Create a Bigquery dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3G2BXqswb_J"
      },
      "outputs": [],
      "source": [
        "BQ_DATASET = \"Mobile_Gaming\"  # @param {type:\"string\"}\n",
        "LOCATION = \"US\"\n",
        "\n",
        "!bq mk --location=$LOCATION --dataset $PROJECT_ID:$BQ_DATASET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRUOFELefqf1"
      },
      "outputs": [],
      "source": [
        "# General\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "# Data Engineering\n",
        "import pandas as pd\n",
        "# Vertex AI and its Feature Store\n",
        "from google.cloud import aiplatform as vertex_ai\n",
        "from google.cloud import bigquery\n",
        "# EntityType\n",
        "from google.cloud.aiplatform import Feature, Featurestore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgP_N3MSpnd3"
      },
      "source": [
        "### Define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_k2ejxQsET8"
      },
      "outputs": [],
      "source": [
        "# Data Engineering and Feature Engineering\n",
        "FEATURES_TABLE = \"wide_features_table\"  # @param {type:\"string\"}\n",
        "MIN_DATE = \"2018-10-03\"\n",
        "MAX_DATE = \"2018-10-04\"\n",
        "FEATURES_TABLE_DAY_ONE = f\"wide_features_table_{MIN_DATE}\"\n",
        "FEATURES_TABLE_DAY_TWO = f\"wide_features_table_{MAX_DATE}\"\n",
        "FEATURESTORE_ID = \"mobile_gaming\"  # @param {type:\"string\"}\n",
        "ENTITY_TYPE_ID = \"user\"\n",
        "\n",
        "# BQ Model Training and Deployment\n",
        "MODEL_NAME = f\"churn_logit_classifier_{TIMESTAMP}\"\n",
        "MODEL_TYPE = \"LOGISTIC_REG\"\n",
        "AUTO_CLASS_WEIGHTS = \"TRUE\"\n",
        "MAX_ITERATIONS = \"50\"\n",
        "INPUT_LABEL_COLS = \"churned\"\n",
        "JOB_ID = f\"extract_{MODEL_NAME}_{TIMESTAMP}\"\n",
        "MODEL_SOURCE = bigquery.model.ModelReference.from_api_repr(\n",
        "    {\"projectId\": PROJECT_ID, \"datasetId\": BQ_DATASET, \"modelId\": MODEL_NAME}\n",
        ")\n",
        "SERVING_DIR = \"serving_dir\"\n",
        "DESTINATION_URI = f\"{BUCKET_URI}/model\"\n",
        "EXTRACT_JOB_CONFIG = bigquery.ExtractJobConfig(destination_format=\"ML_TF_SAVED_MODEL\")\n",
        "VERSION = \"v1\"\n",
        "SERVING_CONTAINER_IMAGE_URI = (\n",
        "    \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-7:latest\"\n",
        ")\n",
        "ENDPOINT_NAME = \"mobile_gaming_churn\"\n",
        "DEPLOYED_MODEL_NAME = f\"churn_logistic_classifier_{VERSION}\"\n",
        "\n",
        "# Vertex AI Feature store\n",
        "ONLINE_STORE_NODES_COUNT = 3\n",
        "ENTITY_ID = \"user\"\n",
        "API_ENDPOINT = f\"{REGION}-aiplatform.googleapis.com\"\n",
        "FEATURE_TIME = \"user_first_engagement\"\n",
        "ENTITY_ID_FIELD = \"user_pseudo_id\"\n",
        "BQ_SOURCE_URI_DAY_ONE = f\"bq://{PROJECT_ID}.{BQ_DATASET}.{FEATURES_TABLE_DAY_ONE}\"\n",
        "BQ_SOURCE_URI_DAY_TWO = f\"bq://{PROJECT_ID}.{BQ_DATASET}.{FEATURES_TABLE_DAY_TWO}\"\n",
        "BQ_DESTINATION_OUTPUT_URI = f\"bq://{PROJECT_ID}.{BQ_DATASET}.train_snapshot_{TIMESTAMP}\"\n",
        "SERVING_FEATURE_IDS = {\"customer\": [\"*\"]}\n",
        "READ_INSTANCES_TABLE = f\"ground_truth_{TIMESTAMP}\"\n",
        "READ_INSTANCES_URI = f\"bq://{PROJECT_ID}.{BQ_DATASET}.{READ_INSTANCES_TABLE}\"\n",
        "\n",
        "# Vertex AI AutoML model\n",
        "DATASET_NAME = f\"churn_mobile_gaming_{TIMESTAMP}\"\n",
        "AUTOML_TRAIN_JOB_NAME = f\"automl_classifier_training_{TIMESTAMP}\"\n",
        "AUTOML_MODEL_NAME = f\"churn_automl_classifier_{TIMESTAMP}\"\n",
        "MODEL_DEPLOYED_NAME = \"churn_automl_classifier_v1\"\n",
        "SERVING_MACHINE_TYPE = \"n1-highcpu-4\"\n",
        "MIN_NODES = 1\n",
        "MAX_NODES = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTjqqE0jafnn"
      },
      "source": [
        "### Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dJ_TGTVak7u"
      },
      "outputs": [],
      "source": [
        "def run_bq_query(query: str):\n",
        "    \"\"\"\n",
        "    An helper function to run a BigQuery job\n",
        "    Args:\n",
        "        query: a formatted SQL query\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    try:\n",
        "        job = bq_client.query(query)\n",
        "        _ = job.result()\n",
        "    except RuntimeError as error:\n",
        "        print(error)\n",
        "\n",
        "\n",
        "def upload_model(\n",
        "    display_name: str,\n",
        "    serving_container_image_uri: str,\n",
        "    artifact_uri: str,\n",
        "    sync: bool = True,\n",
        ") -> vertex_ai.Model:\n",
        "    \"\"\"\n",
        "\n",
        "    Args:\n",
        "        display_name: The name of Vertex AI Model artefact\n",
        "        serving_container_image_uri: The uri of the serving image\n",
        "        artifact_uri: The uri of artefact to import\n",
        "        sync:\n",
        "\n",
        "    Returns: Vertex AI Model\n",
        "\n",
        "    \"\"\"\n",
        "    model = vertex_ai.Model.upload(\n",
        "        display_name=display_name,\n",
        "        artifact_uri=artifact_uri,\n",
        "        serving_container_image_uri=serving_container_image_uri,\n",
        "        sync=sync,\n",
        "    )\n",
        "    model.wait()\n",
        "    print(model.display_name)\n",
        "    print(model.resource_name)\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_endpoint(display_name: str) -> vertex_ai.Endpoint:\n",
        "    \"\"\"\n",
        "    An utility to create a Vertex AI Endpoint\n",
        "    Args:\n",
        "        display_name: The name of Endpoint\n",
        "\n",
        "    Returns: Vertex AI Endpoint\n",
        "\n",
        "    \"\"\"\n",
        "    endpoint = vertex_ai.Endpoint.create(display_name=display_name)\n",
        "\n",
        "    print(endpoint.display_name)\n",
        "    print(endpoint.resource_name)\n",
        "    return endpoint\n",
        "\n",
        "\n",
        "def deploy_model(\n",
        "    model: vertex_ai.Model,\n",
        "    machine_type: str,\n",
        "    endpoint: vertex_ai.Endpoint = None,\n",
        "    deployed_model_display_name: str = None,\n",
        "    min_replica_count: int = 1,\n",
        "    max_replica_count: int = 1,\n",
        "    sync: bool = True,\n",
        ") -> vertex_ai.Model:\n",
        "    \"\"\"\n",
        "    An helper function to deploy a Vertex AI Endpoint\n",
        "    Args:\n",
        "        model: A Vertex AI Model\n",
        "        machine_type: The type of machine to serve the model\n",
        "        endpoint: An Vertex AI Endpoint\n",
        "        deployed_model_display_name: The name of the model\n",
        "        min_replica_count: Minimum number of serving replicas\n",
        "        max_replica_count: Max number of serving replicas\n",
        "        sync: Whether to execute method synchronously\n",
        "\n",
        "    Returns: vertex_ai.Model\n",
        "\n",
        "    \"\"\"\n",
        "    model_deployed = model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        deployed_model_display_name=deployed_model_display_name,\n",
        "        machine_type=machine_type,\n",
        "        min_replica_count=min_replica_count,\n",
        "        max_replica_count=max_replica_count,\n",
        "        sync=sync,\n",
        "    )\n",
        "\n",
        "    model_deployed.wait()\n",
        "\n",
        "    print(model_deployed.display_name)\n",
        "    print(model_deployed.resource_name)\n",
        "    return model_deployed\n",
        "\n",
        "\n",
        "def endpoint_predict_sample(\n",
        "    instances: list, endpoint: vertex_ai.Endpoint\n",
        ") -> vertex_ai.models.Prediction:\n",
        "    \"\"\"\n",
        "    An helper function to get prediction from Vertex AI Endpoint\n",
        "    Args:\n",
        "        instances: The list of instances to score\n",
        "        endpoint: An Vertex AI Endpoint\n",
        "\n",
        "    Returns:\n",
        "        vertex_ai.models.Prediction\n",
        "\n",
        "    \"\"\"\n",
        "    prediction = endpoint.predict(instances=instances)\n",
        "    print(prediction)\n",
        "    return prediction\n",
        "\n",
        "\n",
        "def simulate_prediction(\n",
        "    endpoint: vertex_ai.Endpoint, online_sample: dict\n",
        ") -> vertex_ai.models.Prediction:\n",
        "    \"\"\"\n",
        "    An helper function to simulate online prediction with customer entity type\n",
        "        - format entities for prediction\n",
        "        - retrive static features with a singleton lookup operations from Vertex AI Feature store\n",
        "        - run the prediction request and get back the result\n",
        "    Args:\n",
        "        endpoint:\n",
        "        online_sample:\n",
        "\n",
        "    Returns:\n",
        "        vertex_ai.models.Prediction\n",
        "    \"\"\"\n",
        "    online_features = pd.DataFrame.from_dict(online_sample)\n",
        "    entity_ids = online_features[\"entity_id\"].tolist()\n",
        "\n",
        "    customer_aggregated_features = customer_entity_type.read(\n",
        "        entity_ids=entity_ids,\n",
        "        feature_ids=[\n",
        "            \"cnt_user_engagement\",\n",
        "            \"cnt_level_start_quickplay\",\n",
        "            \"cnt_level_end_quickplay\",\n",
        "            \"cnt_level_complete_quickplay\",\n",
        "            \"cnt_level_reset_quickplay\",\n",
        "            \"cnt_post_score\",\n",
        "            \"cnt_spend_virtual_currency\",\n",
        "            \"cnt_ad_reward\",\n",
        "            \"cnt_challenge_a_friend\",\n",
        "            \"cnt_completed_5_levels\",\n",
        "            \"cnt_use_extra_steps\",\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    prediction_sample_df = pd.merge(\n",
        "        customer_aggregated_features.set_index(\"entity_id\"),\n",
        "        online_features.set_index(\"entity_id\"),\n",
        "        left_index=True,\n",
        "        right_index=True,\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    prediction_sample = prediction_sample_df.to_dict(\"records\")\n",
        "    prediction = endpoint.predict(prediction_sample)\n",
        "    return prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MewDhNbrXf_N"
      },
      "source": [
        "# Setting the Online (real-time) prediction scenario\n",
        "\n",
        "As we mentioned at the beginning, this section would simulate the original but this time we introduce Vertex AI for online (real-time) serving. In particular, we will\n",
        "\n",
        "1.   Create static features including demographic and behavioral attibutes\n",
        "2.   Training a simple BQML model\n",
        "3.   Export and deploy the model to Vertex AI endpoint\n",
        "\n",
        "\n",
        "<img src=\"./assets/data_processing.png\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ffd54e97270"
      },
      "source": [
        "## Initiate clients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f48872501fb"
      },
      "outputs": [],
      "source": [
        "bq_client = bigquery.Client(project=PROJECT_ID, location=LOCATION)\n",
        "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmMWIpCwsET9"
      },
      "source": [
        "## Data and Feature Engineering \n",
        "\n",
        "The original dataset contains raw event data we cannot ingest in the feature store as they are.\n",
        "In this section, we will pre-process the raw data into an appropriate format. \n",
        "\n",
        "**Notice we simulate those transformations in different point of time (day one and day two).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8avYy5QOv02s"
      },
      "source": [
        "### Label, Demographic and Behavioral Transformations\n",
        "\n",
        "This section is based on the [Churn prediction for game developers using Google Analytics 4 (GA4) and BigQuery ML](https://cloud.google.com/blog/topics/developers-practitioners/churn-prediction-game-developers-using-google-analytics-4-ga4-and-bigquery-ml?utm_source=linkedin&utm_medium=unpaidsoc&utm_campaign=FY21-Q2-Google-Cloud-Tech-Blog&utm_content=google-analytics-4&utm_term=-) blog article by Minhaz Kazi and Polong Lin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nabXJGAOsET9"
      },
      "outputs": [],
      "source": [
        "preprocess_sql_query = f\"\"\"\n",
        "CREATE OR REPLACE TABLE\n",
        "  `{PROJECT_ID}.{BQ_DATASET}.{FEATURES_TABLE}` AS\n",
        "WITH\n",
        "  # query to create label --------------------------------------------------------------------------------\n",
        "  get_label AS (\n",
        "  SELECT\n",
        "    user_pseudo_id,\n",
        "    user_first_engagement,\n",
        "    user_last_engagement,\n",
        "    # EXTRACT(MONTH from TIMESTAMP_MICROS(user_first_engagement)) as month,\n",
        "    # EXTRACT(DAYOFYEAR from TIMESTAMP_MICROS(user_first_engagement)) as julianday,\n",
        "    # EXTRACT(DAYOFWEEK from TIMESTAMP_MICROS(user_first_engagement)) as dayofweek,\n",
        "\n",
        "    #add 24 hr to user's first touch\n",
        "    (user_first_engagement + 86400000000) AS ts_24hr_after_first_engagement,\n",
        "\n",
        "    #churned = 1 if last_touch within 24 hr of app installation, else 0\n",
        "    IF (user_last_engagement < (user_first_engagement + 86400000000),\n",
        "        1,\n",
        "        0 ) AS churned,\n",
        "\n",
        "    #bounced = 1 if last_touch within 10 min, else 0\n",
        "    IF (user_last_engagement <= (user_first_engagement + 600000000),\n",
        "        1,\n",
        "        0 ) AS bounced,\n",
        "  FROM\n",
        "    (\n",
        "      SELECT\n",
        "      user_pseudo_id,\n",
        "      MIN(event_timestamp) AS user_first_engagement,\n",
        "      MAX(event_timestamp) AS user_last_engagement\n",
        "      FROM\n",
        "        `firebase-public-project.analytics_153293282.events_*`\n",
        "      WHERE event_name=\"user_engagement\"\n",
        "      GROUP BY\n",
        "        user_pseudo_id\n",
        "    )\n",
        "  GROUP BY 1,2,3),\n",
        "\n",
        "  # query to create class weights --------------------------------------------------------------------------------\n",
        "  get_class_weights AS (\n",
        "  SELECT\n",
        "    CAST(COUNT(*) / (2*(COUNT(*) - SUM(churned))) AS STRING) AS class_weight_zero,\n",
        "    CAST(COUNT(*) / (2*SUM(churned)) AS STRING) AS class_weight_one,\n",
        "  FROM\n",
        "    get_label\n",
        "    ),\n",
        "\n",
        "  # query to extract demographic data for each user ---------------------------------------------------------\n",
        "  get_demographic_data AS (\n",
        "  SELECT * EXCEPT (row_num)\n",
        "  FROM (\n",
        "    SELECT\n",
        "      user_pseudo_id,\n",
        "      geo.country as country,\n",
        "      device.operating_system as operating_system,\n",
        "      device.language as language,\n",
        "      ROW_NUMBER() OVER (PARTITION BY user_pseudo_id ORDER BY event_timestamp DESC) AS row_num\n",
        "    FROM `firebase-public-project.analytics_153293282.events_*`\n",
        "    WHERE event_name=\"user_engagement\")\n",
        "  WHERE row_num = 1),\n",
        "\n",
        "  # query to extract behavioral data for each user ----------------------------------------------------------\n",
        "  get_behavioral_data AS (\n",
        "  SELECT\n",
        "    user_pseudo_id,\n",
        "    SUM(IF(event_name = 'user_engagement', 1, 0)) AS cnt_user_engagement,\n",
        "    SUM(IF(event_name = 'level_start_quickplay', 1, 0)) AS cnt_level_start_quickplay,\n",
        "    SUM(IF(event_name = 'level_end_quickplay', 1, 0)) AS cnt_level_end_quickplay,\n",
        "    SUM(IF(event_name = 'level_complete_quickplay', 1, 0)) AS cnt_level_complete_quickplay,\n",
        "    SUM(IF(event_name = 'level_reset_quickplay', 1, 0)) AS cnt_level_reset_quickplay,\n",
        "    SUM(IF(event_name = 'post_score', 1, 0)) AS cnt_post_score,\n",
        "    SUM(IF(event_name = 'spend_virtual_currency', 1, 0)) AS cnt_spend_virtual_currency,\n",
        "    SUM(IF(event_name = 'ad_reward', 1, 0)) AS cnt_ad_reward,\n",
        "    SUM(IF(event_name = 'challenge_a_friend', 1, 0)) AS cnt_challenge_a_friend,\n",
        "    SUM(IF(event_name = 'completed_5_levels', 1, 0)) AS cnt_completed_5_levels,\n",
        "    SUM(IF(event_name = 'use_extra_steps', 1, 0)) AS cnt_use_extra_steps,\n",
        "  FROM (\n",
        "    SELECT\n",
        "      e.*\n",
        "    FROM\n",
        "      `firebase-public-project.analytics_153293282.events_*` e\n",
        "    JOIN\n",
        "      get_label r\n",
        "    ON\n",
        "      e.user_pseudo_id = r.user_pseudo_id\n",
        "    WHERE\n",
        "      e.event_timestamp <= r.ts_24hr_after_first_engagement\n",
        "    )\n",
        "  GROUP BY 1)\n",
        "\n",
        "SELECT\n",
        "    PARSE_TIMESTAMP('%Y-%m-%d %H:%M:%S', FORMAT_TIMESTAMP('%Y-%m-%d %H:%M:%S', TIMESTAMP_MICROS(ret.user_first_engagement))) AS user_first_engagement,\n",
        "    # ret.month,\n",
        "    # ret.julianday,\n",
        "    # ret.dayofweek,\n",
        "    dem.*,\n",
        "    CAST(IFNULL(beh.cnt_user_engagement, 0) AS FLOAT64)  AS cnt_user_engagement,\n",
        "    CAST(IFNULL(beh.cnt_level_start_quickplay, 0) AS FLOAT64) AS cnt_level_start_quickplay,\n",
        "    CAST(IFNULL(beh.cnt_level_end_quickplay, 0) AS FLOAT64) AS cnt_level_end_quickplay,\n",
        "    CAST(IFNULL(beh.cnt_level_complete_quickplay, 0) AS FLOAT64) AS cnt_level_complete_quickplay,\n",
        "    CAST(IFNULL(beh.cnt_level_reset_quickplay, 0) AS FLOAT64) AS cnt_level_reset_quickplay,\n",
        "    CAST(IFNULL(beh.cnt_post_score, 0) AS FLOAT64) AS cnt_post_score,\n",
        "    CAST(IFNULL(beh.cnt_spend_virtual_currency, 0) AS FLOAT64) AS cnt_spend_virtual_currency,\n",
        "    CAST(IFNULL(beh.cnt_ad_reward, 0) AS FLOAT64) AS cnt_ad_reward,\n",
        "    CAST(IFNULL(beh.cnt_challenge_a_friend, 0) AS FLOAT64) AS cnt_challenge_a_friend,\n",
        "    CAST(IFNULL(beh.cnt_completed_5_levels, 0) AS FLOAT64) AS cnt_completed_5_levels,\n",
        "    CAST(IFNULL(beh.cnt_use_extra_steps, 0) AS FLOAT64) AS cnt_use_extra_steps,\n",
        "    ret.churned as churned,\n",
        "    CASE\n",
        "      WHEN churned = 0 THEN ( SELECT class_weight_zero FROM get_class_weights)\n",
        "      ELSE ( SELECT class_weight_one\n",
        "       FROM get_class_weights)\n",
        "    END AS class_weights\n",
        "FROM\n",
        "  get_label ret\n",
        "LEFT OUTER JOIN\n",
        "  get_demographic_data dem\n",
        "ON \n",
        "  ret.user_pseudo_id = dem.user_pseudo_id\n",
        "LEFT OUTER JOIN \n",
        "  get_behavioral_data beh\n",
        "ON\n",
        "  ret.user_pseudo_id = beh.user_pseudo_id\n",
        "WHERE ret.bounced = 0\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6CjIOmDsET-"
      },
      "outputs": [],
      "source": [
        "run_bq_query(preprocess_sql_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrEKovOhCrUq"
      },
      "source": [
        "### Create table to update entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26db23c2242a"
      },
      "outputs": [],
      "source": [
        "processed_sql_query_day_one = f\"\"\"\n",
        "CREATE OR REPLACE TABLE \n",
        "  `{PROJECT_ID}.{BQ_DATASET}.{FEATURES_TABLE_DAY_ONE}` AS\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  `{PROJECT_ID}.{BQ_DATASET}.{FEATURES_TABLE}`\n",
        "WHERE\n",
        "    user_first_engagement < '{MAX_DATE}'\n",
        "\"\"\"\n",
        "\n",
        "processed_sql_query_day_two = f\"\"\"\n",
        "CREATE OR REPLACE TABLE \n",
        "  `{PROJECT_ID}.{BQ_DATASET}.{FEATURES_TABLE_DAY_TWO}` AS\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  `{PROJECT_ID}.{BQ_DATASET}.{FEATURES_TABLE}`\n",
        "WHERE\n",
        "  user_first_engagement >= '{MAX_DATE}'\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AOgLg0HDETq"
      },
      "outputs": [],
      "source": [
        "queries = processed_sql_query_day_one, processed_sql_query_day_two\n",
        "for query in queries:\n",
        "    run_bq_query(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhHi_HNoRoOr"
      },
      "source": [
        "## Model Training\n",
        "\n",
        "We created demographic and aggregate behavioral features. It is time to train our BQML model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw6_3O4hez8Q"
      },
      "source": [
        "#### Train an Logistic classifier model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hl7wdrVX9lM"
      },
      "outputs": [],
      "source": [
        "train_model_query = f\"\"\"\n",
        "CREATE OR REPLACE MODEL `{PROJECT_ID}.{BQ_DATASET}.{MODEL_NAME}`\n",
        "OPTIONS(MODEL_TYPE='{MODEL_TYPE}',\n",
        "        AUTO_CLASS_WEIGHTS={AUTO_CLASS_WEIGHTS},\n",
        "        MAX_ITERATIONS={MAX_ITERATIONS},\n",
        "        INPUT_LABEL_COLS=['{INPUT_LABEL_COLS}'])\n",
        "AS SELECT * EXCEPT(user_first_engagement, user_pseudo_id, class_weights)\n",
        "   FROM `{PROJECT_ID}.{BQ_DATASET}.{FEATURES_TABLE_DAY_ONE}`;\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rimoWLsazDY"
      },
      "outputs": [],
      "source": [
        "run_bq_query(train_model_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5728b2bbeba"
      },
      "source": [
        "## Model Deployment\n",
        "\n",
        "Once we get the model, you can export it and deploy to an Vertex AI Endpoint. \n",
        "\n",
        "This is just one of the 5 ways to use BigQuery and Vertex AI together. [Check](https://cloud.google.com/blog/products/ai-machine-learning/five-integrations-between-vertex-ai-and-bigquery) this article to know more about them. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSqS17C_g5EM"
      },
      "source": [
        "#### Export the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ao5nIUpSZft"
      },
      "outputs": [],
      "source": [
        "model_extract_job = bigquery.ExtractJob(\n",
        "    client=bq_client,\n",
        "    job_id=JOB_ID,\n",
        "    source=MODEL_SOURCE,\n",
        "    destination_uris=[DESTINATION_URI],\n",
        "    job_config=EXTRACT_JOB_CONFIG,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmgTZFIRWNd4"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    job = model_extract_job.result()\n",
        "except job.error_result as error:\n",
        "    print(error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYuDkBMzRSh2"
      },
      "source": [
        "#### (Locally) Check the SavedModel format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDwHPcT81zu1"
      },
      "outputs": [],
      "source": [
        "%%bash -s \"$SERVING_DIR\" \"$DESTINATION_URI\" \n",
        "mkdir -p -m 777 $1\n",
        "gsutil cp -r $2 $1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Z4sMYVr1ueO"
      },
      "outputs": [],
      "source": [
        "%%bash -s \"$SERVING_DIR\"\n",
        "saved_model_cli show --dir $1/model/ --all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qiQ7ViSXgNy"
      },
      "source": [
        "#### Upload and Deploy Model on Vertex AI Endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYc8ZKsrX6M1"
      },
      "outputs": [],
      "source": [
        "bq_model = upload_model(\n",
        "    display_name=MODEL_NAME,\n",
        "    serving_container_image_uri=SERVING_CONTAINER_IMAGE_URI,\n",
        "    artifact_uri=DESTINATION_URI,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWvYHYYdBhTj"
      },
      "outputs": [],
      "source": [
        "endpoint = create_endpoint(display_name=ENDPOINT_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbiSSYT1CsiC"
      },
      "outputs": [],
      "source": [
        "deployed_model = deploy_model(\n",
        "    model=bq_model,\n",
        "    machine_type=\"n1-highcpu-4\",\n",
        "    endpoint=endpoint,\n",
        "    deployed_model_display_name=DEPLOYED_MODEL_NAME,\n",
        "    min_replica_count=1,\n",
        "    max_replica_count=1,\n",
        "    sync=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "By3DCTmxW9qG"
      },
      "source": [
        "#### Test predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRl5kmKHXAHM"
      },
      "outputs": [],
      "source": [
        "instance = {\n",
        "    \"cnt_ad_reward\": 0,\n",
        "    \"cnt_challenge_a_friend\": 0,\n",
        "    \"cnt_completed_5_levels\": 0,\n",
        "    \"cnt_level_complete_quickplay\": 0,\n",
        "    \"cnt_level_end_quickplay\": 0,\n",
        "    \"cnt_level_reset_quickplay\": 0,\n",
        "    \"cnt_level_start_quickplay\": 0,\n",
        "    \"cnt_post_score\": 0,\n",
        "    \"cnt_spend_virtual_currency\": 0,\n",
        "    \"cnt_use_extra_steps\": 0,\n",
        "    \"cnt_user_engagement\": 14,\n",
        "    \"country\": \"United States\",\n",
        "    \"language\": \"en-us\",\n",
        "    \"operating_system\": \"ANDROID\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2881927fc130"
      },
      "outputs": [],
      "source": [
        "bqml_predictions = endpoint_predict_sample(instances=[instance], endpoint=endpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Yv8MenWXrRX"
      },
      "source": [
        "# Serve ML features at scale with low latency\n",
        "\n",
        "At that point, **we deploy our simple model which would requires fetching aggregated attributes as input features in real time**. \n",
        "\n",
        "That's why **we need a datastore optimized for singleton lookup operations** which would be able to scale and serve those aggregated feature online in low latency. \n",
        "\n",
        "In other terms, we need to introduce Vertex AI Feature Store. Again, we assume you already know how to set up and work with a Vertex AI Feature store.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx__2-assET-"
      },
      "source": [
        "## Feature store for features management\n",
        "\n",
        "In this section, we explore all Feature store management activities from create a Featurestore resource all way down to read feature values online.\n",
        "\n",
        "Below you can see the feature store data model and a plain representation of how the data will be organized.\n",
        "\n",
        "<img src=\"./assets/data_model_3.png\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dNlxda2sET_"
      },
      "source": [
        "### Create featurestore, ```mobile_gaming```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfBpw2J5P1sa"
      },
      "outputs": [],
      "source": [
        "print(f\"Listing all featurestores in {PROJECT_ID}\")\n",
        "feature_store_list = Featurestore.list()\n",
        "if len(list(feature_store_list)) == 0:\n",
        "    print(f\"The {PROJECT_ID} is empty!\")\n",
        "else:\n",
        "    for fs in feature_store_list:\n",
        "        print(\"Found featurestore: {}\".format(fs.resource_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2en8I7TSe4b"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    mobile_gaming_feature_store = Featurestore.create(\n",
        "        featurestore_id=FEATURESTORE_ID,\n",
        "        online_store_fixed_node_count=ONLINE_STORE_NODES_COUNT,\n",
        "        labels={\"team\": \"dataoffice\", \"app\": \"mobile_gaming\"},\n",
        "        sync=True,\n",
        "    )\n",
        "except RuntimeError as error:\n",
        "    print(error)\n",
        "else:\n",
        "    FEATURESTORE_RESOURCE_NAME = mobile_gaming_feature_store.resource_name\n",
        "    print(f\"Feature store created: {FEATURESTORE_RESOURCE_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rN-vlvPUsET_"
      },
      "source": [
        "### Create the ```User``` entity type and its features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbZ2RQ5XbuRq"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    user_entity_type = mobile_gaming_feature_store.create_entity_type(\n",
        "        entity_type_id=ENTITY_ID, description=\"User Entity\", sync=True\n",
        "    )\n",
        "except RuntimeError as error:\n",
        "    print(error)\n",
        "else:\n",
        "    USER_ENTITY_RESOURCE_NAME = user_entity_type.resource_name\n",
        "    print(\"Entity type name is\", USER_ENTITY_RESOURCE_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2PIAprPmnhB"
      },
      "source": [
        "### Set Feature Monitoring\n",
        "\n",
        "Feature [monitoring](https://cloud.google.com/vertex-ai/docs/featurestore/monitoring) is in preview, so you need to use v1beta1 Python which is a lower-level API than the one we've used so far in this notebook. \n",
        "\n",
        "The easiest way to set this for now is using [console UI](https://console.cloud.google.com/vertex-ai/features). For completeness, below is example to do this using v1beta1 SDK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6im2c3ymiwC"
      },
      "outputs": [],
      "source": [
        "from google.cloud.aiplatform_v1beta1 import \\\n",
        "    FeaturestoreServiceClient as v1beta1_FeaturestoreServiceClient\n",
        "from google.cloud.aiplatform_v1beta1.types import \\\n",
        "    entity_type as v1beta1_entity_type_pb2\n",
        "from google.cloud.aiplatform_v1beta1.types import \\\n",
        "    featurestore_monitoring as v1beta1_featurestore_monitoring_pb2\n",
        "from google.cloud.aiplatform_v1beta1.types import \\\n",
        "    featurestore_service as v1beta1_featurestore_service_pb2\n",
        "from google.protobuf.duration_pb2 import Duration\n",
        "\n",
        "v1beta1_admin_client = v1beta1_FeaturestoreServiceClient(\n",
        "    client_options={\"api_endpoint\": API_ENDPOINT}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gp9xaLQXn0CS"
      },
      "outputs": [],
      "source": [
        "v1beta1_admin_client.update_entity_type(\n",
        "    v1beta1_featurestore_service_pb2.UpdateEntityTypeRequest(\n",
        "        entity_type=v1beta1_entity_type_pb2.EntityType(\n",
        "            name=v1beta1_admin_client.entity_type_path(\n",
        "                PROJECT_ID, REGION, FEATURESTORE_ID, ENTITY_ID\n",
        "            ),\n",
        "            monitoring_config=v1beta1_featurestore_monitoring_pb2.FeaturestoreMonitoringConfig(\n",
        "                snapshot_analysis=v1beta1_featurestore_monitoring_pb2.FeaturestoreMonitoringConfig.SnapshotAnalysis(\n",
        "                    monitoring_interval=Duration(seconds=86400),  # 1 day\n",
        "                ),\n",
        "            ),\n",
        "        ),\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ustwKOMle8Qp"
      },
      "source": [
        "### Create features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijeZCTKIfCRL"
      },
      "source": [
        "#### Create Feature configuration\n",
        "\n",
        "For simplicity, I created the configuration in a declarative way. Of course, we can create an helper function to built it from Bigquery schema.\n",
        "Also notice that we want to pass some feature on-fly. In this case, it country, operating system and language looks perfect for that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX_uYmjUgd9x"
      },
      "outputs": [],
      "source": [
        "feature_configs = {\n",
        "    \"country\": {\n",
        "        \"value_type\": \"STRING\",\n",
        "        \"description\": \"The country of customer\",\n",
        "        \"labels\": {\"status\": \"passed\"},\n",
        "    },\n",
        "    \"operating_system\": {\n",
        "        \"value_type\": \"STRING\",\n",
        "        \"description\": \"The operating system of device\",\n",
        "        \"labels\": {\"status\": \"passed\"},\n",
        "    },\n",
        "    \"language\": {\n",
        "        \"value_type\": \"STRING\",\n",
        "        \"description\": \"The language of device\",\n",
        "        \"labels\": {\"status\": \"passed\"},\n",
        "    },\n",
        "    \"cnt_user_engagement\": {\n",
        "        \"value_type\": \"DOUBLE\",\n",
        "        \"description\": \"A variable of user engagement level\",\n",
        "        \"labels\": {\"status\": \"passed\"},\n",
        "    },\n",
        "    \"cnt_level_start_quickplay\": {\n",
        "        \"value_type\": \"DOUBLE\",\n",
        "        \"description\": \"A variable of user engagement with start level\",\n",
        "        \"labels\": {\"status\": \"passed\"},\n",
        "    },\n",
        "    \"cnt_level_end_quickplay\": {\n",
        "        \"value_type\": \"DOUBLE\",\n",
        "        \"description\": \"A variable of user engagement with end level\",\n",
        "        \"labels\": {\"status\": \"passed\"},\n",
        "    },\n",
        "    \"cnt_level_complete_quickplay\": {\n",
        "        \"value_type\": \"DOUBLE\",\n",
        "        \"description\": \"A variable of user engagement with complete status\",\n",
        "        \"labels\": {\"status\": \"passed\"},\n",
        "    },\n",
        "    \"cnt_level_reset_quickplay\": {\n",
        "        \"value_type\": \"DOUBLE\",\n",
        "        \"description\": \"A variable of user engagement with reset status\",\n",
        "        \"labels\": {\"status\": \"passed\"},\n",
        "    },\n",
        "    \"cnt_post_score\": {\n",
        "        \"value_type\": \"DOUBLE\",\n",
        "        \"description\": \"A variable of user score\",\n",
        "        \"labels\": {\"status\": \"passed\"},\n",
        "    },\n",
        "    \"cnt_spend_virtual_currency\": {\n",
        "        \"value_type\": \"DOUBLE\",\n",
        "        \"description\": \"A variable of user virtual amount\",\n",
        "        \"labels\": {\"status\": \"passed\"},\n",
        "    },\n",
        "    \"cnt_ad_reward\": {\n",
        "        \"value_type\": \"DOUBLE\",\n",
        "        \"description\": \"A variable of user reward\",\n",
        "        \"labels\": {\"status\": \"passed\"},\n",
        "    },\n",
        "    \"cnt_challenge_a_friend\": {\n",
        "        \"value_type\": \"DOUBLE\",\n",
        "        \"description\": \"A variable of user challenges with friends\",\n",
        "        \"labels\": {\"status\": \"passed\"},\n",
        "    },\n",
        "    \"cnt_completed_5_levels\": {\n",
        "        \"value_type\": \"DOUBLE\",\n",
        "        \"description\": \"A variable of user level 5 completed\",\n",
        "        \"labels\": {\"status\": \"passed\"},\n",
        "    },\n",
        "    \"cnt_use_extra_steps\": {\n",
        "        \"value_type\": \"DOUBLE\",\n",
        "        \"description\": \"A variable of user extra steps\",\n",
        "        \"labels\": {\"status\": \"passed\"},\n",
        "    },\n",
        "    \"churned\": {\n",
        "        \"value_type\": \"INT64\",\n",
        "        \"description\": \"A variable of user extra steps\",\n",
        "        \"labels\": {\"status\": \"passed\"},\n",
        "    },\n",
        "    \"class_weights\": {\n",
        "        \"value_type\": \"STRING\",\n",
        "        \"description\": \"A variable of class weights\",\n",
        "        \"labels\": {\"status\": \"passed\"},\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErkruXPJkPuy"
      },
      "source": [
        "#### Create features using `batch_create_features` method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsCAO_IfsEUC"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    user_entity_type.batch_create_features(feature_configs=feature_configs, sync=True)\n",
        "except RuntimeError as error:\n",
        "    print(error)\n",
        "else:\n",
        "    for feature in user_entity_type.list_features():\n",
        "        print(\"\")\n",
        "        print(f\"The resource name of {feature.name} feature is\", feature.resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WisJk18qqgs"
      },
      "source": [
        "### Search features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzqyarMZqvZS"
      },
      "outputs": [],
      "source": [
        "feature_query = \"feature_id:cnt_user_engagement\"\n",
        "searched_features = Feature.search(query=feature_query)\n",
        "searched_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugtBfW5gsEUD"
      },
      "source": [
        "### Import ```User``` feature values using ```ingest_from_bq``` method\n",
        "\n",
        "You need to import feature values before you can use them for online/offline serving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QgjW2vhsEUE"
      },
      "outputs": [],
      "source": [
        "FEATURES_IDS = [feature.name for feature in user_entity_type.list_features()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwltj62V8gTU"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    user_entity_type.ingest_from_bq(\n",
        "        feature_ids=FEATURES_IDS,\n",
        "        feature_time=FEATURE_TIME,\n",
        "        bq_source_uri=BQ_SOURCE_URI_DAY_ONE,\n",
        "        entity_id_field=ENTITY_ID_FIELD,\n",
        "        disable_online_serving=False,\n",
        "        worker_count=20,\n",
        "        sync=True,\n",
        "    )\n",
        "except RuntimeError as error:\n",
        "    print(error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2s3Nk5UuAvd"
      },
      "source": [
        "**Comment: How does Vertex AI Feature Store mitigate training serving skew?**\n",
        "\n",
        "Let's just think about what is happening for a second. \n",
        "\n",
        "We just ingest customer behavioral features we engineered before when we trained the model. And we are now going to serve the same features for online prediction.\n",
        "\n",
        "But, what if those attributes on the incoming prediction requests would differ with respect to the one calculated during the model training? In particular, what if the correct attributes have different characteristics as the data the model was trained on? At that point, you should start perceiving this idea of **skew** between training and serving data. So what? Imagine now that the mobile gaming app go trending and users start challenging friends more frequently. This would change the distribution of the `cnt_challenge_a_friend`. But the model, which estimates your churn probability, was trained on a different distribution. And if we assume that type and frequency of ads depend on those predictions, it would happen that you target wrong users with wrong ads with an expected frequency because this offline-online feature inconsistency.\n",
        "\n",
        "**Vertex AI Feature store** addresses those skew by an ingest-one and and re-used many logic. Indeed, once the feature is computed, the same features would be available both in training and serving. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6V2-YQthT4V"
      },
      "source": [
        "## Simulate online prediction requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVrvAW5VaPzU"
      },
      "outputs": [],
      "source": [
        "online_sample = {\n",
        "    \"entity_id\": [\"DE346CDD4A6F13969F749EA8047F282A\"],\n",
        "    \"country\": [\"United States\"],\n",
        "    \"operating_system\": [\"IOS\"],\n",
        "    \"language\": [\"en\"],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUku2KDfHJxC"
      },
      "outputs": [],
      "source": [
        "prediction = simulate_prediction(endpoint=endpoint, online_sample=online_sample)\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c9330928aa1"
      },
      "source": [
        "# Train a new churn ML model using Vertex AI AutoML\n",
        "\n",
        "Now assume that you have a meeting with the team and you decide to use Vertex AI AutoML to train a new version of the model.\n",
        "\n",
        "But while you were discussing about that, new data where ingested into the feature store.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDlkhkCPK29y"
      },
      "source": [
        "## Ingest new data in the feature store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-_w9IRCQTV9"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    user_entity_type.ingest_from_bq(\n",
        "        feature_ids=FEATURES_IDS,\n",
        "        feature_time=FEATURE_TIME,\n",
        "        bq_source_uri=BQ_SOURCE_URI_DAY_TWO,\n",
        "        entity_id_field=ENTITY_ID_FIELD,\n",
        "        disable_online_serving=False,\n",
        "        worker_count=1,\n",
        "        sync=True,\n",
        "    )\n",
        "except RuntimeError as error:\n",
        "    print(error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saZZ3zWKX1YK"
      },
      "source": [
        "## Avoid data leakage with point-in-time lookup to fetch training data\n",
        "\n",
        "Now, without a datastore with a timestamp data model, some data leakage would happen and you would end by training the new model on a different dataset. As a consequence, you cannot compare those models. In order to avoid that, **you need to be able to train model on the same data at same specific point in time we use in the previous version of the model**. \n",
        "\n",
        "<center><img src=\"./assets/point_in_time_2.png\"/><center/>\n",
        "\n",
        "**With the Vertex AI Feature store, you can fetch feature values corresponding to a particular timestamp thanks to point-in-time lookup capability.** In terms of SDK, you need to define a `read instances` object which is a list of entity id / timestamp pairs, where the entity id is the `user_pseudo_id` and `user_first_engagement` indicates we want to read the latest information available about that user. In this way, we will be able to reproduce the exact same training sample you need for the new model.\n",
        "\n",
        "Let's see how to do that. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHNbIHqFcQiM"
      },
      "source": [
        "### Define query for reading instances at a specific point in time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QntX3uQ93ysM"
      },
      "outputs": [],
      "source": [
        "# WHERE ABS(MOD(FARM_FINGERPRINT(STRING(user_first_engagement, 'UTC')), 10)) < 8\n",
        "\n",
        "read_instances_query = f\"\"\"\n",
        "CREATE OR REPLACE TABLE\n",
        "  `{PROJECT_ID}.{BQ_DATASET}.{READ_INSTANCES_TABLE}` AS\n",
        "    SELECT\n",
        "      user_pseudo_id  as customer,\n",
        "      TIMESTAMP_TRUNC(CURRENT_TIMESTAMP(), SECOND, \"UTC\") as timestamp\n",
        "    FROM\n",
        "      `{BQ_DATASET}.{FEATURES_TABLE_DAY_ONE}` AS e\n",
        "    ORDER BY\n",
        "      user_first_engagement\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTB3853wcYm6"
      },
      "source": [
        "### Create the BigQuery instances table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PRgsZl09vJ0"
      },
      "outputs": [],
      "source": [
        "run_bq_query(read_instances_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwSsdk2AcdTf"
      },
      "source": [
        "### Serve features for batch training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnP0Q5XtwfYM"
      },
      "outputs": [],
      "source": [
        "mobile_gaming_feature_store.batch_serve_to_bq(\n",
        "    bq_destination_output_uri=BQ_DESTINATION_OUTPUT_URI,\n",
        "    serving_feature_ids=SERVING_FEATURE_IDS,\n",
        "    read_instances_uri=READ_INSTANCES_URI,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vjvv7NKnYfcU"
      },
      "source": [
        "## Train and Deploy AutoML model on Vertex AI\n",
        "\n",
        "Now that we reproduce the training sample, we use the Vertex AI SDK to train an new version of the model using Vertex AI AutoML.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wz3ho5_ex_Y"
      },
      "source": [
        "### Create the Managed Tabular Dataset from a CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhNFi986fXe8"
      },
      "outputs": [],
      "source": [
        "dataset = vertex_ai.TabularDataset.create(\n",
        "    display_name=DATASET_NAME,\n",
        "    bq_source=BQ_DESTINATION_OUTPUT_URI,\n",
        ")\n",
        "\n",
        "dataset.resource_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z1Kwg6pe0Jx"
      },
      "source": [
        "### Create and Launch the Training Job to build the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9Xyu8JCiFFk"
      },
      "outputs": [],
      "source": [
        "automl_training_job = vertex_ai.AutoMLTabularTrainingJob(\n",
        "    display_name=AUTOML_TRAIN_JOB_NAME,\n",
        "    optimization_prediction_type=\"classification\",\n",
        "    optimization_objective=\"maximize-au-roc\",\n",
        "    column_transformations=[\n",
        "        {\"categorical\": {\"column_name\": \"country\"}},\n",
        "        {\"categorical\": {\"column_name\": \"operating_system\"}},\n",
        "        {\"categorical\": {\"column_name\": \"language\"}},\n",
        "        {\"numeric\": {\"column_name\": \"cnt_user_engagement\"}},\n",
        "        {\"numeric\": {\"column_name\": \"cnt_level_start_quickplay\"}},\n",
        "        {\"numeric\": {\"column_name\": \"cnt_level_end_quickplay\"}},\n",
        "        {\"numeric\": {\"column_name\": \"cnt_level_complete_quickplay\"}},\n",
        "        {\"numeric\": {\"column_name\": \"cnt_level_reset_quickplay\"}},\n",
        "        {\"numeric\": {\"column_name\": \"cnt_post_score\"}},\n",
        "        {\"numeric\": {\"column_name\": \"cnt_spend_virtual_currency\"}},\n",
        "        {\"numeric\": {\"column_name\": \"cnt_ad_reward\"}},\n",
        "        {\"numeric\": {\"column_name\": \"cnt_challenge_a_friend\"}},\n",
        "        {\"numeric\": {\"column_name\": \"cnt_completed_5_levels\"}},\n",
        "        {\"numeric\": {\"column_name\": \"cnt_use_extra_steps\"}},\n",
        "    ],\n",
        ")\n",
        "\n",
        "# This will take around an 2 hours to run\n",
        "automl_model = automl_training_job.run(\n",
        "    dataset=dataset,\n",
        "    target_column=INPUT_LABEL_COLS,\n",
        "    training_fraction_split=0.8,\n",
        "    validation_fraction_split=0.1,\n",
        "    test_fraction_split=0.1,\n",
        "    weight_column=\"class_weights\",\n",
        "    model_display_name=AUTOML_MODEL_NAME,\n",
        "    disable_early_stopping=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWZmWjnye4N5"
      },
      "source": [
        "### Deploy Model to the same Endpoint with Traffic Splitting\n",
        "\n",
        "Vertex AI Endpoint provides a managed traffic splitting service. All you need to do is to define the splitting policy and then the service will deal it for you. \n",
        "\n",
        "Be sure that both models have the same serving function. In our case both BQML Logistic classifier and Vertex AI AutoML support same prediction format. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tuoqc0i18KOI"
      },
      "outputs": [],
      "source": [
        "model_deployed_id = endpoint.list_models()[0].id\n",
        "RETRAIN_TRAFFIC_SPLIT = {\"0\": 50, model_deployed_id: 50}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_d1NSaod2D-Z"
      },
      "outputs": [],
      "source": [
        "endpoint.deploy(\n",
        "    automl_model,\n",
        "    deployed_model_display_name=MODEL_DEPLOYED_NAME,\n",
        "    traffic_split=RETRAIN_TRAFFIC_SPLIT,\n",
        "    machine_type=SERVING_MACHINE_TYPE,\n",
        "    accelerator_count=0,\n",
        "    min_replica_count=MIN_NODES,\n",
        "    max_replica_count=MAX_NODES,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e600b031a50"
      },
      "source": [
        "## Time to simulate online predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k42QhcjcG5pe"
      },
      "outputs": [],
      "source": [
        "for i in range(2000):\n",
        "    simulate_prediction(endpoint=endpoint, online_sample=online_sample)\n",
        "    time.sleep(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fb651556252"
      },
      "source": [
        "Below the Vertex AI Endpoint UI result you would able to see after the online prediction simulation ends\n",
        "\n",
        "<img src=\"./assets/prediction_results.jpg\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NESfOgK5nkNu"
      },
      "outputs": [],
      "source": [
        "# delete feature store\n",
        "mobile_gaming_feature_store.delete(sync=True, force=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4W28-97kfPDb"
      },
      "outputs": [],
      "source": [
        "# delete Vertex AI resources\n",
        "endpoint.undeploy_all()\n",
        "bq_model.delete()\n",
        "automl_model.delete()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c9351f8950a"
      },
      "outputs": [],
      "source": [
        "%%bash -s \"$SERVING_DIR\"\n",
        "rm -Rf $1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx_vKniMq9ZX"
      },
      "outputs": [],
      "source": [
        "# Warning: Setting this to true will delete everything in your bucket\n",
        "delete_bucket = False\n",
        "\n",
        "if delete_bucket and \"BUCKET_URI\" in globals():\n",
        "    ! gsutil -m rm -r $BUCKET_URI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLVhx7bQeJ6H"
      },
      "outputs": [],
      "source": [
        "# Delete the BigQuery Dataset\n",
        "!bq rm -r -f -d $PROJECT_ID:$BQ_DATASET"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "mobile_gaming_feature_store.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

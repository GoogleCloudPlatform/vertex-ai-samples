{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/official/feature_store/featurestore-tutorials-demo.ipynb\"\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/official/feature_store/featurestore-tutorials-demo.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurestore Tutorials Demonstarting ImportFeatureValues, ExportFeatureValues APIs and batch_serve_to_df method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This Colab introduces Vertex AI Feature Store, a managed cloud service for machine learning engineers and data scientists to store, serve, manage and share machine learning features at a large scale.\n",
    "\n",
    "This Colab assumes that you understand basic Google Cloud concepts such as [Project](https://cloud.google.com/storage/docs/projects), [Storage](https://cloud.google.com/storage), and [Vertex AI](https://cloud.google.com/vertex-ai/docs). Some machine learning knowledge is also helpful but not required.\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "This Colab uses an ecommerce dataset as an example throughout all the sessions. The task is to train a model to recommend products for a user based on the ratings. \n",
    "\n",
    "The dataset used in this notebook consists of order items data since 2018 for an online ecommerce store. This dataset is publicly available at bigquery-public-data.thelook_ecommerce.order_items Big-Query table which can be accessed by pinning the bigquery-public-data project in BigQuery. The table consists of various fields related to each of the order items like the order_id, product_id, user_id, status, and price when it is created when it has been shipped, etc. Among these fields, the current\n",
    "notebook makes use of the following fields assuming their purpose is as described below :\n",
    "\n",
    "    * user_id: The Id of the user. \n",
    "    * product_id: The Id of the product.\n",
    "    * created_at: When the user has placed the order.\n",
    "    * status: The status of the order (Shipped, Processing, Cancelled, Returned, and Completed).\n",
    "\n",
    "\n",
    "### Objective\n",
    "\n",
    "The objectives of this notebook include: \n",
    "\n",
    "* Load the dataset from BigQuery.\n",
    "* Preprocess the features in the dataset(Feature Engineering).\n",
    "* Import your features into Vertex AI Feature Store using ImportFeatureValues API.\n",
    "* Read feature values into dataframe using batch_serve_to_df.\n",
    "* Use custom trained Keras model to build a recommender system that recommends products for a user.\n",
    "* Export Feature Values into Big Query Table using ExportFeatureValues API.\n",
    "* Use Exported data, to recommend products to users.\n",
    "\n",
    "### Costs \n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "* Cloud BigQuery\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze4-nDLfK4pw"
   },
   "source": [
    "### Set up your local development environment\n",
    "\n",
    "**If you are using Colab or Google Cloud Notebooks**, your environment already meets\n",
    "all the requirements to run this notebook. You can skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCuSR8GkAgzl"
   },
   "source": [
    "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
    "You need the following:\n",
    "\n",
    "* The Google Cloud SDK\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* Jupyter notebook running in a virtual environment with Python 3\n",
    "\n",
    "The Google Cloud guide to [Setting up a Python development\n",
    "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
    "installation guide](https://jupyter.org/install) provide detailed instructions\n",
    "for meeting these requirements. The following steps provide a condensed set of\n",
    "instructions:\n",
    "\n",
    "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "1. [Install\n",
    "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
    "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
    "\n",
    "1. To install Jupyter, run `pip install jupyter` on the\n",
    "command-line in a terminal shell.\n",
    "\n",
    "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
    "\n",
    "1. Open this notebook in the Jupyter Notebook Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "### Install additional packages\n",
    "\n",
    "For this Colab, you need the Vertex SDK for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in /home/jupyter/.local/lib/python3.7/site-packages (1.16.1)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.4.1)\n",
      "Requirement already satisfied: packaging<22.0.0dev,>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (21.3)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-aiplatform) (2.8.2)\n",
      "Requirement already satisfied: protobuf<4.0.0dev,>=3.19.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (3.20.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.20.3)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (2.3.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (2.34.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /home/jupyter/.local/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.56.4)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.6.6)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.27.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.46.1)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.46.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.3.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.3.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.12.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging<22.0.0dev,>=14.3->google-cloud-aiplatform) (3.0.9)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (4.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (5.0.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.2.7)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.1.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.26.9)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.15.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.4.8)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.21)\n"
     ]
    }
   ],
   "source": [
    "! pip install {USER_FLAG} --upgrade google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "After you install the SDK, you need to restart the notebook kernel so it can find the packages. You can restart kernel from *Kernel -> Restart Kernel*, or by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "EzrelQZ22IZj"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "## Before you begin\n",
    "\n",
    "### Select a GPU runtime\n",
    "\n",
    "**Make sure you're running this notebook in a GPU runtime if you have the option. In Colab, select \"Runtime --> Change runtime type > GPU\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. [Enable the Vertex AI API and Compute Engine API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component).\n",
    "\n",
    "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    "\n",
    "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  christian-cool-project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJYoRfYng0XZ"
   },
   "source": [
    "Otherwise, set your project ID here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"python-docs-samples-tests\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr--iN2kAylZ"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "**If you are using Google Cloud Notebooks**, your environment is already\n",
    "authenticated. Skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "**If you are using Colab**, run the cell below and follow the instructions\n",
    "when prompted to authenticate your account via oAuth.\n",
    "\n",
    "**Otherwise**, follow these steps:\n",
    "\n",
    "1. In the Cloud Console, go to the [**Create service account key**\n",
    "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
    "\n",
    "2. Click **Create service account**.\n",
    "\n",
    "3. In the **Service account name** field, enter a name, and\n",
    "   click **Create**.\n",
    "\n",
    "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
    "into the filter box, and select\n",
    "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
    "\n",
    "5. Click *Create*. A JSON file that contains your key downloads to your\n",
    "local environment.\n",
    "\n",
    "6. Enter the path to your service account key as the\n",
    "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# If on Google Cloud Notebooks, then don't execute this code\n",
    "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Feature Engineering on the BQ Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we perform the Feature Engineering on the `bigquery-public-data.thelook_ecommerce.order_items` using Pandas. \n",
    "* Load the data from the BigQuery into the Pandas Dataframe. \n",
    "* The columns used from above table are mentioned in [Dataset](#Dataset) Section.\n",
    "* Created `ratings` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime,timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecomm_bq_tables():\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    bqclient_master = bigquery.Client()\n",
    "\n",
    "    # Download query results.\n",
    "    query_string = \"\"\"\n",
    "      SELECT\n",
    "        CAST(user_id AS STRING) AS user_id,\n",
    "        product_id,\n",
    "        created_at,\n",
    "        status\n",
    "      FROM\n",
    "        `bigquery-public-data.thelook_ecommerce.order_items`\n",
    "    \"\"\"\n",
    "\n",
    "    bq_table = (\n",
    "        bqclient_master.query(query_string)\n",
    "        .result()\n",
    "        .to_dataframe(\n",
    "            create_bqstorage_client=True,\n",
    "        )\n",
    "    )\n",
    "    return bq_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bq = ecomm_bq_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181235, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29089</td>\n",
       "      <td>13606</td>\n",
       "      <td>2021-10-07 13:09:00+00:00</td>\n",
       "      <td>Shipped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57515</td>\n",
       "      <td>13606</td>\n",
       "      <td>2022-07-09 11:08:26+00:00</td>\n",
       "      <td>Complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98027</td>\n",
       "      <td>13606</td>\n",
       "      <td>2022-07-03 08:39:17+00:00</td>\n",
       "      <td>Returned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21718</td>\n",
       "      <td>13606</td>\n",
       "      <td>2021-11-07 15:21:48+00:00</td>\n",
       "      <td>Cancelled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58187</td>\n",
       "      <td>13606</td>\n",
       "      <td>2021-09-24 03:16:25+00:00</td>\n",
       "      <td>Cancelled</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id  product_id                created_at     status\n",
       "0   29089       13606 2021-10-07 13:09:00+00:00    Shipped\n",
       "1   57515       13606 2022-07-09 11:08:26+00:00   Complete\n",
       "2   98027       13606 2022-07-03 08:39:17+00:00   Returned\n",
       "3   21718       13606 2021-11-07 15:21:48+00:00  Cancelled\n",
       "4   58187       13606 2021-09-24 03:16:25+00:00  Cancelled"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the rating\n",
    "score_mapping = {\n",
    "    \"Cancelled\": 0,\n",
    "    \"Returned\": 1,\n",
    "    \"Processing\": 2,\n",
    "    \"Shipped\": 3,\n",
    "    \"Complete\": 4,\n",
    "}\n",
    "df_bq[\"rating\"] = df_bq[\"status\"].map(score_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_rating = min(df_bq[\"rating\"])\n",
    "max_rating = max(df_bq[\"rating\"])\n",
    "df_bq[\"rating\"]=df_bq[\"rating\"].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>status</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29089</td>\n",
       "      <td>13606</td>\n",
       "      <td>2021-10-07 13:09:00+00:00</td>\n",
       "      <td>Shipped</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57515</td>\n",
       "      <td>13606</td>\n",
       "      <td>2022-07-09 11:08:26+00:00</td>\n",
       "      <td>Complete</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98027</td>\n",
       "      <td>13606</td>\n",
       "      <td>2022-07-03 08:39:17+00:00</td>\n",
       "      <td>Returned</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21718</td>\n",
       "      <td>13606</td>\n",
       "      <td>2021-11-07 15:21:48+00:00</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58187</td>\n",
       "      <td>13606</td>\n",
       "      <td>2021-09-24 03:16:25+00:00</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id  product_id                created_at     status  rating\n",
       "0   29089       13606 2021-10-07 13:09:00+00:00    Shipped    0.75\n",
       "1   57515       13606 2022-07-09 11:08:26+00:00   Complete    1.00\n",
       "2   98027       13606 2022-07-03 08:39:17+00:00   Returned    0.25\n",
       "3   21718       13606 2021-11-07 15:21:48+00:00  Cancelled    0.00\n",
       "4   58187       13606 2021-09-24 03:16:25+00:00  Cancelled    0.00"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare a list of users who bought products until last week. The dataset has `product_id` and `list of users`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_week_date = datetime.now()-pd.to_timedelta(\"7day\")\n",
    "\n",
    "df_filtered = df_bq[(df_bq['created_at'] < past_week_date.isoformat() + \"Z\")].reset_index()\n",
    "\n",
    "result=df_filtered.groupby(['product_id'])['user_id'].apply(list).to_dict()\n",
    "\n",
    "df_prod_user_list = pd.DataFrame(result.items(),columns=[\"product_id\",\"user_id\"])\n",
    "\n",
    "df_prod_user_list['product_id'] = df_prod_user_list['product_id'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[10044, 17057, 62183, 62770, 48508, 63949, 713...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[4066, 16761, 34709]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[19563, 67295, 76971, 11057, 53165]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[44504, 19432, 6299, 86842, 50585, 75086]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[22681, 38697]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  product_id                                            user_id\n",
       "0          1  [10044, 17057, 62183, 62770, 48508, 63949, 713...\n",
       "1          2                               [4066, 16761, 34709]\n",
       "2          3                [19563, 67295, 76971, 11057, 53165]\n",
       "3          4          [44504, 19432, 6299, 86842, 50585, 75086]\n",
       "4          5                                     [22681, 38697]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prod_user_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bq.drop(\"status\",axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading the Feature Engineered data into the BQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will load the pandas feature engineered data from the previous section into a BigQuery Table which will be used as a source table for the `ImportFeatureValues` data ingestion for `users` and `products` Entity Type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create a dataset for output\n",
    "\n",
    "You need a BigQuery dataset to host the output data in `us-central1`. Input the name of the dataset you want to created and specify the name of the table you want to store the output later. These will be used later in the notebook.\n",
    "\n",
    "**Make sure that the table name does NOT already exist**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DESTINATION dataset\n",
    "DESTINATION_DATA_SET = \"product_recommendation\"  # @param {type:\"string\"}\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "DESTINATION_DATA_SET = \"{prefix}_{timestamp}\".format(\n",
    "    prefix=DESTINATION_DATA_SET, timestamp=TIMESTAMP\n",
    ")\n",
    "\n",
    "DESTINATION_PATTERN = \"bq://{project}.{dataset}.{table}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "USERS_SOURCE_TABLE_NAME = \"user_prod_rating_data\"\n",
    "USERS_SOURCE_TABLE_URI = DESTINATION_PATTERN.format(\n",
    "    project=PROJECT_ID, dataset=DESTINATION_DATA_SET, table=USERS_SOURCE_TABLE_NAME\n",
    ")\n",
    "\n",
    "PRODUCTS_SOURCE_TABLE_NAME = \"prod_users_list_data\"\n",
    "PRODUCTS_SOURCE_TABLE_URI = DESTINATION_PATTERN.format(\n",
    "    project=PROJECT_ID, dataset=DESTINATION_DATA_SET, table=PRODUCTS_SOURCE_TABLE_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "RKhmymT-O0vy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset christian-cool-project.product_recommendation_20220812230922\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "dataset_id = \"{}.{}\".format(client.project, DESTINATION_DATA_SET)\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "dataset.location = REGION\n",
    "dataset = client.create_dataset(dataset)\n",
    "print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading `df_bq` data into BQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None RUNNING\n",
      "Running ...\n",
      "None DONE\n"
     ]
    }
   ],
   "source": [
    "# Load data to BQ\n",
    "job = client.load_table_from_dataframe(df_bq, f\"{DESTINATION_DATA_SET}.{USERS_SOURCE_TABLE_NAME}\")\n",
    "print(job.errors,job.state)\n",
    "\n",
    "while job.running():\n",
    "    from time import sleep\n",
    "\n",
    "    sleep(30)\n",
    "    print(\"Running ...\")\n",
    "    \n",
    "print(job.errors, job.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading `df_prod_user_list` data into BQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table christian-cool-project.product_recommendation_20220812230922.prod_users_list_data\n"
     ]
    }
   ],
   "source": [
    "# Create a table\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"product_id\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"user_id\", \"STRING\", \"REPEATED\"),\n",
    "]\n",
    "table_id = f\"{PROJECT_ID}.{DESTINATION_DATA_SET}.{PRODUCTS_SOURCE_TABLE_NAME}\"\n",
    "table = bigquery.Table(table_id, schema=schema)\n",
    "client.create_table(table, exists_ok=True)\n",
    "print(\n",
    "    \"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None RUNNING\n",
      "Running ...\n",
      "None DONE\n"
     ]
    }
   ],
   "source": [
    "# Load data to BQ\n",
    "job = client.load_table_from_dataframe(df_prod_user_list, table_id)\n",
    "print(job.errors,job.state)\n",
    "\n",
    "while job.running():\n",
    "    from time import sleep\n",
    "\n",
    "    sleep(30)\n",
    "    print(\"Running ...\")\n",
    "    \n",
    "print(job.errors, job.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "## Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "isNzmylQXjly"
   },
   "outputs": [],
   "source": [
    "# Other than project ID, featurestore ID and endpoints needs to be set\n",
    "API_ENDPOINT = \"us-central1-aiplatform.googleapis.com\"  # @param {type:\"string\"}\n",
    "FEATURESTORE_ID = \"ecomm_recommendation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform\n",
    "from google.cloud.aiplatform_v1beta1 import (FeaturestoreOnlineServingServiceClient,\n",
    "                                        FeaturestoreServiceClient)\n",
    "from google.cloud.aiplatform_v1beta1.types import FeatureSelector, IdMatcher, DestinationFeatureSetting\n",
    "from google.cloud.aiplatform_v1beta1.types import entity_type as entity_type_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import feature as feature_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import featurestore as featurestore_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import \\\n",
    "    FeaturestoreMonitoringConfig as featurestore_monitoring_config_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import \\\n",
    "    featurestore_online_service as featurestore_online_service_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import \\\n",
    "    featurestore_service as featurestore_service_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import io as io_pb2\n",
    "from google.protobuf.duration_pb2 import Duration\n",
    "from google.protobuf.timestamp_pb2 import Timestamp\n",
    "\n",
    "# Create admin_client for CRUD and data_client for reading feature values.\n",
    "admin_client = FeaturestoreServiceClient(client_options={\"api_endpoint\": API_ENDPOINT})\n",
    "data_client = FeaturestoreOnlineServingServiceClient(\n",
    "    client_options={\"api_endpoint\": API_ENDPOINT}\n",
    ")\n",
    "\n",
    "# Represents featurestore resource path.\n",
    "BASE_RESOURCE_PATH = admin_client.common_location_path(PROJECT_ID, REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology and Concept\n",
    "\n",
    "### Featurestore Data model\n",
    "\n",
    "Vertex AI Feature Store organizes data with the following 3 important hierarchical concepts:\n",
    "```\n",
    "Featurestore -> EntityType -> Feature\n",
    "```\n",
    "* **Featurestore**: the place to store your features\n",
    "* **EntityType**: under a Featurestore, an *EntityType* describes an object to be modeled, a real one, or a virtual one.\n",
    "* **Feature**: under an EntityType, a *feature* describes an attribute of the EntityType\n",
    "\n",
    "In this ecommerce example, you will create a feature store called *ecomm_recommendation*. This store has 2 entity types: *users* and *products*. The user's entity type has the product_id and rating features. The product entity type has user_list and product_name features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9UvxYyGUimKw"
   },
   "source": [
    "## Create Featurestore and Define Schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "buQBIv3ZL3A0"
   },
   "source": [
    "### Create Featurestore\n",
    "\n",
    "The method to create a featurestore returns a\n",
    "[long-running operation](https://google.aip.dev/151) (LRO). An LRO starts an asynchronous job. LROs are returned by other API\n",
    "methods too. Calling\n",
    "`create_fs_lro.result()` waits for the LRO to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "FscHZa0DXjmC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"projects/813969514060/locations/us-central1/featurestores/ecomm_recommendation\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    create_fs_lro = admin_client.create_featurestore(\n",
    "        featurestore_service_pb2.CreateFeaturestoreRequest(\n",
    "            parent=BASE_RESOURCE_PATH,\n",
    "            featurestore_id=FEATURESTORE_ID,\n",
    "            featurestore=featurestore_pb2.Featurestore(\n",
    "                online_serving_config=featurestore_pb2.Featurestore.OnlineServingConfig(\n",
    "                    scaling=featurestore_pb2.Featurestore.OnlineServingConfig.Scaling(\n",
    "                        min_node_count=1,\n",
    "                        max_node_count=10\n",
    "                    ),\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    # Wait for LRO to finish and get the LRO result.\n",
    "    print(create_fs_lro.result())\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpmJq75zXjmT",
    "tags": []
   },
   "source": [
    "### Create Users Entity Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method to create a EntityType returns a\n",
    "[long-running operation](https://google.aip.dev/151) (LRO). Calling\n",
    "`users_entity_type_lro.result()` waits for the LRO to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "s9eZ7aJLXjmT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"projects/813969514060/locations/us-central1/featurestores/ecomm_recommendation/entityTypes/users\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    users_entity_type_lro = admin_client.create_entity_type(\n",
    "        featurestore_service_pb2.CreateEntityTypeRequest(\n",
    "            parent=admin_client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID),\n",
    "            entity_type_id=\"users\",\n",
    "            entity_type=entity_type_pb2.EntityType(\n",
    "                description=\"Details of the users in ecommerce website\",\n",
    "                monitoring_config=featurestore_monitoring_config_pb2(\n",
    "                    import_features_analysis=featurestore_monitoring_config_pb2.ImportFeaturesAnalysis(\n",
    "                        state=featurestore_monitoring_config_pb2.ImportFeaturesAnalysis.State.ENABLED\n",
    "                    ),\n",
    "                    snapshot_analysis=featurestore_monitoring_config_pb2.SnapshotAnalysis(\n",
    "                        monitoring_interval=Duration(seconds=86400), #1 day\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    # Similarly, wait for EntityType creation operation.\n",
    "    print(users_entity_type_lro.result())\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Products Entity Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Calling\n",
    "`products_entity_type_lro.result()` waits for the LRO to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"projects/813969514060/locations/us-central1/featurestores/ecomm_recommendation/entityTypes/products\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    products_entity_type_lro = admin_client.create_entity_type(\n",
    "        featurestore_service_pb2.CreateEntityTypeRequest(\n",
    "            parent=admin_client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID),\n",
    "            entity_type_id=\"products\",\n",
    "            entity_type=entity_type_pb2.EntityType(\n",
    "                description=\"Details of the products in ecommerce website\",\n",
    "                monitoring_config=featurestore_monitoring_config_pb2(\n",
    "                    import_features_analysis=featurestore_monitoring_config_pb2.ImportFeaturesAnalysis(\n",
    "                        state=featurestore_monitoring_config_pb2.ImportFeaturesAnalysis.State.ENABLED\n",
    "                    ),\n",
    "                    snapshot_analysis=featurestore_monitoring_config_pb2.SnapshotAnalysis(\n",
    "                        monitoring_interval=Duration(seconds=86400), #1 day\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    # Similarly, wait for EntityType creation operation.\n",
    "    print(products_entity_type_lro.result())\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJW4q-0jO2Xf",
    "tags": []
   },
   "source": [
    "### Create Batch Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method to create batch features returns a\n",
    "[long-running operation](https://google.aip.dev/151) (LRO). Calling\n",
    "`users_features_lro.result()` waits for the LRO to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features for users Entity Type\n",
    "users_features_info = {\n",
    "    \"product_id\": [\"INT64\", \"Id of the product\"],\n",
    "    \"rating\": [\"DOUBLE\", \"Rating of the product\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features {\n",
      "  name: \"projects/813969514060/locations/us-central1/featurestores/ecomm_recommendation/entityTypes/users/features/product_id\"\n",
      "}\n",
      "features {\n",
      "  name: \"projects/813969514060/locations/us-central1/featurestores/ecomm_recommendation/entityTypes/users/features/rating\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create features for the 'users' entity.\n",
    "try:\n",
    "    users_features_lro = admin_client.batch_create_features(\n",
    "        parent=admin_client.entity_type_path(\n",
    "            PROJECT_ID, REGION, FEATURESTORE_ID, \"users\"\n",
    "        ),\n",
    "        requests=[\n",
    "            featurestore_service_pb2.CreateFeatureRequest(\n",
    "                feature=feature_pb2.Feature(\n",
    "                    value_type=feature_pb2.Feature.ValueType[info[0]],\n",
    "                    description=info[1]\n",
    "                ),\n",
    "                feature_id=column_name,\n",
    "            ) for column_name, info in users_features_info.items()  \n",
    "        ],\n",
    "    )\n",
    "    print(users_features_lro.result())\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `products_features_lro.result()` waits for the LRO to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features for products Entity Type\n",
    "products_features_info = {\n",
    "    \"users_list\": [\"STRING_ARRAY\", \"List of user ids who bought product\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features {\n",
      "  name: \"projects/813969514060/locations/us-central1/featurestores/ecomm_recommendation/entityTypes/products/features/users_list\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create features for the 'products' entity.\n",
    "try:\n",
    "    products_features_lro = admin_client.batch_create_features(\n",
    "        parent=admin_client.entity_type_path(\n",
    "            PROJECT_ID, REGION, FEATURESTORE_ID, \"products\"\n",
    "        ),\n",
    "        requests=[\n",
    "            featurestore_service_pb2.CreateFeatureRequest(\n",
    "                feature=feature_pb2.Feature(\n",
    "                    value_type=feature_pb2.Feature.ValueType[info[0]],\n",
    "                    description=info[1]\n",
    "                ),\n",
    "                feature_id=column_name,\n",
    "            ) for column_name, info in products_features_info.items()  \n",
    "        ],\n",
    "    )\n",
    "    print(products_features_lro.result())\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3n5XdK8Xjmw",
    "tags": []
   },
   "source": [
    "## Import Feature Values\n",
    "\n",
    "You need to import feature values before you can use them for online/offline serving. In this step, you will learn how to import feature values by calling the ImportFeatureValues API.\n",
    "\n",
    "### Source Data Format and Layout\n",
    "\n",
    "As mentioned above, BigQuery table/Avro/CSV are supported. No matter what format you are using, each imported entity *must* have an ID; also, each entity can *optionally* have a timestamp, specifying when the feature values are generated. This Colab uses Feature Engineered Big Query table as an input. The table schema for two entity types is as follows:\n",
    "\n",
    "**For the Users entity**:\n",
    "```\n",
    "schema = {\n",
    "  \"name\": \"users\",\n",
    "  \"fields\": [\n",
    "      {\n",
    "       \"name\":\"product_id\",\n",
    "       \"type\":[\"null\",\"integer\"]\n",
    "      },\n",
    "      {\n",
    "       \"name\":\"rating\",\n",
    "       \"type\":[\"null\",\"double\"]\n",
    "      },\n",
    "  ]\n",
    " }\n",
    "```\n",
    "\n",
    "**For the Products entity**:\n",
    "```\n",
    "schema = {\n",
    "  \"name\": \"products\",\n",
    "  \"fields\": [\n",
    "      {\n",
    "       \"name\":\"users_list\",\n",
    "       \"type\":[\"null\",\"string_array\"]\n",
    "      }\n",
    "  ]\n",
    " }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import Feature Values for `users` Entity Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "RUhm6-yzXjmx"
   },
   "outputs": [],
   "source": [
    "import_users_request = featurestore_service_pb2.ImportFeatureValuesRequest(\n",
    "    entity_type=admin_client.entity_type_path(\n",
    "        PROJECT_ID, REGION, FEATURESTORE_ID, \"users\"\n",
    "    ),\n",
    "    bigquery_source=io_pb2.BigQuerySource(\n",
    "        input_uri = USERS_SOURCE_TABLE_URI           \n",
    "    ),\n",
    "    entity_id_field=\"user_id\",\n",
    "    feature_specs=[\n",
    "        # Features  \n",
    "        featurestore_service_pb2.ImportFeatureValuesRequest.FeatureSpec(id=\"product_id\"),\n",
    "        featurestore_service_pb2.ImportFeatureValuesRequest.FeatureSpec(id=\"rating\")\n",
    "    ],\n",
    "    feature_time_field=\"created_at\",\n",
    "    worker_count=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "qwznuUiwjwJF"
   },
   "outputs": [],
   "source": [
    "# Start to import, will take a couple of minutes\n",
    "ingestion_lro = admin_client.import_feature_values(import_users_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "_sDl3ZcrF64T"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "imported_entity_count: 181235\n",
       "imported_feature_value_count: 362470"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Polls for the LRO status and prints when the LRO has completed\n",
    "ingestion_lro.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Feature Values for `products` Entity Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_time():\n",
    "    return int(datetime.now().timestamp())\n",
    "\n",
    "def past_6days():\n",
    "    return int((datetime.now()-timedelta(days=6)).timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_products_request = featurestore_service_pb2.ImportFeatureValuesRequest(\n",
    "    entity_type=admin_client.entity_type_path(\n",
    "        PROJECT_ID, REGION, FEATURESTORE_ID, \"products\"\n",
    "    ),\n",
    "    bigquery_source=io_pb2.BigQuerySource(\n",
    "        input_uri = PRODUCTS_SOURCE_TABLE_URI\n",
    "    ),\n",
    "    entity_id_field=\"product_id\",\n",
    "    feature_specs=[\n",
    "        # Features  \n",
    "        featurestore_service_pb2.ImportFeatureValuesRequest.FeatureSpec(id=\"users_list\", source_field=\"user_id\")\n",
    "    ],\n",
    "    feature_time=Timestamp(seconds=past_6days()),\n",
    "    worker_count=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start to import, will take a couple of minutes\n",
    "ingestion_lro = admin_client.import_feature_values(import_products_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "imported_entity_count: 29042\n",
       "imported_feature_value_count: 29042"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Polls for the LRO status and prints when the LRO has completed\n",
    "ingestion_lro.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8dLJ9nuDFgI"
   },
   "source": [
    "## Batch Serving\n",
    "\n",
    "Batch Serving is used to fetch a large batch of feature values for high throughput, typically for training a model or batch prediction. In this section, you will learn how to train a model example by calling the `batch_serve_to_df` method.\n",
    "\n",
    "### Use case\n",
    "\n",
    "**The task** is to prepare a dataset to train a model, which recommends products for a given user. To achieve this, you need 2 sets of input:\n",
    "\n",
    "*   Features: you already imported into the feature store.\n",
    "*   Labels: the ground-truth data recorded that is rating.\n",
    "\n",
    "To be more specific, the ground-truth observation is described in Table 1 and the desired dataset is described in Table 2. Each row in Table 2 is a result of joining the imported feature values from Vertex AI Feature Store according to the entity IDs and timestamps in Table 1. In this example,  the `product_id` and `rating` features from `users` are chosen to batch train. \n",
    "\n",
    "batch_serve_to_df method takes Table 1 as\n",
    "input for read_instances_df argument joins all required feature values from the feature store, and returns Table 2 for training.\n",
    "\n",
    "<h4 align=\"center\">Table 1. Ground-truth Data</h4>\n",
    "\n",
    "users | timestamp            \n",
    "----- | -------------------- \n",
    "87228 | 2022-07-01T00:00:00Z \n",
    "16173 | 2022-07-01T18:09:43Z \n",
    "...   | ...      | ...     \n",
    "\n",
    "\n",
    "<h4 align=\"center\">Table 2. Expected Training Data Generated by batch_serve_to_df (Positive Samples)</h4>\n",
    "\n",
    "feature_timestamp            | entity_type_users | product_id | rating |\n",
    "-------------------- | ----------------- | --------------- | ---------------- |\n",
    "2022-07-01T00:00:00Z | 87228 | 4567 | 0.5 |\n",
    "2022-07-01T00:00:00Z | 16173 | 5490 | 0.75 |\n",
    "... | ... | ... | ... | ...  \n",
    "\n",
    "#### Why timestamp?\n",
    "\n",
    "Note that there is a `timestamp` column in Table 2. This indicates the time when the ground-truth was observed. This is to avoid data inconsistency.\n",
    "\n",
    "For example, the 1st row of Table 2 indicates that id `87228` brought product on `2022-07-01T00:00:00Z`. The feature store keeps feature values for all timestamps but fetches feature values *only* at the given timestamp during batch serving.\n",
    "             \n",
    "### Batch Serve To DataFrame\n",
    "\n",
    "Assemble the request which specifies the following info:\n",
    "\n",
    "*   Where is the label data, i.e., Table 1.\n",
    "*   Which features are read, i.e., the column names in Table 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will get the dataframe from the feature store using batch_serve_to_df and store it into a csv file that will be used for training the recommender model in Vertex AI.\n",
    "\n",
    "* Create the GCS Bucket.\n",
    "* Enable Uniform Bucket Level Access for the created bucket.\n",
    "* Export the entityType Id (`users`) and `timestamp` columns as csv into the created GCS bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Creation of df_batch dataframe which is used as input to batch_serve_to_df**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"featurestore-bucket\"\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}-{TIMESTAMP}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    BUCKET_NAME = PROJECT_ID + \"fs-\" + TIMESTAMP\n",
    "    BUCKET_URI = \"gs://\" + BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Only if your bucket doesnt already exist: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://featurestore-bucket-20220812231952/...\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling Uniform bucket-level access for gs://featurestore-bucket-20220812231952...\n"
     ]
    }
   ],
   "source": [
    "! gsutil uniformbucketlevelaccess set on $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timezone\n",
    "past_week_date = (datetime.now()-pd.to_timedelta(\"7day\")).isoformat()+\"Z\"\n",
    "df_sorted = df_bq.sort_values('created_at',ascending = False,ignore_index=True)\n",
    "df_sorted.rename(columns = {'user_id':'users'}, inplace = True)\n",
    "df_sorted = df_sorted[df_sorted['created_at']<=past_week_date].reset_index()\n",
    "df_sorted['created_at'] = df_sorted['created_at'].astype(str)\n",
    "df_sorted['timestamp'] = df_sorted['created_at'].map(lambda x: datetime.fromisoformat(x).astimezone(timezone.utc))\n",
    "df_batch = df_sorted[['users','timestamp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>users</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56876</td>\n",
       "      <td>2022-08-05 23:18:31+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94801</td>\n",
       "      <td>2022-08-05 23:18:08+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11215</td>\n",
       "      <td>2022-08-05 23:14:47+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58474</td>\n",
       "      <td>2022-08-05 23:14:22+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65028</td>\n",
       "      <td>2022-08-05 23:12:57+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   users                 timestamp\n",
       "0  56876 2022-08-05 23:18:31+00:00\n",
       "1  94801 2022-08-05 23:18:08+00:00\n",
       "2  11215 2022-08-05 23:14:47+00:00\n",
       "3  58474 2022-08-05 23:14:22+00:00\n",
       "4  65028 2022-08-05 23:12:57+00:00"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_batch.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving Featurestore feature values: projects/813969514060/locations/us-central1/featurestores/ecomm_recommendation\n",
      "Serve Featurestore feature values backing LRO: projects/813969514060/locations/us-central1/featurestores/ecomm_recommendation/operations/7600792285017538560\n",
      "Featurestore feature values served. Resource name: projects/813969514060/locations/us-central1/featurestores/ecomm_recommendation\n"
     ]
    }
   ],
   "source": [
    "my_featurestore = aiplatform.Featurestore(featurestore_name=FEATURESTORE_ID)\n",
    "batch_serve = my_featurestore.batch_serve_to_df(\n",
    "serving_feature_ids= {'users':['product_id','rating']},\n",
    "read_instances_df = df_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>entity_type_users</th>\n",
       "      <th>product_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-11-25 01:19:43+00:00</td>\n",
       "      <td>12994</td>\n",
       "      <td>768</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-03-20 04:06:31+00:00</td>\n",
       "      <td>70428</td>\n",
       "      <td>768</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-06-01 06:34:33+00:00</td>\n",
       "      <td>40034</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-08-06 15:43:55+00:00</td>\n",
       "      <td>60029</td>\n",
       "      <td>1280</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-04 02:51:01+00:00</td>\n",
       "      <td>46254</td>\n",
       "      <td>1536</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  timestamp entity_type_users  product_id  rating\n",
       "0 2019-11-25 01:19:43+00:00             12994         768     0.0\n",
       "1 2022-03-20 04:06:31+00:00             70428         768     0.0\n",
       "2 2020-06-01 06:34:33+00:00             40034        1024     0.0\n",
       "3 2021-08-06 15:43:55+00:00             60029        1280     0.0\n",
       "4 2021-03-04 02:51:01+00:00             46254        1536     0.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_serve.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input csv file for training the model. This contains data till past week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE_NAME = \"training_data\"\n",
    "INPUT_CSV_URI = f\"{BUCKET_URI}/{INPUT_FILE_NAME}_{TIMESTAMP}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_serve.to_csv(INPUT_CSV_URI, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Keras Model in Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we train the Custom Keras Model for recommending products for a given user with data from the `batch_serve_to_df`method shown in the previous section.\n",
    "\n",
    "You create a custom-trained model using Keras from a Python script in a Docker container using the Vertex SDK for Python, and then get a prediction from the deployed model by sending data.\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Create a Vertex AI custom `TrainingPipeline` for training a model.\n",
    "- Train a TensorFlow model.\n",
    "- Deploy the `Model` resource to a serving `Endpoint` resource.\n",
    "- Make a prediction.\n",
    "- Undeploy the `Model` resource.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Vertex AI SDK for Python\n",
    "\n",
    "Import the Vertex AI SDK for Python into your Python environment and initialize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform import gapic as aip\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Set hardware accelerators\n",
    "\n",
    "You can set hardware accelerators for both training and prediction.\n",
    "\n",
    "Set the variables `TRAIN_GPU/TRAIN_NGPU` and `DEPLOY_GPU/DEPLOY_NGPU` to use a container image supporting a GPU and the number of GPUs allocated to the virtual machine (VM) instance. For example, to use a GPU container image with 4 Nvidia Tesla K80 GPUs allocated to each VM, you would specify:\n",
    "\n",
    "    (aip.AcceleratorType.NVIDIA_TESLA_K80, 4)\n",
    "\n",
    "See the [locations where accelerators are available](https://cloud.google.com/vertex-ai/docs/general/locations#accelerators).\n",
    "\n",
    "Otherwise specify `(None, None)` to use a container image to run on a CPU.\n",
    "\n",
    "Learn [which accelerators are available in your region.](https://cloud.google.com/vertex-ai/docs/general/locations#accelerators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_GPU, TRAIN_NGPU = (aip.AcceleratorType.NVIDIA_TESLA_K80, 1)\n",
    "\n",
    "DEPLOY_GPU, DEPLOY_NGPU = (aip.AcceleratorType.NVIDIA_TESLA_K80, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-8:latest AcceleratorType.NVIDIA_TESLA_K80 1\n",
      "Deployment: us-docker.pkg.dev/vertex-ai/prediction/tf2-gpu.2-8:latest AcceleratorType.NVIDIA_TESLA_K80 1\n"
     ]
    }
   ],
   "source": [
    "TRAIN_VERSION = \"tf-gpu.2-8\"\n",
    "DEPLOY_VERSION = \"tf2-gpu.2-8\"\n",
    "\n",
    "TRAIN_IMAGE = \"us-docker.pkg.dev/vertex-ai/training/{}:latest\".format(TRAIN_VERSION)\n",
    "DEPLOY_IMAGE = \"us-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(DEPLOY_VERSION)\n",
    "\n",
    "print(\"Training:\", TRAIN_IMAGE, TRAIN_GPU, TRAIN_NGPU)\n",
    "print(\"Deployment:\", DEPLOY_IMAGE, DEPLOY_GPU, DEPLOY_NGPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train machine type n1-standard-4\n",
      "Deploy machine type n1-standard-4\n"
     ]
    }
   ],
   "source": [
    "MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Train machine type\", TRAIN_COMPUTE)\n",
    "\n",
    "MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a managed tabular dataset from CSV file.\n",
    "\n",
    "Your first step in training a model is to create a managed dataset instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TabularDataset\n",
      "Create TabularDataset backing LRO: projects/813969514060/locations/us-central1/datasets/9136152177469816832/operations/3322372639015567360\n",
      "TabularDataset created. Resource name: projects/813969514060/locations/us-central1/datasets/9136152177469816832\n",
      "To use this TabularDataset in another session:\n",
      "ds = aiplatform.TabularDataset('projects/813969514060/locations/us-central1/datasets/9136152177469816832')\n"
     ]
    }
   ],
   "source": [
    "dataset = aiplatform.TabularDataset.create(\n",
    "    display_name=\"sample-product-recommend\",gcs_source=[INPUT_CSV_URI]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a model\n",
    "\n",
    "There are two ways you can train a model using a container image:\n",
    "\n",
    "- **Use a Vertex AI pre-built container**. If you use a pre-built training container, you must additionally specify a Python package to install into the container image. This Python package contains your training code.\n",
    "\n",
    "- **Use your own custom container image**. If you use your own container, the container image must contain your training code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the command args for the training script\n",
    "\n",
    "Prepare the command-line arguments to pass to your training script.\n",
    "- `args`: The command line arguments to pass to the corresponding Python module. In this example, they are:\n",
    "  - `\"--epochs=\" + EPOCHS`: The number of epochs for training.\n",
    "  - `\"--batch_size=\" + BATCH_SIZE`: The batch size for training.\n",
    "  - `\"--distribute=\" + TRAIN_STRATEGY` : The training distribution strategy to use for single or distributed training.\n",
    "     - `\"single\"`: single device.\n",
    "     - `\"mirror\"`: all GPU devices on a single compute instance.\n",
    "     - `\"multi\"`: all GPU devices on all compute instances.\n",
    "  - `\"--training_data=\" + GCS_PATH`: The path to the csv w training data from feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_NAME = \"custom_job_\" + TIMESTAMP\n",
    "\n",
    "if not TRAIN_NGPU or TRAIN_NGPU < 2:\n",
    "    TRAIN_STRATEGY = \"single\"\n",
    "else:\n",
    "    TRAIN_STRATEGY = \"mirror\"\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "CMDARGS = [\n",
    "    \"--epochs=\" + str(EPOCHS),\n",
    "    \"--batch_size=\" + str(BATCH_SIZE),\n",
    "    \"--distribute=\" + TRAIN_STRATEGY,\n",
    "    \"--training_data=\" + INPUT_CSV_URI\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training script\n",
    "\n",
    "In the next cell, write the contents of the training script, `task.py`. In summary, the script does the following:\n",
    "\n",
    "- Loads the data from the BigQuery table using the BigQuery Python client library.\n",
    "- Builds a model using TF.Keras model API.\n",
    "- Compiles the model (`compile()`).\n",
    "- Sets a training distribution strategy according to the argument `args.distribute`.\n",
    "- Trains the model (`fit()`) with epochs and batch size according to the arguments `args.epochs` and `args.batch_size`\n",
    "- Gets the directory where to save the model artifacts from the environment variable `AIP_MODEL_DIR`. This variable is [set by the training service](https://cloud.google.com/vertex-ai/docs/training/code-requirements#environment-variables).\n",
    "- Saves the trained model to the model directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile task.py\n",
    "\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "# Read args\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--epochs', dest='epochs',\n",
    "                    default=10, type=int,\n",
    "                    help='Number of epochs.')\n",
    "parser.add_argument('--batch_size', dest='batch_size',\n",
    "                    default=10, type=int,\n",
    "                    help='Batch size.')\n",
    "parser.add_argument('--distribute', dest='distribute', type=str, default='single',\n",
    "                    help='Distributed training strategy.')\n",
    "parser.add_argument('--training_data', dest='training_data', type=str,\n",
    "                    help=\"URI of the training data in BQ\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "# Collect the arguments\n",
    "training_data_uri = args.training_data\n",
    "\n",
    "# Single Machine, single compute device\n",
    "if args.distribute == 'single':\n",
    "    if tf.test.is_gpu_available():\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "    else:\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "# Single Machine, multiple compute device\n",
    "elif args.distribute == 'mirror':\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "# Multiple Machine, multiple compute device\n",
    "elif args.distribute == 'multi':\n",
    "    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
    "\n",
    "# Set up training variables\n",
    "LABEL_COLUMN = \"rating\"\n",
    "UNUSED_COLUMNS = [\"timestamp\"]\n",
    "NA_VALUES = [\"NA\", \".\", \" \", \"\", \"null\"]\n",
    "\n",
    "# # Possible categorical values\n",
    "RATING = [0,1,2,3,4]\n",
    "\n",
    "# Set up BigQuery clients\n",
    "bqclient = bigquery.Client()\n",
    "\n",
    "df_train = pd.read_csv(training_data_uri)\n",
    "\n",
    "# Remove NA values\n",
    "def clean_dataframe(df):\n",
    "    return df.replace(to_replace=NA_VALUES, value=np.NaN).dropna()\n",
    "\n",
    "df_train = clean_dataframe(df_train)\n",
    "\n",
    "# Declaring the constants\n",
    "NUM_USERS = df_train['entity_type_users'].nunique()\n",
    "NUM_PRODUCTS = df_train['product_id'].nunique()\n",
    "\n",
    "def convert_dataframe_to_dataset(\n",
    "    df_train,\n",
    "):\n",
    "    NUMERIC_COLUMNS = [\"entity_type_users\",\"product_id\",\"rating\"]\n",
    "    df_train[NUMERIC_COLUMNS] = df_train[NUMERIC_COLUMNS].astype(\"float32\")\n",
    "    df_train = df_train.drop(columns=UNUSED_COLUMNS)\n",
    "\n",
    "    df_train_x, df_train_y = df_train, df_train.pop(LABEL_COLUMN)\n",
    "\n",
    "    y_train = np.asarray(df_train_y).astype(\"float32\")\n",
    "\n",
    "    # Convert to numpy representation\n",
    "    x_train = np.asarray(df_train_x)\n",
    "\n",
    "    dataset_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    return dataset_train\n",
    "\n",
    "# Create datasets\n",
    "dataset_train = convert_dataframe_to_dataset(df_train)\n",
    "\n",
    "# Shuffle train set\n",
    "dataset_train = dataset_train.shuffle(len(df_train))\n",
    "\n",
    "EMBEDDING_SIZE = 50\n",
    "class RecommenderNet(tf.keras.Model):\n",
    "        def __init__(self, num_users, num_products, embedding_size, **kwargs):\n",
    "            super(RecommenderNet, self).__init__(**kwargs)\n",
    "            self.num_users = num_users\n",
    "            self.num_products = num_products\n",
    "            self.embedding_size = embedding_size\n",
    "            self.user_embedding = tf.keras.layers.Embedding(\n",
    "                num_users,\n",
    "                embedding_size,\n",
    "                embeddings_initializer=\"he_normal\",\n",
    "                embeddings_regularizer=tf.keras.regularizers.l2(1e-6),\n",
    "            )\n",
    "            self.user_bias = tf.keras.layers.Embedding(num_users, 1)\n",
    "            self.product_embedding = tf.keras.layers.Embedding(\n",
    "                num_products,\n",
    "                embedding_size,\n",
    "                embeddings_initializer=\"he_normal\",\n",
    "                embeddings_regularizer=tf.keras.regularizers.l2(1e-6),\n",
    "            )\n",
    "            self.product_bias = tf.keras.layers.Embedding(num_products, 1)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            user_vector = self.user_embedding(inputs[:, 0])\n",
    "            user_bias = self.user_bias(inputs[:, 0])\n",
    "            product_vector = self.product_embedding(inputs[:, 1])\n",
    "            product_bias = self.product_bias(inputs[:, 1])\n",
    "            dot_user_product = tf.tensordot(user_vector, product_vector, 2)\n",
    "            # Add all the components (including bias)\n",
    "            x = dot_user_product + user_bias + product_bias\n",
    "            # The sigmoid activation forces the rating to between 0 and 1\n",
    "            return tf.nn.sigmoid(x)\n",
    "\n",
    "def create_model(num_users,num_products):\n",
    "    # Create model\n",
    "        model = RecommenderNet(num_users, num_products, EMBEDDING_SIZE)\n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        )\n",
    "        return model\n",
    "\n",
    "# Create the model\n",
    "with strategy.scope():\n",
    "    model = create_model(num_users=NUM_USERS,num_products=NUM_PRODUCTS)\n",
    "\n",
    "# Set up datasets\n",
    "NUM_WORKERS = strategy.num_replicas_in_sync\n",
    "# Here the batch size scales up by number of workers since\n",
    "# `tf.data.Dataset.batch` expects the global batch size.\n",
    "GLOBAL_BATCH_SIZE = args.batch_size * NUM_WORKERS\n",
    "dataset_train = dataset_train.batch(GLOBAL_BATCH_SIZE)\n",
    "\n",
    "# Train the model\n",
    "model.fit(dataset_train, epochs=args.epochs)\n",
    "\n",
    "tf.saved_model.save(model, os.getenv(\"AIP_MODEL_DIR\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model\n",
    "\n",
    "Define your custom `TrainingPipeline` on Vertex AI.\n",
    "\n",
    "Use the `CustomTrainingJob` class to define the `TrainingPipeline`. The class takes the following parameters:\n",
    "\n",
    "- `display_name`: The user-defined name of this training pipeline.\n",
    "- `script_path`: The local path to the training script.\n",
    "- `container_uri`: The URI of the training container image.\n",
    "- `requirements`: The list of Python package dependencies of the script.\n",
    "- `model_serving_container_image_uri`: The URI of a container that can serve predictions for your model  either a pre-built container or a custom container.\n",
    "\n",
    "Use the `run` function to start training. The function takes the following parameters:\n",
    "\n",
    "- `args`: The command line arguments to be passed to the Python script.\n",
    "- `replica_count`: The number of worker replicas.\n",
    "- `model_display_name`: The display name of the `Model` if the script produces a managed `Model`.\n",
    "- `machine_type`: The type of machine to use for training.\n",
    "- `accelerator_type`: The hardware accelerator type.\n",
    "- `accelerator_count`: The number of accelerators to attach to a worker replica.\n",
    "\n",
    "The `run` function creates a training pipeline that trains and creates a `Model` object. After the training pipeline completes, the `run` function returns the `Model` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training script copied to:\n",
      "gs://featurestore-bucket-20220812231952/aiplatform-2022-08-12-23:20:47.464-aiplatform_custom_trainer_script-0.1.tar.gz.\n",
      "Training Output directory:\n",
      "gs://featurestore-bucket-20220812231952/aiplatform-custom-training-2022-08-12-23:20:47.566 \n",
      "No dataset split provided. The service will use a default split.\n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/175281120043073536?project=813969514060\n",
      "CustomTrainingJob projects/813969514060/locations/us-central1/trainingPipelines/175281120043073536 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/813969514060/locations/us-central1/trainingPipelines/175281120043073536 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/813969514060/locations/us-central1/trainingPipelines/175281120043073536 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/813969514060/locations/us-central1/trainingPipelines/175281120043073536 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/813969514060/locations/us-central1/trainingPipelines/175281120043073536 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/7766256621650444288?project=813969514060\n",
      "CustomTrainingJob projects/813969514060/locations/us-central1/trainingPipelines/175281120043073536 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/813969514060/locations/us-central1/trainingPipelines/175281120043073536 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/813969514060/locations/us-central1/trainingPipelines/175281120043073536 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/813969514060/locations/us-central1/trainingPipelines/175281120043073536 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/813969514060/locations/us-central1/trainingPipelines/175281120043073536 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/813969514060/locations/us-central1/trainingPipelines/175281120043073536 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/813969514060/locations/us-central1/trainingPipelines/175281120043073536 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/813969514060/locations/us-central1/trainingPipelines/175281120043073536 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "job = aiplatform.CustomTrainingJob(\n",
    "    display_name=JOB_NAME,\n",
    "    script_path=\"task.py\",\n",
    "    container_uri=TRAIN_IMAGE,\n",
    "    requirements=[\"google-cloud-bigquery>=2.20.0\", \"db-dtypes\"],\n",
    "    model_serving_container_image_uri=DEPLOY_IMAGE,\n",
    ")\n",
    "\n",
    "MODEL_DISPLAY_NAME = \"product-recommender-\" + TIMESTAMP\n",
    "\n",
    "# Start the training\n",
    "if TRAIN_GPU:\n",
    "    model = job.run(\n",
    "        dataset=dataset,\n",
    "        model_display_name=MODEL_DISPLAY_NAME,\n",
    "        args=CMDARGS,\n",
    "        replica_count=1,\n",
    "        machine_type=TRAIN_COMPUTE,\n",
    "        accelerator_type=TRAIN_GPU.name,\n",
    "        accelerator_count=TRAIN_NGPU,\n",
    "    )\n",
    "else:\n",
    "    model = job.run(\n",
    "        dataset=dataset,\n",
    "        model_display_name=MODEL_DISPLAY_NAME,\n",
    "        args=CMDARGS,\n",
    "        replica_count=1,\n",
    "        machine_type=TRAIN_COMPUTE,\n",
    "        accelerator_count=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy the model\n",
    "\n",
    "Before you use your model to make predictions, you must deploy it to an `Endpoint`. You can do this by calling the `deploy` function on the `Model` resource. This will do two things:\n",
    "\n",
    "1. Create an `Endpoint` resource for deploying the `Model` resource.\n",
    "2. Deploy the `Model` resource to the `Endpoint` resource.\n",
    "\n",
    "\n",
    "The function takes the following parameters:\n",
    "\n",
    "- `deployed_model_display_name`: A human-readable name for the deployed model.\n",
    "- `traffic_split`: Percent of traffic at the endpoint that goes to this model, which is specified as a dictionary of one or more key/value pairs.\n",
    "   - If only one model, then specify `{ \"0\": 100 }`, where \"0\" refers to this model being uploaded and 100 means 100% of the traffic.\n",
    "   - If there are existing models on the endpoint, for which the traffic will be split, then use `model_id` to specify `{ \"0\": percent, model_id: percent, ... }`, where `model_id` is the ID of an existing `DeployedModel` on the endpoint. The percentages must add up to 100.\n",
    "- `machine_type`: The type of machine to use for training.\n",
    "- `accelerator_type`: The hardware accelerator type.\n",
    "- `accelerator_count`: The number of accelerators to attach to a worker replica.\n",
    "- `starting_replica_count`: The number of compute instances to initially provision.\n",
    "- `max_replica_count`: The maximum number of compute instances to scale to. In this tutorial, only one instance is provisioned.\n",
    "\n",
    "#### Traffic split\n",
    "\n",
    "The `traffic_split` parameter is specified as a Python dictionary. You can deploy more than one instance of your model to an endpoint, and then set the percentage of traffic that goes to each instance.\n",
    "\n",
    "You can use a traffic split to introduce a new model gradually into production. For example, if you had one existing model in production with 100% of the traffic, you could deploy a new model to the same endpoint, direct 10% of traffic to it, and reduce the original model's traffic to 90%. This allows you to monitor the new model's performance while minimizing the disruption to the majority of users.\n",
    "\n",
    "#### Compute instance scaling\n",
    "\n",
    "You can specify a single instance (or node) to serve your online prediction requests. This tutorial uses a single node, so the variables `MIN_NODES` and `MAX_NODES` are both set to `1`.\n",
    "\n",
    "If you want to use multiple nodes to serve your online prediction requests, set `MAX_NODES` to the maximum number of nodes you want to use. Vertex AI auto-scales the number of nodes used to serve your predictions, up to the maximum number you set. Refer to the [pricing page](https://cloud.google.com/vertex-ai/pricing#prediction-prices) to understand the costs of autoscaling with multiple nodes.\n",
    "\n",
    "#### Endpoint\n",
    "\n",
    "The method will block until the model is deployed and eventually return an `Endpoint` object. If this is the first time a model is deployed to the endpoint, it may take a few additional minutes to complete the provisioning of resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOYED_NAME = \"product_recommendation_deployed-\" + TIMESTAMP\n",
    "\n",
    "TRAFFIC_SPLIT = {\"0\": 100}\n",
    "\n",
    "MIN_NODES = 1\n",
    "MAX_NODES = 1\n",
    "\n",
    "if DEPLOY_GPU:\n",
    "    endpoint = model.deploy(\n",
    "        deployed_model_display_name=DEPLOYED_NAME,\n",
    "        traffic_split=TRAFFIC_SPLIT,\n",
    "        machine_type=DEPLOY_COMPUTE,\n",
    "        accelerator_type=DEPLOY_GPU.name,\n",
    "        accelerator_count=DEPLOY_NGPU,\n",
    "        min_replica_count=MIN_NODES,\n",
    "        max_replica_count=MAX_NODES,\n",
    "    )\n",
    "else:\n",
    "    endpoint = model.deploy(\n",
    "        deployed_model_display_name=DEPLOYED_NAME,\n",
    "        traffic_split=TRAFFIC_SPLIT,\n",
    "        machine_type=DEPLOY_COMPUTE,\n",
    "        accelerator_type=DEPLOY_COMPUTE.name,\n",
    "        accelerator_count=0,\n",
    "        min_replica_count=MIN_NODES,\n",
    "        max_replica_count=MAX_NODES,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exporting the users and products Entity Types Features Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Export Feature Values\n",
    "\n",
    "Export feature values for all entities of a single entity type to a BigQuery table or a Cloud Storage bucket. You can choose to get a snapshot or to fully export feature values. A snapshot returns a single value per feature compared to a full export, which can return multiple values per feature. You cannot select particular entity IDs or include multiple entity types when exporting feature values.\n",
    "\n",
    "In this use case, we are getting past 6 days' data using Export Feature Values to do online prediction and recommend products for a user.\n",
    "\n",
    "Both the snapshot and full export options let you query data by specifying a single timestamp (either the start time or end time) or both timestamps. \n",
    "\n",
    "For snapshots, Vertex AI Feature Store returns the latest feature value within a given time range. In the output, the associated timestamp with each feature value is the snapshot timestamp (not the feature value timestamp).\n",
    "\n",
    "For full exports, Vertex AI Feature Store returns all feature values within a given time range. In the output, the associated timestamp with each feature value is the feature timestamp (the specified timestamp when the feature value was ingested).\n",
    "\n",
    "When you export feature values, you choose which features to query and whether it is a snapshot or a full export. The following sections show an example of full export.\n",
    "\n",
    "#### Null values\n",
    "For snapshots, if the latest feature value is null at a given timestamp, Vertex AI Feature Store returns the previous non-null feature value. If there are no previous non-null values, Vertex AI Feature Store returns null.\n",
    "\n",
    "For full exports, if a feature value is null at a given timestamp, Vertex AI Feature Store returns null for that timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USERS_EXPORT_DESTINATION_TABLE_NAME = \"product_recommendation\"  # @param {type:\"string\"}\n",
    "\n",
    "USERS_EXPORT_DESTINATION_TABLE_URI = DESTINATION_PATTERN.format(\n",
    "    project=PROJECT_ID, dataset=DESTINATION_DATA_SET, table=USERS_EXPORT_DESTINATION_TABLE_NAME\n",
    ")\n",
    "\n",
    "PRODUCTS_EXPORT_DESTINATION_TABLE_NAME = \"prod_users_list_lookup\"  # @param {type:\"string\"}\n",
    "\n",
    "PRODUCTS_EXPORT_DESTINATION_TABLE_URI = DESTINATION_PATTERN.format(\n",
    "    project=PROJECT_ID, dataset=DESTINATION_DATA_SET, table=PRODUCTS_EXPORT_DESTINATION_TABLE_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export Feature Values from `users` Entity Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_users_request = featurestore_service_pb2.ExportFeatureValuesRequest(\n",
    "    entity_type=admin_client.entity_type_path(\n",
    "        PROJECT_ID, REGION, FEATURESTORE_ID, \"users\"\n",
    "    ),\n",
    "    destination=featurestore_service_pb2.FeatureValueDestination(\n",
    "        bigquery_destination=io_pb2.BigQueryDestination(\n",
    "            # Output to BigQuery table created earlier\n",
    "            output_uri=USERS_EXPORT_DESTINATION_TABLE_URI\n",
    "        )\n",
    "    ),\n",
    "    feature_selector=FeatureSelector(\n",
    "                id_matcher=IdMatcher(ids=[\"*\"])\n",
    "            ),\n",
    "    full_export=featurestore_service_pb2.ExportFeatureValuesRequest.FullExport(\n",
    "        start_time=Timestamp(seconds=past_6days()),\n",
    "        end_time=Timestamp(seconds=current_time())\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the Export Feature Values\n",
    "export_serving_lro = admin_client.export_feature_values(export_users_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polls for the LRO status and prints when the LRO has completed\n",
    "export_serving_lro.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export Feature Values from `products` Entity Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_products_request = featurestore_service_pb2.ExportFeatureValuesRequest(\n",
    "    entity_type=admin_client.entity_type_path(\n",
    "        PROJECT_ID, REGION, FEATURESTORE_ID, \"products\"\n",
    "    ),\n",
    "    destination=featurestore_service_pb2.FeatureValueDestination(\n",
    "        bigquery_destination=io_pb2.BigQueryDestination(\n",
    "            # Output to BigQuery table created earlier\n",
    "            output_uri=PRODUCTS_EXPORT_DESTINATION_TABLE_URI\n",
    "        )\n",
    "    ),\n",
    "    feature_selector=FeatureSelector(\n",
    "                id_matcher=IdMatcher(ids=[\"*\"])\n",
    "            ),\n",
    "    full_export=featurestore_service_pb2.ExportFeatureValuesRequest.FullExport(\n",
    "        end_time=Timestamp(seconds=past_6days())\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the Export Feature Values\n",
    "export_serving_lro = admin_client.export_feature_values(export_products_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polls for the LRO status and prints when the LRO has completed\n",
    "export_serving_lro.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the LRO finishes, you should be able to see the result from the [BigQuery console](https://console.cloud.google.com/bigquery) in the dataset created earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Product Recommendation for a user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we perform the online recommendation using the data from the `ExportFeatureValues` API.\n",
    "\n",
    "* Load the `ExportFeatureValues` data into dataframe.\n",
    "* Get Recommendation for a user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_table(table_name):\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    bqclient_master = bigquery.Client()\n",
    "\n",
    "    # Download query results.\n",
    "    query_string = f\"\"\"\n",
    "    SELECT\n",
    "         *\n",
    "    FROM\n",
    "      `{PROJECT_ID}`.{DESTINATION_DATA_SET}.{table_name}\n",
    "    \"\"\"\n",
    "\n",
    "    bq_export = (\n",
    "        bqclient_master.query(query_string)\n",
    "        .result()\n",
    "        .to_dataframe(\n",
    "            create_bqstorage_client=True,\n",
    "        )\n",
    "    )\n",
    "    return bq_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_export = export_table(PRODUCTS_EXPORT_DESTINATION_TABLE_NAME)\n",
    "users_export = export_table(USERS_EXPORT_DESTINATION_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting one user_id to get recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_id = users_export.entity_type_users.sample(1).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_export.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Make an online prediction request\n",
    "\n",
    "Send an online prediction request to your deployed model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Send the prediction request\n",
    "\n",
    "Now that you have test data, you can use it to send a prediction request. Use the `Endpoint` object's `predict` function, which takes the following parameters:\n",
    "\n",
    "- `instances`: A list of penguin measurement instances. According to your custom model, each instance should be an array of numbers. You prepared this list in the previous step.\n",
    "\n",
    "The `predict` function returns a list, where each element in the list corresponds to the instance in the request. In the output for each prediction, you will see the following:\n",
    "\n",
    "- Confidence level for the prediction (`predictions`), between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get list of products bought and not bought by the user from `products` entity type till past week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = products_export.apply(lambda products_export: user_id in products_export['users_list'], axis=1)\n",
    "user_not_bought = products_export[~filtered]['entity_type_products'].astype(\"float32\").to_list()\n",
    "user_bought = products_export[filtered]['entity_type_products'].astype(\"float32\").to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Instance_input = [[float(user_id), k] for k in user_not_bought]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Online prediction using keras custom model for a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = endpoint.predict(instances=Instance_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Top 10 products recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based upon the ratings predicted by recommendation model, We selected top 10 products for the selected `user_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions_array = np.array([predictions.predictions[k][0] for k in range(len(predictions.predictions))])\n",
    "top_rating_indices = predictions_array.argsort()[-10:][::-1]\n",
    "top_predictions = predictions_array[top_rating_indices]\n",
    "top_10_products = [int(Instance_input[k][1]) for k in top_rating_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see Top 10 list of products recommended for a user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c",
    "tags": []
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n",
    "- Training Job\n",
    "- Model\n",
    "- Endpoint\n",
    "- Cloud Storage Bucket\n",
    "- Featurestore\n",
    "- BQ Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Undeploy the model\n",
    "\n",
    "To undeploy your `Model` resource from the serving `Endpoint` resource, use the endpoint's `undeploy` method with the following parameter:\n",
    "\n",
    "- `deployed_model_id`: The model deployment identifier returned by the endpoint service when the `Model` resource was deployed. You can retrieve the deployed models using the endpoint's `deployed_models` property.\n",
    "\n",
    "Since this is the only deployed model on the `Endpoint` resource, you can omit `traffic_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deployed_model_id = endpoint.list_models()[0].id\n",
    "endpoint.undeploy(deployed_model_id=deployed_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the training job\n",
    "job.delete()\n",
    "\n",
    "# Delete the model\n",
    "model.delete()\n",
    "\n",
    "# Delete the endpoint\n",
    "endpoint.delete()\n",
    "\n",
    "# Delete Bucket\n",
    "! gsutil -m rm -r $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deleting Featurestore and BigQuery dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a2WfqhbFzQBF"
   },
   "outputs": [],
   "source": [
    "delete_fs_lro = admin_client.delete_featurestore(\n",
    "    request=featurestore_service_pb2.DeleteFeaturestoreRequest(\n",
    "        name=admin_client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID),\n",
    "        force=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Deleted Featurestore\", delete_fs_lro.result())\n",
    "\n",
    "client.delete_dataset(\n",
    "    DESTINATION_DATA_SET, delete_contents=True, not_found_ok=True\n",
    ")  # Make an API request.\n",
    "\n",
    "print(\"Deleted dataset '{}'.\".format(DESTINATION_DATA_SET))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ze4-nDLfK4pw"
   ],
   "name": "gapic-feature-store.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m92",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m92"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

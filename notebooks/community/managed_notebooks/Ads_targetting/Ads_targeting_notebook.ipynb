{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d3bdd17",
   "metadata": {},
   "source": [
    "# Training a multi-class classification model for Ads-targeting usecase\n",
    "\n",
    "## Table Of Contents\n",
    "* [Overview](#section-1)\n",
    "* [Dataset](#section-2)\n",
    "* [Objective](#section-3)\n",
    "* [Costs](#section-4)\n",
    "* [Tutorial](#section-5)\n",
    "\t- [Fetch the required data from Bigquery](#section-5)\n",
    "    - [Preprocess the data](#section-6)\n",
    "    - [Train a Tensorflow model](#section-7)\n",
    "    - [Run the model on test data](#section-8)\n",
    "    - [Automating the execution of the notebook using Executor](#section-9)\n",
    "    - [Scheduled Runs on Executor](#section-10)\n",
    "    - [Parametrizing the variables](#section-11)\n",
    "* [Save the model to a GCS path](#section-12)\n",
    "* [Clean Up](#section-13)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16af573f",
   "metadata": {},
   "source": [
    "## Overview\n",
    "<a name=\"section-1\"></a>\n",
    "\n",
    "This tutorial demonstrates building a machine-learning model for an Ads-targeting use case. Ads-targeting is an advertisement technique where chosen or tailor-made ads are shown to the customers based on their past behavior and preferences.Targeted ads are meant to reach certain customers based on demographics, psychographics, behavior and other second-order activities that are learned usually through data collected from the customers. \n",
    "\n",
    "## Dataset\n",
    "<a name=\"section-2\"></a>\n",
    "This notebook uses the following dataset in Bigquery : ```looker-private-demo.ecomm```. The dataset consists of information about various advertisement campaigns including the demographics of users who have clicked and made some purchases after seeing the ads. For the current tutorial, top 3 campaigns from USA will be selected from this dataset and user information for those who have made purchases shall be used to train a model with the campaigns as the classes. The idea is to see if the advertisement and the user data can be used to identify which campaign suits best for the user.\n",
    "\n",
    "The dataset can be accessed by pinning the ```looker-private-demo``` project in Bigquery. Instead of going to Bigquery UI, this process can be performed from the current Jupyter environment(on Vertex-AI's managed-instance) itself. Vertex-AI's managed instances support browsing through the datasets and tables from Bigquery through its **Bigquery In Notebooks** feature. \n",
    "\n",
    "<img src=\"images/Bigquery_UI_new.PNG\"></img>\n",
    "\n",
    "## Objective\n",
    "<a name=\"section-3\"></a>\n",
    "This notebook demonstrates collecting required data from Bigquery, preprocessing it and training a multi-class classification model on an E-commerce dataset. The steps performed include the following :\n",
    "\n",
    "- Fetch the required data from Bigquery\n",
    "- Preprocess the data\n",
    "- Train a Tensorflow(>=2.4) classification model\n",
    "- Evaluate the loss for the trained model\n",
    "- Automating the notebook execution using Executor feature\n",
    "- Save the model to a GCS path\n",
    "- Clean up of the created resources\n",
    "\n",
    "### Costs \n",
    "<a name=\"section-4\"></a>\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Bigquery\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing), [Bigquery\n",
    "pricing](https://cloud.google.com/bigquery/pricing) and [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad80808e",
   "metadata": {},
   "source": [
    "## Tutorial\n",
    "\n",
    "### Fetch the required data from Bigquery \n",
    "<a name=\"section-5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668c1c6a",
   "metadata": {},
   "source": [
    "#@bigquery\n",
    "\n",
    "WITH\n",
    "  traindata AS (\n",
    "  SELECT\n",
    "    b.* EXCEPT(ad_event_id,\n",
    "      user_id),\n",
    "    c.* EXCEPT(id),\n",
    "    d.* EXCEPT(keyword_id,\n",
    "      ad_id),\n",
    "    a.amount,\n",
    "    a.device_type,\n",
    "    e.name\n",
    "  FROM\n",
    "    `looker-private-demo.ecomm.ad_events` a\n",
    "  JOIN (\n",
    "    SELECT\n",
    "      ad_event_id,\n",
    "      user_id,\n",
    "      state,\n",
    "      os,\n",
    "      browser\n",
    "    FROM\n",
    "      `looker-private-demo.ecomm.events`\n",
    "    WHERE\n",
    "      event_type=\"Purchase\"\n",
    "      AND country=\"USA\") b\n",
    "  ON\n",
    "    a.id = b.ad_event_id\n",
    "  JOIN (\n",
    "    SELECT\n",
    "      id,\n",
    "      gender,\n",
    "      age\n",
    "    FROM\n",
    "      `looker-private-demo.ecomm.users`) c\n",
    "  ON\n",
    "    b.user_id = c.id\n",
    "  JOIN (\n",
    "    SELECT\n",
    "      keyword_id,\n",
    "      ad_id,\n",
    "      cpc_bid_amount,\n",
    "      bidding_strategy_type,\n",
    "      quality_score,\n",
    "      keyword_match_type\n",
    "    FROM\n",
    "      `looker-private-demo.ecomm.keywords`\n",
    "    WHERE\n",
    "      cpc_bid_amount <= 3000) d\n",
    "  ON\n",
    "    a.keyword_id = d.keyword_id\n",
    "  JOIN (\n",
    "    SELECT\n",
    "      ad_id,\n",
    "      name\n",
    "    FROM\n",
    "      `looker-private-demo.ecomm.ad_groups`) e\n",
    "  ON\n",
    "    d.ad_id = e.ad_id )\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  traindata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77b1fc9",
   "metadata": {},
   "source": [
    "Once the results from Bigquery are displayed in the above cell, press the **Query and load as DataFrame** button and execute the generated code stub to fetch the data into into the current notebook as a dataframe. \n",
    "\n",
    "*Note : By default the data is loaded into \"df\" variable and it could be changed before executing the cell if required.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "12edaae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following two lines are only necessary to run once.\n",
    "# Comment out otherwise for speed-up.\n",
    "from google.cloud.bigquery import Client, QueryJobConfig\n",
    "client = Client()\n",
    "\n",
    "query = \"\"\"WITH traindata AS (\n",
    "SELECT b.* except(ad_event_id, user_id), c.* except(id), d.* except(keyword_id, ad_id), a.amount, a.device_type, e.name\n",
    "FROM `looker-private-demo.ecomm.ad_events` a\n",
    "JOIN \n",
    "(SELECT ad_event_id, user_id, state, os, browser from `looker-private-demo.ecomm.events` WHERE event_type=\"Purchase\" AND country=\"USA\") b\n",
    "ON a.id = b.ad_event_id\n",
    "JOIN \n",
    "(SELECT id, gender, age FROM `looker-private-demo.ecomm.users`) c\n",
    "ON b.user_id = c.id\n",
    "JOIN\n",
    "(SELECT keyword_id, ad_id, cpc_bid_amount, bidding_strategy_type, quality_score, keyword_match_type FROM `looker-private-demo.ecomm.keywords`\n",
    "WHERE cpc_bid_amount <= 3000) d\n",
    "ON a.keyword_id = d.keyword_id\n",
    "JOIN\n",
    "(SELECT ad_id, name FROM `looker-private-demo.ecomm.ad_groups`) e\n",
    "ON d.ad_id = e.ad_id\n",
    ")\n",
    "SELECT * FROM traindata\"\"\"\n",
    "job = client.query(query)\n",
    "df = job.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c07fe8f",
   "metadata": {},
   "source": [
    "### Preprocess the data\n",
    "<a name=\"section-6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8632011f",
   "metadata": {},
   "source": [
    "Import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "87497ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from google.cloud import storage\n",
    "from seaborn import heatmap\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c321599a",
   "metadata": {},
   "source": [
    "Select the necessary columns from the E-commerce data and divide them based on their type(numerical/categorical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c72a7f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'name'\n",
    "categ_cols = ['state', 'os', 'browser', 'gender', 'bidding_strategy_type', \n",
    "             'keyword_match_type', 'device_type']\n",
    "num_cols = ['age', 'cpc_bid_amount', 'quality_score', 'amount']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fae9bd",
   "metadata": {},
   "source": [
    "From the current dataset, only the top 3 camapigns will be chosen to target the users. All the relevant information about the advertisement and the user who purchased an item after seeing the advertisement is available in the dataframe already. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c1498768",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['name'].isin(['Tops & Tees', 'Active', 'Accessories'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9f5f8a",
   "metadata": {},
   "source": [
    "Encode the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "88d49f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['name'] = df['name'].map({'Tops & Tees':0, 'Active':1, 'Accessories':2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62f2d68",
   "metadata": {},
   "source": [
    "One-hot encode the categorical variables. After one-hot encoding, the first level-column is dropped to avoid [dummy-variable trap](https://en.wikipedia.org/wiki/Dummy_variable_(statistics)) scenario. This process is called *dummy-encoding*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c1647543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8794, 71)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_cols(data, col):\n",
    "    # Creating a dummy variable for the variable 'CategoryID' and dropping the first one.\n",
    "    categ = pd.get_dummies(data[col],prefix=col,drop_first=True)\n",
    "    #Adding the results to the master dataframe\n",
    "    data = pd.concat([data,categ],axis=1)\n",
    "    return data\n",
    "\n",
    "## dummy-encode the categorical fields\n",
    "for i in categ_cols: \n",
    "    df = encode_cols(df, i)\n",
    "    df.drop(columns=[i], inplace=True)\n",
    "\n",
    "## check the data's shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec26636",
   "metadata": {},
   "source": [
    "Split the data into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "75b6e557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7035, 70) (1759, 70)\n"
     ]
    }
   ],
   "source": [
    "X = df[[i for i in df.columns if i != target]].copy()\n",
    "y = df[target].copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8, random_state=36)\n",
    "print (X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac5e973",
   "metadata": {},
   "source": [
    "Scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ade9f852",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train.loc[:,num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "X_test.loc[:, num_cols] = scaler.transform(X_test[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0600de7c",
   "metadata": {},
   "source": [
    "### Train a Tensorflow model\n",
    "<a name=\"section-7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1304f39e",
   "metadata": {},
   "source": [
    "Convert target column to categorical encoded colum (one-hot encoded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e0dbb3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_categ = to_categorical(y_train)\n",
    "y_test_categ = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a8089b",
   "metadata": {},
   "source": [
    "Define hyperparameters for model training. \n",
    "\n",
    "*Note: Comment or remove the parameters from the following cell if they are provided already as an input parameter through the Executor feature.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fd05a7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = \"sgd\"\n",
    "num_hidden_layers = 3\n",
    "num_neurons = [64, 128, 256]\n",
    "activ_func = ['relu', 'relu', 'relu']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5a4352",
   "metadata": {},
   "source": [
    "Define the architecture and compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2a6c8f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_34 (Dense)             (None, 64)                4544      \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 46,659\n",
      "Trainable params: 46,659\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "## construct the neural network as per the defined parameters\n",
    "for i in range(num_hidden_layers) : \n",
    "    if i == 0 :\n",
    "        ## add the input layer\n",
    "        model.add(Dense(num_neurons[i], activation=activ_func[i], input_shape=(X_train.shape[1], )))\n",
    "    else : \n",
    "        ## add the hidden layers\n",
    "        model.add(Dense(num_neurons[i], activation=activ_func[i]))\n",
    "\n",
    "## add the output layer\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "## compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\" , optimizer=optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99a7900",
   "metadata": {},
   "source": [
    "Fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5914e66a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "220/220 [==============================] - 1s 2ms/step - loss: 1.0987\n",
      "Epoch 2/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0959\n",
      "Epoch 3/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0949\n",
      "Epoch 4/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0934\n",
      "Epoch 5/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0923\n",
      "Epoch 6/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0911\n",
      "Epoch 7/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0902\n",
      "Epoch 8/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0891\n",
      "Epoch 9/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0881\n",
      "Epoch 10/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0874\n",
      "Epoch 11/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0865\n",
      "Epoch 12/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0856\n",
      "Epoch 13/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0850\n",
      "Epoch 14/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0841\n",
      "Epoch 15/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0832\n",
      "Epoch 16/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0825\n",
      "Epoch 17/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0819\n",
      "Epoch 18/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0810\n",
      "Epoch 19/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0802\n",
      "Epoch 20/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0794\n",
      "Epoch 21/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0788\n",
      "Epoch 22/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0780\n",
      "Epoch 23/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0772\n",
      "Epoch 24/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0765\n",
      "Epoch 25/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0760\n",
      "Epoch 26/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0752\n",
      "Epoch 27/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0744\n",
      "Epoch 28/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0734\n",
      "Epoch 29/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0725\n",
      "Epoch 30/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0722\n",
      "Epoch 31/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0715\n",
      "Epoch 32/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0706\n",
      "Epoch 33/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0694\n",
      "Epoch 34/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0689\n",
      "Epoch 35/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0682\n",
      "Epoch 36/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0673\n",
      "Epoch 37/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0663\n",
      "Epoch 38/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0657\n",
      "Epoch 39/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0644\n",
      "Epoch 40/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0640\n",
      "Epoch 41/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0630\n",
      "Epoch 42/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0619\n",
      "Epoch 43/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0605\n",
      "Epoch 44/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0599\n",
      "Epoch 45/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0591\n",
      "Epoch 46/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0577\n",
      "Epoch 47/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0568\n",
      "Epoch 48/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0558\n",
      "Epoch 49/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0547\n",
      "Epoch 50/50\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.0538\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train_categ, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3885f85",
   "metadata": {},
   "source": [
    "### Run the model on test data\n",
    "<a name=\"section-8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895608da",
   "metadata": {},
   "source": [
    "Evaluate the model on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5b600628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 0s 1ms/step - loss: 1.0868\n",
      "Test results - Loss: 1.0867596864700317\n"
     ]
    }
   ],
   "source": [
    "test_results = model.evaluate(X_test, y_test_categ, verbose=1)\n",
    "print(f'Test results - Loss: {test_results}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baaf11b",
   "metadata": {},
   "source": [
    "### Automating the execution of the notebook using Executor\n",
    "<a name=\"section-9\"></a>\n",
    "\n",
    "Running the noteboook from start to end has become more prowerful with the new executor feature. Hitting the executor button would bring up a form that can be filled with the choice of the environment, machine-type, input parameters etc. After submitting it, the notebook gets executed as a job in the Vertex-ai custom training jobs. The running jobs can be monitored from the <b>Notebook Executor</b> pane in the menu on the left.\n",
    "\n",
    "<img src=\"images/executor.png\"></img>\n",
    "\n",
    "\n",
    "Executor gives us the freedom to choose the environment and machine-type while automating the runs similar to Vertex-Ai training jobs without switching to the training-jobs UI. Apart from the custom-container that replicates the existing kernel by default, pre-built environments like Tensorflow-Enterprise, PyTorch etc. can also be selected to run the notebook. Furthermore the required compute-power can be specified by choosing from the list of machine-types available including GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ac924a",
   "metadata": {},
   "source": [
    "### Scheduled Runs on Executor\n",
    "<a name=\"section-10\"></a>\n",
    "\n",
    "The runs can also be scheduled recurringly with the Executor. To do so, <b>Schedule-based recurring executions </b> needs to be selected in the run type insetead of <b>One-time execution</b>. Further, the frequency of the job and the time when it needs to execute can be provided in the form itself.\n",
    "\n",
    "\n",
    "<img src=\"images/executor_scheduled_runs2.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bbe47b",
   "metadata": {},
   "source": [
    "### Parametrizing the variables\n",
    "<a name=\"section-11\"></a>\n",
    "\n",
    "Executor also makes it easy to run the notebooks with different set of input paramters. If required, the needed constants in the notebook can be treated as arguments to a function and while submitting the Executor form, those constants can be given as input parameters.\n",
    "\n",
    "<img src=\"images/executor_input_parameters.png\"></img>\n",
    "\n",
    "The hyperparameters defined during the model-training step can be passed as arguments while submitting this executor form. Of course, the values defined in this notebook should be removed or commented out before submitting for execution. Otherwise, the input parameters would just be overwritten in the job. Executor feature would thus allow us to also run the notebook as a training job with different parameter settings everytime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e1ce7c",
   "metadata": {},
   "source": [
    "### Save the model to a GCS path\n",
    "<a name=\"section-12\"></a>\n",
    "\n",
    "Tensorflow's *model.save()* method supports GCS paths as well as the local file paths while writing the model object to a file. It needs to be ensured that the service-account being used to run this notebook has write permissions to the specified GCS path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ee0b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_PATH = \"gs://[your-bucket-name]/[path-to-save]/\"\n",
    "model.save(GCS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ced8399",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "<a name=\"section-13\"></a>\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814afbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil -m rm -r [gcs-folder-path-to-delete]"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "managed-notebooks.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/managed-notebooks:m80"
  },
  "kernelspec": {
   "display_name": "TensorFlow 2 (Local)",
   "language": "python",
   "name": "local-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8c4e360024a"
      },
      "source": [
        "# Taxi fare prediction using chicago taxi-cab dataset\n",
        "\n",
        "## Table of contents\n",
        "* [Overview](#section-1)\n",
        "* [Dataset](#section-2)\n",
        "* [Objective](#section-3)\n",
        "* [Costs](#section-4)\n",
        "* [Data analysis](#section-5)\n",
        "* [Fit a simple linear regression model](#section-6)\n",
        "* [Save the model and upload to a GCS bucket](#section-7)\n",
        "* [Deploy the model on Vertex AI with support for Vertex Explainable AI](#section-8)\n",
        "* [Get explanations from the deployed model](#section-9)\n",
        "* [Clean up](#section-10)\n",
        "\n",
        "## Overview\n",
        "<a name=\"section-1\"></a>\n",
        "\n",
        "This notebooks demonstrates analysis, feature selection, model building and deployment with Vertex Explainable AI configured on Vertex AI on a subset of the Chicago Taxi-cab dataset for Taxi-fare prediction problem.\n",
        "\n",
        "Note: This notebook file was developed to run in a [Vertex AI Workbench managed notebooks](https://console.cloud.google.com/vertex-ai/workbench/list/managed) instance using the Python(Local) kernel. Some components of this notebook may not work in other notebook environments.\n",
        "\n",
        "## Dataset\n",
        "<a name=\"section-2\"></a>\n",
        "\n",
        "The Chicago Taxi-cab dataset includes taxi trips from 2013 to the present, reported to the City of Chicago in its role as a regulatory agency. To protect privacy but allow for aggregate analyses, the Taxi ID is consistent for any given taxi medallion number but does not show the number, Census Tracts are suppressed in some cases, and times are rounded to the nearest 15 minutes. Due to the data reporting process, not all trips are reported but the City believes that most are. This dataset is publicly available on Bigquery under the public datasets with the Table ID : `bigquery-public-data.chicago_taxi_trips.taxi_trips` and also as public dataset on Kaggle Datasets at : [Chicago Taxi Trips Dataset](https://www.kaggle.com/chicago/chicago-taxi-trips-bq).\n",
        "\n",
        " For more information about this dataset and how it was created, please refer [Chicago Digital website](http://digital.cityofchicago.org/index.php/chicago-taxi-data-released).\n",
        "\n",
        "## Objective\n",
        "<a name=\"section-3\"></a>\n",
        "\n",
        "The goal of this notebook is to provide an overview on the latest Vertex AI features like Explainable AI  and Bigquery in Notebook by trying to solve a Taxi-fare prediction problem. The steps followed in this notebook include : \n",
        "\n",
        "- Loading the dataset using `Bigquery in Notebooks`.\n",
        "- Performing exploratory data analysis on the dataset.\n",
        "- Feature selection and preprocessing.\n",
        "- Building a linear regression model using scikit-learn.\n",
        "- Configuring the model for Vertex Explainable AI.\n",
        "- Deploying the model to Vertex AI.\n",
        "- Testing the deployed model.\n",
        "- Clean up.\n",
        "\n",
        "## Costs\n",
        "<a name=\"section-4\"></a>\n",
        "\n",
        "This tutorial uses the following billable components of Google Cloud:\n",
        "\n",
        "- Vertex AI\n",
        "- Bigquery\n",
        "- Cloud Storage\n",
        "\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing), [Bigquery pricing](https://cloud.google.com/bigquery/pricing) and [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ed1f5e85640"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "042ebe9fe074"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"\"\n",
        "\n",
        "# Get your Google Cloud project ID from gcloud\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID: \", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "750bf2883c2d"
      },
      "source": [
        "Otherwise, set your project ID here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c6db1ca88b9"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
        "    PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fed4b24ea061"
      },
      "source": [
        "## Select or Create Cloud Storage Bucket for storing the model\n",
        "\n",
        "When you create a model resource on Vertex AI using the Cloud SDK, you need to give a Cloud Storage bucket uri of the model where the model is stored. Using the model saved, you can then create Vertex AI model and endpoint resources in order to serve online predictions.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. It must be unique across all Cloud Storage buckets.You may also change the REGION variable, which is used for operations throughout the rest of this notebook. Make sure to choose a region where Vertex AI services are available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4f5f52d977e2"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"[your-bucket-name]\"\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
        "LOCATION = \"us-central1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f9a6a5c91cf"
      },
      "outputs": [],
      "source": [
        "# Set a default bucketname in case bucket name is not given\n",
        "if BUCKET_NAME == \"\" or BUCKET_NAME == \"[your-bucket-name]\" or BUCKET_NAME is None:\n",
        "\n",
        "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "    BUCKET_NAME = PROJECT_ID + \"aip-\" + TIMESTAMP\n",
        "    BUCKET_URI = \"gs://\" + BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e27907c70873"
      },
      "source": [
        "<b>Only if your bucket doesn't already exist</b>: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95702536e547"
      },
      "source": [
        "## Import the required libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0cc49ab6e69"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6128121efc03"
      },
      "source": [
        "Next, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "637ea7607c58"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94d579ea1834"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# load the required libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5166f42557ad"
      },
      "source": [
        "The dataset is quite a large and noisy one and so data from a specific date range will be used. Based on various blogs and resources that are available online, many of them seem to have used the data from around May-2018 which gave some really good results compared to the other date ranges. While there are also some complicated research models propsed for the same problem like considering the weather data, holidays and seasons etc., the current notebook only explores a simple linear regression model as our main objective is to demonstrate the model deployment with Vertex Explainable AI configured on Vertex AI.\n",
        "\n",
        "## Accessing the data through Bigquery in Notebooks\n",
        "`Bigquery in Notebooks` feature of Vertex AI's managed notebooks allows us to use Bigquery and its features from the notebook itself eliminating the need to switch between tabs everytime. For every cell in the notebook, there is an option for Bigquery integration at the top right selecting which would enable us to compose a SQL query that can be executed in Bigquery. \n",
        "\n",
        "The chosen dataset consists of the following fields :\n",
        "- `unique_key` : Unique identifier for the trip.\n",
        "- `taxi_id` : A unique identifier for the taxi.\n",
        "- `trip_start_timestamp`: When the trip started, rounded to the nearest 15 minutes.\n",
        "- `trip_end_timestamp`: When the trip ended, rounded to the nearest 15 minutes.\n",
        "- `trip_seconds`: Time of the trip in seconds.\n",
        "- `trip_miles`: Distance of the trip in miles.\n",
        "- `pickup_census_tract`: The Census Tract where the trip began. For privacy, this Census Tract is not shown for some trips.\n",
        "- `dropoff_census_tract`: The Census Tract where the trip ended. For privacy, this Census Tract is not shown for some trips.\n",
        "- `pickup_community_area`: The Community Area where the trip began.\n",
        "- `dropoff_community_area`: The Community Area where the trip ended.\n",
        "- `fare`: The fare for the trip.\n",
        "- `tips`: The tip for the trip. Cash tips generally will not be recorded.\n",
        "- `tolls`: The tolls for the trip.\n",
        "- `extras`: Extra charges for the trip.\n",
        "- `trip_total`: Total cost of the trip, the total of the fare, tips, tolls, and extras.\n",
        "- `payment_type`: Type of payment for the trip.\n",
        "- `company`: The taxi company.\n",
        "- `pickup_latitude`: The latitude of the center of the pickup census tract or the community area if the census tract has been hidden for privacy.\n",
        "- `pickup_longitude`: The longitude of the center of the pickup census tract or the community area if the census tract has been hidden for privacy.\n",
        "- `pickup_location`: The location of the center of the pickup census tract or the community area if the census tract has been hidden for privacy.\n",
        "- `dropoff_latitude`: The latitude of the center of the dropoff census tract or the community area if the census tract has been hidden for privacy.\n",
        "- `dropoff_longitude`: The longitude of the center of the dropoff census tract or the community area if the census tract has been hidden for privacy.\n",
        "- `dropoff_location`: The location of the center of the dropoff census tract or the community area if the census tract has been hidden for privacy.\n",
        "\n",
        "Among the available fields in the dataset, only the fields that seem common and relevant for analysis and modeling like `taxi_id`, `trip_start_timestamp`, `trip_seconds`, `trip_miles`, `payment_type` and `trip_total` are selected. Further, the field `trip_total` is treated as the target variable that would be predicted by the machine learning model. Apparently, this field is a summation of `fare`,`tips`,`tolls` and `extras` fields and so because of their correlation with the target variable, they are being excluded for modeling. Due to the volume of the data, a subset of the dataset over the course of one week i.e., 12-May-2018 to 18-May-2018 is being considered. Within this date range itself, the datapoints can be noisy and so a few conditions like the following are considered : \n",
        "\n",
        "- Time taken for the trip > 0.\n",
        "- Distance covered during the trip > 0.\n",
        "- Total trip charges > 0 and\n",
        "- Pickup and dropoff areas are valid(not empty)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53e0070caf3d"
      },
      "source": [
        "#@bigquery\n",
        "\n",
        "select \n",
        "-- select the required fields\n",
        "taxi_id, trip_start_timestamp, \n",
        "trip_seconds, trip_miles, trip_total, \n",
        "payment_type\n",
        "\n",
        "from `bigquery-public-data.chicago_taxi_trips.taxi_trips` \n",
        "where \n",
        "-- specify the required criteria\n",
        "trip_start_timestamp >= '2018-05-12' and \n",
        "trip_end_timestamp <= '2018-05-18' and\n",
        "trip_seconds > 0 and\n",
        "trip_miles > 0 and\n",
        "trip_total > 3 and\n",
        "pickup_community_area is not NULL and \n",
        "dropoff_community_area is not NULL\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "781341730c28"
      },
      "source": [
        "The Bigquery integration also allows us to load the queried data into a pandas dataframe using the `Query and load as DataFrame` button. Clicking the button adds a new cell below that provides a code snippet to load the data into a dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beab4629fa0c"
      },
      "outputs": [],
      "source": [
        "# The following two lines are only necessary to run once.\n",
        "# Comment out otherwise for speed-up.\n",
        "from google.cloud.bigquery import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "query = \"\"\"select \n",
        "taxi_id, trip_start_timestamp, \n",
        "trip_seconds, trip_miles, trip_total, \n",
        "payment_type, pickup_community_area, \n",
        "dropoff_community_area \n",
        "\n",
        "from `bigquery-public-data.chicago_taxi_trips.taxi_trips` \n",
        "where \n",
        "trip_start_timestamp >= '2018-05-12' and \n",
        "trip_end_timestamp <= '2018-05-18' and\n",
        "trip_seconds > 0 and trip_seconds < 6*60*60 and\n",
        "trip_miles > 0 and\n",
        "trip_total > 3 and\n",
        "pickup_community_area is not NULL and \n",
        "dropoff_community_area is not NULL\"\"\"\n",
        "job = client.query(query)\n",
        "df = job.to_dataframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96d61011e159"
      },
      "source": [
        "Check the fields in the data and the shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d99448551de8"
      },
      "outputs": [],
      "source": [
        "# check the dataframe's shape\n",
        "print(df.shape)\n",
        "# check the columns in the dataframe\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6a8b2bbc2ed"
      },
      "source": [
        "Check some sample data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "551f0d136f8d"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f15942c4af7"
      },
      "source": [
        "Check the dtypes of fields in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "328d4395da97"
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07e223a7261f"
      },
      "source": [
        "Check for null values in the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4ff422f9ab6"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0feadc628e4"
      },
      "source": [
        "Depending on the percentage of null values in the data, one can choose to either drop them or impute them with mean/median(for numerical values) and mode(for categorical values). In the current data, there doesn't seem to be any null values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4078b6656c7"
      },
      "source": [
        "Check the numerical distributions of the fields (numerical). In case there are any fields with constant values, those fields can be dropped as they don't add any value to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91c411c8c3d9"
      },
      "outputs": [],
      "source": [
        "df.describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eb5c84d2efe"
      },
      "source": [
        "In the current dataset, `trip_total` is the target field. To access the fields by their type easily, identify the categorical and numerical fields in the data and save them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91deacb8f533"
      },
      "outputs": [],
      "source": [
        "target = \"trip_total\"\n",
        "categ_cols = [\"payment_type\", \"pickup_community_area\", \"dropoff_community_area\"]\n",
        "num_cols = [\"trip_seconds\", \"trip_miles\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d6579050b8a"
      },
      "source": [
        "## Analyze numerical data\n",
        "<a name=\"section-5\"></a>\n",
        "\n",
        "To further anaylyze the data, there are various plots that can be used on numerical and categorical fields. In case of numerical data, one can use histograms and box-plots while bar charts are suited for categorical data to better understand the distribution of the data and the outliers in the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa2d6258b509"
      },
      "source": [
        "Plot Histograms and Box-plots on the numerical fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2066c6a1730c"
      },
      "outputs": [],
      "source": [
        "for i in num_cols + [target]:\n",
        "    _, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    df[i].plot(kind=\"hist\", bins=100, ax=ax[0])\n",
        "    ax[0].set_title(str(i) + \" -Histogram\")\n",
        "    df[i].plot(kind=\"box\", ax=ax[1])\n",
        "    ax[1].set_title(str(i) + \" -Boxplot\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3672976d67b"
      },
      "source": [
        "The field `trip_seconds` describes the time taken for the trip in seconds. Optionally, it can be converted into hours for an easier understanding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "245b8798305a"
      },
      "outputs": [],
      "source": [
        "df[\"trip_hours\"] = round(df[\"trip_seconds\"] / 3600, 2)\n",
        "df[\"trip_hours\"].plot(kind=\"box\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69980501e4cd"
      },
      "source": [
        "Similarly, another field `trip_speed` can be added by dividing `trip_miles` and `trip_hours` to understand the speed of the trip in miles/hour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc4d4e00e111"
      },
      "outputs": [],
      "source": [
        "df[\"trip_speed\"] = round(df[\"trip_miles\"] / df[\"trip_hours\"], 2)\n",
        "df[\"trip_speed\"].plot(kind=\"box\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58d57879aa8a"
      },
      "source": [
        "So far we've only considered to look at the univariate plots. To better understand the relationship between the variables, a pair-plot can be plotted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8970a559dc3"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(\n",
        "    data=df[[\"trip_seconds\", \"trip_miles\", \"trip_total\", \"trip_speed\"]].sample(10000)\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b69e8094ba39"
      },
      "source": [
        "From the box-plots and the histograms plotted so far, it is evident that there are some outliers causing skewness in the data which perhaps could be removed. Also, we can certainly see some linear relationship between the independent variables considered in the pair-plot i.e., `trip_seconds` and `trip_miles` and the dependant variable `trip_total`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2c7cfd7204c"
      },
      "source": [
        "Restrict the data based on the following conditions to remove the outliers in the data to some extent :\n",
        "- Total charge being at least more than $3.\n",
        "- Total miles driven greater than 0 and less than 300 miles.\n",
        "- Total seconds driven at least 1 minute.\n",
        "- Total hours driven not more than 2 hours.\n",
        "- Speed of the trip not being more than 70 mph.\n",
        "\n",
        "These conditions are based on some general assumptions as clearly there were some recording errors like speed being greater than 500 mph and travel-time being more than 5 hours that led to outliers in the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d011deaaec3"
      },
      "outputs": [],
      "source": [
        "# set constraints to remove outliers\n",
        "df = df[df[\"trip_total\"] > 3]\n",
        "\n",
        "df = df[(df[\"trip_miles\"] > 0) & (df[\"trip_miles\"] < 300)]\n",
        "\n",
        "df = df[df[\"trip_seconds\"] >= 60]\n",
        "\n",
        "df = df[df[\"trip_hours\"] <= 2]\n",
        "\n",
        "df = df[df[\"trip_speed\"] <= 70]\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "341b581e2155"
      },
      "source": [
        "## Analyze Categorical data\n",
        "\n",
        "Further, explore the categorical data by plotting the distribution of all the levels in each field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1c6179b8a438"
      },
      "outputs": [],
      "source": [
        "for i in categ_cols:\n",
        "    print(df[i].unique().shape)\n",
        "    df[i].value_counts(normalize=True).plot(kind=\"bar\", figsize=(10, 4))\n",
        "    plt.title(i)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a40a4b2d9d6a"
      },
      "source": [
        "From the above analysis, one can see that almost 99% of the transaction types are Cash and Credit Card. While there are also other type of transactions, their distribution is very less. In such a case, the lower distribution levels can be dropped. On the other hand, total number of pickup and dropoff community areas both seem to have the same levels which make sense. In this case also, one can choose to omit the lower distribution levels but it has to be made sure that both the fields have the same levels afterwards. In the current notebook, we'd keep them as is and proceed with the modeling.\n",
        "\n",
        "The relationships between the target variable and the categorical fields can be represented through boxplots. For each level, the corresponding distribution of the target variable can be identified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "203598ed5098"
      },
      "outputs": [],
      "source": [
        "for i in categ_cols:\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    sns.boxplot(x=i, y=target, data=df)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.title(i)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f49125a8a866"
      },
      "source": [
        "There seems to be one case where the `trip_total` is over 3000 and has the same pickup and dropoff community area i.e., 28 which is clearly an outlier compared to the rest of the points. This datapoint can be removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59cfd540ec3d"
      },
      "outputs": [],
      "source": [
        "df = df[df[\"trip_total\"] < 3000].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1315e5155505"
      },
      "source": [
        "Keep only the `Credit Card` and `Cash` payment types. Further, encode them by assigning 0 for `Credit Card` and 1 for `Cash` payment types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6a0dbe01b61"
      },
      "outputs": [],
      "source": [
        "# add payment_type\n",
        "df = df[df[\"payment_type\"].isin([\"Credit Card\", \"Cash\"])].reset_index(drop=True)\n",
        "# encode the payment types\n",
        "df[\"payment_type\"] = df[\"payment_type\"].apply(\n",
        "    lambda x: 0 if x == \"Credit Card\" else (1 if x == \"Cash\" else None)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58a1d9f0a122"
      },
      "source": [
        "There are also timestamp fields in the data that can prove to be useful. `trip_start_timestamp` represents the start timestamp of the taxi-trip and fields like what day of week it was and what hour it was can be dervied from it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c5baac285ea"
      },
      "outputs": [],
      "source": [
        "df[\"trip_start_timestamp\"] = pd.to_datetime(df[\"trip_start_timestamp\"])\n",
        "df[\"dayofweek\"] = df[\"trip_start_timestamp\"].dt.dayofweek\n",
        "df[\"hour\"] = df[\"trip_start_timestamp\"].dt.hour"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30ae02a15aa1"
      },
      "source": [
        "Since the current dataset is considered only for a week, if there isn't much variation in the newly dervied fields with respect to the target variable, they can be dropped.\n",
        "\n",
        "Plot sum and average of the `trip_total` with respect to the `dayofweek`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d51087f4221e"
      },
      "outputs": [],
      "source": [
        "# plot sum and average of trip_total w.r.t the dayofweek\n",
        "_, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "df[[\"dayofweek\", \"trip_total\"]].groupby(\"dayofweek\").trip_total.sum().plot(\n",
        "    kind=\"bar\", ax=ax[0]\n",
        ")\n",
        "ax[0].set_title(\"Sum of trip_total\")\n",
        "df[[\"dayofweek\", \"trip_total\"]].groupby(\"dayofweek\").trip_total.mean().plot(\n",
        "    kind=\"bar\", ax=ax[1]\n",
        ")\n",
        "ax[1].set_title(\"Avg. of trip_total\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea69561b95c3"
      },
      "source": [
        "Plot sum and average of the `trip_total` with respect to the `hour`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "541b65e2a39a"
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "df[[\"hour\", \"trip_total\"]].groupby(\"hour\").trip_total.sum().plot(kind=\"bar\", ax=ax[0])\n",
        "ax[0].set_title(\"Sum of trip_total\")\n",
        "df[[\"hour\", \"trip_total\"]].groupby(\"hour\").trip_total.mean().plot(kind=\"bar\", ax=ax[1])\n",
        "ax[1].set_title(\"Avg. of trip_total\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "739e985af704"
      },
      "source": [
        "As these plots don't seem to have constant figures with respect to the target variable across their levels, they can be considered for training. In fact, to simplify things these dervied features can be bucketed into less number of levels.\n",
        "\n",
        "`dayofweek` field can be bucketed into a binary field considering whether or not it was a weekend. If it is a weekday, the record can be assigned 1, else 0. Similarly, `hour` field can also be bucketed and encoded. The normal working hours in Chicago can be assumed to be between *8AM*-*10PM* and if the value falls in between the working hours, it can be encoded as 1, else 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad00e88e98bb"
      },
      "outputs": [],
      "source": [
        "# bucket and encode the dayofweek and hour\n",
        "df[\"dayofweek\"] = df[\"dayofweek\"].apply(lambda x: 0 if x in [5, 6] else 1)\n",
        "df[\"hour\"] = df[\"hour\"].apply(lambda x: 0 if x in [23, 0, 1, 2, 3, 4, 5, 6, 7] else 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "884deb0356ab"
      },
      "source": [
        "Check the data distribution before training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "597608af71e8"
      },
      "outputs": [],
      "source": [
        "df.describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe87612faa94"
      },
      "source": [
        "## Divide the data in Train and Test sets\n",
        "\n",
        "Split the preprocessed dataset into train and test sets so that the linear regression model can be validated on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e2cee9d6322"
      },
      "outputs": [],
      "source": [
        "cols = [\n",
        "    \"trip_seconds\",\n",
        "    \"trip_miles\",\n",
        "    \"payment_type\",\n",
        "    \"pickup_community_area\",\n",
        "    \"dropoff_community_area\",\n",
        "    \"dayofweek\",\n",
        "    \"hour\",\n",
        "    \"trip_speed\",\n",
        "]\n",
        "x = df[cols].copy()\n",
        "y = df[target].copy()\n",
        "\n",
        "# split the data into 75-25% ratio\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    x, y, train_size=0.75, test_size=0.25, random_state=13\n",
        ")\n",
        "X_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b7e470de1da"
      },
      "source": [
        "## Fit a Simple Linear Regression model\n",
        "<a name=\"section-6\"></a>\n",
        "\n",
        "Fit a linear regression model using Sklearn's LinearRegression method on the train data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5195bdddf71d"
      },
      "outputs": [],
      "source": [
        "# Building the regression model\n",
        "reg = LinearRegression()\n",
        "reg.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7589e2e3e31d"
      },
      "source": [
        "Print the `R2 score` and `RMSE` values for the model on train and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aa6843df9df"
      },
      "outputs": [],
      "source": [
        "# print test R2 score\n",
        "y_train_pred = reg.predict(X_train)\n",
        "train_score = r2_score(y_train, y_train_pred)\n",
        "train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
        "y_test_pred = reg.predict(X_test)\n",
        "test_score = r2_score(y_test, y_test_pred)\n",
        "test_rmse = mean_squared_error(y_test, y_test_pred, squared=False)\n",
        "print(\"Train R2-score:\", train_score, \"Train RMSE:\", train_rmse)\n",
        "print(\"Test R2-score:\", test_score, \"Test RMSE:\", test_rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ef6b44f0f93"
      },
      "source": [
        "A low RMSE error and a train and test R2 score of 0.93 suggests that the model has fitted well on the data. Further, the coefficients learned by the model for each of its independent variables can also be checked by checking the `coef_` attribute of the sklearn model. \n",
        "\n",
        "Check the coefficients learned by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "facea9070f80"
      },
      "outputs": [],
      "source": [
        "coef_df = pd.DataFrame({\"col\": cols, \"coeff\": reg.coef_})\n",
        "coef_df.set_index(\"col\").plot(kind=\"bar\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcaed0b52e60"
      },
      "source": [
        "## Save the model and upload to a GCS bucket.\n",
        "<a name=\"section-7\"></a>\n",
        "\n",
        "To deploy the model on Vertex AI, the model needs to be stored in a Cloud Storage bucket first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "264a67fd8fc7"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "from google.cloud import storage\n",
        "\n",
        "FILE_NAME = \"model.joblib\"\n",
        "joblib.dump(reg, FILE_NAME)\n",
        "\n",
        "# Upload the saved model file to Cloud Storage\n",
        "BLOB_PATH = \"taxicab_fare_prediction/\"\n",
        "\n",
        "BLOB_NAME = BLOB_PATH + FILE_NAME\n",
        "\n",
        "bucket = storage.Client().bucket(BUCKET_NAME)\n",
        "blob = bucket.blob(BLOB_NAME)\n",
        "blob.upload_from_filename(FILE_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f8ecfa6a19b"
      },
      "source": [
        "## Deploy the Model on Vertex AI with support for Vertex Explainable AI\n",
        "<a name=\"section-8\"></a>\n",
        "\n",
        "Configure the Vertex Explainable AI before deploying the model. For further details, see [Configuring Vertex Explainable AI in Vertex AI models](https://cloud.google.com/vertex-ai/docs/explainable-ai/configuring-explanations#scikit-learn-and-xgboost-pre-built-containers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dc63c440d4b"
      },
      "outputs": [],
      "source": [
        "MODEL_DISPLAY_NAME = \"taxi_fare_prediction_model\"\n",
        "ARTIFACT_GCS_PATH = f\"{BUCKET_URI}/{BLOB_PATH}\"\n",
        "\n",
        "# Feature-name(Inp_feature) and Output-name(Model_output) can be arbitrary\n",
        "exp_metadata = {\"inputs\": {\"Input_feature\": {}}, \"outputs\": {\"Predicted_taxi_fare\": {}}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49ac289687d8"
      },
      "source": [
        "Create a model resource from the uploaded model with explanation metadata configured."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ab9d32ab6a2"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "from google.cloud.aiplatform_v1.types import SampledShapleyAttribution\n",
        "from google.cloud.aiplatform_v1.types.explanation import ExplanationParameters\n",
        "\n",
        "# Create a Vertex AI model resource with support for Vertex Explainable AI\n",
        "\n",
        "aiplatform.init(project=PROJECT, location=LOCATION)\n",
        "\n",
        "model = aiplatform.Model.upload(\n",
        "    display_name=MODEL_DISPLAY_NAME,\n",
        "    artifact_uri=ARTIFACT_GCS_PATH,\n",
        "    serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-24:latest\",\n",
        "    explanation_metadata=exp_metadata,\n",
        "    explanation_parameters=ExplanationParameters(\n",
        "        sampled_shapley_attribution=SampledShapleyAttribution(path_count=25)\n",
        "    ),\n",
        ")\n",
        "\n",
        "model.wait()\n",
        "\n",
        "print(model.display_name)\n",
        "print(model.resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ed1bd9f0957"
      },
      "source": [
        "Create an Endpoint resource for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0313e9d567c6"
      },
      "outputs": [],
      "source": [
        "ENDPOINT_DISPLAY_NAME = \"taxi_fare_prediction_endpoint\"\n",
        "\n",
        "endpoint = aiplatform.Endpoint.create(\n",
        "    display_name=ENDPOINT_DISPLAY_NAME, project=PROJECT, location=LOCATION\n",
        ")\n",
        "\n",
        "print(endpoint.display_name)\n",
        "print(endpoint.resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a1b280dbec6"
      },
      "source": [
        "Save the Endpoint Id for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6516bfdd5066"
      },
      "outputs": [],
      "source": [
        "ENDPOINT_ID = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eaab1c54d66"
      },
      "source": [
        "Deploy the model to the created endpoint with the required machine-type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ea0f52526343"
      },
      "outputs": [],
      "source": [
        "DEPLOYED_MODEL_NAME = \"taxi_fare_prediction_deployment\"\n",
        "MACHINE_TYPE = \"n1-standard-2\"\n",
        "\n",
        "# deploy the model to the endpoint\n",
        "model.deploy(\n",
        "    endpoint=endpoint,\n",
        "    deployed_model_display_name=DEPLOYED_MODEL_NAME,\n",
        "    machine_type=MACHINE_TYPE,\n",
        ")\n",
        "\n",
        "model.wait()\n",
        "\n",
        "print(model.display_name)\n",
        "print(model.resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "686cfdcbaef8"
      },
      "source": [
        "Save the ID of the deployed model. The ID of the deployed model can also checked using the `endpoint.list_models()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "018f0fdb1d60"
      },
      "outputs": [],
      "source": [
        "DEPLOYED_MODEL_ID = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b751978ff665"
      },
      "source": [
        "## Get explanations from the deployed model.\n",
        "<a name=\"section-9\"></a>\n",
        "\n",
        "For testing the deployed online model, select two instances from the test data as payload."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2424e7980c08"
      },
      "outputs": [],
      "source": [
        "# format the top 2 test instances as the request's payload\n",
        "test_json = {\"instances\": [X_test.iloc[0].tolist(), X_test.iloc[1].tolist()]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01532047a99e"
      },
      "source": [
        "Call the endpoint with the payload request and parse the response for explanations. The explanations consists of attributions on the independent variables used for training the model which are based on the configured attribution method. In this case, we've used the `Sampled Shapely` method which assigns credit for the outcome to each feature, and considers different permutations of the features. This method provides a sampling approximation of exact Shapley values. Further information on the attribution methods for explantions can be found at [Overview of ExplainableAI](https://cloud.google.com/vertex-ai/docs/explainable-ai/overview) page."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6278ba230865"
      },
      "outputs": [],
      "source": [
        "features = X_train.columns.to_list()\n",
        "\n",
        "\n",
        "def plot_attributions(attrs):\n",
        "    \"\"\"\n",
        "    Function to plot the features and their attributions for an instance\n",
        "    \"\"\"\n",
        "    rows = {\"feature_name\": [], \"attribution\": []}\n",
        "    for i, val in enumerate(features):\n",
        "        rows[\"feature_name\"].append(val)\n",
        "        rows[\"attribution\"].append(attrs[\"Input_feature\"][i])\n",
        "    attr_df = pd.DataFrame(rows).set_index(\"feature_name\")\n",
        "    attr_df.plot(kind=\"bar\")\n",
        "    plt.show()\n",
        "    return\n",
        "\n",
        "\n",
        "def explain_tabular_sample(\n",
        "    project: str, location: str, endpoint_id: str, instances: list\n",
        "):\n",
        "    \"\"\"\n",
        "    Function to make an explanation request for the specified payload and generate feature attribution plots\n",
        "    \"\"\"\n",
        "    aiplatform.init(project=project, location=location)\n",
        "\n",
        "    endpoint = aiplatform.Endpoint(endpoint_id)\n",
        "\n",
        "    response = endpoint.explain(instances=instances)\n",
        "    print(\"#\" * 10 + \"Explanations\" + \"#\" * 10)\n",
        "    for explanation in response.explanations:\n",
        "        print(\" explanation\")\n",
        "        # Feature attributions.\n",
        "        attributions = explanation.attributions\n",
        "\n",
        "        for attribution in attributions:\n",
        "            print(\"  attribution\")\n",
        "            print(\"   baseline_output_value:\", attribution.baseline_output_value)\n",
        "            print(\"   instance_output_value:\", attribution.instance_output_value)\n",
        "            print(\"   output_display_name:\", attribution.output_display_name)\n",
        "            print(\"   approximation_error:\", attribution.approximation_error)\n",
        "            print(\"   output_name:\", attribution.output_name)\n",
        "            output_index = attribution.output_index\n",
        "            for output_index in output_index:\n",
        "                print(\"   output_index:\", output_index)\n",
        "\n",
        "            plot_attributions(attribution.feature_attributions)\n",
        "\n",
        "    print(\"#\" * 10 + \"Predictions\" + \"#\" * 10)\n",
        "    for prediction in response.predictions:\n",
        "        print(prediction)\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "test_json = [X_test.iloc[0].tolist(), X_test.iloc[1].tolist()]\n",
        "prediction = explain_tabular_sample(PROJECT, LOCATION, ENDPOINT_ID, test_json)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87cf259efb64"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "Since the Chicago-Taxicab dataset is continuously updating, one can preform the same kind of analysis and model training every time a new set of data is available. The date range can also be increased from a week to a month or more depending on the quality of data. Most of the steps followed in this notebook would still be valid and can be applied over the new data unless the data is too noisy. Perhaps, the notebook itself can be scheduled to run at the specified times to retrain the model using the scheduling option of the [Vertex AI workbench's Executor](https://console.cloud.google.com/vertex-ai/workbench/list/executions) feature. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eae8d94e3641"
      },
      "source": [
        "## Clean Up\n",
        "<a name=\"section-10\"></a>\n",
        "\n",
        "Delete the resources created in this notebook.\n",
        "\n",
        "Undeploy the model by specifying the `DEPLOYED_MODEL_ID`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24d89ecfdd0f"
      },
      "outputs": [],
      "source": [
        "endpoint.undeploy(deployed_model_id=DEPLOYED_MODEL_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f5b2b5ecb24"
      },
      "source": [
        "Delete the endpoint resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a1f310992ee"
      },
      "outputs": [],
      "source": [
        "endpoint.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da9a90cc72b8"
      },
      "source": [
        "Delete the model resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6d68880a444"
      },
      "outputs": [],
      "source": [
        "model.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc93e0545f73"
      },
      "source": [
        "Remove the contents of the created Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31624f07f954"
      },
      "outputs": [],
      "source": [
        "! gsutil -m rm -r $BUCKET_URI"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "chicago_taxi_fare_prediction.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aa9846856f05"
   },
   "source": [
    "# Recommender System on Retail data on Vertex-AI using PySpark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1caa36b69240"
   },
   "source": [
    "## Table Of Contents\n",
    "* [Overview](#section-1)\n",
    "* [Dataset](#section-2)\n",
    "* [Objective](#section-3)\n",
    "* [Costs](#section-4)\n",
    "* [Create a DataProc cluster with component gateway enabled and JupyterLab extension](#section-5)\n",
    "* [Connect to the cluster from the notebook](#section-6)\n",
    "* [Explore the data](#section-7)\n",
    "* [Define the ALS Model](#section-8)\n",
    "* [Evaluate the model](#section-9)\n",
    "* [Save the ALS model to GCS](#section-10)  \n",
    "* [Write the recommendations to Bigquery](#section-11)\n",
    "* [Clean up](#section-12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ec25b247838"
   },
   "source": [
    "## Overview\n",
    "<a name=\"section-1\"></a>\n",
    "\n",
    "Recommender systems are powerful tools that model existing customer behavior to generate recommendations. These models generally build complex matrices and map out existing customer preferences in order to find intersecting interests and offer recommendations. These matrices can be very large and will benefit from distributed computing and large memory pools. This is a perfect application for Vertex-AI notebooks running PySpark on DataProc cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ccfcd2355f3"
   },
   "source": [
    "## Dataset\n",
    "<a name=\"section-2\"></a>\n",
    "\n",
    "This notebook uses the following dataset in Bigquery : ```looker-private-demo.retail```. It can be accessed by pinning the ```looker-private-demo``` project in Bigquery. Instead of going to Bigquery UI, this process can be performed from the current Jupyter environment(on Vertex-AI's managed-instance) itself. Vertex-AI's managed instances support browsing through the datasets and tables from Bigquery through its **Bigquery In Notebooks** feature. \n",
    "\n",
    "<img src=\"images/Bigquery_UI_new.PNG\"></img>\n",
    "\n",
    "In this dataset, ```retail.order_items``` table will be used to train the recommendation system using PySpark. This table contains information on various orders related to the users and items(products) in the dataset.\n",
    "\n",
    "\n",
    "\n",
    "## Objective\n",
    "<a name=\"section-3\"></a>\n",
    "\n",
    "The objective of this tutorial is to build a recommendation model with [Collaborative filtering](https://en.wikipedia.org/wiki/Collaborative_filtering) approach using the interactive PySpark features offered by the managed instances on Vertex-AI. To do so, a remotely connected DataProc cluster and the <a href=\"http://dl.acm.org/citation.cfm?id=1608614\"><b>Alternating Least Squares(ALS)</b></a> method implemented in the PySpark's MLlib library are used.\n",
    "\n",
    "The steps performed in this notebook are as follows :\n",
    "1. Remotely connect to a Dataproc cluster with Pyspark and JupyterLab from the notebook.\n",
    "2. Exploring the dataset in Bigquery from the notebook.\n",
    "3. Preprocessing the data.\n",
    "4. Training a PySpark ALS model on the data.\n",
    "5. Evaluating the ALS model.\n",
    "6. Generating recommendations.\n",
    "7. Saving the recommendations to a Bigquery using Pyspark-Bigquery connector.\n",
    "8. Saving the ALS model to a Cloud Storage Bucket.\n",
    "9. Cleaning up the resources.\n",
    "\n",
    "\n",
    "## Costs\n",
    "<a name=\"section-4\"></a>\n",
    "This tutorial uses the following billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Dataproc\n",
    "* Bigquery\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing), [Dataproc pricing](https://cloud.google.com/dataproc/pricing), [Bigquery\n",
    "pricing](https://cloud.google.com/bigquery/pricing) and [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing) and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Otherwise*, set your project ID here. *italicized text*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append it onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "\n",
    "{TODO: Adjust wording in the first paragraph to fit your use case - explain how your tutorial uses the Cloud Storage bucket. The example below shows how Vertex AI uses the bucket for training.}\n",
    "\n",
    "When you submit a training job using the Cloud SDK, you upload a Python package\n",
    "containing your training code to a Cloud Storage bucket. Vertex AI runs\n",
    "the code from this package. In this tutorial, Vertex AI also saves the\n",
    "trained model that results from your job in the same bucket. Using this model artifact, you can then\n",
    "create Vertex AI model and endpoint resources in order to serve\n",
    "online predictions.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
    "Cloud Storage buckets.\n",
    "\n",
    "You may also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook. Make sure to [choose a region where Vertex AI services are\n",
    "available](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions). You may\n",
    "not use a Multi-Regional Storage bucket for training with Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "REGION = \"[your-region]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally**, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ef78f6261a0",
    "tags": []
   },
   "source": [
    "## Before you begin\n",
    "\n",
    "As the ALS model approach is compute intensive and could take a lot of time to train on a regular notebook environment, a Dataproc cluster with PySpark environemnt is used in this notebook. The steps taken to connect to the cluster are as follows : \n",
    "\n",
    "\n",
    "## Create a DataProc cluster with component gateway enabled and JupyterLab extension\n",
    "<a name=\"section-5\"></a>\n",
    "The following gcloud command can be executed to create the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "950554272656"
   },
   "outputs": [],
   "source": [
    "CLUSTER_NAME = \"[your-cluster-name]\"\n",
    "CLUSTER_REGION = \"[your-cluster-region]\"\n",
    "CLUSTER_ZONE = \"[your-cluster-zone]\"\n",
    "MACHINE_TYPE = \"[your=machine-type]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1e3e719e27f6"
   },
   "outputs": [],
   "source": [
    "! gcloud dataproc clusters create $CLUSTER_NAME \\\n",
    "--enable-component-gateway \\\n",
    "--region $CLUSTER_REGION \\\n",
    "--zone $CLUSTER_ZONE \\\n",
    "--single-node \\\n",
    "--master-machine-type $MACHINE_TYPE \\\n",
    "--master-boot-disk-size 100 \\\n",
    "--image-version 2.0-debian10 \\\n",
    "--optional-components JUPYTER \\\n",
    "--project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2961b0a96176"
   },
   "source": [
    "Alternatively, the cluster can be created through the Dataproc console as well. Additional settings like network configuratons, service-accounts etc. can be configured there if required. Among all the settings while configuring the cluster, the following steps are needed to be followed for this tutorial :\n",
    "\n",
    "- Provide a name for the cluster.\n",
    "- Select a region and zone for the cluster.\n",
    "- Select the cluster type as single-node. For small and proof-of-concept usecases, it is recommended to select a single-node cluster. If needed, other types can also be selected based on the requirement.\n",
    "- Enable Component gateway.\n",
    "- In the optional components, select Jupyter Notebook.\n",
    "- (optional) Select the machine-type (preferably high-mem machine type).\n",
    "- Create the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b864144904fb"
   },
   "source": [
    "## Connect to the cluster from the notebook\n",
    "<a name=\"section-6\"></a>\n",
    "When the created Dataproc cluste is running, the corresponding runtime can be remotely set as a kernel in the notebook. The created cluster's name will appear in the list of kernels that can be selected for this notebook. Select the Python3 kernel on the remote cluster environment for this tutorial.\n",
    "\n",
    "<img src=\"images/cluster_kernel_selection.png\"></img>\n",
    "\n",
    "***Note :**\n",
    "- The remote kernels might take some time(~3min.) to show up in the list of kernels.\n",
    "- PySpark code in this tutorial can be run on either PySpark or Python3 kernel on the remote Dataproc cluster. But to run the optional part where the recommendations are saved to a Bigquery table using a Bigquery connector, it is recommended that Python3 kernel(remote) is selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fc8d6f0cc21b"
   },
   "source": [
    "## Tutorial "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03c8dc1024f7"
   },
   "source": [
    "## Explore the data\n",
    "<a name=\"section-7\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffc1113d6504"
   },
   "source": [
    "Vertex-AI's managed-instances provide us the ability to explore the Bigquery content from its **Bigquery In Notebooks** pane on left. This feature allows us to look at the meta-data and preview of the table-content, query the table and also get a description of the data in the tables.\n",
    "\n",
    "<img src=\"images/BQ_view_table_new.PNG\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fb418e82451"
   },
   "source": [
    "Check the distribution of the STATUS field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfebfe18e127"
   },
   "source": [
    "#@bigquery\n",
    "SELECT STATUS, COUNT(*) order_count FROM looker-private-demo.retail.order_items GROUP BY 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dd73d32bc80"
   },
   "source": [
    "Join the ```order_items``` table with the ```inventory_items``` table from the same dataset to retrieve the PRODUCT_IDs for the orders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4abc4fb13e25"
   },
   "source": [
    "#@bigquery\n",
    "WITH user_prod_table AS (\n",
    "SELECT USER_ID, PRODUCT_ID, STATUS FROM looker-private-demo.retail.order_items AS a\n",
    "join\n",
    "(SELECT ID, PRODUCT_ID FROM looker-private-demo.retail.inventory_items) AS b\n",
    "on a.inventory_item_id = b.ID )\n",
    "\n",
    "SELECT USER_ID, PRODUCT_ID, STATUS from user_prod_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5072aded172b"
   },
   "source": [
    "To fetch the data into into the current environment as a dataframe, press the **Query and load as DataFrame** button and execute the generated code stub.\n",
    "\n",
    "*Note : By default the data is loaded into \"df\" variable and it could be changed before executing the cell if required.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ba1a54b04d9a"
   },
   "outputs": [],
   "source": [
    "# The following two lines are only necessary to run once.\n",
    "# Comment out otherwise for speed-up.\n",
    "from google.cloud.bigquery import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "query = \"\"\"WITH user_prod_table AS (\n",
    "SELECT USER_ID, PRODUCT_ID, STATUS FROM looker-private-demo.retail.order_items AS a\n",
    "join\n",
    "(SELECT ID, PRODUCT_ID FROM looker-private-demo.retail.inventory_items) AS b\n",
    "on a.inventory_item_id = b.ID )\n",
    "\n",
    "SELECT USER_ID, PRODUCT_ID, STATUS from user_prod_table\"\"\"\n",
    "job = client.query(query)\n",
    "df = job.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acb1077cbf1e"
   },
   "source": [
    "### Preprocess the Data\n",
    "\n",
    "To run PySpark's ALS method on the existing data, it is necessary that there are some fields to quantify the relation between the USER_IDs and the PRODUCT_IDs like for example *ratings given by the user*. If such fields alredy exist in the data, they can be treated as an *explicit feedback* for the ALS model. Otherwise, the fields indicative of such relation can be given as an *implicit feedback*. More about the feedbacks for PySpark's ALS method can be checked [here](https://spark.apache.org/docs/2.2.0/ml-collaborative-filtering.html#explicit-vs-implicit-feedback).\n",
    "\n",
    "In the current dataset, as there are no such numerical fileds, the STATUS field is further used to quantify the association between a USER_ID and  a PRODUCT_ID. Based on when they occur during an order lifecycle and how likely the user is going to like the order, the STATUS field is quantified by assigning the following ratings : \n",
    "- Cancelled - 1\n",
    "- Returned - 2\n",
    "- Processing - 3\n",
    "- Shipped - 4\n",
    "- Complete - 5\n",
    "\n",
    "These ratings given are subjective and can be modified according to the usecase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "365dca51641f"
   },
   "outputs": [],
   "source": [
    "score_mapping = {\n",
    "    \"Cancelled\": 1,\n",
    "    \"Returned\": 2,\n",
    "    \"Processing\": 3,\n",
    "    \"Shipped\": 4,\n",
    "    \"Complete\": 5,\n",
    "}\n",
    "df[\"RATING\"] = df[\"STATUS\"].map(score_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a88eb70c6dec"
   },
   "source": [
    "Check the distribution of the newly generated RATING field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cfe29a1f9f16"
   },
   "outputs": [],
   "source": [
    "df[\"RATING\"].plot(kind=\"hist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "367bc4912977"
   },
   "source": [
    "Load the required methods and classes from PySpark MLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b674b037dce0"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1c3c74870bcf"
   },
   "source": [
    "Generate a Spark session with Bigquery-Spark connector configured.\n",
    "\n",
    "*Note : If the notebook is connected to a Dataproc cluster, the session object would show \"yarn\" as the Master.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2e55770bb894"
   },
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"Recommendations\")\n",
    "    .config(\"spark.jars\", \"gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89b6a2146b7c"
   },
   "source": [
    "Convert the pandas dataframe to a spark dataframe for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0d1d5b5e6a56"
   },
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(df[[\"USER_ID\", \"PRODUCT_ID\", \"RATING\"]])\n",
    "spark_df.printSchema()\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5b159af5c085"
   },
   "source": [
    "### Split the data into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "951e7c2bf8fe"
   },
   "outputs": [],
   "source": [
    "(train, test) = spark_df.randomSplit([0.8, 0.2], seed=36)\n",
    "train.count(), test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dea0589dd474"
   },
   "source": [
    "## Define the ALS Model\n",
    "<a name=\"section-8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99e0365f32be"
   },
   "source": [
    "The PySpark ALS recommender, Alternating Least Squares, is a matrix factorization algorithm. The idea is to build a matrix that maps users to actions. The actions can be reviews, purchases, various options taken, etc. Due to the complexity and size of the matrix, PySpark can run the algorithm in parallel.\n",
    "\n",
    "ALS will attempt to estimate the rating matrix R as the product of two lower-rank matrices, X and Y. Typically these approximations are called ‘factor’ matrices. During each iteration, one of the factor matrices is held constant, while the other is solved for using least squares. The newly-solved factor matrix is then held constant while solving for the other factor matrix.\n",
    "\n",
    "PySpark uses a blocked implementation of the ALS factorization algorithm that groups the two sets of factors (referred to as “users” and “products”) into blocks and reduces communication by only sending one copy of each user vector to each product block on each iteration, and only for the product blocks that need that user’s feature vector.\n",
    "\n",
    "Essentially instead of finding the low-rank approximations to the rating matrix R, this finds the approximations for a preference matrix P where the elements of P are 1 if r > 0 and 0 if r <= 0. The ratings then act as ‘confidence’ values related to the strength of indicated user preferences rather than explicit ratings given to items (more information about PySpark's ALS can be found [here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.recommendation.ALS.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b47e5bfe0c30"
   },
   "outputs": [],
   "source": [
    "als = ALS(\n",
    "    userCol=\"USER_ID\",\n",
    "    itemCol=\"PRODUCT_ID\",\n",
    "    ratingCol=\"RATING\",\n",
    "    nonnegative=True,\n",
    "    implicitPrefs=False,\n",
    "    coldStartStrategy=\"drop\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9726f220acb5"
   },
   "source": [
    "ALS model tries to predict the ratings between users and items and so RMSE can be used for evaluating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50ae46e7db7c"
   },
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\", labelCol=\"RATING\", predictionCol=\"prediction\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01d3b2a2a801"
   },
   "source": [
    "Define a hyperparameter grid for Cross-Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0397bf82ba6"
   },
   "outputs": [],
   "source": [
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(als.rank, [10, 50])\n",
    "    .addGrid(als.regParam, [0.01, 0.1, 0.2])\n",
    "    .build()\n",
    ")\n",
    "print(\"No. of settings to be tested: \", len(param_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98f3c23ff0a1"
   },
   "source": [
    "Perform Crossvalidation and save the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "13617284cdb3"
   },
   "outputs": [],
   "source": [
    "cv = CrossValidator(\n",
    "    estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=3\n",
    ")\n",
    "model = cv.fit(train)\n",
    "best_model = model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66bc83eb4adb"
   },
   "outputs": [],
   "source": [
    "print(\"##Parameters for the Best Model##\")\n",
    "print(\"Rank:\", best_model._java_obj.parent().getRank())\n",
    "print(\"MaxIter:\", best_model._java_obj.parent().getMaxIter())\n",
    "print(\"RegParam:\", best_model._java_obj.parent().getRegParam())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12a02668017e"
   },
   "source": [
    "## Evaluate the model\n",
    "<a name=\"section-9\"></a>\n",
    "Evaluate the model by computing the RMSE on the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04d7e30661fa"
   },
   "outputs": [],
   "source": [
    "# View the rating predictions by the model on train and test sets\n",
    "train_predictions = best_model.transform(train)\n",
    "train_RMSE = evaluator.evaluate(train_predictions)\n",
    "\n",
    "test_predictions = best_model.transform(test)\n",
    "test_RMSE = evaluator.evaluate(test_predictions)\n",
    "\n",
    "print(\"Train RMSE \", train_RMSE)\n",
    "print(\"Test RMSE \", test_RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "521dc4a5a688"
   },
   "source": [
    "### Generate recommendations for all users\n",
    "\n",
    "Required number of recommendations for the users can be generated using the ALS model's *recommendForAllUsers()* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c7ddfd001900"
   },
   "outputs": [],
   "source": [
    "# Generate 10 product recommendations for all users\n",
    "nrecommendations = best_model.recommendForAllUsers(10)\n",
    "nrecommendations.limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "219e986f5c23"
   },
   "source": [
    "### Generate recommendations for a specific user\n",
    "\n",
    "The earlier step already generated and stored the specified number of product recommendations for all users in *nrecommendations* dataframe object. To obtain recommendations for a single user, this dataframe object can be queried simply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "782b17eb3621"
   },
   "outputs": [],
   "source": [
    "# get product recommendations for the selected user (USER_ID = 1)\n",
    "nrecommendations.where(nrecommendations.USER_ID == 1).select(\n",
    "    \"recommendations.PRODUCT_ID\", \"recommendations.rating\"\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "964c4d610d31"
   },
   "source": [
    "## Save the ALS model to GCS (optional)\n",
    "<a name=\"section-10\"></a>\n",
    "Pyspark's ALS.save() method will create a folder at the specified path where it saves the trained model. With GCS file browser available in the managed-instance environment, this method can directly save the model to a GCS bucket.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73db311320d0"
   },
   "source": [
    "Use the ALS object's *.save()* function to write the model to the GCS bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9f910addca92"
   },
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "GCS_MODEL_PATH = \"gs://\"+BUCKET_NAME+\"/recommender_systems/\"\n",
    "best_model.save(GCS_MODEL_PATH + \"rcmd_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "665a05361fbd"
   },
   "source": [
    "## Write the recommendations to Bigquery (optional)\n",
    "<a name=\"section-11\"></a>\n",
    "In order to serve the recommendations to the end-users or any applications, the output from the *recommendForAllUsers()* method can be saved to a Bigquery table using Spark's Bigquery connector.\n",
    "\n",
    "\n",
    "### Create a Dataset in Bigquery\n",
    "The following cell creates a new dataset in Bigquery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22e136731378"
   },
   "source": [
    "#@bigquery\n",
    "-- create a dataset in Bigquery\n",
    "CREATE SCHEMA recommender_sys\n",
    "OPTIONS(\n",
    "  location=\"us\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4910f961001f"
   },
   "source": [
    "### Write the Recommendations to Bigquery\n",
    "\n",
    "PySpark's Bigquery-connector requires two necessary fields i.e., *Bigquery Table name* and a *GCS path to write the temporary files* while saving the model. These two fields can be provided accordingly while writing the recommendations to Bigquery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63b76c37123d"
   },
   "outputs": [],
   "source": [
    "DATASET = \"[your-dataset-name]\"\n",
    "TABLE = \"[your-bigquery-table-name]\"\n",
    "GCS_TEMP_PATH = \"[your-gcs-path]\"\n",
    "\n",
    "nrecommendations.write.format(\"bigquery\").option(\n",
    "    \"table\", \"{}.{}\".format(DATASET, TABLE)\n",
    ").option(\"temporaryGcsBucket\", GCS_TEMP_PATH).mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6c3a6d501600"
   },
   "source": [
    "## Clean up\n",
    "<a name=\"section-12\"></a>\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72488a2999a6"
   },
   "outputs": [],
   "source": [
    "# remove the Bigquery dataset created for storing the recommendations and all of its tables\n",
    "! bq rm -r -f -d $PROJECT:$DATASET\n",
    "\n",
    "# remove the GCS bucket created and all of its tables\n",
    "! gsutil rm -r gs://$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75cfc772a3f4"
   },
   "outputs": [],
   "source": [
    "# delete the created Dataproc cluster\n",
    "! gcloud dataproc clusters delete $CLUSTER_NAME --region=$CLUSTER_REGION"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Recommender-system_notebook.ipynb",
   "toc_visible": true
  },
  "environment": {
   "name": "managed-notebooks.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/managed-notebooks:m80"
  },
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1cc1c1fa076"
      },
      "source": [
        "# Pricing Optimization \n",
        "## Table of contents\n",
        "* [Overview](#section-1)\n",
        "* [Dataset](#section-2)\n",
        "* [Objective](#section-3)\n",
        "* [Costs](#section-4)\n",
        "* [Create a BigQuery dataset](#section-5)\n",
        "* [Load the dataset from Cloud Storage](#section-6)\n",
        "* [Data Analysis](#section-7)\n",
        "* [Preprocess the data for training](#section-8)\n",
        "* [Train the model using BigQuery ML](#section-9)\n",
        "* [Generate forecasts from the model](#section-10)\n",
        "* [Interpret the results to choose the best price](#section-11)\n",
        "* [Clean Up](#section-12)\n",
        "\n",
        "\n",
        "## Overview\n",
        "<a name=\"section-1\"></a>\n",
        "This notebook demonstrates analysis of pricing optimization on [CDM Pricing Data](https://github.com/trifacta/trifacta-google-cloud/tree/main/design-pattern-pricing-optimization) and automating the workflow using Vertex AI Workbench's managed notebooks.\n",
        "\n",
        "<b>Note</b>: This notebook is designed to run on managed notebooks instance of Vertex AI Workbench. Some components of this notebook may not work in other notebook environments.\n",
        "\n",
        "## Dataset\n",
        "<a name=\"section-2\"></a>\n",
        "The dataset used in this notebook is a part of the [CDM Pricing Data](https://github.com/trifacta/trifacta-google-cloud/blob/main/design-pattern-pricing-optimization/CDM_Pricing_large_table.csv) which consists of products sales information on specified dates.\n",
        "\n",
        "## Objective\n",
        "<a name=\"section-3\"></a>\n",
        "The objective of this notebook is to build a pricing optimization model using Vertex AI on GCP. The following steps have been followed in this usecase :  \n",
        "\n",
        "- Load the required dataset from a Cloud Storage bucket.\n",
        "- Analyze the fields present in the dataset.\n",
        "- Process the data to build a model.\n",
        "- Build a BigQuery ML forecast model on the processed data.\n",
        "- Get forecasted values from the BigQuery ML model.\n",
        "- Interpret the forecasts to identify best prices.\n",
        "- Clean up.\n",
        "\n",
        "\n",
        "## Costs\n",
        "<a name=\"section-4\"></a>\n",
        "This tutorial uses the following billable components of Google Cloud:\n",
        "\n",
        "- Vertex AI\n",
        "- Bigquery\n",
        "- Cloud Storage\n",
        "\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing), [Bigquery pricing](https://cloud.google.com/bigquery/pricing) and [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ed1f5e85640"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3f30148b66d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"\"\n",
        "\n",
        "# Get your Google Cloud project ID from gcloud\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID: \", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "750bf2883c2d"
      },
      "source": [
        "Otherwise, set your project ID here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c6db1ca88b9"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
        "    PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a1c270c7d34"
      },
      "source": [
        "### Import the required libraries and define constants\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acc6fac1fa55"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from google.cloud import bigquery\n",
        "from google.cloud.bigquery import Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a06006dff8f9"
      },
      "outputs": [],
      "source": [
        "DATASET = \"[your-bigquery-dataset-id]\"  # set the Bigquery dataset-id\n",
        "TRAINING_DATA_TABLE = \"[your-bigquery-table-id-to-store-the-training-data]\"  # set the Bigquery table-id to store the training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "016c3d47cc69"
      },
      "source": [
        "## Create a BigQuery dataset\n",
        "<a name=\"section-5\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12ccd8d7956e"
      },
      "source": [
        "#@bigquery\n",
        "-- create a dataset in BigQuery\n",
        "\n",
        "CREATE SCHEMA pricing_optimization\n",
        "OPTIONS(\n",
        "  location=\"us\"\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c106b978a79b"
      },
      "source": [
        "## Load the dataset from Cloud Storage\n",
        "<a name=\"section-6\"></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aeae9da9796"
      },
      "outputs": [],
      "source": [
        "DATA_LOCATION = \"gs://cloud-samples-data/ai-platform-unified/datasets/tabular/cdm_pricing_large_table.csv\"\n",
        "df = pd.read_csv(DATA_LOCATION)\n",
        "print(df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b98d5f09842"
      },
      "source": [
        "We will build a forecast model on this data and thus determine the best price for a product. For this type of model, we may not be using many fields but just the sales and price related ones. For the current execrcise, we will just focus on the following fields :\n",
        "- `Product_ID`\n",
        "- `Customer_Hierarchy`\n",
        "- `Fiscal_Date`\n",
        "- `List_Price_Converged`\n",
        "- `Invoiced_quantity_in_Pieces`\n",
        "- `Net_Sales`\n",
        "\n",
        "\n",
        "\n",
        "## Data Analysis\n",
        "<a name=\"section-7\"></a>\n",
        "\n",
        "First, we will explore the data and distributions.\n",
        "\n",
        "Select the required columns from the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af4b41c5eb1f"
      },
      "outputs": [],
      "source": [
        "id_col = \"Product_ID\"\n",
        "date_col = \"Fiscal_Date\"\n",
        "categ_cols = [\"Customer_Hierarchy\"]\n",
        "num_cols = [\"List_Price_Converged\", \"Invoiced_quantity_in_Pieces\", \"Net_Sales\"]\n",
        "\n",
        "df = df[[id_col, date_col] + categ_cols + num_cols].copy()\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d780043ee5b"
      },
      "source": [
        "Check the column types and null values in the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f54c445a1288"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd817b414c4d"
      },
      "source": [
        "This data description reveals that there are no null values in the data. Also, the field `Fiscal_Date` which is a date field is loaded as an object type. \n",
        "\n",
        "Change the type of the date field to datetime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b160fac085c8"
      },
      "outputs": [],
      "source": [
        "df[\"Fiscal_Date\"] = pd.to_datetime(df[\"Fiscal_Date\"], infer_datetime_format=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb4778578064"
      },
      "source": [
        "Plot the distributions for the categorical fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd0467cd57c3"
      },
      "outputs": [],
      "source": [
        "for i in categ_cols:\n",
        "    df[i].value_counts(normalize=True).plot(kind=\"bar\")\n",
        "    plt.title(i)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "145deed255e0"
      },
      "source": [
        "Plot the distributions for the numerical fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f934137c6d82"
      },
      "outputs": [],
      "source": [
        "for i in num_cols:\n",
        "    _, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "    df[i].plot(kind=\"box\", ax=ax[0])\n",
        "    df[i].plot(kind=\"hist\", ax=ax[1])\n",
        "    ax[0].set_title(i + \"-Boxplot\")\n",
        "    ax[1].set_title(i + \"-Histogram\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9b9c2e58380"
      },
      "source": [
        "Check maximum date and minimum date in Fiscal_Date column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2a10aa689f9d"
      },
      "outputs": [],
      "source": [
        "print(df[\"Fiscal_Date\"].max())\n",
        "print(df[\"Fiscal_Date\"].min())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4834f63e2e59"
      },
      "source": [
        "Check the product distribution across each category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4664877f5304"
      },
      "outputs": [],
      "source": [
        "grp_cols = [\"Customer_Hierarchy\", \"Product_ID\"]\n",
        "grp_df = df[grp_cols].groupby(by=grp_cols).count().reset_index()\n",
        "grp_df.groupby(\"Customer_Hierarchy\").nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01ed02b9c8fd"
      },
      "source": [
        "Check the percentage changes in the orders based on the percentage changes in the price."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0b2c428cb135"
      },
      "outputs": [],
      "source": [
        "# aggregate the data\n",
        "df_aggr = (\n",
        "    df.groupby([\"Product_ID\", \"List_Price_Converged\"])\n",
        "    .agg({\"Fiscal_Date\": min, \"Invoiced_quantity_in_Pieces\": sum, \"Net_Sales\": sum})\n",
        "    .reset_index()\n",
        ")\n",
        "# rename the aggregated columns\n",
        "df_aggr.rename(\n",
        "    columns={\n",
        "        \"Fiscal_Date\": \"First_price_date\",\n",
        "        \"Invoiced_quantity_in_Pieces\": \"Total_ordered_pieces\",\n",
        "        \"Net_Sales\": \"Total_net_sales\",\n",
        "    },\n",
        "    inplace=True,\n",
        ")\n",
        "\n",
        "# sort values chronologically\n",
        "df_aggr.sort_values(by=[\"Product_ID\", \"First_price_date\"], inplace=True)\n",
        "df_aggr.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# add columns for previous values\n",
        "df_aggr[\"Previous_List\"] = df_aggr.groupby([\"Product_ID\"])[\n",
        "    \"List_Price_Converged\"\n",
        "].shift()\n",
        "df_aggr[\"Previous_Total_ordered_pieces\"] = df_aggr.groupby([\"Product_ID\"])[\n",
        "    \"Total_ordered_pieces\"\n",
        "].shift()\n",
        "\n",
        "# average price change across sku's\n",
        "df_aggr[\"price_change_perc\"] = (\n",
        "    (df_aggr[\"List_Price_Converged\"] - df_aggr[\"Previous_List\"])\n",
        "    / df_aggr[\"Previous_List\"].fillna(0)\n",
        "    * 100\n",
        ")\n",
        "df_aggr[\"order_change_perc\"] = (\n",
        "    (df_aggr[\"Total_ordered_pieces\"] - df_aggr[\"Previous_Total_ordered_pieces\"])\n",
        "    / df_aggr[\"Previous_Total_ordered_pieces\"].fillna(0)\n",
        "    * 100\n",
        ")\n",
        "\n",
        "# plot a scatterplot to visualize the changes\n",
        "sns.scatterplot(\n",
        "    x=\"price_change_perc\",\n",
        "    y=\"order_change_perc\",\n",
        "    data=df_aggr,\n",
        "    hue=\"Product_ID\",\n",
        "    legend=False,\n",
        ")\n",
        "plt.title(\"Percentage of change in price vs order\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8259e916fe25"
      },
      "source": [
        "For most of the products, we see that the percentage change in orders are high where the percentage changes in the prices are low. This suggests that too much change in the prices can affect the number of orders. \n",
        "\n",
        "**Note**: There seem to be some outliers in the data as percentage changes greater than 800 are found and as evident from the box plots made earlier. In the current exercise, we will not take any manual measures to deal with outliers as we will create a BigQuery ML timeseries model that already deals with outliers.\n",
        "\n",
        "## Preprocess the data for training\n",
        "<a name=\"section-8\"></a>\n",
        "\n",
        "Check which `Product_ID`s that have the maximum orders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5cbc7709c6a"
      },
      "outputs": [],
      "source": [
        "df_orders = df.groupby([\"Product_ID\", \"Customer_Hierarchy\"], as_index=False)[\n",
        "    \"Invoiced_quantity_in_Pieces\"\n",
        "].sum()\n",
        "df_orders.loc[\n",
        "    df_orders.groupby(\"Customer_Hierarchy\")[\"Invoiced_quantity_in_Pieces\"].idxmax()\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd6d227e513e"
      },
      "source": [
        "From the above result, we can infer the following :\n",
        "\n",
        "- Under **Food** category, **SKU 62** has maximum orders.\n",
        "- Under **Manufacturing** category, **SKU 17** has maximum orders.\n",
        "- Under **Paper** category, **SKU 107** has maximum orders.\n",
        "- Under **Publishing** category, **SKU 8** has maximum orders.\n",
        "- Under **Utilities** category, **SKU 140** has maximum orders.\n",
        "\n",
        "Given there are too many ids and only a few records for most of them, we will consider only the above `Product_ID`s for which there are maximum number of orders. \n",
        "\n",
        "**Note**: The `Invoiced_quantity_in_Pieces` field seem to be a *float* type rather than an *int* type as it should be. This could be probably because of the data itself might be averaged in the first place."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dbc0d64d157"
      },
      "source": [
        "Check the various prices available for these `Product_ID`s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acc1dbd2d838"
      },
      "outputs": [],
      "source": [
        "df_type_food = df[(df[\"Product_ID\"] == \"SKU 62\") & (df[\"Customer_Hierarchy\"] == \"Food\")]\n",
        "print(\"Food :\")\n",
        "print(df_type_food[\"List_Price_Converged\"].value_counts())\n",
        "df_type_manuf = df[\n",
        "    (df[\"Product_ID\"] == \"SKU 17\") & (df[\"Customer_Hierarchy\"] == \"Manufacturing\")\n",
        "]\n",
        "print(\"Manufacturing :\")\n",
        "print(df_type_manuf[\"List_Price_Converged\"].value_counts())\n",
        "df_type_paper = df[\n",
        "    (df[\"Product_ID\"] == \"SKU 107\") & (df[\"Customer_Hierarchy\"] == \"Paper\")\n",
        "]\n",
        "print(\"Paper :\")\n",
        "print(df_type_paper[\"List_Price_Converged\"].value_counts())\n",
        "df_type_pub = df[\n",
        "    (df[\"Product_ID\"] == \"SKU 8\") & (df[\"Customer_Hierarchy\"] == \"Publishing\")\n",
        "]\n",
        "print(\"Publishing :\")\n",
        "print(df_type_pub[\"List_Price_Converged\"].value_counts())\n",
        "df_type_util = df[\n",
        "    (df[\"Product_ID\"] == \"SKU 140\") & (df[\"Customer_Hierarchy\"] == \"Utilities\")\n",
        "]\n",
        "print(\"Utilities :\")\n",
        "print(df_type_util[\"List_Price_Converged\"].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f023af578c0f"
      },
      "source": [
        "In the publishing category, `Product_ID` `SKU 8` and `SKU 17` has less than or equal to two different prices in the entire data and so we will exclude them and consider the rest for building the forecast model. The idea here is to train a forecast model on the timeseries data for products with different prices.\n",
        "\n",
        "Join the data for all the `Product_ID`s into one dataframe and remove duplicate records."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a44771cc4c20"
      },
      "outputs": [],
      "source": [
        "df_final = pd.concat([df_type_food, df_type_paper, df_type_util])\n",
        "df_final = (\n",
        "    df_final[\n",
        "        [\n",
        "            \"Product_ID\",\n",
        "            \"Fiscal_Date\",\n",
        "            \"Customer_Hierarchy\",\n",
        "            \"List_Price_Converged\",\n",
        "            \"Invoiced_quantity_in_Pieces\",\n",
        "        ]\n",
        "    ]\n",
        "    .drop_duplicates()\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "df_final.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "add5063df368"
      },
      "source": [
        "Save the data to a BigQuery table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd82ba56571f"
      },
      "outputs": [],
      "source": [
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    # Specify a (partial) schema. All columns are always written to the\n",
        "    # table. The schema is used to assist in data type definitions.\n",
        "    schema=[\n",
        "        bigquery.SchemaField(\"Product_ID\", bigquery.enums.SqlTypeNames.STRING),\n",
        "        bigquery.SchemaField(\"Fiscal_Date\", bigquery.enums.SqlTypeNames.DATE),\n",
        "        bigquery.SchemaField(\"List_Price_Converged\", bigquery.enums.SqlTypeNames.FLOAT),\n",
        "        bigquery.SchemaField(\n",
        "            \"Invoiced_quantity_in_Pieces\", bigquery.enums.SqlTypeNames.FLOAT\n",
        "        ),\n",
        "    ],\n",
        "    # Optionally, set the write disposition. BigQuery appends loaded rows\n",
        "    # to an existing table by default, but with WRITE_TRUNCATE write\n",
        "    # disposition it replaces the table with the loaded data.\n",
        "    write_disposition=\"WRITE_TRUNCATE\",\n",
        ")\n",
        "\n",
        "# save the dataframe to a table in the created dataset\n",
        "job = bq_client.load_table_from_dataframe(\n",
        "    df_final,\n",
        "    \"{}.{}.{}\".format(PROJECT_ID, DATASET, TRAINING_DATA_TABLE),\n",
        "    job_config=job_config,\n",
        ")  # Make an API request.\n",
        "job.result()  # Wait for the job to complete."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fca77641b03b"
      },
      "source": [
        "# Train the model using BigQuery ML\n",
        "<a name=\"section-9\"></a>\n",
        "\n",
        "Train an [Arima-Plus](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create-time-series) model on the data using BigQuery ML."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cded27507891"
      },
      "source": [
        "#@bigquery\n",
        "create or replace model pricing_optimization.bqml_arima\n",
        "options\n",
        " (model_type = 'ARIMA_PLUS',\n",
        "  time_series_timestamp_col = 'Fiscal_Date',\n",
        "  time_series_data_col = 'Invoiced_quantity_in_Pieces',\n",
        "  time_series_id_col = 'ID'\n",
        " ) as\n",
        "select\n",
        " Fiscal_Date,\n",
        " Concat(Product_ID,\"_\" ,Cast(List_Price_Converged as string)) as ID,\n",
        " Invoiced_quantity_in_Pieces\n",
        "from\n",
        " pricing_optimization.TRAINING_DATA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "332fd11ff32b"
      },
      "source": [
        "## Generate forecasts from the model\n",
        "<a name=\"section-10\"></a>\n",
        "\n",
        "Predict the sales for the next 30 days for each id and save to a dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef926cdbf28e"
      },
      "outputs": [],
      "source": [
        "client = Client()\n",
        "\n",
        "query = '''\n",
        "DECLARE HORIZON STRING DEFAULT \"30\"; #number of values to forecast\n",
        "DECLARE CONFIDENCE_LEVEL STRING DEFAULT \"0.90\"; ## required confidence level\n",
        "\n",
        "EXECUTE IMMEDIATE format(\"\"\"\n",
        "    SELECT\n",
        "      *\n",
        "    FROM \n",
        "      ML.FORECAST(MODEL pricing_optimization.bqml_arima, \n",
        "                  STRUCT(%s AS horizon, \n",
        "                         %s AS confidence_level)\n",
        "                 )\n",
        "    \"\"\",HORIZON,CONFIDENCE_LEVEL)'''\n",
        "job = client.query(query)\n",
        "dfforecast = job.to_dataframe()\n",
        "dfforecast.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "608c7de72dae"
      },
      "source": [
        "## Interpret the results to choose the best price\n",
        "<a name=\"section-11\"></a>\n",
        "\n",
        "Calculate average forecast values for the forecast duration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1e193680400"
      },
      "outputs": [],
      "source": [
        "dfforecast_avg = (\n",
        "    dfforecast[[\"ID\", \"forecast_value\"]].groupby(\"ID\", as_index=False).mean()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ce395d652a3"
      },
      "source": [
        "Extract the ID and Price fields from the ID field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "452c56fa58ed"
      },
      "outputs": [],
      "source": [
        "dfforecast_avg[\"Product_ID\"] = dfforecast_avg[\"ID\"].apply(lambda x: x.split(\"_\")[0])\n",
        "dfforecast_avg[\"Price\"] = dfforecast_avg[\"ID\"].apply(lambda x: x.split(\"_\")[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cee67f4028f"
      },
      "source": [
        "Plot the average forecasted sales vs. the price of the product."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb351c8f383d"
      },
      "outputs": [],
      "source": [
        "for i in dfforecast_avg[\"Product_ID\"].unique():\n",
        "    dfforecast_avg[dfforecast_avg[\"Product_ID\"] == i].set_index(\"Price\").sort_values(\n",
        "        \"forecast_value\"\n",
        "    ).plot(kind=\"bar\")\n",
        "    plt.title(\"Price vs. Average Sales for \" + i)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67ff3acc74a5"
      },
      "source": [
        "Based on the plots for price vs. the average forecasted orders, it can be said that to avail the maximum orders, each of the considered `Product_ID`s can follow the below prices :\n",
        "- SKU 107's price range can be from 4.44 - 4.73 units\n",
        "- SKU 140's price can be 1.95 units\n",
        "- SKU 62's price can be 4.23 units\n",
        "\n",
        "\n",
        "## Clean Up\n",
        "<a name=\"section-12\"></a>\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial. The following code deletes the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d78908b8134d"
      },
      "outputs": [],
      "source": [
        "# Construct a BigQuery client object.\n",
        "client = bigquery.Client()\n",
        "\n",
        "# TODO(developer): Set model_id to the ID of the model to fetch.\n",
        "dataset_id = \"{PROJECT}.{DATASET}\".format(PROJECT=PROJECT_ID, DATASET=DATASET)\n",
        "\n",
        "# Use the delete_contents parameter to delete a dataset and its contents.\n",
        "# Use the not_found_ok parameter to not receive an error if the dataset has already been deleted.\n",
        "client.delete_dataset(\n",
        "    dataset_id, delete_contents=True, not_found_ok=True\n",
        ")  # Make an API request.\n",
        "\n",
        "print(\"Deleted dataset '{}'.\".format(dataset_id))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "pricing-optimization.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

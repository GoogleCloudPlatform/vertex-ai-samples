{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa9846856f05"
      },
      "source": [
        "# Build a recommender system with retail data on Vertex AI using PySpark\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1caa36b69240"
      },
      "source": [
        "## Table of contents\n",
        "\n",
        "* [Overview](#section-1)\n",
        "* [Dataset](#section-2)\n",
        "* [Objective](#section-3)\n",
        "* [Costs](#section-4)\n",
        "* [Create a Dataproc cluster with component gateway enabled and JupyterLab extension](#section-5)\n",
        "* [Connect to the cluster from the notebook](#section-6)\n",
        "* [Explore the data](#section-7)\n",
        "* [Define the ALS Model](#section-8)\n",
        "* [Evaluate the model](#section-9)\n",
        "* [Save the ALS model to Cloud Storage](#section-10)  \n",
        "* [Write the recommendations to BigQuery](#section-11)\n",
        "* [Clean up](#section-12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ec25b247838"
      },
      "source": [
        "## Overview\n",
        "<a name=\"section-1\"></a>\n",
        "\n",
        "Recommender systems are powerful tools that model existing customer behavior to generate recommendations. These models generally build complex matrices and map out existing customer preferences in order to find intersecting interests and offer recommendations. These matrices can be very large and will benefit from distributed computing and large memory pools. In a Vertex AI Workbench managed notebooks instance, you can use distributed computing by processing your data in PySpark on a Dataproc cluster.\n",
        "\n",
        "*Note: This notebook file was designed to run in a [Vertex AI Workbench managed notebooks](https://cloud.google.com/vertex-ai/docs/workbench/managed/create-instance) instance using a `Python 3` kernel generated by a Dataproc runtime. Some components of this notebook may not work in other notebook environments.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ccfcd2355f3"
      },
      "source": [
        "## Dataset\n",
        "<a name=\"section-2\"></a>\n",
        "\n",
        "This notebook uses the `looker-private-demo.retail` dataset in BigQuery. The dataset can be accessed by pinning the `looker-private-demo` project in BigQuery. Instead of going to the BigQuery user interface, this process can be performed from the JupyterLab user interface on a Vertex AI Workbench managed notebooks instance. Vertex AI Workbench managed notebooks instances support browsing through the datasets and tables from BigQuery through its BigQuery integration. \n",
        "\n",
        "<img src=\"images/Bigquery_UI_new.PNG\"></img>\n",
        "\n",
        "In this dataset, the `retail.order_items` table will be used to train the recommendation system using PySpark. This table contains information on various orders related to the users and items (products) in the dataset.\n",
        "\n",
        "## Objective\n",
        "<a name=\"section-3\"></a>\n",
        "\n",
        "This tutorial builds a recommendation model with a [collaborative filtering](https://en.wikipedia.org/wiki/Collaborative_filtering) approach using the interactive PySpark features offered by the Vertex AI Workbench's managed notebooks instances. You'll set up a remotely-connected Dataproc cluster and use the <a href=\"http://dl.acm.org/citation.cfm?id=1608614\">Alternating Least Squares(ALS)</a> method implemented in PySpark's MLlib library.\n",
        "\n",
        "The steps performed in this notebook are:\n",
        "\n",
        "1. Connect your managed notebooks instance to a Dataproc cluster with PySpark.\n",
        "2. Explore the dataset in BigQuery from within the notebook.\n",
        "3. Preprocess the data.\n",
        "4. Train a PySpark ALS model on the data.\n",
        "5. Evaluate the ALS model.\n",
        "6. Generate recommendations.\n",
        "7. Save the recommendations to a BigQuery table using the PySpark-BigQuery connector.\n",
        "8. Save the ALS model to a Cloud Storage bucket.\n",
        "9. Clean up the resources.\n",
        "\n",
        "## Costs\n",
        "<a name=\"section-4\"></a>\n",
        "This tutorial uses the following billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Dataproc\n",
        "* BigQuery\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing), [Dataproc pricing](https://cloud.google.com/dataproc/pricing), [BigQuery\n",
        "pricing](https://cloud.google.com/bigquery/pricing) and [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing) and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ae6fd90ce34"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0b690120659f"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"\"\n",
        "\n",
        "# Get your Google Cloud project ID from gcloud\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID: \", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c23bde1f51c5"
      },
      "source": [
        "Otherwise, set your project ID here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a258c0ac1442"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
        "    PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "826232e3a213"
      },
      "source": [
        "#### Timestamp\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append it onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47aebdd88e9e"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98ca9d3da7e6"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "When you submit a training job using the Cloud SDK, you upload a Python package\n",
        "containing your training code to a Cloud Storage bucket. Vertex AI runs\n",
        "the code from this package. In this tutorial, Vertex AI also saves the\n",
        "trained model that results from your job in the same bucket. Using this model artifact, you can then\n",
        "create Vertex AI model and endpoint resources in order to serve\n",
        "online predictions.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
        "Cloud Storage buckets.\n",
        "\n",
        "You may also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook. Make sure to [choose a region where Vertex AI services are\n",
        "available](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions). You may\n",
        "not use a Multi-Regional Storage bucket for training with Vertex AI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ead6150209c"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}\n",
        "REGION = \"[your-region]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "933efba860bc"
      },
      "outputs": [],
      "source": [
        "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
        "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26fd8d22e96f"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d201bf34f895"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f62553e5054"
      },
      "source": [
        "**Finally**, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73f53d17ae47"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ef78f6261a0"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "The ALS model approach is compute-intensive and could take a lot of time to train on a regular notebook environment, so this tutorial uses a Dataproc cluster with PySpark environment.\n",
        "\n",
        "### Create a Dataproc cluster with component gateway enabled and JupyterLab extension\n",
        "<a name=\"section-5\"></a>\n",
        "\n",
        "Create the cluster using the following `gcloud` command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "950554272656"
      },
      "outputs": [],
      "source": [
        "CLUSTER_NAME = \"[your-cluster-name]\"\n",
        "CLUSTER_REGION = \"[your-cluster-region]\"\n",
        "CLUSTER_ZONE = \"[your-cluster-zone]\"\n",
        "MACHINE_TYPE = \"[your=machine-type]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e3e719e27f6"
      },
      "outputs": [],
      "source": [
        "! gcloud dataproc clusters create $CLUSTER_NAME \\\n",
        "--enable-component-gateway \\\n",
        "--region $CLUSTER_REGION \\\n",
        "--zone $CLUSTER_ZONE \\\n",
        "--single-node \\\n",
        "--master-machine-type $MACHINE_TYPE \\\n",
        "--master-boot-disk-size 100 \\\n",
        "--image-version 2.0-debian10 \\\n",
        "--optional-components JUPYTER \\\n",
        "--project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2961b0a96176"
      },
      "source": [
        "Alternatively, the cluster can be created through the Dataproc console as well. Additional settings like network configuratons and service-accounts can be configured there if required. While configuring the cluster, make sure you complete the following:\n",
        "\n",
        "- Provide a name for the cluster.\n",
        "- Select a region and zone for the cluster.\n",
        "- Select the cluster type as single-node. For small and proof-of-concept use-cases, a single-node cluster is recommended.\n",
        "- Enable the component gateway.\n",
        "- In the optional components, select Jupyter Notebook.\n",
        "- (Optional) Select the machine-type (preferably a high-mem machine type).\n",
        "- Create the cluster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b864144904fb"
      },
      "source": [
        "## Connect to the cluster from the notebook\n",
        "<a name=\"section-6\"></a>\n",
        "\n",
        "When the new Dataproc cluster is running, the corresponding runtime appears as a kernel in the notebook. The created cluster's name will appear in the list of kernels that can be selected for this notebook. In the top right corner of this notebook file, click the current kernel name, **Python (local)**, and then select the Python 3 kernel that is running on your Dataproc cluster.\n",
        "\n",
        "<img src=\"images/cluster_kernel_selection.png\"></img>\n",
        "\n",
        "Note the following:\n",
        "\n",
        "- Your Dataproc kernel might take a few minutes to show up in the list of kernels.\n",
        "- PySpark code in this tutorial can be run on either a PySpark or Python 3 kernel on the Dataproc cluster, but to run the optional code that saves recommendations to a BigQuery table, the Python 3 kernel is recommended."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc8d6f0cc21b"
      },
      "source": [
        "## Tutorial "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03c8dc1024f7"
      },
      "source": [
        "## Explore the data\n",
        "<a name=\"section-7\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffc1113d6504"
      },
      "source": [
        "Vertex AI Workbench managed notebooks instances let you explore the BigQuery content from within the managed notebooks instance using a BigQuery integration. This feature lets you look at the metadata and preview of table content, query tables, and get a description of the data in the tables.\n",
        "\n",
        "<img src=\"images/BQ_view_table_new.PNG\"></img>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fb418e82451"
      },
      "source": [
        "Check the distribution of the `STATUS` field."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfebfe18e127"
      },
      "source": [
        "#@bigquery\n",
        "SELECT STATUS, COUNT(*) order_count FROM looker-private-demo.retail.order_items GROUP BY 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dd73d32bc80"
      },
      "source": [
        "Join the `order_items` table with the `inventory_items` table from the same dataset to retrieve the product IDs for the orders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4abc4fb13e25"
      },
      "source": [
        "#@bigquery\n",
        "WITH user_prod_table AS (\n",
        "SELECT USER_ID, PRODUCT_ID, STATUS FROM looker-private-demo.retail.order_items AS a\n",
        "join\n",
        "(SELECT ID, PRODUCT_ID FROM looker-private-demo.retail.inventory_items) AS b\n",
        "on a.inventory_item_id = b.ID )\n",
        "\n",
        "SELECT USER_ID, PRODUCT_ID, STATUS from user_prod_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5072aded172b"
      },
      "source": [
        "Once the results from BigQuery are displayed in the above cell, click the **Query and load as DataFrame** button and execute the generated code stub to fetch the data into the current notebook as a dataframe.\n",
        "\n",
        "*Note: By default the data is loaded into a `df` variable, though this can be changed before executing the cell if required.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba1a54b04d9a"
      },
      "outputs": [],
      "source": [
        "# The following two lines are only necessary to run once.\n",
        "# Comment out otherwise for speed-up.\n",
        "from google.cloud.bigquery import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "query = \"\"\"WITH user_prod_table AS (\n",
        "SELECT USER_ID, PRODUCT_ID, STATUS FROM looker-private-demo.retail.order_items AS a\n",
        "join\n",
        "(SELECT ID, PRODUCT_ID FROM looker-private-demo.retail.inventory_items) AS b\n",
        "on a.inventory_item_id = b.ID )\n",
        "\n",
        "SELECT USER_ID, PRODUCT_ID, STATUS from user_prod_table\"\"\"\n",
        "job = client.query(query)\n",
        "df = job.to_dataframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acb1077cbf1e"
      },
      "source": [
        "### Preprocess the Data\n",
        "\n",
        "To run PySpark's ALS method on the existing data, there must be some fields to quantify the relationship between a `USER_ID` and a `PRODUCT_ID`, such as *ratings given by the user*. If such fields already exist in the data, they can be treated as an *explicit feedback* for the ALS model. Otherwise, the fields indicative of a relationship can be given as an *implicit feedback*. Learn more about [feedback for PySpark's ALS method](https://spark.apache.org/docs/2.2.0/ml-collaborative-filtering.html#explicit-vs-implicit-feedback).\n",
        "\n",
        "In the current dataset, as there are no such numerical fields, the `STATUS` field is further used to quantify the association between a `USER_ID` and  a `PRODUCT_ID`. Based on when they occur during an order lifecycle and how likely the user is going to like the order, the `STATUS` field is assigned one of the following ratings:\n",
        "\n",
        "- Cancelled - 1\n",
        "- Returned - 2\n",
        "- Processing - 3\n",
        "- Shipped - 4\n",
        "- Complete - 5\n",
        "\n",
        "The ratings given are subjective and can be modified according to the use case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "365dca51641f"
      },
      "outputs": [],
      "source": [
        "score_mapping = {\n",
        "    \"Cancelled\": 1,\n",
        "    \"Returned\": 2,\n",
        "    \"Processing\": 3,\n",
        "    \"Shipped\": 4,\n",
        "    \"Complete\": 5,\n",
        "}\n",
        "df[\"RATING\"] = df[\"STATUS\"].map(score_mapping)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a88eb70c6dec"
      },
      "source": [
        "Check the distribution of the newly generated `RATING` field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfe29a1f9f16"
      },
      "outputs": [],
      "source": [
        "df[\"RATING\"].plot(kind=\"hist\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "367bc4912977"
      },
      "source": [
        "Load the required methods and classes from PySpark MLlib."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b674b037dce0"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c3c74870bcf"
      },
      "source": [
        "Generate a Spark session with the BigQuery-Spark connector configured.\n",
        "\n",
        "*Note: If the notebook is connected to a Dataproc cluster, the session object would show `yarn` as the Master.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2e55770bb894"
      },
      "outputs": [],
      "source": [
        "spark = (\n",
        "    SparkSession.builder.appName(\"Recommendations\")\n",
        "    .config(\"spark.jars\", \"gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89b6a2146b7c"
      },
      "source": [
        "Convert the pandas dataframe to a spark dataframe for further processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d1d5b5e6a56"
      },
      "outputs": [],
      "source": [
        "spark_df = spark.createDataFrame(df[[\"USER_ID\", \"PRODUCT_ID\", \"RATING\"]])\n",
        "spark_df.printSchema()\n",
        "spark_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b159af5c085"
      },
      "source": [
        "### Split the data into train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "951e7c2bf8fe"
      },
      "outputs": [],
      "source": [
        "(train, test) = spark_df.randomSplit([0.8, 0.2], seed=36)\n",
        "train.count(), test.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dea0589dd474"
      },
      "source": [
        "## Define the ALS Model\n",
        "<a name=\"section-8\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99e0365f32be"
      },
      "source": [
        "The PySpark ALS recommender, Alternating Least Squares, is a matrix factorization algorithm. The idea is to build a matrix that maps users to actions. The actions can be reviews, purchases, various options taken, and more. Due to the complexity and size of the matrix, PySpark can run the algorithm in parallel.\n",
        "\n",
        "ALS will attempt to estimate the rating matrix R as the product of two lower-rank matrices, X and Y. Typically these approximations are called \"factor\" matrices. During each iteration, one of the factor matrices is held constant, while the other is solved for using least squares. The newly-solved factor matrix is then held constant while solving for the other factor matrix.\n",
        "\n",
        "PySpark uses a blocked implementation of the ALS factorization algorithm that groups the two sets of factors (referred to as “users” and “products”) into blocks and reduces communication by only sending one copy of each user vector to each product block on each iteration, and only for the product blocks that need that user’s feature vector.\n",
        "\n",
        "Essentially instead of finding the low-rank approximations to the rating matrix R, this finds the approximations for a preference matrix P where the elements of P are 1 if r > 0 and 0 if r <= 0. The ratings then act as confidence values related to the strength of indicated user preferences rather than explicit ratings given to items. Learn more [about PySpark's ALS algorithm](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.recommendation.ALS.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b47e5bfe0c30"
      },
      "outputs": [],
      "source": [
        "als = ALS(\n",
        "    userCol=\"USER_ID\",\n",
        "    itemCol=\"PRODUCT_ID\",\n",
        "    ratingCol=\"RATING\",\n",
        "    nonnegative=True,\n",
        "    implicitPrefs=False,\n",
        "    coldStartStrategy=\"drop\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9726f220acb5"
      },
      "source": [
        "The ALS model tries to predict the ratings between users and items and so RMSE can be used for evaluating the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50ae46e7db7c"
      },
      "outputs": [],
      "source": [
        "evaluator = RegressionEvaluator(\n",
        "    metricName=\"rmse\", labelCol=\"RATING\", predictionCol=\"prediction\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01d3b2a2a801"
      },
      "source": [
        "Define a hyperparameter grid for cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0397bf82ba6"
      },
      "outputs": [],
      "source": [
        "param_grid = (\n",
        "    ParamGridBuilder()\n",
        "    .addGrid(als.rank, [10, 50])\n",
        "    .addGrid(als.regParam, [0.01, 0.1, 0.2])\n",
        "    .build()\n",
        ")\n",
        "print(\"No. of settings to be tested: \", len(param_grid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98f3c23ff0a1"
      },
      "source": [
        "Perform cross-validation and save the best model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13617284cdb3"
      },
      "outputs": [],
      "source": [
        "cv = CrossValidator(\n",
        "    estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=3\n",
        ")\n",
        "model = cv.fit(train)\n",
        "best_model = model.bestModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66bc83eb4adb"
      },
      "outputs": [],
      "source": [
        "print(\"##Parameters for the Best Model##\")\n",
        "print(\"Rank:\", best_model._java_obj.parent().getRank())\n",
        "print(\"MaxIter:\", best_model._java_obj.parent().getMaxIter())\n",
        "print(\"RegParam:\", best_model._java_obj.parent().getRegParam())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12a02668017e"
      },
      "source": [
        "## Evaluate the model\n",
        "<a name=\"section-9\"></a>\n",
        "Evaluate the model by computing the RMSE on the train and test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04d7e30661fa"
      },
      "outputs": [],
      "source": [
        "# View the rating predictions by the model on train and test sets\n",
        "train_predictions = best_model.transform(train)\n",
        "train_RMSE = evaluator.evaluate(train_predictions)\n",
        "\n",
        "test_predictions = best_model.transform(test)\n",
        "test_RMSE = evaluator.evaluate(test_predictions)\n",
        "\n",
        "print(\"Train RMSE \", train_RMSE)\n",
        "print(\"Test RMSE \", test_RMSE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "521dc4a5a688"
      },
      "source": [
        "### Generate recommendations for all users\n",
        "\n",
        "The required number of recommendations for the users can be generated using the ALS model's `recommendForAllUsers()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7ddfd001900"
      },
      "outputs": [],
      "source": [
        "# Generate 10 product recommendations for all users\n",
        "nrecommendations = best_model.recommendForAllUsers(10)\n",
        "nrecommendations.limit(10).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "219e986f5c23"
      },
      "source": [
        "### Generate recommendations for a specific user\n",
        "\n",
        "The earlier step already generated and stored the specified number of product recommendations for all users in the `nrecommendations` dataframe object. To obtain recommendations for a single user, this dataframe object can be queried."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "782b17eb3621"
      },
      "outputs": [],
      "source": [
        "# get product recommendations for the selected user (USER_ID = 1)\n",
        "nrecommendations.where(nrecommendations.USER_ID == 1).select(\n",
        "    \"recommendations.PRODUCT_ID\", \"recommendations.rating\"\n",
        ").collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "964c4d610d31"
      },
      "source": [
        "## Save the ALS model to Cloud Storage (optional)\n",
        "<a name=\"section-10\"></a>\n",
        "\n",
        "PySpark's `ALS.save()` method creates a folder at the specified path where it saves the trained model. A Cloud Storage file browser is available in the managed notebooks instance's environment, which you can use to save the model to a Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73db311320d0"
      },
      "source": [
        "Use the ALS object's `.save()` function to write the model to the Cloud Storage bucket. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f910addca92"
      },
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "GCS_MODEL_PATH = \"gs://\" + BUCKET_NAME + \"/recommender_systems/\"\n",
        "best_model.save(GCS_MODEL_PATH + \"rcmd_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "665a05361fbd"
      },
      "source": [
        "## Write the recommendations to BigQuery (optional)\n",
        "<a name=\"section-11\"></a>\n",
        "\n",
        "In order to serve the recommendations to the end-users or any applications, the output from the `recommendForAllUsers()` method can be saved to a BigQuery table using Spark's BigQuery connector.\n",
        "\n",
        "### Create a Dataset in BigQuery\n",
        "\n",
        "The following cell creates a new dataset in BigQuery."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22e136731378"
      },
      "source": [
        "#@bigquery\n",
        "-- create a dataset in BigQuery\n",
        "CREATE SCHEMA recommender_sys\n",
        "OPTIONS(\n",
        "  location=\"us\"\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4910f961001f"
      },
      "source": [
        "### Write the Recommendations to BigQuery\n",
        "\n",
        "PySpark's BigQuery connector requires two necessary fields: a *BigQuery Table name* and a *Cloud Storage path to write the temporary files* while saving the model. These two fields can be provided while writing the recommendations to BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63b76c37123d"
      },
      "outputs": [],
      "source": [
        "DATASET = \"[your-dataset-name]\"\n",
        "TABLE = \"[your-bigquery-table-name]\"\n",
        "GCS_TEMP_PATH = \"[your-cloud-storage-path]\"\n",
        "\n",
        "nrecommendations.write.format(\"bigquery\").option(\n",
        "    \"table\", \"{}.{}\".format(DATASET, TABLE)\n",
        ").option(\"temporaryGcsBucket\", GCS_TEMP_PATH).mode(\"overwrite\").save()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c3a6d501600"
      },
      "source": [
        "## Clean up\n",
        "<a name=\"section-12\"></a>\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72488a2999a6"
      },
      "outputs": [],
      "source": [
        "# remove the BigQuery dataset created for storing the recommendations and all of its tables\n",
        "! bq rm -r -f -d $PROJECT:$DATASET\n",
        "\n",
        "# remove the Cloud Storage bucket created and all of its tables\n",
        "! gsutil rm -r gs://$BUCKET_NAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75cfc772a3f4"
      },
      "outputs": [],
      "source": [
        "# delete the created Dataproc cluster\n",
        "! gcloud dataproc clusters delete $CLUSTER_NAME --region=$CLUSTER_REGION"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "recommender-system-on-retail-data.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

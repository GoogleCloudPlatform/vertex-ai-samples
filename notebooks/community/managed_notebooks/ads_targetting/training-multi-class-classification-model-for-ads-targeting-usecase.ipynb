{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1d3bdd17",
      "metadata": {
        "id": "503077811e70"
      },
      "source": [
        "# Training a multi-class classification model for Ads-targeting usecase\n",
        "\n",
        "## Table Of Contents\n",
        "* [Overview](#section-1)\n",
        "* [Dataset](#section-2)\n",
        "* [Objective](#section-3)\n",
        "* [Costs](#section-4)\n",
        "* [Tutorial](#section-5)\n",
        "\t- [Fetch the required data from Bigquery](#section-5)\n",
        "    - [Preprocess the data](#section-6)\n",
        "    - [Train a Tensorflow model](#section-7)\n",
        "    - [Run the model on test data](#section-8)\n",
        "    - [Automating the execution of the notebook using Executor](#section-9)\n",
        "    - [Scheduled Runs on Executor](#section-10)\n",
        "    - [Parametrizing the variables](#section-11)\n",
        "* [Save the model to a GCS path](#section-12)\n",
        "* [Clean Up](#section-13)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16af573f",
      "metadata": {
        "id": "00f095e91b2d"
      },
      "source": [
        "## Overview\n",
        "<a name=\"section-1\"></a>\n",
        "\n",
        "This tutorial demonstrates building a machine-learning model for an Ads-targeting use case. Ads-targeting is an advertisement technique where chosen or tailor-made ads are shown to the customers based on their past behavior and preferences.Targeted ads are meant to reach certain customers based on demographics, psychographics, behavior and other second-order activities that are learned usually through data collected from the customers. \n",
        "\n",
        "## Dataset\n",
        "<a name=\"section-2\"></a>\n",
        "This notebook uses the following dataset in Bigquery : ```looker-private-demo.ecomm```. The dataset consists of information about various advertisement campaigns including the demographics of users who have clicked and made some purchases after seeing the ads. For the current tutorial, top 3 campaigns from USA will be selected from this dataset and user information for those who have made purchases shall be used to train a model with the campaigns as the classes. The idea is to see if the advertisement and the user data can be used to identify which campaign suits best for the user.\n",
        "\n",
        "The dataset can be accessed by pinning the ```looker-private-demo``` project in Bigquery. Instead of going to Bigquery UI, this process can be performed from the current Jupyter environment(on Vertex-AI's managed-instance) itself. Vertex-AI's managed instances support browsing through the datasets and tables from Bigquery through its **Bigquery In Notebooks** feature. \n",
        "\n",
        "<img src=\"images/Bigquery_UI_new.PNG\"></img>\n",
        "\n",
        "## Objective\n",
        "<a name=\"section-3\"></a>\n",
        "This notebook demonstrates collecting required data from Bigquery, preprocessing it and training a multi-class classification model on an E-commerce dataset. The steps performed include the following :\n",
        "\n",
        "- Fetch the required data from Bigquery\n",
        "- Preprocess the data\n",
        "- Train a Tensorflow(>=2.4) classification model\n",
        "- Evaluate the loss for the trained model\n",
        "- Automating the notebook execution using Executor feature\n",
        "- Save the model to a GCS path\n",
        "- Clean up of the created resources\n",
        "\n",
        "### Costs \n",
        "<a name=\"section-4\"></a>\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Bigquery\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing), [Bigquery\n",
        "pricing](https://cloud.google.com/bigquery/pricing) and [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ad6548b-1045-44fb-bee9-0ecabaeef3af",
      "metadata": {
        "id": "a7be962cec10"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1d03e9e-69e4-47b0-a709-340e2859ded7",
      "metadata": {
        "id": "684595f229b3"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"\"\n",
        "\n",
        "# Get your Google Cloud project ID from gcloud\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID: \", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37c087ef-d1a9-4cd2-890f-c75a41cd1685",
      "metadata": {
        "id": "d0058f55f8cf"
      },
      "source": [
        "*Otherwise*, set your project ID here. *italicized text*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "031764eb-dada-49f9-a676-2c8a1c412a26",
      "metadata": {
        "id": "19579640c063"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
        "    PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a99c895-aa12-4798-8a5e-bad1783eaa69",
      "metadata": {
        "id": "b2b04f364669"
      },
      "source": [
        "#### Timestamp\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append it onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1a82012-28b9-47ef-ac9c-47a4df5cf838",
      "metadata": {
        "id": "a9ee95826661"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b781db66-d19c-4c2d-b565-1be3b8944c81",
      "metadata": {
        "id": "7f0d924a47aa"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "\n",
        "{TODO: Adjust wording in the first paragraph to fit your use case - explain how your tutorial uses the Cloud Storage bucket. The example below shows how Vertex AI uses the bucket for training.}\n",
        "\n",
        "When you submit a training job using the Cloud SDK, you upload a Python package\n",
        "containing your training code to a Cloud Storage bucket. Vertex AI runs\n",
        "the code from this package. In this tutorial, Vertex AI also saves the\n",
        "trained model that results from your job in the same bucket. Using this model artifact, you can then\n",
        "create Vertex AI model and endpoint resources in order to serve\n",
        "online predictions.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
        "Cloud Storage buckets.\n",
        "\n",
        "You may also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook. Make sure to [choose a region where Vertex AI services are\n",
        "available](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions). You may\n",
        "not use a Multi-Regional Storage bucket for training with Vertex AI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfbeefd5-c60e-4a5c-854f-98a1c906d773",
      "metadata": {
        "id": "59ebce2a16fa"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}\n",
        "REGION = \"[your-region]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef3d6a5f-a66d-4098-8339-869ea64fb5db",
      "metadata": {
        "id": "d0caa7b3c8b8"
      },
      "outputs": [],
      "source": [
        "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
        "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c07223f-a013-41db-bbe6-042232d3fa30",
      "metadata": {
        "id": "7a7c87b6f171"
      },
      "source": [
        "`**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket.`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e40041a-bd98-4f63-90c2-bc31abb1b24d",
      "metadata": {
        "id": "76f7e3e043f5"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "788398b6-7107-4f0f-a055-7602dced3626",
      "metadata": {
        "id": "f4353e94da5f"
      },
      "source": [
        "**Finally**, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07e1bbf0-6137-412b-b3c5-a25e6e86108d",
      "metadata": {
        "id": "6f092d6ebc92"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad80808e",
      "metadata": {
        "id": "9abd63bb1a85"
      },
      "source": [
        "## Tutorial\n",
        "\n",
        "### Fetch the required data from Bigquery \n",
        "<a name=\"section-5\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "668c1c6a",
      "metadata": {
        "id": "4a25861cf833"
      },
      "source": [
        "#@bigquery\n",
        "\n",
        "WITH\n",
        "  traindata AS (\n",
        "  SELECT\n",
        "    b.* EXCEPT(ad_event_id,\n",
        "      user_id),\n",
        "    c.* EXCEPT(id),\n",
        "    d.* EXCEPT(keyword_id,\n",
        "      ad_id),\n",
        "    a.amount,\n",
        "    a.device_type,\n",
        "    e.name\n",
        "  FROM\n",
        "    `looker-private-demo.ecomm.ad_events` a\n",
        "  JOIN (\n",
        "    SELECT\n",
        "      ad_event_id,\n",
        "      user_id,\n",
        "      state,\n",
        "      os,\n",
        "      browser\n",
        "    FROM\n",
        "      `looker-private-demo.ecomm.events`\n",
        "    WHERE\n",
        "      event_type=\"Purchase\"\n",
        "      AND country=\"USA\") b\n",
        "  ON\n",
        "    a.id = b.ad_event_id\n",
        "  JOIN (\n",
        "    SELECT\n",
        "      id,\n",
        "      gender,\n",
        "      age\n",
        "    FROM\n",
        "      `looker-private-demo.ecomm.users`) c\n",
        "  ON\n",
        "    b.user_id = c.id\n",
        "  JOIN (\n",
        "    SELECT\n",
        "      keyword_id,\n",
        "      ad_id,\n",
        "      cpc_bid_amount,\n",
        "      bidding_strategy_type,\n",
        "      quality_score,\n",
        "      keyword_match_type\n",
        "    FROM\n",
        "      `looker-private-demo.ecomm.keywords`\n",
        "    WHERE\n",
        "      cpc_bid_amount <= 3000) d\n",
        "  ON\n",
        "    a.keyword_id = d.keyword_id\n",
        "  JOIN (\n",
        "    SELECT\n",
        "      ad_id,\n",
        "      name\n",
        "    FROM\n",
        "      `looker-private-demo.ecomm.ad_groups`) e\n",
        "  ON\n",
        "    d.ad_id = e.ad_id )\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  traindata"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b77b1fc9",
      "metadata": {
        "id": "923fdd823683"
      },
      "source": [
        "Once the results from Bigquery are displayed in the above cell, press the **Query and load as DataFrame** button and execute the generated code stub to fetch the data into into the current notebook as a dataframe. \n",
        "\n",
        "*Note : By default the data is loaded into \"df\" variable and it could be changed before executing the cell if required.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12edaae8",
      "metadata": {
        "id": "f8b5112f231f"
      },
      "outputs": [],
      "source": [
        "# The following two lines are only necessary to run once.\n",
        "# Comment out otherwise for speed-up.\n",
        "from google.cloud.bigquery import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "query = \"\"\"WITH traindata AS (\n",
        "SELECT b.* except(ad_event_id, user_id), c.* except(id), d.* except(keyword_id, ad_id), a.amount, a.device_type, e.name\n",
        "FROM `looker-private-demo.ecomm.ad_events` a\n",
        "JOIN\n",
        "(SELECT ad_event_id, user_id, state, os, browser from `looker-private-demo.ecomm.events` WHERE event_type=\"Purchase\" AND country=\"USA\") b\n",
        "ON a.id = b.ad_event_id\n",
        "JOIN\n",
        "(SELECT id, gender, age FROM `looker-private-demo.ecomm.users`) c\n",
        "ON b.user_id = c.id\n",
        "JOIN\n",
        "(SELECT keyword_id, ad_id, cpc_bid_amount, bidding_strategy_type, quality_score, keyword_match_type FROM `looker-private-demo.ecomm.keywords`\n",
        "WHERE cpc_bid_amount <= 3000) d\n",
        "ON a.keyword_id = d.keyword_id\n",
        "JOIN\n",
        "(SELECT ad_id, name FROM `looker-private-demo.ecomm.ad_groups`) e\n",
        "ON d.ad_id = e.ad_id\n",
        ")\n",
        "SELECT * FROM traindata\"\"\"\n",
        "job = client.query(query)\n",
        "df = job.to_dataframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c07fe8f",
      "metadata": {
        "id": "7f69bea65019"
      },
      "source": [
        "### Preprocess the data\n",
        "<a name=\"section-6\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8632011f",
      "metadata": {
        "id": "e8503e799eec"
      },
      "source": [
        "Import the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87497ccc",
      "metadata": {
        "id": "5b11973ccf76"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c321599a",
      "metadata": {
        "id": "e48d156d8bb6"
      },
      "source": [
        "Select the necessary columns from the E-commerce data and divide them based on their type(numerical/categorical)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c72a7f23",
      "metadata": {
        "id": "7cea2c44c50b"
      },
      "outputs": [],
      "source": [
        "target = \"name\"\n",
        "categ_cols = [\n",
        "    \"state\",\n",
        "    \"os\",\n",
        "    \"browser\",\n",
        "    \"gender\",\n",
        "    \"bidding_strategy_type\",\n",
        "    \"keyword_match_type\",\n",
        "    \"device_type\",\n",
        "]\n",
        "num_cols = [\"age\", \"cpc_bid_amount\", \"quality_score\", \"amount\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77fae9bd",
      "metadata": {
        "id": "ace612851261"
      },
      "source": [
        "From the current dataset, only the top 3 camapigns will be chosen to target the users. All the relevant information about the advertisement and the user who purchased an item after seeing the advertisement is available in the dataframe already. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1498768",
      "metadata": {
        "id": "7282fbab4586"
      },
      "outputs": [],
      "source": [
        "df = df[df[\"name\"].isin([\"Tops & Tees\", \"Active\", \"Accessories\"])]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf9f5f8a",
      "metadata": {
        "id": "f89106348ffe"
      },
      "source": [
        "Encode the target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88d49f45",
      "metadata": {
        "id": "d6cd256f455a"
      },
      "outputs": [],
      "source": [
        "df[\"name\"] = df[\"name\"].map({\"Tops & Tees\": 0, \"Active\": 1, \"Accessories\": 2})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e62f2d68",
      "metadata": {
        "id": "8902f763d1ca"
      },
      "source": [
        "One-hot encode the categorical variables. After one-hot encoding, the first level-column is dropped to avoid [dummy-variable trap](https://en.wikipedia.org/wiki/Dummy_variable_(statistics)) scenario. This process is called *dummy-encoding*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1647543",
      "metadata": {
        "id": "d57706df2441"
      },
      "outputs": [],
      "source": [
        "def encode_cols(data, col):\n",
        "    # Creating a dummy variable for the variable 'CategoryID' and dropping the first one.\n",
        "    categ = pd.get_dummies(data[col], prefix=col, drop_first=True)\n",
        "    # Adding the results to the master dataframe\n",
        "    data = pd.concat([data, categ], axis=1)\n",
        "    return data\n",
        "\n",
        "\n",
        "# dummy-encode the categorical fields\n",
        "for i in categ_cols:\n",
        "    df = encode_cols(df, i)\n",
        "    df.drop(columns=[i], inplace=True)\n",
        "\n",
        "# check the data's shape\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ec26636",
      "metadata": {
        "id": "3abf027eda2d"
      },
      "source": [
        "Split the data into train and test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75b6e557",
      "metadata": {
        "id": "0072d44b6163"
      },
      "outputs": [],
      "source": [
        "X = df[[i for i in df.columns if i != target]].copy()\n",
        "y = df[target].copy()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, train_size=0.8, random_state=36\n",
        ")\n",
        "print(X_train.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ac5e973",
      "metadata": {
        "id": "d1a32b9d9640"
      },
      "source": [
        "Scale the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ade9f852",
      "metadata": {
        "id": "9620e04d8db2"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_train.loc[:, num_cols] = scaler.fit_transform(X_train[num_cols])\n",
        "X_test.loc[:, num_cols] = scaler.transform(X_test[num_cols])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0600de7c",
      "metadata": {
        "id": "5b9f3ca04f91"
      },
      "source": [
        "### Train a Tensorflow model\n",
        "<a name=\"section-7\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1304f39e",
      "metadata": {
        "id": "3e7656556a48"
      },
      "source": [
        "Convert target column to categorical encoded colum (one-hot encoded)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0dbb3fb",
      "metadata": {
        "id": "ebc87650ae13"
      },
      "outputs": [],
      "source": [
        "y_train_categ = to_categorical(y_train)\n",
        "y_test_categ = to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3a8089b",
      "metadata": {
        "id": "3dd0014a7e1d"
      },
      "source": [
        "Define hyperparameters for model training. \n",
        "\n",
        "*Note: Comment or remove the parameters from the following cell if they are provided already as an input parameter through the Executor feature.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd05a7b2",
      "metadata": {
        "id": "ec020b36af20"
      },
      "outputs": [],
      "source": [
        "optimizer = \"sgd\"\n",
        "num_hidden_layers = 3\n",
        "num_neurons = [64, 128, 256]\n",
        "activ_func = [\"relu\", \"relu\", \"relu\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f5a4352",
      "metadata": {
        "id": "406b731f576b"
      },
      "source": [
        "Define the architecture and compile the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a6c8f1f",
      "metadata": {
        "id": "57839a187cf0"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "# construct the neural network as per the defined parameters\n",
        "for i in range(num_hidden_layers):\n",
        "    if i == 0:\n",
        "        # add the input layer\n",
        "        model.add(\n",
        "            Dense(\n",
        "                num_neurons[i],\n",
        "                activation=activ_func[i],\n",
        "                input_shape=(X_train.shape[1],),\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        # add the hidden layers\n",
        "        model.add(Dense(num_neurons[i], activation=activ_func[i]))\n",
        "\n",
        "# add the output layer\n",
        "model.add(Dense(3, activation=\"softmax\"))\n",
        "# compile the model\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d99a7900",
      "metadata": {
        "id": "4ab12c34f258"
      },
      "source": [
        "Fit the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5914e66a",
      "metadata": {
        "id": "9321005e55ae"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train, y_train_categ, epochs=50, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3885f85",
      "metadata": {
        "id": "51a2d0b52df3"
      },
      "source": [
        "### Run the model on test data\n",
        "<a name=\"section-8\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "895608da",
      "metadata": {
        "id": "f08445f2cd02"
      },
      "source": [
        "Evaluate the model on test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b600628",
      "metadata": {
        "id": "599df6d2b9a4"
      },
      "outputs": [],
      "source": [
        "test_results = model.evaluate(X_test, y_test_categ, verbose=1)\n",
        "print(f\"Test results - Loss: {test_results}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9baaf11b",
      "metadata": {
        "id": "9769168778e8"
      },
      "source": [
        "### Automating the execution of the notebook using Executor\n",
        "<a name=\"section-9\"></a>\n",
        "\n",
        "Running the noteboook from start to end has become more prowerful with the new executor feature. Hitting the executor button would bring up a form that can be filled with the choice of the environment, machine-type, input parameters etc. After submitting it, the notebook gets executed as a job in the Vertex-ai custom training jobs. The running jobs can be monitored from the <b>Notebook Executor</b> pane in the menu on the left.\n",
        "\n",
        "<img src=\"images/executor.png\"></img>\n",
        "\n",
        "\n",
        "Executor gives us the freedom to choose the environment and machine-type while automating the runs similar to Vertex-Ai training jobs without switching to the training-jobs UI. Apart from the custom-container that replicates the existing kernel by default, pre-built environments like Tensorflow-Enterprise, PyTorch etc. can also be selected to run the notebook. Furthermore the required compute-power can be specified by choosing from the list of machine-types available including GPUs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60ac924a",
      "metadata": {
        "id": "cf486c351581"
      },
      "source": [
        "### Scheduled Runs on Executor\n",
        "<a name=\"section-10\"></a>\n",
        "\n",
        "The runs can also be scheduled recurringly with the Executor. To do so, <b>Schedule-based recurring executions </b> needs to be selected in the run type insetead of <b>One-time execution</b>. Further, the frequency of the job and the time when it needs to execute can be provided in the form itself.\n",
        "\n",
        "\n",
        "<img src=\"images/executor_scheduled_runs2.png\"></img>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14bbe47b",
      "metadata": {
        "id": "6561007ac7f2"
      },
      "source": [
        "### Parametrizing the variables\n",
        "<a name=\"section-11\"></a>\n",
        "\n",
        "Executor also makes it easy to run the notebooks with different set of input paramters. If required, the needed constants in the notebook can be treated as arguments to a function and while submitting the Executor form, those constants can be given as input parameters.\n",
        "\n",
        "<img src=\"images/executor_input_parameters.png\"></img>\n",
        "\n",
        "The hyperparameters defined during the model-training step can be passed as arguments while submitting this executor form. Of course, the values defined in this notebook should be removed or commented out before submitting for execution. Otherwise, the input parameters would just be overwritten in the job. Executor feature would thus allow us to also run the notebook as a training job with different parameter settings everytime."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60e1ce7c",
      "metadata": {
        "id": "c81db97fa3e8"
      },
      "source": [
        "### Save the model to a GCS path\n",
        "<a name=\"section-12\"></a>\n",
        "\n",
        "Tensorflow's *model.save()* method supports GCS paths as well as the local file paths while writing the model object to a file. It needs to be ensured that the service-account being used to run this notebook has write permissions to the specified GCS path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8ee0b08",
      "metadata": {
        "id": "2dcefeb6a2d8"
      },
      "outputs": [],
      "source": [
        "GCS_PATH = \"gs://\" + BUCKET_NAME + \"/[path-to-save]/\"\n",
        "model.save(GCS_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ced8399",
      "metadata": {
        "id": "29c0ca2a517a"
      },
      "source": [
        "## Clean Up\n",
        "<a name=\"section-13\"></a>\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "814afbc3",
      "metadata": {
        "id": "4ab69210d5a8"
      },
      "outputs": [],
      "source": [
        "! gsutil -m rm -r [gcs-folder-path-to-delete]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "training-multi-class-classification-model-for-ads-targeting-usecase.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

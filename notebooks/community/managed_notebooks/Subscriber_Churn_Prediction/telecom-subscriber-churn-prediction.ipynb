{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64f7165bd1ac"
   },
   "source": [
    "# Telecom subscriber churn prediction on Vertex-AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Of Contents\n",
    "* [Overview](#section-1)\n",
    "* [Dataset](#section-2)\n",
    "* [Objective](#section-3)\n",
    "* [Costs](#section-4)\n",
    "* [Perform EDA](#section-5)\n",
    "* [Train a Logistic-Regression model using sklearn](#section-6)\n",
    "* [Evaluate the trained model](#section-7)\n",
    "* [Save the model to GCS path](#section-8)\n",
    "* [Create a model with EXplainable-AI(XAI) support in Vertex-AI](#section-9)\n",
    "* [Cleaning up](#section-10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ccb933a1de0"
   },
   "source": [
    "## Overview\n",
    "<a name=\"section-1\"></a>\n",
    "\n",
    "\n",
    "This example demonstrates building a subscriber churn prediction model on a [Telecom customer churn dataset](https://www.kaggle.com/c/customer-churn-prediction-2020/overview) dataset. The generated churn model is further deployed to Vertex-ai endpoints and explanations are generated using the XAI feature of Vertex-AI within this notebook. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bae972f3229"
   },
   "source": [
    "## Dataset\n",
    "<a name=\"section-2\"></a>\n",
    "\n",
    "\n",
    "\n",
    "The dataset itself and an overview of the dataset used in this notebook can be found on the \"[Customer Churn Prediction 2020](https://www.kaggle.com/c/customer-churn-prediction-2020/data)\" Kaggle competition page. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75e3d160163c"
   },
   "source": [
    "## Objective\n",
    "<a name=\"section-3\"></a>\n",
    "\n",
    "This notebook demonstrates performing analysis, preprocessing and training a churn prediction model on a tabular churn dataset. The steps performed include the following :\n",
    "\n",
    "- Load data from a GCS path\n",
    "- Perform Exploratory Data Analysis(EDA)\n",
    "- Preprocessing the data\n",
    "- train an sklearn model\n",
    "- evaluate the sklearn model\n",
    "- save the model to a GCS path\n",
    "- create a model and an endpoint in Vertex-ai\n",
    "- deploy the trained model to an endpoint\n",
    "- generate predictions and explanations on test data from the hosted model\n",
    "- Undeploy the model resource\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "556bf343c423"
   },
   "source": [
    "## Costs \n",
    "<a name=\"section-4\"></a>\n",
    "\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44b8ae8e2d19"
   },
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ab450121b368"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "USER_FLAG = \"\"\n",
    "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
    "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "606337930991"
   },
   "source": [
    "Install the latest version of the Vertex AI client library.\n",
    "\n",
    "Run the following command in your virtual environment to install the Vertex SDK for Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9f52f949a77b"
   },
   "outputs": [],
   "source": [
    "! pip install {USER_FLAG} --upgrade google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e67139e68463"
   },
   "source": [
    "Install the Cloud Storage library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ad918f94f5d"
   },
   "outputs": [],
   "source": [
    "! pip install {USER_FLAG} --upgrade google-cloud-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eb0c1e24a8f0"
   },
   "source": [
    "Install the <b>category_encoders</b> library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deb95a7f2104"
   },
   "outputs": [],
   "source": [
    "! pip install --upgrade category_encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "184560c1b742"
   },
   "source": [
    "Install the Seaborn library for the EDA step. If Vertex-ai's managed-instances are being used, this step is optional as the library is already available in the Python3 kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0d99cdcdc470"
   },
   "outputs": [],
   "source": [
    "! pip install --upgrade seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b012ef94ce80"
   },
   "source": [
    "## Before you begin \n",
    "\n",
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). {TODO: Update the APIs needed for your tutorial. Edit the API names, and update the link to append the API IDs, separating each one with a comma. For example, container.googleapis.com,cloudbuild.googleapis.com}\n",
    "\n",
    "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    "\n",
    "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e663bd062c6f"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append it onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "953fa6e5ddda"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "637c90fe1062"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "\n",
    "When you create a model in Vertex-AI using the Cloud SDK, you give a Cloud Storage path where the trained model is saved. \n",
    "In this tutorial, Vertex AI saves the trained model to a GCS bucket. Using this model artifact, you can then\n",
    "create Vertex AI model and endpoint resources in order to serve\n",
    "online predictions.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
    "Cloud Storage buckets.\n",
    "\n",
    "You may also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook. Make sure to [choose a region where Vertex AI services are\n",
    "available](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions). You may\n",
    "not use a Multi-Regional Storage bucket for training with Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65e1e634c920"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "REGION = \"[your-region]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68d1f4908641"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cb016da6de3"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e66f36b71bc3"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51a8afe42b5e"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "835bf2f84d86"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32e48bbf1267"
   },
   "source": [
    "### Copy dataset into your Cloud Storage bucket\n",
    "\n",
    "Copy the <b>train.csv</b> file from the \"[Customer Churn Prediction 2020](https://www.kaggle.com/c/customer-churn-prediction-2020/data)\" Kaggle competition page to the created bucket. In Vertex-ai's managed instances, file upload to a GCS bucket can be performed using the GCS browser pane from the Jupyter Lab environment itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91a6f77f3d94"
   },
   "source": [
    "## Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b02bcf983b82"
   },
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a1c9e504395c"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import json\n",
    "# configure to don't display the warnings\n",
    "import warnings\n",
    "\n",
    "import category_encoders as ce\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "from google.cloud import aiplatform, storage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, plot_roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e37354341588"
   },
   "source": [
    "### Load data from GCS path using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d949de05d3e3"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"gs://cloud-samples-data/vertex-ai/managed_notebooks/telecom_churn_prediction/train.csv\"\n",
    ")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ca1a5b9da481"
   },
   "source": [
    "## Perform EDA\n",
    "<a name=\"section-5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fc64740bd98"
   },
   "source": [
    "Check the datatypes and null counts of the fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1305e25e8c85"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dd5d560c25f8"
   },
   "source": [
    "Check class imbalance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cabae9da850"
   },
   "source": [
    "The current dataset doesn't have any null or empty fields in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a12839626b40"
   },
   "outputs": [],
   "source": [
    "df[\"churn\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2473365db828"
   },
   "source": [
    "There are 14% churners in the data which is not bad for training a churn prediction model. If the class-imablance seems to be high, oversampling or undersampling techniques can be considered to balance the class distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fa0f8adef6f2"
   },
   "source": [
    "Separate the caetgorical and numerical columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1c94426e3b2"
   },
   "outputs": [],
   "source": [
    "categ_cols = [\"state\", \"area_code\", \"international_plan\", \"voice_mail_plan\"]\n",
    "target = \"churn\"\n",
    "num_cols = [i for i in df.columns if i not in categ_cols and i != target]\n",
    "print(len(categ_cols), len(num_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1c1f4744269d"
   },
   "source": [
    "Plot level distribution for the categorical columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bb016790dcdc"
   },
   "outputs": [],
   "source": [
    "for i in categ_cols:\n",
    "    df[i].value_counts().plot(kind=\"bar\")\n",
    "    plt.title(i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7c1eaf4363f"
   },
   "outputs": [],
   "source": [
    "print(num_cols)\n",
    "df[\"number_vmail_messages\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7024efb18a27"
   },
   "source": [
    "Check distribution for the numerical columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11165f493bbc"
   },
   "outputs": [],
   "source": [
    "for i in num_cols:\n",
    "    # check the Price field's distribution\n",
    "    _, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    df[i].plot(kind=\"box\", ax=ax[0])\n",
    "    df[i].plot(kind=\"hist\", ax=ax[1])\n",
    "    plt.title(i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eec92dc756a9"
   },
   "outputs": [],
   "source": [
    "# check pairplots for selected features\n",
    "selected_features = [\n",
    "    \"total_day_calls\",\n",
    "    \"total_eve_calls\",\n",
    "    \"number_customer_service_calls\",\n",
    "    \"number_vmail_messages\",\n",
    "    \"account_length\",\n",
    "    \"total_day_charge\",\n",
    "    \"total_eve_charge\",\n",
    "]\n",
    "sns.pairplot(df[selected_features])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56c8911c7588"
   },
   "source": [
    "Plot a heatmap of the correlation matrix for the numerical features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79fdafb9e5a6"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(df[num_cols].corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19ef89810012"
   },
   "source": [
    "### Observations from EDA :\n",
    "\n",
    "- There are many levels/categories in the categorical field <b>state</b>. In the further steps, creating one hot encoding vector for this field can increase the columns drastically and so binary encoding technique will be considered for encoding this field.\n",
    "- There are only 9% of customers in the data who have had international plans.\n",
    "- There are only a few customers who make frequent calls to customer-service.\n",
    "- Only 25% of the customers had at least 16 vocie-mail messages and thus the skewness in the distribution of \"number_vmail_messages\" field.\n",
    "- Most of the feature combinations in the pair plot show a circular pattern that suggests that there is almost no correlation between the corresponding two features.\n",
    "- There seems to be a high correlation between minutes and charge. Either one of them can be dropped to avoid multi-collinearity or redundant features in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28f0b40ea577"
   },
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7dd6ade12cd"
   },
   "source": [
    "Drop the fields corresponding to the highly correlated features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bb46ec159a3"
   },
   "outputs": [],
   "source": [
    "drop_cols = [\n",
    "    \"total_day_charge\",\n",
    "    \"total_eve_charge\",\n",
    "    \"total_night_charge\",\n",
    "    \"total_intl_charge\",\n",
    "]\n",
    "df.drop(columns=drop_cols, inplace=True)\n",
    "num_cols = list(set(num_cols).difference(set(drop_cols)))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcc77c18541c"
   },
   "source": [
    "Binary encode the state feature(as there are many levels/categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9f8e1fdcbb8"
   },
   "outputs": [],
   "source": [
    "encoder = ce.BinaryEncoder(cols=[\"state\"], return_df=True)\n",
    "data_encoded = encoder.fit_transform(df)\n",
    "data_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c59a90d8961c"
   },
   "source": [
    "One-hot encode(dropping the first level-column to avoid dummy-variable trap scenarios) the remaining categorical variables \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21c8c6262dbc"
   },
   "outputs": [],
   "source": [
    "def encode_cols(data, col):\n",
    "    # Creating a dummy variable for the variable 'CategoryID' and dropping the first one.\n",
    "    categ = pd.get_dummies(data[col], prefix=col, drop_first=True)\n",
    "    # Adding the results to the master dataframe\n",
    "    data = pd.concat([data, categ], axis=1)\n",
    "    return data\n",
    "\n",
    "\n",
    "for i in categ_cols + [target]:\n",
    "    if i != \"state\":\n",
    "        data_encoded = encode_cols(data_encoded, i)\n",
    "        data_encoded.drop(columns=[i], inplace=True)\n",
    "\n",
    "data_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7ee5d51d900"
   },
   "source": [
    "Check the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "980c42c98f3a"
   },
   "outputs": [],
   "source": [
    "data_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b402c7db5d1a"
   },
   "source": [
    "Check the columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7e67c06b9938"
   },
   "outputs": [],
   "source": [
    "data_encoded.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a809e0c43ff"
   },
   "source": [
    "Split the data into train-test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "17aaffef959d"
   },
   "outputs": [],
   "source": [
    "X = data_encoded[[i for i in data_encoded.columns if i not in [\"churn_yes\"]]].copy()\n",
    "y = data_encoded[\"churn_yes\"].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.7, test_size=0.3, random_state=100\n",
    ")\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1875f9ec661f"
   },
   "source": [
    "Scale the numerical data using MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ed095d60fa47"
   },
   "outputs": [],
   "source": [
    "sc = MinMaxScaler()\n",
    "X_train.loc[:, num_cols] = sc.fit_transform(X_train[num_cols])\n",
    "X_test.loc[:, num_cols] = sc.transform(X_test[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "229cfba1fd32"
   },
   "source": [
    "## Train a Logistic-Regression model using sklearn\n",
    "<a name=\"section-6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37a536623470"
   },
   "source": [
    "Argument class_weight adjusts the class weights to the target feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7acc267ad4c1"
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(class_weight=\"balanced\")\n",
    "model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18e23a047402"
   },
   "source": [
    "## Evaluate the trained model\n",
    "<a name=\"section-7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e59e96403fdf"
   },
   "source": [
    "#### Plot the ROC and show AUC on train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "038852060275"
   },
   "source": [
    "Plot the ROC for the model on train data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "482d7eb78295"
   },
   "outputs": [],
   "source": [
    "plot_roc_curve(model, X_train, y_train, drop_intermediate=False)\n",
    "plt.show()\n",
    "\n",
    "# plot the ROC for the model on test data\n",
    "plot_roc_curve(model, X_test, y_test, drop_intermediate=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cb393fe2f3c4"
   },
   "source": [
    "#### Determine the optimal threhsold for the binary classification \n",
    "\n",
    "In general, the logistic regression model outputs probability scores between 0 and 1 and a threshold needs to be determined to assign a class-label. Depending on the sensitivity(True-positive rate) and specificity(True-negative rate) of the model, an optimal thresold can be  determined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9482b0a4d3a"
   },
   "source": [
    "Create columns with 10 different probability cutoffs \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f19ea3f3a2ce"
   },
   "outputs": [],
   "source": [
    "y_train_pred = model.predict_proba(X_train)[:, 1]\n",
    "numbers = [float(x) / 10 for x in range(10)]\n",
    "y_train_pred_df = pd.DataFrame({\"true\": y_train, \"pred\": y_train_pred})\n",
    "for i in numbers:\n",
    "    y_train_pred_df[i] = y_train_pred_df.pred.map(lambda x: 1 if x > i else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cff355e58790"
   },
   "source": [
    "Now calculate accuracy sensitivity and specificity for various probability cutoffs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "572a9dc237f6"
   },
   "outputs": [],
   "source": [
    "cutoff_df = pd.DataFrame(columns=[\"prob\", \"accuracy\", \"sensitivity\", \"specificity\"])\n",
    "\n",
    "# compute the parameters for each threshold considered\n",
    "for i in numbers:\n",
    "    cm1 = confusion_matrix(y_train_pred_df.true, y_train_pred_df[i])\n",
    "    total1 = sum(sum(cm1))\n",
    "    accuracy = (cm1[0, 0] + cm1[1, 1]) / total1\n",
    "\n",
    "    speci = cm1[0, 0] / (cm1[0, 0] + cm1[0, 1])\n",
    "    sensi = cm1[1, 1] / (cm1[1, 0] + cm1[1, 1])\n",
    "    cutoff_df.loc[i] = [i, accuracy, sensi, speci]\n",
    "\n",
    "# Let's plot accuracy sensitivity and specificity for various probabilities.\n",
    "cutoff_df.plot.line(x=\"prob\", y=[\"accuracy\", \"sensitivity\", \"specificity\"])\n",
    "plt.title(\"Comparison of performance across various thresholds\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ae7362a31f0"
   },
   "source": [
    "In general, a model with balanced sensitivity and specificity is preferred. In the current case, the threshold where the sensitivity and specifity curves intersect can be considered as an optimal threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95d07b5422a6"
   },
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "\n",
    "# Evaluate train and test sets\n",
    "y_test_pred = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# to get the performance stats, lets define a handy function\n",
    "\n",
    "\n",
    "def print_stats(y_true, y_pred):\n",
    "    # Confusion matrix\n",
    "\n",
    "    confusion = confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
    "    print(\"Confusion Matrix: \")\n",
    "    print(confusion)\n",
    "\n",
    "    TP = confusion[1, 1]  # true positive\n",
    "    TN = confusion[0, 0]  # true negatives\n",
    "    FP = confusion[0, 1]  # false positives\n",
    "    FN = confusion[1, 0]  # false negatives\n",
    "\n",
    "    # Let's see the sensitivity or recall of our logistic regression model\n",
    "    sensitivity = TP / float(TP + FN)\n",
    "    print(\"sensitivity = \", sensitivity)\n",
    "    # Let us calculate specificity\n",
    "    specificity = TN / float(TN + FP)\n",
    "    print(\"specificity = \", specificity)\n",
    "    # Calculate false postive rate - predicting conversion when customer didn't convert\n",
    "    fpr = FP / float(TN + FP)\n",
    "    print(\"False positive rate = \", fpr)\n",
    "    # positive predictive value\n",
    "    precision = TP / float(TP + FP)\n",
    "    print(\"precision = \", precision)\n",
    "    # accuracy\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    print(\"accuracy = \", accuracy)\n",
    "    return\n",
    "\n",
    "\n",
    "y_train_pred_sm = [1 if i > threshold else 0 for i in y_train_pred]\n",
    "y_test_pred_sm = [1 if i > threshold else 0 for i in y_test_pred]\n",
    "# Print the metrics for the model\n",
    "# on train data\n",
    "print(\"Train Data : \")\n",
    "print_stats(y_train, y_train_pred_sm)\n",
    "print(\"\\n\", \"*\" * 30, \"\\n\")\n",
    "# on test data\n",
    "print(\"Test Data : \")\n",
    "print_stats(y_test, y_test_pred_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b64f7190aad6"
   },
   "source": [
    "While the model's sensitivity and specificity are looking decent, the precision can be identified to be low. This type of situation is generally acceptable to some extent because from a business standpoint in Telecom industry, it still makes sense to identify churners even though it means there'd be some mis-classifications of non-churner as a churner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7fc7b467b6f"
   },
   "source": [
    "## Save the model to GCS path\n",
    "<a name=\"section-8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ae43e214775f"
   },
   "source": [
    "Save the trained model to a local file \"model.joblib\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94ad4c932974"
   },
   "outputs": [],
   "source": [
    "FILE_NAME =  \"[your-file-name]\"\n",
    "joblib.dump(model, FILE_NAME)\n",
    "\n",
    "# Upload the saved model file to GCS\n",
    "BLOB_PATH = (\n",
    "    \"[your-blob-path]\"  # leave blank if no folders inside the bucket are needed.\n",
    ")\n",
    "\n",
    "\n",
    "BLOB_NAME = BLOB_PATH + FILE_NAME\n",
    "\n",
    "bucket = storage.Client().bucket(BUCKET_NAME)\n",
    "blob = bucket.blob(BLOB_NAME)\n",
    "blob.upload_from_filename(FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aa045b28545b"
   },
   "source": [
    "## Create a model with EXplainable-AI(XAI) support in Vertex-AI\n",
    "<a name=\"section-9\"></a>\n",
    "\n",
    "Before creating a model, configure the explanations for the model so that deployed model. For further details, please refer [Configuring-Explanations in Vertex-AI](https://cloud.google.com/vertex-ai/docs/explainable-ai/configuring-explanations#scikit-learn-and-xgboost-pre-built-containers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a90b693b0a17"
   },
   "outputs": [],
   "source": [
    "MODEL_DISPLAY_NAME = \"[your-model-display-name]\"\n",
    "ARTIFACT_GCS_PATH = f\"gs://{BUCKET_NAME}/{BLOB_PATH}\"\n",
    "\n",
    "# Feature-name(Inp_feature) and Output-name(Model_output) can be arbitrary\n",
    "exp_metadata = {\"inputs\": {\"Inp_feature\": {}}, \"outputs\": {\"Model_output\": {}}}\n",
    "\n",
    "with open(\"explanation-metadata.json\", \"w\") as fp:\n",
    "    json.dump(exp_metadata, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3db48702657f"
   },
   "outputs": [],
   "source": [
    "! gcloud beta ai models upload \\\n",
    "  --region=$REGION \\\n",
    "  --display-name=$MODEL_DISPLAY_NAME \\\n",
    "  --container-image-uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-24:latest\" \\\n",
    "  --artifact-uri=$ARTIFACT_GCS_PATH \\\n",
    "  --explanation-method=sampled-shapley \\\n",
    "  --explanation-path-count=25 \\\n",
    "  --explanation-metadata-file=explanation-metadata.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34c83135a85d"
   },
   "source": [
    "Get the MODEL_ID from the created model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5a757299183c"
   },
   "outputs": [],
   "source": [
    "! gcloud beta ai models list \\\n",
    "  --region $REGION \\\n",
    "  --filter=display_name=$MODEL_DISPLAY_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43809d29263a"
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"80985628455469056\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ab34c4b7183c"
   },
   "source": [
    "### Create an Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2330cab1093"
   },
   "outputs": [],
   "source": [
    "ENDPOINT_DISPLAY_NAME = \"[your-endpoint-display-name]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39d0c55a362d"
   },
   "outputs": [],
   "source": [
    "! gcloud beta ai endpoints create \\\n",
    "  --region=$REGION \\\n",
    "  --display-name=$ENDPOINT_DISPLAY_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ae4c69ef8a8c"
   },
   "source": [
    "Verify that the endpoint has been created and save the ENDPOINT_ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5c11c1ddc679"
   },
   "outputs": [],
   "source": [
    "! gcloud beta ai endpoints list \\\n",
    "  --region=$REGION \\\n",
    "  --filter=display_name=$ENDPOINT_DISPLAY_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6aa73d9a88d3"
   },
   "outputs": [],
   "source": [
    "ENDPOINT_ID = \"[your-endpoint-id]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84a64b2a7399"
   },
   "source": [
    "### Deploy the model to the created endpoint\n",
    "\n",
    "Configure the depoyment name, machine-type and other parameters for the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eb1c5e1a2fdf"
   },
   "outputs": [],
   "source": [
    "DEPLOYED_MODEL_NAME = \"[deployment-model-name]\"\n",
    "MACHINE_TYPE = \"[your-machine-type]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ccc83cb2281f"
   },
   "outputs": [],
   "source": [
    "! gcloud beta ai endpoints deploy-model $ENDPOINT_ID\\\n",
    "  --region=$REGION \\\n",
    "  --model=$MODEL_ID \\\n",
    "  --display-name=$DEPLOYED_MODEL_NAME \\\n",
    "  --machine-type=$MACHINE_TYPE \\\n",
    "  --traffic-split=0=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9bb578a29d01"
   },
   "outputs": [],
   "source": [
    "# Save the ID of the deployed model\n",
    "DEPLOYED_MODEL_ID = \"[your-deployed-model-id]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21a50d4e9946"
   },
   "source": [
    "### Get Explanations from the deployed model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b50c31e0552"
   },
   "source": [
    "Get Explanations for some test instances from the hosted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52cb8915b41c"
   },
   "outputs": [],
   "source": [
    "# format the top 2 test instances as the request's payload\n",
    "test_json = {\"instances\": [X_test.iloc[0].tolist(), X_test.iloc[1].tolist()]}\n",
    "with open(\"request.json\", \"w\") as fp:\n",
    "    json.dump(test_json, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ce49e1e6045d"
   },
   "outputs": [],
   "source": [
    "! gcloud beta ai endpoints explain $ENDPOINT_ID \\\n",
    "  --region=$REGION \\\n",
    "  --json-request=request.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1e476d770667"
   },
   "source": [
    "### Plot the feature attributions from the Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "81804daa141e"
   },
   "outputs": [],
   "source": [
    "features = X_train.columns.to_list()\n",
    "\n",
    "\n",
    "def plot_attributions(attrs):\n",
    "    \"\"\"\n",
    "    Function to plot the features and their attributions for an instance\n",
    "    \"\"\"\n",
    "    rows = {\"feature_name\": [], \"attribution\": []}\n",
    "    for i, val in enumerate(features):\n",
    "        rows[\"feature_name\"].append(val)\n",
    "        rows[\"attribution\"].append(attrs[\"Inp_feature\"][i])\n",
    "    attr_df = pd.DataFrame(rows).set_index(\"feature_name\")\n",
    "    attr_df.plot(kind=\"bar\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "def explain_tabular_sample(\n",
    "    project: str, location: str, endpoint_id: str, instances: list\n",
    "):\n",
    "    \"\"\"\n",
    "    Function to make an explanation request for the specified payload and generate feature attribution plots\n",
    "    \"\"\"\n",
    "    aiplatform.init(project=project, location=location)\n",
    "\n",
    "    endpoint = aiplatform.Endpoint(endpoint_id)\n",
    "\n",
    "    response = endpoint.explain(instances=instances)\n",
    "    print(\"#\" * 10 + \"Explanations\" + \"#\" * 10)\n",
    "    for explanation in response.explanations:\n",
    "        print(\" explanation\")\n",
    "        # Feature attributions.\n",
    "        attributions = explanation.attributions\n",
    "\n",
    "        for attribution in attributions:\n",
    "            print(\"  attribution\")\n",
    "            print(\"   baseline_output_value:\", attribution.baseline_output_value)\n",
    "            print(\"   instance_output_value:\", attribution.instance_output_value)\n",
    "            print(\"   output_display_name:\", attribution.output_display_name)\n",
    "            print(\"   approximation_error:\", attribution.approximation_error)\n",
    "            print(\"   output_name:\", attribution.output_name)\n",
    "            output_index = attribution.output_index\n",
    "            for output_index in output_index:\n",
    "                print(\"   output_index:\", output_index)\n",
    "\n",
    "            plot_attributions(attribution.feature_attributions)\n",
    "\n",
    "    print(\"#\" * 10 + \"Predictions\" + \"#\" * 10)\n",
    "    for prediction in response.predictions:\n",
    "        print(prediction)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "test_json = [X_test.iloc[0].tolist(), X_test.iloc[1].tolist()]\n",
    "prediction = explain_tabular_sample(\"vertex-ai-dev\", REGION, ENDPOINT_ID, test_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20dd765c9e30"
   },
   "source": [
    "## Cleaning up\n",
    "<a name=\"section-10\"></a>\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a14413a0184c"
   },
   "outputs": [],
   "source": [
    "# Undeploy the model resource\n",
    "! gcloud ai endpoints undeploy-model $ENDPOINT_ID --deployed-model-id=$DEPLOYED_MODEL_ID --quiet --region=$REGION\n",
    "\n",
    "# Delete endpoint resource\n",
    "! gcloud ai endpoints delete $ENDPOINT_ID --quiet --region=$REGION\n",
    "\n",
    "# Delete model resource\n",
    "! gcloud ai models delete $MODEL_ID --quiet --region=$REGION\n",
    "\n",
    "# Delete Cloud Storage objects that were created\n",
    "# gsutil -m rm -r $BUCKET_NAME"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Subscriber-Churn-notebook.ipynb",
   "toc_visible": true
  },
  "environment": {
   "name": "managed-notebooks.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/managed-notebooks:m80"
  },
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

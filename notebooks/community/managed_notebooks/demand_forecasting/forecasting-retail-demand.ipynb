{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a81007f6093"
      },
      "source": [
        "# Forecasting Retail Demand with Vertex AI and BQML "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "033afdb243c4"
      },
      "source": [
        "## Table Of Contents\n",
        "* [Overview](#section-1)\n",
        "* [Dataset](#section-2)\n",
        "* [Objective](#section-3)\n",
        "* [Costs](#section-4)\n",
        "* [Explore Data](#section-5)\n",
        "* [Modeling with BigQuery and the ARIMA model](#section-6)\n",
        "* [Evaluating model](#section-7)\n",
        "* [Executor](#section-8)\n",
        "* [Clean Up](#section-9)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc1f01b84273"
      },
      "source": [
        "## Overview \n",
        "<a name=\"section-1\"></a>\n",
        "\n",
        "\n",
        "In this notebook, we explore demand forecasting using bigquery public retail dataset. Being able to measure and forecast customer demand can help retailers better understand their customers, stock shelves with the right products, offer targeted promotions, and generally, better plan and manage their budgets. We apply an ARIMA model (Autoregressive integrated moving average) from BQML on retail data. This notebook demonstrates how to train and evaluate a BQML model for demand forecasting datasets, extract actionable future insights.\n",
        "\n",
        "You have to follow this steps before going through this notebook inside Vertex-AI notebook instance :\n",
        "* Create a managed notebook instance in Vertex-AI with python3 (\"NumPy/SciPy/scikit-learn\") environment on a n1-standard4 instance.\n",
        "\n",
        "\n",
        "#### ARIMA Modeling with BQML \n",
        "\n",
        "The <a href='https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average'>ARIMA model</a> is designed to analyze historical data, spot patterns over time, and project them into the future, i.e. forecasting. The model is available inside BigQuery ML and enables users to create and execute machine learning models directly in BigQuery using SQL queries. Working with BQML is advantageous, as it already has access to the data, it can handle most of the modeling details automatically if desired, and will store both the model and any predictions also inside BigQuery. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "286d39afc239"
      },
      "source": [
        "## Dataset \n",
        "<a name=\"section-2\"></a>\n",
        "\n",
        "This notebook uses the bigquery public retail data set.\n",
        "The data covers 10 US stores and includes item level, department, product categories, and store details. In addition, it has explanatory variables such as price, gross_margin. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbab58d4ae1a"
      },
      "source": [
        "## Objective\n",
        "<a name=\"section-3\"></a>\n",
        "In this tutorial we follow below steps\n",
        "* Explore Data\n",
        "* Model with BigQuery and the ARIMA model\n",
        "* Evaluate the model\n",
        "* Evaluate the Model Results using BQML (on Training Data)\n",
        "* Evalute the Model Results - MAE, MAPE, MSE, RMSE (on Test data)\n",
        "* Executor feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17e0532066d7"
      },
      "source": [
        "## Costs\n",
        "<a name=\"section-4\"></a>\n",
        "This tutorial uses the following billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Bigquery\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing), [Bigquery\n",
        "pricing](https://cloud.google.com/bigquery/pricing) and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75568876f1a4"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "**Note:** This notebook does not require a GPU runtime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "276011faac6f"
      },
      "source": [
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "1. [Enable the Vertex AI, Cloud Storage, and Compute Engine APIs](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component,storage-component.googleapis.com). \n",
        "\n",
        "1. Follow the \"**Configuring your project**\" instructions from the Vertex Pipelines documentation.\n",
        "\n",
        "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
        "\n",
        "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3b3a6979860"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b453e095976a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"\"\n",
        "\n",
        "# Get your Google Cloud project ID from gcloud\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID: \", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4c6d0a9e66c"
      },
      "source": [
        "Otherwise, set your project ID here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eab2101e413d"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
        "    PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07fc8daffbf9"
      },
      "source": [
        "#### Timestamp\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append it onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abc79bf099b2"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4edb2c269d4"
      },
      "source": [
        "## Import libraries and define constants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b81a2c71fa3a"
      },
      "source": [
        "Load the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c64de5cbad25"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.cloud.bigquery import Client\n",
        "from sklearn.metrics import (mean_absolute_error,\n",
        "                             mean_absolute_percentage_error,\n",
        "                             mean_squared_error)\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7592011d7825"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"\n",
        "DATASET = \"[your-dataset-id]\"\n",
        "SALES_TABLE = \"[your-bigquery-table-name]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36d3a8aec700"
      },
      "source": [
        "Create a bigquery datatset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ce9876ee882"
      },
      "source": [
        "#@bigquery\n",
        "CREATE SCHEMA demandforecasting\n",
        "OPTIONS(\n",
        "  location=\"us\"\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7002b223b2b5"
      },
      "source": [
        "## Explore the Data\n",
        "<a name=\"section-5\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a3ee00b6b9f"
      },
      "source": [
        "Viewing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b134d3155d5"
      },
      "source": [
        "#@bigquery\n",
        "SELECT * FROM `looker-private-demo.retail.transaction_detail`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d52803e14f2"
      },
      "source": [
        "Creating a view in which we will ve extracting the important fields and will be selecting store with id 10 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc37b15df336"
      },
      "source": [
        "#@bigquery\n",
        "CREATE OR REPLACE VIEW demandforecasting.important_fields AS\n",
        "(\n",
        "    SELECT transaction_timestamp,line_items from `looker-private-demo.retail.transaction_detail` WHERE store_id = 10\n",
        ")   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9ca8a8bbf82"
      },
      "source": [
        "Viewing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ffe2d8e816c"
      },
      "source": [
        "#@bigquery\n",
        "SELECT * FROM demandforecasting.important_fields"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f03b43de905"
      },
      "source": [
        "Converting timestamp to date for transaction_timestamp column "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "221986867dbd"
      },
      "source": [
        "#@bigquery\n",
        "CREATE OR REPLACE VIEW demandforecasting.data_after_converting_timestamp_to_date AS\n",
        "(\n",
        "    SELECT EXTRACT(DATE FROM transaction_timestamp AT TIME ZONE \"UTC\") AS date,line_items from demandforecasting.important_fields\n",
        ")   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0babc4a23fa"
      },
      "source": [
        "Viewing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2181af3cae88"
      },
      "source": [
        "#@bigquery\n",
        "SELECT * FROM demandforecasting.data_after_converting_timestamp_to_date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d3a85f95f9d"
      },
      "outputs": [],
      "source": [
        "# The following two lines are only necessary to run once.\n",
        "# Comment out otherwise for speed-up.\n",
        "\n",
        "client = Client()\n",
        "\n",
        "query = \"\"\"SELECT * FROM demandforecasting.data_after_converting_timestamp_to_date\"\"\"\n",
        "job = client.query(query)\n",
        "df_intermediary = job.to_dataframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40a78a81c379"
      },
      "outputs": [],
      "source": [
        "df_intermediary.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2f685f15000"
      },
      "source": [
        "Line items is array of structs. We have to split each array into individual items\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caf01494d657"
      },
      "source": [
        "#@bigquery\n",
        "CREATE OR REPLACE VIEW demandforecasting.split_array_of_structs AS\n",
        " \n",
        "(SELECT date,line_items\n",
        "FROM demandforecasting.data_after_converting_timestamp_to_date, UNNEST(line_items) AS line_items)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c677762b34a"
      },
      "source": [
        "Viewing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cbbded0bf50"
      },
      "source": [
        "#@bigquery\n",
        "SELECT * FROM demandforecasting.split_array_of_structs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7acc57476f1c"
      },
      "source": [
        "We want only product_id column so ignoring other columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "704e7c73ec0c"
      },
      "source": [
        "#@bigquery\n",
        "CREATE OR REPLACE VIEW demandforecasting.splitting_struct_columns AS\n",
        " \n",
        "(SELECT date,line_items.product_id as product_id\n",
        "FROM demandforecasting.split_array_of_structs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f9d72c483e8"
      },
      "source": [
        "Viewing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6103f7ff48ff"
      },
      "source": [
        "#@bigquery\n",
        "SELECT * FROM demandforecasting.splitting_struct_columns "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c0c4245acb7"
      },
      "source": [
        "Here we are counting the sales of a product for each date"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "368f75a291d1"
      },
      "source": [
        "#@bigquery\n",
        "CREATE OR REPLACE VIEW demandforecasting.sales_count_per_date AS\n",
        " \n",
        "(SELECT date,product_id,COUNT(*) as sales_count\n",
        "FROM demandforecasting.splitting_struct_columns GROUP BY date,product_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7291d3970a2c"
      },
      "source": [
        "#@bigquery\n",
        "SELECT * FROM demandforecasting.sales_count_per_date"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "722fd013c28c"
      },
      "source": [
        "Here we are creating a view for top five sold products. The products are chosen for which the sum of sales over all dates is highest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97db6ee61087"
      },
      "source": [
        "#@bigquery\n",
        "CREATE OR REPLACE VIEW demandforecasting.top_five_products AS (\n",
        "    WITH topsellingitems AS(\n",
        "         SELECT \n",
        "            product_id,\n",
        "            sum(sales_count) sum_sales\n",
        "        FROM\n",
        "            `demandforecasting.sales_count_per_date` \n",
        "        GROUP BY \n",
        "            product_id\n",
        "        ORDER BY sum_sales DESC\n",
        "        LIMIT 5 #Top N\n",
        "    )\n",
        "    SELECT \n",
        "        date,\n",
        "        product_id,\n",
        "        sales_count\n",
        "    FROM\n",
        "        `demandforecasting.sales_count_per_date` \n",
        "    WHERE\n",
        "        product_id IN (SELECT product_id FROM topsellingitems)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "182ce3346563"
      },
      "source": [
        "#@bigquery\n",
        "SELECT * FROM demandforecasting.top_five_products\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "495c76e598a6"
      },
      "outputs": [],
      "source": [
        "# The following two lines are only necessary to run once.\n",
        "# Comment out otherwise for speed-up.\n",
        "\n",
        "client = Client()\n",
        "\n",
        "query = \"\"\"SELECT * FROM demandforecasting.top_five_products\"\"\"\n",
        "job = client.query(query)\n",
        "df = job.to_dataframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d11dc64ae2a"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb63bbdcead4"
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef761dcd0109"
      },
      "source": [
        "Convert date column data type from object to datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0534d564d09e"
      },
      "outputs": [],
      "source": [
        "df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y-%m-%d\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ec5fa4b1ea0"
      },
      "source": [
        "Here we have sales of products on only specific dates. For forecasting model we want continuous dates. So for each product we are putting zero for  sales_count column on dates which the product is not sold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dc20e915cd0"
      },
      "source": [
        "Here we are creating a dates dataframe from lowest date of all dates to highest date of all dates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd6423ef739f"
      },
      "source": [
        "#@bigquery\n",
        "SELECT MIN(DATE) FROM demandforecasting.top_five_products"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e09f3221d77"
      },
      "source": [
        "#@bigquery\n",
        "SELECT MAX(DATE) FROM demandforecasting.top_five_products"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d41c099464ac"
      },
      "outputs": [],
      "source": [
        "dates = pd.date_range(start=\"2016-12-17\", end=\"2021-10-06\").to_frame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b1f67e375fc"
      },
      "outputs": [],
      "source": [
        "dates.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49eb81d44b65"
      },
      "source": [
        "For top five products we follow below step\n",
        "* Select a product_id, merge the product product_id rows with dates dataframe so that we will get a continuous dates dataframe for each product\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7194f59c71b5"
      },
      "outputs": [],
      "source": [
        "print(\"data before for a product with product_id 20552\")\n",
        "df.loc[df[\"product_id\"] == 20552].sort_values(by=[\"date\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "326f4f48f5cc"
      },
      "source": [
        "As we can see from 2016-12-17 to 2016-12-30 there are no dates displayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0b443c8d160"
      },
      "outputs": [],
      "source": [
        "df1 = (\n",
        "    pd.merge(\n",
        "        df.loc[df[\"product_id\"] == 20552],\n",
        "        dates,\n",
        "        left_on=\"date\",\n",
        "        right_on=0,\n",
        "        how=\"outer\",\n",
        "    )\n",
        "    .sort_values(by=[\"date\"])\n",
        "    .drop(columns=0)\n",
        ")  # merging dates dataframe with product_id matching rows\n",
        "df1[\"product_id\"] = 20552  # product_id will be null so making it the specified values\n",
        "df1.reset_index(inplace=True, drop=True)  # making index to start from 0\n",
        "df1 = df1.fillna(0)  # for sales_count making null values as 0\n",
        "df1[\"sales_count\"] = pd.to_numeric(\n",
        "    df1[\"sales_count\"], downcast=\"integer\"\n",
        ")  # convert sales_count column to integer\n",
        "print(\"data after converting for a product with product_id 20552\")\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4074c59c4fcc"
      },
      "outputs": [],
      "source": [
        "df2 = (\n",
        "    pd.merge(\n",
        "        df.loc[df[\"product_id\"] == 13596],\n",
        "        dates,\n",
        "        left_on=\"date\",\n",
        "        right_on=0,\n",
        "        how=\"outer\",\n",
        "    )\n",
        "    .sort_values(by=[\"date\"])\n",
        "    .drop(columns=0)\n",
        ")  # merging dates dataframe with product_id matching rows\n",
        "df2[\"product_id\"] = 13596  # product_id will be null so making it the specified values\n",
        "df2.reset_index(inplace=True, drop=True)  # making index to start from 0\n",
        "df2 = df2.fillna(0)  # for sales_count making null values as 0\n",
        "df2[\"sales_count\"] = pd.to_numeric(\n",
        "    df2[\"sales_count\"], downcast=\"integer\"\n",
        ")  # convert sales_count column to integer\n",
        "df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "589ffdbf42f1"
      },
      "outputs": [],
      "source": [
        "df3 = (\n",
        "    pd.merge(\n",
        "        df.loc[df[\"product_id\"] == 23641],\n",
        "        dates,\n",
        "        left_on=\"date\",\n",
        "        right_on=0,\n",
        "        how=\"outer\",\n",
        "    )\n",
        "    .sort_values(by=[\"date\"])\n",
        "    .drop(columns=0)\n",
        ")  # merging dates dataframe with product_id matching rows\n",
        "df3[\"product_id\"] = 23641  # product_id will be null so making it the specified values\n",
        "df3.reset_index(inplace=True, drop=True)  # making index to start from 0\n",
        "df3 = df3.fillna(0)  # for sales_count making null values as 0\n",
        "df3[\"sales_count\"] = pd.to_numeric(\n",
        "    df3[\"sales_count\"], downcast=\"integer\"\n",
        ")  # convert sales_count column to integer\n",
        "df3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "df19f886f7b7"
      },
      "outputs": [],
      "source": [
        "df4 = (\n",
        "    pd.merge(\n",
        "        df.loc[df[\"product_id\"] == 28305],\n",
        "        dates,\n",
        "        left_on=\"date\",\n",
        "        right_on=0,\n",
        "        how=\"outer\",\n",
        "    )\n",
        "    .sort_values(by=[\"date\"])\n",
        "    .drop(columns=0)\n",
        ")  # merging dates dataframe with product_id matching rows\n",
        "df4[\"product_id\"] = 28305  # product_id will be null so making it the specified values\n",
        "df4.reset_index(inplace=True, drop=True)  # making index to start from 0\n",
        "df4 = df4.fillna(0)  # for sales_count making null values as 0\n",
        "df4[\"sales_count\"] = pd.to_numeric(\n",
        "    df4[\"sales_count\"], downcast=\"integer\"\n",
        ")  # convert sales_count column to integer\n",
        "df4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bde5882369e"
      },
      "outputs": [],
      "source": [
        "df5 = (\n",
        "    pd.merge(\n",
        "        df.loc[df[\"product_id\"] == 20547],\n",
        "        dates,\n",
        "        left_on=\"date\",\n",
        "        right_on=0,\n",
        "        how=\"outer\",\n",
        "    )\n",
        "    .sort_values(by=[\"date\"])\n",
        "    .drop(columns=0)\n",
        ")  # merging dates dataframe with product_id matching rows\n",
        "df5[\"product_id\"] = 20547  # product_id will be null so making it the specified values\n",
        "df5.reset_index(inplace=True, drop=True)  # making index to start from 0\n",
        "df5 = df5.fillna(0)  # for sales_count making null values as 0\n",
        "df5[\"sales_count\"] = pd.to_numeric(\n",
        "    df5[\"sales_count\"], downcast=\"integer\"\n",
        ")  # convert sales_count column to integer\n",
        "df5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a2d033b733e"
      },
      "source": [
        "Merge all the dataframes into a new dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bf570c0c6913"
      },
      "outputs": [],
      "source": [
        "pdList = [df1, df2, df3, df4, df5]  # List of your dataframes\n",
        "new_df = pd.concat(pdList)\n",
        "new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d258952fc71"
      },
      "outputs": [],
      "source": [
        "new_df.reset_index(inplace=True, drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5312dfe1be15"
      },
      "outputs": [],
      "source": [
        "new_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97e1289b2106"
      },
      "source": [
        "Seeing the different product id's to plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5891c2082a4b"
      },
      "source": [
        "#@bigquery\n",
        "SELECT DISTINCT product_id from demandforecasting.top_five_products"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa06b0e893cb"
      },
      "source": [
        "Plotting sales for a specific product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cdce0080aa1"
      },
      "outputs": [],
      "source": [
        "plt.plot(\n",
        "    new_df.loc[new_df[\"product_id\"] == 20552][\"date\"],\n",
        "    new_df.loc[new_df[\"product_id\"] == 20552][\"sales_count\"],\n",
        ")\n",
        "plt.xticks(rotation=\"vertical\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fef99223f63"
      },
      "outputs": [],
      "source": [
        "plt.plot(\n",
        "    new_df.loc[new_df[\"product_id\"] == 20547][\"date\"],\n",
        "    new_df.loc[new_df[\"product_id\"] == 20547][\"sales_count\"],\n",
        ")\n",
        "plt.xticks(rotation=\"vertical\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d281151b0e2a"
      },
      "outputs": [],
      "source": [
        "plt.plot(\n",
        "    new_df.loc[new_df[\"product_id\"] == 28305][\"date\"],\n",
        "    new_df.loc[new_df[\"product_id\"] == 28305][\"sales_count\"],\n",
        ")\n",
        "plt.xticks(rotation=\"vertical\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a50eeeeaedb0"
      },
      "outputs": [],
      "source": [
        "plt.plot(\n",
        "    new_df.loc[new_df[\"product_id\"] == 23641][\"date\"],\n",
        "    new_df.loc[new_df[\"product_id\"] == 23641][\"sales_count\"],\n",
        ")\n",
        "plt.xticks(rotation=\"vertical\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d002f0717ee"
      },
      "outputs": [],
      "source": [
        "plt.plot(\n",
        "    new_df.loc[new_df[\"product_id\"] == 13596][\"date\"],\n",
        "    new_df.loc[new_df[\"product_id\"] == 13596][\"sales_count\"],\n",
        ")\n",
        "plt.xticks(rotation=\"vertical\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "755a811fbad5"
      },
      "source": [
        "Checking data types for new_df dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "220ec814875c"
      },
      "outputs": [],
      "source": [
        "new_df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff06b56ffa7a"
      },
      "source": [
        "Creating a big query table out of new_df dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d303f1cfd90"
      },
      "outputs": [],
      "source": [
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    # Specify a (partial) schema. All columns are always written to the\n",
        "    # table. The schema is used to assist in data type definitions.\n",
        "    schema=[\n",
        "        bigquery.SchemaField(\"product_id\", bigquery.enums.SqlTypeNames.INTEGER),\n",
        "        bigquery.SchemaField(\"date\", bigquery.enums.SqlTypeNames.DATE),\n",
        "        bigquery.SchemaField(\"sales_count\", bigquery.enums.SqlTypeNames.INTEGER),\n",
        "    ],\n",
        "    # Optionally, set the write disposition. BigQuery appends loaded rows\n",
        "    # to an existing table by default, but with WRITE_TRUNCATE write\n",
        "    # disposition it replaces the table with the loaded data.\n",
        "    write_disposition=\"WRITE_TRUNCATE\",\n",
        ")\n",
        "\n",
        "# save the dataframe to a table in the created dataset\n",
        "job = bq_client.load_table_from_dataframe(\n",
        "    new_df, \"{}.{}.{}\".format(PROJECT_ID, DATASET, SALES_TABLE), job_config=job_config\n",
        ")  # Make an API request.\n",
        "job.result()  # Wait for the job to complete."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2e0e9aa67cd"
      },
      "source": [
        "Creating training data by choosing starting and ending data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1fffed34704"
      },
      "outputs": [],
      "source": [
        "# select the date-range and item-id(top 5) for training-data and create a table for the same\n",
        "TRAININGDATA_STARTDATE = \"2016-12-17\"\n",
        "TRAININGDATA_ENDDATE = \"2021-6-01\"\n",
        "query = \"\"\"\n",
        "CREATE OR REPLACE TABLE {DATASET}.training_data AS (\n",
        "    SELECT\n",
        "        *\n",
        "    FROM\n",
        "        `{DATASET}.{SALES_TABLE}`\n",
        "    WHERE\n",
        "        date BETWEEN '{STARTDATE}' AND '{ENDDATE}'\n",
        "        );\n",
        "\"\"\".format(\n",
        "    STARTDATE=TRAININGDATA_STARTDATE,\n",
        "    ENDDATE=TRAININGDATA_ENDDATE,\n",
        "    DATASET=DATASET,\n",
        "    SALES_TABLE=SALES_TABLE,\n",
        ")\n",
        "# execute the query (as it is a create query, there won't be any tabular output)\n",
        "_ = bq_client.query(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f7d9d2d4229"
      },
      "source": [
        "Select the original data for plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b82fc7c7bf6"
      },
      "outputs": [],
      "source": [
        "df_historical = new_df[\n",
        "    (new_df[\"date\"] >= pd.to_datetime(TRAININGDATA_STARTDATE))\n",
        "    & (new_df[\"date\"] <= pd.to_datetime(TRAININGDATA_ENDDATE))\n",
        "].copy()\n",
        "df_historical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c25b75dc957a"
      },
      "source": [
        "## Modeling with BigQuery and the ARIMA model\n",
        "<a name=\"section-6\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69ac23f216a4"
      },
      "source": [
        "#@bigquery\n",
        "CREATE OR REPLACE MODEL demandforecasting.arima_model\n",
        "\n",
        "OPTIONS(\n",
        "  MODEL_TYPE='ARIMA',\n",
        "  TIME_SERIES_TIMESTAMP_COL='date', \n",
        "  TIME_SERIES_DATA_COL='sales_count',\n",
        "  TIME_SERIES_ID_COL='product_id',\n",
        "  HOLIDAY_REGION='US'\n",
        "    \n",
        ") AS\n",
        "\n",
        "SELECT \n",
        "    date,\n",
        "    product_id,\n",
        "    sales_count\n",
        "FROM\n",
        "  demandforecasting.training_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00638d9f7e55"
      },
      "outputs": [],
      "source": [
        "# Train an ARIMA model on the created dataset\n",
        "query = \"\"\"\n",
        "CREATE OR REPLACE MODEL `{PROJECT_ID}.{DATASET}.arima_model`\n",
        "\n",
        "OPTIONS(\n",
        "  MODEL_TYPE='ARIMA',\n",
        "  TIME_SERIES_TIMESTAMP_COL='date',\n",
        "  TIME_SERIES_DATA_COL='sales_count',\n",
        "  TIME_SERIES_ID_COL='product_id') AS\n",
        "\n",
        "SELECT\n",
        "    date,\n",
        "    product_id,\n",
        "    sales_count\n",
        "FROM\n",
        "  `{PROJECT_ID}.{DATASET}.training_data`\n",
        "\"\"\".format(\n",
        "    PROJECT_ID=PROJECT_ID, DATASET=DATASET\n",
        ")\n",
        "# execute the query\n",
        "job = bq_client.query(query)\n",
        "job.result()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a02551627598"
      },
      "source": [
        "# Evaluating the model\n",
        "<a name=\"section-7\"></a>\n",
        "\n",
        "\n",
        "To evaluate the trained model, lets get forecasts for 90 days from the last date of the training-data. In BQML, we can provide the number of days of forecast we need using the \"HORIZON\" argument. Apart from number of days of forecast, we can also specify the confidence interval for the forecast by specifying \"CONFIDENCE_LEVEL\" argument."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87ef4ba64a3a"
      },
      "source": [
        "#@bigquery dfforecast \n",
        "\n",
        "DECLARE HORIZON STRING DEFAULT \"90\";\n",
        "DECLARE CONFIDENCE_LEVEL STRING DEFAULT \"0.90\";\n",
        "\n",
        "EXECUTE IMMEDIATE format('''\n",
        "    SELECT\n",
        "      *\n",
        "    FROM\n",
        "      ML.FORECAST(MODEL demandforecasting.arima_model,\n",
        "                  STRUCT(%s AS horizon,\n",
        "                         %s AS confidence_level)\n",
        "                 )\n",
        "    ''',HORIZON,CONFIDENCE_LEVEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "801528e3e2c7"
      },
      "source": [
        "Load the data into a dataframe \"dfforecast\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1277e846d10"
      },
      "outputs": [],
      "source": [
        "client = Client()\n",
        "\n",
        "query = '''DECLARE HORIZON STRING DEFAULT \"90\"; #number of values to forecast\n",
        "DECLARE CONFIDENCE_LEVEL STRING DEFAULT \"0.90\"; ## required confidence level\n",
        "\n",
        "EXECUTE IMMEDIATE format(\"\"\"\n",
        "    SELECT\n",
        "      *\n",
        "    FROM\n",
        "      ML.FORECAST(MODEL demandforecasting.arima_model,\n",
        "                  STRUCT(%s AS horizon,\n",
        "                         %s AS confidence_level)\n",
        "                 )\n",
        "    \"\"\",HORIZON,CONFIDENCE_LEVEL)'''\n",
        "job = client.query(query)\n",
        "dfforecast = job.to_dataframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83b07f243b72"
      },
      "outputs": [],
      "source": [
        "dfforecast.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e01aa3098f8"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of rows: {dfforecast.shape[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fa36b762a33"
      },
      "source": [
        "Clean the historical and forecasted values for plotting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "073a2dd6a5d3"
      },
      "outputs": [],
      "source": [
        "df_historical.sort_values(by=[\"product_id\", \"date\"], inplace=True)\n",
        "dfforecast.sort_values(by=[\"product_id\", \"forecast_timestamp\"], inplace=True)\n",
        "\n",
        "# Select the actual data to plot against the forecasted data\n",
        "day_diff = (new_df[\"date\"] - pd.to_datetime(TRAININGDATA_ENDDATE)).dt.days\n",
        "df_actual_90d = new_df[new_df[\"product_id\"].isin(dfforecast[\"product_id\"].unique())][\n",
        "    (day_diff > 0) & (day_diff <= 90)\n",
        "].copy()\n",
        "df_actual_90d.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d40a95ad0616"
      },
      "source": [
        "Plot the historical and forecast data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6845ad425d3"
      },
      "outputs": [],
      "source": [
        "def plot_hist_forecast(\n",
        "    historical, forecast, actual, hist_start=\"\", hist_end=\"\", title=\"\"\n",
        "):\n",
        "    if hist_start != \"\":\n",
        "        historical = historical[\n",
        "            historical[\"date\"] >= pd.to_datetime(hist_start, format=\"%Y-%m-%d\")\n",
        "        ].copy()\n",
        "    if hist_end != \"\":\n",
        "        historical = historical[\n",
        "            historical[\"date\"] <= pd.to_datetime(hist_end, format=\"%Y-%m-%d\")\n",
        "        ].copy()\n",
        "\n",
        "    plt.figure(figsize=(15, 4))\n",
        "    plt.plot(historical[\"date\"], historical[\"sales_count\"], label=\"historical\")\n",
        "    # Plot the forecast data\n",
        "    plt.plot(\n",
        "        forecast[\"forecast_timestamp\"],\n",
        "        forecast[\"forecast_value\"],\n",
        "        label=\"forecast\",\n",
        "        linestyle=\"--\",\n",
        "    )\n",
        "    # Plot the actual data\n",
        "    plt.plot(actual[\"date\"], actual[\"sales_count\"], label=\"actual\")\n",
        "    # plot the confidence interval\n",
        "    confidence_level = forecast[\"confidence_level\"].iloc[0] * 100\n",
        "    low_CI = forecast[\"confidence_interval_lower_bound\"]\n",
        "    upper_CI = forecast[\"confidence_interval_upper_bound\"]\n",
        "\n",
        "    # Shade the confidence interval\n",
        "    plt.fill_between(\n",
        "        forecast[\"forecast_timestamp\"],\n",
        "        low_CI,\n",
        "        upper_CI,\n",
        "        color=\"#539caf\",\n",
        "        alpha=0.4,\n",
        "        label=f\"{confidence_level} confidence interval\",\n",
        "    )\n",
        "    plt.legend()\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "    return\n",
        "\n",
        "\n",
        "product_id_list = dfforecast[\"product_id\"].unique()\n",
        "for i in item_id_list:\n",
        "    print(\"Product_id : \", i)\n",
        "    plot_hist_forecast(\n",
        "        df_historical[df_historical[\"product_id\"] == i],\n",
        "        dfforecast[dfforecast[\"product_id\"] == i],\n",
        "        df_actual_90d[df_actual_90d[\"product_id\"] == i],\n",
        "        hist_start=\"2021-02-01\",\n",
        "        title=i,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76b5f6fab0b8"
      },
      "source": [
        "While most of the predictions are looking decent, you can also see that the actual ranges fall into the 90% confidence interval suggested by the model. Under the hood, BQML performs many computationally expensive tasks even considering the seasonal and holiday information.\n",
        "<img src=\"https://cloud.google.com/bigquery-ml/images/BQ_ARIMA_diagram.png\"> <a href=\"https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create-time-series\"> Source</a> </img>\n",
        "\n",
        "The coefficients learned by BQML's ARIMA model can also be checked by simply querying for <a href=\"https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-arima-coefficients\">ARIMA_COEFFICIENTS</a> from the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "934fb81f6ad1"
      },
      "source": [
        "#@bigquery\n",
        "SELECT\n",
        "  *\n",
        "FROM \n",
        "  ML.ARIMA_COEFFICIENTS(MODEL demandforecasting.arima_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f37aa9ddfb8f"
      },
      "source": [
        "In the above results, \n",
        "- <b>product_id</b> column represents the index column we've specified while training the ARIMA model.\n",
        "- <b>ar_coefficients</b> column corresponds to the autoregressive coefficients in the ARIMA algorithm(non-seasonal p).\n",
        "- <b>ma_coefficients</b> refers to the moving average coefficients in the ARIMA algorithm(non-seasonal q).\n",
        "- <b>intercept_or_drift</b> is the constant term in the ARIMA algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27bf1ef88ea7"
      },
      "source": [
        "## Evaluating the model results using BQML \n",
        "<a name=\"section-7-subsection-1\"></a>\n",
        "\n",
        "\n",
        "Nevertheless, BQML also provies us with the <a href=\"https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-evaluate\">ML.EVALUATE</a> function to see check the evaluation metrics of the trained model. For ARIMA model, we can see the model being evaluated on log_likelihood, AIC and variance measures. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4f0ef9b1a04"
      },
      "source": [
        "#@bigquery\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  ML.EVALUATE(MODEL demandforecasting.arima_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3be6d3e1607e"
      },
      "source": [
        "## Evaluting the model results - MAE, MAPE, MSE, RMSE (on Test data)\n",
        "<a name=\"section-7-subsection-2\"></a>\n",
        "\n",
        "\n",
        "Generally, to evaluate forecast model, we have various metrics depending on how we want to evaluate. For starters, we can choose from the following : \n",
        "* <b>Mean Absolute Error (MAE)</b> : Average of the absolute differences between the actual values and the forecasted values.\n",
        "* <b>Mean Absolute Percentage Error (MAPE)</b> : Average of the percentages of absolute difference between the actual and forecasted values to the actual values.\n",
        "* <b>Mean Squared Error (MSE)</b> : Average of squared differences between the actual and forecasted values.\n",
        "* <b>Root Mean Squared Error (RMSE)</b> : Root of MSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d78abc6aee46"
      },
      "outputs": [],
      "source": [
        "df_actual_90d.sort_values(by=[\"product_id\", \"date\"], inplace=True)\n",
        "df_actual_90d.reset_index(drop=True, inplace=True)\n",
        "dfforecast.sort_values(by=[\"product_id\", \"forecast_timestamp\"], inplace=True)\n",
        "dfforecast.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "132bb1acdd75"
      },
      "outputs": [],
      "source": [
        "errors = {\"product_id\": [], \"MAE\": [], \"MAPE\": [], \"MSE\": [], \"RMSE\": []}\n",
        "for i in product_id_list:\n",
        "    mae = mean_absolute_error(\n",
        "        df_actual_90d[df_actual_90d[\"product_id\"] == i][\"sales_count\"],\n",
        "        dfforecast[dfforecast[\"product_id\"] == i][\"forecast_value\"],\n",
        "    )\n",
        "    mape = mean_absolute_percentage_error(\n",
        "        df_actual_90d[df_actual_90d[\"product_id\"] == i][\"sales_count\"],\n",
        "        dfforecast[dfforecast[\"product_id\"] == i][\"forecast_value\"],\n",
        "    )\n",
        "    mse = mean_squared_error(\n",
        "        df_actual_90d[df_actual_90d[\"product_id\"] == i][\"sales_count\"],\n",
        "        dfforecast[dfforecast[\"product_id\"] == i][\"forecast_value\"],\n",
        "        squared=True,\n",
        "    )\n",
        "    rmse = mean_squared_error(\n",
        "        df_actual_90d[df_actual_90d[\"product_id\"] == i][\"sales_count\"],\n",
        "        dfforecast[dfforecast[\"product_id\"] == i][\"forecast_value\"],\n",
        "        squared=False,\n",
        "    )\n",
        "    errors[\"product_id\"].append(i)\n",
        "    errors[\"MAE\"].append(mae)\n",
        "    errors[\"MAPE\"].append(mape)\n",
        "    errors[\"MSE\"].append(mse)\n",
        "    errors[\"RMSE\"].append(rmse)\n",
        "errors = pd.DataFrame(errors)\n",
        "errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d84da670e5ae"
      },
      "source": [
        "From the values obtained for various error measures, you can identify that for product id's 20552, 13596 error measures are high. It has to be noted that these error measures are an aggregate of all the individual forecasts made during the test period and so we get an overall picture of the model's performance over the selected period. Ideally, the lower these error measures, the better the model is good at forecasting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56105a7f9a46"
      },
      "source": [
        "## Executor\n",
        "<a name=\"section-8\"></a>\n",
        "From the navbar you see below select executor option\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "942c4866b758"
      },
      "source": [
        "<img src=\"images/navbar_exe.png\" ></img>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31dba6ec2c0e"
      },
      "source": [
        "Give the execution name. Select the cloud storage bucket,machine type, accelerator type. For environment you can choose python 3 here from the environment options\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a56743fe76f"
      },
      "source": [
        "<img src=\"images/exe_form.png\" style=\"height:500px;width:500px\"></img>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af652028084a"
      },
      "source": [
        "<img src=\"images/python3_env_selection.png\" style=\"height:500px;width:500px\"></img>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d37a2baf139"
      },
      "source": [
        "In type option you can select Schedule-based recurring executions if you want to schelule the execution like every 1 hour."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1981e3e49db"
      },
      "source": [
        "<img src=\"images/schedule.png\" style=\"height:500px;\"></img>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "791a0bfc0cb9"
      },
      "source": [
        "You can see the history of executions by selecting last but one item on the down from the side nav bar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e898ff47f952"
      },
      "source": [
        "<img src=\"images/side_nav.png\" style=\"height:500px;\"></img>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eb4d9d73c57"
      },
      "source": [
        "You can see history of executions here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45447f8e1cc0"
      },
      "source": [
        "<img src=\"images/list_execution.png\" style=\"height:500px;width:500px\"></img>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb08d55eaec1"
      },
      "source": [
        "You can see history of schedules here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c32707e57e4"
      },
      "source": [
        "<img src=\"images/list_schedule.png\" style=\"height:500px;width:500px\"></img>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23ee85f65a0b"
      },
      "source": [
        "## Clean up\n",
        "<a name=\"section-9\"></a>\n",
        "\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial(The following code deletes entire dataset).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b70ac0dc210"
      },
      "outputs": [],
      "source": [
        "# Construct a BigQuery client object.\n",
        "client = bigquery.Client()\n",
        "\n",
        "# TODO(developer): Set model_id to the ID of the model to fetch.\n",
        "dataset_id = \"your-project-id.dataset-name\"\n",
        "\n",
        "# Use the delete_contents parameter to delete a dataset and its contents.\n",
        "# Use the not_found_ok parameter to not receive an error if the dataset has already been deleted.\n",
        "client.delete_dataset(\n",
        "    dataset_id, delete_contents=True, not_found_ok=True\n",
        ")  # Make an API request.\n",
        "\n",
        "print(\"Deleted dataset '{}'.\".format(dataset_id))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "forecasting-retail-demand.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1531e5f4-57fe-4943-8db9-336d9efeb225",
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b929d4b-984a-44b3-a73b-88977f422bec",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://github.com/googleapis/python-aiplatform/blob/custom-prediction-routine/samples/notebooks/prediction/SDK_Triton_PyTorch_Local_Prediction.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe3a5bf-d610-4c9c-b069-6ba015992479",
   "metadata": {
    "id": "tvgnzT1CKxrO",
    "tags": []
   },
   "source": [
    "## Overview\n",
    "\n",
    "This tutorial demonstrates how to use Vertex AI SDK to locally test [NVIDIA Triton inference server](https://developer.nvidia.com/nvidia-triton-inference-server) to serve a PyTorch model and deploy it to Vertex AI Predictions. This is currently an **experimental** feature and is not yet officially supported by the Vertex AI SDK. In this tutorial, we'll be installing the Vertex AI SDK from an experimental branch on github. \n",
    "\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "This tutorial uses R.A. Fisher's Iris dataset, a small dataset that is popular for trying out machine learning techniques. Each instance has four numerical features, which are different measurements of a flower, and a target label that\n",
    "marks it as one of three types of iris: Iris setosa, Iris versicolour, or Iris virginica.\n",
    "\n",
    "This tutorial uses [the Iris dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/iris).\n",
    "\n",
    "### Objective\n",
    "\n",
    "The goal is to:\n",
    "- Train a model that uses a flower's measurements as input to predict what type of iris it is with PyTorch.\n",
    "- Save the model.\n",
    "- Test the NVIDIA Triton inference server locally.\n",
    "- Upload and deploy Triton inference server as custom container to Vertex Prediction.\n",
    "\n",
    "This tutorial focuses more on deploying this model with Vertex AI than on\n",
    "the design of the model itself.\n",
    "\n",
    "### Costs \n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28096662-ec20-4e46-bfe0-949de3681464",
   "metadata": {
    "id": "ze4-nDLfK4pw"
   },
   "source": [
    "### Set up your local development environment\n",
    "\n",
    "**If you are using Google Cloud Notebooks**, your environment already meets\n",
    "all the requirements to run this notebook. You can skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027218a5-9032-4ce6-86a5-245b7c11408b",
   "metadata": {
    "id": "gCuSR8GkAgzl"
   },
   "source": [
    "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
    "You need the following:\n",
    "\n",
    "* Docker\n",
    "* Git\n",
    "* Google Cloud SDK (gcloud)\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* Jupyter notebook running in a virtual environment with Python 3\n",
    "\n",
    "The Google Cloud guide to [Setting up a Python development\n",
    "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
    "installation guide](https://jupyter.org/install) provide detailed instructions\n",
    "for meeting these requirements. The following steps provide a condensed set of\n",
    "instructions:\n",
    "\n",
    "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "1. [Install\n",
    "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
    "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
    "\n",
    "1. To install Jupyter, run `pip install jupyter` on the\n",
    "command-line in a terminal shell.\n",
    "\n",
    "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
    "\n",
    "1. Open this notebook in the Jupyter Notebook Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9551bc-a5d1-4948-89bc-eef863f25001",
   "metadata": {
    "id": "i7EUnXsZhAGF",
    "tags": []
   },
   "source": [
    "### Install additional packages\n",
    "\n",
    "Install additional package dependencies not installed in your notebook environment, such as NumPy, Scikit-learn, FastAPI, Uvicorn, and joblib. Use the latest major GA version of each package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a329f34-5cd8-479e-b376-2643c0f38744",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "pandas\n",
    "torch==1.11.0\n",
    "google-cloud-storage>=1.26.0,<2.0.0dev\n",
    "google-cloud-aiplatform[prediction] @ git+https://github.com/googleapis/python-aiplatform.git@custom-prediction-routine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d60938-91d3-492e-9a7a-13865f45f0da",
   "metadata": {},
   "source": [
    "**The model you deploy will have a different set of dependencies pre-installed than your notebook environment has. You should not assume that because things work in the notebook, they will work in the model. Instead, we will be very explicit about the dependencies for the model by listing them in requirements.txt and then use `pip install` to install the exact same dependencies in the notebook. Please note, of course, that there is a chance that we miss a dependency in requirements.txt that already exists in the notebook. If that's the case, things will run in the notebook, but not in the model. To guard against that, we will test the model locally before deploying to the cloud.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6455533d-695c-4992-a52b-77ee170b4121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the same dependencies used in the serving container in the notebook\n",
    "# environment.\n",
    "%pip install -U --user -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121525f9-c666-4bab-9419-6ce18cb69b31",
   "metadata": {
    "id": "hhq5zEbGg0XX",
    "tags": []
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe42f2f2-e014-442c-a67d-396b8a40c6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3600fe-d496-4a2f-95f5-587233b4d702",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2347f2f6-d6f8-40be-9871-c9d7ecc98f21",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. [Enable the Vertex AI API and Compute Engine API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component).\n",
    "\n",
    "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    "\n",
    "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` or `%` as shell commands, and it interpolates Python variables with `$` or `{}` into these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b0255c-50d3-42cb-9b84-8c167ac8d436",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfd6274-0403-4ad2-93e8-61816aeff64b",
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "# Get your Google Cloud project ID from gcloud\n",
    "shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "\n",
    "try:\n",
    "    PROJECT_ID = shell_output[0]\n",
    "except IndexError:\n",
    "    PROJECT_ID = None\n",
    "\n",
    "print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b24998d-d2cb-4a5f-bea5-bcc110def5c5",
   "metadata": {
    "id": "qJYoRfYng0XZ"
   },
   "source": [
    "Otherwise, set your project ID here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e94601-bbba-4ec8-903b-5179d0862ecc",
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"[PROJ_ID]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb8852d-2b73-49f0-8a54-39eb6f3372e3",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Configure project and resource names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b410de-513a-40ea-9ee5-afe099233cf7",
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
    "MODEL_ARTIFACT_DIR = \"triton-pytorch\"  # @param {type:\"string\"}\n",
    "REPOSITORY = \"custom-container-prediction-sdk\"  # @param {type:\"string\"}\n",
    "IMAGE = \"triton-pytorch\"  # @param {type:\"string\"}\n",
    "MODEL_DISPLAY_NAME = \"triton-pytorch\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea10c9dc-b6aa-4f01-8b45-44010f300930",
   "metadata": {
    "id": "ca1a915d641d"
   },
   "source": [
    "`REGION` - Used for operations\n",
    "throughout the rest of this notebook. Make sure to [choose a region where Cloud\n",
    "Vertex AI services are\n",
    "available](https://cloud.google.com/vertex-ai/docs/general/locations#feature-availability). You may\n",
    "not use a Multi-Regional Storage bucket for prediction with Vertex AI.\n",
    "\n",
    "`MODEL_ARTIFACT_DIR` - Folder directory path to your model artifacts within a Cloud Storage bucket, for example: \"my-models/fraud-detection/trial-4\"\n",
    "\n",
    "`REPOSITORY` - Name of the Artifact Repository to create or use.\n",
    "\n",
    "`IMAGE` - Name of the container image that will be pushed.\n",
    "\n",
    "`MODEL_DISPLAY_NAME` - Display name of Vertex AI Model resource."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ac3d25-144a-4e0c-af09-93ae7e3f3e81",
   "metadata": {
    "id": "62f861b68b50"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "To update your model artifacts without re-building the container, you must upload your model\n",
    "artifacts and any custom code to Cloud Storage.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
    "Cloud Storage buckets and start with `gs://`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ee1453-2201-4536-8a7c-d41b1e796b2a",
   "metadata": {
    "id": "9724b00aeead"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://[BUCKET_NAME]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9848211d-ec12-401b-9ab9-2b9d6ce1f196",
   "metadata": {
    "id": "58cb4f5895f0"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091c12fd-715c-4333-9803-006fac24697c",
   "metadata": {
    "id": "2d2208676cee"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd60e2b-1b3c-468d-bf48-c882799845e5",
   "metadata": {
    "id": "c664a5abc11a"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9034982-e030-4287-9316-6506cfbccd72",
   "metadata": {
    "id": "2c1b1c29f5f6"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c614983-b301-4e9b-b53b-323960fc7530",
   "metadata": {
    "id": "3c2d091d9e73"
   },
   "source": [
    "### Set up directories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d23455-bca7-4687-a3b1-67789fa86daa",
   "metadata": {
    "id": "3c2d091d9e73"
   },
   "source": [
    "Decide the directory to put your model artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2fb119-eaa2-49bb-b921-3ff5ecce54b2",
   "metadata": {
    "id": "3c2d091d9e73"
   },
   "outputs": [],
   "source": [
    "MODEL_ARTIFACTS_DIRECTORY = \"model_artifacts\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0a4243-7dea-41f2-9b71-93bf8b8a60c8",
   "metadata": {
    "id": "6e74556ea0b4"
   },
   "outputs": [],
   "source": [
    "%mkdir $MODEL_ARTIFACTS_DIRECTORY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5d52e0-a7b5-4b09-9e45-62f5fb380a2e",
   "metadata": {},
   "source": [
    "## Training a PyTorch model\n",
    "\n",
    "### Download iris data\n",
    "In this example, we want to build a classifier for the simple [iris dataset](https://archive.ics.uci.edu/ml/datasets/iris). So first, we download the data csv file locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3e7d73-909d-47b4-9bc8-23feb21b60c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data_iris\"\n",
    "\n",
    "%mkdir $DATA_DIR\n",
    "\n",
    "LOCAL_DATA_FILE = f\"{DATA_DIR}/iris.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1047fb4a-9b59-4486-848b-fe00743c0ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\", LOCAL_DATA_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f92768-ad40-41da-99c2-2b968d9aa31b",
   "metadata": {},
   "source": [
    "### Build a PyTorch NN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed69d91-4d2e-4184-9e57-ebed01e1e45e",
   "metadata": {
    "id": "4b816cd52f4b"
   },
   "source": [
    "Make sure that pytorch package is [installed](https://pytorch.org/get-started/locally/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b6f059-cf98-436e-bb0d-55aac9e38450",
   "metadata": {
    "id": "43e47249f736"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "print('PyTorch Version: {}'.format(torch.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dfbb46-af2c-4ec9-bfb0-b158d3266580",
   "metadata": {},
   "source": [
    "#### Step 1. Load data\n",
    "\n",
    "In this step, we are going to:\n",
    "1. Load the data to Pandas Dataframe.\n",
    "1. Convert the class feature (species) from string to a numeric indicator.\n",
    "1. Split the Dataframe into input feature (xtrain) and target feature (ytrain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c61b530-424d-409e-b34e-7aa11f076750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "CLASS_VOCAB = ['setosa', 'versicolor', 'virginica']\n",
    "\n",
    "datatrain = pd.read_csv(LOCAL_DATA_FILE, names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'])\n",
    "\n",
    "#change string value to numeric\n",
    "datatrain.loc[datatrain['species']=='Iris-setosa', 'species']=0\n",
    "datatrain.loc[datatrain['species']=='Iris-versicolor', 'species']=1\n",
    "datatrain.loc[datatrain['species']=='Iris-virginica', 'species']=2\n",
    "datatrain = datatrain.apply(pd.to_numeric)\n",
    "\n",
    "#change dataframe to array\n",
    "datatrain_array = datatrain.values\n",
    "\n",
    "#split x and y (feature and target)\n",
    "xtrain = datatrain_array[:,:4]\n",
    "ytrain = datatrain_array[:,4]\n",
    "\n",
    "input_features = xtrain.shape[1]\n",
    "num_classes = len(CLASS_VOCAB)\n",
    "\n",
    "print('Records loaded: {}'.format(len(xtrain)))\n",
    "print('Number of input features: {}'.format(input_features))\n",
    "print('Number of classes: {}'.format(num_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2d8d82-9397-490f-afde-0408b28fb365",
   "metadata": {},
   "source": [
    "#### Step 2. Set model parameters\n",
    "You can try different values for **hidden_units** or **learning_rate**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479a742f-884b-4cc1-8f3c-b6327577aadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_UNITS = 10\n",
    "LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22238b8-c2a1-4bfa-8e9a-33e3e2340683",
   "metadata": {},
   "source": [
    "#### Step 3. Define the PyTorch NN model\n",
    "Here, we build a a neural network with one hidden layer, and a Softmax output layer for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb01a65-003c-49cc-83b2-cd8a8ef02b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(input_features, HIDDEN_UNITS),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(HIDDEN_UNITS, num_classes),\n",
    "    torch.nn.Softmax()\n",
    ")\n",
    "\n",
    "loss_metric = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f179a2d-0424-4723-930c-aa40266b95af",
   "metadata": {},
   "source": [
    "#### Step 4. Train the model\n",
    "We are going to train the model for **num_epoch** epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dc5398-91a7-4125-b978-dbb292b9c7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10000\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    x = Variable(torch.Tensor(xtrain).float())\n",
    "    y = Variable(torch.Tensor(ytrain).long())\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    loss = loss_metric(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch) % 1000 == 0:\n",
    "        print('Epoch [{}/{}] Loss: {}'.format(epoch+1, NUM_EPOCHS, round(loss.item(),3)))\n",
    "        \n",
    "print('Epoch [{}/{}] Loss: {}'.format(epoch+1, NUM_EPOCHS, round(loss.item(),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562d6e45-601d-4f95-95e5-6c129e57d3f5",
   "metadata": {},
   "source": [
    "#### Step 5. Save the model\n",
    "\n",
    "Triton inference server requires PyTorch models to be saved as [the TorchScript format](https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f778650-9c99-437d-a833-6c185e1094c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example input you would normally provide to your model's forward() method.\n",
    "example = torch.rand(1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6f2389-6f20-449e-a162-f876036baa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"pytorch\"\n",
    "LOCAL_MODEL_DIRECTORY = f\"{MODEL_ARTIFACTS_DIRECTORY}/{MODEL_NAME}\"\n",
    "LOCAL_MODEL_VERSION_1 = f\"{LOCAL_MODEL_DIRECTORY}/1\"\n",
    "\n",
    "%mkdir $LOCAL_MODEL_DIRECTORY\n",
    "%mkdir $LOCAL_MODEL_VERSION_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f0f848-706b-427b-aa38-edc6ae0938ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_MODEL_FILE = f\"{LOCAL_MODEL_VERSION_1}/model.pt\"\n",
    "\n",
    "# Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing.\n",
    "traced_script_module = torch.jit.trace(model, example)\n",
    "\n",
    "# Save the TorchScript model\n",
    "traced_script_module.save(LOCAL_MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b84cd6e-dc59-4e22-9d1e-df5a55b175fa",
   "metadata": {},
   "source": [
    "#### Step 6. Test the loaded TorchScript model for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0da5137-fb7f-4287-8b97-c8f6c365840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_classifier = torch.jit.load(LOCAL_MODEL_FILE)\n",
    "\n",
    "def predict_class(instances):\n",
    "    instances = torch.Tensor(instances)\n",
    "    output = iris_classifier(instances)\n",
    "    _ , predicted = torch.max(output, 1)\n",
    "    return predicted\n",
    "\n",
    "predicted = predict_class(xtrain[0:5])\n",
    "print([CLASS_VOCAB[class_index] for class_index in predicted])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb4a8db-e674-41f1-9c82-ddb5c20ba14e",
   "metadata": {},
   "source": [
    "## Prepare for model configuration for Triton\n",
    "\n",
    "Each model in a [model repository](https://github.com/triton-inference-server/server/blob/main/docs/model_repository.md) must include a [model configuration](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md) that provides required and optional information about the model. To load PyTorch models, the model configuration needs to set `backend` and `platform` to `pytorch` and `pytorch_libtorch` respectively. There are also [special conventions for PyTorch backend](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13213824-aa79-4d8e-bfd7-d26fa59009e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $LOCAL_MODEL_DIRECTORY/config.pbtxt\n",
    "backend: \"pytorch\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size: 8\n",
    "input {\n",
    "    name: \"INPUT__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 4 ]\n",
    "}\n",
    "output {\n",
    "    name: \"OUTPUT__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1 ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad469a8f-2b2c-4214-b2b6-34d219fb059f",
   "metadata": {
    "id": "3849066a33bd"
   },
   "source": [
    "## Upload model artifacts to Cloud Storage\n",
    "\n",
    "Before you can deploy your model for serving, Vertex AI needs access to the following files in Cloud Storage:\n",
    "\n",
    "* `model.pt` (model artifact)\n",
    "* `config.pbtxt` (model configuration)\n",
    "\n",
    "Run the following commands to upload your files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec664e2-e145-4987-b8ba-1826d1c38f1f",
   "metadata": {
    "id": "ca67ee52d4d9"
   },
   "outputs": [],
   "source": [
    "!gsutil cp -r {MODEL_ARTIFACTS_DIRECTORY}/* {BUCKET_NAME}/{MODEL_ARTIFACT_DIR}/\n",
    "!gsutil ls {BUCKET_NAME}/{MODEL_ARTIFACT_DIR}/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3501541c-4e77-4f47-b6d9-e7ce504daf4a",
   "metadata": {},
   "source": [
    "## Use NVIDIA Triton inference server\n",
    "\n",
    "### Initiate Local Model with NVIDIA Triton inference server\n",
    "\n",
    "In this tutorial, we use [NVIDIA Triton inference server](https://developer.nvidia.com/nvidia-triton-inference-server) to serve the PyTorch model. The images are hosted on `nvcr.io` and all released tags can be found in [this link](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver/tags)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ca40b8-9b22-4e68-bf36-a3af7bb618af",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRITON_VERSION = \"21.08\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d345c8-eb28-4117-97a7-9ed78d387a5b",
   "metadata": {},
   "source": [
    "With the Custom Prediction Routine features, Vertex SDK allows you to load the existing images to test them locally and then deploy them to Vertex AI Prediction easily.\n",
    "\n",
    "To use Triton inference server on Vertex AI, we need to set up needed parameters such as predict route, health route, ports, and args. By default, Triton listens for HTTP requests on port `8000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7628ee9d-d6be-4c4c-af7c-f41537121457",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform.prediction import LocalModel\n",
    "\n",
    "local_model = LocalModel.create(\n",
    "    serving_container_image_uri=f\"nvcr.io/nvidia/tritonserver:{TRITON_VERSION}-py3\",\n",
    "    serving_container_predict_route=f\"/v2/models/{MODEL_NAME}/infer\",\n",
    "    serving_container_health_route=f\"/v2/models/{MODEL_NAME}\",\n",
    "    serving_container_ports=[8000],\n",
    "    serving_container_args=[\"tritonserver\", \"--model-repository=$(AIP_STORAGE_URI)\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfe3de3-d3c5-432b-b7db-ee8cb62c4dc2",
   "metadata": {},
   "source": [
    "You can check out the serving container spec of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3414da6c-b894-4006-8c75-65168f05feb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model.get_serving_container_spec()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10ee633-8b13-470a-a544-4ca7dd2409df",
   "metadata": {
    "id": "8b62ddf1def3"
   },
   "source": [
    "### Store test instances\n",
    "\n",
    "To learn more about formatting input instances in JSON, [read the documentation.](https://cloud.google.com/vertex-ai/docs/predictions/online-predictions-custom-models#request-body-details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aeff69-c0e2-4268-ac5d-4893c8a47792",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"instances.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b9b97e-4f3a-419d-98de-407423318ad1",
   "metadata": {
    "id": "b6605e9e6186"
   },
   "outputs": [],
   "source": [
    "%%writefile $INPUT_FILE\n",
    "{\n",
    "    \"id\": \"0\",\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"INPUT__0\",\n",
    "            \"shape\": [2, 4],\n",
    "            \"datatype\": \"FP32\",\n",
    "            \"data\": [[6.7, 3.1, 4.7, 1.5], [4.6, 3.1, 1.5, 0.2]]\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd410e3-0173-42b4-bedf-731045016027",
   "metadata": {},
   "source": [
    "### Run and test the container locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ad3c9d-261d-46fd-b102-fe2e468ff396",
   "metadata": {
    "id": "8phJWBhe7u9u",
    "tags": []
   },
   "source": [
    "#### Set up credentials\n",
    "\n",
    "Setting up credentials is only required to run the custom serving container locally. Credentials set up is required to execute the `Predictor`'s `load` function, which downloads the model artifacts from Google Cloud Storage.\n",
    "\n",
    "To access Google Cloud Storage in your project, you'll need to set up credentials by using one of the following:\n",
    "\n",
    "1. User account\n",
    "\n",
    "1. Service account\n",
    "\n",
    "You can learn more about each of the above [here](https://cloud.google.com/docs/authentication#principals)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561d97ec-ce2b-4f65-9f82-268c42e628e9",
   "metadata": {},
   "source": [
    "**Option 1. Use Google user credentials**\n",
    "\n",
    "First authorize Google auth library to access the Cloud Platform with Google user credentials. This step involves an interactive prompt which requires user inputs. If you are using a non-interactive shell, you should open a terminal to run this command. See [here](https://jupyterlab.readthedocs.io/en/stable/user/terminal.html) for how to open a terminal in a Vertex Workbench notebook environment.\n",
    "\n",
    "Note that if you are running the command below on a machine without a window manager/browser, it will prompt you to run the command `gcloud auth application-default login --remote-bootstrap=\"...\"`. Only machines that can launch web browsers, e.g. laptops or workstations, are allowed to run this command. After completing the authentication steps using your browser, you'll need to copy and paste the verification code back into the original terminal to continue the login flow. If you are running this command on a machine with web browser support, there is no need to use a separate machine.\n",
    "\n",
    "This command will print the credential file as `Credentials saved to file: [CREDENTIALS_FILE]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d4b1c4-2287-4f28-a16a-577eb9d0199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f79cddb-5f1e-46dc-b829-355eae9df182",
   "metadata": {},
   "source": [
    "Specify the credential file, which is a json file, as the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc35d9e9-cdc8-4a75-bb0e-4dd32d7eeb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREDENTIALS_FILE = \"[CREDENTIALS_FILE]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46042f2-0a65-4f7d-8c20-44b7158144de",
   "metadata": {},
   "source": [
    "Next, authorize gcloud to access the Cloud Platform with Google user credentials. The user credentials need to have `setIamPolicy` permission in the project. See [here](https://cloud.google.com/resource-manager/docs/access-control-proj) for more details.\n",
    "\n",
    "Similar to the previous step, this also requires user inputs. Instructions for the previous step apply here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9c8f45-d807-4f1a-a6ca-ced0fe3ac376",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e680bb4d-2fb1-4b79-af77-032d1a5ae6fd",
   "metadata": {},
   "source": [
    "Grant roles to the user account. Use the email address that you used in the gcloud auth step, e.g., `myemail@gmail.com`. Since we will be using this user account to download the model artifacts from Google Cloud Storage, we grant the user account **Storage Object Viewer** role. See [IAM roles for Cloud Storage](https://cloud.google.com/storage/docs/access-control/iam-roles) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd75a81-c84f-4179-b0ec-966ea46b6c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_ACCOUNT = \"[USER_ACCOUNT]\"  # @param {type:\"string\"}\n",
    "\n",
    "!gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "    --member=user:$USER_ACCOUNT --role=roles/storage.objectViewer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1fb339-227a-4d2b-b9b0-80812ac5aa4e",
   "metadata": {},
   "source": [
    "Initialize the Vertex AI SDK with the project ID, which is required for using user credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe5ce61-fef7-48cc-97f8-4182e0a1eba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d4a5bb-f340-4d4e-ba24-392711b47464",
   "metadata": {},
   "source": [
    "**Option 2. Use Google Service Account credentials**\n",
    "\n",
    "Skip this if you've already completed Option 1 above. First enable the IAM API if it's not already enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5f6a5a-4ff3-4e7e-b591-3470d6c110e1",
   "metadata": {
    "id": "mERTlLb6dluB"
   },
   "outputs": [],
   "source": [
    "!gcloud services enable iam.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e3704a-2bae-4c4f-8bca-34f77449d26d",
   "metadata": {},
   "source": [
    "Specify service account name which should be unique in your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd11517-c715-4439-bca3-71be24ef69e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[SERVICE_ACCOUNT]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baec7ff-3147-487c-baef-5903730a27a5",
   "metadata": {},
   "source": [
    "Create the service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca0a455-4268-4a5f-9506-b8b6317daf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud iam service-accounts create $SERVICE_ACCOUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47645903-f7e6-436a-8d78-b9004e4113e5",
   "metadata": {},
   "source": [
    "Authorize gcloud to access the Cloud Platform with Google user credentials. The user credentials need to have `setIamPolicy` permission in the project. See [here](https://cloud.google.com/resource-manager/docs/access-control-proj) for more details.\n",
    "\n",
    "This step involves an interactive prompt which requires user inputs. If you are using a non-interactive shell, you should open a terminal to run this command. See [here](https://jupyterlab.readthedocs.io/en/stable/user/terminal.html) for how to open a terminal in a Vertex Workbench notebook environment.\n",
    "\n",
    "Note that if you are running the command below on a machine without a window manager/browser, it will prompt you to run the command `gcloud auth application-default login --remote-bootstrap=\"...\"`. Only machines that can launch web browsers, e.g. laptops or workstations, are allowed to run this command. After completing the authentication steps using your browser, you'll need to copy and paste the verification code back into the original terminal to continue the login flow. If you are running this command on a machine with web browser support, there is no need to use a separate machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793eb46d-585e-4f43-995f-b0006ca0cdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a6b6bf-bd3c-4cec-9141-1b6a6303796e",
   "metadata": {},
   "source": [
    "Grant roles to the service account. Since we will be using this service account to download the model artifacts from Google Cloud Storage, we grant the service account **Storage Object Viewer** role. See [IAM roles for Cloud Storage](https://cloud.google.com/storage/docs/access-control/iam-roles) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1c6f36-9e0a-4961-912f-bba6141c8e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "    --member=serviceAccount:{SERVICE_ACCOUNT}@{PROJECT_ID}.iam.gserviceaccount.com \\\n",
    "    --role=roles/storage.objectViewer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa2d76-8e9b-4aab-97be-002cc6c7f727",
   "metadata": {},
   "source": [
    "Specify the service account key file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824232e6-a8a6-4e38-aab1-65ee5f63fe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREDENTIALS_FILE = \"./credentials.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d308202-c865-4c70-ac13-a74e2b95e8f6",
   "metadata": {},
   "source": [
    "Generate the service account key file. This will be used while running the custom serving container locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1a6eed-ec03-469d-a886-0bb3c3f4626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud iam service-accounts keys create $CREDENTIALS_FILE \\\n",
    "    --iam-account=\"{SERVICE_ACCOUNT}@{PROJECT_ID}.iam.gserviceaccount.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a81811-52b5-45a4-bfd1-17a6b62347a0",
   "metadata": {
    "id": "pJe8TCr4ec4F"
   },
   "source": [
    "#### Run and send requests to the container locally\n",
    "\n",
    "With the Custom Prediction Routine feature, it is easy to test the container locally.\n",
    "\n",
    "Note: You need to have the credentials set up in the previous step and pass the path to the credentials while running the container. The service account should have the **Storage Object Viewer** permission.\n",
    "\n",
    "In this example, the container executes a prediction request and a health check.\n",
    "\n",
    "Note: It will be a bit slow for the first time you run the following command because we need to pull the image from `nvcr.io`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a02e58-b612-45a2-8756-d06e689925a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with local_model.deploy_to_local_endpoint(\n",
    "    artifact_uri=f\"{BUCKET_NAME}/{MODEL_ARTIFACT_DIR}\",\n",
    "    credential_path=CREDENTIALS_FILE,\n",
    ") as local_endpoint:\n",
    "    predict_response = local_endpoint.predict(\n",
    "        request_file=INPUT_FILE,\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "    )\n",
    "    \n",
    "    health_check_response = local_endpoint.run_health_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb531fb6-5161-4fe8-989b-9d29a19dc670",
   "metadata": {},
   "source": [
    "Print out the predict response and its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492cc6b2-26f4-49f8-89a2-4a3cd8ed1d61",
   "metadata": {
    "id": "ce629eea32fd"
   },
   "outputs": [],
   "source": [
    "predict_response, predict_response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26161034-de5c-4737-a7e0-d4324cfa507f",
   "metadata": {},
   "source": [
    "Print out the health check response and its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c0320f-43a7-4e8f-98c6-dd195dbcba32",
   "metadata": {
    "id": "56986f93438e"
   },
   "outputs": [],
   "source": [
    "health_check_response, health_check_response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb45af05-0585-4ca9-9881-b6c8e19218d3",
   "metadata": {
    "id": "a29fcbbe0188"
   },
   "source": [
    "Also print out all the container logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207f9356-65b1-42fc-999a-c2dbaecb0619",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_endpoint.print_container_logs(show_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13f2b95-f3a0-4818-98d7-8140bf6b4ad3",
   "metadata": {},
   "source": [
    "### Change image uris to Artifact Registry\n",
    "\n",
    "Because Vertex AI Prediction currently does not support `nvcr.io` repositories, we need to copy the images to Artifact Registry. We can easily initiate another Local Model instance with the copied images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e8631a-f3fa-4c5d-a274-12121b4f6922",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model_ar = local_model.copy_image(f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d4a33f-8f37-42f7-b981-249c2e7b5b07",
   "metadata": {},
   "source": [
    "Print out the serving container spec of the new Local Model instance. The image uri should be changed to Artifact Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e15fd8-c9f1-4d12-9dd9-50ca5174c0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model_ar.get_serving_container_spec()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4086db94-b658-40e7-a31e-1d54a176248b",
   "metadata": {
    "id": "212b2935ea12",
    "tags": []
   },
   "source": [
    "### Push the container to artifact registry\n",
    "\n",
    "Configure Docker to access Artifact Registry. Then push your container image to your Artifact Registry repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55c82d5-ed23-499f-b912-4adccf01e7b5",
   "metadata": {
    "id": "wSFXCj3LdluJ"
   },
   "outputs": [],
   "source": [
    "!gcloud services list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1e4041-def7-4b13-add5-fd8b6a4d6cb9",
   "metadata": {
    "id": "ABE9UpwSdluK"
   },
   "source": [
    "If `artifactregistry.googleapis.com` is not enabled in your project, enable the API before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beed29c0-7398-4a6c-986e-e93054423f5e",
   "metadata": {
    "id": "qDhLoQMydluK"
   },
   "outputs": [],
   "source": [
    "!gcloud services enable artifactregistry.googleapis.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5384ec7f-079c-4515-896b-066f3a730b55",
   "metadata": {
    "id": "09ffe2434e3d"
   },
   "outputs": [],
   "source": [
    "!gcloud beta artifacts repositories create {REPOSITORY} \\\n",
    "    --repository-format=docker \\\n",
    "    --location=$REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76535f62-83e2-46dd-843f-51a3ac62e132",
   "metadata": {
    "id": "293437024749"
   },
   "outputs": [],
   "source": [
    "!gcloud auth configure-docker {REGION}-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab231ad-9e06-4ed3-9d7d-3e2ebfcdc49a",
   "metadata": {},
   "source": [
    "Use SDK to push the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef129ce5-b92b-4271-b5d6-cf848e1f087b",
   "metadata": {
    "id": "1dd7448f4703"
   },
   "outputs": [],
   "source": [
    "local_model_ar.push_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7677e2a-53b0-4360-94dd-2d4c43e09fe1",
   "metadata": {
    "id": "b438bfa2129f"
   },
   "source": [
    "## Deploy to Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6391de8d-cbc5-443d-b1d9-b7b748aed6f3",
   "metadata": {
    "id": "4ae19df6a33e"
   },
   "source": [
    "### Upload the NVIDIA Triton inference server model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e358c5-2798-4dd8-9ec8-9bb9d11f5486",
   "metadata": {
    "id": "8d682d8388ec"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36320f90-9580-4768-8895-92d745dce26a",
   "metadata": {
    "id": "574fb82d3eed"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c4223b-ce8b-4078-8a81-cf9ac2794147",
   "metadata": {},
   "source": [
    "Use the LocalModel instance to upload the model. It will populate the container spec automatically for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d65d00-60cc-4314-8d70-0fb27fee87ea",
   "metadata": {
    "id": "2738154345d5"
   },
   "outputs": [],
   "source": [
    "model = local_model_ar.upload(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    artifact_uri=f\"{BUCKET_NAME}/{MODEL_ARTIFACT_DIR}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9da34a3-2ae3-4e51-b8fa-5a5dab2cab21",
   "metadata": {
    "id": "bd1b85afc7df"
   },
   "source": [
    "### Deploy the model on Vertex AI\n",
    "After this step completes, the model is deployed and ready for online prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8226a1-f81a-4dd2-9240-59e51b9b7cb7",
   "metadata": {
    "id": "62cf66498a28"
   },
   "outputs": [],
   "source": [
    "endpoint = model.deploy(machine_type=\"n1-standard-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099573fd-96d9-4cf6-8d80-98996601bd4f",
   "metadata": {
    "id": "6883e7b07143",
    "tags": []
   },
   "source": [
    "## Send predictions\n",
    "\n",
    "Need to use [raw predict](https://cloud.google.com/sdk/gcloud/reference/ai/endpoints/raw-predict) to send arbitrary payloads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5352171-4775-4e31-845e-22a0d292e344",
   "metadata": {},
   "source": [
    "### Using REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75def59b-8be5-4973-bc7c-e350ddfbdbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT_ID = endpoint.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f88cd8-e7bc-4fd5-a951-30c6c51673e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl \\\n",
    "-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "--data-binary @$INPUT_FILE \\\n",
    "https://{REGION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/endpoints/{ENDPOINT_ID}:rawPredict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298e3725-e630-4e32-bb4c-063fa80c9a45",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38325a46-e99e-42bd-83cb-3b17da4f5814",
   "metadata": {
    "id": "sx_vKniMq9ZX"
   },
   "outputs": [],
   "source": [
    "# Undeploy model and delete endpoint\n",
    "endpoint.delete(force=True)\n",
    "\n",
    "# Delete the model resource\n",
    "model.delete()\n",
    "\n",
    "# Delete the container image from Artifact Registry\n",
    "!gcloud artifacts docker images delete \\\n",
    "    --quiet \\\n",
    "    --delete-tags \\\n",
    "    {REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE}"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

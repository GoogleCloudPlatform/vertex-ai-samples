{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d9bbf86da5e"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Vertex AI Model Garden GenAI Workshop for Image Generation (inspired by Stable Diffusion WebUI)\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_pytorch_stable_diffusion_gradio.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_stable_diffusion_gradio.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates starting a playground based on [Gradio UI](https://www.gradio.app/), inspired by the famous [Stable Diffusion WebUI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) project, which allows users to interact with the stable diffusion models more easily and intuitively. The playground now support `text-to-image`, `image-inpainting`, `controlnet-canny`, `instruct-pix2pix`, and `SD 4x upscaler` tasks\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Deploy model to a [Vertex AI Endpoint resource](https://cloud.google.com/vertex-ai/docs/predictions/using-private-endpoints).\n",
        "- Run online predictions for `text-to-image`, `image-inpainting`, `controlnet-canny`, `instruct-pix2pix`, and `SD 4x upscaler` tasks, from the UI.\n",
        "- Adjust the parameters, such as prompt, negative_prompt, num_inference_steps, and check out the generated images for best image quality.\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "879fca33129c"
      },
      "source": [
        "## Run the playground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "855d6b96f291"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project and prepare the dependencies\n",
        "\n",
        "# @markdown [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "! pip3 install --upgrade gradio==3.50.0 opencv-python\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
        "\n",
        "# Set up the default SERVICE_ACCOUNT.\n",
        "SERVICE_ACCOUNT = None\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "\n",
        "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
        "\n",
        "# The pre-built serving docker image. It contains serving scripts and models.\n",
        "SERVE_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-diffusers-serve-opt:20240403_0836_RC00\"\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user(project_id=PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1cc26e68d7b0"
      },
      "outputs": [],
      "source": [
        "# @title Start the playground\n",
        "\n",
        "# @markdown This is a playground for image generation similar to the popular [Stable Diffusion WebUI](https://github.com/AUTOMATIC1111/stable-diffusion-webui).\n",
        "# @markdown After the cell runs, this playground is avaible in a separate browser tab if you click the public URL.\n",
        "# @markdown Sometsomething similar to [\"https://####.gradio.live\"](#) in the output of the cell.\n",
        "\n",
        "# @markdown Five tasks `text-to-image`, `image-inpainting`, `controlnet-canny`, `instruct-pix2pix` and `SD 4x upscaler` are currently supported.\n",
        "\n",
        "# @markdown **How to use:**\n",
        "# @markdown 1. Important: Notebook cell reruns create new public URLs. Previous URLs will stop working.\n",
        "# @markdown 1. Before you start, you need to select a Vertex prediction endpoint, with a matching model deployed to the endpoint\n",
        "# @markdown from the endpoint dropdown list, that has been deployed in the project and region;\n",
        "# @markdown 1. Make sure the selected endpoint/model match with the chosen task. Mismatched task and model will produce unreliable results.\n",
        "# @markdown 1. If no models were deployed in the past, you can create a new Vertex prediction\n",
        "# @markdown endpoint by selecting your favorite model and click \"Deploy\".\n",
        "# @markdown 1. New model deployment takes ~20 minutes. You can check the progress at [Vertex Online Prediction](https://console.cloud.google.com/vertex-ai/online-prediction/endpoints).\n",
        "# @markdown 1. Adjust the prompt/negative-prompt, image-dimension, inference steps, guidance-scale to achieve the optimum image quality and inference latency.\n",
        "# @markdown 1. Don't forget to undeploy the models after all the experiment to avoid continuous charges to the project.\n",
        "\n",
        "# @markdown **Note: we support the following models now:**\n",
        "# @markdown Other models (with the same task) may work, but they are not tested please use with caution.\n",
        "# @markdown 1. Text-to-Image models\n",
        "# @markdown    > runwayml/stable-diffusion-v1-5 \\\n",
        "# @markdown    > stabilityai/stable-diffusion-2-1 \\\n",
        "# @markdown    > stabilityai/stable-diffusion-xl-base-1.0 \\\n",
        "# @markdown    > stabilityai/stable-diffusion-xl-base-1.0 - refiner \\\n",
        "# @markdown    > latent-consistency/lcm-sdxl \\\n",
        "# @markdown    > latent-consistency/lcm-lora-sdxl \\\n",
        "# @markdown    > stabilityai/sdxl-turbo \\\n",
        "# @markdown 1. Image-inpainting models\n",
        "# @markdown    > runwayml/stable-diffusion-inpainting \\\n",
        "# @markdown    > kandinsky-community/kandinsky-2-2-decoder-inpaint \\\n",
        "# @markdown    > diffusers/stable-diffusion-xl-1.0-inpainting-0.1\n",
        "# @markdown 1. Controlnet-canny models\n",
        "# @markdown    > lllyasviel/sd-controlnet-canny\n",
        "# @markdown 1. Instruct-pix2pix models\n",
        "# @markdown    > timbrooks/instruct-pix2pix\n",
        "# @markdown 1. SD 4x upscaler models\n",
        "# @markdown    > stabilityai/stable-diffusion-x4-upscaler\n",
        "\n",
        "import base64\n",
        "from datetime import datetime\n",
        "from io import BytesIO\n",
        "\n",
        "import cv2\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "from google.cloud import aiplatform\n",
        "from PIL import Image\n",
        "\n",
        "style_list = [\n",
        "    {\n",
        "        \"name\": \"(No style)\",\n",
        "        \"prompt\": \"{prompt}\",\n",
        "        \"negative_prompt\": \"\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Cinematic\",\n",
        "        \"prompt\": \"cinematic still {prompt} . emotional, harmonious, vignette, highly detailed, high budget, bokeh, cinemascope, moody, epic, gorgeous, film grain, grainy\",\n",
        "        \"negative_prompt\": \"anime, cartoon, graphic, text, painting, crayon, graphite, abstract, glitch, deformed, mutated, ugly, disfigured\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Photographic\",\n",
        "        \"prompt\": \"cinematic photo {prompt} . 35mm photograph, film, bokeh, professional, 4k, highly detailed\",\n",
        "        \"negative_prompt\": \"drawing, painting, crayon, sketch, graphite, impressionist, noisy, blurry, soft, deformed, ugly\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Anime\",\n",
        "        \"prompt\": \"anime artwork {prompt} . anime style, key visual, vibrant, studio anime,  highly detailed\",\n",
        "        \"negative_prompt\": \"photo, deformed, black and white, realism, disfigured, low contrast\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Manga\",\n",
        "        \"prompt\": \"manga style {prompt} . vibrant, high-energy, detailed, iconic, Japanese comic style\",\n",
        "        \"negative_prompt\": \"ugly, deformed, noisy, blurry, low contrast, realism, photorealistic, Western comic style\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Digital Art\",\n",
        "        \"prompt\": \"concept art {prompt} . digital artwork, illustrative, painterly, matte painting, highly detailed\",\n",
        "        \"negative_prompt\": \"photo, photorealistic, realism, ugly\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Pixel art\",\n",
        "        \"prompt\": \"pixel-art {prompt} . low-res, blocky, pixel art style, 8-bit graphics\",\n",
        "        \"negative_prompt\": \"sloppy, messy, blurry, noisy, highly detailed, ultra textured, photo, realistic\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Fantasy art\",\n",
        "        \"prompt\": \"ethereal fantasy concept art of  {prompt} . magnificent, celestial, ethereal, painterly, epic, majestic, magical, fantasy art, cover art, dreamy\",\n",
        "        \"negative_prompt\": \"photographic, realistic, realism, 35mm film, dslr, cropped, frame, text, deformed, glitch, noise, noisy, off-center, deformed, cross-eyed, closed eyes, bad anatomy, ugly, disfigured, sloppy, duplicate, mutated, black and white\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Neonpunk\",\n",
        "        \"prompt\": \"neonpunk style {prompt} . cyberpunk, vaporwave, neon, vibes, vibrant, stunningly beautiful, crisp, detailed, sleek, ultramodern, magenta highlights, dark purple shadows, high contrast, cinematic, ultra detailed, intricate, professional\",\n",
        "        \"negative_prompt\": \"painting, drawing, illustration, glitch, deformed, mutated, cross-eyed, ugly, disfigured\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"3D Model\",\n",
        "        \"prompt\": \"professional 3d model {prompt} . octane render, highly detailed, volumetric, dramatic lighting\",\n",
        "        \"negative_prompt\": \"ugly, deformed, noisy, low poly, blurry, painting\",\n",
        "    },\n",
        "]\n",
        "\n",
        "styles = {k[\"name\"]: (k[\"prompt\"], k[\"negative_prompt\"]) for k in style_list}\n",
        "STYLE_NAMES = list(styles.keys())\n",
        "DEFAULT_STYLE_NAME = \"(No style)\"\n",
        "\n",
        "\n",
        "def create_job_name(prefix):\n",
        "    now = datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
        "    job_name = f\"{prefix}-gradio-{now}\"\n",
        "    return job_name\n",
        "\n",
        "\n",
        "def base64_to_image(image_str: str) -> Image:\n",
        "    \"\"\"Convert base64 encoded string to an image.\"\"\"\n",
        "    image = Image.open(BytesIO(base64.b64decode(image_str)))\n",
        "    return image\n",
        "\n",
        "\n",
        "def image_to_base64(image: Image, format=\"JPEG\") -> str:\n",
        "    buffer = BytesIO()\n",
        "    image.save(buffer, format=format)\n",
        "    image_str = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
        "    return image_str\n",
        "\n",
        "\n",
        "def canny(image, low_threshold=100, high_threshold=200) -> Image.Image:\n",
        "    image = np.array(image)\n",
        "    image = cv2.Canny(image, low_threshold, high_threshold)\n",
        "\n",
        "    image = image[:, :, None]\n",
        "    image = np.concatenate([image, image, image], axis=2)\n",
        "    image = Image.fromarray(image)\n",
        "    return image\n",
        "\n",
        "\n",
        "def is_image_generation_endpoint(endpoint: aiplatform.Endpoint) -> bool:\n",
        "    \"\"\"Returns True if the endpoint is an image generation endpoint.\"\"\"\n",
        "    return (\n",
        "        \"sd\" in endpoint.display_name.lower()\n",
        "        or \"diffusion\" in endpoint.display_name.lower()\n",
        "        or \"inpaint\" in endpoint.display_name.lower()\n",
        "        or \"controlnet\" in endpoint.display_name.lower()\n",
        "        or \"pix2pix\" in endpoint.display_name.lower()\n",
        "        or \"upscaler\" in endpoint.display_name.lower()\n",
        "        or \"canny\" in endpoint.display_name.lower()\n",
        "        or \"gradio\" in endpoint.display_name.lower()\n",
        "    )\n",
        "\n",
        "\n",
        "def list_endpoints() -> list[str]:\n",
        "    \"\"\"Returns all valid prediction endpoints for in the project and region.\"\"\"\n",
        "    # Gets all the valid endpoints in the project and region.\n",
        "    endpoints = aiplatform.Endpoint.list(order_by=\"create_time desc\")\n",
        "    # Filters out the endpoints which do not have a deployed model, and the endpoint is for image generation\n",
        "    endpoints = list(\n",
        "        filter(\n",
        "            lambda endpoint: endpoint.traffic_split\n",
        "            and is_image_generation_endpoint(endpoint),\n",
        "            endpoints,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    endpoint_names = list(\n",
        "        map(\n",
        "            lambda endpoint: f\"{endpoint.name} - {endpoint.display_name[:40]}\",\n",
        "            endpoints,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return endpoint_names\n",
        "\n",
        "\n",
        "def get_endpoint(endpoint_name: str) -> aiplatform.Endpoint:\n",
        "    \"\"\"Returns a Vertex endpoint for the given endpoint_name.\"\"\"\n",
        "\n",
        "    endpoint_id = endpoint_name.split(\" - \")[0]\n",
        "    endpoint = aiplatform.Endpoint(\n",
        "        f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_id}\"\n",
        "    )\n",
        "\n",
        "    return endpoint\n",
        "\n",
        "\n",
        "def get_task_name(model_name: str) -> str:\n",
        "    \"\"\"Returns the corresponding task name for the given model_name.\"\"\"\n",
        "\n",
        "    model_to_task_dict = {\n",
        "        \"runwayml/stable-diffusion-v1-5\": \"text-to-image\",\n",
        "        \"stabilityai/stable-diffusion-2-1\": \"text-to-image\",\n",
        "        \"stabilityai/stable-diffusion-xl-base-1.0\": \"text-to-image-sdxl\",\n",
        "        \"stabilityai/stable-diffusion-xl-base-1.0 - refiner\": \"text-to-image-refiner\",\n",
        "        \"latent-consistency/lcm-sdxl\": \"text-to-image-sdxl-lcm\",\n",
        "        \"latent-consistency/lcm-lora-sdxl\": \"text-to-image-sdxl-lcm-lora\",\n",
        "        \"stabilityai/sdxl-turbo\": \"text-to-image-sdxl-turbo\",\n",
        "        \"runwayml/stable-diffusion-inpainting\": \"image-inpainting\",\n",
        "        \"kandinsky-community/kandinsky-2-2-decoder-inpaint\": \"image-inpainting\",\n",
        "        \"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\": \"image-inpainting\",\n",
        "        \"timbrooks/instruct-pix2pix\": \"instruct-pix2pix\",\n",
        "        \"lllyasviel/sd-controlnet-canny\": \"controlnet\",\n",
        "        \"stabilityai/stable-diffusion-x4-upscaler\": \"conditioned-super-res\",\n",
        "    }\n",
        "\n",
        "    if model_name not in model_to_task_dict.keys():\n",
        "        print(model_name)\n",
        "        raise gr.Error(\"Please select a valid model name for Endpoint creation.\")\n",
        "\n",
        "    return model_to_task_dict[model_name]\n",
        "\n",
        "\n",
        "def deploy_model(model_name: str) -> aiplatform.Endpoint:\n",
        "    \"\"\"Creates a new Vertex prediction endpoint and deploys a model to it.\"\"\"\n",
        "    refiner_model_id = \"stabilityai/stable-diffusion-xl-refiner-1.0\"\n",
        "\n",
        "    if not model_name:\n",
        "        raise gr.Error(\"Please select a valid model name for model list.\")\n",
        "        return\n",
        "\n",
        "    gr.Info(\"Model deployment started. Please wait...\")\n",
        "\n",
        "    model_name = model_name.split(\": \")[1]\n",
        "    task_name = get_task_name(model_name)\n",
        "    model_id = model_name\n",
        "    if (\n",
        "        model_name == \"stabilityai/stable-diffusion-xl-base-1.0 - refiner\"\n",
        "        or model_name == \"latent-consistency/lcm-sdxl\"\n",
        "        or model_name == \"latent-consistency/lcm-lora-sdxl\"\n",
        "    ):\n",
        "        model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "\n",
        "    display_name = create_job_name(model_name)\n",
        "    endpoint = aiplatform.Endpoint.create(display_name=display_name)\n",
        "    serving_env = {\n",
        "        \"MODEL_ID\": model_id,\n",
        "        \"TASK\": task_name,\n",
        "    }\n",
        "    if model_name == \"stabilityai/stable-diffusion-xl-base-1.0 - refiner\":\n",
        "        serving_env = {\n",
        "            **serving_env,\n",
        "            \"REFINER_MODEL_ID\": refiner_model_id,\n",
        "        }\n",
        "\n",
        "    display_name = create_job_name(model_name)\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=SERVE_DOCKER_URI,\n",
        "        serving_container_ports=[7080],\n",
        "        serving_container_predict_route=\"/predictions/diffusers_serving\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "        serving_container_environment_variables=serving_env,\n",
        "    )\n",
        "    machine_type = \"g2-standard-8\"\n",
        "    accelerator_type = \"NVIDIA_L4\"\n",
        "\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=1,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=SERVICE_ACCOUNT,\n",
        "        sync=False,\n",
        "    )\n",
        "\n",
        "    gr.Info(\n",
        "        f\"Model {display_name} is being deployed. It may take ~20 minutes to complete.\"\n",
        "    )\n",
        "\n",
        "    return endpoint\n",
        "\n",
        "\n",
        "def get_default_dimension(model_name: str) -> int:\n",
        "    \"\"\"Returns the default dimension for the given model_name.\"\"\"\n",
        "\n",
        "    dimension = 512\n",
        "    if not model_name:\n",
        "        return dimension\n",
        "\n",
        "    if \"stable-diffusion-xl\" in model_name or \"sdxl\" in model_name:\n",
        "        dimension = 1024\n",
        "    if \"stable-diffusion-2-1\" in model_name:\n",
        "        dimension = 768\n",
        "    if \"sdxl-turbo\" in model_name:\n",
        "        dimension = 512\n",
        "\n",
        "    return dimension\n",
        "\n",
        "\n",
        "def get_default_guidance_scale(model_name: str) -> int:\n",
        "    \"\"\"Returns the default guidance scale for the given model_name.\"\"\"\n",
        "\n",
        "    guidance_scale = 7.5\n",
        "    if not model_name:\n",
        "        return guidance_scale\n",
        "\n",
        "    if \"lcm\" in model_name or \"sdxl-turbo\" in model_name:\n",
        "        guidance_scale = 0\n",
        "\n",
        "    return guidance_scale\n",
        "\n",
        "\n",
        "def get_default_num_inference_steps(model_name: str) -> int:\n",
        "    \"\"\"Returns the default num_inference_steps for the given model_name.\"\"\"\n",
        "\n",
        "    num_inference_steps = 25\n",
        "    if not model_name:\n",
        "        return num_inference_steps\n",
        "\n",
        "    if \"lcm\" in model_name:\n",
        "        num_inference_steps = 8\n",
        "    elif \"sdxl-turbo\" in model_name:\n",
        "        num_inference_steps = 2\n",
        "\n",
        "    return num_inference_steps\n",
        "\n",
        "\n",
        "def apply_style(style_name: str, positive: str, negative: str = \"\") -> tuple[str, str]:\n",
        "    p, n = styles.get(style_name, styles[DEFAULT_STYLE_NAME])\n",
        "    return p.replace(\"{prompt}\", positive), n + negative\n",
        "\n",
        "\n",
        "def generate_images(\n",
        "    endpoint_name,\n",
        "    style_name=None,\n",
        "    prompt=\"\",\n",
        "    negative_prompt=\"\",\n",
        "    num_samples=1,\n",
        "    guidance_scale=7.5,\n",
        "    num_inference_steps=25,\n",
        "    image_dimension=512,\n",
        ") -> list[Image.Image]:\n",
        "    if not endpoint_name:\n",
        "        raise gr.Error(\"Please select (or deploy) a model first!\")\n",
        "\n",
        "    prompt, negative_prompt = apply_style(style_name, prompt, negative_prompt)\n",
        "    instances = [\n",
        "        {\n",
        "            \"prompt\": prompt,\n",
        "            \"negative_prompt\": negative_prompt,\n",
        "            \"height\": image_dimension,\n",
        "            \"width\": image_dimension,\n",
        "            \"guidance_scale\": guidance_scale,\n",
        "            \"num_inference_steps\": num_inference_steps,\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    if len(instances) == 1 and num_samples > 1:\n",
        "        instances = instances * num_samples\n",
        "\n",
        "    response = get_endpoint(endpoint_name).predict(instances=instances)\n",
        "    images = [base64_to_image(image) for image in response.predictions]\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "def inpaint_generate_images(\n",
        "    endpoint_name: str,\n",
        "    style_name=None,\n",
        "    prompt=\"\",\n",
        "    negative_prompt=\"\",\n",
        "    num_samples=1,\n",
        "    guidance_scale=7.5,\n",
        "    num_inference_steps=25,\n",
        "    dict=None,\n",
        "    image_dimension=512,\n",
        ") -> list[Image.Image]:\n",
        "    if not endpoint_name:\n",
        "        raise gr.Error(\"Please select (or deploy) a model first!\")\n",
        "\n",
        "    prompt, negative_prompt = apply_style(style_name, prompt, negative_prompt)\n",
        "    default_dimension = 512\n",
        "    # Set the default_dimension=1024 if the model is `stable-diffusion-xl-1.0-inpainting-0.1`.\n",
        "    if \"stable-diffusion-xl\" in endpoint_name:\n",
        "        default_dimension = 1024\n",
        "\n",
        "    init_image = (\n",
        "        dict[\"image\"].convert(\"RGB\").resize((default_dimension, default_dimension))\n",
        "    )\n",
        "    mask = dict[\"mask\"].convert(\"RGB\").resize((default_dimension, default_dimension))\n",
        "\n",
        "    instances = [\n",
        "        {\n",
        "            \"prompt\": prompt,\n",
        "            \"negative_prompt\": negative_prompt,\n",
        "            \"image\": image_to_base64(init_image),\n",
        "            \"mask_image\": image_to_base64(mask),\n",
        "            \"height\": image_dimension,\n",
        "            \"width\": image_dimension,\n",
        "            \"guidance_scale\": guidance_scale,\n",
        "            \"num_inference_steps\": num_inference_steps,\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    if len(instances) == 1 and num_samples > 1:\n",
        "        instances = instances * num_samples\n",
        "\n",
        "    response = get_endpoint(endpoint_name).predict(instances=instances)\n",
        "    images = [base64_to_image(image) for image in response.predictions]\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "def instruct_pix2pix_generate_images(\n",
        "    endpoint_name: str,\n",
        "    style_name=None,\n",
        "    prompt=\"\",\n",
        "    negative_prompt=\"\",\n",
        "    num_samples=1,\n",
        "    guidance_scale=7.5,\n",
        "    num_inference_steps=25,\n",
        "    init_image=None,\n",
        "    image_dimension=512,\n",
        ") -> list[Image.Image]:\n",
        "    if not endpoint_name:\n",
        "        raise gr.Error(\"Please select (or deploy) a model first!\")\n",
        "\n",
        "    prompt, negative_prompt = apply_style(style_name, prompt, negative_prompt)\n",
        "    instances = [\n",
        "        {\n",
        "            \"prompt\": prompt,\n",
        "            \"negative_prompt\": negative_prompt,\n",
        "            \"image\": image_to_base64(init_image),\n",
        "            \"height\": image_dimension,\n",
        "            \"width\": image_dimension,\n",
        "            \"guidance_scale\": guidance_scale,\n",
        "            \"num_inference_steps\": num_inference_steps,\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    if len(instances) == 1 and num_samples > 1:\n",
        "        instances = instances * num_samples\n",
        "\n",
        "    response = get_endpoint(endpoint_name).predict(instances=instances)\n",
        "    images = [base64_to_image(image) for image in response.predictions]\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "def upscaler_generate_images(\n",
        "    endpoint_name: str,\n",
        "    style_name=None,\n",
        "    prompt=\"\",\n",
        "    negative_prompt=\"\",\n",
        "    num_samples=1,\n",
        "    guidance_scale=7.5,\n",
        "    num_inference_steps=25,\n",
        "    init_image=None,\n",
        "    image_dimension=512,\n",
        ") -> list[Image.Image]:\n",
        "\n",
        "    if not endpoint_name:\n",
        "        raise gr.Error(\"Please select (or deploy) a model first!\")\n",
        "\n",
        "    prompt, negative_prompt = apply_style(style_name, prompt, negative_prompt)\n",
        "    default_dimension = 256\n",
        "    init_image = init_image.convert(\"RGB\").resize(\n",
        "        (default_dimension, default_dimension)\n",
        "    )\n",
        "    instances = [\n",
        "        {\n",
        "            \"prompt\": prompt,\n",
        "            \"negative_prompt\": negative_prompt,\n",
        "            \"image\": image_to_base64(init_image),\n",
        "            \"height\": image_dimension,\n",
        "            \"width\": image_dimension,\n",
        "            \"guidance_scale\": guidance_scale,\n",
        "            \"num_inference_steps\": num_inference_steps,\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    if len(instances) == 1 and num_samples > 1:\n",
        "        instances = instances * num_samples\n",
        "\n",
        "    response = get_endpoint(endpoint_name).predict(instances=instances)\n",
        "    images = [base64_to_image(image) for image in response.predictions]\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "def controlnet_generate_images(\n",
        "    endpoint_name: str,\n",
        "    style_name=None,\n",
        "    prompt=\"\",\n",
        "    negative_prompt=\"\",\n",
        "    num_samples=1,\n",
        "    guidance_scale=7.5,\n",
        "    num_inference_steps=25,\n",
        "    init_image=None,\n",
        "    image_dimension=512,\n",
        "    canny_low_threshold=100,\n",
        "    canny_high_threshold=200,\n",
        ") -> list[Image.Image]:\n",
        "    if not endpoint_name:\n",
        "        raise gr.Error(\"Please select (or deploy) a model first!\")\n",
        "\n",
        "    prompt, negative_prompt = apply_style(style_name, prompt, negative_prompt)\n",
        "    init_image = init_image.convert(\"RGB\").resize((image_dimension, image_dimension))\n",
        "    canny_image = canny(init_image, canny_low_threshold, canny_high_threshold)\n",
        "\n",
        "    instances = [\n",
        "        {\n",
        "            \"prompt\": prompt,\n",
        "            \"negative_prompt\": negative_prompt,\n",
        "            \"image\": image_to_base64(canny_image),\n",
        "            \"height\": image_dimension,\n",
        "            \"width\": image_dimension,\n",
        "            \"guidance_scale\": guidance_scale,\n",
        "            \"num_inference_steps\": num_inference_steps,\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    if len(instances) == 1 and num_samples > 1:\n",
        "        instances = instances * num_samples\n",
        "\n",
        "    response = get_endpoint(endpoint_name).predict(instances=instances)\n",
        "    images = [base64_to_image(image) for image in response.predictions]\n",
        "\n",
        "    images.insert(0, canny_image)\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "def select_interface(interface_name: str):\n",
        "    if interface_name == \"Text2Image\":\n",
        "        return {\n",
        "            image_input: gr.update(visible=False, value=None, label=\"Upload\"),\n",
        "            image_output: gr.update(value=None),\n",
        "            generate_button: gr.update(visible=True),\n",
        "            inpaint_generate_button: gr.update(visible=False),\n",
        "            instruct_pix2pix_generate_button: gr.update(visible=False),\n",
        "            controlnet_generate_button: gr.update(visible=False),\n",
        "            upscaler_generate_button: gr.update(visible=False),\n",
        "            canny_low_threshold: gr.update(visible=False),\n",
        "            canny_high_threshold: gr.update(visible=False),\n",
        "        }\n",
        "\n",
        "    elif interface_name == \"Inpainting\":\n",
        "        return {\n",
        "            image_input: gr.update(\n",
        "                visible=True, value=None, tool=\"sketch\", label=\"Upload\"\n",
        "            ),\n",
        "            image_output: gr.update(value=None),\n",
        "            generate_button: gr.update(visible=False),\n",
        "            inpaint_generate_button: gr.update(visible=True),\n",
        "            instruct_pix2pix_generate_button: gr.update(visible=False),\n",
        "            controlnet_generate_button: gr.update(visible=False),\n",
        "            upscaler_generate_button: gr.update(visible=False),\n",
        "            canny_low_threshold: gr.update(visible=False),\n",
        "            canny_high_threshold: gr.update(visible=False),\n",
        "        }\n",
        "\n",
        "    elif interface_name == \"Instruct pix2pix\":\n",
        "        return {\n",
        "            image_input: gr.update(\n",
        "                visible=True, value=None, tool=\"None\", label=\"Upload\"\n",
        "            ),\n",
        "            image_output: gr.update(value=None),\n",
        "            generate_button: gr.update(visible=False),\n",
        "            inpaint_generate_button: gr.update(visible=False),\n",
        "            instruct_pix2pix_generate_button: gr.update(visible=True),\n",
        "            controlnet_generate_button: gr.update(visible=False),\n",
        "            upscaler_generate_button: gr.update(visible=False),\n",
        "            canny_low_threshold: gr.update(visible=False),\n",
        "            canny_high_threshold: gr.update(visible=False),\n",
        "        }\n",
        "\n",
        "    elif interface_name == \"ControlNet Canny\":\n",
        "        return {\n",
        "            image_input: gr.update(\n",
        "                visible=True, value=None, tool=\"None\", label=\"Upload a reference image\"\n",
        "            ),\n",
        "            image_output: gr.update(value=None),\n",
        "            generate_button: gr.update(visible=False),\n",
        "            inpaint_generate_button: gr.update(visible=False),\n",
        "            instruct_pix2pix_generate_button: gr.update(visible=False),\n",
        "            controlnet_generate_button: gr.update(visible=True),\n",
        "            upscaler_generate_button: gr.update(visible=False),\n",
        "            canny_low_threshold: gr.update(visible=True),\n",
        "            canny_high_threshold: gr.update(visible=True),\n",
        "        }\n",
        "\n",
        "    elif interface_name == \"SD 4x Upscaler\":\n",
        "        return {\n",
        "            image_input: gr.update(\n",
        "                visible=True, value=None, tool=\"None\", label=\"Upload\"\n",
        "            ),\n",
        "            image_output: gr.update(value=None),\n",
        "            generate_button: gr.update(visible=False),\n",
        "            inpaint_generate_button: gr.update(visible=False),\n",
        "            instruct_pix2pix_generate_button: gr.update(visible=False),\n",
        "            controlnet_generate_button: gr.update(visible=False),\n",
        "            upscaler_generate_button: gr.update(visible=True),\n",
        "            canny_low_threshold: gr.update(visible=False),\n",
        "            canny_high_threshold: gr.update(visible=False),\n",
        "        }\n",
        "\n",
        "\n",
        "def update_default_parameters(model_name: str):\n",
        "    \"\"\"Updates the default inference parameters based on the selected model.\"\"\"\n",
        "    return {\n",
        "        guidance_scale: gr.update(value=get_default_guidance_scale(model_name)),\n",
        "        num_inference_steps: gr.update(\n",
        "            value=get_default_num_inference_steps(model_name)\n",
        "        ),\n",
        "        image_dimension: gr.update(value=get_default_dimension(model_name)),\n",
        "    }\n",
        "\n",
        "\n",
        "tip_text = r\"\"\"\n",
        "1. Select a Vertex prediction endpoint with a model deployed for your chosen task. Mismatched models can lead to unreliable outcomes.\n",
        "2. New model deployment takes ~20 minutes. You can check the progress at [Vertex Online Prediction](https://console.cloud.google.com/vertex-ai/online-prediction/endpoints).\n",
        "3. After the model deployment is complete, restart the playground in Colab to see the updated endpoint list.\n",
        "\"\"\"\n",
        "\n",
        "css = \"\"\"\n",
        ".gradio-container {\n",
        "  width: 90% !important\n",
        "}\n",
        "\"\"\"\n",
        "with gr.Blocks(\n",
        "    css=css, theme=gr.themes.Default(primary_hue=\"orange\", secondary_hue=\"blue\")\n",
        ") as demo:\n",
        "    gr.Markdown(\"# Model Garden Playground for Image Generation\")\n",
        "\n",
        "    interfaces_box = gr.Radio(\n",
        "        show_label=False,\n",
        "        choices=[\n",
        "            \"Text2Image\",\n",
        "            \"Inpainting\",\n",
        "            \"Instruct pix2pix\",\n",
        "            \"ControlNet Canny\",\n",
        "            \"SD 4x Upscaler\",\n",
        "        ],\n",
        "        value=\"Text2Image\",\n",
        "    )\n",
        "    with gr.Accordion(\"How To Use\", open=False):\n",
        "        tip = gr.Markdown(tip_text)\n",
        "\n",
        "    with gr.Row(equal_height=True):\n",
        "        with gr.Column(scale=3):\n",
        "            prompt = gr.Textbox(label=\"Prompt\", lines=1)\n",
        "            negative_prompt = gr.Textbox(label=\"Negative Prompt\", lines=1)\n",
        "        with gr.Column(scale=1):\n",
        "            endpoint_name = gr.Dropdown(\n",
        "                label=\"Select a model previously deployed on Vertex\",\n",
        "                choices=list_endpoints(),\n",
        "                value=None,\n",
        "            )\n",
        "            with gr.Row():\n",
        "                selected_model = gr.Dropdown(\n",
        "                    scale=7,\n",
        "                    label=\"Deploy a new model to Vertex\",\n",
        "                    choices=[\n",
        "                        \"txt2img: runwayml/stable-diffusion-v1-5\",\n",
        "                        \"txt2img: stabilityai/stable-diffusion-2-1\",\n",
        "                        \"txt2img: stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "                        \"txt2img: stabilityai/stable-diffusion-xl-base-1.0 - refiner\",\n",
        "                        \"txt2img: latent-consistency/lcm-sdxl\",\n",
        "                        \"txt2img: latent-consistency/lcm-lora-sdxl\",\n",
        "                        \"txt2img: stabilityai/sdxl-turbo\",\n",
        "                        \"inpainting: runwayml/stable-diffusion-inpainting\",\n",
        "                        \"inpainting: kandinsky-community/kandinsky-2-2-decoder-inpaint\",\n",
        "                        \"inpainting: diffusers/stable-diffusion-xl-1.0-inpainting-0.1\",\n",
        "                        \"instruct-pix2pix: timbrooks/instruct-pix2pix\",\n",
        "                        \"controlnet: lllyasviel/sd-controlnet-canny\",\n",
        "                        \"upscaler: stabilityai/stable-diffusion-x4-upscaler\",\n",
        "                    ],\n",
        "                    value=None,\n",
        "                )\n",
        "                deploy_model_button = gr.Button(\n",
        "                    \"Deploy\", scale=1, variant=\"primary\", min_width=10\n",
        "                )\n",
        "\n",
        "    with gr.Row(equal_height=True):\n",
        "        with gr.Column(scale=2):\n",
        "            generate_button = gr.Button(\"Generate\", variant=\"primary\")\n",
        "            inpaint_generate_button = gr.Button(\n",
        "                \"Generate\", variant=\"primary\", visible=False\n",
        "            )\n",
        "            instruct_pix2pix_generate_button = gr.Button(\n",
        "                \"Generate\", variant=\"primary\", visible=False\n",
        "            )\n",
        "            controlnet_generate_button = gr.Button(\n",
        "                \"Generate\", variant=\"primary\", visible=False\n",
        "            )\n",
        "            upscaler_generate_button = gr.Button(\n",
        "                \"Generate\", variant=\"primary\", visible=False\n",
        "            )\n",
        "\n",
        "            num_samples = gr.Slider(\n",
        "                label=\"Number of samples\", value=1, step=1, minimum=1, maximum=4\n",
        "            )\n",
        "            image_dimension = gr.Slider(\n",
        "                label=\"Image dimension\", value=768, step=256, minimum=512, maximum=1024\n",
        "            )\n",
        "            num_inference_steps = gr.Slider(\n",
        "                label=\"Sampling steps\", value=25, step=1, minimum=1, maximum=100\n",
        "            )\n",
        "            guidance_scale = gr.Slider(\n",
        "                label=\"Guidance scale\", value=7.5, step=0.5, minimum=0, maximum=20.0\n",
        "            )\n",
        "            with gr.Accordion(\"Styles\", open=False):\n",
        "                style_selection = gr.Radio(\n",
        "                    show_label=True,\n",
        "                    container=True,\n",
        "                    interactive=True,\n",
        "                    choices=STYLE_NAMES,\n",
        "                    value=DEFAULT_STYLE_NAME,\n",
        "                    label=\"Image Style\",\n",
        "                )\n",
        "                canny_low_threshold = gr.Slider(\n",
        "                    label=\"Canny low threshold\",\n",
        "                    value=100,\n",
        "                    step=5,\n",
        "                    minimum=1,\n",
        "                    maximum=255,\n",
        "                    visible=False,\n",
        "                )\n",
        "                canny_high_threshold = gr.Slider(\n",
        "                    label=\"Canny high threshold\",\n",
        "                    value=200,\n",
        "                    step=5,\n",
        "                    minimum=1,\n",
        "                    maximum=255,\n",
        "                    visible=False,\n",
        "                )\n",
        "\n",
        "        with gr.Column(scale=5):\n",
        "            with gr.Row(equal_height=True):\n",
        "                image_input = gr.Image(\n",
        "                    tool=\"sketch\",\n",
        "                    type=\"pil\",\n",
        "                    label=\"Upload\",\n",
        "                    height=500,\n",
        "                    interactive=True,\n",
        "                    visible=False,\n",
        "                )\n",
        "                image_output = gr.Gallery(\n",
        "                    label=\"Generated Images\",\n",
        "                    rows=1,\n",
        "                    height=500,\n",
        "                    preview=True,\n",
        "                )\n",
        "\n",
        "    interfaces_box.change(\n",
        "        select_interface,\n",
        "        interfaces_box,\n",
        "        [\n",
        "            image_input,\n",
        "            image_output,\n",
        "            generate_button,\n",
        "            inpaint_generate_button,\n",
        "            instruct_pix2pix_generate_button,\n",
        "            controlnet_generate_button,\n",
        "            upscaler_generate_button,\n",
        "            canny_low_threshold,\n",
        "            canny_high_threshold,\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    endpoint_name.change(\n",
        "        update_default_parameters,\n",
        "        endpoint_name,\n",
        "        [\n",
        "            guidance_scale,\n",
        "            num_inference_steps,\n",
        "            image_dimension,\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    deploy_model_button.click(\n",
        "        deploy_model,\n",
        "        inputs=[selected_model],\n",
        "        outputs=[],\n",
        "    )\n",
        "\n",
        "    generate_button.click(\n",
        "        generate_images,\n",
        "        inputs=[\n",
        "            endpoint_name,\n",
        "            style_selection,\n",
        "            prompt,\n",
        "            negative_prompt,\n",
        "            num_samples,\n",
        "            guidance_scale,\n",
        "            num_inference_steps,\n",
        "            image_dimension,\n",
        "        ],\n",
        "        outputs=image_output,\n",
        "    )\n",
        "\n",
        "    inpaint_generate_button.click(\n",
        "        inpaint_generate_images,\n",
        "        inputs=[\n",
        "            endpoint_name,\n",
        "            style_selection,\n",
        "            prompt,\n",
        "            negative_prompt,\n",
        "            num_samples,\n",
        "            guidance_scale,\n",
        "            num_inference_steps,\n",
        "            image_input,\n",
        "            image_dimension,\n",
        "        ],\n",
        "        outputs=image_output,\n",
        "    )\n",
        "\n",
        "    instruct_pix2pix_generate_button.click(\n",
        "        instruct_pix2pix_generate_images,\n",
        "        inputs=[\n",
        "            endpoint_name,\n",
        "            style_selection,\n",
        "            prompt,\n",
        "            negative_prompt,\n",
        "            num_samples,\n",
        "            guidance_scale,\n",
        "            num_inference_steps,\n",
        "            image_input,\n",
        "            image_dimension,\n",
        "        ],\n",
        "        outputs=image_output,\n",
        "    )\n",
        "\n",
        "    controlnet_generate_button.click(\n",
        "        controlnet_generate_images,\n",
        "        inputs=[\n",
        "            endpoint_name,\n",
        "            style_selection,\n",
        "            prompt,\n",
        "            negative_prompt,\n",
        "            num_samples,\n",
        "            guidance_scale,\n",
        "            num_inference_steps,\n",
        "            image_input,\n",
        "            image_dimension,\n",
        "            canny_low_threshold,\n",
        "            canny_high_threshold,\n",
        "        ],\n",
        "        outputs=image_output,\n",
        "    )\n",
        "\n",
        "    upscaler_generate_button.click(\n",
        "        upscaler_generate_images,\n",
        "        inputs=[\n",
        "            endpoint_name,\n",
        "            style_selection,\n",
        "            prompt,\n",
        "            negative_prompt,\n",
        "            num_samples,\n",
        "            guidance_scale,\n",
        "            num_inference_steps,\n",
        "            image_input,\n",
        "            image_dimension,\n",
        "        ],\n",
        "        outputs=image_output,\n",
        "    )\n",
        "\n",
        "demo.queue(concurrency_count=5, max_size=10)\n",
        "demo.launch(share=True, inline=False, inbrowser=True, debug=True, show_error=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_pytorch_stable_diffusion_gradio.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d9bbf86da5e"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Vertex AI Model Garden - Stable Diffusion Gradio UI\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_pytorch_stable_diffusion_gradio.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_stable_diffusion_gradio.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates starting a playground based on [Gradio UI](https://www.gradio.app/), which allows users to interact with the stable diffusion models more easily and intuitively. The playground now support `text-to-image`, `image-to-image` and `image-inpainting` tasks.\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Deploy model to a [Vertex AI Endpoint resource](https://cloud.google.com/vertex-ai/docs/predictions/using-private-endpoints).\n",
        "- Run online predictions for `text-to-image`, `image-to-image` and `image-inpainting` tasks, from the UI.\n",
        "- Adjust the parameters, such as prompt, negative_prompt, num_inference_steps, and check out the generated images.\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "855d6b96f291"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "! pip3 install --upgrade gradio==3.48.0\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
        "\n",
        "# Set up the default SERVICE_ACCOUNT.\n",
        "SERVICE_ACCOUNT = None\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "\n",
        "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
        "\n",
        "# The pre-built serving docker image. It contains serving scripts and models.\n",
        "SERVE_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-diffusers-serve-opt:20240223_1230_RC00\"\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user(project_id=PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cc26e68d7b0"
      },
      "outputs": [],
      "source": [
        "# @title Start the SD Playground on Gradio UI\n",
        "\n",
        "# @markdown This is a simple playground similar to the popular [stable diffusion webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui).\n",
        "# @markdown This UI is avaible in a separate browser tab if you click the public URL after the cell runs.\n",
        "# @markdown The public URL is something similar to \"https://####.gradio.live\". Click the URL to open the playground.\n",
        "\n",
        "# @markdown Before you start, you need to select an existing Vertex prediction endpoint from the dropdown list\n",
        "# @markdown which has been deployed in the project and region; If no models were deployed in the past, you can\n",
        "# @markdown create a new Vertex prediction endpoint by selecting your favorite model and click \"Deploy\".\n",
        "\n",
        "# @markdown Four tasks `text-to-image`, `image-inpainting`, `instruct-pix2pix` and `SD 4x upscaler` are currently supported.\n",
        "\n",
        "import base64\n",
        "from io import BytesIO\n",
        "\n",
        "import gradio as gr\n",
        "from google.cloud import aiplatform\n",
        "from PIL import Image\n",
        "\n",
        "# The pre-built serving docker image. It contains serving scripts and models.\n",
        "SERVE_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-diffusers-serve-opt:20240306_1230_RC00\"\n",
        "\n",
        "\n",
        "def base64_to_image(image_str: str) -> Image:\n",
        "    \"\"\"Convert base64 encoded string to an image.\"\"\"\n",
        "    image = Image.open(BytesIO(base64.b64decode(image_str)))\n",
        "    return image\n",
        "\n",
        "\n",
        "def image_to_base64(image: Image, format=\"JPEG\") -> str:\n",
        "    buffer = BytesIO()\n",
        "    image.save(buffer, format=format)\n",
        "    image_str = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
        "    return image_str\n",
        "\n",
        "\n",
        "def list_endpoints() -> list[str]:\n",
        "    \"\"\"Returns all valid prediction endpoints for in the project and region.\"\"\"\n",
        "    # Gets all the valid endpoints in the project and region.\n",
        "    endpoints = aiplatform.Endpoint.list(order_by=\"create_time desc\")\n",
        "    # Filters out the endpoints which do not have a deployed model\n",
        "    endpoints = list(filter(lambda endpoint: endpoint.traffic_split, endpoints))\n",
        "\n",
        "    endpoint_names = list(\n",
        "        map(\n",
        "            lambda endpoint: f\"{endpoint.name} - {endpoint.display_name[:40]}\",\n",
        "            endpoints,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if not endpoint_names:\n",
        "        raise gr.Warning(\n",
        "            \"No prediction endpoints were found. Please create an Endpoint first.\"\n",
        "        )\n",
        "\n",
        "    return endpoint_names\n",
        "\n",
        "\n",
        "def get_endpoint(endpoint_name: str) -> aiplatform.Endpoint:\n",
        "    \"\"\"Returns a Vertex endpoint for the given endpoint_name.\"\"\"\n",
        "\n",
        "    endpoint_id = endpoint_name.split(\" - \")[0]\n",
        "    endpoint = aiplatform.Endpoint(\n",
        "        f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_id}\"\n",
        "    )\n",
        "\n",
        "    return endpoint\n",
        "\n",
        "\n",
        "def get_task_name(model_name: str) -> str:\n",
        "    \"\"\"Returns the corresponding task name for the given model_name.\"\"\"\n",
        "\n",
        "    model_to_task_dict = {\n",
        "        \"runwayml/stable-diffusion-v1-5\": \"text-to-image\",\n",
        "        \"stabilityai/stable-diffusion-2-1\": \"text-to-image\",\n",
        "        \"stabilityai/stable-diffusion-xl-base-1.0\": \"text-to-image-sdxl\",\n",
        "        \"stabilityai/stable-diffusion-xl-base-1.0 - refiner\": \"text-to-image-refiner\",\n",
        "        \"latent-consistency/lcm-sdxl\": \"text-to-image-sdxl-lcm\",\n",
        "        \"latent-consistency/lcm-lora-sdxl\": \"text-to-image-sdxl-lcm-lora\",\n",
        "        \"stabilityai/sdxl-turbo\": \"text-to-image-sdxl-turbo\",\n",
        "        \"runwayml/stable-diffusion-inpainting\": \"image-inpainting\",\n",
        "        \"kandinsky-community/kandinsky-2-2-decoder-inpaint\": \"image-inpainting\",\n",
        "        \"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\": \"image-inpainting\",\n",
        "        \"timbrooks/instruct-pix2pix\": \"instruct-pix2pix\",\n",
        "        \"stabilityai/stable-diffusion-x4-upscaler\": \"conditioned-super-res\",\n",
        "    }\n",
        "\n",
        "    if model_name not in model_to_task_dict.keys():\n",
        "        raise gr.Error(\"Please select a valid model name for Endpoint creation.\")\n",
        "\n",
        "    return model_to_task_dict[model_name]\n",
        "\n",
        "\n",
        "def deploy_model(model_name: str) -> aiplatform.Endpoint:\n",
        "    \"\"\"Creates a new Vertex prediction endpoint and deploys a model to it.\"\"\"\n",
        "    refiner_model_id = \"stabilityai/stable-diffusion-xl-refiner-1.0\"\n",
        "\n",
        "    if not model_name:\n",
        "        raise gr.Error(\"Please select a valid model name for model list.\")\n",
        "        return\n",
        "\n",
        "    gr.Info(\"Model is being deployed. It may take ~20 minutes to complete.\")\n",
        "\n",
        "    task_name = get_task_name(model_name)\n",
        "    model_id = model_name\n",
        "    if (\n",
        "        model_name == \"stabilityai/stable-diffusion-xl-base-1.0 - refiner\"\n",
        "        or model_name == \"latent-consistency/lcm-sdxl\"\n",
        "        or model_name == \"latent-consistency/lcm-lora-sdxl\"\n",
        "    ):\n",
        "        model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "\n",
        "    endpoint = aiplatform.Endpoint.create(display_name=model_name)\n",
        "    serving_env = {\n",
        "        \"MODEL_ID\": model_id,\n",
        "        \"TASK\": task_name,\n",
        "    }\n",
        "    if model_name == \"stabilityai/stable-diffusion-xl-base-1.0 - refiner\":\n",
        "        serving_env = {\n",
        "            **serving_env,\n",
        "            \"REFINER_MODEL_ID\": refiner_model_id,\n",
        "        }\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=SERVE_DOCKER_URI,\n",
        "        serving_container_ports=[7080],\n",
        "        serving_container_predict_route=\"/predictions/diffusers_serving\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "        serving_container_environment_variables=serving_env,\n",
        "    )\n",
        "    machine_type = \"g2-standard-8\"\n",
        "    accelerator_type = \"NVIDIA_L4\"\n",
        "\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=1,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=SERVICE_ACCOUNT,\n",
        "    )\n",
        "\n",
        "    gr.Info(\"Model have been deployed successfully.\")\n",
        "\n",
        "    return endpoint\n",
        "\n",
        "\n",
        "def get_default_dimension(model_name: str) -> int:\n",
        "    \"\"\"Returns the default dimension for the given model_name.\"\"\"\n",
        "\n",
        "    dimension = 512\n",
        "    if not model_name:\n",
        "        return dimension\n",
        "\n",
        "    if \"stable-diffusion-xl\" in model_name or \"sdxl\" in model_name:\n",
        "        dimension = 1024\n",
        "    elif \"stable-diffusion-2-1\" in model_name:\n",
        "        dimension = 768\n",
        "\n",
        "    return dimension\n",
        "\n",
        "\n",
        "def get_default_guidance_scale(model_name: str) -> int:\n",
        "    \"\"\"Returns the default guidance scale for the given model_name.\"\"\"\n",
        "\n",
        "    guidance_scale = 7.5\n",
        "    if not model_name:\n",
        "        return guidance_scale\n",
        "\n",
        "    if \"lcm\" in model_name or \"sdxl-turbo\" in model_name:\n",
        "        guidance_scale = 0\n",
        "\n",
        "    return guidance_scale\n",
        "\n",
        "\n",
        "def get_default_num_inference_steps(model_name: str) -> int:\n",
        "    \"\"\"Returns the default num_inference_steps for the given model_name.\"\"\"\n",
        "\n",
        "    num_inference_steps = 25\n",
        "    if not model_name:\n",
        "        return num_inference_steps\n",
        "\n",
        "    if \"lcm\" in model_name:\n",
        "        num_inference_steps = 8\n",
        "    elif \"sdxl-turbo\" in model_name:\n",
        "        num_inference_steps = 2\n",
        "\n",
        "    return num_inference_steps\n",
        "\n",
        "\n",
        "def generate_images(\n",
        "    endpoint_name,\n",
        "    prompt,\n",
        "    negative_prompt=\"\",\n",
        "    num_samples=1,\n",
        "    guidance_scale=7.5,\n",
        "    num_inference_steps=25,\n",
        "    height=512,\n",
        "    width=512,\n",
        ") -> list[Image.Image]:\n",
        "    if not endpoint_name:\n",
        "        raise gr.Error(\"Please select (or deploy) a model first!\")\n",
        "\n",
        "    instances = [\n",
        "        {\n",
        "            \"prompt\": prompt,\n",
        "            \"negative_prompt\": negative_prompt,\n",
        "            \"height\": height,\n",
        "            \"width\": width,\n",
        "            \"guidance_scale\": guidance_scale,\n",
        "            \"num_inference_steps\": num_inference_steps,\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    if len(instances) == 1 and num_samples > 1:\n",
        "        instances = instances * num_samples\n",
        "\n",
        "    response = get_endpoint(endpoint_name).predict(instances=instances)\n",
        "    images = [base64_to_image(image) for image in response.predictions]\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "def inpaint_generate_images(\n",
        "    endpoint_name: str,\n",
        "    prompt=\"\",\n",
        "    negative_prompt=\"\",\n",
        "    num_samples=1,\n",
        "    guidance_scale=7.5,\n",
        "    num_inference_steps=25,\n",
        "    dict=None,\n",
        "    height=512,\n",
        "    width=512,\n",
        ") -> list[Image.Image]:\n",
        "    if not endpoint_name:\n",
        "        raise gr.Error(\"Please select (or deploy) a model first!\")\n",
        "\n",
        "    default_dimension = 512\n",
        "    # Set the default_dimension=1024 if the model is `stable-diffusion-xl-1.0-inpainting-0.1`.\n",
        "    if \"stable-diffusion-xl\" in endpoint_name:\n",
        "        default_dimension = 1024\n",
        "\n",
        "    init_image = (\n",
        "        dict[\"image\"].convert(\"RGB\").resize((default_dimension, default_dimension))\n",
        "    )\n",
        "    mask = dict[\"mask\"].convert(\"RGB\").resize((default_dimension, default_dimension))\n",
        "\n",
        "    instances = [\n",
        "        {\n",
        "            \"prompt\": prompt,\n",
        "            \"negative_prompt\": negative_prompt,\n",
        "            \"image\": image_to_base64(init_image),\n",
        "            \"mask_image\": image_to_base64(mask),\n",
        "            \"height\": height,\n",
        "            \"width\": width,\n",
        "            \"guidance_scale\": guidance_scale,\n",
        "            \"num_inference_steps\": num_inference_steps,\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    if len(instances) == 1 and num_samples > 1:\n",
        "        instances = instances * num_samples\n",
        "\n",
        "    response = get_endpoint(endpoint_name).predict(instances=instances)\n",
        "    images = [base64_to_image(image) for image in response.predictions]\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "def instruct_pix2pix_generate_images(\n",
        "    endpoint_name: str,\n",
        "    prompt=\"\",\n",
        "    negative_prompt=\"\",\n",
        "    num_samples=1,\n",
        "    guidance_scale=7.5,\n",
        "    num_inference_steps=25,\n",
        "    init_image=None,\n",
        "    height=512,\n",
        "    width=512,\n",
        ") -> list[Image.Image]:\n",
        "    if not endpoint_name:\n",
        "        raise gr.Error(\"Please select (or deploy) a model first!\")\n",
        "\n",
        "    instances = [\n",
        "        {\n",
        "            \"prompt\": prompt,\n",
        "            \"negative_prompt\": negative_prompt,\n",
        "            \"image\": image_to_base64(init_image),\n",
        "            \"height\": height,\n",
        "            \"width\": width,\n",
        "            \"guidance_scale\": guidance_scale,\n",
        "            \"num_inference_steps\": num_inference_steps,\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    if len(instances) == 1 and num_samples > 1:\n",
        "        instances = instances * num_samples\n",
        "\n",
        "    response = get_endpoint(endpoint_name).predict(instances=instances)\n",
        "    images = [base64_to_image(image) for image in response.predictions]\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "def upscaler_generate_images(\n",
        "    endpoint_name: str,\n",
        "    prompt=\"\",\n",
        "    negative_prompt=\"\",\n",
        "    num_samples=1,\n",
        "    guidance_scale=7.5,\n",
        "    num_inference_steps=25,\n",
        "    init_image=None,\n",
        "    height=512,\n",
        "    width=512,\n",
        ") -> list[Image.Image]:\n",
        "    if not endpoint_name:\n",
        "        raise gr.Error(\"Please select (or deploy) a model first!\")\n",
        "\n",
        "    default_dimension = 256\n",
        "\n",
        "    init_image = init_image.convert(\"RGB\").resize(\n",
        "        (default_dimension, default_dimension)\n",
        "    )\n",
        "    instances = [\n",
        "        {\n",
        "            \"prompt\": prompt,\n",
        "            \"negative_prompt\": negative_prompt,\n",
        "            \"image\": image_to_base64(init_image),\n",
        "            \"height\": height,\n",
        "            \"width\": width,\n",
        "            \"guidance_scale\": guidance_scale,\n",
        "            \"num_inference_steps\": num_inference_steps,\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    if len(instances) == 1 and num_samples > 1:\n",
        "        instances = instances * num_samples\n",
        "\n",
        "    response = get_endpoint(endpoint_name).predict(instances=instances)\n",
        "    images = [base64_to_image(image) for image in response.predictions]\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "def select_interface(interface_name: str):\n",
        "    if interface_name == \"Text2Image pipeline\":\n",
        "        return {\n",
        "            endpoint_name: gr.update(visible=True, value=None),\n",
        "            prompt: gr.update(visible=True, value=None),\n",
        "            negative_prompt: gr.update(visible=True, value=None),\n",
        "            image_input: gr.update(visible=False, value=None),\n",
        "            generate_button: gr.update(visible=True),\n",
        "            inpaint_generate_button: gr.update(visible=False),\n",
        "            instruct_pix2pix_generate_button: gr.update(visible=False),\n",
        "            upscaler_generate_button: gr.update(visible=False),\n",
        "        }\n",
        "\n",
        "    elif interface_name == \"Inpaint Pipeline\":\n",
        "        return {\n",
        "            endpoint_name: gr.update(visible=True, value=None),\n",
        "            prompt: gr.update(visible=True, value=None),\n",
        "            negative_prompt: gr.update(visible=True, value=None),\n",
        "            image_input: gr.update(visible=True, value=None, tool=\"sketch\"),\n",
        "            generate_button: gr.update(visible=False),\n",
        "            inpaint_generate_button: gr.update(visible=True),\n",
        "            instruct_pix2pix_generate_button: gr.update(visible=False),\n",
        "            upscaler_generate_button: gr.update(visible=False),\n",
        "        }\n",
        "\n",
        "    elif interface_name == \"Instruct pix2pix Pipeline\":\n",
        "        return {\n",
        "            endpoint_name: gr.update(visible=True, value=None),\n",
        "            prompt: gr.update(visible=True, value=None),\n",
        "            negative_prompt: gr.update(visible=True, value=None),\n",
        "            image_input: gr.update(visible=True, value=None, tool=\"None\"),\n",
        "            generate_button: gr.update(visible=False),\n",
        "            inpaint_generate_button: gr.update(visible=False),\n",
        "            instruct_pix2pix_generate_button: gr.update(visible=True),\n",
        "            upscaler_generate_button: gr.update(visible=False),\n",
        "        }\n",
        "\n",
        "    elif interface_name == \"SD 4x Upscaler Pipeline\":\n",
        "        return {\n",
        "            endpoint_name: gr.update(visible=True, value=None),\n",
        "            prompt: gr.update(visible=True, value=None),\n",
        "            negative_prompt: gr.update(visible=True, value=None),\n",
        "            image_input: gr.update(visible=True, value=None, tool=\"None\"),\n",
        "            generate_button: gr.update(visible=False),\n",
        "            inpaint_generate_button: gr.update(visible=False),\n",
        "            instruct_pix2pix_generate_button: gr.update(visible=False),\n",
        "            upscaler_generate_button: gr.update(visible=True),\n",
        "        }\n",
        "\n",
        "\n",
        "def update_default_parameters(model_name: str):\n",
        "    \"\"\"Updates the default inference parameters based on the selected model.\"\"\"\n",
        "    return {\n",
        "        guidance_scale: gr.update(value=get_default_guidance_scale(model_name)),\n",
        "        num_inference_steps: gr.update(\n",
        "            value=get_default_num_inference_steps(model_name)\n",
        "        ),\n",
        "        height: gr.update(value=get_default_dimension(model_name)),\n",
        "        width: gr.update(value=get_default_dimension(model_name)),\n",
        "    }\n",
        "\n",
        "\n",
        "with gr.Blocks(\n",
        "    theme=gr.themes.Default(primary_hue=\"orange\", secondary_hue=\"blue\")\n",
        ") as demo:\n",
        "    gr.Markdown(\"# Stable Diffusion Playground \")\n",
        "\n",
        "    with gr.Tab(\"Tasks\"):\n",
        "        interfaces_box = gr.Radio(\n",
        "            show_label=False,\n",
        "            choices=[\n",
        "                \"Text2Image pipeline\",\n",
        "                \"Inpaint Pipeline\",\n",
        "                \"Instruct pix2pix Pipeline\",\n",
        "                \"SD 4x Upscaler Pipeline\",\n",
        "            ],\n",
        "            value=\"Text2Image pipeline\",\n",
        "        )\n",
        "\n",
        "    with gr.Row(equal_height=True):\n",
        "        with gr.Column(scale=3):\n",
        "            prompt = gr.Textbox(label=\"Prompt\", lines=1)\n",
        "            negative_prompt = gr.Textbox(label=\"Negative Prompt\", lines=1)\n",
        "        with gr.Column(scale=1):\n",
        "            endpoint_name = gr.Dropdown(\n",
        "                label=\"Select a model previously deployed on Vertex\",\n",
        "                choices=list_endpoints(),\n",
        "                value=None,\n",
        "            )\n",
        "            with gr.Row():\n",
        "                selected_model = gr.Dropdown(\n",
        "                    scale=7,\n",
        "                    label=\"Deploy a new model to Vertex\",\n",
        "                    choices=[\n",
        "                        \"runwayml/stable-diffusion-v1-5\",\n",
        "                        \"stabilityai/stable-diffusion-2-1\",\n",
        "                        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "                        \"stabilityai/stable-diffusion-xl-base-1.0 - refiner\",\n",
        "                        \"latent-consistency/lcm-sdxl\",\n",
        "                        \"latent-consistency/lcm-lora-sdxl\",\n",
        "                        \"stabilityai/sdxl-turbo\",\n",
        "                        \"runwayml/stable-diffusion-inpainting\",\n",
        "                        \"kandinsky-community/kandinsky-2-2-decoder-inpaint\",\n",
        "                        \"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\",\n",
        "                        \"timbrooks/instruct-pix2pix\",\n",
        "                        \"stabilityai/stable-diffusion-x4-upscaler\",\n",
        "                    ],\n",
        "                    value=None,\n",
        "                )\n",
        "                deploy_model_button = gr.Button(\n",
        "                    \"Deploy\", scale=1, variant=\"primary\", min_width=10\n",
        "                )\n",
        "\n",
        "    with gr.Row(equal_height=True):\n",
        "        with gr.Column(scale=1):\n",
        "            generate_button = gr.Button(\"Generate\", variant=\"primary\")\n",
        "            inpaint_generate_button = gr.Button(\n",
        "                \"Generate\", variant=\"primary\", visible=False\n",
        "            )\n",
        "            instruct_pix2pix_generate_button = gr.Button(\n",
        "                \"Generate\", variant=\"primary\", visible=False\n",
        "            )\n",
        "            upscaler_generate_button = gr.Button(\n",
        "                \"Generate\", variant=\"primary\", visible=False\n",
        "            )\n",
        "\n",
        "            num_samples = gr.Slider(\n",
        "                label=\"Number of samples\", value=1, step=1, minimum=1, maximum=4\n",
        "            )\n",
        "            height = gr.Slider(\n",
        "                label=\"Height\", value=768, step=256, minimum=512, maximum=1024\n",
        "            )\n",
        "            width = gr.Slider(\n",
        "                label=\"Width\", value=768, step=256, minimum=512, maximum=1024\n",
        "            )\n",
        "            num_inference_steps = gr.Slider(\n",
        "                label=\"Sampling steps\", value=25, step=1, minimum=1, maximum=100\n",
        "            )\n",
        "            guidance_scale = gr.Slider(\n",
        "                label=\"Guidance scale\", value=7.5, step=0.5, minimum=0, maximum=20.0\n",
        "            )\n",
        "\n",
        "        with gr.Column(scale=3):\n",
        "            with gr.Row(equal_height=True):\n",
        "                image_input = gr.Image(\n",
        "                    source=\"upload\",\n",
        "                    tool=\"sketch\",\n",
        "                    type=\"pil\",\n",
        "                    label=\"Upload\",\n",
        "                    visible=False,\n",
        "                    height=400,\n",
        "                )\n",
        "                image_output = gr.Gallery(\n",
        "                    show_label=False, rows=1, height=400, preview=True\n",
        "                )\n",
        "\n",
        "    interfaces_box.change(\n",
        "        select_interface,\n",
        "        interfaces_box,\n",
        "        [\n",
        "            endpoint_name,\n",
        "            prompt,\n",
        "            negative_prompt,\n",
        "            image_input,\n",
        "            generate_button,\n",
        "            inpaint_generate_button,\n",
        "            instruct_pix2pix_generate_button,\n",
        "            upscaler_generate_button,\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    endpoint_name.change(\n",
        "        update_default_parameters,\n",
        "        endpoint_name,\n",
        "        [\n",
        "            guidance_scale,\n",
        "            num_inference_steps,\n",
        "            height,\n",
        "            width,\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    deploy_model_button.click(\n",
        "        deploy_model,\n",
        "        inputs=[selected_model],\n",
        "        outputs=[],\n",
        "    )\n",
        "\n",
        "    generate_button.click(\n",
        "        generate_images,\n",
        "        inputs=[\n",
        "            endpoint_name,\n",
        "            prompt,\n",
        "            negative_prompt,\n",
        "            num_samples,\n",
        "            guidance_scale,\n",
        "            num_inference_steps,\n",
        "            height,\n",
        "            width,\n",
        "        ],\n",
        "        outputs=image_output,\n",
        "    )\n",
        "\n",
        "    inpaint_generate_button.click(\n",
        "        inpaint_generate_images,\n",
        "        inputs=[\n",
        "            endpoint_name,\n",
        "            prompt,\n",
        "            negative_prompt,\n",
        "            num_samples,\n",
        "            guidance_scale,\n",
        "            num_inference_steps,\n",
        "            image_input,\n",
        "            height,\n",
        "            width,\n",
        "        ],\n",
        "        outputs=image_output,\n",
        "    )\n",
        "\n",
        "    instruct_pix2pix_generate_button.click(\n",
        "        instruct_pix2pix_generate_images,\n",
        "        inputs=[\n",
        "            endpoint_name,\n",
        "            prompt,\n",
        "            negative_prompt,\n",
        "            num_samples,\n",
        "            guidance_scale,\n",
        "            num_inference_steps,\n",
        "            image_input,\n",
        "            height,\n",
        "            width,\n",
        "        ],\n",
        "        outputs=image_output,\n",
        "    )\n",
        "\n",
        "    upscaler_generate_button.click(\n",
        "        upscaler_generate_images,\n",
        "        inputs=[\n",
        "            endpoint_name,\n",
        "            prompt,\n",
        "            negative_prompt,\n",
        "            num_samples,\n",
        "            guidance_scale,\n",
        "            num_inference_steps,\n",
        "            image_input,\n",
        "            height,\n",
        "            width,\n",
        "        ],\n",
        "        outputs=image_output,\n",
        "    )\n",
        "\n",
        "demo.queue(concurrency_count=5, max_size=10)\n",
        "demo.launch(share=True, inline=False, inbrowser=True, debug=True, show_error=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_pytorch_stable_diffusion_gradio.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

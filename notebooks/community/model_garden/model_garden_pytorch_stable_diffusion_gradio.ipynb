{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7d9bbf86da5e"
      },
      "outputs": [],
      "source": [
        "# Copyright 2026 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Vertex AI Model Garden GenAI Workshop for Image Generation (inspired by Stable Diffusion WebUI)\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_pytorch_stable_diffusion_gradio.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_stable_diffusion_gradio.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates starting a playground based on [Gradio UI](https://www.gradio.app/), inspired by the popular [Stable Diffusion WebUI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) project, which allows users to interact with the stable diffusion models more easily and intuitively. The playground now support `text-to-image`, `image-inpainting`, `controlnet-canny`, `instruct-pix2pix`, and `SD 4x upscaler` tasks.\n",
        "\n",
        "This notebook also supports Dreambooth finetune the stable diffusion 1.5/2.1 models.\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Deploy model to a [Vertex AI Endpoint resource](https://cloud.google.com/vertex-ai/docs/predictions/using-private-endpoints).\n",
        "- Run online predictions for `text-to-image`, `image-inpainting`, `controlnet-canny`, `instruct-pix2pix`, and `SD 4x upscaler` tasks, from the UI.\n",
        "- Adjust the parameters, such as prompt, negative_prompt, num_inference_steps, and check out the generated images for best image quality.\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "879fca33129c"
      },
      "source": [
        "## Run the playground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "855d6b96f291"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project and prepare the dependencies\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown 2. **[Optional]** [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs. Set the BUCKET_URI for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`) should be located in the same region as where the notebook was launched. Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\"). If not set, a unique GCS bucket will be created instead.\n",
        "\n",
        "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown 3. **[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment.\n",
        "\n",
        "REGION = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown 4. If you want to run predictions with A100 80GB or H100 GPUs, we recommend using the regions listed below. **NOTE:** Make sure you have associated quota in selected regions. Click the links to see your current quota for each GPU type: [Nvidia A100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_a100_80gb_gpus), [Nvidia H100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_h100_gpus). You can request for quota following the instructions at [\"Request a higher quota\"](https://cloud.google.com/docs/quota/view-manage#requesting_higher_quota).\n",
        "\n",
        "# @markdown | Machine Type | Accelerator Type | Recommended Regions |\n",
        "# @markdown | ----------- | ----------- | ----------- |\n",
        "# @markdown | a2-ultragpu-1g | 1 NVIDIA_A100_80GB | us-central1, us-east4, europe-west4, asia-southeast1, us-east4 |\n",
        "# @markdown | a3-highgpu-2g | 2 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "# @markdown | a3-highgpu-4g | 4 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "# @markdown | a3-highgpu-8g | 8 NVIDIA_H100_80GB | us-central1, europe-west4, us-west1, asia-southeast1 |\n",
        "\n",
        "! pip3 install --upgrade gradio==4.29.0 opencv-python\n",
        "# Uninstall nest-asyncio and uvloop as a workaround to https://github.com/gradio-app/gradio/issues/8238#issuecomment-2101066984\n",
        "! pip3 uninstall --yes nest-asyncio uvloop\n",
        "# A workaround for the compatibility between the fastapi and pydantic\n",
        "! pip3 install fastapi==0.112.3\n",
        "\n",
        "import datetime\n",
        "import glob\n",
        "import importlib\n",
        "import os\n",
        "import shutil\n",
        "import uuid\n",
        "from itertools import chain\n",
        "\n",
        "import cv2\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "from google.cloud import aiplatform, storage\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
        "common_util = importlib.import_module(\n",
        "    \"vertex-ai-samples.notebooks.community.model_garden.docker_source_codes.notebook_util.common_util\"\n",
        ")\n",
        "\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "if not REGION:\n",
        "    if not os.environ.get(\"GOOGLE_CLOUD_REGION\"):\n",
        "        raise ValueError(\n",
        "            \"REGION must be set. See\"\n",
        "            \" https://cloud.google.com/vertex-ai/docs/general/locations for\"\n",
        "            \" available cloud locations.\"\n",
        "        )\n",
        "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
        "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
        "\n",
        "# Cloud Storage bucket for storing the experiment artifacts.\n",
        "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
        "# prefer using your own GCS bucket, change the value yourself below.\n",
        "now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "\n",
        "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
        "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}-{str(uuid.uuid4())[:4]}\"\n",
        "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
        "else:\n",
        "    assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
        "    shell_output = ! gsutil ls -Lb {BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
        "    bucket_region = shell_output[0].strip().lower()\n",
        "    if bucket_region != REGION:\n",
        "        raise ValueError(\n",
        "            \"Bucket region %s is different from notebook region %s\"\n",
        "            % (bucket_region, REGION)\n",
        "        )\n",
        "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
        "\n",
        "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
        "MODEL_BUCKET = os.path.join(BUCKET_URI, \"stable_diffusion\")\n",
        "\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "# Gets the default SERVICE_ACCOUNT.\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
        "\n",
        "\n",
        "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
        "\n",
        "! gcloud config set project $PROJECT_ID\n",
        "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/storage.admin\"\n",
        "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/aiplatform.user\"\n",
        "\n",
        "# @markdown Set use_dedicated_endpoint to False if you don't want to use [dedicated endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment#create-dedicated-endpoint). Note that [dedicated endpoint does not support VPC Service Controls](https://cloud.google.com/vertex-ai/docs/predictions/choose-endpoint-type), uncheck the box if you are using VPC-SC.\n",
        "use_dedicated_endpoint = True  # @param {type:\"boolean\"}\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1cc26e68d7b0"
      },
      "outputs": [],
      "source": [
        "# @title Start the playground\n",
        "\n",
        "# @markdown This is a playground for image generation similar to the popular [Stable Diffusion WebUI](https://github.com/AUTOMATIC1111/stable-diffusion-webui).\n",
        "# @markdown After the cell runs, this playground is available in a separate browser tab if you click the public URL.\n",
        "# @markdown Something similar to [\"https://####.gradio.live\"](#) in the output of the cell.\n",
        "\n",
        "# @markdown For Image generations, five tasks `text-to-image`, `image-inpainting`, `controlnet-canny`, `instruct-pix2pix` and `SD 4x upscaler` are currently supported.\n",
        "\n",
        "# @markdown **How to use:**\n",
        "# @markdown 1. Important: Notebook cell reruns create new public URLs. Previous URLs will stop working.\n",
        "# @markdown 1. Before you start, you need to select a Vertex prediction endpoint, with a matching model deployed to the endpoint\n",
        "# @markdown from the endpoint dropdown list, that has been deployed in the project and region;\n",
        "# @markdown 1. Make sure the selected endpoint/model match with the chosen task. Mismatched task and model will produce unreliable results.\n",
        "# @markdown 1. If no models were deployed in the past, you can create a new Vertex prediction\n",
        "# @markdown endpoint by selecting your favorite model and click \"Deploy\".\n",
        "# @markdown 1. New model deployment takes ~20 minutes. You can check the progress at [Vertex Online Prediction](https://console.cloud.google.com/vertex-ai/online-prediction/endpoints).\n",
        "# @markdown 1. Adjust the prompt/negative-prompt, image-dimension, inference steps, guidance-scale to achieve the optimum image quality and inference latency.\n",
        "# @markdown 1. Don't forget to undeploy the models after all the experiment to avoid continuous charges to the project.\n",
        "\n",
        "# @markdown **Note: we support the following models now:**\n",
        "# @markdown Other models (with the same task) may work, but they are not tested so use with caution.\n",
        "# @markdown 1. Text-to-Image models\n",
        "# @markdown    > stabilityai/stable-diffusion-2-1 \\\n",
        "# @markdown    > stabilityai/stable-diffusion-xl-base-1.0 \\\n",
        "# @markdown    > stabilityai/stable-diffusion-xl-base-1.0 - refiner \\\n",
        "# @markdown    > latent-consistency/lcm-sdxl \\\n",
        "# @markdown    > latent-consistency/lcm-lora-sdxl \\\n",
        "# @markdown    > stabilityai/sdxl-turbo \\\n",
        "# @markdown    > runDiffusion/Juggernaut-XL-Lightning \\\n",
        "# @markdown    > stablediffusionapi/juggernaut-xl-v9 \\\n",
        "# @markdown    > lykon/dreamshaper-xl-v2-turbo \\\n",
        "# @markdown    > stablediffusionapi/anything-xl \\\n",
        "# @markdown    > stablediffusionapi/turbovision-xl \\\n",
        "# @markdown    > bytedance/sdxl-lightning\n",
        "# @markdown 1. Image-inpainting models\n",
        "# @markdown    > kandinsky-community/kandinsky-2-2-decoder-inpaint \\\n",
        "# @markdown    > diffusers/stable-diffusion-xl-1.0-inpainting-0.1\n",
        "# @markdown 1. Controlnet-canny models\n",
        "# @markdown    > lllyasviel/sd-controlnet-canny\n",
        "# @markdown 1. Instruct-pix2pix models\n",
        "# @markdown    > timbrooks/instruct-pix2pix\n",
        "# @markdown 1. SD 4x upscaler models\n",
        "# @markdown    > stabilityai/stable-diffusion-x4-upscaler\n",
        "\n",
        "# @markdown For dreambooth finetune task, we now support finetuning from the following base models: \\\n",
        "# @markdown 1. Stable diffusion text to image models\n",
        "# @markdown    > stabilityai/stable-diffusion-2-1 with a resolution of 512x512 \\\n",
        "# @markdown    > stabilityai/stable-diffusion-2-1 with a resolution of 768x768 \\\n",
        "\n",
        "style_list = [\n",
        "    {\n",
        "        \"name\": \"(No style)\",\n",
        "        \"prompt\": \"{prompt}\",\n",
        "        \"negative_prompt\": \"\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Cinematic\",\n",
        "        \"prompt\": \"cinematic still {prompt} . emotional, harmonious, vignette, highly detailed, high budget, bokeh, cinemascope, moody, epic, gorgeous, film grain, grainy\",\n",
        "        \"negative_prompt\": \"anime, cartoon, graphic, text, painting, crayon, graphite, abstract, glitch, deformed, mutated, ugly, disfigured\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Photographic\",\n",
        "        \"prompt\": \"cinematic photo {prompt} . 35mm photograph, film, bokeh, professional, 4k, highly detailed\",\n",
        "        \"negative_prompt\": \"drawing, painting, crayon, sketch, graphite, impressionist, noisy, blurry, soft, deformed, ugly\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Anime\",\n",
        "        \"prompt\": \"anime artwork {prompt} . anime style, key visual, vibrant, studio anime,  highly detailed\",\n",
        "        \"negative_prompt\": \"photo, deformed, black and white, realism, disfigured, low contrast\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Manga\",\n",
        "        \"prompt\": \"manga style {prompt} . vibrant, high-energy, detailed, iconic, Japanese comic style\",\n",
        "        \"negative_prompt\": \"ugly, deformed, noisy, blurry, low contrast, realism, photorealistic, Western comic style\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Digital Art\",\n",
        "        \"prompt\": \"concept art {prompt} . digital artwork, illustrative, painterly, matte painting, highly detailed\",\n",
        "        \"negative_prompt\": \"photo, photorealistic, realism, ugly\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Pixel art\",\n",
        "        \"prompt\": \"pixel-art {prompt} . low-res, blocky, pixel art style, 8-bit graphics\",\n",
        "        \"negative_prompt\": \"sloppy, messy, blurry, noisy, highly detailed, ultra textured, photo, realistic\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Fantasy art\",\n",
        "        \"prompt\": \"ethereal fantasy concept art of  {prompt} . magnificent, celestial, ethereal, painterly, epic, majestic, magical, fantasy art, cover art, dreamy\",\n",
        "        \"negative_prompt\": \"photographic, realistic, realism, 35mm film, dslr, cropped, frame, text, deformed, glitch, noise, noisy, off-center, deformed, cross-eyed, closed eyes, bad anatomy, ugly, disfigured, sloppy, duplicate, mutated, black and white\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Neonpunk\",\n",
        "        \"prompt\": \"neonpunk style {prompt} . cyberpunk, vaporwave, neon, vibes, vibrant, stunningly beautiful, crisp, detailed, sleek, ultramodern, magenta highlights, dark purple shadows, high contrast, cinematic, ultra detailed, intricate, professional\",\n",
        "        \"negative_prompt\": \"painting, drawing, illustration, glitch, deformed, mutated, cross-eyed, ugly, disfigured\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"3D Model\",\n",
        "        \"prompt\": \"professional 3d model {prompt} . octane render, highly detailed, volumetric, dramatic lighting\",\n",
        "        \"negative_prompt\": \"ugly, deformed, noisy, low poly, blurry, painting\",\n",
        "    },\n",
        "]\n",
        "\n",
        "styles = {k[\"name\"]: (k[\"prompt\"], k[\"negative_prompt\"]) for k in style_list}\n",
        "STYLE_NAMES = list(styles.keys())\n",
        "DEFAULT_STYLE_NAME = \"(No style)\"\n",
        "\n",
        "\n",
        "def get_bucket_and_blob_name(filepath):\n",
        "    # The gcs path is of the form gs://\n",
        "    gs_suffix = filepath.split(\"gs://\", 1)[1]\n",
        "    return tuple(gs_suffix.split(\"/\", 1))\n",
        "\n",
        "\n",
        "def canny(image, low_threshold=100, high_threshold=200) -> Image.Image:\n",
        "    image = np.array(image)\n",
        "    image = cv2.Canny(image, low_threshold, high_threshold)\n",
        "\n",
        "    image = image[:, :, None]\n",
        "    image = np.concatenate([image, image, image], axis=2)\n",
        "    image = Image.fromarray(image)\n",
        "    return image\n",
        "\n",
        "\n",
        "def upload_local_dir_to_gcs(local_dir_path, gcs_dir_path):\n",
        "    \"\"\"Uploads files in a local directory to a GCS directory.\"\"\"\n",
        "    client = storage.Client()\n",
        "    bucket_name = gcs_dir_path.split(\"/\")[2]\n",
        "    bucket = client.get_bucket(bucket_name)\n",
        "    for local_file in glob.glob(local_dir_path + \"/**\"):\n",
        "        if not os.path.isfile(local_file):\n",
        "            continue\n",
        "        filename = local_file[1 + len(local_dir_path) :]\n",
        "        gcs_file_path = os.path.join(gcs_dir_path, filename)\n",
        "        _, blob_name = get_bucket_and_blob_name(gcs_file_path)\n",
        "        blob = bucket.blob(blob_name)\n",
        "        blob.upload_from_filename(local_file)\n",
        "        print(\"Copied {} to {}.\".format(local_file, gcs_file_path))\n",
        "\n",
        "\n",
        "def is_image_generation_endpoint(endpoint: aiplatform.Endpoint) -> bool:\n",
        "    \"\"\"Returns True if the endpoint is an image generation endpoint.\"\"\"\n",
        "    return (\n",
        "        \"sd\" in endpoint.display_name.lower()\n",
        "        or \"diffusion\" in endpoint.display_name.lower()\n",
        "        or \"inpaint\" in endpoint.display_name.lower()\n",
        "        or \"controlnet\" in endpoint.display_name.lower()\n",
        "        or \"pix2pix\" in endpoint.display_name.lower()\n",
        "        or \"upscaler\" in endpoint.display_name.lower()\n",
        "        or \"canny\" in endpoint.display_name.lower()\n",
        "        or \"gradio\" in endpoint.display_name.lower()\n",
        "    )\n",
        "\n",
        "\n",
        "def list_endpoints() -> list[str]:\n",
        "    \"\"\"Returns all valid prediction endpoints for in the project and region.\"\"\"\n",
        "    # Gets all the valid endpoints in the project and region.\n",
        "    endpoints = aiplatform.Endpoint.list(order_by=\"create_time desc\")\n",
        "    # Filters out the endpoints which do not have a deployed model, and the endpoint is for image generation\n",
        "    endpoints = list(\n",
        "        filter(\n",
        "            lambda endpoint: endpoint.traffic_split\n",
        "            and is_image_generation_endpoint(endpoint),\n",
        "            endpoints,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    endpoint_names = list(\n",
        "        map(\n",
        "            lambda endpoint: f\"{endpoint.name} - {endpoint.display_name[:40]}\",\n",
        "            endpoints,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return endpoint_names\n",
        "\n",
        "\n",
        "def get_endpoint(endpoint_name: str) -> aiplatform.Endpoint:\n",
        "    \"\"\"Returns a Vertex endpoint for the given endpoint_name.\"\"\"\n",
        "\n",
        "    endpoint_id = endpoint_name.split(\" - \")[0]\n",
        "    endpoint = aiplatform.Endpoint(\n",
        "        f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_id}\"\n",
        "    )\n",
        "\n",
        "    return endpoint\n",
        "\n",
        "\n",
        "def get_publisher_model_id(model_name: str) -> str | None:\n",
        "    \"\"\"Returns the corresponding task name for the given model_name.\"\"\"\n",
        "\n",
        "    model_to_publisher_dict = {\n",
        "        \"stabilityai/stable-diffusion-2-1\": (\"stability-ai\", \"stable-diffusion-2-1\"),\n",
        "        \"stabilityai/stable-diffusion-xl-base-1.0\": (\n",
        "            \"stability-ai\",\n",
        "            \"stable-diffusion-xl-base\",\n",
        "        ),\n",
        "        \"stabilityai/stable-diffusion-xl-base-1.0 - refiner\": (\n",
        "            \"stability-ai\",\n",
        "            \"stable-diffusion-xl-base\",\n",
        "        ),\n",
        "        \"latent-consistency/lcm-sdxl\": (\"stability-ai\", \"stable-diffusion-xl-lcm\"),\n",
        "        \"latent-consistency/lcm-lora-sdxl\": (\"stability-ai\", \"stable-diffusion-xl-lcm\"),\n",
        "        \"stabilityai/sdxl-turbo\": None,\n",
        "        \"bytedance/sdxl-lightning\": (\"bytedance\", \"stable-diffusion-xl-lightning\"),\n",
        "        \"runDiffusion/Juggernaut-XL-Lightning\": None,\n",
        "        \"stablediffusionapi/juggernaut-xl-v9\": None,\n",
        "        \"lykon/dreamshaper-xl-v2-turbo\": None,\n",
        "        \"stablediffusionapi/anything-xl\": None,\n",
        "        \"stablediffusionapi/turbovision_xl\": None,\n",
        "        \"kandinsky-community/kandinsky-2-2-decoder-inpaint\": None,\n",
        "        \"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\": (\n",
        "            \"runwayml\",\n",
        "            \"stable-diffusion-inpainting\",\n",
        "        ),\n",
        "        \"timbrooks/instruct-pix2pix\": (\"timbrooks\", \"instruct-pix2pix\"),\n",
        "        \"lllyasviel/sd-controlnet-canny\": (\"lllyasviel\", \"control-net\"),\n",
        "        \"stabilityai/stable-diffusion-x4-upscaler\": (\n",
        "            \"stability-ai\",\n",
        "            \"stable-diffusion-4x-upscaler\",\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    if model_name not in model_to_publisher_dict.keys():\n",
        "        print(model_name)\n",
        "        raise gr.Error(\"Select a valid model name for Endpoint creation.\")\n",
        "\n",
        "    if model_to_publisher_dict[model_name]:\n",
        "        publisher = model_to_publisher_dict[model_name][0]\n",
        "        publisher_model_id = model_to_publisher_dict[model_name][1]\n",
        "    else:\n",
        "        publisher = \"hf-\" + model_name.split(\"/\")[0]\n",
        "        publisher_model_id = model_name.split(\"/\")[1]\n",
        "    return f\"publishers/{publisher}/models/{publisher_model_id}\"\n",
        "\n",
        "\n",
        "def get_task_name(model_name: str) -> str:\n",
        "    \"\"\"Returns the corresponding task name for the given model_name.\"\"\"\n",
        "\n",
        "    model_to_task_dict = {\n",
        "        \"stabilityai/stable-diffusion-2-1\": \"text-to-image\",\n",
        "        \"stabilityai/stable-diffusion-xl-base-1.0\": \"text-to-image-sdxl\",\n",
        "        \"stabilityai/stable-diffusion-xl-base-1.0 - refiner\": \"text-to-image-refiner\",\n",
        "        \"latent-consistency/lcm-sdxl\": \"text-to-image-sdxl-lcm\",\n",
        "        \"latent-consistency/lcm-lora-sdxl\": \"text-to-image-sdxl-lcm-lora\",\n",
        "        \"stabilityai/sdxl-turbo\": \"text-to-image-sdxl-turbo\",\n",
        "        \"bytedance/sdxl-lightning\": \"text-to-image-sdxl-lightning\",\n",
        "        \"runDiffusion/Juggernaut-XL-Lightning\": \"text-to-image-custom\",\n",
        "        \"stablediffusionapi/juggernaut-xl-v9\": \"text-to-image-custom\",\n",
        "        \"lykon/dreamshaper-xl-v2-turbo\": \"text-to-image-custom\",\n",
        "        \"stablediffusionapi/anything-xl\": \"text-to-image-custom\",\n",
        "        \"stablediffusionapi/turbovision_xl\": \"text-to-image-custom\",\n",
        "        \"kandinsky-community/kandinsky-2-2-decoder-inpaint\": \"image-inpainting\",\n",
        "        \"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\": \"image-inpainting\",\n",
        "        \"timbrooks/instruct-pix2pix\": \"instruct-pix2pix\",\n",
        "        \"lllyasviel/sd-controlnet-canny\": \"controlnet\",\n",
        "        \"stabilityai/stable-diffusion-x4-upscaler\": \"conditioned-super-res\",\n",
        "    }\n",
        "\n",
        "    if model_name not in model_to_task_dict.keys():\n",
        "        print(model_name)\n",
        "        raise gr.Error(\"Select a valid model name for Endpoint creation.\")\n",
        "\n",
        "    return model_to_task_dict[model_name]\n",
        "\n",
        "\n",
        "def deploy_model(model_name: str) -> aiplatform.Endpoint:\n",
        "    \"\"\"Preprocess the model_name, before deploying the model to vertex.\"\"\"\n",
        "    if not model_name:\n",
        "        raise gr.Error(\"Select a valid model name for model list.\")\n",
        "\n",
        "    model_id = model_name.split(\": \")[1]\n",
        "    publisher_model_id = get_publisher_model_id(model_id)\n",
        "    task_name = get_task_name(model_id)\n",
        "\n",
        "    return deploy_model_vertex(\n",
        "        model_id, publisher_model_id, task_name, lora_id, use_dedicated_endpoint\n",
        "    )\n",
        "\n",
        "\n",
        "def deploy_model_vertex(\n",
        "    model_id: str,\n",
        "    publisher_model_id: str | None,\n",
        "    task_name: str,\n",
        "    lora_id: str = \"\",\n",
        "    use_dedicated_endpoint: bool = False,\n",
        ") -> aiplatform.Endpoint:\n",
        "    \"\"\"\n",
        "    Creates a new Vertex prediction endpoint and deploys a model to it.\n",
        "    The `model_id` can either be a HF model resource id, or a GCS path where\n",
        "    the model checkpoint is saved (usually after finetune).\n",
        "    \"\"\"\n",
        "    refiner_model_id = \"stabilityai/stable-diffusion-xl-refiner-1.0\"\n",
        "\n",
        "    gr.Info(\"Model deployment started. Let's wait...\")\n",
        "\n",
        "    display_name = common_util.create_job_name(model_id)\n",
        "    deploy_model_id = model_id\n",
        "    if (\n",
        "        model_id == \"stabilityai/stable-diffusion-xl-base-1.0 - refiner\"\n",
        "        or model_id == \"latent-consistency/lcm-sdxl\"\n",
        "        or model_id == \"latent-consistency/lcm-lora-sdxl\"\n",
        "    ):\n",
        "        deploy_model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "\n",
        "    endpoint = aiplatform.Endpoint.create(\n",
        "        display_name=display_name,\n",
        "        dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
        "    )\n",
        "    serving_env = {\n",
        "        \"MODEL_ID\": deploy_model_id,\n",
        "        \"TASK\": task_name,\n",
        "        \"DEPLOY_SOURCE\": \"notebook_gradio\",\n",
        "    }\n",
        "    if model_id == \"stabilityai/stable-diffusion-xl-base-1.0 - refiner\":\n",
        "        serving_env = {\n",
        "            **serving_env,\n",
        "            \"REFINER_MODEL_ID\": refiner_model_id,\n",
        "        }\n",
        "    if lora_id:\n",
        "        serving_env = {\n",
        "            **serving_env,\n",
        "            \"LORA_ID\": lora_id,\n",
        "        }\n",
        "\n",
        "    if \"text-to-image\" in task_name:\n",
        "        model = upload_model_pytorch_inference_container(\n",
        "            model_id, publisher_model_id, serving_env\n",
        "        )\n",
        "    else:\n",
        "        model = upload_model_pytorch_diffusers_serve_container(\n",
        "            model_id, publisher_model_id, serving_env\n",
        "        )\n",
        "\n",
        "    machine_type = \"g2-standard-8\"\n",
        "    accelerator_type = \"NVIDIA_L4\"\n",
        "    accelerator_count = 1\n",
        "\n",
        "    common_util.check_quota(\n",
        "        project_id=PROJECT_ID,\n",
        "        region=REGION,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        is_for_training=False,\n",
        "    )\n",
        "\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        deploy_request_timeout=1800,\n",
        "        sync=False,\n",
        "        system_labels={\n",
        "            \"NOTEBOOK_NAME\": \"model_garden_pytorch_stable_diffusion_gradio.ipynb\"\n",
        "        },\n",
        "    )\n",
        "\n",
        "    gr.Info(\n",
        "        f\"Model {display_name} is being deployed. It may take ~20 minutes to complete.\"\n",
        "    )\n",
        "\n",
        "    return endpoint\n",
        "\n",
        "\n",
        "# The pre-built vertex-model-garden/pytorch-inference docker.\n",
        "PYTORCH_INFERENCE_DOCKER_URI = \"us-docker.pkg.dev/deeplearning-platform-release/vertex-model-garden/pytorch-inference.cu125.0-4.ubuntu2204.py310\"\n",
        "\n",
        "\n",
        "def upload_model_pytorch_inference_container(\n",
        "    model_id: str, publisher_model_id: str | None, serving_env: dict\n",
        ") -> aiplatform.Model:\n",
        "    return aiplatform.Model.upload(\n",
        "        display_name=model_id,\n",
        "        serving_container_image_uri=PYTORCH_INFERENCE_DOCKER_URI,\n",
        "        serving_container_ports=[7080],\n",
        "        serving_container_predict_route=\"/predict\",\n",
        "        serving_container_health_route=\"/health\",\n",
        "        serving_container_environment_variables=serving_env,\n",
        "        model_garden_source_model_name=publisher_model_id,\n",
        "    )\n",
        "\n",
        "\n",
        "# The pre-built serving docker image. They contains serving scripts and models.\n",
        "SERVE_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-diffusers-serve-opt:20240605_1400_RC00\"\n",
        "\n",
        "\n",
        "def upload_model_pytorch_diffusers_serve_container(\n",
        "    model_id: str, publisher_model_id: str | None, serving_env: dict\n",
        ") -> aiplatform.Model:\n",
        "    return aiplatform.Model.upload(\n",
        "        display_name=model_id,\n",
        "        serving_container_image_uri=SERVE_DOCKER_URI,\n",
        "        serving_container_ports=[7080],\n",
        "        serving_container_predict_route=\"/predictions/diffusers_serving\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "        serving_container_environment_variables=serving_env,\n",
        "        model_garden_source_model_name=publisher_model_id,\n",
        "    )\n",
        "\n",
        "\n",
        "def get_default_dimension(model_name: str) -> int:\n",
        "    \"\"\"Returns the default dimension for the given model_name.\"\"\"\n",
        "\n",
        "    dimension = 512\n",
        "    if not model_name:\n",
        "        return dimension\n",
        "\n",
        "    model_name = model_name.lower()\n",
        "    if (\n",
        "        \"stable-diffusion-xl\" in model_name\n",
        "        or \"xl\" in model_name\n",
        "        or \"lightning\" in model_name\n",
        "    ):\n",
        "        dimension = 1024\n",
        "    if \"stable-diffusion-2-1\" in model_name:\n",
        "        dimension = 768\n",
        "    if \"sdxl-turbo\" in model_name:\n",
        "        dimension = 512\n",
        "\n",
        "    return dimension\n",
        "\n",
        "\n",
        "def get_default_guidance_scale(model_name: str) -> int:\n",
        "    \"\"\"Returns the default guidance scale for the given model_name.\"\"\"\n",
        "\n",
        "    guidance_scale = 7.5\n",
        "    if not model_name:\n",
        "        return guidance_scale\n",
        "\n",
        "    model_name = model_name.lower()\n",
        "    if \"xl-base\" in model_name or \"refiner\" in model_name:\n",
        "        guidance_scale = 7.5\n",
        "\n",
        "    if \"lcm\" in model_name or \"sdxl-turbo\" in model_name:\n",
        "        guidance_scale = 0\n",
        "\n",
        "    if \"sdxl-lightning\" in model_name:\n",
        "        guidance_scale = 1\n",
        "\n",
        "    if \"dreamshaper-xl-v2-turbo\" in model_name:\n",
        "        guidance_scale = 2\n",
        "\n",
        "    if \"lightning\" in model_name or \"turbovision_xl\" in model_name:\n",
        "        guidance_scale = 1.5\n",
        "\n",
        "    return guidance_scale\n",
        "\n",
        "\n",
        "def get_default_num_inference_steps(model_name: str) -> int:\n",
        "    \"\"\"Returns the default num_inference_steps for the given model_name.\"\"\"\n",
        "\n",
        "    num_inference_steps = 25\n",
        "    if not model_name:\n",
        "        return num_inference_steps\n",
        "\n",
        "    model_name = model_name.lower()\n",
        "    if \"lcm\" in model_name:\n",
        "        num_inference_steps = 8\n",
        "    elif \"sdxl-turbo\" in model_name:\n",
        "        num_inference_steps = 2\n",
        "    elif \"juggernaut-xl-lightning\" in model_name:\n",
        "        num_inference_steps = 5\n",
        "    elif \"dreamshaper-xl-v2-turbo\" in model_name:\n",
        "        num_inference_steps = 8\n",
        "    elif \"juggernaut-xl-v9\" in model_name:\n",
        "        num_inference_steps = 25\n",
        "    elif \"turbovision_xl\" in model_name:\n",
        "        num_inference_steps = 5\n",
        "    elif \"sdxl-lightning\" in model_name:\n",
        "        num_inference_steps = 4\n",
        "\n",
        "    return num_inference_steps\n",
        "\n",
        "\n",
        "def apply_style(style_name: str, positive: str, negative: str = \"\") -> tuple[str, str]:\n",
        "    p, n = styles.get(style_name, styles[DEFAULT_STYLE_NAME])\n",
        "    return p.replace(\"{prompt}\", positive), n + negative\n",
        "\n",
        "\n",
        "def generate_images(\n",
        "    endpoint_name,\n",
        "    style_name=None,\n",
        "    prompt=\"\",\n",
        "    negative_prompt=\"\",\n",
        "    num_samples=1,\n",
        "    guidance_scale=7.5,\n",
        "    num_inference_steps=25,\n",
        "    image_dimension=512,\n",
        ") -> list[Image.Image]:\n",
        "    if not endpoint_name:\n",
        "        raise gr.Error(\"Select (or deploy) a model first!\")\n",
        "\n",
        "    prompt, negative_prompt = apply_style(style_name, prompt, negative_prompt)\n",
        "\n",
        "    instances = [{\"text\": prompt}]\n",
        "    parameters = {\n",
        "        \"negative_prompt\": negative_prompt,\n",
        "        \"height\": image_dimension,\n",
        "        \"width\": image_dimension,\n",
        "        \"guidance_scale\": guidance_scale,\n",
        "        \"num_inference_steps\": num_inference_steps,\n",
        "    }\n",
        "\n",
        "    if len(instances) == 1 and num_samples > 1:\n",
        "        instances = instances * num_samples\n",
        "\n",
        "    response = get_endpoint(endpoint_name).predict(\n",
        "        instances=instances, parameters=parameters\n",
        "    )\n",
        "    images = [\n",
        "        common_util.base64_to_image(prediction[\"output\"])\n",
        "        for prediction in response.predictions\n",
        "    ]\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "def inpaint_generate_images(\n",
        "    endpoint_name: str,\n",
        "    style_name=None,\n",
        "    prompt=\"\",\n",
        "    negative_prompt=\"\",\n",
        "    num_samples=1,\n",
        "    guidance_scale=7.5,\n",
        "    num_inference_steps=25,\n",
        "    dict=None,\n",
        "    image_dimension=512,\n",
        ") -> list[Image.Image]:\n",
        "    if not endpoint_name:\n",
        "        raise gr.Error(\"Select (or deploy) a model first!\")\n",
        "\n",
        "    prompt, negative_prompt = apply_style(style_name, prompt, negative_prompt)\n",
        "    default_dimension = 512\n",
        "    # Set the default_dimension=1024 if the model is `stable-diffusion-xl-1.0-inpainting-0.1`.\n",
        "    if \"stable-diffusion-xl\" in endpoint_name:\n",
        "        default_dimension = 1024\n",
        "\n",
        "    init_image = (\n",
        "        dict[\"background\"]\n",
        "        .convert(mode=\"RGB\")\n",
        "        .resize((default_dimension, default_dimension))\n",
        "    )\n",
        "    # Replace the transparent pixels with white. Required in rgba -> rgb conversion.\n",
        "    mask_rgba = np.array(dict[\"layers\"][0])\n",
        "    mask_rgba[mask_rgba[..., -1] == 0] = [255, 255, 255, 0]\n",
        "    mask = Image.fromarray(mask_rgba).convert(mode=\"L\")\n",
        "    mask = ImageOps.invert(mask).resize((default_dimension, default_dimension))\n",
        "\n",
        "    instances = [\n",
        "        {\n",
        "            \"prompt\": prompt,\n",
        "            \"negative_prompt\": negative_prompt,\n",
        "            \"image\": common_util.image_to_base64(init_image),\n",
        "            \"mask_image\": common_util.image_to_base64(mask),\n",
        "            \"height\": image_dimension,\n",
        "            \"width\": image_dimension,\n",
        "            \"guidance_scale\": guidance_scale,\n",
        "            \"num_inference_steps\": num_inference_steps,\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    if len(instances) == 1 and num_samples > 1:\n",
        "        instances = instances * num_samples\n",
        "\n",
        "    response = get_endpoint(endpoint_name).predict(instances=instances)\n",
        "    images = [common_util.base64_to_image(image) for image in response.predictions]\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "def instruct_pix2pix_generate_images(\n",
        "    endpoint_name: str,\n",
        "    style_name=None,\n",
        "    prompt=\"\",\n",
        "    negative_prompt=\"\",\n",
        "    num_samples=1,\n",
        "    guidance_scale=7.5,\n",
        "    num_inference_steps=25,\n",
        "    init_image=None,\n",
        "    image_dimension=512,\n",
        ") -> list[Image.Image]:\n",
        "    if not endpoint_name:\n",
        "        raise gr.Error(\"Select (or deploy) a model first!\")\n",
        "\n",
        "    prompt, negative_prompt = apply_style(style_name, prompt, negative_prompt)\n",
        "    instances = [\n",
        "        {\n",
        "            \"prompt\": prompt,\n",
        "            \"negative_prompt\": negative_prompt,\n",
        "            \"image\": common_util.image_to_base64(init_image),\n",
        "            \"height\": image_dimension,\n",
        "            \"width\": image_dimension,\n",
        "            \"guidance_scale\": guidance_scale,\n",
        "            \"num_inference_steps\": num_inference_steps,\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    if len(instances) == 1 and num_samples > 1:\n",
        "        instances = instances * num_samples\n",
        "\n",
        "    response = get_endpoint(endpoint_name).predict(instances=instances)\n",
        "    images = [common_util.base64_to_image(image) for image in response.predictions]\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "def upscaler_generate_images(\n",
        "    endpoint_name: str,\n",
        "    style_name=None,\n",
        "    prompt=\"\",\n",
        "    negative_prompt=\"\",\n",
        "    num_samples=1,\n",
        "    guidance_scale=7.5,\n",
        "    num_inference_steps=25,\n",
        "    init_image=None,\n",
        "    image_dimension=512,\n",
        ") -> list[Image.Image]:\n",
        "\n",
        "    if not endpoint_name:\n",
        "        raise gr.Error(\"Select (or deploy) a model first!\")\n",
        "\n",
        "    prompt, negative_prompt = apply_style(style_name, prompt, negative_prompt)\n",
        "    default_dimension = 256\n",
        "    init_image = init_image.convert(\"RGB\").resize(\n",
        "        (default_dimension, default_dimension)\n",
        "    )\n",
        "    instances = [\n",
        "        {\n",
        "            \"prompt\": prompt,\n",
        "            \"negative_prompt\": negative_prompt,\n",
        "            \"image\": common_util.image_to_base64(init_image),\n",
        "            \"height\": image_dimension,\n",
        "            \"width\": image_dimension,\n",
        "            \"guidance_scale\": guidance_scale,\n",
        "            \"num_inference_steps\": num_inference_steps,\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    if len(instances) == 1 and num_samples > 1:\n",
        "        instances = instances * num_samples\n",
        "\n",
        "    response = get_endpoint(endpoint_name).predict(instances=instances)\n",
        "    images = [common_util.base64_to_image(image) for image in response.predictions]\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "def controlnet_generate_images(\n",
        "    endpoint_name: str,\n",
        "    style_name=None,\n",
        "    prompt=\"\",\n",
        "    negative_prompt=\"\",\n",
        "    num_samples=1,\n",
        "    guidance_scale=7.5,\n",
        "    num_inference_steps=25,\n",
        "    init_image=None,\n",
        "    image_dimension=512,\n",
        "    canny_low_threshold=100,\n",
        "    canny_high_threshold=200,\n",
        ") -> list[Image.Image]:\n",
        "    if not endpoint_name:\n",
        "        raise gr.Error(\"Select (or deploy) a model first!\")\n",
        "\n",
        "    prompt, negative_prompt = apply_style(style_name, prompt, negative_prompt)\n",
        "    init_image = init_image.convert(\"RGB\").resize((image_dimension, image_dimension))\n",
        "    canny_image = canny(init_image, canny_low_threshold, canny_high_threshold)\n",
        "\n",
        "    instances = [\n",
        "        {\n",
        "            \"prompt\": prompt,\n",
        "            \"negative_prompt\": negative_prompt,\n",
        "            \"image\": common_util.image_to_base64(canny_image),\n",
        "            \"height\": image_dimension,\n",
        "            \"width\": image_dimension,\n",
        "            \"guidance_scale\": guidance_scale,\n",
        "            \"num_inference_steps\": num_inference_steps,\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    if len(instances) == 1 and num_samples > 1:\n",
        "        instances = instances * num_samples\n",
        "\n",
        "    response = get_endpoint(endpoint_name).predict(instances=instances)\n",
        "    images = [common_util.base64_to_image(image) for image in response.predictions]\n",
        "\n",
        "    images.insert(0, canny_image)\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "###############################\n",
        "#   Image Generation          #\n",
        "###############################\n",
        "def create_generation_workshop():\n",
        "    tip_text = r\"\"\"\n",
        "        1. Select a Vertex prediction endpoint with a model deployed for your chosen task. Mismatched models can lead to unreliable outcomes.\n",
        "        2. New model deployment takes ~20 minutes. You can check the progress at [Vertex Online Prediction](https://console.cloud.google.com/vertex-ai/online-prediction/endpoints).\n",
        "        3. After the model deployment is complete, restart the playground in Colab to see the updated endpoint list.\n",
        "        \"\"\"\n",
        "\n",
        "    def select_interface(interface_name: str) -> gr.Blocks:\n",
        "        if interface_name == \"Text2Image\":\n",
        "            return {\n",
        "                image_input: gr.update(visible=False, value=None, label=\"Upload\"),\n",
        "                inpainting_input: gr.update(visible=False),\n",
        "                image_output: gr.update(value=None),\n",
        "                generate_button: gr.update(visible=True),\n",
        "                inpaint_generate_button: gr.update(visible=False),\n",
        "                instruct_pix2pix_generate_button: gr.update(visible=False),\n",
        "                controlnet_generate_button: gr.update(visible=False),\n",
        "                upscaler_generate_button: gr.update(visible=False),\n",
        "                canny_low_threshold: gr.update(visible=False),\n",
        "                canny_high_threshold: gr.update(visible=False),\n",
        "            }\n",
        "\n",
        "        elif interface_name == \"Inpainting\":\n",
        "            return {\n",
        "                image_input: gr.update(visible=False),\n",
        "                inpainting_input: gr.update(visible=True, value=None, label=\"Upload\"),\n",
        "                image_output: gr.update(value=None),\n",
        "                generate_button: gr.update(visible=False),\n",
        "                inpaint_generate_button: gr.update(visible=True),\n",
        "                instruct_pix2pix_generate_button: gr.update(visible=False),\n",
        "                controlnet_generate_button: gr.update(visible=False),\n",
        "                upscaler_generate_button: gr.update(visible=False),\n",
        "                canny_low_threshold: gr.update(visible=False),\n",
        "                canny_high_threshold: gr.update(visible=False),\n",
        "            }\n",
        "\n",
        "        elif interface_name == \"Instruct pix2pix\":\n",
        "            return {\n",
        "                image_input: gr.update(visible=True, value=None, label=\"Upload\"),\n",
        "                inpainting_input: gr.update(visible=False),\n",
        "                image_output: gr.update(value=None),\n",
        "                generate_button: gr.update(visible=False),\n",
        "                inpaint_generate_button: gr.update(visible=False),\n",
        "                instruct_pix2pix_generate_button: gr.update(visible=True),\n",
        "                controlnet_generate_button: gr.update(visible=False),\n",
        "                upscaler_generate_button: gr.update(visible=False),\n",
        "                canny_low_threshold: gr.update(visible=False),\n",
        "                canny_high_threshold: gr.update(visible=False),\n",
        "            }\n",
        "\n",
        "        elif interface_name == \"ControlNet Canny\":\n",
        "            return {\n",
        "                image_input: gr.update(\n",
        "                    visible=True,\n",
        "                    value=None,\n",
        "                    label=\"Upload a reference image\",\n",
        "                ),\n",
        "                inpainting_input: gr.update(visible=False),\n",
        "                image_output: gr.update(value=None),\n",
        "                generate_button: gr.update(visible=False),\n",
        "                inpaint_generate_button: gr.update(visible=False),\n",
        "                instruct_pix2pix_generate_button: gr.update(visible=False),\n",
        "                controlnet_generate_button: gr.update(visible=True),\n",
        "                upscaler_generate_button: gr.update(visible=False),\n",
        "                canny_low_threshold: gr.update(visible=True),\n",
        "                canny_high_threshold: gr.update(visible=True),\n",
        "            }\n",
        "\n",
        "        elif interface_name == \"SD 4x Upscaler\":\n",
        "            return {\n",
        "                image_input: gr.update(visible=True, value=None, label=\"Upload\"),\n",
        "                inpainting_input: gr.update(visible=False),\n",
        "                image_output: gr.update(value=None),\n",
        "                generate_button: gr.update(visible=False),\n",
        "                inpaint_generate_button: gr.update(visible=False),\n",
        "                instruct_pix2pix_generate_button: gr.update(visible=False),\n",
        "                controlnet_generate_button: gr.update(visible=False),\n",
        "                upscaler_generate_button: gr.update(visible=True),\n",
        "                canny_low_threshold: gr.update(visible=False),\n",
        "                canny_high_threshold: gr.update(visible=False),\n",
        "            }\n",
        "\n",
        "    def update_default_parameters(model_name: str):\n",
        "        \"\"\"Updates the default inference parameters based on the selected model.\"\"\"\n",
        "        return {\n",
        "            guidance_scale: gr.update(value=get_default_guidance_scale(model_name)),\n",
        "            num_inference_steps: gr.update(\n",
        "                value=get_default_num_inference_steps(model_name)\n",
        "            ),\n",
        "            image_dimension: gr.update(value=get_default_dimension(model_name)),\n",
        "        }\n",
        "\n",
        "    with gr.Blocks() as demo:\n",
        "        interfaces_box = gr.Radio(\n",
        "            show_label=False,\n",
        "            choices=[\n",
        "                \"Text2Image\",\n",
        "                \"Inpainting\",\n",
        "                \"Instruct pix2pix\",\n",
        "                \"ControlNet Canny\",\n",
        "                \"SD 4x Upscaler\",\n",
        "            ],\n",
        "            value=\"Text2Image\",\n",
        "        )\n",
        "        with gr.Accordion(\"How To Use\", open=False):\n",
        "            gr.Markdown(tip_text)\n",
        "\n",
        "        with gr.Row(equal_height=True):\n",
        "            with gr.Column(scale=3):\n",
        "                prompt = gr.Textbox(label=\"Prompt\", lines=1)\n",
        "                negative_prompt = gr.Textbox(label=\"Negative Prompt\", lines=1)\n",
        "            with gr.Column(scale=1):\n",
        "                endpoint_name = gr.Dropdown(\n",
        "                    label=\"Select a model previously deployed on Vertex\",\n",
        "                    choices=list_endpoints(),\n",
        "                    value=None,\n",
        "                )\n",
        "                with gr.Row():\n",
        "                    selected_model = gr.Dropdown(\n",
        "                        scale=7,\n",
        "                        label=\"Deploy a new model to Vertex\",\n",
        "                        choices=[\n",
        "                            \"txt2img: stabilityai/stable-diffusion-2-1\",\n",
        "                            \"txt2img: stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "                            \"txt2img: stabilityai/stable-diffusion-xl-base-1.0 - refiner\",\n",
        "                            \"txt2img: latent-consistency/lcm-sdxl\",\n",
        "                            \"txt2img: latent-consistency/lcm-lora-sdxl\",\n",
        "                            \"txt2img: stabilityai/sdxl-turbo\",\n",
        "                            \"txt2img: bytedance/sdxl-lightning\",\n",
        "                            \"txt2img: runDiffusion/juggernaut-xl-lightning\",\n",
        "                            \"txt2img: stablediffusionapi/juggernaut-xl-v9\",\n",
        "                            \"txt2img: lykon/dreamshaper-xl-v2-turbo\",\n",
        "                            \"txt2img: stablediffusionapi/anything-xl\",\n",
        "                            \"txt2img: stablediffusionapi/turbovision_xl\",\n",
        "                            \"inpainting: kandinsky-community/kandinsky-2-2-decoder-inpaint\",\n",
        "                            \"inpainting: diffusers/stable-diffusion-xl-1.0-inpainting-0.1\",\n",
        "                            \"instruct-pix2pix: timbrooks/instruct-pix2pix\",\n",
        "                            \"controlnet: lllyasviel/sd-controlnet-canny\",\n",
        "                            \"upscaler: stabilityai/stable-diffusion-x4-upscaler\",\n",
        "                        ],\n",
        "                        value=None,\n",
        "                    )\n",
        "                    deploy_model_button = gr.Button(\n",
        "                        \"Deploy\", scale=1, variant=\"primary\", min_width=10\n",
        "                    )\n",
        "\n",
        "        with gr.Row(equal_height=True):\n",
        "            with gr.Column(scale=2):\n",
        "                generate_button = gr.Button(\"Generate\", variant=\"primary\")\n",
        "                inpaint_generate_button = gr.Button(\n",
        "                    \"Generate\", variant=\"primary\", visible=False\n",
        "                )\n",
        "                instruct_pix2pix_generate_button = gr.Button(\n",
        "                    \"Generate\", variant=\"primary\", visible=False\n",
        "                )\n",
        "                controlnet_generate_button = gr.Button(\n",
        "                    \"Generate\", variant=\"primary\", visible=False\n",
        "                )\n",
        "                upscaler_generate_button = gr.Button(\n",
        "                    \"Generate\", variant=\"primary\", visible=False\n",
        "                )\n",
        "\n",
        "                num_samples = gr.Slider(\n",
        "                    label=\"Number of samples\", value=1, step=1, minimum=1, maximum=4\n",
        "                )\n",
        "                image_dimension = gr.Slider(\n",
        "                    label=\"Image dimension\",\n",
        "                    value=768,\n",
        "                    step=256,\n",
        "                    minimum=512,\n",
        "                    maximum=1024,\n",
        "                )\n",
        "                num_inference_steps = gr.Slider(\n",
        "                    label=\"Sampling steps\", value=25, step=1, minimum=1, maximum=100\n",
        "                )\n",
        "                guidance_scale = gr.Slider(\n",
        "                    label=\"Guidance scale\", value=7.5, step=0.5, minimum=0, maximum=20.0\n",
        "                )\n",
        "                with gr.Accordion(\"Styles\", open=False):\n",
        "                    style_selection = gr.Radio(\n",
        "                        show_label=True,\n",
        "                        container=True,\n",
        "                        interactive=True,\n",
        "                        choices=STYLE_NAMES,\n",
        "                        value=DEFAULT_STYLE_NAME,\n",
        "                        label=\"Image Style\",\n",
        "                    )\n",
        "                    canny_low_threshold = gr.Slider(\n",
        "                        label=\"Canny low threshold\",\n",
        "                        value=100,\n",
        "                        step=5,\n",
        "                        minimum=1,\n",
        "                        maximum=255,\n",
        "                        visible=False,\n",
        "                    )\n",
        "                    canny_high_threshold = gr.Slider(\n",
        "                        label=\"Canny high threshold\",\n",
        "                        value=200,\n",
        "                        step=5,\n",
        "                        minimum=1,\n",
        "                        maximum=255,\n",
        "                        visible=False,\n",
        "                    )\n",
        "\n",
        "            with gr.Column(scale=5):\n",
        "                with gr.Row(equal_height=True):\n",
        "                    image_input = gr.Image(\n",
        "                        type=\"pil\",\n",
        "                        label=\"Upload\",\n",
        "                        sources=\"upload\",\n",
        "                        height=500,\n",
        "                        interactive=True,\n",
        "                        visible=False,\n",
        "                    )\n",
        "                    inpainting_input = gr.ImageMask(\n",
        "                        type=\"pil\",\n",
        "                        label=\"Upload\",\n",
        "                        sources=\"upload\",\n",
        "                        eraser=None,\n",
        "                        height=500,\n",
        "                        interactive=True,\n",
        "                        visible=False,\n",
        "                    )\n",
        "                    image_output = gr.Gallery(\n",
        "                        label=\"Generated Images\",\n",
        "                        rows=1,\n",
        "                        height=500,\n",
        "                        preview=True,\n",
        "                    )\n",
        "\n",
        "    interfaces_box.change(\n",
        "        select_interface,\n",
        "        interfaces_box,\n",
        "        [\n",
        "            image_input,\n",
        "            inpainting_input,\n",
        "            image_output,\n",
        "            generate_button,\n",
        "            inpaint_generate_button,\n",
        "            instruct_pix2pix_generate_button,\n",
        "            controlnet_generate_button,\n",
        "            upscaler_generate_button,\n",
        "            canny_low_threshold,\n",
        "            canny_high_threshold,\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    endpoint_name.change(\n",
        "        update_default_parameters,\n",
        "        endpoint_name,\n",
        "        [\n",
        "            guidance_scale,\n",
        "            num_inference_steps,\n",
        "            image_dimension,\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    deploy_model_button.click(\n",
        "        deploy_model,\n",
        "        inputs=[selected_model],\n",
        "        outputs=[],\n",
        "    )\n",
        "\n",
        "    generate_button.click(\n",
        "        generate_images,\n",
        "        inputs=[\n",
        "            endpoint_name,\n",
        "            style_selection,\n",
        "            prompt,\n",
        "            negative_prompt,\n",
        "            num_samples,\n",
        "            guidance_scale,\n",
        "            num_inference_steps,\n",
        "            image_dimension,\n",
        "        ],\n",
        "        outputs=image_output,\n",
        "    )\n",
        "\n",
        "    inpaint_generate_button.click(\n",
        "        inpaint_generate_images,\n",
        "        inputs=[\n",
        "            endpoint_name,\n",
        "            style_selection,\n",
        "            prompt,\n",
        "            negative_prompt,\n",
        "            num_samples,\n",
        "            guidance_scale,\n",
        "            num_inference_steps,\n",
        "            inpainting_input,\n",
        "            image_dimension,\n",
        "        ],\n",
        "        outputs=image_output,\n",
        "    )\n",
        "\n",
        "    instruct_pix2pix_generate_button.click(\n",
        "        instruct_pix2pix_generate_images,\n",
        "        inputs=[\n",
        "            endpoint_name,\n",
        "            style_selection,\n",
        "            prompt,\n",
        "            negative_prompt,\n",
        "            num_samples,\n",
        "            guidance_scale,\n",
        "            num_inference_steps,\n",
        "            image_input,\n",
        "            image_dimension,\n",
        "        ],\n",
        "        outputs=image_output,\n",
        "    )\n",
        "\n",
        "    controlnet_generate_button.click(\n",
        "        controlnet_generate_images,\n",
        "        inputs=[\n",
        "            endpoint_name,\n",
        "            style_selection,\n",
        "            prompt,\n",
        "            negative_prompt,\n",
        "            num_samples,\n",
        "            guidance_scale,\n",
        "            num_inference_steps,\n",
        "            image_input,\n",
        "            image_dimension,\n",
        "            canny_low_threshold,\n",
        "            canny_high_threshold,\n",
        "        ],\n",
        "        outputs=image_output,\n",
        "    )\n",
        "\n",
        "    upscaler_generate_button.click(\n",
        "        upscaler_generate_images,\n",
        "        inputs=[\n",
        "            endpoint_name,\n",
        "            style_selection,\n",
        "            prompt,\n",
        "            negative_prompt,\n",
        "            num_samples,\n",
        "            guidance_scale,\n",
        "            num_inference_steps,\n",
        "            image_input,\n",
        "            image_dimension,\n",
        "        ],\n",
        "        outputs=image_output,\n",
        "    )\n",
        "    return demo\n",
        "\n",
        "\n",
        "###############################\n",
        "#   Dreambooth finetuning     #\n",
        "###############################\n",
        "def pad_image(image):\n",
        "    w, h = image.size\n",
        "    if w == h:\n",
        "        return image\n",
        "    elif w > h:\n",
        "        new_image = Image.new(image.mode, (w, w), (0, 0, 0))\n",
        "        new_image.paste(image, (0, (w - h) // 2))\n",
        "        return new_image\n",
        "    else:\n",
        "        new_image = Image.new(image.mode, (h, h), (0, 0, 0))\n",
        "        new_image.paste(image, ((h - w) // 2, 0))\n",
        "        return new_image\n",
        "\n",
        "\n",
        "def get_dreambooth_base_model_and_resolution(base_model_select: str) -> tuple[str, int]:\n",
        "    if base_model_select == \"sd-v2-1-768\":\n",
        "        return \"stabilityai/stable-diffusion-2-1\", 768\n",
        "    elif base_model_select == \"sd-v2-1-512\":\n",
        "        return \"stabilityai/stable-diffusion-2-1\", 512\n",
        "    elif base_model_select == \"sdxl-dreambooth-lora-1024\":\n",
        "        return \"stabilityai/stable-diffusion-xl-base-1.0\", 1024\n",
        "    else:\n",
        "        raise gr.Error(f\"Invalid base model: {base_model_select}\")\n",
        "\n",
        "\n",
        "# The pre-built training docker image. They contain training scripts and models.\n",
        "TRAIN_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-train:20240318_0936_RC00\"\n",
        "\n",
        "\n",
        "def create_dreambooth_workshop() -> gr.Blocks:\n",
        "    def prepare_instance_images(\n",
        "        base_model_select: str,\n",
        "        concept_prompt: str,\n",
        "        file_collection: list[gr.File],\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Prepares the instance images for training from the provided dir of images.\n",
        "        \"\"\"\n",
        "        if not file_collection:\n",
        "            raise gr.Error(gr.Markdown(\"Provide a few valid instance images!\"))\n",
        "\n",
        "        if not concept_prompt:\n",
        "            raise gr.Error(\"Provide a unique concept prompt!\")\n",
        "\n",
        "        resolution = 768\n",
        "        if base_model_select == \"sd-v2-1-512\" or base_model_select == \"sd-v1-5-512\":\n",
        "            resolution = 512\n",
        "        elif base_model_select == \"sdxl-dreambooth-lora-1024\":\n",
        "            resolution = 1024\n",
        "\n",
        "        local_tmp_dir = \"/tmp/instance_images\"\n",
        "        if os.path.exists(local_tmp_dir):\n",
        "            shutil.rmtree(local_tmp_dir)\n",
        "        os.makedirs(local_tmp_dir, exist_ok=True)\n",
        "\n",
        "        image_counter = 0\n",
        "        for file_temp in file_collection:\n",
        "            image_counter += 1\n",
        "            file = Image.open(file_temp.name)\n",
        "            image = pad_image(file)\n",
        "            image = image.resize((resolution, resolution))\n",
        "            image = image.convert(\"RGB\")\n",
        "            image.save(\n",
        "                os.path.join(local_tmp_dir, f\"{concept_prompt}_{image_counter}.jpg\"),\n",
        "                format=\"JPEG\",\n",
        "                quality=100,\n",
        "            )\n",
        "\n",
        "        # Upload the processed images to the GCS dir.\n",
        "        instant_image_dir = os.path.join(BUCKET_URI, f\"instance_images-{now}\")\n",
        "        upload_local_dir_to_gcs(local_tmp_dir, instant_image_dir)\n",
        "\n",
        "    def deploy_finetuned_model(\n",
        "        base_model_select: str, output_dir: str\n",
        "    ) -> aiplatform.Endpoint:\n",
        "        if base_model_select == \"sdxl-dreambooth-lora-1024\":\n",
        "            return deploy_model_vertex(\n",
        "                model_id=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "                lora_id=output_dir,\n",
        "                task_name=\"text-to-image-sdxl\",\n",
        "                use_dedicated_endpoint=True,\n",
        "            )\n",
        "\n",
        "        return deploy_model_vertex(\n",
        "            model_id=output_dir,\n",
        "            task_name=\"text-to-image\",\n",
        "            use_dedicated_endpoint=use_dedicated_endpoint,\n",
        "        )\n",
        "\n",
        "    def dreambooth_start_training(*inputs):\n",
        "        [\n",
        "            output_dir,\n",
        "            base_model_select,\n",
        "            concept_prompt,\n",
        "            learning_rate,\n",
        "            train_steps,\n",
        "            deploy_after_tuning,\n",
        "            train_text_encoder,\n",
        "            checkpointing_steps,\n",
        "            train_batch_size,\n",
        "            gradient_accumulation_steps,\n",
        "            lr_scheduler,\n",
        "            lr_warmup_steps,\n",
        "            seed,\n",
        "        ] = inputs[:13]\n",
        "\n",
        "        if not concept_prompt:\n",
        "            raise gr.Error(\"Enter a concept prompt first!\")\n",
        "\n",
        "        if not inputs or len(inputs) < 13:\n",
        "            raise gr.Error(\"Provide a local folder with (5-10) instance image!\")\n",
        "\n",
        "        if not base_model_select:\n",
        "            raise gr.Error(\"Select a base model!\")\n",
        "\n",
        "        gr.Info(f\"Dreambooth finetune with task {base_model_select} started!\")\n",
        "\n",
        "        file_collection = list(chain.from_iterable(inputs[13:]))\n",
        "        base_model_id, resolution = get_dreambooth_base_model_and_resolution(\n",
        "            base_model_select\n",
        "        )\n",
        "\n",
        "        image_dir_gcs = os.path.join(BUCKET_URI, f\"instance_images-{now}\")\n",
        "        image_dir_gcsfuse = image_dir_gcs.replace(\"gs://\", \"/gcs/\")\n",
        "        tuned_model_dir_gcs = output_dir\n",
        "        tuned_model_dir_gcsfuse = tuned_model_dir_gcs.replace(\"gs://\", \"/gcs/\")\n",
        "\n",
        "        print(\"The tuned model will be saved at:\", output_dir)\n",
        "        output_dir_bucket = \"/\".join(output_dir.split(\"/\")[:3])  # noqa: F841\n",
        "        # Provision permissions to the two SERVICE_ACCOUNTs with the provided GCS bucket\n",
        "        ! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $output_dir_bucket\n",
        "        ! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT_CC}:roles/storage.admin $output_dir_bucket\n",
        "\n",
        "        prepare_instance_images(\n",
        "            base_model_select=base_model_select,\n",
        "            concept_prompt=concept_prompt,\n",
        "            file_collection=list(file_collection),\n",
        "        )\n",
        "\n",
        "        # Worker pool spec.\n",
        "        machine_type = \"g2-standard-8\"\n",
        "        num_nodes = 1\n",
        "        gpu_type = \"NVIDIA_L4\"\n",
        "        num_gpus = 1\n",
        "\n",
        "        common_util.check_quota(\n",
        "            project_id=PROJECT_ID,\n",
        "            region=REGION,\n",
        "            accelerator_type=gpu_type,\n",
        "            accelerator_count=num_gpus,\n",
        "            is_for_training=True,\n",
        "        )\n",
        "\n",
        "        # Add labels for the finetuning job.\n",
        "        labels = {\n",
        "            \"mg-source\": \"notebook\",\n",
        "            \"mg-notebook-name\": \"model_garden_pytorch_stable_diffusion_gradio.ipynb\".split(\n",
        "                \".\"\n",
        "            )[\n",
        "                0\n",
        "            ],\n",
        "        }\n",
        "\n",
        "        labels[\"mg-tune\"] = \"publishers-stability-ai-models-sd\"\n",
        "        versioned_model_id = base_model_select\n",
        "        labels[\"versioned-mg-tune\"] = f\"{labels['mg-tune']}-{versioned_model_id}\"\n",
        "\n",
        "        # Setup training job.\n",
        "        job_name = common_util.create_job_name(f\"{base_model_id}-dreambooth\")\n",
        "        job = aiplatform.CustomContainerTrainingJob(\n",
        "            display_name=job_name,\n",
        "            container_uri=TRAIN_DOCKER_URI,\n",
        "            labels=labels,\n",
        "        )\n",
        "\n",
        "        gr.Info(f\"Custom Training job {job_name} is being submitted. Let's wait.\")\n",
        "\n",
        "        task = \"text-to-image-dreambooth\"\n",
        "        if base_model_select == \"sdxl-dreambooth-lora-1024\":\n",
        "            task = \"text-to-image-dreambooth-lora-sdxl\"\n",
        "\n",
        "        args = [\n",
        "            f\"--task={task}\",\n",
        "            f\"--pretrained_model_name_or_path={base_model_id}\",\n",
        "            f\"--instance_data_dir={image_dir_gcsfuse}\",\n",
        "            f\"--class_data_dir={image_dir_gcsfuse}\",\n",
        "            f\"--output_dir={tuned_model_dir_gcsfuse}\",\n",
        "            f\"--instance_prompt={concept_prompt}\",\n",
        "            f\"--resolution={int(resolution)}\",\n",
        "            f\"--train_batch_size={int(train_batch_size)}\",\n",
        "            f\"--gradient_accumulation_steps={int(gradient_accumulation_steps)}\",\n",
        "            f\"--learning_rate={learning_rate}\",\n",
        "            f\"--lr_scheduler={lr_scheduler}\",\n",
        "            f\"--lr_warmup_steps={int(lr_warmup_steps)}\",\n",
        "            f\"--max_train_steps={int(train_steps)}\",\n",
        "            f\"--checkpointing_steps={int(checkpointing_steps)}\",\n",
        "            f\"--seed={int(seed)}\",\n",
        "            \"--gradient_checkpointing\",\n",
        "            \"--mixed_precision=fp16\",\n",
        "            \"--use_8bit_adam\",\n",
        "        ]\n",
        "\n",
        "        if base_model_select == \"sdxl-dreambooth-lora-1024\":\n",
        "            args.append(\n",
        "                \"--pretrained_vae_model_name_or_path=madebyollin/sdxl-vae-fp16-fix\"\n",
        "            )\n",
        "        if train_text_encoder:\n",
        "            args.append(\"--train_text_encoder\")\n",
        "\n",
        "        job.run(\n",
        "            args=args,\n",
        "            replica_count=num_nodes,\n",
        "            machine_type=machine_type,\n",
        "            accelerator_type=gpu_type,\n",
        "            accelerator_count=num_gpus,\n",
        "            sync=True,\n",
        "        )\n",
        "\n",
        "        if deploy_after_tuning:\n",
        "            deploy_finetuned_model(base_model_select, output_dir)\n",
        "\n",
        "    def update_default_lr(base_model_select: str):\n",
        "        \"\"\"Updates the default learning rate based on the selected base model.\"\"\"\n",
        "        lr = 2e-6\n",
        "        if base_model_select == \"sdxl-dreambooth-lora-1024\":\n",
        "            lr = 1e-4\n",
        "\n",
        "        return {\n",
        "            learning_rate: gr.update(value=lr),\n",
        "        }\n",
        "\n",
        "    def update_default_checkpoing_steps(train_steps: int):\n",
        "        \"\"\"Updates the default checkpointing_steps based on the training steps.\"\"\"\n",
        "        return {checkpointing_steps: gr.update(value=int(train_steps // 2))}\n",
        "\n",
        "    dreambooth_tip_text = r\"\"\"\n",
        "        1. Upload 5-10 images of the object/face on from different angles/perspectives.\n",
        "        2. Dreambooth training overfits easily. You need to tune the learning_rate and train_steps to get the best results.\n",
        "           Refer to the [Dreambooth Tips](https://huggingface.co/blog/dreambooth) for more details.\n",
        "        3. After the training job started, you can check the job status at\n",
        "           [Vertex Custom Training](https://console.cloud.google.com/vertex-ai/training/custom-jobs?project={PROJECT_ID}).\n",
        "        4. If you choose to deploy to Vertex after tuning, you can check the deployed endpoint at [Vertex Online Prediction](https://console.cloud.google.com/vertex-ai/online-prediction/endpoints?project={PROJECT_ID}).\n",
        "        \"\"\"\n",
        "\n",
        "    with gr.Blocks() as demo:\n",
        "        with gr.Accordion(\"How To Use\", open=False):\n",
        "            gr.Markdown(dreambooth_tip_text)\n",
        "        with gr.Row(equal_height=True):\n",
        "            with gr.Column():\n",
        "                base_model_select = gr.Dropdown(\n",
        "                    label=\"Select a base model\",\n",
        "                    choices=[\n",
        "                        \"sd-v2-1-768\",\n",
        "                        \"sd-v2-1-512\",\n",
        "                        \"sd-v1-5-512\",\n",
        "                        \"sdxl-dreambooth-lora-1024\",\n",
        "                    ],\n",
        "                    value=\"sd-v2-1-768\",\n",
        "                    interactive=True,\n",
        "                )\n",
        "                concept_prompt = gr.Textbox(\n",
        "                    label=\"Concept prompt - use a unique, made up word to avoid collisions\",\n",
        "                    lines=1,\n",
        "                )\n",
        "                learning_rate = gr.Number(\n",
        "                    label=\"Learning rate\", value=2e-6, interactive=True\n",
        "                )\n",
        "                train_steps = gr.Number(\n",
        "                    label=\"Number of training steps\", value=800, interactive=True\n",
        "                )\n",
        "                deploy_after_tuning = gr.Checkbox(\n",
        "                    label=\"Deploy to Vertex after tuning\", value=True, interactive=True\n",
        "                )\n",
        "\n",
        "                with gr.Accordion(\"Advanced Settings\", open=False):\n",
        "                    train_text_encoder = gr.Checkbox(\n",
        "                        label=\"Train text encoder\", value=False, interactive=True\n",
        "                    )\n",
        "                    checkpointing_steps = gr.Number(\n",
        "                        label=\"Checkpointing steps\", value=400, interactive=True\n",
        "                    )\n",
        "                    train_batch_size = gr.Number(\n",
        "                        label=\"Training batch size\", value=1, interactive=True\n",
        "                    )\n",
        "                    gradient_accumulation_steps = gr.Number(\n",
        "                        label=\"Gradient accumulation steps\", value=1, interactive=True\n",
        "                    )\n",
        "                    lr_scheduler = gr.Dropdown(\n",
        "                        label=\"Learning rate scheduler\",\n",
        "                        choices=[\n",
        "                            \"constant\",\n",
        "                            \"linear\",\n",
        "                            \"cosine\",\n",
        "                            \"cosine_with_restarts\",\n",
        "                            \"polynomial\",\n",
        "                            \"constant_with_warmup\",\n",
        "                            \"piecewise_costant\",\n",
        "                        ],\n",
        "                        value=\"constant\",\n",
        "                    )\n",
        "                    lr_warmup_steps = gr.Number(\n",
        "                        label=\"Warmup steps\", value=0, interactive=True\n",
        "                    )\n",
        "                    seed = gr.Number(label=\"Seed\", value=42, interactive=True)\n",
        "\n",
        "            with gr.Column():\n",
        "                train_button = gr.Button(\"Start Training\", variant=\"primary\")\n",
        "\n",
        "                file_collection = []\n",
        "                file_collection.append(\n",
        "                    gr.File(\n",
        "                        label=\"Upload 5-10 reference images for your concept\",\n",
        "                        file_types=[\"image\"],\n",
        "                        file_count=\"multiple\",\n",
        "                        interactive=True,\n",
        "                        visible=True,\n",
        "                    )\n",
        "                )\n",
        "\n",
        "                output_dir_gcs = os.path.join(BUCKET_URI, f\"tuned_model-{now}\")\n",
        "                output_dir = gr.Textbox(\n",
        "                    label=\"The GCS directory to save the tuned model. Defaults to the staging bucket set/created at initiation.\",\n",
        "                    value=output_dir_gcs,\n",
        "                    lines=1,\n",
        "                    interactive=True,\n",
        "                    visible=True,\n",
        "                )\n",
        "\n",
        "    base_model_select.change(\n",
        "        update_default_lr,\n",
        "        base_model_select,\n",
        "        [\n",
        "            learning_rate,\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    train_steps.change(\n",
        "        update_default_checkpoing_steps,\n",
        "        train_steps,\n",
        "        [\n",
        "            checkpointing_steps,\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    train_button.click(\n",
        "        dreambooth_start_training,\n",
        "        inputs=[\n",
        "            output_dir,\n",
        "            base_model_select,\n",
        "            concept_prompt,\n",
        "            learning_rate,\n",
        "            train_steps,\n",
        "            deploy_after_tuning,\n",
        "            train_text_encoder,\n",
        "            checkpointing_steps,\n",
        "            train_batch_size,\n",
        "            gradient_accumulation_steps,\n",
        "            lr_scheduler,\n",
        "            lr_warmup_steps,\n",
        "            seed,\n",
        "        ]\n",
        "        + file_collection,\n",
        "        outputs=[],\n",
        "    )\n",
        "\n",
        "    return demo\n",
        "\n",
        "\n",
        "css = \"\"\"\n",
        "    .gradio-container {\n",
        "      width: 90% !important\n",
        "    }\n",
        "\"\"\"\n",
        "with gr.Blocks(\n",
        "    css=css, theme=gr.themes.Default(primary_hue=\"orange\", secondary_hue=\"blue\")\n",
        ") as demo:\n",
        "    gr.Markdown(\"# Model Garden Playground for Image Generation\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"Image Generation\"):\n",
        "            create_generation_workshop()\n",
        "        with gr.TabItem(\"Dreambooth Finetune\"):\n",
        "            create_dreambooth_workshop()\n",
        "\n",
        "show_debug_logs = True  # @param {type: \"boolean\"}\n",
        "demo.queue()\n",
        "demo.launch(share=True, inline=False, debug=show_debug_logs, show_error=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oawqNiQi6SK"
      },
      "source": [
        "## Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pwR6RCmbzNh8"
      },
      "outputs": [],
      "source": [
        "# @markdown Delete temporary GCS buckets.\n",
        "\n",
        "delete_bucket = False  # @param {type:\"boolean\"}\n",
        "if delete_bucket:\n",
        "    ! gsutil -m rm -r $BUCKET_NAME"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_pytorch_stable_diffusion_gradio.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

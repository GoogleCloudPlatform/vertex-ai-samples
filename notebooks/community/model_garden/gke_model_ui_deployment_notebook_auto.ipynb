{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Pr9TgOcV9vAXeqGiyTaTI5kS",
      "metadata": {
        "cellView": "form",
        "id": "Pr9TgOcV9vAXeqGiyTaTI5kS"
      },
      "outputs": [],
      "source": [
        "# Copyright 2026 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M1CpgYundFwz",
      "metadata": {
        "id": "M1CpgYundFwz"
      },
      "source": [
        "# Get started with your deployed model on GKE\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fgke_model_ui_deployment_notebook_auto.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/gke_model_ui_deployment_notebook_auto.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "t2jj2XOgkS4F",
      "metadata": {
        "id": "t2jj2XOgkS4F"
      },
      "source": [
        "# Overview\n",
        "\n",
        "This notebook will guide you through the initial step of testing your recently\n",
        "deployed model with text prompts. Depending on your deployed model's inference\n",
        "setup, the notebook utilizes either Text Generation Inference\n",
        "[TGI](https://huggingface.co/docs/text-generation-inference/en/index) or\n",
        "[vLLM](https://developers.googleblog.com/en/inference-with-gemma-using-dataflow-and-vllm/#:~:text=model%20frameworks%20simple.-,What%20is%20vLLM%3F,-vLLM%20is%20an),\n",
        "two efficient serving frameworks that enhance the performance of your GPU model.\n",
        "Ready to see your deployed model respond? Run the cells below and start\n",
        "experimenting with different prompts!\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "Before proceeding with this notebook, ensure you have already deployed a model\n",
        "using the Google Cloud Console. You can find an overview of AI and Machine\n",
        "Learning services on\n",
        "[GKE AI/ML](https://console.cloud.google.com/kubernetes/aiml/overview).\n",
        "\n",
        "### Objective\n",
        "\n",
        "Enable prompt-based testing of the AI model deployed on GKE\n",
        "\n",
        "### GPUs\n",
        "\n",
        "GPUs let you accelerate specific workloads running on your nodes, such as\n",
        "machine learning and data processing. GKE provides a range of machine type\n",
        "options for node configuration, including machine types with NVIDIA H100, L4,\n",
        "and A100 GPUs.\n",
        "\n",
        "### Understanding the Inference Frameworks\n",
        "\n",
        "Your model is running on one of two popular and efficient serving frameworks:\n",
        "vLLM or Text Generation Inference (TGI). The following sections provide a brief\n",
        "overview of each to give you context on the underlying technology powering your\n",
        "model.\n",
        "\n",
        "#### TGI\n",
        "\n",
        "TGI is a highly optimized open-source LLM serving framework that can increase\n",
        "serving throughput on GPUs. TGI includes features such as:\n",
        "\n",
        "*   Optimized transformer implementation with PagedAttention\n",
        "*   Continuous batching to improve the overall serving throughput\n",
        "*   Tensor parallelism and distributed serving on multiple GPUs\n",
        "\n",
        "To learn more, refer to the\n",
        "[TGI documentation](https://github.com/huggingface/text-generation-inference/blob/main/README.md)\n",
        "\n",
        "#### vLLM\n",
        "\n",
        "vLLM is another fast and easy-to-use library for LLM inference and serving. It's\n",
        "known for its high throughput and efficiency, and it leverages PagedAttention.\n",
        "Key features include:\n",
        "\n",
        "*   PagedAttention: Efficient memory management for handling long sequences and\n",
        "    dynamic workloads.\n",
        "*   Continuous batching: Maximizes GPU utilization by batching incoming\n",
        "    requests.\n",
        "*   High-throughput serving: Designed for production-level serving with low\n",
        "    latency.\n",
        "*   Optimized CUDA kernels.\n",
        "\n",
        "To learn more, refer to the\n",
        "[vLLM documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/vllm/use-vllm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XMf-T58TkDy1",
      "metadata": {
        "cellView": "form",
        "id": "XMf-T58TkDy1"
      },
      "outputs": [],
      "source": [
        "# @title # Connect to Google Cloud Project\n",
        "# @markdown #### Run this cell to configure your Google Cloud environment for Kubernetes (GKE) operations.\n",
        "# @markdown\n",
        "# @markdown #### Actions:\n",
        "# @markdown 1.  **Connects to Project:** Retrieves and sets your Google Cloud project ID.\n",
        "# @markdown 3.  **Installs `kubectl`:** Installs the Kubernetes command-line tool.\n",
        "\n",
        "import os\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Set up gcloud.\n",
        "! gcloud config set project \"$PROJECT_ID\"\n",
        "! gcloud services enable container.googleapis.com\n",
        "\n",
        "# Add kubectl to the set of available tools.\n",
        "! mkdir -p /tools/google-cloud-sdk/.install\n",
        "! gcloud components install kubectl --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IKGTaN84p8rX",
      "metadata": {
        "cellView": "form",
        "id": "IKGTaN84p8rX"
      },
      "outputs": [],
      "source": [
        "# @title # Chat completion for text-only models {vertical-output: true}\n",
        "# @markdown Run cell to prompt the model server for prediction.\n",
        "# @markdown\n",
        "# @markdown * **user_prompt (string):** This is the text prompt you provide to the language model. It's the question or instruction e (e.g., \"Explain neural networks\").\n",
        "# @markdown * **temperature (number):** This  parameter controls the randomness of the model's output. It influences how the model selects the next token in the sequence it generates. Typical values range from 0.2 to 1.0.\n",
        "# @markdown * **max_tokens (number):** This parameter refers to the maximum number of tokens (words or sub-word units) that the model is allowed to generate in its response.\n",
        "# @markdown\n",
        "\n",
        "import json\n",
        "import subprocess\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import Markdown, clear_output, display\n",
        "\n",
        "CLUSTER = \"\"  # @param {type:\"string\", isTemplate:true}\n",
        "REGION = \"\"  # @param {type:\"string\", isTemplate:true}\n",
        "NAMESPACE = \"\"  # @param {type:\"string\", isTemplate:true}\n",
        "DEPLOYMENT = \"\"  # @param {type:\"string\", isTemplate:true}\n",
        "POD_PORT = \"\"  # @param {type:\"string\", isTemplate:true}\n",
        "\n",
        "\n",
        "def _run_kubectl(cmd, timeout=60):\n",
        "    \"\"\"Executes a kubectl command.\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            cmd, capture_output=True, text=True, check=True, timeout=timeout\n",
        "        )\n",
        "        return result.stdout.strip()\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        raise RuntimeError(\n",
        "            f\"Kubectl command failed: {' '.join(e.cmd)}\\nStderr: {e.stderr}\"\n",
        "        ) from e\n",
        "    except subprocess.TimeoutExpired as e:\n",
        "        raise RuntimeError(f\"Kubectl command timed out: {' '.join(e.cmd)}\") from e\n",
        "\n",
        "\n",
        "def fetch_cluster_credentials(cluster, region, project_id):\n",
        "    \"\"\"Ensures credentials for the target GKE cluster.\"\"\"\n",
        "    cred_cmd = [\n",
        "        \"gcloud\",\n",
        "        \"container\",\n",
        "        \"clusters\",\n",
        "        \"get-credentials\",\n",
        "        cluster,\n",
        "        f\"--location={region}\",\n",
        "        f\"--project={project_id}\",\n",
        "    ]\n",
        "    _run_kubectl(cred_cmd)\n",
        "\n",
        "\n",
        "def get_deployment_selector_labels(deployment_name, namespace):\n",
        "    \"\"\"Retrieves the selector labels for a given Kubernetes deployment.\"\"\"\n",
        "    cmd = [\n",
        "        \"kubectl\",\n",
        "        \"get\",\n",
        "        \"deployment\",\n",
        "        deployment_name,\n",
        "        \"-n\",\n",
        "        namespace,\n",
        "        \"-o\",\n",
        "        \"json\",\n",
        "    ]\n",
        "    deployment_json = _run_kubectl(cmd)\n",
        "    deployment_data = json.loads(deployment_json)\n",
        "\n",
        "    selector_labels = (\n",
        "        deployment_data.get(\"spec\", {}).get(\"selector\", {}).get(\"matchLabels\")\n",
        "    )\n",
        "    if not selector_labels:\n",
        "        raise RuntimeError(\n",
        "            f\"No selector labels found for deployment '{deployment_name}' in\"\n",
        "            f\" namespace '{namespace}'.\"\n",
        "        )\n",
        "    return selector_labels\n",
        "\n",
        "\n",
        "def get_running_pod_name(deployment_name, namespace):\n",
        "    \"\"\"Retrieves the name of a running pod associated with a deployment.\"\"\"\n",
        "    selector_labels = get_deployment_selector_labels(deployment_name, namespace)\n",
        "    label_selector_str = \",\".join(f\"{k}={v}\" for k, v in selector_labels.items())\n",
        "\n",
        "    cmd = [\n",
        "        \"kubectl\",\n",
        "        \"get\",\n",
        "        \"pods\",\n",
        "        \"-n\",\n",
        "        namespace,\n",
        "        \"-o\",\n",
        "        \"json\",\n",
        "        \"-l\",\n",
        "        label_selector_str,\n",
        "        \"--field-selector=status.phase=Running\",\n",
        "    ]\n",
        "    pods_json = _run_kubectl(cmd)\n",
        "    pods_data = json.loads(pods_json)\n",
        "\n",
        "    if not pods_data.get(\"items\"):\n",
        "        raise RuntimeError(\n",
        "            f\"No running pods found for deployment '{deployment_name}' in namespace\"\n",
        "            f\" '{namespace}' with selector '{label_selector_str}'.\"\n",
        "        )\n",
        "    return pods_data[\"items\"][0][\"metadata\"][\"name\"]\n",
        "\n",
        "\n",
        "def check_vllm_inference_label(pod_name, namespace):\n",
        "    \"\"\"Checks if the specified pod has the vLLM inference server label.\"\"\"\n",
        "    cmd = [\"kubectl\", \"get\", \"pod\", pod_name, \"-n\", namespace, \"-o\", \"json\"]\n",
        "    pod_json = _run_kubectl(cmd)\n",
        "    labels = json.loads(pod_json).get(\"metadata\", {}).get(\"labels\", {})\n",
        "    return labels.get(\"ai.gke.io/inference-server\") == \"vllm\"\n",
        "\n",
        "\n",
        "def send_inference_request(\n",
        "    request_payload, pod_name, pod_port, is_vllm_inference, namespace\n",
        "):\n",
        "    \"\"\"Sends an inference request to the specified pod and returns the model's response.\"\"\"\n",
        "    json_data_escaped = json.dumps(request_payload).replace(\"'\", \"'\\\\''\")\n",
        "    curl_cmd = (\n",
        "        f\"kubectl exec -n {namespace} -t {pod_name} -- curl -s -X POST\"\n",
        "        f' http://localhost:{pod_port}/generate -H \"Content-Type:'\n",
        "        ' application/json\"'\n",
        "        f\" -d '{json_data_escaped}' 2> /dev/null\"\n",
        "    )\n",
        "\n",
        "    response_raw = _run_kubectl([\"bash\", \"-c\", curl_cmd])\n",
        "\n",
        "    if not response_raw:\n",
        "        raise RuntimeError(f\"Empty response received from pod '{pod_name}'.\")\n",
        "\n",
        "    try:\n",
        "        first_line = response_raw.splitlines()[0]\n",
        "        data = json.loads(first_line)\n",
        "    except json.JSONDecodeError as e:\n",
        "        raise RuntimeError(\n",
        "            f\"Failed to decode JSON response from pod: {e}. Raw: {response_raw}\"\n",
        "        ) from e\n",
        "    except IndexError:\n",
        "        raise RuntimeError(\n",
        "            f\"Unexpected empty response line from pod. Raw: {response_raw}\"\n",
        "        )\n",
        "\n",
        "    if is_vllm_inference:\n",
        "        predictions = data.get(\"predictions\")\n",
        "        if isinstance(predictions, list) and predictions:\n",
        "            return predictions[0]\n",
        "        raise RuntimeError(f\"Unexpected vLLM response format. Raw data: {data}\")\n",
        "    else:  # TGI format\n",
        "        generated_text = data.get(\"generated_text\")\n",
        "        if generated_text is not None:\n",
        "            return generated_text\n",
        "        raise RuntimeError(f\"Unexpected TGI response format. Raw data: {data}\")\n",
        "\n",
        "\n",
        "# --- Main Execution Logic ---\n",
        "\n",
        "\n",
        "def execute_chat_completion(\n",
        "    deployment_name, namespace, pod_port, user_prompt, temperature, max_tokens\n",
        "):\n",
        "    \"\"\"Executes the full chat completion process: fetches credentials, finds a pod,\n",
        "\n",
        "    determines inference type, sends a request, and returns the response.\n",
        "    \"\"\"\n",
        "    display(Markdown(\"Establishing cluster credentials...\"))\n",
        "    fetch_cluster_credentials(CLUSTER, REGION, PROJECT_ID)\n",
        "\n",
        "    display(Markdown(\"Retrieving pod information...\"))\n",
        "    pod_name = get_running_pod_name(deployment_name, namespace)\n",
        "    display(Markdown(f\"Successfully identified pod: `{pod_name}`\"))\n",
        "\n",
        "    is_vllm = check_vllm_inference_label(pod_name, namespace)\n",
        "\n",
        "    request_payload = {\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"prompt\" if is_vllm else \"inputs\": user_prompt,\n",
        "    }\n",
        "    display(Markdown(\"Sending inference request...\"))\n",
        "    response = send_inference_request(\n",
        "        request_payload, pod_name, pod_port, is_vllm, namespace\n",
        "    )\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "# --- Widgets Setup ---\n",
        "user_prompt_widget = widgets.Textarea(\n",
        "    value=\"What is AI?\",\n",
        "    description=\"User Prompt:\",\n",
        "    layout=widgets.Layout(width=\"95%\", height=\"100px\"),\n",
        ")\n",
        "\n",
        "temperature_widget = widgets.FloatSlider(\n",
        "    value=0.50, min=0.0, max=1.0, step=0.01, description=\"Temperature:\"\n",
        ")\n",
        "\n",
        "max_tokens_widget = widgets.IntSlider(\n",
        "    value=250, min=1, max=2048, step=1, description=\"Max Tokens:\"\n",
        ")\n",
        "\n",
        "submit_button = widgets.Button(description=\"Submit\")\n",
        "output_area_response = widgets.Output()\n",
        "\n",
        "\n",
        "# --- Submit Button Logic ---\n",
        "def on_submit_clicked(b):\n",
        "    with output_area_response:\n",
        "        clear_output()\n",
        "        display(Markdown(\"Loading...\"))\n",
        "\n",
        "        try:\n",
        "            model_response = execute_chat_completion(\n",
        "                DEPLOYMENT,\n",
        "                NAMESPACE,\n",
        "                POD_PORT,\n",
        "                user_prompt_widget.value,\n",
        "                temperature_widget.value,\n",
        "                max_tokens_widget.value,\n",
        "            )\n",
        "            clear_output()\n",
        "            display(Markdown(f\"**Response:**\\n\\n{model_response}\"))\n",
        "        except Exception as e:\n",
        "            clear_output()\n",
        "            display(Markdown(f\"**An error occurred:**\\n```\\n{e}\\n```\"))\n",
        "\n",
        "\n",
        "# --- Display Widgets ---\n",
        "submit_button.on_click(on_submit_clicked)\n",
        "display(\n",
        "    user_prompt_widget,\n",
        "    temperature_widget,\n",
        "    max_tokens_widget,\n",
        "    submit_button,\n",
        "    output_area_response,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b6ZM2K3fux0",
      "metadata": {
        "id": "5b6ZM2K3fux0"
      },
      "source": [
        "# Next Steps: Integrating the GKE Service Endpoint\n",
        "\n",
        "After successfully deploying a model on Google Kubernetes Engine (GKE) and\n",
        "verifying it via a notebook, the next step is to integrate it into various\n",
        "applications. This involves making HTTP requests to the service's endpoint from\n",
        "your application code.\n",
        "\n",
        "### Exposing the Service\n",
        "\n",
        "To make your deployed model accessible to applications, you'll need to expose\n",
        "its service endpoint. Google Kubernetes Engine offers several ways to do this:\n",
        "\n",
        "1.  **Ingress:** Configure an Ingress resource to route external HTTP(S) traffic\n",
        "    to your service. Set up Ingress for either an internal Load Balancer\n",
        "    (accessible only within your VPC) or an external Load Balancer (accessible\n",
        "    from the internet).\n",
        "    [Learn more about GKE Ingress](https://cloud.google.com/kubernetes-engine/docs/concepts/ingress).\n",
        "2.  **Gateway API:** A more modern and feature-rich API for managing traffic\n",
        "    routing in Kubernetes. Similar to Ingress, Gateway API allows you to define\n",
        "    how external and internal traffic should be directed to your services.\n",
        "    [Explore GKE Gateway API](https://cloud.google.com/kubernetes-engine/docs/concepts/gateway-api).\n",
        "\n",
        "### Setting Up Autoscaling\n",
        "\n",
        "Ensure your model serving can handle varying traffic by configuring the\n",
        "Horizontal Pod Autoscaler (HPA). HPA automatically scales the number of Pods\n",
        "based on resource utilization or custom metrics, optimizing performance and\n",
        "cost.\n",
        "[See how to configure HPA](https://cloud.google.com/kubernetes-engine/docs/how-to/horizontal-pod-autoscaling).\n",
        "\n",
        "### Setting Up Monitoring\n",
        "\n",
        "Monitor the health and performance of your deployed model using Google Cloud\n",
        "Managed Service for Prometheus. Configure your model serving to expose\n",
        "Prometheus metrics for comprehensive insights.\n",
        "[Get started with Google Cloud Managed Prometheus](https://cloud.google.com/kubernetes-engine/docs/how-to/configure-automatic-application-monitoring).\n",
        "\n",
        "### Additional Resources:\n",
        "\n",
        "*   #### Kubernetes Documentation:\n",
        "\n",
        "    *   Services:\n",
        "        https://kubernetes.io/docs/concepts/services-networking/service/\n",
        "\n",
        "*   #### Google Cloud Documentation:\n",
        "\n",
        "    *   Google Kubernetes Engine (GKE):\n",
        "        https://cloud.google.com/kubernetes-engine\n",
        "    *   Cloud Load Balancing:\n",
        "        https://cloud.google.com/load-balancing/docs/ingress\n",
        "    *   Gateway API on GKE:\n",
        "        https://cloud.google.com/kubernetes-engine/docs/concepts/gateway-api\n",
        "    *   Learn about GPUs in GKE:\n",
        "        https://cloud.google.com/kubernetes-engine/docs/concepts/gpus\n",
        "\n",
        "*   #### Python requests Library:\n",
        "\n",
        "    *   https://requests.readthedocs.io/en/latest/\n",
        "\n",
        "*   #### LangChain with Google Integrations:\n",
        "\n",
        "    *   The Langchain documentation is very useful:\n",
        "        https://python.langchain.com/docs/integrations/providers/google/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "gke_model_ui_deployment_notebook_auto.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

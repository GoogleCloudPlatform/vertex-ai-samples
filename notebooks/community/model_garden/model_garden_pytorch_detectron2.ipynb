{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2026 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Vertex AI Model Garden - Detectron2\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_pytorch_detectron2.ipynb\">\n",
        "      <img alt=\"Workbench logo\" src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" width=\"32px\"><br> Run in Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_pytorch_detectron2.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_detectron2.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates finetuning Detectron2 based [Faster R-CNN](https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md#faster-r-cnn) and\n",
        "[RetinaNet](https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md#retinanet)\n",
        "for image detection task and [Mask R-CNN](https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md#coco-instance-segmentation-baselines-with-mask-r-cnn) for segmentation task and deploying them on Vertex AI for online prediction. This notebook assumes that the input training data is in [COCO format](https://opencv.org/introduction-to-the-coco-dataset/). If you do not have your own dataset, this notebook also shows how to download and prepare the Balloon dataset for training.\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Finetune a Detectron2 based Faster R-CNN, RetinaNet, or Mask R-CNN model.\n",
        "- Upload the model to [Model Registry](https://cloud.google.com/vertex-ai/docs/model-registry/introduction).\n",
        "- Deploy the model on [Endpoint](https://cloud.google.com/vertex-ai/docs/predictions/using-private-endpoints).\n",
        "- Run online predictions for image object detection and segmentation.\n",
        "\n",
        "### File a bug\n",
        "\n",
        "File a bug on [GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/issues/new) if you encounter any issue with the notebook.\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "264c07757582"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dc8ee367fb42"
      },
      "outputs": [],
      "source": [
        "# @title Import the necessary packages\n",
        "\n",
        "# Install libraries for COCO format conversion of datasets.\n",
        "!pip install --upgrade --quiet pycocotools==2.0.6\n",
        "!pip install --upgrade --quiet opencv-python==4.7.0.72"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5244aac3d929"
      },
      "source": [
        "Restart the notebook kernel after installs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "567212ff53a6"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown 2. For finetuning, **[click here](https://console.cloud.google.com/iam-admin/quotas?location=us-central1&metric=aiplatform.googleapis.com%2Frestricted_image_training_nvidia_a100_80gb_gpus)** to check if your project already has the required 8 Nvidia A100 80 GB GPUs in the us-central1 region. If yes, then run this notebook in the us-central1 region. If you do not have 8 Nvidia A100 80 GPUs or have more GPU requirements than this, then schedule your job with Nvidia H100 GPUs via Dynamic Workload Scheduler using [these instructions](https://cloud.google.com/vertex-ai/docs/training/schedule-jobs-dws). For Dynamic Workload Scheduler, check the [us-central1](https://console.cloud.google.com/iam-admin/quotas?location=us-central1&metric=aiplatform.googleapis.com%2Fcustom_model_training_preemptible_nvidia_h100_gpus) or [europe-west4](https://console.cloud.google.com/iam-admin/quotas?location=europe-west4&metric=aiplatform.googleapis.com%2Fcustom_model_training_preemptible_nvidia_h100_gpus) quota for Nvidia H100 GPUs. If you do not have enough GPUs, then you can follow [these instructions](https://cloud.google.com/docs/quotas/view-manage#viewing_your_quota_console) to request quota.\n",
        "\n",
        "# @markdown 3. For serving, **[click here](https://console.cloud.google.com/iam-admin/quotas?location=us-central1&metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_l4_gpus)** to check if your project already has the required 1 L4 GPU in the us-central1 region.  If yes, then run this notebook in the us-central1 region. If you need more L4 GPUs for your project, then you can follow [these instructions](https://cloud.google.com/docs/quotas/view-manage#viewing_your_quota_console) to request more. Alternatively, if you want to run predictions with A100 80GB or H100 GPUs, we recommend using the regions listed below. **NOTE:** Make sure you have associated quota in selected regions. Click the links to see your current quota for each GPU type: [Nvidia A100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_a100_80gb_gpus), [Nvidia H100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_h100_gpus).\n",
        "\n",
        "# @markdown > | Machine Type | Accelerator Type | Recommended Regions |\n",
        "# @markdown | ----------- | ----------- | ----------- |\n",
        "# @markdown | a2-ultragpu-1g | 1 NVIDIA_A100_80GB | us-central1, us-east4, europe-west4, asia-southeast1, us-east4 |\n",
        "# @markdown | a3-highgpu-2g | 2 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "# @markdown | a3-highgpu-4g | 4 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "# @markdown | a3-highgpu-8g | 8 NVIDIA_H100_80GB | us-central1, europe-west4, us-west1, asia-southeast1 |\n",
        "\n",
        "# @markdown 4. **[Optional]** [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs. Set the BUCKET_URI for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`) should be located in the same region as where the notebook was launched. Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\"). If not set, a unique GCS bucket will be created instead.\n",
        "\n",
        "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown 5. **[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment.\n",
        "\n",
        "REGION = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "import base64\n",
        "import datetime\n",
        "import importlib\n",
        "# Import the necessary packages\n",
        "import json\n",
        "import os\n",
        "import uuid\n",
        "\n",
        "import cv2\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "# Upgrade Vertex AI SDK.\n",
        "! pip3 install --upgrade --quiet 'google-cloud-aiplatform==1.103.0'\n",
        "\n",
        "if os.environ.get(\"VERTEX_PRODUCT\") != \"COLAB_ENTERPRISE\":\n",
        "    ! pip install --upgrade tensorflow\n",
        "! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
        "\n",
        "common_util = importlib.import_module(\n",
        "    \"vertex-ai-samples.notebooks.community.model_garden.docker_source_codes.notebook_util.common_util\"\n",
        ")\n",
        "\n",
        "models, endpoints = {}, {}\n",
        "\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "if not REGION:\n",
        "    if not os.environ.get(\"GOOGLE_CLOUD_REGION\"):\n",
        "        raise ValueError(\n",
        "            \"REGION must be set. See\"\n",
        "            \" https://cloud.google.com/vertex-ai/docs/general/locations for\"\n",
        "            \" available cloud locations.\"\n",
        "        )\n",
        "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
        "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
        "\n",
        "# Cloud Storage bucket for storing the experiment artifacts.\n",
        "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
        "# prefer using your own GCS bucket, change the value yourself below.\n",
        "now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "\n",
        "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
        "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}-{str(uuid.uuid4())[:4]}\"\n",
        "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
        "else:\n",
        "    assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
        "    shell_output = ! gsutil ls -Lb {BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
        "    bucket_region = shell_output[0].strip().lower()\n",
        "    if bucket_region != REGION:\n",
        "        raise ValueError(\n",
        "            \"Bucket region %s is different from notebook region %s\"\n",
        "            % (bucket_region, REGION)\n",
        "        )\n",
        "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
        "\n",
        "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
        "MODEL_BUCKET = os.path.join(BUCKET_URI, \"detectron2\")\n",
        "\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "# Gets the default SERVICE_ACCOUNT.\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
        "\n",
        "\n",
        "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
        "\n",
        "! gcloud config set project $PROJECT_ID\n",
        "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/storage.admin\"\n",
        "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/aiplatform.user\"\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(\n",
        "    project=PROJECT_ID,\n",
        "    location=REGION,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XcYUGwr-AJGY"
      },
      "outputs": [],
      "source": [
        "# @title Define helper functions and constants\n",
        "\n",
        "# The pre-built training docker image. It contains training scripts and models.\n",
        "TRAIN_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-detectron2-train\"\n",
        "# The pre-built serving docker image. It contains serving scripts and models.\n",
        "SERVE_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-detectron2-serve\"\n",
        "\n",
        "# @markdown Set use_dedicated_endpoint to False if you don't want to use [dedicated endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment#create-dedicated-endpoint). Note that [dedicated endpoint does not support VPC Service Controls](https://cloud.google.com/vertex-ai/docs/predictions/choose-endpoint-type), uncheck the box if you are using VPC-SC.\n",
        "use_dedicated_endpoint = True  # @param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "def gcs_fuse_path(path: str) -> str:\n",
        "    \"\"\"Try to convert path to gcsfuse path if it starts with gs:// else do not modify it.\"\"\"\n",
        "    path = path.strip()\n",
        "    if path.startswith(\"gs://\"):\n",
        "        return \"/gcs/\" + path[5:]\n",
        "    return path\n",
        "\n",
        "\n",
        "def deploy_model(\n",
        "    project: str,\n",
        "    location: str,\n",
        "    display_name: str,\n",
        "    serving_container_image_uri: str,\n",
        "    model_pth_file: str,\n",
        "    model_cfg_yaml_file: str,\n",
        "    publisher_model_id: str,\n",
        "    service_account: str,\n",
        "    test_threshold: float = 0.5,\n",
        "    use_dedicated_endpoint: bool = True,\n",
        "):\n",
        "\n",
        "    endpoint = aiplatform.Endpoint.create(\n",
        "        display_name=display_name,\n",
        "        dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
        "    )\n",
        "\n",
        "    serving_env = {\n",
        "        \"MODEL_PTH_FILE\": model_pth_file,\n",
        "        \"CONFIG_YAML_FILE\": model_cfg_yaml_file,\n",
        "        \"TEST_THRESHOLD\": test_threshold,\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "    }\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=display_name,\n",
        "        serving_container_image_uri=serving_container_image_uri,\n",
        "        serving_container_ports=[7080],\n",
        "        serving_container_predict_route=\"/predictions/detectron2_serving\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "        serving_container_environment_variables=serving_env,\n",
        "        model_garden_source_model_name=(\n",
        "            f\"publishers/google/models/{publisher_model_id}\"\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=\"n1-highmem-16\",\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=service_account,\n",
        "        system_labels={\n",
        "            \"NOTEBOOK_NAME\": \"model_garden_pytorch_detectron2.ipynb\",\n",
        "            \"NOTEBOOK_ENVIRONMENT\": common_util.get_deploy_source(),\n",
        "        },\n",
        "    )\n",
        "\n",
        "    print(model.display_name)\n",
        "    print(model.resource_name)\n",
        "    return endpoint\n",
        "\n",
        "\n",
        "# Prediction\n",
        "\n",
        "\n",
        "def get_prediction_instances(local_test_filepath):\n",
        "    with open(local_test_filepath, \"rb\") as input_file:\n",
        "        encoded_string = base64.b64encode(input_file.read()).decode(\"utf-8\")\n",
        "\n",
        "    instances = [\n",
        "        {\n",
        "            \"data\": {\"b64\": encoded_string},\n",
        "        }\n",
        "    ]\n",
        "    return instances\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "# Mask encoding related\n",
        "import pycocotools.mask as mask_util\n",
        "\n",
        "\n",
        "def decode_rle_masks(pred_masks_rle):\n",
        "    return np.stack([mask_util.decode(rle) for rle in pred_masks_rle])\n",
        "\n",
        "\n",
        "import collections\n",
        "\n",
        "# Visualization\n",
        "from PIL import Image, ImageColor, ImageDraw, ImageFont\n",
        "\n",
        "\n",
        "def load_img(local_path):\n",
        "    return Image.open(local_path).convert(\"RGB\")\n",
        "\n",
        "\n",
        "def draw_bounding_box_on_image_array(\n",
        "    image,\n",
        "    ymin,\n",
        "    xmin,\n",
        "    ymax,\n",
        "    xmax,\n",
        "    color=\"red\",\n",
        "    thickness=4,\n",
        "    display_str_list=(),\n",
        "    use_normalized_coordinates=True,\n",
        "):\n",
        "    \"\"\"Adds a bounding box to an image (numpy array).\n",
        "\n",
        "    Bounding box coordinates can be specified in either absolute (pixel) or\n",
        "    normalized coordinates by setting the use_normalized_coordinates argument.\n",
        "\n",
        "    Args:\n",
        "      image: a numpy array with shape [height, width, 3].\n",
        "      ymin: ymin of bounding box.\n",
        "      xmin: xmin of bounding box.\n",
        "      ymax: ymax of bounding box.\n",
        "      xmax: xmax of bounding box.\n",
        "      color: color to draw bounding box. Default is red.\n",
        "      thickness: line thickness. Default value is 4.\n",
        "      display_str_list: list of strings to display in box\n",
        "                        (each to be shown on its own line).\n",
        "      use_normalized_coordinates: If True (default), treat coordinates\n",
        "        ymin, xmin, ymax, xmax as relative to the image.  Otherwise treat\n",
        "        coordinates as absolute.\n",
        "    \"\"\"\n",
        "    image_pil = Image.fromarray(np.uint8(image)).convert(\"RGB\")\n",
        "    draw_bounding_box_on_image(\n",
        "        image_pil,\n",
        "        ymin,\n",
        "        xmin,\n",
        "        ymax,\n",
        "        xmax,\n",
        "        color,\n",
        "        thickness,\n",
        "        display_str_list,\n",
        "        use_normalized_coordinates,\n",
        "    )\n",
        "    np.copyto(image, np.array(image_pil))\n",
        "\n",
        "\n",
        "def get_font_size(font: ImageFont.FreeTypeFont, text: str):\n",
        "    left, top, right, bottom = font.getbbox(text)\n",
        "    return right - left, bottom - top\n",
        "\n",
        "\n",
        "def draw_bounding_box_on_image(\n",
        "    image,\n",
        "    ymin,\n",
        "    xmin,\n",
        "    ymax,\n",
        "    xmax,\n",
        "    color=\"red\",\n",
        "    thickness=4,\n",
        "    display_str_list=(),\n",
        "    use_normalized_coordinates=True,\n",
        "):\n",
        "    \"\"\"Adds a bounding box to an image.\n",
        "\n",
        "    Bounding box coordinates can be specified in either absolute (pixel) or\n",
        "    normalized coordinates by setting the use_normalized_coordinates argument.\n",
        "\n",
        "    Each string in display_str_list is displayed on a separate line above the\n",
        "    bounding box in black text on a rectangle filled with the input 'color'.\n",
        "    If the top of the bounding box extends to the edge of the image, the strings\n",
        "    are displayed below the bounding box.\n",
        "\n",
        "    Args:\n",
        "      image: a PIL.Image object.\n",
        "      ymin: ymin of bounding box.\n",
        "      xmin: xmin of bounding box.\n",
        "      ymax: ymax of bounding box.\n",
        "      xmax: xmax of bounding box.\n",
        "      color: color to draw bounding box. Default is red.\n",
        "      thickness: line thickness. Default value is 4.\n",
        "      display_str_list: list of strings to display in box\n",
        "                        (each to be shown on its own line).\n",
        "      use_normalized_coordinates: If True (default), treat coordinates\n",
        "        ymin, xmin, ymax, xmax as relative to the image.  Otherwise treat\n",
        "        coordinates as absolute.\n",
        "    \"\"\"\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    im_width, im_height = image.size\n",
        "    if use_normalized_coordinates:\n",
        "        (left, right, top, bottom) = (\n",
        "            xmin * im_width,\n",
        "            xmax * im_width,\n",
        "            ymin * im_height,\n",
        "            ymax * im_height,\n",
        "        )\n",
        "    else:\n",
        "        (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n",
        "    draw.line(\n",
        "        [(left, top), (left, bottom), (right, bottom), (right, top), (left, top)],\n",
        "        width=thickness,\n",
        "        fill=color,\n",
        "    )\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"arial.ttf\", 24)\n",
        "    except OSError:\n",
        "        font = ImageFont.load_default()\n",
        "\n",
        "    # If the total height of the display strings added to the top of the bounding\n",
        "    # box exceeds the top of the image, stack the strings below the bounding box\n",
        "    # instead of above.\n",
        "    display_str_heights = [get_font_size(font, ds)[1] for ds in display_str_list]\n",
        "    # Each display_str has a top and bottom margin of 0.05x.\n",
        "    total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)\n",
        "\n",
        "    if top > total_display_str_height:\n",
        "        text_bottom = top\n",
        "    else:\n",
        "        text_bottom = bottom + total_display_str_height\n",
        "    # Reverse list and print from bottom to top.\n",
        "    for display_str in display_str_list[::-1]:\n",
        "        text_width, text_height = get_font_size(font, display_str)\n",
        "        margin = np.ceil(0.05 * text_height)\n",
        "        draw.rectangle(\n",
        "            [\n",
        "                (left, text_bottom - text_height - 2 * margin),\n",
        "                (left + text_width, text_bottom),\n",
        "            ],\n",
        "            fill=color,\n",
        "        )\n",
        "        draw.text(\n",
        "            (left + margin, text_bottom - text_height - margin),\n",
        "            display_str,\n",
        "            fill=\"black\",\n",
        "            font=font,\n",
        "        )\n",
        "        text_bottom -= text_height - 2 * margin\n",
        "\n",
        "\n",
        "def draw_mask_on_image_array(image, mask, color=\"red\", alpha=0.4):\n",
        "    \"\"\"Draws mask on an image.\n",
        "\n",
        "    Args:\n",
        "      image: uint8 numpy array with shape (img_height, img_height, 3)\n",
        "      mask: a uint8 numpy array of shape (img_height, img_height) with\n",
        "        values between either 0 or 1.\n",
        "      color: color to draw the keypoints with. Default is red.\n",
        "      alpha: transparency value between 0 and 1. (default: 0.4)\n",
        "\n",
        "    Raises:\n",
        "      ValueError: On incorrect data type for image or masks.\n",
        "    \"\"\"\n",
        "    if image.dtype != np.uint8:\n",
        "        raise ValueError(\"`image` not of type np.uint8\")\n",
        "    if mask.dtype != np.uint8:\n",
        "        raise ValueError(\"`mask` not of type np.uint8\")\n",
        "    if np.any(np.logical_and(mask != 1, mask != 0)):\n",
        "        raise ValueError(\"`mask` elements should be in [0, 1]\")\n",
        "    if image.shape[:2] != mask.shape:\n",
        "        raise ValueError(\n",
        "            \"The image has spatial dimensions %s but the mask has \"\n",
        "            \"dimensions %s\" % (image.shape[:2], mask.shape)\n",
        "        )\n",
        "    rgb = ImageColor.getrgb(color)\n",
        "    pil_image = Image.fromarray(image)\n",
        "\n",
        "    solid_color = np.expand_dims(np.ones_like(mask), axis=2) * np.reshape(\n",
        "        list(rgb), [1, 1, 3]\n",
        "    )\n",
        "    pil_solid_color = Image.fromarray(np.uint8(solid_color)).convert(\"RGBA\")\n",
        "    pil_mask = Image.fromarray(np.uint8(255.0 * alpha * mask)).convert(\"L\")\n",
        "    pil_image = Image.composite(pil_solid_color, pil_image, pil_mask)\n",
        "    np.copyto(image, np.array(pil_image.convert(\"RGB\")))\n",
        "\n",
        "\n",
        "STANDARD_COLORS = [\n",
        "    \"AliceBlue\",\n",
        "    \"Chartreuse\",\n",
        "    \"Aqua\",\n",
        "    \"Aquamarine\",\n",
        "    \"Azure\",\n",
        "    \"Beige\",\n",
        "    \"Bisque\",\n",
        "    \"BlanchedAlmond\",\n",
        "    \"BlueViolet\",\n",
        "    \"BurlyWood\",\n",
        "    \"CadetBlue\",\n",
        "    \"AntiqueWhite\",\n",
        "    \"Chocolate\",\n",
        "    \"Coral\",\n",
        "    \"CornflowerBlue\",\n",
        "    \"Cornsilk\",\n",
        "    \"Crimson\",\n",
        "    \"Cyan\",\n",
        "    \"DarkCyan\",\n",
        "    \"DarkGoldenRod\",\n",
        "    \"DarkGrey\",\n",
        "    \"DarkKhaki\",\n",
        "    \"DarkOrange\",\n",
        "    \"DarkOrchid\",\n",
        "    \"DarkSalmon\",\n",
        "    \"DarkSeaGreen\",\n",
        "    \"DarkTurquoise\",\n",
        "    \"DarkViolet\",\n",
        "    \"DeepPink\",\n",
        "    \"DeepSkyBlue\",\n",
        "    \"DodgerBlue\",\n",
        "    \"FireBrick\",\n",
        "    \"FloralWhite\",\n",
        "    \"ForestGreen\",\n",
        "    \"Fuchsia\",\n",
        "    \"Gainsboro\",\n",
        "    \"GhostWhite\",\n",
        "    \"Gold\",\n",
        "    \"GoldenRod\",\n",
        "    \"Salmon\",\n",
        "    \"Tan\",\n",
        "    \"HoneyDew\",\n",
        "    \"HotPink\",\n",
        "    \"IndianRed\",\n",
        "    \"Ivory\",\n",
        "    \"Khaki\",\n",
        "    \"Lavender\",\n",
        "    \"LavenderBlush\",\n",
        "    \"LawnGreen\",\n",
        "    \"LemonChiffon\",\n",
        "    \"LightBlue\",\n",
        "    \"LightCoral\",\n",
        "    \"LightCyan\",\n",
        "    \"LightGoldenRodYellow\",\n",
        "    \"LightGray\",\n",
        "    \"LightGrey\",\n",
        "    \"LightGreen\",\n",
        "    \"LightPink\",\n",
        "    \"LightSalmon\",\n",
        "    \"LightSeaGreen\",\n",
        "    \"LightSkyBlue\",\n",
        "    \"LightSlateGray\",\n",
        "    \"LightSlateGrey\",\n",
        "    \"LightSteelBlue\",\n",
        "    \"LightYellow\",\n",
        "    \"Lime\",\n",
        "    \"LimeGreen\",\n",
        "    \"Linen\",\n",
        "    \"Magenta\",\n",
        "    \"MediumAquaMarine\",\n",
        "    \"MediumOrchid\",\n",
        "    \"MediumPurple\",\n",
        "    \"MediumSeaGreen\",\n",
        "    \"MediumSlateBlue\",\n",
        "    \"MediumSpringGreen\",\n",
        "    \"MediumTurquoise\",\n",
        "    \"MediumVioletRed\",\n",
        "    \"MintCream\",\n",
        "    \"MistyRose\",\n",
        "    \"Moccasin\",\n",
        "    \"NavajoWhite\",\n",
        "    \"OldLace\",\n",
        "    \"Olive\",\n",
        "    \"OliveDrab\",\n",
        "    \"Orange\",\n",
        "    \"OrangeRed\",\n",
        "    \"Orchid\",\n",
        "    \"PaleGoldenRod\",\n",
        "    \"PaleGreen\",\n",
        "    \"PaleTurquoise\",\n",
        "    \"PaleVioletRed\",\n",
        "    \"PapayaWhip\",\n",
        "    \"PeachPuff\",\n",
        "    \"Peru\",\n",
        "    \"Pink\",\n",
        "    \"Plum\",\n",
        "    \"PowderBlue\",\n",
        "    \"Purple\",\n",
        "    \"Red\",\n",
        "    \"RosyBrown\",\n",
        "    \"RoyalBlue\",\n",
        "    \"SaddleBrown\",\n",
        "    \"Green\",\n",
        "    \"SandyBrown\",\n",
        "    \"SeaGreen\",\n",
        "    \"SeaShell\",\n",
        "    \"Sienna\",\n",
        "    \"Silver\",\n",
        "    \"SkyBlue\",\n",
        "    \"SlateBlue\",\n",
        "    \"SlateGray\",\n",
        "    \"SlateGrey\",\n",
        "    \"Snow\",\n",
        "    \"SpringGreen\",\n",
        "    \"SteelBlue\",\n",
        "    \"GreenYellow\",\n",
        "    \"Teal\",\n",
        "    \"Thistle\",\n",
        "    \"Tomato\",\n",
        "    \"Turquoise\",\n",
        "    \"Violet\",\n",
        "    \"Wheat\",\n",
        "    \"White\",\n",
        "    \"WhiteSmoke\",\n",
        "    \"Yellow\",\n",
        "    \"YellowGreen\",\n",
        "]\n",
        "\n",
        "\n",
        "def visualize_boxes_and_labels_on_image_array(\n",
        "    image,\n",
        "    boxes,\n",
        "    classes,\n",
        "    scores,\n",
        "    category_index,\n",
        "    instance_masks=None,\n",
        "    use_normalized_coordinates=False,\n",
        "    max_boxes_to_draw=20,\n",
        "    min_score_thresh=0.5,\n",
        "    agnostic_mode=False,\n",
        "    line_thickness=4,\n",
        "    groundtruth_box_visualization_color=\"black\",\n",
        "    skip_scores=False,\n",
        "    skip_labels=False,\n",
        "):\n",
        "    \"\"\"Overlay labeled boxes on an image with formatted scores and label names.\n",
        "\n",
        "    This function groups boxes that correspond to the same location\n",
        "    and creates a display string for each detection and overlays these\n",
        "    on the image. Note that this function modifies the image in place, and returns\n",
        "    that same image.\n",
        "\n",
        "    Args:\n",
        "      image: uint8 numpy array with shape (img_height, img_width, 3)\n",
        "      boxes: a numpy array of shape [N, 4]\n",
        "      classes: a numpy array of shape [N]. Note that class indices are 1-based,\n",
        "        and match the keys in the label map.\n",
        "      scores: a numpy array of shape [N] or None.  If scores=None, then\n",
        "        this function assumes that the boxes to be plotted are groundtruth\n",
        "        boxes and plot all boxes as black with no classes or scores.\n",
        "      category_index: a dict containing category dictionaries (each holding\n",
        "        category index `id` and category name `name`) keyed by category indices.\n",
        "      instance_masks: a numpy array of shape [N, image_height, image_width] with\n",
        "        values ranging between 0 and 1, can be None.\n",
        "      instance_boundaries: a numpy array of shape [N, image_height, image_width]\n",
        "        with values ranging between 0 and 1, can be None.\n",
        "      keypoints: a numpy array of shape [N, num_keypoints, 2], can\n",
        "        be None\n",
        "      use_normalized_coordinates: whether boxes is to be interpreted as\n",
        "        normalized coordinates or not.\n",
        "      max_boxes_to_draw: maximum number of boxes to visualize.  If None, draw\n",
        "        all boxes.\n",
        "      min_score_thresh: minimum score threshold for a box to be visualized\n",
        "      agnostic_mode: boolean (default: False) controlling whether to evaluate in\n",
        "        class-agnostic mode or not.  This mode will display scores but ignore\n",
        "        classes.\n",
        "      line_thickness: integer (default: 4) controlling line width of the boxes.\n",
        "      groundtruth_box_visualization_color: box color for visualizing groundtruth\n",
        "        boxes\n",
        "      skip_scores: whether to skip score when drawing a single detection\n",
        "      skip_labels: whether to skip label when drawing a single detection\n",
        "\n",
        "    Returns:\n",
        "      uint8 numpy array with shape (img_height, img_width, 3) with overlaid boxes.\n",
        "    \"\"\"\n",
        "    # Create a display string (and color) for every box location, group any boxes\n",
        "    # that correspond to the same location.\n",
        "    box_to_display_str_map = collections.defaultdict(list)\n",
        "    box_to_color_map = collections.defaultdict(str)\n",
        "    box_to_instance_masks_map = {}\n",
        "    if not max_boxes_to_draw:\n",
        "        max_boxes_to_draw = boxes.shape[0]\n",
        "    for i in range(min(max_boxes_to_draw, boxes.shape[0])):\n",
        "        if scores is None or scores[i] > min_score_thresh:\n",
        "            box = tuple(boxes[i].tolist())\n",
        "            if instance_masks is not None:\n",
        "                box_to_instance_masks_map[box] = instance_masks[i]\n",
        "            if scores is None:\n",
        "                box_to_color_map[box] = groundtruth_box_visualization_color\n",
        "            else:\n",
        "                display_str = \"\"\n",
        "                if not skip_labels:\n",
        "                    if not agnostic_mode:\n",
        "                        if classes[i] in category_index.keys():\n",
        "                            class_name = category_index[classes[i]][\"name\"]\n",
        "                        else:\n",
        "                            class_name = \"N/A\"\n",
        "                        display_str = str(class_name)\n",
        "                if not skip_scores:\n",
        "                    if not display_str:\n",
        "                        display_str = \"{}%\".format(int(100 * scores[i]))\n",
        "                    else:\n",
        "                        display_str = \"{}: {}%\".format(\n",
        "                            display_str, int(100 * scores[i])\n",
        "                        )\n",
        "                box_to_display_str_map[box].append(display_str)\n",
        "                if agnostic_mode:\n",
        "                    box_to_color_map[box] = \"DarkOrange\"\n",
        "                else:\n",
        "                    box_to_color_map[box] = STANDARD_COLORS[\n",
        "                        classes[i] % len(STANDARD_COLORS)\n",
        "                    ]\n",
        "\n",
        "    # Draw all boxes onto image.\n",
        "    for box, color in box_to_color_map.items():\n",
        "        # Using Detectron2 style output.\n",
        "        xmin, ymin, xmax, ymax = box\n",
        "        if instance_masks is not None:\n",
        "            draw_mask_on_image_array(image, box_to_instance_masks_map[box], color=color)\n",
        "        draw_bounding_box_on_image_array(\n",
        "            image,\n",
        "            ymin,\n",
        "            xmin,\n",
        "            ymax,\n",
        "            xmax,\n",
        "            color=color,\n",
        "            thickness=line_thickness,\n",
        "            display_str_list=box_to_display_str_map[box],\n",
        "            use_normalized_coordinates=use_normalized_coordinates,\n",
        "        )\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E5rmCWMiSFG"
      },
      "source": [
        "## Optional - Download a sample dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RcoOH0yEpJQ7"
      },
      "outputs": [],
      "source": [
        "# @title Download the balloon dataset\n",
        "\n",
        "# @markdown This step is only necessary if you don't have your own dataset and wish to use the Balloon dataset for demonstration purposes.\n",
        "\n",
        "# @markdown In case you are using your own dataset, kindly convert it to [COCO format](https://opencv.org/introduction-to-the-coco-dataset/).\n",
        "\n",
        "!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.1/balloon_dataset.zip\n",
        "!unzip balloon_dataset.zip > /dev/null\n",
        "\n",
        "local_balloon_data_directory = \"balloon\"\n",
        "BALLOON_DATA_GCS_PATH = os.path.join(BUCKET_URI, \"balloon_dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ah0lXyvXpJQ8"
      },
      "outputs": [],
      "source": [
        "# @title Convert Balloon data to COCO format\n",
        "\n",
        "\n",
        "def save_coco_format_json(img_dir, output_coco_format_json_filename):\n",
        "    # Load original balloon data json file\n",
        "    json_file = os.path.join(img_dir, \"via_region_data.json\")\n",
        "    with open(json_file) as f:\n",
        "        imgs_anns = json.load(f)\n",
        "\n",
        "    output_coco_format_dict = {}\n",
        "    # We only have one class: balloon.\n",
        "    output_coco_format_dict[\"categories\"] = [{\"id\": 0, \"name\": \"balloon\"}]\n",
        "    output_coco_format_dict[\"images\"] = []\n",
        "    output_coco_format_dict[\"annotations\"] = []\n",
        "    annotation_idx = 0\n",
        "    for image_idx, v in enumerate(imgs_anns.values()):\n",
        "        filename = os.path.join(img_dir, v[\"filename\"])\n",
        "        height, width = cv2.imread(filename).shape[:2]\n",
        "        image_item = {\n",
        "            \"id\": image_idx,\n",
        "            \"width\": width,\n",
        "            \"height\": height,\n",
        "            \"file_name\": v[\"filename\"],\n",
        "        }\n",
        "        output_coco_format_dict[\"images\"].append(image_item)\n",
        "\n",
        "        # Process all regions in this image.\n",
        "        annos = v[\"regions\"]\n",
        "        for _, anno in annos.items():\n",
        "            assert not anno[\"region_attributes\"]\n",
        "            anno = anno[\"shape_attributes\"]\n",
        "            px = anno[\"all_points_x\"]\n",
        "            py = anno[\"all_points_y\"]\n",
        "            poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
        "            poly = [p for x in poly for p in x]\n",
        "\n",
        "            annotation_idx += 1\n",
        "            annotation_item = {\n",
        "                \"id\": annotation_idx,\n",
        "                \"image_id\": image_idx,\n",
        "                # x, y, width, height\n",
        "                \"bbox\": [\n",
        "                    int(np.min(px)),\n",
        "                    int(np.min(py)),\n",
        "                    int(np.max(px) - np.min(px)),\n",
        "                    int(np.max(py) - np.min(py)),\n",
        "                ],\n",
        "                \"iscrowd\": 0,\n",
        "                # Only have one category.\n",
        "                \"category_id\": 0,\n",
        "                \"segmentation\": [poly],\n",
        "            }\n",
        "            RLEs = mask_util.frPyObjects([poly], width, height)\n",
        "            RLE = mask_util.merge(RLEs)\n",
        "            annotation_item[\"area\"] = float(mask_util.area(RLE))\n",
        "            output_coco_format_dict[\"annotations\"].append(annotation_item)\n",
        "\n",
        "    # Save output file.\n",
        "    json_file = os.path.join(img_dir, output_coco_format_json_filename)\n",
        "    with open(json_file, \"w\") as f:\n",
        "        json.dump(output_coco_format_dict, f)\n",
        "\n",
        "\n",
        "save_coco_format_json(\n",
        "    os.path.join(local_balloon_data_directory, \"train\"),\n",
        "    \"balloon_train_coco_format.json\",\n",
        ")\n",
        "save_coco_format_json(\n",
        "    os.path.join(local_balloon_data_directory, \"val\"), \"balloon_val_coco_format.json\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0OykIZen9gCC"
      },
      "outputs": [],
      "source": [
        "# @title Upload the Balloon data Cloud Storage.\n",
        "\n",
        "\n",
        "def get_bucket_and_blob_name(filepath):\n",
        "    # The gcs path is of the form gs://<bucket-name>/<blob-name>\n",
        "    gs_suffix = filepath.split(\"gs://\", 1)[1]\n",
        "    return tuple(gs_suffix.split(\"/\", 1))\n",
        "\n",
        "\n",
        "def upload_local_dir_to_gcs(local_dir_path, gcs_dir_path):\n",
        "    \"\"\"Uploads files in a local directory to a GCS directory.\"\"\"\n",
        "    ! gcloud storage cp -R $local_dir_path $gcs_dir_path\n",
        "\n",
        "\n",
        "upload_local_dir_to_gcs(\n",
        "    os.path.join(local_balloon_data_directory, \"train\"),\n",
        "    os.path.join(BALLOON_DATA_GCS_PATH, \"train\"),\n",
        ")\n",
        "upload_local_dir_to_gcs(\n",
        "    os.path.join(local_balloon_data_directory, \"val\"),\n",
        "    os.path.join(BALLOON_DATA_GCS_PATH, \"val\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBv28PhtifaM"
      },
      "source": [
        "## Finetune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aec22792ee84"
      },
      "outputs": [],
      "source": [
        "# @title Finetune with Detectron2\n",
        "# @markdown You will use the Vertex AI SDK to create and run the training job with the model-garden detectron2 training docker. You can choose one of the Faster R-CNN, RetinaNet, or Mask R-CNN models to finetune by uncommenting the corresponding code sections below. The training uses one V100 GPU and runs for around 3 mins once the training job begins.\n",
        "TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "JOB_NAME = \"detectron2_balloon_\" + TIMESTAMP\n",
        "\n",
        "container_uri = TRAIN_DOCKER_URI\n",
        "staging_bucket = os.path.join(BUCKET_URI, \"training/temporal\")\n",
        "TRAINING_ACCELERATOR_TYPE = \"NVIDIA_TESLA_V100\"\n",
        "TRAINING_MACHINE_TYPE = \"n1-standard-4\"\n",
        "TRAINING_AACCELERATOR_COUNT = 1\n",
        "\n",
        "# Dataset and output directory related parameters.\n",
        "train_dataset_name = \"balloon_train\"  # @param {type:\"string\"}\n",
        "train_coco_json_file = os.path.join(\n",
        "    BALLOON_DATA_GCS_PATH, \"train/balloon_train_coco_format.json\"\n",
        ")\n",
        "train_coco_json_file = gcs_fuse_path(train_coco_json_file)\n",
        "train_image_root = os.path.join(BALLOON_DATA_GCS_PATH, \"train\")\n",
        "train_image_root = gcs_fuse_path(train_image_root)\n",
        "val_dataset_name = \"balloon_val\"  # @param {type:\"string\"}\n",
        "val_coco_json_file = os.path.join(\n",
        "    BALLOON_DATA_GCS_PATH, \"val/balloon_val_coco_format.json\"\n",
        ")\n",
        "val_coco_json_file = gcs_fuse_path(val_coco_json_file)\n",
        "val_image_root = os.path.join(BALLOON_DATA_GCS_PATH, \"val\")\n",
        "val_image_root = gcs_fuse_path(val_image_root)\n",
        "output_dir = os.path.join(BUCKET_URI, JOB_NAME)\n",
        "\n",
        "#################################################\n",
        "# Model and dataset related parameters for Mask R-CNN.\n",
        "config_file = \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"\n",
        "config_file = gcs_fuse_path(config_file)\n",
        "remainder_args_list = []\n",
        "remainder_args_list += [\"DATASETS.TRAIN\"] + [\n",
        "    '(\"{train_dataset_name}\",)'.format(train_dataset_name=train_dataset_name)\n",
        "]\n",
        "remainder_args_list += [\"DATASETS.TEST\"] + [\n",
        "    '(\"{val_dataset_name}\",)'.format(val_dataset_name=val_dataset_name)\n",
        "]\n",
        "remainder_args_list += [\"DATALOADER.NUM_WORKERS\"] + [\"2\"]\n",
        "remainder_args_list += [\"SOLVER.IMS_PER_BATCH\"] + [\"2\"]\n",
        "remainder_args_list += [\"SOLVER.MAX_ITER\"] + [\"300\"]\n",
        "remainder_args_list += [\"SOLVER.STEPS\"] + [\"[]\"]\n",
        "remainder_args_list += [\"MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE\"] + [\"128\"]\n",
        "remainder_args_list += [\"MODEL.ROI_HEADS.NUM_CLASSES\"] + [\"1\"]\n",
        "publisher_model_id = \"mask-r-cnn\"\n",
        "#################################################\n",
        "\n",
        "# #################################################\n",
        "# # Model and dataset related parameters for RetinaNet.\n",
        "# config_file='COCO-Detection/retinanet_R_50_FPN_3x.yaml'\n",
        "# config_file = gcs_fuse_path(config_file)\n",
        "# remainder_args_list = []\n",
        "# remainder_args_list += ['DATASETS.TRAIN'] + ['(\"{train_dataset_name}\",)'.format(train_dataset_name=train_dataset_name)]\n",
        "# remainder_args_list += ['DATASETS.TEST'] + ['(\"{val_dataset_name}\",)'.format(val_dataset_name=val_dataset_name)]\n",
        "# remainder_args_list += ['DATALOADER.NUM_WORKERS'] + ['2']\n",
        "# remainder_args_list += ['SOLVER.IMS_PER_BATCH'] + ['2']\n",
        "# remainder_args_list += ['SOLVER.MAX_ITER'] + ['300']\n",
        "# remainder_args_list += ['SOLVER.STEPS'] + ['[]']\n",
        "# remainder_args_list += ['MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE'] + ['128']\n",
        "# remainder_args_list += ['MODEL.ROI_HEADS.NUM_CLASSES'] + ['1']\n",
        "# remainder_args_list += ['MODEL.RETINANET.NUM_CLASSES'] + ['1']\n",
        "# publisher_model_id = \"retinanet\"\n",
        "# #################################################\n",
        "\n",
        "# #################################################\n",
        "# # Model and dataset related parameters for Faster R-CNN.\n",
        "# config_file='COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml'\n",
        "# config_file = gcs_fuse_path(config_file)\n",
        "# remainder_args_list = []\n",
        "# remainder_args_list += ['DATASETS.TRAIN'] + ['(\"{train_dataset_name}\",)'.format(train_dataset_name=train_dataset_name)]\n",
        "# remainder_args_list += ['DATASETS.TEST'] + ['(\"{val_dataset_name}\",)'.format(val_dataset_name=val_dataset_name)]\n",
        "# remainder_args_list += ['DATALOADER.NUM_WORKERS'] + ['2']\n",
        "# remainder_args_list += ['SOLVER.IMS_PER_BATCH'] + ['2']\n",
        "# remainder_args_list += ['SOLVER.MAX_ITER'] + ['300']\n",
        "# remainder_args_list += ['SOLVER.STEPS'] + ['[]']\n",
        "# remainder_args_list += ['MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE'] + ['128']\n",
        "# remainder_args_list += ['MODEL.ROI_HEADS.NUM_CLASSES'] + ['1']\n",
        "# publisher_model_id = \"faster-r-cnn'\"\n",
        "# #################################################\n",
        "\n",
        "# Create argument list for docker.\n",
        "# NOTE: Config file flag name has hyphen instead\n",
        "# of underscore: 'config-file'.\n",
        "lr = 0.00025\n",
        "docker_args_list = [\n",
        "    \"--train_dataset_name\",\n",
        "    f\"{train_dataset_name}\",\n",
        "    \"--train_coco_json_file\",\n",
        "    f\"{train_coco_json_file}\",\n",
        "    \"--train_image_root\",\n",
        "    f\"{train_image_root}\",\n",
        "    \"--val_dataset_name\",\n",
        "    f\"{val_dataset_name}\",\n",
        "    \"--val_coco_json_file\",\n",
        "    f\"{val_coco_json_file}\",\n",
        "    \"--val_image_root\",\n",
        "    f\"{val_image_root}\",\n",
        "    \"--lr\",\n",
        "    f\"{lr}\",\n",
        "    \"--num-gpus\",\n",
        "    f\"{TRAINING_AACCELERATOR_COUNT}\",\n",
        "    \"--output_dir\",\n",
        "    f\"{gcs_fuse_path(output_dir)}\",\n",
        "    \"--config-file\",\n",
        "    f\"{config_file}\",\n",
        "]\n",
        "docker_args_list += remainder_args_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2ELphfgj1f3Q"
      },
      "outputs": [],
      "source": [
        "# @title Create and run training job.\n",
        "# Click on the generated link in the output under \"View backing custom job:\" to see your run in the Cloud Console.\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=staging_bucket)\n",
        "training_job = aiplatform.CustomContainerTrainingJob(\n",
        "    display_name=JOB_NAME,\n",
        "    container_uri=container_uri,\n",
        ")\n",
        "\n",
        "common_util.check_quota(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    accelerator_type=TRAINING_ACCELERATOR_TYPE,\n",
        "    accelerator_count=TRAINING_AACCELERATOR_COUNT,\n",
        "    is_for_training=True,\n",
        ")\n",
        "\n",
        "LABEL = \"detectron2-training\"\n",
        "\n",
        "training_job.run(\n",
        "    args=docker_args_list,\n",
        "    base_output_dir=f\"{output_dir}\",\n",
        "    replica_count=1,\n",
        "    machine_type=TRAINING_MACHINE_TYPE,\n",
        "    accelerator_type=TRAINING_ACCELERATOR_TYPE,\n",
        "    accelerator_count=TRAINING_AACCELERATOR_COUNT,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rXvIgegikCd"
      },
      "source": [
        "## Deploy model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3R_3DgCRD3U-"
      },
      "outputs": [],
      "source": [
        "# @title Upload models to model registry\n",
        "\n",
        "# @markdown This section uploads the model to Model Registry and deploys it on an Endpoint resource. The model deployment step will take ~15 minutes to complete. You need to set the model path below from the training output Cloud Storage directory.\n",
        "\n",
        "PRETRAINED_MODEL_PTH_FILE = os.path.join(output_dir, \"model_final.pth\")\n",
        "PRETRAINED_MODEL_CFG_YAML_FILE = os.path.join(output_dir, \"config.yaml\")\n",
        "TEST_THRESHOLD = 0.7\n",
        "PREDICTION_CONTAINER_URI = SERVE_DOCKER_URI\n",
        "PREDICTION_DISPLAY_NAME = \"upload_detectron2_\" + datetime.datetime.now().strftime(\n",
        "    \"%Y%m%d_%H%M%S\"\n",
        ")\n",
        "\n",
        "LABEL = \"detectron2-prediction\"\n",
        "endpoints[LABEL] = deploy_model(\n",
        "    project=PROJECT_ID,\n",
        "    location=REGION,\n",
        "    display_name=PREDICTION_DISPLAY_NAME,\n",
        "    serving_container_image_uri=PREDICTION_CONTAINER_URI,\n",
        "    model_pth_file=PRETRAINED_MODEL_PTH_FILE,\n",
        "    model_cfg_yaml_file=PRETRAINED_MODEL_CFG_YAML_FILE,\n",
        "    publisher_model_id=publisher_model_id,\n",
        "    test_threshold=TEST_THRESHOLD,\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        ")\n",
        "\n",
        "print(\"The uploaded model name is: \", PREDICTION_DISPLAY_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vbIW9me1F2RY"
      },
      "outputs": [],
      "source": [
        "# @title Run predictions\n",
        "\n",
        "endpoint_id = endpoints[LABEL].name\n",
        "\n",
        "local_test_filepath = os.path.join(\n",
        "    local_balloon_data_directory, \"val/410488422_5f8991f26e_b.jpg\"\n",
        ")\n",
        "instances = get_prediction_instances(local_test_filepath)\n",
        "\n",
        "\n",
        "response = endpoints[LABEL].predict(\n",
        "    instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
        ")\n",
        "\n",
        "prediction = json.loads(response.predictions[0])\n",
        "print(prediction)\n",
        "\n",
        "print(\"Predict the test image: \", local_test_filepath)\n",
        "\n",
        "# Draw boxes and masks.\n",
        "img = load_img(local_test_filepath)\n",
        "\n",
        "boxes = prediction[\"boxes\"]\n",
        "classes = prediction[\"classes\"]\n",
        "scores = prediction[\"scores\"]\n",
        "if prediction[\"masks_rle\"]:\n",
        "    masks_numpy = decode_rle_masks(prediction[\"masks_rle\"])\n",
        "else:\n",
        "    masks_numpy = None\n",
        "img.save(\"./sample.jpg\")\n",
        "output_image_array = visualize_boxes_and_labels_on_image_array(\n",
        "    image=np.array(img),\n",
        "    boxes=np.array(boxes),\n",
        "    classes=np.array(classes),\n",
        "    scores=np.array(scores),\n",
        "    category_index={0: {\"name\": \"balloon\"}},\n",
        "    instance_masks=masks_numpy,\n",
        ")\n",
        "output_image = Image.fromarray(np.uint8(output_image_array))\n",
        "output_image.save(\"./sample_preds.jpg\")\n",
        "print('Prediction image saved to \"./sample_preds.jpg\" ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP9ZYHaeYCaf"
      },
      "source": [
        "## Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aP9ZYHaeYCas"
      },
      "outputs": [],
      "source": [
        "# @title Delete the resources\n",
        "# @markdown This section deletes the training job and the endpoint to recycle the resources and avoid unnecessary continuous charges that may incur.\n",
        "# @markdown Delete the bucket if you don't need it anymore.\n",
        "\n",
        "training_job.delete()\n",
        "\n",
        "# Delete endpoint.\n",
        "for endpoint in endpoints.values():\n",
        "    endpoint.delete(force=True)\n",
        "\n",
        "\n",
        "delete_bucket = False  # @param {type:\"boolean\"}\n",
        "if delete_bucket:\n",
        "    ! gsutil -m rm -r $BUCKET_NAME"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_pytorch_detectron2.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d9bbf86da5e"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Vertex AI Model Garden - Stable Diffusion V1.5\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_stable_diffusion.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_stable_diffusion.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_pytorch_stable_diffusion.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "Open in Vertex AI Workbench\n",
        "    </a>\n",
        "    (a Python-3 GPU notebook with preinstalled HuggingFace/transformer libraries is recommended)\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates running local inference for [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) on either [Colab](https://colab.research.google.com) or [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench). This notebook also demonstrates finetuning runwayml/stable-diffusion-v1-5 with [Dreambooth](https://huggingface.co/docs/diffusers/training/dreambooth), finetuning with [LoRA](https://huggingface.co/docs/diffusers/training/text2image#lora), and finetuning with [Dreambooth and LoRA](https://huggingface.co/docs/diffusers/training/dreambooth#finetuning-with-lora) and deploying it on Vertex AI for online prediction.\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Run local predictions for text-to-image and text-guided-image-to-image with serving dockers.\n",
        "- Full parameter finetune stable-diffusion-v1.5 model with [Dreambooth](https://huggingface.co/docs/diffusers/training/dreambooth).\n",
        "- Finetune the stable-diffusion-v1.5 model with [LoRA](https://huggingface.co/docs/diffusers/training/text2image#lora).\n",
        "- Finetune the stable-diffusion-v1.5 model with [Dreambooth + LoRA](https://huggingface.co/docs/diffusers/training/dreambooth#finetuning-with-lora).\n",
        "- Upload models to [Vertex AI Model Registry](https://cloud.google.com/vertex-ai/docs/model-registry/introduction).\n",
        "- Deploy models to a [Vertex AI Endpoint resource](https://cloud.google.com/vertex-ai/docs/predictions/using-private-endpoints).\n",
        "- Run online predictions for text-to-image and text-guided-image-to-image.\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "264c07757582"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "**NOTE**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioensNKM8ned"
      },
      "source": [
        "### Setup notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d73ffa0c0b83"
      },
      "source": [
        "#### Colab only\n",
        "Run the following commands for Colab and skip this section if you are using Workbench."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2707b02ef5df"
      },
      "outputs": [],
      "source": [
        "if \"google.colab\" in str(get_ipython()):\n",
        "    ! pip3 install --upgrade google-cloud-aiplatform\n",
        "    from google.colab import auth as google_auth\n",
        "\n",
        "    google_auth.authenticate_user()\n",
        "    ! pip3 install --upgrade pip\n",
        "    ! pip3 install torch==2.0.1+cu118\n",
        "    ! pip3 install transformers==4.27.1\n",
        "    ! pip3 install diffusers==0.20.1\n",
        "    ! pip3 install datasets==2.9.0\n",
        "    ! pip3 install accelerate==0.21.0\n",
        "    # Install gdown for downloading example training images.\n",
        "    ! pip3 install gdown\n",
        "    # Remove wrong cublas version.\n",
        "    ! pip3 uninstall nvidia_cublas_cu11 --yes\n",
        "\n",
        "    # Restart the notebook kernel after installs.\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb671e75ca7b"
      },
      "source": [
        "#### Workbench only\n",
        "1.   Follow [this link](https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_pytorch_stable_diffusion.ipynb) to deploy the notebook to a Vertex AI Workbench Instance.\n",
        "2.   Select `Create a new Notebook`.\n",
        "3.   Click `Advanced Options`.\n",
        "4.   In the **Environment** tab, select `Debian 10` for **Operating System** and select `Custom Container` for **Environment**.\n",
        "5.   Set `Docker container image` to `us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/transformers-notebook`.\n",
        "6.   Under **Machine configuration**, select 1 `T4` GPU and select `Install NVIDIA GPU driver automatically for me`.\n",
        "7.   Click `Create` to create the Vertex AI Workbench instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb7adab99e41"
      },
      "source": [
        "### Setup Google Cloud project\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "1. [Enable the Vertex AI API and Compute Engine API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component).\n",
        "\n",
        "1. [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs.\n",
        "\n",
        "1. [Create a service account](https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console) with `Vertex AI User` and `Storage Object Admin` roles for deploying fine tuned model to Vertex AI endpoint."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c460088b873"
      },
      "source": [
        "Fill following variables for experiments environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "855d6b96f291"
      },
      "outputs": [],
      "source": [
        "# Cloud project id.\n",
        "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# The region you want to launch jobs in.\n",
        "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "# The Cloud Storage bucket for storing experiments output. Fill it without the 'gs://' prefix.\n",
        "GCS_BUCKET = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# The service account for deploying fine tuned model.\n",
        "SERVICE_ACCOUNT = \"\"  # @param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e828eb320337"
      },
      "source": [
        "Initialize Vertex-AI API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12cd25839741"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=GCS_BUCKET)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cc825514deb"
      },
      "source": [
        "### Define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b42bd4fa2b2d"
      },
      "outputs": [],
      "source": [
        "# The pre-built training docker images. They contain training scripts and models.\n",
        "TRAIN_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-diffusers-train:latest\"\n",
        "PEFT_TRAIN_DOCKER_URI = (\n",
        "    \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-train\"\n",
        ")\n",
        "\n",
        "# The pre-built serving docker images. They contains serving scripts and models.\n",
        "SERVE_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-diffusers-serve-opt:20240223_1230_RC00\"\n",
        "PEFT_SERVE_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-serve:20231112_0948_RC00\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c250872074f"
      },
      "source": [
        "### Define common functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "354da31189dc"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import glob\n",
        "import os\n",
        "from datetime import datetime\n",
        "from io import BytesIO\n",
        "\n",
        "import requests\n",
        "from google.cloud import aiplatform, storage\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def create_job_name(prefix):\n",
        "    user = os.environ.get(\"USER\")\n",
        "    now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    job_name = f\"{prefix}-{user}-{now}\"\n",
        "    return job_name\n",
        "\n",
        "\n",
        "def download_image(url):\n",
        "    response = requests.get(url)\n",
        "    return Image.open(BytesIO(response.content))\n",
        "\n",
        "\n",
        "def image_to_base64(image, format=\"JPEG\"):\n",
        "    buffer = BytesIO()\n",
        "    image.save(buffer, format=format)\n",
        "    image_str = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
        "    return image_str\n",
        "\n",
        "\n",
        "def base64_to_image(image_str):\n",
        "    image = Image.open(BytesIO(base64.b64decode(image_str)))\n",
        "    return image\n",
        "\n",
        "\n",
        "def image_grid(imgs, rows=2, cols=2):\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i % cols * w, i // cols * h))\n",
        "    return grid\n",
        "\n",
        "\n",
        "def deploy_model(model_id, task):\n",
        "    model_name = model_id\n",
        "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-{task}-endpoint\")\n",
        "    serving_env = {\n",
        "        \"MODEL_ID\": model_id,\n",
        "        \"TASK\": task,\n",
        "    }\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=SERVE_DOCKER_URI,\n",
        "        serving_container_ports=[7080],\n",
        "        serving_container_predict_route=\"/predictions/diffusers_serving\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "        serving_container_environment_variables=serving_env,\n",
        "    )\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=\"g2-standard-8\",\n",
        "        accelerator_type=\"NVIDIA_L4\",\n",
        "        accelerator_count=1,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=SERVICE_ACCOUNT,\n",
        "    )\n",
        "    return model, endpoint\n",
        "\n",
        "\n",
        "def deploy_model_peft(\n",
        "    model_name,\n",
        "    model_id,\n",
        "    finetuned_lora_model_path,\n",
        "    task,\n",
        "    machine_type=\"g2-standard-8\",\n",
        "    accelerator_type=\"NVIDIA_L4\",\n",
        "):\n",
        "    \"\"\"Deploys trained models into Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
        "    serving_env = {\n",
        "        \"MODEL_ID\": model_id,\n",
        "        \"TASK\": task,\n",
        "    }\n",
        "    if finetuned_lora_model_path:\n",
        "        serving_env[\"FINETUNED_LORA_MODEL_PATH\"] = finetuned_lora_model_path\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=PEFT_SERVE_DOCKER_URI,\n",
        "        serving_container_ports=[7080],\n",
        "        serving_container_predict_route=\"/predictions/peft_serving\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "        serving_container_environment_variables=serving_env,\n",
        "    )\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=1,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=SERVICE_ACCOUNT,\n",
        "    )\n",
        "    return model, endpoint\n",
        "\n",
        "\n",
        "def get_bucket_and_blob_name(filepath):\n",
        "    # The gcs path is of the form gs://<bucket-name>/<blob-name>\n",
        "    gs_suffix = filepath.split(\"gs://\", 1)[1]\n",
        "    return tuple(gs_suffix.split(\"/\", 1))\n",
        "\n",
        "\n",
        "def upload_local_dir_to_gcs(local_dir_path, gcs_dir_path):\n",
        "    \"\"\"Uploads files in a local directory to a GCS directory.\"\"\"\n",
        "    client = storage.Client()\n",
        "    bucket_name = gcs_dir_path.split(\"/\")[2]\n",
        "    bucket = client.get_bucket(bucket_name)\n",
        "    for local_file in glob.glob(local_dir_path + \"/**\"):\n",
        "        if not os.path.isfile(local_file):\n",
        "            continue\n",
        "        filename = local_file[1 + len(local_dir_path) :]\n",
        "        gcs_file_path = os.path.join(gcs_dir_path, filename)\n",
        "        _, blob_name = get_bucket_and_blob_name(gcs_file_path)\n",
        "        blob = bucket.blob(blob_name)\n",
        "        blob.upload_from_filename(local_file)\n",
        "        print(\"Copied {} to {}.\".format(local_file, gcs_file_path))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8a42fa49305"
      },
      "source": [
        "## Run inferences locally"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d5ebc91c786"
      },
      "source": [
        "### Text-to-image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d39ed8c97cc5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from diffusers import DPMSolverMultistepScheduler, StableDiffusionPipeline\n",
        "from diffusers.models.attention_processor import AttnProcessor2_0\n",
        "\n",
        "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "\n",
        "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "# We enable the memory-efficient attention implementation in PyTorch 2.0 which\n",
        "# automatically enables several optimizations depending on the inputs and the GPU type.\n",
        "pipe.unet.set_attn_processor(AttnProcessor2_0())\n",
        "\n",
        "prompt = \"a photo of an astronaut riding a horse on mars\"\n",
        "\n",
        "results = pipe(prompt=prompt, guidance_scale=7.5, num_inference_steps=25)\n",
        "images = results.images\n",
        "nsfw_detects = results.nsfw_content_detected\n",
        "display(images[0])\n",
        "print(nsfw_detects[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aed5ed7b6f6"
      },
      "source": [
        "### Text-guided image-to-image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faadabb7728f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionImg2ImgPipeline\n",
        "\n",
        "device = \"cuda\"\n",
        "model_id_or_path = \"runwayml/stable-diffusion-v1-5\"\n",
        "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "    model_id_or_path, torch_dtype=torch.float16\n",
        ")\n",
        "pipe = pipe.to(device)\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n",
        "\n",
        "response = requests.get(url)\n",
        "init_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "init_image = init_image.resize((768, 512))\n",
        "\n",
        "prompt = \"A fantasy landscape, trending on artstation\"\n",
        "\n",
        "results = pipe(prompt=prompt, image=init_image, strength=0.75, guidance_scale=7.5)\n",
        "images = results.images\n",
        "nsfw_detects = results.nsfw_content_detected\n",
        "display(images[0])\n",
        "print(nsfw_detects[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e70e3519ff8b"
      },
      "source": [
        "## Finetune with Dreambooth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIIgB6Fw_6Qk"
      },
      "source": [
        "### Finetune\n",
        "\n",
        "This section uses [dreambooth](https://dreambooth.github.io/) to finetune the [stable-diffusion-v1.5](https://huggingface.co/runwayml/stable-diffusion-v1-5) model with [5 dog images](https://drive.google.com/drive/folders/1BO_dyz-p65qhBRRMRA4TbZ8qW4rB99JZ) to personalize the text-to-image model.\n",
        "\n",
        "It finetunes both text encoder and unet of the stable diffusion model up to 800 steps. The whole finetuning job takes 30 minutes to finish using 1 A100 GPU.\n",
        "\n",
        "The full model will be saved after the finetuning job finishes and it can be loaded by the [StableDiffusionPipeline](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/text2img) to run inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34048707df5c"
      },
      "outputs": [],
      "source": [
        "# Download example training images.\n",
        "!gdown --folder https://drive.google.com/drive/folders/1BO_dyz-p65qhBRRMRA4TbZ8qW4rB99JZ\n",
        "\n",
        "# Upload data to Cloud Storage bucket.\n",
        "upload_local_dir_to_gcs(\"dog\", f\"gs://{GCS_BUCKET}/dreambooth/dog\")\n",
        "upload_local_dir_to_gcs(\"dog\", f\"gs://{GCS_BUCKET}/dreambooth/dog_class\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "969cfeb79317"
      },
      "source": [
        "**NOTE**: If the upload step fails due to lacking of permission, you need to [grant the Storage Object Admin role](https://cloud.google.com/storage/docs/access-control/using-iam-permissions) for the Cloud account of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65467b361315"
      },
      "outputs": [],
      "source": [
        "# The pre-trained model to be loaded.\n",
        "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "\n",
        "# Input and output path.\n",
        "instance_dir = f\"/gcs/{GCS_BUCKET}/dreambooth/dog\"\n",
        "class_dir = f\"/gcs/{GCS_BUCKET}/dreambooth/dog_class\"\n",
        "output_dir = f\"/gcs/{GCS_BUCKET}/dreambooth/output\"\n",
        "\n",
        "# Worker pool spec.\n",
        "machine_type = \"a2-highgpu-1g\"\n",
        "num_nodes = 1\n",
        "gpu_type = \"NVIDIA_TESLA_A100\"\n",
        "num_gpus = 1\n",
        "\n",
        "# Setup training job.\n",
        "job_name = create_job_name(\"dreambooth-stable-diffusion\")\n",
        "job = aiplatform.CustomContainerTrainingJob(\n",
        "    display_name=job_name,\n",
        "    container_uri=TRAIN_DOCKER_URI,\n",
        ")\n",
        "\n",
        "# Pass training arguments and launch job.\n",
        "# See https://github.com/huggingface/diffusers/blob/v0.16.0/examples/dreambooth/train_dreambooth.py#L75\n",
        "# for a full list of training arguments.\n",
        "model = job.run(\n",
        "    args=[\n",
        "        \"dreambooth/train_dreambooth.py\",\n",
        "        f\"--pretrained_model_name_or_path={model_id}\",\n",
        "        \"--train_text_encoder\",\n",
        "        f\"--instance_data_dir={instance_dir}\",\n",
        "        f\"--class_data_dir={class_dir}\",\n",
        "        f\"--output_dir={output_dir}\",\n",
        "        \"--with_prior_preservation\",\n",
        "        \"--prior_loss_weight=1.0\",\n",
        "        \"--instance_prompt='a photo of sks dog'\",\n",
        "        \"--class_prompt='a photo of dog'\",\n",
        "        \"--resolution=512\",\n",
        "        \"--train_batch_size=1\",\n",
        "        \"--gradient_checkpointing\",\n",
        "        \"--learning_rate=2e-6\",\n",
        "        \"--lr_scheduler=constant\",\n",
        "        \"--lr_warmup_steps=0\",\n",
        "        \"--num_class_images=200\",\n",
        "        \"--max_train_steps=800\",\n",
        "    ],\n",
        "    replica_count=num_nodes,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=gpu_type,\n",
        "    accelerator_count=num_gpus,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf7f82732e61"
      },
      "source": [
        "### Upload and Deploy models\n",
        "\n",
        "This section uploads the model to Model Registry and deploys it on the Endpoint.\n",
        "\n",
        "The model deployment step will take ~15 minutes to complete."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd7b56421392"
      },
      "source": [
        "#### Text-to-image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d331b1ea337"
      },
      "source": [
        "Deploy the stable diffusion model for the text-to-image task.\n",
        "\n",
        "Once deployed, you can send a batch of text prompts to the endpoint to generated images.\n",
        "\n",
        "When deployed on one V100 GPU, the averaged inference time of a request is ~15 seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bf55e38815dc"
      },
      "outputs": [],
      "source": [
        "# Set the model_id to \"runwayml/stable-diffusion-v1-5\" to load the OSS pre-trained model.\n",
        "model_text_to_image, endpoint_text_to_image = deploy_model(\n",
        "    model_id=f\"gs://{GCS_BUCKET}/dreambooth/output\", task=\"text-to-image\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80b3fd2ace09"
      },
      "source": [
        "NOTE: The model weights will be downloaded after the deployment succeeds. Thus additional 5 minutes of waiting time is needed **after** the above model deployment step succeeds and before you run the next step below. Otherwise you might see a `ServiceUnavailable: 503 502:Bad Gateway` error when you send requests to the endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ab04da3ec9a"
      },
      "outputs": [],
      "source": [
        "instances = [\n",
        "    {\"prompt\": \"A picture of a sks dog in a house\"},\n",
        "    {\"prompt\": \"A picture of a sks dog catching a frisbee\"},\n",
        "    {\"prompt\": \"A picture of a sks dog in front of a computer\"},\n",
        "    {\"prompt\": \"A picture of a sks dog in a bucket\"},\n",
        "]\n",
        "response = endpoint_text_to_image.predict(instances=instances)\n",
        "images = [base64_to_image(image) for image in response.predictions]\n",
        "image_grid(images)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1e51f764a60"
      },
      "source": [
        "#### Text-guided image-to-image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa686a54047c"
      },
      "source": [
        "Deploy the stable diffusion model for the text-guided image-to-image task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65e32356fbd1"
      },
      "outputs": [],
      "source": [
        "# Set the model_id to a GCS path, like \"gs://GCS_BUCKET/dreambooth/output\", to load the dreambooth finetuned model above.\n",
        "model_image_to_image, endpoint_image_to_image = deploy_model(\n",
        "    model_id=\"runwayml/stable-diffusion-v1-5\", task=\"image-to-image\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsjYFLvNymc0"
      },
      "source": [
        "NOTE: The model weights will be downloaded after the deployment succeeds. Thus additional 5 minutes of waiting time is needed **after** the above model deployment step succeeds and before you run the next step below. Otherwise you might see a `ServiceUnavailable: 503 502:Bad Gateway` error when you send requests to the endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83a50fd4a1ed"
      },
      "outputs": [],
      "source": [
        "init_image = download_image(\n",
        "    \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n",
        ")\n",
        "display(init_image)\n",
        "instances = [\n",
        "    {\n",
        "        \"prompt\": \"A fantasy landscape, trending on artstation\",\n",
        "        \"image\": image_to_base64(init_image),\n",
        "    },\n",
        "]\n",
        "response = endpoint_image_to_image.predict(instances=instances)\n",
        "images = [base64_to_image(image) for image in response.predictions]\n",
        "display(images[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3-T96nh1Q8K"
      },
      "source": [
        "## Finetune and deploy with LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY-iQTub1UJg"
      },
      "source": [
        "### Finetune\n",
        "\n",
        "This section uses [LoRA](https://arxiv.org/abs/2106.09685) to finetune the [stable-diffusion-v1.5](https://huggingface.co/runwayml/stable-diffusion-v1-5) model with [lambdalabs/pokemon-blip-captions](https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions).\n",
        "\n",
        "Finetuning with LoRA\n",
        "\n",
        "The LoRA weights will be saved after the finetuning job finishes and it can be loaded by the [StableDiffusionPipeline](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/text2img) to run inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6Q5CMgX2py9"
      },
      "outputs": [],
      "source": [
        "# The pre-trained model to be loaded.\n",
        "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "DATASET_NAME = \"lambdalabs/pokemon-blip-captions\"\n",
        "\n",
        "# Output path.\n",
        "output_dir_lora = f\"/gcs/{GCS_BUCKET}/lora/output\"\n",
        "gs_output_dir_lora = f\"gs://{GCS_BUCKET}/lora/output\"\n",
        "\n",
        "# Worker pool spec.\n",
        "machine_type = \"n1-standard-8\"\n",
        "num_nodes = 1\n",
        "gpu_type = \"NVIDIA_TESLA_V100\"\n",
        "num_gpus = 1\n",
        "\n",
        "\n",
        "# Setup training job.\n",
        "job_name = create_job_name(\"lora-stable-diffusion\")\n",
        "job = aiplatform.CustomContainerTrainingJob(\n",
        "    display_name=job_name,\n",
        "    container_uri=PEFT_TRAIN_DOCKER_URI,\n",
        ")\n",
        "\n",
        "# Set task to \"text-to-image-lora\" to finetune with LoRA.\n",
        "# Pass training arguments and launch job.\n",
        "# See https://github.com/huggingface/diffusers/blob/87ae330056f6942817656c8f7146283e90cf986b/examples/text_to_image/train_text_to_image_lora.py#L84\n",
        "# for a full list of training arguments.\n",
        "model = job.run(\n",
        "    args=[\n",
        "        \"--task=text-to-image-lora\",\n",
        "        f\"--pretrained_model_name_or_path={model_id}\",\n",
        "        f\"--dataset_name={DATASET_NAME}\",\n",
        "        \"--caption_column=text\",\n",
        "        \"--resolution=512\",\n",
        "        \"--random_flip\",\n",
        "        \"--train_batch_size=1\",\n",
        "        \"--num_train_epochs=100\",\n",
        "        \"--checkpointing_steps=5000\",\n",
        "        \"--learning_rate=1e-04\",\n",
        "        \"--lr_scheduler=constant\",\n",
        "        \"--lr_warmup_steps=0\",\n",
        "        f\"--output_dir={output_dir_lora}\",\n",
        "        \"--validation_prompt='cute dragon creature'\",\n",
        "        \"--seed=42\",\n",
        "    ],\n",
        "    replica_count=num_nodes,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=gpu_type,\n",
        "    accelerator_count=num_gpus,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AairVI_a7yyr"
      },
      "source": [
        "### Deploy\n",
        "\n",
        "Deploy the stable diffusion model for the text-to-image task.\n",
        "\n",
        "Once deployed, you can send a batch of text prompts to the endpoint to generated images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e154LQnB8Pap"
      },
      "outputs": [],
      "source": [
        "model_with_peft, endpoint_with_peft = deploy_model_peft(\n",
        "    model_name=create_job_name(prefix=\"lora-stable-diffusion\"),\n",
        "    model_id=model_id,\n",
        "    finetuned_lora_model_path=gs_output_dir_lora,\n",
        "    task=\"text-to-image-lora\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1f019sH8kse"
      },
      "source": [
        "NOTE: The model weights will be downloaded after the deployment succeeds. Thus additional 5 minutes of waiting time is needed after the above model deployment step succeeds and before you run the next step below. Otherwise you might see a ServiceUnavailable: 503 502:Bad Gateway error when you send requests to the endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVPCsaBp8odc"
      },
      "outputs": [],
      "source": [
        "instances = [\n",
        "    {\"prompt\": \"A pokemon with green eyes and red legs.\"},\n",
        "    {\"prompt\": \"A pokemon with yellow tails and blue arms.\"},\n",
        "    {\"prompt\": \"A pokemon with three heads and green feet.\"},\n",
        "    {\"prompt\": \"A dragon pokemon with red and white stripes.\"},\n",
        "]\n",
        "response = endpoint_with_peft.predict(instances=instances)\n",
        "images = [base64_to_image(image) for image in response.predictions]\n",
        "image_grid(images)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jTetcepAZeO"
      },
      "source": [
        "## Finetune with Dreambooth and LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gQFXhVuAcc-"
      },
      "source": [
        "### Finetune\n",
        "\n",
        "This section uses [dreambooth](https://dreambooth.github.io/) and [LoRA](https://arxiv.org/abs/2106.09685) to finetune the [stable-diffusion-v1.5](https://huggingface.co/runwayml/stable-diffusion-v1-5) model with [5 dog images](https://drive.google.com/drive/folders/1BO_dyz-p65qhBRRMRA4TbZ8qW4rB99JZ) to personalize the text-to-image model.\n",
        "\n",
        "The full model will be saved after the finetuning job finishes and it can be loaded by the [StableDiffusionPipeline](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/text2img) to run inference.\n",
        "\n",
        "Note: When using LoRA we can use a much higher learning rate compared to vanilla dreambooth. Here we use `1e-4` instead of the usual `2e-6`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dh1JNKYDUuwZ"
      },
      "outputs": [],
      "source": [
        "# Download example training images.\n",
        "!gdown --folder https://drive.google.com/drive/folders/1BO_dyz-p65qhBRRMRA4TbZ8qW4rB99JZ\n",
        "\n",
        "# Upload data to Cloud Storage bucket.\n",
        "upload_local_dir_to_gcs(\"dog\", f\"gs://{GCS_BUCKET}/dreambooth-lora/dog\")\n",
        "upload_local_dir_to_gcs(\"dog\", f\"gs://{GCS_BUCKET}/dreambooth-lora/dog_class\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22zEZjQKU4R-"
      },
      "source": [
        "**NOTE**: If the upload step fails due to lacking of permission, you need to [grant the Storage Object Admin role](https://cloud.google.com/storage/docs/access-control/using-iam-permissions) for the Cloud account of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKUXH9b9Bvta"
      },
      "outputs": [],
      "source": [
        "# The pre-trained model to be loaded.\n",
        "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "\n",
        "# Input and output path.\n",
        "instance_dir = f\"/gcs/{GCS_BUCKET}/dreambooth-lora/dog\"\n",
        "output_dir_dreambooth_lora = f\"/gcs/{GCS_BUCKET}/dreambooth-lora/output\"\n",
        "gs_output_dir_dreambooth_lora = f\"gs://{GCS_BUCKET}/dreambooth-lora/output\"\n",
        "\n",
        "\n",
        "# Worker pool spec.\n",
        "machine_type = \"n1-highmem-8\"\n",
        "num_nodes = 1\n",
        "gpu_type = \"NVIDIA_TESLA_V100\"\n",
        "num_gpus = 1\n",
        "\n",
        "# Setup training job.\n",
        "job_name = create_job_name(\"dreambooth-lora-stable-diffusion\")\n",
        "job = aiplatform.CustomContainerTrainingJob(\n",
        "    display_name=job_name,\n",
        "    container_uri=PEFT_TRAIN_DOCKER_URI,\n",
        ")\n",
        "\n",
        "# Set task to \"text-to-image-dreambooth-lora\" to finetune using Dreambooth and\n",
        "# LoRA.\n",
        "# Pass training arguments and launch job.\n",
        "# See https://github.com/huggingface/diffusers/blob/87ae330056f6942817656c8f7146283e90cf986b/examples/dreambooth/train_dreambooth_lora.py#L142\n",
        "# for a full list of training arguments.\n",
        "model = job.run(\n",
        "    args=[\n",
        "        \"--task=text-to-image-dreambooth-lora\",\n",
        "        f\"--pretrained_model_name_or_path={model_id}\",\n",
        "        f\"--instance_data_dir={instance_dir}\",\n",
        "        f\"--output_dir={output_dir_dreambooth_lora}\",\n",
        "        \"--instance_prompt='a photo of sks dog'\",\n",
        "        \"--resolution=512\",\n",
        "        \"--train_batch_size=1\",\n",
        "        \"--gradient_accumulation_steps=1\",\n",
        "        \"--checkpointing_steps=100\",\n",
        "        \"--learning_rate=1e-4\",\n",
        "        \"--lr_scheduler=constant\",\n",
        "        \"--lr_warmup_steps=0\",\n",
        "        \"--max_train_steps=500\",\n",
        "        \"--validation_prompt='A photo of sks dog in a bucket'\",\n",
        "        \"--validation_epochs=50\",\n",
        "        \"--seed=0\",\n",
        "    ],\n",
        "    replica_count=num_nodes,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=gpu_type,\n",
        "    accelerator_count=num_gpus,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqrVgsVODmZb"
      },
      "source": [
        "### Deploy\n",
        "\n",
        "Deploy the stable diffusion model for the text-to-image task.\n",
        "\n",
        "Once deployed, you can send a batch of text prompts to the endpoint to generated images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ye3_N5bxDmZc"
      },
      "outputs": [],
      "source": [
        "model_with_peft_dreambooth, endpoint_with_peft_dreambooth = deploy_model_peft(\n",
        "    model_name=create_job_name(prefix=\"dreambooth-lora-stable-diffusion\"),\n",
        "    model_id=model_id,\n",
        "    finetuned_lora_model_path=gs_output_dir_dreambooth_lora,\n",
        "    task=\"text-to-image-lora\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xT1Bh_XDmZc"
      },
      "source": [
        "NOTE: The model weights will be downloaded after the deployment succeeds. Thus additional 5 minutes of waiting time is needed after the above model deployment step succeeds and before you run the next step below. Otherwise you might see a ServiceUnavailable: 503 502:Bad Gateway error when you send requests to the endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gH23yBtYDmZd"
      },
      "outputs": [],
      "source": [
        "instances = [\n",
        "    {\"prompt\": \"A picture of a sks dog on a chair\"},\n",
        "    {\"prompt\": \"A picture of a sks dog wearing a green hat\"},\n",
        "    {\"prompt\": \"A picture of a sks dog in front of a tree\"},\n",
        "    {\"prompt\": \"A picture of a sks dog in a bucket\"},\n",
        "]\n",
        "response = endpoint_with_peft_dreambooth.predict(instances=instances)\n",
        "images = [base64_to_image(image) for image in response.predictions]\n",
        "image_grid(images)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed3795d474b9"
      },
      "source": [
        "## Clean up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b53b883257b4"
      },
      "outputs": [],
      "source": [
        "# Undeploy model and delete endpoints.\n",
        "endpoint_text_to_image.delete(force=True)\n",
        "endpoint_image_to_image.delete(force=True)\n",
        "endpoint_with_peft.delete(force=True)\n",
        "endpoint_with_peft_dreambooth.delete(force=True)\n",
        "# Delete models.\n",
        "model_text_to_image.delete()\n",
        "model_image_to_image.delete()\n",
        "model_with_peft.delete()\n",
        "model_with_peft_dreambooth.delete()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_pytorch_stable_diffusion.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SgQ6t5bqZVlH"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Vertex AI Model Garden + Reasoning Engine - Build, Deploy and Test Agents using a Self-deployed Endpoint\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_pytorch_deployed_model_reasoning_engine.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_deployed_model_reasoning_engine.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates how to build, deploy and test three types of agents using [Reasoning Engine](https://cloud.google.com/vertex-ai/generative-ai/docs/reasoning-engine/overview) with self-deployed model in Vertex AI.\n",
        "\n",
        "[Reasoning Engine](https://cloud.google.com/vertex-ai/generative-ai/docs/reasoning-engine/overview) (LangChain on Vertex AI) is a managed service in Vertex AI that helps you build and deploy model-based agents. It gives you the flexibility to choose how much reasoning you want to delegate to the LLM and how much you want to handle with custom code.\n",
        "\n",
        "A previous [notebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_openai_api_llama3_1.ipynb) demonstrates how to use Llama 3.1 models as Model-as-a-service (MaaS) to build `chatbot` and `translator` agents.\n",
        "\n",
        "\n",
        "### Objective\n",
        "    \n",
        "- Integrate with Reasoning Engine: Use the Vertex AI SDK to build three simple agents with the deployed endpoint:\n",
        "    - A Chatbot Agent\n",
        "    - A Translator Agent\n",
        "    - An Agent that uses [an Exchange Rate Tool](https://cloud.google.com/vertex-ai/generative-ai/docs/reasoning-engine/develop#define-function)\n",
        "- Test your agent locally.\n",
        "- Deploy and test your agent on the Reasoning Engine.\n",
        "\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "264c07757582"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "language": "python",
      "metadata": {
        "cellView": "form",
        "id": "YXFGIp1l-qtT"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown 2. Create a bucket created for reasoning engine.\n",
        "\n",
        "BUCKET_NAME = \"\"  # @param {type:\"string\", placeholder: \"[your-bucket-name]\"}\n",
        "STAGING_BUCKET = f\"gs://{BUCKET_NAME}\"\n",
        "\n",
        "# @markdown 3. You can find the deployed model endpoint in the [Vertex AI console](https://console.cloud.google.com/vertex-ai/endpoints).\n",
        "DEPLOYED_MODEL_ENDPOINT = \"\"  # @param {type:\"string\", placeholder: \"[your-deployed-model-endpoint]\"}\n",
        "\n",
        "# Import the necessary packages\n",
        "\n",
        "# Upgrade Vertex AI SDK.\n",
        "! pip3 install --upgrade --quiet \\\n",
        "    \"google-cloud-aiplatform>=1.64.0\" \\\n",
        "    cloudpickle==3.0.0 \\\n",
        "    pydantic==2.7.4 \\\n",
        "    requests \\\n",
        "    langchain-openai\n",
        "! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
        "\n",
        "import os\n",
        "import requests\n",
        "from typing import Tuple\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "    \n",
        "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
        "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blbSoA-3gSyN"
      },
      "source": [
        "### Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KsF63jInoByL"
      },
      "outputs": [],
      "source": [
        "# @title Authenticate your notebook environment (Colab only)\n",
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SLq7whlDoByL"
      },
      "outputs": [],
      "source": [
        "# @title Initialize Vertex AI SDK for Python\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TFkSqK91ki8E"
      },
      "outputs": [],
      "source": [
        "# @title Import libraries\n",
        "\n",
        "# @markdown Import libraries to use in this tutorial.\n",
        "\n",
        "import google.auth\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from vertexai.preview import reasoning_engines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71WcCJ57gJZM"
      },
      "source": [
        "### Chat with `Reasoning Engine`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sgG-NyAvoByL"
      },
      "outputs": [],
      "source": [
        "# @title `Reasoning Engine` use self-deployed API endpoint with different configuration\n",
        "\n",
        "# @markdown To use the self-deployed API endpoint with Reasoning Engine capabilities, you need to request the access token and configure the langchain ChatOpenAI to point to the API endpoint.\n",
        "\n",
        "# @markdown In previous [notebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_openai_api_llama3_1.ipynb), we demonstrated how to `Ask Llama 3.1 using different model configuration`.\n",
        "\n",
        "# @markdown In this colab, we will show you how to use the `Reasoning Agent` to send a request to the self-deployed API endpoint with different model configuration.\n",
        "\n",
        "\n",
        "def model_builder(\n",
        "    *,\n",
        "    model_name: str,\n",
        "    model_kwargs=None,\n",
        "    project: str,  # Specified via vertexai.init\n",
        "    location: str,  # Specified via vertexai.init\n",
        "    **kwargs,\n",
        "):\n",
        "\n",
        "    # Note: the credential lives for 1 hour by default.\n",
        "    # After expiration, it must be refreshed.\n",
        "    creds, _ = google.auth.default(\n",
        "        scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n",
        "    )\n",
        "    auth_req = google.auth.transport.requests.Request()\n",
        "    creds.refresh(auth_req)\n",
        "\n",
        "    if model_kwargs is None:\n",
        "        model_kwargs = {}\n",
        "\n",
        "    return ChatOpenAI(\n",
        "        model=\"\",\n",
        "        base_url=DEPLOYED_MODEL_ENDPOINT,\n",
        "        api_key=creds.token,\n",
        "        **model_kwargs,\n",
        "    )\n",
        "\n",
        "\n",
        "# @markdown Use the following parameters to generate different answers:\n",
        "# @markdown *   `temperature` to control the randomness of the response\n",
        "# @markdown *   `top_p` to control the quality of the response\n",
        "\n",
        "temperature = 1.0  # @param {type:\"number\"}\n",
        "top_p = 1.0  # @param {type:\"number\"}\n",
        "\n",
        "agent = reasoning_engines.LangchainAgent(\n",
        "    model=\"\",  # Required.\n",
        "    model_builder=model_builder,  # Required.\n",
        "    model_kwargs={\n",
        "        \"temperature\": temperature,  # Optional.\n",
        "        \"top_p\": top_p,  # Optional.\n",
        "        \"extra_body\": {},\n",
        "    },\n",
        ")\n",
        "\n",
        "# @markdown Now we can test the model and agent behavior to ensure that it's working as expected before we deploy it:\n",
        "\n",
        "response = agent.query(input=\"Hello, how are you!\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rn-cHx7xoByL"
      },
      "outputs": [],
      "source": [
        "# @title Deploy your agent on Vertex AI\n",
        "\n",
        "# @markdown Now that you've specified a model, and reasoning for your agent and tested it out, you're ready to deploy your agent as a remote service in Vertex AI!\n",
        "\n",
        "remote_agent = reasoning_engines.ReasoningEngine.create(\n",
        "    agent,\n",
        "    requirements=[\n",
        "        \"google-cloud-aiplatform[langchain,reasoningengine]\",\n",
        "        \"cloudpickle==3.0.0\",\n",
        "        \"pydantic==2.7.4\",\n",
        "        \"requests\",\n",
        "        \"langchain-openai\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "response = remote_agent.query(input=\"Hello, how are you!\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MjwPLr0LoByL"
      },
      "outputs": [],
      "source": [
        "# @title Reusing your deployed agent from other applications or SDKs\n",
        "\n",
        "# @markdown The remotely deployed `Reasoning Engine` is now available for import and use. You can access it within your current notebook session, a different notebook, or a Python script.\n",
        "\n",
        "REASONING_ENGINE_RESOURCE_NAME = remote_agent.resource_name\n",
        "print(REASONING_ENGINE_RESOURCE_NAME)\n",
        "\n",
        "# Afterwards, you can use the below code:\n",
        "\n",
        "# from vertexai.preview import reasoning_engines`\n",
        "\n",
        "# remote_agent = reasoning_engines.ReasoningEngine(REASONING_ENGINE_RESOURCE_NAME)`\n",
        "# response = remote_agent.query(input=query)`\n",
        "\n",
        "# @markdown Alternatively, you can query your agent from other programming languages using any of the [available client libraries in Vertex AI](https://cloud.google.com/vertex-ai/docs/start/client-libraries), including C#, Java, Node.js, Python, Go, or REST API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ato0F6XFoByL"
      },
      "source": [
        "### Simple Translator Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bI3JqaeMoByL"
      },
      "outputs": [],
      "source": [
        "# @title Use Reasoning Engine to build a simple translator agent\n",
        "\n",
        "# @markdown In previous [notebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_openai_api_llama3_1.ipynb), we demonstrates how to use `LangChain Expression Language` (LCEL) to build a simple chain which translates some `text_to_translate` to the specified `target_language`.\n",
        "\n",
        "# @markdown In this colab, we will show you how to use the `Reasoning Agent` to build and deploy the agent.\n",
        "\n",
        "\n",
        "def lcel_builder(*, model, **kwargs):\n",
        "\n",
        "    template = \"\"\"Translate the following {text} to {target_language}:\"\"\"\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"text\", \"target_language\"], template=template\n",
        "    )\n",
        "\n",
        "    return prompt | model | StrOutputParser()\n",
        "\n",
        "\n",
        "agent = reasoning_engines.LangchainAgent(\n",
        "    model=\"\",\n",
        "    model_builder=model_builder,\n",
        "    runnable_builder=lcel_builder,\n",
        ")\n",
        "\n",
        "text_to_translate = \"\"  # @param {type:\"string\", placeholder:\"Hello, how are you!\"}\n",
        "target_language = \"\"  # @param {type:\"string\", placeholder:\"Italian\"}\n",
        "\n",
        "response = agent.query(\n",
        "    input={\"text\": text_to_translate, \"target_language\": target_language}\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XNF9slsSEHLz"
      },
      "outputs": [],
      "source": [
        "# @title Deploy your agent on Vertex AI\n",
        "\n",
        "# @markdown Now that you've specified a model, and reasoning for your agent and tested it out, you're ready to deploy your agent as a remote service in Vertex AI!\n",
        "\n",
        "remote_agent = reasoning_engines.ReasoningEngine.create(\n",
        "    agent,\n",
        "    requirements=[\n",
        "        \"google-cloud-aiplatform[langchain,reasoningengine]\",\n",
        "        \"cloudpickle==3.0.0\",\n",
        "        \"pydantic==2.7.4\",\n",
        "        \"requests\",\n",
        "        \"langchain-openai\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "response = remote_agent.query(\n",
        "    input={\"text\": text_to_translate, \"target_language\": target_language}\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tnbhCYwbERSx"
      },
      "outputs": [],
      "source": [
        "# @title Reusing your deployed agent from other applications or SDKs\n",
        "\n",
        "# @markdown The remotely deployed `Reasoning Engine` is now available for import and use. You can access it within your current notebook session, a different notebook, or a Python script.\n",
        "\n",
        "REASONING_ENGINE_RESOURCE_NAME = remote_agent.resource_name\n",
        "print(REASONING_ENGINE_RESOURCE_NAME)\n",
        "\n",
        "# Afterwards, you can use the below code:\n",
        "\n",
        "# from vertexai.preview import reasoning_engines`\n",
        "\n",
        "# remote_agent = reasoning_engines.ReasoningEngine(REASONING_ENGINE_RESOURCE_NAME)`\n",
        "# response = remote_agent.query(input=query)`\n",
        "\n",
        "# @markdown Alternatively, you can query your agent from other programming languages using any of the [available client libraries in Vertex AI](https://cloud.google.com/vertex-ai/docs/start/client-libraries), including C#, Java, Node.js, Python, Go, or REST API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lI86Eu7oByL"
      },
      "source": [
        "### Exchange Rate Tool\n",
        "\n",
        "[Function calling](https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/function-calling) lets developers create a description of a function in their code, then pass that description to a language model in a request. The response from the model includes the name of a function that matches the description and the arguments to call it with.\n",
        "\n",
        "In this example, we will use an Exchange Rate tool in the Reasoning Engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Dmv42OOOoByL"
      },
      "outputs": [],
      "source": [
        "# @title Agent that uses an Exchange Rate Tool\n",
        "\n",
        "# @markdown Tools and functions enable the generative model to interact with external systems, databases, document stores, and other APIs so that the model can get the most up-to-date information or take action with those systems.\n",
        "\n",
        "# @markdown In this example, you'll define a function called get_exchange_rate that uses the requests library to retrieve real-time currency exchange information from an API:\n",
        "\n",
        "\n",
        "def get_exchange_rate(\n",
        "    currency_from: str = \"USD\",\n",
        "    currency_to: str = \"EUR\",\n",
        "    currency_date: str = \"latest\",\n",
        "):\n",
        "    \"\"\"Retrieves the exchange rate between two currencies on a specified date.\n",
        "    Args:\n",
        "        currency_from: The source currency code.\n",
        "        currency_to: The target currency code.\n",
        "        currency_date: The date to retrieve the exchange rate.\n",
        "    Returns:\n",
        "        Exchange rate between two currencies on a specified date.\n",
        "    \"\"\"\n",
        "    response = requests.get(\n",
        "        f\"https://api.frankfurter.app/{currency_date}\",\n",
        "        params={\"from\": currency_from, \"to\": currency_to},\n",
        "    )\n",
        "    return response.json()\n",
        "\n",
        "\n",
        "get_exchange_rate(currency_from=\"USD\", currency_to=\"SEK\")\n",
        "\n",
        "\n",
        "agent = reasoning_engines.LangchainAgent(\n",
        "    model=\"\",  # Required.\n",
        "    model_builder=model_builder,  # Required.\n",
        "    tools=[get_exchange_rate],  # Optional.\n",
        "    agent_executor_kwargs={\n",
        "        \"return_intermediate_steps\": True,\n",
        "        \"stream_runnable\": False,\n",
        "    },  # Optional.\n",
        ")\n",
        "\n",
        "# @markdown Test the function with sample inputs to ensure that it's working as expected:\n",
        "response = agent.query(\n",
        "    input=\"What's the exchange rate from US dollars to Swedish currency at 2024-07-26?\"\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6y2G_bjbDam_"
      },
      "outputs": [],
      "source": [
        "# @title Deploy your agent on Vertex AI\n",
        "\n",
        "# @markdown Now that you've specified a model, and reasoning for your agent and tested it out, you're ready to deploy your agent as a remote service in Vertex AI!\n",
        "\n",
        "remote_agent = reasoning_engines.ReasoningEngine.create(\n",
        "    agent,\n",
        "    requirements=[\n",
        "        \"google-cloud-aiplatform[langchain,reasoningengine]\",\n",
        "        \"cloudpickle==3.0.0\",\n",
        "        \"pydantic==2.7.4\",\n",
        "        \"requests\",\n",
        "        \"langchain-openai\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "response = remote_agent.query(\n",
        "    input=\"What's the exchange rate from US dollars to Swedish currency at 2024-07-26?\"\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oEbI1hm1KoQE"
      },
      "outputs": [],
      "source": [
        "# @title Reusing your deployed agent from other applications or SDKs\n",
        "\n",
        "# @markdown The remotely deployed `Reasoning Engine` is now available for import and use. You can access it within your current notebook session, a different notebook, or a Python script.\n",
        "\n",
        "REASONING_ENGINE_RESOURCE_NAME = remote_agent.resource_name\n",
        "print(REASONING_ENGINE_RESOURCE_NAME)\n",
        "\n",
        "# Afterwards, you can use the below code:\n",
        "\n",
        "# from vertexai.preview import reasoning_engines`\n",
        "\n",
        "# remote_agent = reasoning_engines.ReasoningEngine(REASONING_ENGINE_RESOURCE_NAME)`\n",
        "# response = remote_agent.query(input=query)`\n",
        "\n",
        "# @markdown Alternatively, you can query your agent from other programming languages using any of the [available client libraries in Vertex AI](https://cloud.google.com/vertex-ai/docs/start/client-libraries), including C#, Java, Node.js, Python, Go, or REST API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JETd33jIDcjm"
      },
      "source": [
        "## Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "911406c1561e"
      },
      "outputs": [],
      "source": [
        "# @title Delete the buckets and reasoning engines\n",
        "\n",
        "delete_bucket = False  # @param {type:\"boolean\"}\n",
        "if delete_bucket:\n",
        "    ! gsutil -m rm -r $BUCKET_NAME\n",
        "\n",
        "delete_reasoning_engine = False  # @param {type:\"boolean\"}\n",
        "\n",
        "if delete_reasoning_engine:\n",
        "    remote_agent.delete()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_pytorch_deployed_model_reasoning_engine.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

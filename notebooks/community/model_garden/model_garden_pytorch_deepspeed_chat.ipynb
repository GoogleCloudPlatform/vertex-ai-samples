{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d9bbf86da5e"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Vertex AI Model Garden - Deepspeed-chat\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_deepspeed_chat.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_deepspeed_chat.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_pytorch_deepspeed_chat.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "Open in Vertex AI Workbench\n",
        "    </a>\n",
        "    (a Python-3 CPU notebook is recommended)\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates training [DeepSpeed-Chat](https://github.com/microsoft/DeepSpeedExamples/blob/master/applications/DeepSpeed-Chat) with [OPT model](https://github.com/facebookresearch/metaseq) using [RLHF](https://arxiv.org/abs/2203.02155) and deploying it on Vertex AI for online prediction.\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Train the deepspeed-chat with three steps of RLHF using [Vertex AI custom training](https://cloud.google.com/vertex-ai/docs/training/overview) to generate a ChatGPT-like model .\n",
        "- Upload the model to [Vertex AI Model Registry](https://cloud.google.com/vertex-ai/docs/model-registry/introduction).\n",
        "- Deploy the model to a [Vertex AI Endpoint resource](https://cloud.google.com/vertex-ai/docs/predictions/using-private-endpoints).\n",
        "- Run online predictions to serve the chatbot model.\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "264c07757582"
      },
      "source": [
        "## Setup environment\n",
        "\n",
        "**NOTE**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d73ffa0c0b83"
      },
      "source": [
        "### Colab only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2707b02ef5df"
      },
      "outputs": [],
      "source": [
        "!pip3 install --upgrade google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b60a4d7100bf"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth as google_auth\n",
        "\n",
        "google_auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb7adab99e41"
      },
      "source": [
        "### Setup Google Cloud project\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "1. [Enable the Vertex AI API and Compute Engine API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component).\n",
        "\n",
        "1. [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs.\n",
        "\n",
        "1. [Create a service account](https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console) with `Vertex AI User` and `Storage Object Admin` roles for deploying fine tuned model to Vertex AI endpoint."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c460088b873"
      },
      "source": [
        "Fill following variables for experiments environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "855d6b96f291"
      },
      "outputs": [],
      "source": [
        "# Cloud project id.\n",
        "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# The region you want to launch jobs in.\n",
        "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "# The Cloud Storage bucket for storing experiments output. Fill it without the 'gs://' prefix.\n",
        "GCS_BUCKET = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# The service account for deploying fine tuned model.\n",
        "SERVICE_ACCOUNT = \"\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e828eb320337"
      },
      "source": [
        "Initialize Vertex-AI API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12cd25839741"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=GCS_BUCKET)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cc825514deb"
      },
      "source": [
        "### Define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b42bd4fa2b2d"
      },
      "outputs": [],
      "source": [
        "# TODO(b/315894399): The train docker has been removed. If this notebook is\n",
        "# going to be published, go through OSS legal review, and push the docker image\n",
        "# to vertex-ai.\n",
        "\n",
        "# The pre-built training docker image. It contains training scripts and models.\n",
        "TRAIN_DOCKER_URI = \"\"\n",
        "\n",
        "# The pre-built serving docker image. It contains serving scripts and models.\n",
        "SERVE_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai-restricted/vertex-vision-model-garden-dockers/pytorch-deepspeed-chat-serve\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c250872074f"
      },
      "source": [
        "### Define common functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "354da31189dc"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=GCS_BUCKET)\n",
        "\n",
        "import os\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "def create_job_name(prefix):\n",
        "    user = os.environ.get(\"USER\")\n",
        "    now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    job_name = f\"{user}-{prefix}-{now}\"\n",
        "    return job_name\n",
        "\n",
        "\n",
        "def deploy_model(model_id, task):\n",
        "    model_name = \"deepspeed-chat\"\n",
        "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-{task}-endpoint\")\n",
        "    serving_env = {\n",
        "        \"MODEL_ID\": model_id,\n",
        "        \"TASK\": task,\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "    }\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=SERVE_DOCKER_URI,\n",
        "        serving_container_ports=[7080],\n",
        "        serving_container_predict_route=\"/predictions/deepspeed_chat_serving\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "        serving_container_environment_variables=serving_env,\n",
        "    )\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=\"n1-standard-8\",\n",
        "        accelerator_type=\"NVIDIA_TESLA_V100\",\n",
        "        accelerator_count=1,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=SERVICE_ACCOUNT,\n",
        "    )\n",
        "    return model, endpoint\n",
        "\n",
        "\n",
        "class Chatbot:\n",
        "    def __init__(self, endpoint):\n",
        "        self.endpoint = endpoint\n",
        "        self.clear_context()\n",
        "\n",
        "    def clear_context(self):\n",
        "        self.last_response = \"\"\n",
        "        self.num_rounds = 0\n",
        "\n",
        "    def send_request(self, prompt):\n",
        "        instances = [{\"prompt\": prompt}]\n",
        "        response = endpoint.predict(instances=instances).predictions[0]\n",
        "        return response\n",
        "\n",
        "    def talk(self, prompt, with_context=True):\n",
        "        if with_context is False:\n",
        "            self.clear_context()\n",
        "\n",
        "        prompt = self.last_response + f\"Human: {prompt}\\n Assistant: \"\n",
        "        response = self.send_request(prompt)\n",
        "        self.num_rounds += 1\n",
        "\n",
        "        question_poses = [m.start() for m in re.finditer(\"Human: \", response)]\n",
        "        last_question_pos = -1\n",
        "        if len(question_poses) > self.num_rounds:\n",
        "            last_question_pos = question_poses[self.num_rounds]\n",
        "        if last_question_pos != -1:\n",
        "            response = response[0:last_question_pos]\n",
        "        self.last_response = response + \"\\n\\n\"\n",
        "        return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e70e3519ff8b"
      },
      "source": [
        "## Fine tune model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dc65d8f0689"
      },
      "source": [
        "This section trains DeepSpeed-Chat with RLHF all three steps:\n",
        "\n",
        "**Step-1: Supervised fine tuning**\n",
        "\n",
        "This step fine tunes the actor model through a supervised training. It is very similar to standard language model finetuning on casual language tasks. The main difference is from the dataset resources, SFT will collect high-quality query-answer pairs to finetune the model for human-perferred generation. See [here](https://github.com/microsoft/DeepSpeedExamples/blob/master/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/README.md) for the detailed settings of this step.\n",
        "\n",
        "**Step-2: Reward model fine tuning**\n",
        "\n",
        "This step is similar to Step-1 Supervised Fine-Tuning (SFT) finetuning. However, there are several key differences between RM and SFT finetuning, like training data difference, training objective difference, etc. See [here](https://github.com/microsoft/DeepSpeedExamples/blob/master/applications/DeepSpeed-Chat/training/step2_reward_model_finetuning/README.md) for the detailed settings of this step.\n",
        "\n",
        "**Step-3: Reinforcement Learning with Human Feedback**\n",
        "\n",
        "This step uses RLHF to fine tune the actor model further more. See [here](https://github.com/microsoft/DeepSpeedExamples/blob/master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/README.md) for the detailed settings of this step. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02b83b31d1f0"
      },
      "source": [
        "### Run three steps with default settings\n",
        "\n",
        "This example run the official [train.py](https://github.com/microsoft/DeepSpeedExamples/blob/master/applications/DeepSpeed-Chat/train.py) script to launch the job. It takes [facebook/opt-1.3b](https://huggingface.co/facebook/opt-1.3b) as the actor model and [facebook/opt-350m](https://huggingface.co/facebook/opt-350m) as the reward model. It completes the RLHF three steps on a single A100-40GB GPU in 7 hours. The training datasets are:\n",
        "- Dahoas/rm-static\n",
        "- Dahoas/full-hh-rlhf\n",
        "- Dahoas/synthetic-instruct-gptj-pairwise\n",
        "- yitingxie/rlhf-reward-datasets\n",
        "- openai/webgpt_comparisons stanfordnlp/SHP\n",
        "\n",
        "**NOTE:** The [official example](https://github.com/microsoft/DeepSpeedExamples/blob/master/applications/DeepSpeed-Chat/README.md#-one-single-script-completes-all-three-steps-of-rlhf-training-and-generate-your-first-chatgpt-model) runs the single GPU experiment on A6000-48GB GPU. This notebook uses A100-40GB GPU instead, which requires to enable `--gradient_checkpointing` to avoid CUDA OOM. So it takes longer time to compelete 3 steps RLHF training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65467b361315"
      },
      "outputs": [],
      "source": [
        "machine_type = \"a2-highgpu-1g\"\n",
        "gpu_type = \"NVIDIA_TESLA_A100\"\n",
        "num_gpus = 1\n",
        "\n",
        "job_name = create_job_name(\"deepspeed-chat\")\n",
        "output_dir = f\"/gcs/{GCS_BUCKET}/deepspeed-chat\"\n",
        "\n",
        "job = aiplatform.CustomContainerTrainingJob(\n",
        "    display_name=job_name, container_uri=TRAIN_DOCKER_URI, command=[\"python\"]\n",
        ")\n",
        "\n",
        "job.run(\n",
        "    args=[\n",
        "        \"train.py\",\n",
        "        f\"--output-dir={output_dir}\",\n",
        "        \"--actor-model=facebook/opt-1.3b\",\n",
        "        \"--reward-model=facebook/opt-350m\",\n",
        "        \"--deployment-type=single_node\",\n",
        "    ],\n",
        "    boot_disk_size_gb=600,\n",
        "    replica_count=1,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=gpu_type,\n",
        "    accelerator_count=num_gpus,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf253fc2c8f6"
      },
      "source": [
        "### Run any steps with custom settings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6de912d395c1"
      },
      "source": [
        "If you want to launch the job with any one or multiple steps with custom arguments, use the `run.py` script we provide. The following example runs only step-1 and step-3 with custom arguments. See [step-1](https://github.com/microsoft/DeepSpeedExamples/blob/master/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/main.py), [step-2](https://github.com/microsoft/DeepSpeedExamples/blob/master/applications/DeepSpeed-Chat/training/step2_reward_model_finetuning/main.py), [step-3](https://github.com/microsoft/DeepSpeedExamples/blob/master/applications/DeepSpeed-Chat/training/step3_rlhf_finetuning/main.py) runner for supported arguments respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "982e3ef32dda"
      },
      "outputs": [],
      "source": [
        "machine_type = \"a2-highgpu-1g\"\n",
        "gpu_type = \"NVIDIA_TESLA_A100\"\n",
        "num_gpus = 8\n",
        "\n",
        "job_name = create_job_name(\"deepspeed-chat\")\n",
        "output_dir = f\"/gcs/{GCS_BUCKET}/deepspeed-chat\"\n",
        "\n",
        "job = aiplatform.CustomContainerTrainingJob(\n",
        "    display_name=job_name, container_uri=TRAIN_DOCKER_URI, command=[\"python\"]\n",
        ")\n",
        "\n",
        "job.run(\n",
        "    args=[\n",
        "        \"run.py\",\n",
        "        \"--step1_args\",\n",
        "        \"data_path='Dahoas/rm-static Dahoas/full-hh-rlhf Dahoas/synthetic-instruct-gptj-pairwise yitingxie/rlhf-reward-datasets'\",\n",
        "        \"data_split=2,4,4\",\n",
        "        \"model_name_or_path=facebook/opt-1.3b\",\n",
        "        \"per_device_train_batch_size=4\",\n",
        "        \"per_device_eval_batch_size=4\",\n",
        "        \"num_train_epochs=1\",\n",
        "        \"zero_stage=2\",\n",
        "        \"deepspeed\",\n",
        "        f\"output_dir={output_dir}/step-1\",\n",
        "        \"--step3_args\",\n",
        "        \"data_path=Dahoas/rm-static\",\n",
        "        \"data_split=2,4,4\",\n",
        "        f\"actor_model_name_or_path={output_dir}/step-1\"\n",
        "        \"critic_model_name_or_path=facebook/opt-350m\",\n",
        "        \"num_padding_at_beginning=1\",\n",
        "        \"per_device_train_batch_size=4\",\n",
        "        \"per_device_mini_train_batch_size=4\",\n",
        "        \"generation_batch_numbers=1\",\n",
        "        \"ppo_epochs=1\",\n",
        "        \"num_train_epochs=1\",\n",
        "        \"deepspeed\",\n",
        "        \"actor_zero_stage=2\",\n",
        "        \"critic_zero_stage=2\",\n",
        "        \"enable_ema\",\n",
        "        f\"output_dir={output_dir}/step-3\",\n",
        "    ],\n",
        "    boot_disk_size_gb=600,\n",
        "    replica_count=1,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=gpu_type,\n",
        "    accelerator_count=num_gpus,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf7f82732e61"
      },
      "source": [
        "## Upload and Deploy models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cc26e68d7b0"
      },
      "source": [
        "This section uploads the fine-tuned model to Model Registry and deploys it on the Endpoint with one V100 GPU.\n",
        "\n",
        "The model deployment step will take ~20 minutes to complete.\n",
        "\n",
        "NOTE: The model weights will be downloaded after the deployment succeeds. Thus additional 5 minutes of waiting time is needed **after** the above model deployment step succeeds and before you run the next step below. Otherwise you might see a `ServiceUnavailable: 503 502:Bad Gateway` error when you send requests to the endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bf55e38815dc"
      },
      "outputs": [],
      "source": [
        "# Set the model_id to \"facebook/opt-1.3b\" to load the OSS pre-trained model.\n",
        "model, endpoint = deploy_model(\n",
        "    model_id=f\"gs://{GCS_BUCKET}/deepspeed-chat/actor-models/1.3b\", task=\"chatbot\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64ca6d40b474"
      },
      "source": [
        "Once deployment succeeds, you can send requests to the endpoint with text prompts. The model will generate text like a chatbot.\n",
        "\n",
        "Example:\n",
        "```\n",
        "Human: What is a car?\n",
        " Assistant:  A car is a vehicle that is used for transportation.  It can be a vehicle that is used for driving, or it can be a vehicle that is used for riding.  It can also be a vehicle that is used for carrying passengers.\n",
        "\n",
        "Human: Are you a car?\n",
        " Assistant:  I am a human.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ab04da3ec9a"
      },
      "outputs": [],
      "source": [
        "chatbot = Chatbot(endpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1862bf92fe43"
      },
      "outputs": [],
      "source": [
        "print(chatbot.talk(\"What is a car?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "117e1beaabae"
      },
      "outputs": [],
      "source": [
        "print(chatbot.talk(\"Are you a car?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfa0784ef890"
      },
      "source": [
        "The Chatbot class implements the conversation context mechanism for you. It will remember the conversation context by default when you can the `talk()` method. If you want to start a new conversation, you can set the second argument of `talk()` to `False` indicating it to clear the context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bd27ab2c20e8"
      },
      "outputs": [],
      "source": [
        "print(chatbot.talk(\"What is your name?\", False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af21a3cff1e0"
      },
      "source": [
        "## Undeploy model and clean up resource"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "911406c1561e"
      },
      "outputs": [],
      "source": [
        "# Undeploy model and delete endpoint.\n",
        "endpoint.delete(force=True)\n",
        "\n",
        "# Delete models.\n",
        "model.delete()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_pytorch_deepspeed_chat.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

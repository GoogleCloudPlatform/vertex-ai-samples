{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FvusFZ1lHNIh"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEF8AdbMHNIh"
      },
      "source": [
        "# Vertex AI Model Garden GenAI Workshop for Whisper Large v3\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_pytorch_whisper_large_v3_gradio.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_whisper_large_v3_gradio.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDVdfbWXJn8Y"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates starting a playground for [openai/whisper-large-v3](https://huggingface.co/openai/whisper-large-v3), [openai/whisper-large-v3-turbo](https://huggingface.co/openai/whisper-large-v3-turbo) model based on [Gradio UI](https://www.gradio.app/), which allows users to interact with the ASR model.\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Deploy model to a [Vertex AI Endpoint resource](https://cloud.google.com/vertex-ai/docs/predictions/using-private-endpoints).\n",
        "- Run online predictions for `audio-to-text` tasks from the UI.\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3yxOr7vJn8Y"
      },
      "source": [
        "## Run the playground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hmHDYTk9Jn8Y"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project and prepare dependencies\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown 2. **[Optional]** [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs. Set the BUCKET_URI for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`) should be located in the same region as where the notebook was launched. Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\"). If not set, a unique GCS bucket will be created instead.\n",
        "\n",
        "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown 3. **[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment.\n",
        "\n",
        "REGION = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown 4. If you want to run predictions with A100 80GB or H100 GPUs, we recommend using the regions listed below. **NOTE:** Make sure you have associated quota in selected regions. Click the links to see your current quota for each GPU type: [Nvidia A100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_a100_80gb_gpus), [Nvidia H100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_h100_gpus).\n",
        "\n",
        "# @markdown > | Machine Type | Accelerator Type | Recommended Regions |\n",
        "# @markdown | ----------- | ----------- | ----------- |\n",
        "# @markdown | a2-ultragpu-1g | 1 NVIDIA_A100_80GB | us-central1, us-east4, europe-west4, asia-southeast1, us-east4 |\n",
        "# @markdown | a3-highgpu-8g | 8 NVIDIA_H100_80GB | us-central1, us-west1, europe-west4, asia-southeast1 |\n",
        "\n",
        "! pip3 install --upgrade gradio==5.1.0\n",
        "! pip3 install scipy==1.14.1\n",
        "\n",
        "import datetime\n",
        "import importlib\n",
        "import os\n",
        "import uuid\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
        "\n",
        "common_util = importlib.import_module(\n",
        "    \"vertex-ai-samples.community-content.vertex_model_garden.model_oss.notebook_util.common_util\"\n",
        ")\n",
        "\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "if not REGION:\n",
        "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
        "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
        "\n",
        "# Cloud Storage bucket for storing the experiment artifacts.\n",
        "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
        "# prefer using your own GCS bucket, change the value yourself below.\n",
        "now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "\n",
        "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
        "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}-{str(uuid.uuid4())[:4]}\"\n",
        "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
        "else:\n",
        "    assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
        "    shell_output = ! gsutil ls -Lb {BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
        "    bucket_region = shell_output[0].strip().lower()\n",
        "    if bucket_region != REGION:\n",
        "        raise ValueError(\n",
        "            \"Bucket region %s is different from notebook region %s\"\n",
        "            % (bucket_region, REGION)\n",
        "        )\n",
        "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
        "\n",
        "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
        "MODEL_BUCKET = os.path.join(BUCKET_URI, \"whisper_large_v3\")\n",
        "\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "# Gets the default SERVICE_ACCOUNT.\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
        "\n",
        "\n",
        "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
        "\n",
        "! gcloud config set project $PROJECT_ID\n",
        "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/storage.admin\"\n",
        "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/aiplatform.user\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AWnhhOHmJn8Y"
      },
      "outputs": [],
      "source": [
        "# @title Start the playground\n",
        "\n",
        "# @markdown This is a playground for running Whisper Large V3 models.\n",
        "\n",
        "# @markdown After running the cell, a public URL ([\"https://*.gradio.live\"](#)) will appear in the cell output. The playground is available in a separate browser tab when you click the URL.\n",
        "\n",
        "# @markdown **How to use:**\n",
        "# @markdown 1. Select or deploy model\n",
        "# @markdown   1. In the playground, select a previous deployed Whisper model.\n",
        "# @markdown   2. If you don't have any deployed model, deploy a new model in the playground.\n",
        "# @markdown   3. New deployment takes ~20 minutes. You can check the progress at [Vertex Online Prediction](https://console.cloud.google.com/vertex-ai/online-prediction/endpoints).\n",
        "# @markdown   4. After the model deployment is complete, restart the playground in Colab to see the updated endpoint list.\n",
        "# @markdown 1. Inference\n",
        "# @markdown   1. In the \"Inference\" section, fill in prompt and parameters\n",
        "# @markdown   2. Click \"Generate\" to generate image from text prompt.\n",
        "\n",
        "# @markdown **Important notes**\n",
        "# @markdown 1. Reruning this notebook cell creates a new public URL. Previous URLs will stop working.\n",
        "# @markdown 2. After experiments, manually undeploy models to avoid continuous charges to the project.\n",
        "\n",
        "import base64\n",
        "import io\n",
        "import math\n",
        "\n",
        "import gradio as gr\n",
        "import scipy.signal as sps\n",
        "from google.cloud import aiplatform\n",
        "from scipy.io.wavfile import write\n",
        "\n",
        "# The pre-built serving docker image. It contains serving scripts and models.\n",
        "SERVE_DOCKER_URI = \"us-docker.pkg.dev/deeplearning-platform-release/vertex-model-garden/pytorch-inference.cu125.0-1.ubuntu2204.py310\"\n",
        "DEFAULT_SAMPLING_RATE = 16000\n",
        "\n",
        "\n",
        "bearer_token = ! gcloud auth print-access-token\n",
        "bearer_token = bearer_token[0].strip()\n",
        "\n",
        "\n",
        "def is_whisper_endpoint(endpoint: aiplatform.Endpoint) -> bool:\n",
        "    \"\"\"Returns True if the endpoint is a Whisper Large v3 endpoint.\"\"\"\n",
        "    return \"whisper-large-v3\" in endpoint.display_name.lower()\n",
        "\n",
        "\n",
        "def list_endpoints() -> list[str]:\n",
        "    \"\"\"Returns all valid prediction endpoints for in the project and region.\"\"\"\n",
        "    # Gets all the valid endpoints in the project and region.\n",
        "    endpoints = aiplatform.Endpoint.list(order_by=\"create_time desc\")\n",
        "    # Filters out the endpoints which do not have a deployed model, and the endpoint is for image generation\n",
        "    endpoints = list(\n",
        "        filter(\n",
        "            lambda endpoint: endpoint.traffic_split and is_whisper_endpoint(endpoint),\n",
        "            endpoints,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    endpoint_names = list(\n",
        "        map(\n",
        "            lambda endpoint: f\"{endpoint.name} - {endpoint.display_name[:40]}\",\n",
        "            endpoints,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return endpoint_names\n",
        "\n",
        "\n",
        "def get_endpoint(endpoint_name: str) -> aiplatform.Endpoint:\n",
        "    \"\"\"Returns a Vertex endpoint for the given endpoint_name.\"\"\"\n",
        "\n",
        "    endpoint_id = endpoint_name.split(\" - \")[0]\n",
        "    endpoint = aiplatform.Endpoint(\n",
        "        f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_id}\"\n",
        "    )\n",
        "\n",
        "    return endpoint\n",
        "\n",
        "\n",
        "def deploy_model(model_id: str, task_id, accelerator_type: str) -> aiplatform.Endpoint:\n",
        "    \"\"\"Creates a new Vertex prediction endpoint and deploys a model to it.\"\"\"\n",
        "\n",
        "    if not model_id:\n",
        "        raise gr.Error(\"Select a valid model name for model list.\")\n",
        "        return\n",
        "\n",
        "    gr.Info(\"Model deployment started.\")\n",
        "\n",
        "    display_name = common_util.create_job_name(model_id)\n",
        "    endpoint = aiplatform.Endpoint.create(display_name=display_name)\n",
        "    serving_env = {\n",
        "        \"MODEL_ID\": model_id,\n",
        "        \"TASK\": task_id,\n",
        "        \"DEPLOY_SOURCE\": \"notebook_gradio\",\n",
        "    }\n",
        "\n",
        "    machine_type_map = {\n",
        "        \"NVIDIA_TESLA_A100\": \"a2-highgpu-1g\",\n",
        "        \"NVIDIA_L4\": \"g2-standard-12\",\n",
        "    }\n",
        "    if accelerator_type not in machine_type_map:\n",
        "        raise gr.Error(\n",
        "            f\"Select a valid accelerator type from {list(machine_type_map.keys())}\"\n",
        "        )\n",
        "        return\n",
        "    machine_type = machine_type_map.get(accelerator_type)\n",
        "    accelerator_count = 1\n",
        "    common_util.check_quota(\n",
        "        project_id=PROJECT_ID,\n",
        "        region=REGION,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        is_for_training=False,\n",
        "    )\n",
        "\n",
        "    display_name = common_util.create_job_name(model_id)\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_id,\n",
        "        serving_container_image_uri=SERVE_DOCKER_URI,\n",
        "        serving_container_ports=[7080],\n",
        "        serving_container_predict_route=\"/predict\",\n",
        "        serving_container_health_route=\"/health\",\n",
        "        serving_container_environment_variables=serving_env,\n",
        "    )\n",
        "\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=SERVICE_ACCOUNT,\n",
        "        sync=False,\n",
        "    )\n",
        "\n",
        "    gr.Info(\n",
        "        f\"Model {display_name} is being deployed. It may take ~20 minutes to complete.\"\n",
        "    )\n",
        "\n",
        "    return endpoint\n",
        "\n",
        "\n",
        "def format_output(prediction):\n",
        "    if \"text\" not in prediction:\n",
        "        gr.Error(\"text is missing from predictions\")\n",
        "        return \"\"\n",
        "    output = prediction.get(\"text\")\n",
        "    if \"timestamps\" in prediction and len(prediction.get(\"timestamps\")) > 0:\n",
        "        output = \"\"\n",
        "        for ts in prediction.get(\"timestamps\"):\n",
        "            timestamp, text = ts.get(\"timestamp\"), ts.get(\"text\")\n",
        "            if len(timestamp) != 2 or len(text) == 0:\n",
        "                continue\n",
        "            output += f\"{timestamp[0]} - {timestamp[1]} : {text}\\n\"\n",
        "    return output\n",
        "\n",
        "\n",
        "def resample(sr, data):\n",
        "    number_of_samples = round(len(data) * float(DEFAULT_SAMPLING_RATE) / sr)\n",
        "    data = sps.resample(data, number_of_samples)\n",
        "    return data\n",
        "\n",
        "\n",
        "def get_bytes(audio):\n",
        "    bytes_wav = bytes()\n",
        "    byte_io = io.BytesIO(bytes_wav)\n",
        "    write(byte_io, DEFAULT_SAMPLING_RATE, audio)\n",
        "    return byte_io.read()\n",
        "\n",
        "\n",
        "def split_audio(audio):\n",
        "    length = round(float(len(audio)) / DEFAULT_SAMPLING_RATE)\n",
        "    print(f\"audio length: {length}\")\n",
        "    print(f\"shape of audio: {audio.shape}\")\n",
        "    splits = math.ceil(length / 5)\n",
        "    arr = []\n",
        "    for i in range(splits):\n",
        "        if i != splits - 1:\n",
        "            arr = arr + [\n",
        "                audio[\n",
        "                    i * 5 * DEFAULT_SAMPLING_RATE : (i + 1) * 5 * DEFAULT_SAMPLING_RATE\n",
        "                ]\n",
        "            ]\n",
        "        else:\n",
        "            arr = arr + [audio[i * 5 * DEFAULT_SAMPLING_RATE :]]\n",
        "    print(f\"splits: {splits}\")\n",
        "    print(f\"arr size: {len(arr)}\")\n",
        "    for i in range(len(arr)):\n",
        "        print(f\"arr[{i}]: {len(arr[i])}\")\n",
        "    return arr\n",
        "\n",
        "\n",
        "def predict(endpoint_name: str, instance: dict, language: str, timestamp: str):\n",
        "    if not endpoint_name:\n",
        "        raise gr.Error(\"Select (or deploy) a model first.\")\n",
        "    ts = \"\"\n",
        "    if timestamp == \"Sentence Level\":\n",
        "        ts = \"sentence\"\n",
        "    if timestamp == \"Word Level\":\n",
        "        ts = \"word\"\n",
        "    parameters = {\n",
        "        \"language\": language,\n",
        "        \"return_timestamps\": ts,\n",
        "    }\n",
        "    endpoint = get_endpoint(endpoint_name)\n",
        "    response = endpoint.predict(instances=[instance], parameters=parameters)\n",
        "    print(response)\n",
        "    if \"details\" in response and \"msg\" in response[\"details\"]:\n",
        "        raise gr.Error(\"Please check inputs: %s\", response[\"details\"][\"msg\"])\n",
        "    if len(response.predictions) != 0:\n",
        "        return format_output(response.predictions[0])\n",
        "    raise gr.Error(f\"Invalid response: {response}\")\n",
        "\n",
        "\n",
        "def predict_audio(endpoint_name: str, audio: tuple, language: str, timestamp: str):\n",
        "    sr, audio = audio\n",
        "    audio = resample(sr, audio)\n",
        "    audio_splits = split_audio(audio)\n",
        "    text = \"\"\n",
        "    for audio in audio_splits:\n",
        "        instance = {\"audio\": base64.b64encode(get_bytes(audio)).decode(\"utf-8\")}\n",
        "        response = predict(endpoint_name, instance, language, timestamp)\n",
        "        if len(response) != 0:\n",
        "            text += format_output(response)\n",
        "            continue\n",
        "    return text\n",
        "\n",
        "\n",
        "def predict_gcs(endpoint_name: str, gcs_uri: str, language: str, timestamp: str):\n",
        "    instance = {\"audio\": gcs_uri, \"bearer_token\": bearer_token}\n",
        "    return predict(endpoint_name, instance, language, timestamp)\n",
        "\n",
        "\n",
        "tip_text = r\"\"\"\n",
        "1. Select a previous deployed Flux model.\n",
        "2. If you don't have any deployed model, deploy a new model. The deployment takes ~20 minutes. You can check the progress at [Vertex Online Prediction](https://console.cloud.google.com/vertex-ai/online-prediction/endpoints). After the model deployment is complete, restart the playground in Colab to see the updated endpoint list.\n",
        "3. In the \"Inference\" section, provide audio, then click \"transcribe\" to generate text from audio.\n",
        "\"\"\"\n",
        "\n",
        "css = \"\"\"\n",
        ".gradio-container {\n",
        "  width: 90% !important\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(\n",
        "    css=css, theme=gr.themes.Default(primary_hue=\"orange\", secondary_hue=\"blue\")\n",
        ") as demo:\n",
        "    gr.Markdown(\"# Model Garden Playground for Whisper Large V3\")\n",
        "\n",
        "    with gr.Accordion(\"How To Use\", open=True):\n",
        "        tip = gr.Markdown(tip_text)\n",
        "\n",
        "    gr.Markdown(\"## Select or deploy model\")\n",
        "    gr.Markdown(\"### Select a previously deployed model\")\n",
        "    gr.Markdown(\"### Deploy a new model to Vertex\")\n",
        "    with gr.Row(equal_height=True):\n",
        "        with gr.Column(scale=7):\n",
        "            endpoints = list_endpoints()\n",
        "            default_endpoint = None if len(endpoints) == 0 else endpoints[0]\n",
        "            endpoint_name = gr.Dropdown(\n",
        "                label=\"Select a model previously deployed on Vertex\",\n",
        "                choices=list_endpoints(),\n",
        "                value=default_endpoint,\n",
        "                interactive=True,\n",
        "            )\n",
        "        with gr.Column(scale=1):\n",
        "            refresh_endpoints_button = gr.Button(\n",
        "                \"Refresh\", scale=1, variant=\"primary\", min_width=10\n",
        "            )\n",
        "            refresh_endpoints_button.click(\n",
        "                lambda: gr.update(choices=list_endpoints()),\n",
        "                outputs=[endpoint_name],\n",
        "            )\n",
        "    gr.Markdown(\"### Deploy a new model to Vertex\")\n",
        "    model_id = gr.Dropdown(\n",
        "        label=\"Select a model to deploy\",\n",
        "        choices=[\n",
        "            \"openai/whisper-large-v3\",\n",
        "            \"openai/whisper-large-v3-turbo\",\n",
        "        ],\n",
        "        value=\"openai/whisper-large-v3-turbo\",\n",
        "        interactive=True,\n",
        "    )\n",
        "    with gr.Row(equal_height=True):\n",
        "        with gr.Column(scale=7):\n",
        "            accelerator_type = gr.Dropdown(\n",
        "                label=\"Select accelerator type for deployment\",\n",
        "                choices=[\n",
        "                    \"NVIDIA_TESLA_A100\",\n",
        "                    \"NVIDIA_L4\",\n",
        "                ],\n",
        "                value=\"NVIDIA_L4\",\n",
        "            )\n",
        "        with gr.Column(scale=1):\n",
        "            deploy_model_button = gr.Button(\n",
        "                \"Deploy\", scale=1, variant=\"primary\", min_width=10\n",
        "            )\n",
        "\n",
        "    gr.Markdown(\"## Inference\")\n",
        "    with gr.Tab(\"GCS\"):\n",
        "        with gr.Row(equal_height=True):\n",
        "            with gr.Column(scale=2):\n",
        "                input_audio = gr.Textbox(\n",
        "                    lines=1,\n",
        "                    placeholder=\"e.g. gs://bucket/path/to/audio.wav\",\n",
        "                    label=\"GCS uri\",\n",
        "                )\n",
        "                gr.Markdown(\"### Parameters\")\n",
        "                language = gr.Textbox(\n",
        "                    lines=1,\n",
        "                    placeholder=\"e.g. french, english etc\",\n",
        "                    label=\"[language (optional)]()\",\n",
        "                )\n",
        "                timestamp = gr.Dropdown(\n",
        "                    label=\"Timestamps\",\n",
        "                    choices=[\n",
        "                        \"No Timestamp\",\n",
        "                        \"Sentence Level\",\n",
        "                        \"Word Level\",\n",
        "                    ],\n",
        "                    value=\"No Timestamp\",\n",
        "                    interactive=True,\n",
        "                )\n",
        "                submit = gr.Button(\"submit\", variant=\"primary\")\n",
        "            with gr.Column(scale=4):\n",
        "                prediction = gr.Textbox(\n",
        "                    lines=1, placeholder=\"output of model\", label=\"prediction\"\n",
        "                )\n",
        "            submit.click(\n",
        "                predict_gcs,\n",
        "                inputs=[endpoint_name, input_audio, language, timestamp],\n",
        "                outputs=[prediction],\n",
        "            )\n",
        "    with gr.Tab(\"Audio\"):\n",
        "        with gr.Row(equal_height=True):\n",
        "            with gr.Column(scale=2):\n",
        "                input_audio = gr.Audio(\n",
        "                    sources=[\"microphone\", \"upload\"],\n",
        "                    waveform_options=gr.WaveformOptions(\n",
        "                        skip_length=2,\n",
        "                        show_controls=False,\n",
        "                    ),\n",
        "                )\n",
        "                gr.Markdown(\"### Parameters\")\n",
        "                language = gr.Textbox(\n",
        "                    lines=1,\n",
        "                    placeholder=\"e.g. french, english etc\",\n",
        "                    label=\"[language (optional)]()\",\n",
        "                )\n",
        "                timestamp = gr.Dropdown(\n",
        "                    label=\"Timestamps\",\n",
        "                    choices=[\n",
        "                        \"No Timestamp\",\n",
        "                        \"Sentence Level\",\n",
        "                        \"Word Level\",\n",
        "                    ],\n",
        "                    value=\"No Timestamp\",\n",
        "                    interactive=True,\n",
        "                )\n",
        "                submit = gr.Button(\"submit\", variant=\"primary\")\n",
        "            with gr.Column(scale=4):\n",
        "                prediction = gr.Textbox(\n",
        "                    lines=1, placeholder=\"output of model\", label=\"prediction\"\n",
        "                )\n",
        "            submit.click(\n",
        "                predict_audio,\n",
        "                inputs=[endpoint_name, input_audio, language, timestamp],\n",
        "                outputs=[prediction],\n",
        "            )\n",
        "\n",
        "    deploy_model_button.click(\n",
        "        lambda model_id, accelerator_type: deploy_model(\n",
        "            model_id, \"audio2text\", accelerator_type\n",
        "        ),\n",
        "        inputs=[model_id, accelerator_type],\n",
        "        outputs=[],\n",
        "    )\n",
        "\n",
        "\n",
        "show_debug_logs = True  # @param {type: \"boolean\"}\n",
        "demo.queue()\n",
        "demo.launch(share=True, inline=False, debug=show_debug_logs, show_error=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_pytorch_whisper_large_v3_gradio.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

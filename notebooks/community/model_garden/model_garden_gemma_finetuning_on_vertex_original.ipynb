{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d9bbf86da5e"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Vertex AI Model Garden - Gemma Finetuning (PEFT + vLLM)\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma_finetuning_on_vertex.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma_finetuning_on_vertex.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_gemma_finetuning_on_vertex.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "Open in Vertex AI Workbench\n",
        "    </a> (A Python-3 CPU notebook is recommended)\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates finetuning and deploying Gemma models with a [Vertex AI Pipeline](https://cloud.google.com/vertex-ai/docs/pipelines/introduction) that can be launched with a single command. This notebook also demonstrates finetuning and deploying Gemma models with [Vertex AI Custom Training Job](https://cloud.google.com/vertex-ai/docs/training/create-custom-job). Using Vertex AI Pipelines is the quickest way to start finetuning Gemma models, while using a Vertex AI Custom Training Job allows for a higher level of customization and control over the finetuning job. All of the examples in this notebook use parameter efficient finetuning methods [PEFT](https://github.com/huggingface/peft) to reduce training and storage costs.\n",
        "\n",
        "This notebook uses [Text moderation APIs](https://cloud.google.com/natural-language/docs/moderating-text) to analyze predictions against a list of safety attributes.\n",
        "\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Finetune, evaluate, and deploy Gemma models with a Vertex AI Pipeline.\n",
        "- Finetune and deploy Gemma models with a Vertex AI Custom Training Job. Optionally, find better tuning parameters with a Vertex AI Hyperparameter Tuning Job.\n",
        "- Send prediction requests to your finetuned Gemma model.\n",
        "\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "* Cloud NL APIs\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), [Cloud NL API pricing](https://cloud.google.com/natural-language/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "264c07757582"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioensNKM8ned"
      },
      "source": [
        "### Colab only\n",
        "Run the following commands for Colab and skip this section if you are using Workbench."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2707b02ef5df"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    ! pip3 install --upgrade google-cloud-aiplatform\n",
        "    ! pip3 install google-cloud-language==2.10.0\n",
        "    from google.colab import auth as google_auth\n",
        "\n",
        "    google_auth.authenticate_user()\n",
        "\n",
        "    # Restart the notebook kernel after installs.\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "038512c9338a"
      },
      "source": [
        "### Workbench only\n",
        "If you are using Workbench, you should find that the necessary dependencies are already pre-installed. If this is not the case or if you have previously modified the existing libraries, you may install the dependencies using the following commands:\n",
        "```\n",
        "! pip3 install --upgrade google-cloud-aiplatform\n",
        "! pip3 install ipython pandas[output_formatting] google-cloud-language==2.10.0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb7adab99e41"
      },
      "source": [
        "### Setup Google Cloud project\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "1. [Enable the Vertex AI API, Compute Engine API and Cloud Natural Language API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component,language.googleapis.com).\n",
        "\n",
        "1. [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs.\n",
        "\n",
        "1. [Create a service account](https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console) with `Vertex AI User` and `Storage Object Admin` roles for deploying fine tuned model to Vertex AI endpoint."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35Dvbzb0hH3-"
      },
      "source": [
        "### Import the necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLsuvskfhOv4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from datetime import datetime\n",
        "from typing import Tuple\n",
        "\n",
        "from google.cloud import aiplatform, language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c460088b873"
      },
      "source": [
        "Set the following variables for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`) should be located in the specified region (`REGION`). Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "855d6b96f291"
      },
      "outputs": [],
      "source": [
        "# Cloud project id.\n",
        "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# Region for launching jobs.\n",
        "REGION = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# Cloud Storage bucket for storing experiments output.\n",
        "# Start with gs:// prefix, e.g. gs://foo_bucket.\n",
        "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
        "\n",
        "! gcloud config set project $PROJECT_ID\n",
        "! gcloud services enable language.googleapis.com\n",
        "\n",
        "\n",
        "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
        "EXPERIMENT_BUCKET = os.path.join(BUCKET_URI, \"peft\")\n",
        "MODEL_BUCKET = os.path.join(BUCKET_URI, \"model\")\n",
        "\n",
        "# The service account looks like:\n",
        "# '@.iam.gserviceaccount.com'\n",
        "# Please go to https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console\n",
        "# and create service account with `Vertex AI User` and `Storage Object Admin` roles.\n",
        "# The service account for deploying fine tuned model.\n",
        "SERVICE_ACCOUNT = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user(project_id=PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3e443b8670c"
      },
      "source": [
        "### Access the Gemma base model\n",
        "\n",
        "The original Gemma models are available for finetuning and serving in Vertex AI.\n",
        "\n",
        "Accept the model agreement to access the models:\n",
        "1. Navigate to the Vertex AI > Model Garden page in the Google Cloud console\n",
        "2. Find the Gemma model card and click on \"VIEW DETAILS\"\n",
        "3. Review the agreement on the model card page\n",
        "4. After clicking the agreement of Gemma, a Cloud Storage bucket containing Gemma pretrained and finetuned models will be shared\n",
        "5. Paste the Cloud Storage bucket link below and assign it to `VERTEX_AI_MODEL_GARDEN_GEMMA`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "260b0f85d439"
      },
      "outputs": [],
      "source": [
        "# Cloud Storage URI to the Gemma base model.\n",
        "VERTEX_AI_MODEL_GARDEN_GEMMA = \"gs://\"  # @param {type:\"string\"}\n",
        "\n",
        "# The Gemma base model.\n",
        "base_model = \"gemma_2b\"  # @param[\"gemma_2b\", \"gemma_2b_instruct\", \"gemma_7b\", \"gemma_7b_instruct\"]\n",
        "\n",
        "assert (\n",
        "    VERTEX_AI_MODEL_GARDEN_GEMMA\n",
        "), \"Please click the agreement of Gemma in Vertex AI Model Garden, and get the GCS path of Gemma model artifacts.\"\n",
        "\n",
        "base_model = os.path.join(VERTEX_AI_MODEL_GARDEN_GEMMA, base_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e828eb320337"
      },
      "source": [
        "### Initialize Vertex AI API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12cd25839741"
      },
      "outputs": [],
      "source": [
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cc825514deb"
      },
      "source": [
        "### Define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b42bd4fa2b2d"
      },
      "outputs": [],
      "source": [
        "# The pre-built training and serving docker images.\n",
        "COMPILED_PIPELINE_PATH = \"placeholder\"\n",
        "TRAIN_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-train:20240220_0936_RC01\"\n",
        "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240220_0936_RC01\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c250872074f"
      },
      "source": [
        "### Define common functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "354da31189dc"
      },
      "outputs": [],
      "source": [
        "def get_job_name_with_datetime(prefix: str) -> str:\n",
        "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
        "    jobs in Vertex AI.\n",
        "    \"\"\"\n",
        "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
        "\n",
        "\n",
        "def moderate_text(text: str) -> language.ModerateTextResponse:\n",
        "    \"\"\"Calls Vertex AI APIs to analyze text moderations.\"\"\"\n",
        "    client = language.LanguageServiceClient()\n",
        "    document = language.Document(\n",
        "        content=text,\n",
        "        type_=language.Document.Type.PLAIN_TEXT,\n",
        "    )\n",
        "    return client.moderate_text(document=document)\n",
        "\n",
        "\n",
        "def show_text_moderation(text: str, response: language.ModerateTextResponse) -> None:\n",
        "    \"\"\"Shows text moderation results.\"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    def confidence(category: language.ClassificationCategory) -> float:\n",
        "        return category.confidence\n",
        "\n",
        "    columns = [\"category\", \"confidence\"]\n",
        "    categories = sorted(response.moderation_categories, key=confidence, reverse=True)\n",
        "    data = ((category.name, category.confidence) for category in categories)\n",
        "    df = pd.DataFrame(columns=columns, data=data)\n",
        "\n",
        "    print(f\"Text analyzed:\\n{text}\")\n",
        "    print(df.to_markdown(index=False, tablefmt=\"presto\", floatfmt=\".0%\"))\n",
        "\n",
        "\n",
        "def deploy_model_vllm(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    service_account: str,\n",
        "    machine_type: str = \"g2-standard-8\",\n",
        "    accelerator_type: str = \"NVIDIA_L4\",\n",
        "    accelerator_count: int = 1,\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
        "\n",
        "    vllm_args = [\n",
        "        \"--host=0.0.0.0\",\n",
        "        \"--port=7080\",\n",
        "        f\"--model={model_id}\",\n",
        "        f\"--tensor-parallel-size={accelerator_count}\",\n",
        "        \"--swap-space=16\",\n",
        "        \"--gpu-memory-utilization=0.95\",\n",
        "        \"--disable-log-stats\",\n",
        "    ]\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
        "        serving_container_command=[\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
        "        serving_container_args=vllm_args,\n",
        "        serving_container_ports=[7080],\n",
        "        serving_container_predict_route=\"/generate\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "    )\n",
        "\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=service_account,\n",
        "    )\n",
        "    return model, endpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb56d402e84a"
      },
      "source": [
        "## Finetune with Vertex AI Pipelines\n",
        "\n",
        "Vertex Model Garden offers a pre-configured pipeline that can be launched with a single API command. This Vertex AI pipeline will fine-tune, evaluate, upload and deploy your desired Gemma model. This pipeline currently supports [huggingface datasets](https://huggingface.co/datasets). Learn about [Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction).\n",
        "\n",
        "This section demonstrates how to finetune and deploy Gemma models with PEFT LoRA. LoRA (Low-Rank Adaptation) is one approach of PEFT (Parameter Efficient FineTuning), where pretrained model weights are frozen and rank decomposition matrices representing the change in model weights are trained during finetuning. Read more about LoRA in the following publication: [Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L. and Chen, W., 2021. Lora: Low-rank adaptation of large language models. *arXiv preprint arXiv:2106.09685*](https://arxiv.org/abs/2106.09685)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb56d402e71a"
      },
      "outputs": [],
      "source": [
        "# Huggingface dataset name. See more huggingface dataset options at https://huggingface.co/datasets.\n",
        "dataset_name = \"timdettmers/openassistant-guanaco\"\n",
        "\n",
        "# Sets the accelerator type to train and deploy the finetuned model.\n",
        "# Acceptable accelerator types are \"NVIDIA_L4\", \"NVIDIA_TESLA_V100\", \"NVIDIA_TESLA_A100\".\n",
        "# The pipeline will automatically configure a sufficient accelerator count and machine type based on your chosen base model and accelerator types.\n",
        "prediction_accelerator_type = \"NVIDIA_L4\"\n",
        "training_accelerator_type = \"NVIDIA_TESLA_V100\"\n",
        "\n",
        "# The supported precision loading types are \"4bit\" and \"float16\".\n",
        "training_precision_mode = \"4bit\"\n",
        "\n",
        "pipeline_parameters = {\n",
        "    \"base_model\": base_model,\n",
        "    \"dataset_name\": dataset_name,\n",
        "    \"prediction_docker_uri\": VLLM_DOCKER_URI,\n",
        "    \"prediction_accelerator_type\": prediction_accelerator_type,\n",
        "    \"training_docker_uri\": TRAIN_DOCKER_URI,\n",
        "    \"training_accelerator_type\": training_accelerator_type,\n",
        "    \"training_precision_mode\": training_precision_mode,\n",
        "    \"training_lora_rank\": 16,\n",
        "    \"training_lora_alpha\": 32,\n",
        "    \"training_lora_dropout\": 0.05,\n",
        "    \"training_steps\": 20,\n",
        "    \"training_warmup_steps\": 10,\n",
        "    \"training_learning_rate\": 2e-4,\n",
        "    \"evaluation_steps\": 10,\n",
        "    \"evaluation_limit\": 100,\n",
        "}\n",
        "\n",
        "pipeline_bucket_uri = os.path.join(BUCKET_URI, \"pipeline_runs\")\n",
        "\n",
        "# Define and launch the Pipeline Job.\n",
        "job = aiplatform.PipelineJob(\n",
        "    display_name=get_job_name_with_datetime(prefix=\"gemma_peft\"),\n",
        "    template_path=COMPILED_PIPELINE_PATH,\n",
        "    pipeline_root=pipeline_bucket_uri,\n",
        "    parameter_values=pipeline_parameters,\n",
        ")\n",
        "\n",
        "job.submit(service_account=SERVICE_ACCOUNT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb56d402e72a"
      },
      "source": [
        "You can monitor the pipeline run by navigating to the \"Vertex AI Pipelines\" page in the Google Cloud Console and selecting your pipeline run. This pipeline job will take up to 1 hour to complete for the Gemma model.\n",
        "\n",
        "Once the pipeline run finishes, your fine-tuned model will be deployed to a Vertex AI endpoint. In the run UI, select the `create-endpoint-and-deploy-model` task to open the Node Info tab on the right. Find the `endpoint_resource_name` in the Output Parameters section and copy it to the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ab04da3ec9a"
      },
      "outputs": [],
      "source": [
        "# Loads an existing endpoint instance using the endpoint resource name in the format projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_ID}:\n",
        "endpoint_resource_name = \"\"  # @param {type:\"string\"}\n",
        "endpoint = aiplatform.Endpoint(endpoint_resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfPDd91qlSlI"
      },
      "source": [
        "See the [Send a prediction request](#scrollTo=GI363hMzG_4U) section for an example on sending requests to this endpoint."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq4iF00YG_4T"
      },
      "source": [
        "## Finetune with Vertex AI Custom Training Jobs\n",
        "\n",
        "This section demonstrates how to finetune and deploy Gemma models with PEFT LoRA on Vertex AI Custom Training Jobs. LoRA (Low-Rank Adaptation) is one approach of PEFT (Parameter Efficient FineTuning), where pretrained model weights are frozen and rank decomposition matrices representing the change in model weights are trained during finetuning. Read more about LoRA in the following publication: [Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L. and Chen, W., 2021. Lora: Low-rank adaptation of large language models. *arXiv preprint arXiv:2106.09685*](https://arxiv.org/abs/2106.09685)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48SMplaSG_4T"
      },
      "source": [
        "### Finetune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc6nobmqG_4T"
      },
      "source": [
        "Use the Vertex AI SDK to create and run the custom training jobs.\n",
        "\n",
        "This example uses the dataset [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco). You can either use a [dataset from huggingface](https://huggingface.co/datasets) or a custom JSONL dataset in [Vertex text model dataset format](https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-supervised#dataset-format) stored in Cloud Storage. The `template` parameter is optional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih8CwHk1G_4T"
      },
      "source": [
        "#### [Optional] Finetune with a custom dataset\n",
        "\n",
        "To use a custom dataset, you should supply a `gs://` URI to a JSONL file in [Vertex text model dataset format](https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-supervised#dataset-format) in the `dataset_name` below.\n",
        "\n",
        "For example, here is one data point from the sample dataset `gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl`:\n",
        "\n",
        "```json\n",
        "{\"input_text\":\"TRANSCRIPT: \\nREASON FOR EVALUATION:,\\n\\n LABEL:\",\"output_text\":\"Chiropractic\"}\n",
        "```\n",
        "\n",
        "To use this sample dataset that contains `input_text` and `output_text` fields, set `dataset_name` to `gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl` and `template` to `vertex_sample`. For advanced usage with custom datatset fields, see [the template example](https://github.com/tloen/alpaca-lora/blob/main/templates/alpaca.json) and supply your own JSON template as `gs://` URIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yv3i0J_GG_4U"
      },
      "outputs": [],
      "source": [
        "# Huggingface dataset name or gs:// URI to a custom JSONL dataset.\n",
        "dataset_name = \"timdettmers/openassistant-guanaco\"  # @param {type:\"string\"}\n",
        "# Optional. Template name or gs:// URI to a custom template.\n",
        "template = \"\"  # @param {type:\"string\"}\n",
        "# Batch size for finetuning.\n",
        "per_device_train_batch_size = 2  # @param{type:\"integer\"}\n",
        "# Runs 10 training steps as a minimal example.\n",
        "max_steps = 10  # @param {type:\"integer\"}\n",
        "# Precision mode for finetuning.\n",
        "finetuning_precision_mode = \"float16\"  # @param[\"4bit\", \"8bit\", \"float16\"]\n",
        "\n",
        "# Worker pool spec.\n",
        "\n",
        "# Finetunes Gemma with 1 L4 (24G).\n",
        "# machine_type = \"g2-standard-8\"\n",
        "# accelerator_type = \"NVIDIA_L4\"\n",
        "# accelerator_count = 1\n",
        "# Finetunes Gemma with 1 V100 (16G).\n",
        "machine_type = \"n1-standard-8\"\n",
        "accelerator_type = \"NVIDIA_TESLA_V100\"\n",
        "accelerator_count = 1\n",
        "\n",
        "replica_count = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbWkiP3fXN6b"
      },
      "source": [
        "Execute the next cell to run the training job, or skip to the next section to try [hyperparameter tuning with Vertex AI](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivVGS9dHXPOz"
      },
      "outputs": [],
      "source": [
        "# Setup training job.\n",
        "job_name = get_job_name_with_datetime(\"gemma-lora-train\")\n",
        "\n",
        "# Pass training arguments and launch job.\n",
        "train_job = aiplatform.CustomContainerTrainingJob(\n",
        "    display_name=job_name,\n",
        "    container_uri=TRAIN_DOCKER_URI,\n",
        ")\n",
        "\n",
        "# Create a GCS folder to store the LORA adapter.\n",
        "lora_adapter_dir = get_job_name_with_datetime(\"gemma-lora-adapter\")\n",
        "lora_output_dir = os.path.join(MODEL_BUCKET, lora_adapter_dir)\n",
        "lora_output_dir_gcsfuse = lora_output_dir.replace(\"gs://\", \"/gcs/\")\n",
        "\n",
        "# Create a GCS folder to store the merged model with the base model and the\n",
        "# finetuned LORA adapter.\n",
        "merged_model_dir = get_job_name_with_datetime(\"gemma-merged-model\")\n",
        "merged_model_output_dir = os.path.join(MODEL_BUCKET, merged_model_dir)\n",
        "merged_model_output_dir_gcsfuse = merged_model_output_dir.replace(\"gs://\", \"/gcs/\")\n",
        "\n",
        "train_job.run(\n",
        "    args=[\n",
        "        \"--task=instruct-lora\",\n",
        "        f\"--pretrained_model_id={base_model}\",\n",
        "        f\"--dataset_name={dataset_name}\",\n",
        "        \"--instruct_column_in_dataset=text\",\n",
        "        f\"--output_dir={lora_output_dir_gcsfuse}\",\n",
        "        f\"--merge_base_and_lora_output_dir={merged_model_output_dir_gcsfuse}\",\n",
        "        f\"--per_device_train_batch_size={per_device_train_batch_size}\",\n",
        "        \"--lora_rank=16\",\n",
        "        \"--lora_alpha=64\",\n",
        "        \"--lora_dropout=0.1\",\n",
        "        f\"--max_steps={max_steps}\",\n",
        "        \"--max_seq_length=512\",\n",
        "        \"--learning_rate=2e-4\",\n",
        "        f\"--precision_mode={finetuning_precision_mode}\",\n",
        "        f\"--template={template}\",\n",
        "    ],\n",
        "    environment_variables={\"WANDB_DISABLED\": True},\n",
        "    replica_count=replica_count,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    boot_disk_size_gb=500,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        ")\n",
        "\n",
        "print(\"LoRA adapter was saved in: \", lora_output_dir)\n",
        "print(\"Trained and merged models were saved in: \", merged_model_output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln1XyqabBXYg"
      },
      "source": [
        "### [Optional] Hyperparameter tuning with Vertex AI\n",
        "\n",
        "You can use the Vertex AI SDK to create and run the [hyperparameter tuning job](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview) to obtain a better performance by experimenting with different hyperparameters such as learning rates.\n",
        "\n",
        "Define the following specifications:\n",
        "\n",
        "- `worker_pool_specs`: Dictionary specifying the machine type and Docker image.\n",
        "\n",
        "- `parameter_spec`: Dictionary specifying the parameters to optimize. The dictionary key is the string assigned to the command line argument for each hyperparameter in your training application code, and the dictionary value is the parameter specification. The parameter specification includes the type, min/max values, and scale for the hyperparameter.\n",
        "\n",
        "- `metric_spec`: Dictionary specifying the metric to optimize. The dictionary key is the hyperparameter_metric_tag that you set in your training application code, and the value is the optimization goal.\n",
        "\n",
        "You can customize the search space by extending the range of learning rates, adding other parameters such as LoRA rank, etc. Please refer to the [hyperparameter tuning documentation](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview) for more information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HE7V8h5oGXwK"
      },
      "source": [
        "#### [Optional] Custom evaluation dataset\n",
        "\n",
        "To obtain a model with better performance on some specific tasks, you might want to run hyperparameter tuning with a custom evaluation dataset. The hyperparameter tuning service will pick the model according to the evaluation dataset and the metrics you selected. You can use any of the following tasks as the `eval_task` in the code cell below:\n",
        "\n",
        "1. The name of a [lm-evaluation-harness task](https://github.com/EleutherAI/lm-evaluation-harness/tree/big-refactor/lm_eval/tasks).\n",
        "\n",
        "2. `custom_likelihood`. Then, add a flag `--eval_dataset_path=<Cloud Storage URI to your JSONL dataset>`. The JSONL file must be in the format in Vertex AI language model's [prepare evaluation dataset](https://cloud.google.com/vertex-ai/docs/generative-ai/models/evaluate-models#classification) page.\n",
        "\n",
        "3. `builtin_eval`. The built-in evaluation loop of the trainer will be used to evaluate the model instead of the [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) library. You can supply any eval dataset in the same format as the training dataset by specifying `--eval_dataset_path`, `--eval_split`, `--eval_template`, and `--eval_column`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APHRit9OGfo1"
      },
      "outputs": [],
      "source": [
        "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
        "\n",
        "job_name = get_job_name_with_datetime(\"gemma-hpt\")\n",
        "\n",
        "eval_task = \"arc_challenge\"  # @param {type:\"string\"}\n",
        "eval_metric_name = \"acc_norm\"  # @param {type:\"string\"}\n",
        "\n",
        "# Evaluate the model on 10 examples.\n",
        "eval_limit = 10  # @param {type:\"integer\"}\n",
        "\n",
        "flags = {\n",
        "    \"learning_rate\": 2e-4,\n",
        "    \"precision_mode\": finetuning_precision_mode,\n",
        "    \"task\": \"instruct-lora\",\n",
        "    \"pretrained_model_id\": base_model,\n",
        "    \"per_device_train_batch_size\": per_device_train_batch_size,\n",
        "    \"output_dir\": lora_output_dir,\n",
        "    \"max_steps\": max_steps,\n",
        "    \"max_seq_length\": 512,\n",
        "    \"lora_rank\": 16,\n",
        "    \"lora_alpha\": 64,\n",
        "    \"lora_dropout\": 0.1,\n",
        "    \"dataset_name\": dataset_name,\n",
        "    \"eval_steps\": max_steps + 1,  # Only evaluates in the end.\n",
        "    \"eval_tasks\": eval_task,\n",
        "    \"eval_limit\": eval_limit,\n",
        "    \"eval_metric_name\": eval_metric_name,\n",
        "}\n",
        "\n",
        "worker_pool_specs = [\n",
        "    {\n",
        "        \"machine_spec\": {\n",
        "            \"machine_type\": machine_type,\n",
        "            \"accelerator_type\": accelerator_type,\n",
        "            \"accelerator_count\": accelerator_count,\n",
        "        },\n",
        "        \"replica_count\": replica_count,\n",
        "        \"container_spec\": {\n",
        "            \"image_uri\": TRAIN_DOCKER_URI,\n",
        "            \"args\": [\"--{}={}\".format(k, v) for k, v in flags.items()],\n",
        "        },\n",
        "    }\n",
        "]\n",
        "metric_spec = {\"model_performance\": \"maximize\"}\n",
        "parameter_spec = {\n",
        "    \"learning_rate\": hpt.DoubleParameterSpec(min=1e-5, max=1e-3, scale=\"linear\"),\n",
        "}\n",
        "train_job = aiplatform.CustomJob(\n",
        "    display_name=job_name,\n",
        "    worker_pool_specs=worker_pool_specs,\n",
        "    staging_bucket=STAGING_BUCKET,\n",
        ")\n",
        "\n",
        "train_hpt_job = aiplatform.HyperparameterTuningJob(\n",
        "    display_name=f\"{job_name}_hpt\",\n",
        "    custom_job=train_job,\n",
        "    metric_spec=metric_spec,\n",
        "    parameter_spec=parameter_spec,\n",
        "    max_trial_count=2,\n",
        "    parallel_trial_count=2,\n",
        ")\n",
        "\n",
        "train_hpt_job.run(service_account=SERVICE_ACCOUNT)\n",
        "\n",
        "print(\"Trained models were saved in: \", lora_output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0gt2coJUr6N"
      },
      "source": [
        "Then, find the best trial from the hyperparameter tuning job and merge the LoRA weights into the base model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvFNivb-UscY"
      },
      "outputs": [],
      "source": [
        "best_trial_id = max(\n",
        "    train_hpt_job.trials, key=lambda trial: trial.final_measurement.metrics[0].value\n",
        ").id\n",
        "lora_output_dir = os.path.join(\n",
        "    lora_output_dir, f\"trial_{best_trial_id}\", f\"checkpoint-{max_steps}\"\n",
        ")\n",
        "lora_output_dir_gcsfuse = lora_output_dir.replace(\"gs://\", \"/gcs/\")\n",
        "print(f\"Best trial {best_trial_id} saved model in:\", lora_output_dir)\n",
        "\n",
        "# Setup LoRA weights merge job.\n",
        "job_name = get_job_name_with_datetime(\"gemma-lora-merge\")\n",
        "\n",
        "# Pass arguments and launch job.\n",
        "merge_job = aiplatform.CustomContainerTrainingJob(\n",
        "    display_name=job_name,\n",
        "    container_uri=TRAIN_DOCKER_URI,\n",
        ")\n",
        "\n",
        "merge_job.run(\n",
        "    args=[\n",
        "        \"--task=merge-causal-language-model-lora\",\n",
        "        f\"--merge_model_precision_mode={finetuning_precision_mode}\",\n",
        "        f\"--pretrained_model_id={base_model}\",\n",
        "        f\"--finetuned_lora_model_dir={lora_output_dir_gcsfuse}\",\n",
        "        f\"--merge_base_and_lora_output_dir={merged_model_output_dir_gcsfuse}\",\n",
        "    ],\n",
        "    environment_variables={\"WANDB_DISABLED\": True},\n",
        "    replica_count=1,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    boot_disk_size_gb=500,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        ")\n",
        "print(\"LoRA adapter was saved in: \", lora_output_dir)\n",
        "print(\"Trained and merged models were saved in: \", merged_model_output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LMBN_gDG_4U"
      },
      "source": [
        "### Deploy fine tuned models with Google Cloud Text Moderation\n",
        "This section uploads the model to Model Registry and deploys it on the Endpoint.\n",
        "\n",
        "The model deployment step will take 15 minutes to 1 hour to complete, depending on the model sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmHW6m8xG_4U"
      },
      "outputs": [],
      "source": [
        "# Finds Vertex AI prediction supported accelerators and regions in\n",
        "# https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
        "\n",
        "# Sets 1 L4 (24G) to deploy Gemma models.\n",
        "machine_type = \"g2-standard-8\"\n",
        "accelerator_type = \"NVIDIA_L4\"\n",
        "accelerator_count = 1\n",
        "\n",
        "model, endpoint = deploy_model_vllm(\n",
        "    model_name=get_job_name_with_datetime(prefix=\"gemma-vllm-serve\"),\n",
        "    model_id=merged_model_output_dir,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        ")\n",
        "print(\"endpoint_name:\", endpoint.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GI363hMzG_4U"
      },
      "source": [
        "NOTE: After the deployment succeeds, the finetuned model will be downloaded from the GCS bucket used in training above. Thus, an additional ~10 minutes (depending on the model sizes) of waiting time is needed **after** the model deployment step above succeeds and before you run the next step below. Otherwise you might see a `ServiceUnavailable: 503 502:Bad Gateway` error when you send requests to the endpoint.\n",
        "\n",
        "### Send a prediction request\n",
        "\n",
        "Once deployment succeeds, you can send requests to the endpoint with text prompts.\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "Human: What is a car?\n",
        "Assistant:  A car, or a motor car, is a road-connected human-transportation system used to move people or goods from one place to another. The term also encompasses a wide range of vehicles, including motorboats, trains, and aircrafts. Cars typically have four wheels, a cabin for passengers, and an engine or motor. They have been around since the early 19th century and are now one of the most popular forms of transportation, used for daily commuting, shopping, and other purposes.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UYUNn60G_4U"
      },
      "outputs": [],
      "source": [
        "# Loads an existing endpoint instance using the endpoint name:\n",
        "# - Using `endpoint_name = endpoint.name` allows us to get the\n",
        "#   endpoint name of the endpoint `endpoint` created in the cell\n",
        "#   above.\n",
        "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
        "#   an existing endpoint with the ID 1234567890123456789.\n",
        "# You may uncomment the code below to load an existing endpoint.\n",
        "\n",
        "# endpoint_name = \"\"  # @param {type:\"string\"}\n",
        "# aip_endpoint_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
        "# )\n",
        "# endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
        "\n",
        "\n",
        "# Overides max_tokens and top_k parameters during inferences.\n",
        "# If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`,\n",
        "# you can reduce the max length, such as set max_tokens as 20.\n",
        "instances = [\n",
        "    {\n",
        "        \"prompt\": \"What is a car?\",\n",
        "        \"max_tokens\": 50,\n",
        "        \"temperature\": 1.0,\n",
        "        \"top_p\": 1.0,\n",
        "        \"top_k\": 10,\n",
        "    },\n",
        "]\n",
        "response = endpoint.predict(instances=instances)\n",
        "\n",
        "for prediction in response.predictions:\n",
        "    print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNkyVVMfG_4U"
      },
      "source": [
        "Text moderation analyzes a document against a list of safety attributes, which include \"harmful categories\" and topics that may be considered sensitive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2T_cXYJhG_4U"
      },
      "outputs": [],
      "source": [
        "for generated_text in response.predictions:\n",
        "    # Send a request to the API.\n",
        "    response = moderate_text(generated_text)\n",
        "    # Show the results.\n",
        "    show_text_moderation(generated_text, response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af21a3cff1e0"
      },
      "source": [
        "## Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "911406c1561e"
      },
      "outputs": [],
      "source": [
        "# Delete the pipeline job.\n",
        "job.delete()\n",
        "\n",
        "# Undeploy model and delete endpoint.\n",
        "endpoint.delete(force=True)\n",
        "\n",
        "# Delete models.\n",
        "model.delete()\n",
        "\n",
        "# Delete Cloud Storage objects that were created\n",
        "delete_bucket = False\n",
        "if delete_bucket:\n",
        "    ! gsutil -m rm -r $STAGING_BUCKET"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_gemma_finetuning_on_vertex_original.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

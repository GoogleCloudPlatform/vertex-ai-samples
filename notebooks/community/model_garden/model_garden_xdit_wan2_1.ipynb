{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "o93MbHqkgfQQ"
      },
      "outputs": [],
      "source": [
        "# Copyright 2026 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3it3nQpgjVY"
      },
      "source": [
        "# Vertex AI Model Garden - Wan 2.1\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_xdit_wan2_1.ipynb\">\n",
        "      <img alt=\"Workbench logo\" src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" width=\"32px\"><br> Run in Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_xdit_wan2_1.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_xdit_wan2_1.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKrbvC2ygtpN"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates deploying the pre-trained [Wan2.1](https://huggingface.co/Wan-AI/models) T2V Diffusers variants on Vertex AI for online prediction.\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Upload the model to [Model Registry](https://cloud.google.com/vertex-ai/docs/model-registry/introduction).\n",
        "- Deploy the model on [Endpoint](https://cloud.google.com/vertex-ai/docs/predictions/using-private-endpoints).\n",
        "- Run online predictions for text-to-video.\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGM0ZG67hF0m"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4aZlD9kbhKxq"
      },
      "outputs": [],
      "source": [
        "# @title Request for quota\n",
        "\n",
        "# @markdown To deploy the largest variant of the Wan2.1 models, you need 1 host of 4 x H100 machine, or 1 host of 1 x A100-80GB machine. To deploy the smaller variant, you need 1 host of 2 x H100 machine, or 1 host of 1 x A100-80GB machine. Check that you have sufficient quota:\n",
        "# @markdown - For Spot VM quota, check [`CustomModelServingPreemptibleH100GPUsPerProjectPerRegion`](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_preemptible_nvidia_h100_gpus).\n",
        "# @markdown - For regular VM quota, check [`CustomModelServingH100GPUsPerProjectPerRegion`](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_h100_gpus).\n",
        "#\n",
        "# @markdown If you don't have sufficient quota, request for quota following the instructions at [\"Request a higher quota\"](https://cloud.google.com/docs/quota/view-manage#requesting_higher_quota).\n",
        "#\n",
        "# @markdown Note: Utilizing 2 x H100 or 4 x H100 provides substantial speedup over 1 x H100 or 1 x A100-80GB. Utilizing 2 x H100 provides a ~2x speedup in inference and 4 x H100 provides a ~3x speedup in inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Gr85kscg6wB"
      },
      "source": [
        "## Run the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xhnNzFlthU-J"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown 2. **[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment.\n",
        "\n",
        "REGION = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown 3. If you want to run predictions with A100 80GB or H100 GPUs, we recommend using the regions listed below. **NOTE:** Make sure you have associated quota in selected regions. Click the links to see your current quota for each GPU type: [Nvidia A100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_a100_80gb_gpus), [Nvidia H100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_h100_gpus). You can request for quota following the instructions at [\"Request a higher quota\"](https://cloud.google.com/docs/quota/view-manage#requesting_higher_quota).\n",
        "\n",
        "# @markdown | Machine Type | Accelerator Type | Recommended Regions |\n",
        "# @markdown | ----------- | ----------- | ----------- |\n",
        "# @markdown | a2-ultragpu-1g | 1 NVIDIA_A100_80GB | us-central1, us-east4, europe-west4, asia-southeast1, us-east4 |\n",
        "# @markdown | a3-highgpu-2g | 2 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "# @markdown | a3-highgpu-4g | 4 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "# @markdown | a3-highgpu-8g | 8 NVIDIA_H100_80GB | us-central1, europe-west4, us-west1, asia-southeast1 |\n",
        "\n",
        "# Upgrade Vertex AI SDK.\n",
        "! pip3 install --upgrade --quiet 'google-cloud-aiplatform==1.103.0'\n",
        "\n",
        "# Import the necessary packages\n",
        "import importlib\n",
        "import os\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "# Upgrade Vertex AI SDK.\n",
        "if os.environ.get(\"VERTEX_PRODUCT\") != \"COLAB_ENTERPRISE\":\n",
        "    ! pip install --upgrade tensorflow\n",
        "! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
        "\n",
        "common_util = importlib.import_module(\n",
        "    \"vertex-ai-samples.notebooks.community.model_garden.docker_source_codes.notebook_util.common_util\"\n",
        ")\n",
        "\n",
        "LABEL = \"xdit_gpu\"\n",
        "models, endpoints = {}, {}\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "if not REGION:\n",
        "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "! gcloud config set project $PROJECT_ID\n",
        "\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(\n",
        "    project=PROJECT_ID,\n",
        "    location=REGION,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edBskPJ8iZRP"
      },
      "source": [
        "## Deploy Wan 2.1 with xDiT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7GZld50Micuf"
      },
      "outputs": [],
      "source": [
        "# @title Select the model variants\n",
        "\n",
        "# @markdown Set the model to deploy.\n",
        "\n",
        "base_model_name = \"Wan2.1-T2V-1.3B-Diffusers\"  # @param [\"Wan2.1-T2V-1.3B-Diffusers\", \"Wan2.1-T2V-14B-Diffusers\"] {isTemplate:true}\n",
        "model_id = \"Wan-AI/\" + base_model_name\n",
        "task = \"text-to-video\"\n",
        "hf_model_id = model_id\n",
        "\n",
        "\n",
        "# @markdown Choose whether to use a [Spot VM](https://cloud.google.com/compute/docs/instances/spot) for the deployment.\n",
        "is_spot = False  # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown Set use_dedicated_endpoint to False if you don't want to use [dedicated endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment#create-dedicated-endpoint). Note that [dedicated endpoint does not support VPC Service Controls](https://cloud.google.com/vertex-ai/docs/predictions/choose-endpoint-type), uncheck the box if you are using VPC-SC.\n",
        "use_dedicated_endpoint = True  # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown Find Vertex AI prediction supported accelerators and regions at https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
        "accelerator_type = \"NVIDIA_H100_80GB\"  # @param [\"NVIDIA_H100_80GB\", \"NVIDIA_A100_80GB\"] {isTemplate:true}\n",
        "\n",
        "PUBLISHER_MODEL_NAME = f\"wan-ai/wan2-1@{base_model_name.lower()}\"\n",
        "\n",
        "if accelerator_type == \"NVIDIA_H100_80GB\":\n",
        "    if is_spot:\n",
        "        resource_id = \"custom_model_serving_preemptible_nvidia_h100_gpus\"\n",
        "    else:\n",
        "        resource_id = \"custom_model_serving_nvidia_h100_gpus\"\n",
        "    if base_model_name in [\"Wan2.1-T2V-1.3B-Diffusers\"]:\n",
        "        machine_type = \"a3-highgpu-2g\"\n",
        "        accelerator_count = 2\n",
        "    elif base_model_name in [\"Wan2.1-T2V-14B-Diffusers\"]:\n",
        "        machine_type = \"a3-highgpu-4g\"\n",
        "        accelerator_count = 4\n",
        "    else:\n",
        "        raise ValueError(f\"Recommended GPU setting not found for: {base_model_name}.\")\n",
        "elif accelerator_type == \"NVIDIA_A100_80GB\":\n",
        "    if is_spot:\n",
        "        resource_id = \"custom_model_serving_preemptible_nvidia_a100_gpus\"\n",
        "    else:\n",
        "        resource_id = \"custom_model_serving_nvidia_a100_gpus\"\n",
        "    if base_model_name in [\"Wan2.1-T2V-1.3B-Diffusers\", \"Wan2.1-T2V-14B-Diffusers\"]:\n",
        "        machine_type = \"a2-ultragpu-1g\"\n",
        "        accelerator_count = 1\n",
        "    else:\n",
        "        raise ValueError(f\"Recommended GPU setting not found for: {base_model_name}.\")\n",
        "else:\n",
        "    raise ValueError(f\"Recommended GPU setting not found for: {base_model_name}.\")\n",
        "\n",
        "common_util.check_quota(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    is_for_training=False,\n",
        ")\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZIr37aqNl5PL"
      },
      "outputs": [],
      "source": [
        "# @title [Option 1] Deploy with Model Garden SDK\n",
        "# @markdown Deploy with Gen AI model-centric SDK. This section uploads the prebuilt model to Model Registry and deploys it to a Vertex AI Endpoint. It takes 15 minutes to 1 hour to finish depending on the size of the model. See [use open models with Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-open-models) for documentation on other use cases.\n",
        "deploy_request_timeout = 1800  # 30 minutes\n",
        "from vertexai import model_garden\n",
        "\n",
        "model = model_garden.OpenModel(PUBLISHER_MODEL_NAME)\n",
        "endpoints[LABEL] = model.deploy(\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        "    spot=is_spot,\n",
        "    deploy_request_timeout=deploy_request_timeout,\n",
        "    accept_eula=False,\n",
        ")\n",
        "\n",
        "endpoint = endpoints[LABEL]\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LQqIAJrzl-VL"
      },
      "outputs": [],
      "source": [
        "# @title [Option 2] Deploy with customized configs\n",
        "\n",
        "# @markdown This section uploads Wan2.1 models to Model Registry and deploys them to a Vertex Prediction Endpoint. It takes ~1 hour to finish.\n",
        "\n",
        "# @markdown It's recommended to use the region selected by the deployment button on the model card. If the deployment button is not available, it's recommended to stay with the default region of the notebook.\n",
        "\n",
        "# The pre-built serving docker image. It contains serving scripts and models.\n",
        "SERVE_DOCKER_URI = \"us-docker.pkg.dev/deeplearning-platform-release/vertex-model-garden/xdit-serve.cu125.0-2.ubuntu2204.py310\"\n",
        "\n",
        "\n",
        "def deploy_model(model_id, task, machine_type, accelerator_type, accelerator_count):\n",
        "    \"\"\"Create a Vertex AI Endpoint and deploy the specified model to the endpoint.\"\"\"\n",
        "    common_util.check_quota(\n",
        "        project_id=PROJECT_ID,\n",
        "        region=REGION,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        is_for_training=False,\n",
        "    )\n",
        "\n",
        "    model_name = model_id\n",
        "\n",
        "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
        "    serving_env = {\n",
        "        \"MODEL_ID\": model_id,\n",
        "        \"TASK\": task,\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "    }\n",
        "\n",
        "    # xDiT serving parameters\n",
        "    serving_env[\"N_GPUS\"] = accelerator_count\n",
        "    if accelerator_count == 2:\n",
        "        serving_env[\"ULYSSES_DEGREE\"] = \"2\"\n",
        "    elif accelerator_count == 4:\n",
        "        serving_env[\"ULYSSES_DEGREE\"] = \"4\"\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=SERVE_DOCKER_URI,\n",
        "        serving_container_ports=[7080],\n",
        "        serving_container_predict_route=\"/predict\",\n",
        "        serving_container_health_route=\"/health\",\n",
        "        serving_container_environment_variables=serving_env,\n",
        "        model_garden_source_model_name=\"publishers/wan-ai/models/wan\",\n",
        "    )\n",
        "\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        deploy_request_timeout=1800,\n",
        "        system_labels={\"NOTEBOOK_NAME\": \"model_garden_xdit_wan2_1.ipynb\"},\n",
        "    )\n",
        "    return model, endpoint\n",
        "\n",
        "\n",
        "models[\"xdit_gpu_custom\"], endpoints[\"xdit_gpu_custom\"] = deploy_model(\n",
        "    model_id=model_id,\n",
        "    task=task,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        ")\n",
        "\n",
        "print(\"endpoint_name:\", endpoints[\"xdit_gpu_custom\"].name)\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZAHADhsRnKp-"
      },
      "outputs": [],
      "source": [
        "# @title Predict\n",
        "\n",
        "# @markdown Once deployment succeeds, you can send requests to the endpoint with text prompts.\n",
        "# @markdown Example:\n",
        "\n",
        "# @markdown ```\n",
        "# @markdown text: A cat waving a sign that says hello world\n",
        "# @markdown ```\n",
        "\n",
        "# @markdown You may adjust the parameters below to achieve best video quality.\n",
        "from IPython.display import HTML\n",
        "\n",
        "text = \"A cat waving a sign that says hello world\"  # @param {type: \"string\"}\n",
        "seed = 42  # @param {type:\"number\"}\n",
        "\n",
        "instances = [{\"text\": text, \"seed\": seed}]\n",
        "parameters = {\n",
        "    \"seed\": seed,\n",
        "}\n",
        "\n",
        "response = endpoints[LABEL].predict(instances=instances, parameters=parameters)\n",
        "\n",
        "video_bytes = response.predictions[0][\"output\"]\n",
        "\n",
        "video_html = f\"\"\"\n",
        "<video controls>\n",
        "<source src=\"data:video/mp4;base64,{video_bytes}\" type=\"video/mp4\">\n",
        "Your browser does not support the video tag.\n",
        "</video>\n",
        "\"\"\"  # Assumes MP4. Change type if needed (e.g., video/webm)\n",
        "\n",
        "display(HTML(video_html))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fMRycKl_ns2m"
      },
      "outputs": [],
      "source": [
        "# @title Delete the models and endpoints\n",
        "\n",
        "# @markdown  Delete the experiment models and endpoints to recycle the resources\n",
        "# @markdown  and avoid unnecessary continuous charges that may incur.\n",
        "\n",
        "# Undeploy model and delete endpoint.\n",
        "for endpoint in endpoints.values():\n",
        "    endpoint.delete(force=True)\n",
        "\n",
        "# Delete models.\n",
        "for model in models.values():\n",
        "    model.delete()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_xdit_wan2_1.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d9bbf86da5e"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Vertex AI Model Garden - Gemma Finetuning\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_gemma_finetuning_on_vertex.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma_finetuning_on_vertex.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates finetuning and deploying Gemma models with [Vertex AI Custom Training Job](https://cloud.google.com/vertex-ai/docs/training/create-custom-job). All of the examples in this notebook use parameter efficient finetuning methods [PEFT (LoRA)](https://github.com/huggingface/peft) to reduce training and storage costs. LoRA (Low-Rank Adaptation) is one approach of Parameter Efficient FineTuning (PEFT), where pretrained model weights are frozen and rank decomposition matrices representing the change in model weights are trained during finetuning. Read more about LoRA in the following publication: [Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L. and Chen, W., 2021. Lora: Low-rank adaptation of large language models. *arXiv preprint arXiv:2106.09685*](https://arxiv.org/abs/2106.09685).\n",
        "\n",
        "\n",
        "After tuning, we can deploy models on Vertex with GPU or TPU.\n",
        "\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Finetune and deploy Gemma models with Vertex AI Custom Training Jobs.\n",
        "- Send prediction requests to your finetuned Gemma model.\n",
        "\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "264c07757582"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "855d6b96f291"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown **[Optional]** Set the GCS BUCKET_URI to store the experiment artifacts, if you want to use your own bucket. **If not set, a unique GCS bucket will be created automatically on your behalf**.\n",
        "\n",
        "import json\n",
        "import os\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "from typing import Tuple\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Cloud Storage bucket for storing the experiment artifacts.\n",
        "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
        "# prefer using your own GCS bucket, please change the value yourself below.\n",
        "now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
        "if not BUCKET_URI.strip() or BUCKET_URI == \"gs://\":\n",
        "    # Create a unique GCS bucket for this notebook if not specified\n",
        "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}-{str(uuid.uuid4())[:4]}\"\n",
        "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
        "assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
        "\n",
        "# Gets the default SERVICE_ACCOUNT.\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "\n",
        "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
        "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
        "\n",
        "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
        "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
        "\n",
        "# Enable Vertex AI and Cloud Compute APIs.\n",
        "! gcloud config set project $PROJECT_ID\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
        "\n",
        "# @markdown ## Access Gemma Models\n",
        "# @markdown For GPU based finetuning and serving, choose between accessing Gemma models on [Hugging Face](https://huggingface.co/)\n",
        "# @markdown or Vertex AI as described below.\n",
        "\n",
        "# @markdown If you already obtained access to Gemma models on [Hugging Face](https://huggingface.co/), you can load models from there.\n",
        "# @markdown Alternatively, you can also load the original Gemma models for finetuning and serving from Vertex AI after accepting the agreement.\n",
        "\n",
        "# @markdown For TPU based finetuning and serving with KerasNLP, choose the Kaggle option.\n",
        "\n",
        "# @markdown **Please only select and fill one of the three following sections.**\n",
        "LOAD_MODEL_FROM = \"Hugging Face\"  # @param [\"Hugging Face\", \"Google Cloud\", \"Kaggle\"] {isTemplate:true}\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ### Access Gemma models on Hugging Face for GPU based finetuning and serving\n",
        "# @markdown You must provide a Hugging Face User Access Token (read) to access the Gemma models. You can follow the [Hugging Face documentation](https://huggingface.co/docs/hub/en/security-tokens) to create a **read** access token and put it in the `HF_TOKEN` field below.\n",
        "\n",
        "HF_TOKEN = \"\"  # @param {type:\"string\", isTemplate:true}\n",
        "if LOAD_MODEL_FROM == \"Hugging Face\":\n",
        "    assert (\n",
        "        HF_TOKEN\n",
        "    ), \"Please provide a read HF_TOKEN to load models from Hugging Face, or select a different model source.\"\n",
        "\n",
        "# @markdown *--- Or ---*\n",
        "# @markdown ### Access Gemma models on Vertex AI for GPU based finetuning and serving\n",
        "# @markdown Accept the model agreement to access the models:\n",
        "# @markdown 1. Open the [Gemma model card](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/335) from [Vertex AI Model Garden](https://cloud.google.com/model-garden).\n",
        "# @markdown 2. Review the agreement on the model card page.\n",
        "# @markdown 3. After accepting the agreement of Gemma, a `https://` link containing Gemma pretrained and finetuned models will be shared.\n",
        "# @markdown 4. Paste the link in the `VERTEX_MODEL_GARDEN_GEMMA` field below.\n",
        "# @markdown **Note:** This will unzip and copy the Gemma model artifacts to your Cloud Storage bucket, which will take around 1 hour.\n",
        "\n",
        "VERTEX_AI_MODEL_GARDEN_GEMMA = \"\"  # @param {type:\"string\", isTemplate:true}\n",
        "\n",
        "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
        "MODEL_BUCKET = os.path.join(BUCKET_URI, \"gemma\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "if LOAD_MODEL_FROM == \"Google Cloud\":\n",
        "    assert (\n",
        "        VERTEX_AI_MODEL_GARDEN_GEMMA\n",
        "    ), \"Please accept the agreement of Gemma in Vertex AI Model Garden and get the URL to Gemma model artifacts, or select a different model source.\"\n",
        "\n",
        "    # Only use the last part in case a full command is pasted.\n",
        "    signed_url = VERTEX_AI_MODEL_GARDEN_GEMMA.split(\" \")[-1].strip('\"')\n",
        "\n",
        "    ! mkdir -p ./gemma\n",
        "    ! curl -X GET \"{signed_url}\" | tar -xzvf - -C ./gemma/\n",
        "    ! gsutil -m cp -R ./gemma/* {MODEL_BUCKET}\n",
        "\n",
        "    base_model_path_prefix = MODEL_BUCKET\n",
        "    HF_TOKEN = \"\"\n",
        "else:\n",
        "    base_model_path_prefix = \"google/\"\n",
        "\n",
        "\n",
        "# The pre-built training and serving docker images.\n",
        "TRAIN_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-train:20240220_0936_RC01\"\n",
        "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240220_0936_RC01\"\n",
        "\n",
        "# The pre-built training and serving docker images for KerasNLP training\n",
        "# and Hex-LLM serving.\n",
        "KERAS_TRAIN_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/jax-keras-train-tpu:20240422_0939_RC00\"\n",
        "KERAS_MODEL_CONVERSION_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/jax-keras-model-conversion:20240422_0949_RC00\"\n",
        "HEXLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai-restricted/vertex-vision-model-garden-dockers/hex-llm-serve:20240220_0936_RC01\"\n",
        "conversion_job = None\n",
        "\n",
        "# @markdown *--- Or ---*\n",
        "# @markdown ### Access Gemma models from Kaggle for TPU based finetuning and serving\n",
        "# @markdown Kaggle credentials are required for KerasNLP training and Hex-LLM deployment with TPUs.\n",
        "# @markdown Generate the Kaggle username and key by following [these instructions](https://github.com/Kaggle/kaggle-api?tab=readme-ov-file#api-credentials).\n",
        "# @markdown You will need to review and accept the model license.\n",
        "KAGGLE_USERNAME = \"\"  # @param {type:\"string\", isTemplate:true}\n",
        "KAGGLE_KEY = \"\"  # @param {type:\"string\", isTemplate:true}\n",
        "if LOAD_MODEL_FROM == \"Kaggle\":\n",
        "    assert (\n",
        "        KAGGLE_USERNAME and KAGGLE_KEY\n",
        "    ), \"Please provide Kaggle credentials to load models from Kaggle, or select a different model source.\"\n",
        "# @markdown ---\n",
        "\n",
        "\n",
        "def get_job_name_with_datetime(prefix: str) -> str:\n",
        "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
        "    jobs in Vertex AI.\n",
        "    \"\"\"\n",
        "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
        "\n",
        "\n",
        "def deploy_model_vllm(\n",
        "    model_name: str,\n",
        "    base_model_id: str,\n",
        "    model_id: str,\n",
        "    service_account: str,\n",
        "    machine_type: str = \"g2-standard-12\",\n",
        "    accelerator_type: str = \"NVIDIA_L4\",\n",
        "    accelerator_count: int = 1,\n",
        "    max_model_len: int = 8192,\n",
        "    dtype: str = \"bfloat16\",\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys models with vLLM on GPU in Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
        "\n",
        "    vllm_args = [\n",
        "        \"--host=0.0.0.0\",\n",
        "        \"--port=7080\",\n",
        "        f\"--model={model_id}\",\n",
        "        f\"--tensor-parallel-size={accelerator_count}\",\n",
        "        \"--swap-space=16\",\n",
        "        \"--gpu-memory-utilization=0.95\",\n",
        "        f\"--max-model-len={max_model_len}\",\n",
        "        f\"--dtype={dtype}\",\n",
        "        \"--disable-log-stats\",\n",
        "    ]\n",
        "\n",
        "    env_vars = {\n",
        "        \"MODEL_ID\": base_model_id,\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "    }\n",
        "    if HF_TOKEN:\n",
        "        env_vars[\"HF_TOKEN\"] = HF_TOKEN\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
        "        serving_container_command=[\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
        "        serving_container_args=vllm_args,\n",
        "        serving_container_ports=[7080],\n",
        "        serving_container_predict_route=\"/generate\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "        serving_container_environment_variables=env_vars,\n",
        "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
        "        serving_container_deployment_timeout=7200,\n",
        "    )\n",
        "\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=service_account,\n",
        "    )\n",
        "    return model, endpoint\n",
        "\n",
        "\n",
        "def deploy_model_hexllm(\n",
        "    model_name: str,\n",
        "    base_model_id: str,\n",
        "    model_id: str,\n",
        "    service_account: str,\n",
        "    machine_type: str = \"ct5lp-hightpu-1t\",\n",
        "    max_num_batched_tokens: int = 11264,\n",
        "    tokens_pad_multiple: int = 1024,\n",
        "    seqs_pad_multiple: int = 32,\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys models with Hex-LLM on TPU in Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(\n",
        "        display_name=f\"{model_name}-endpoint\",\n",
        "        location=TPU_DEPLOYMENT_REGION,\n",
        "    )\n",
        "\n",
        "    num_tpu_chips = int(machine_type[-2])\n",
        "    hexllm_args = [\n",
        "        \"--host=0.0.0.0\",\n",
        "        \"--port=7080\",\n",
        "        \"--log_level=INFO\",\n",
        "        f\"--model={model_id}\",\n",
        "        f\"--tensor_parallel_size={num_tpu_chips}\",\n",
        "        \"--num_nodes=1\",\n",
        "        \"--use_ray\",\n",
        "        \"--batch_mode=continuous\",\n",
        "        f\"--max_num_batched_tokens={max_num_batched_tokens}\",\n",
        "        f\"--tokens_pad_multiple={tokens_pad_multiple}\",\n",
        "        f\"--seqs_pad_multiple={seqs_pad_multiple}\",\n",
        "    ]\n",
        "\n",
        "    env_vars = {\n",
        "        \"MODEL_ID\": base_model_id,\n",
        "        \"PJRT_DEVICE\": \"TPU\",\n",
        "        \"RAY_DEDUP_LOGS\": \"0\",\n",
        "        \"RAY_USAGE_STATS_ENABLED\": \"0\",\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "    }\n",
        "    if KAGGLE_USERNAME and KAGGLE_KEY:\n",
        "        env_vars[\"KAGGLE_USERNAME\"] = KAGGLE_USERNAME\n",
        "        env_vars[\"KAGGLE_KEY\"] = KAGGLE_KEY\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=HEXLLM_DOCKER_URI,\n",
        "        serving_container_command=[\"python\", \"-m\", \"hex_llm.entrypoints.api_server\"],\n",
        "        serving_container_args=hexllm_args,\n",
        "        serving_container_ports=[7080],\n",
        "        serving_container_predict_route=\"/generate\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "        serving_container_environment_variables=env_vars,\n",
        "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
        "        serving_container_deployment_timeout=7200,\n",
        "        location=TPU_DEPLOYMENT_REGION,\n",
        "    )\n",
        "\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=service_account,\n",
        "    )\n",
        "    return model, endpoint\n",
        "\n",
        "\n",
        "def get_quota(project_id: str, region: str, resource_id: str) -> int:\n",
        "    \"\"\"Returns the quota for a resource in a region. Returns -1 if can not figure out the quota.\"\"\"\n",
        "    service_endpoint = \"aiplatform.googleapis.com\"  # noqa: F841\n",
        "    quota_list_output = !gcloud alpha services quota list --service=$service_endpoint  --consumer=projects/$project_id --filter=\"$service_endpoint/$resource_id\" --format=json\n",
        "    # Use '.s' on the command output because it is an SList type.\n",
        "    quota_data = json.loads(quota_list_output.s)\n",
        "    if len(quota_data) == 0 or \"consumerQuotaLimits\" not in quota_data[0]:\n",
        "        return -1\n",
        "    if (\n",
        "        len(quota_data[0][\"consumerQuotaLimits\"]) == 0\n",
        "        or \"quotaBuckets\" not in quota_data[0][\"consumerQuotaLimits\"][0]\n",
        "    ):\n",
        "        return -1\n",
        "    all_regions_data = quota_data[0][\"consumerQuotaLimits\"][0][\"quotaBuckets\"]\n",
        "    for region_data in all_regions_data:\n",
        "        if (\n",
        "            region_data.get(\"dimensions\")\n",
        "            and region_data[\"dimensions\"][\"region\"] == region\n",
        "        ):\n",
        "            if \"effectiveLimit\" in region_data:\n",
        "                return int(region_data[\"effectiveLimit\"])\n",
        "            else:\n",
        "                return 0\n",
        "    return -1\n",
        "\n",
        "\n",
        "def get_resource_id(accelerator_type: str, is_for_training: bool) -> str:\n",
        "    \"\"\"Returns the resource id for a given accelerator type and the use case.\n",
        "    Args:\n",
        "      accelerator_type: The accelerator type.\n",
        "      is_for_training: Whether the resource is used for training. Set false\n",
        "      for serving use case.\n",
        "    Returns:\n",
        "      The resource id.\n",
        "    \"\"\"\n",
        "    training_accelerator_map = {\n",
        "        \"NVIDIA_TESLA_V100\": \"custom_model_training_nvidia_v100_gpus\",\n",
        "        \"NVIDIA_L4\": \"custom_model_training_nvidia_l4_gpus\",\n",
        "        \"NVIDIA_TESLA_A100\": \"custom_model_training_nvidia_a100_gpus\",\n",
        "        \"NVIDIA_TESLA_T4\": \"custom_model_training_nvidia_t4_gpus\",\n",
        "        \"TPU_V5e\": \"custom_model_training_tpu_v5e\",\n",
        "        \"TPU_V3\": \"custom_model_training_tpu_v3\",\n",
        "    }\n",
        "    serving_accelerator_map = {\n",
        "        \"NVIDIA_TESLA_V100\": \"custom_model_serving_nvidia_v100_gpus\",\n",
        "        \"NVIDIA_L4\": \"custom_model_serving_nvidia_l4_gpus\",\n",
        "        \"NVIDIA_TESLA_A100\": \"custom_model_serving_nvidia_a100_gpus\",\n",
        "        \"NVIDIA_TESLA_T4\": \"custom_model_serving_nvidia_t4_gpus\",\n",
        "        \"TPU_V5e\": \"custom_model_serving_tpu_v5e\",\n",
        "    }\n",
        "    if is_for_training:\n",
        "        if accelerator_type in training_accelerator_map:\n",
        "            return training_accelerator_map[accelerator_type]\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Could not find accelerator type: {accelerator_type} for training.\"\n",
        "            )\n",
        "    else:\n",
        "        if accelerator_type in serving_accelerator_map:\n",
        "            return serving_accelerator_map[accelerator_type]\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Could not find accelerator type: {accelerator_type} for serving.\"\n",
        "            )\n",
        "\n",
        "\n",
        "def check_quota(\n",
        "    project_id: str,\n",
        "    region: str,\n",
        "    accelerator_type: str,\n",
        "    accelerator_count: int,\n",
        "    is_for_training: bool,\n",
        "):\n",
        "    \"\"\"Checks if the project and the region has the required quota.\"\"\"\n",
        "    resource_id = get_resource_id(accelerator_type, is_for_training)\n",
        "    quota = get_quota(project_id, region, resource_id)\n",
        "    quota_request_instruction = (\n",
        "        \"Either use \"\n",
        "        \"a different region or request additional quota. Follow \"\n",
        "        \"instructions here \"\n",
        "        \"https://cloud.google.com/docs/quotas/view-manage#requesting_higher_quota\"\n",
        "        \" to check quota in a region or request additional quota for \"\n",
        "        \"your project.\"\n",
        "    )\n",
        "    if quota == -1:\n",
        "        raise ValueError(\n",
        "            f\"\"\"Quota not found for: {resource_id} in {region}.\n",
        "            {quota_request_instruction}\"\"\"\n",
        "        )\n",
        "    if quota < accelerator_count:\n",
        "        raise ValueError(\n",
        "            f\"\"\"Quota not enough for {resource_id} in {region}:\n",
        "            {quota} < {accelerator_count}.\n",
        "            {quota_request_instruction}\"\"\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb56d402e84a"
      },
      "source": [
        "## Finetune with HuggingFace PEFT and Deploy with vLLM on GPUs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KwAW99YZHTdy"
      },
      "outputs": [],
      "source": [
        "# @title Set Dataset\n",
        "\n",
        "# @markdown Use the Vertex AI SDK to create and run the custom training jobs.\n",
        "\n",
        "# @markdown This notebook uses [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) dataset as an example.\n",
        "# @markdown You can set `dataset_name` to any existing [Hugging Face dataset](https://huggingface.co/datasets) name, and set `instruct_column_in_dataset` to the name of the dataset column containing training data. The [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) has only one column `text`, and therefore we set `instruct_column_in_dataset` to `text` in this notebook.\n",
        "\n",
        "# @markdown ### (Optional) Prepare a custom JSONL dataset for finetuning\n",
        "\n",
        "# @markdown You can prepare a JSONL file where each line is a valid JSON string as your custom training dataset. For example, here is one line from the [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) dataset:\n",
        "# @markdown ```\n",
        "# @markdown {\"text\": \"### Human: Hola### Assistant: \\u00a1Hola! \\u00bfEn qu\\u00e9 puedo ayudarte hoy?\"}\n",
        "# @markdown ```\n",
        "\n",
        "# @markdown The JSON object has a key `text`, which should match `instruct_column_in_dataset`; The value should be one training data point, i.e. a string. After you prepared your JSONL file, you can either upload it to [Hugging Face datasets](https://huggingface.co/datasets) or [Google Cloud Storage](https://cloud.google.com/storage).\n",
        "\n",
        "# @markdown - To upload a JSONL dataset to [Hugging Face datasets](https://huggingface.co/datasets), follow the instructions on [Uploading Datasets](https://huggingface.co/docs/hub/en/datasets-adding). Then, set `dataset_name` to the name of your newly created dataset on Hugging Face.\n",
        "\n",
        "# @markdown - To upload a JSONL dataset to [Google Cloud Storage](https://cloud.google.com/storage), follow the instructions on [Upload objects from a filesystem](https://cloud.google.com/storage/docs/uploading-objects). Then, set `dataset_name` to the `gs://` URI to your JSONL file. For example: `gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl`.\n",
        "\n",
        "# @markdown Optionally update the `instruct_column_in_dataset` field below if your JSON objects use a key other than the default `text`.\n",
        "\n",
        "# @markdown ### (Optional) Format your data with custom JSON template\n",
        "\n",
        "# @markdown Sometimes, your dataset might have multiple text columns and you want to construct the training data with a template. You can prepare a JSON template in the following format:\n",
        "\n",
        "# @markdown ```\n",
        "# @markdown {\n",
        "# @markdown   \"description\": \"A short template for vertex sample dataset.\",\n",
        "# @markdown   \"prompt_input\": \"{input_text}{output_text}\",\n",
        "# @markdown   \"prompt_no_input\": \"{input_text}{output_text}\"\n",
        "# @markdown }\n",
        "# @markdown ```\n",
        "\n",
        "# @markdown As an example, the template above can be used to format the following training data (this line comes from `gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl`):\n",
        "\n",
        "# @markdown ```\n",
        "# @markdown {\"input_text\":\"TRANSCRIPT: \\nREASON FOR EVALUATION:,\\n\\n LABEL:\",\"output_text\":\"Chiropractic\"}\n",
        "# @markdown ```\n",
        "\n",
        "# @markdown This example template simply concatenates `input_text` with `output_text`. You can set `template` to `vertex_sample` to try out this built-in template with the dataset `gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl`, or build more complicated JSON templates such as [the alpaca example](https://github.com/tloen/alpaca-lora/blob/main/templates/alpaca.json). To use your own JSON template, please [upload it to Google Cloud Storage](https://cloud.google.com/storage/docs/uploading-objects) and put the `gs://` URI in the `template` field below. Leave `instruct_column_in_dataset` as `text`.\n",
        "\n",
        "# Hugging Face dataset name or gs:// URI to a custom JSONL dataset.\n",
        "dataset_name = \"timdettmers/openassistant-guanaco\"  # @param {type:\"string\"}\n",
        "\n",
        "# Name of the dataset column containing training text input.\n",
        "instruct_column_in_dataset = \"text\"  # @param {type:\"string\"}\n",
        "\n",
        "# Optional. Template name or gs:// URI to a custom template.\n",
        "template = \"\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ivVGS9dHXPOz"
      },
      "outputs": [],
      "source": [
        "# @title Finetune\n",
        "# @markdown Use the Vertex AI SDK to create and run the custom training jobs. It takes around 40 minutes to finetune Gemma-2B for 1000 steps on 1 NVIDIA_TESLA_V100.\n",
        "\n",
        "# @markdown **Note**: To finetune the Gemma 7B models, we recommend setting `finetuning_precision_mode` to `4bit` and using NVIDIA_L4 instead of NVIDIA_TESLA_V100.\n",
        "# @markdown Please click \"Show Code\" to see more details.\n",
        "\n",
        "if LOAD_MODEL_FROM == \"Kaggle\":\n",
        "    print(\n",
        "        \"Skipped: Cannot load model from Kaggle, which is only supported in the KerasNLP section.\"\n",
        "    )\n",
        "else:\n",
        "    # The Gemma base model.\n",
        "    MODEL_ID = \"gemma-2b\"  # @param[\"gemma-2b\", \"gemma-2b-it\", \"gemma-7b\", \"gemma-7b-it\", \"gemma-1.1-2b-it\", \"gemma-1.1-7b-it\"] {isTemplate:true}\n",
        "    # The accelerator to use.\n",
        "    ACCELERATOR_TYPE = \"NVIDIA_TESLA_V100\"  # @param[\"NVIDIA_TESLA_V100\", \"NVIDIA_L4\", \"NVIDIA_TESLA_A100\"]\n",
        "    base_model_id = os.path.join(base_model_path_prefix, MODEL_ID)\n",
        "\n",
        "    # Batch size for finetuning.\n",
        "    per_device_train_batch_size = 1  # @param{type:\"integer\"}\n",
        "    # Runs 10 training steps as a minimal example.\n",
        "    max_steps = 1000  # @param {type:\"integer\"}\n",
        "    # Precision mode for finetuning.\n",
        "    finetuning_precision_mode = \"float16\"  # @param[\"4bit\", \"8bit\", \"float16\"]\n",
        "    # Learning rate.\n",
        "    learning_rate = 2e-4  # @param{type:\"number\"}\n",
        "    # LoRA parameters.\n",
        "    lora_rank = 16  # @param{type:\"integer\"}\n",
        "    lora_alpha = 64  # @param{type:\"integer\"}\n",
        "    lora_dropout = 0.1  # @param{type:\"number\"}\n",
        "\n",
        "    # Worker pool spec.\n",
        "    # Gemma only supports single-GPU finetuning.\n",
        "    accelerator_count = 1\n",
        "    machine_type = None\n",
        "    if \"7b\" in MODEL_ID:\n",
        "        assert (\n",
        "            ACCELERATOR_TYPE != \"NVIDIA_TESLA_V100\"\n",
        "        ), \"Finetuning Gemma 7B models is not supported on NVIDIA_TESLA_V100. Try a GPU with more VRAM.\"\n",
        "        if ACCELERATOR_TYPE == \"NVIDIA_L4\":\n",
        "            assert (\n",
        "                finetuning_precision_mode == \"4bit\"\n",
        "            ), f\"{finetuning_precision_mode} finetuning of Gemma 7B models is not supported on NVIDIA_L4. Try a GPU with more VRAM or use a different precision mode.\"\n",
        "\n",
        "    if ACCELERATOR_TYPE == \"NVIDIA_TESLA_V100\":\n",
        "        machine_type = \"n1-standard-8\"\n",
        "    elif ACCELERATOR_TYPE == \"NVIDIA_L4\":\n",
        "        machine_type = \"g2-standard-12\"\n",
        "    elif ACCELERATOR_TYPE == \"NVIDIA_TESLA_A100\":\n",
        "        machine_type = \"a2-highgpu-1g\"\n",
        "\n",
        "    assert (\n",
        "        machine_type\n",
        "    ), f\"Cannot automatically determine machine type from {ACCELERATOR_TYPE}.\"\n",
        "\n",
        "    replica_count = 1\n",
        "\n",
        "    check_quota(\n",
        "        project_id=PROJECT_ID,\n",
        "        region=REGION,\n",
        "        accelerator_type=ACCELERATOR_TYPE,\n",
        "        accelerator_count=accelerator_count,\n",
        "        is_for_training=True,\n",
        "    )\n",
        "\n",
        "    # Setup training job.\n",
        "    job_name = get_job_name_with_datetime(\"gemma-lora-train\")\n",
        "\n",
        "    # Pass training arguments and launch job.\n",
        "    train_job = aiplatform.CustomContainerTrainingJob(\n",
        "        display_name=job_name,\n",
        "        container_uri=TRAIN_DOCKER_URI,\n",
        "    )\n",
        "\n",
        "    # Create a GCS folder to store the LORA adapter.\n",
        "    lora_adapter_dir = get_job_name_with_datetime(\"gemma-lora-adapter\")\n",
        "    lora_output_dir = os.path.join(STAGING_BUCKET, lora_adapter_dir)\n",
        "\n",
        "    # Create a GCS folder to store the merged model with the base model and the\n",
        "    # finetuned LORA adapter.\n",
        "    merged_model_dir = get_job_name_with_datetime(\"gemma-merged-model\")\n",
        "    merged_model_output_dir = os.path.join(STAGING_BUCKET, merged_model_dir)\n",
        "\n",
        "    train_job.run(\n",
        "        args=[\n",
        "            \"--task=instruct-lora\",\n",
        "            f\"--pretrained_model_id={base_model_id}\",\n",
        "            f\"--dataset_name={dataset_name}\",\n",
        "            f\"--instruct_column_in_dataset={instruct_column_in_dataset}\",\n",
        "            f\"--output_dir={lora_output_dir}\",\n",
        "            f\"--merge_base_and_lora_output_dir={merged_model_output_dir}\",\n",
        "            f\"--per_device_train_batch_size={per_device_train_batch_size}\",\n",
        "            f\"--lora_rank={lora_rank}\",\n",
        "            f\"--lora_alpha={lora_alpha}\",\n",
        "            f\"--lora_dropout={lora_dropout}\",\n",
        "            f\"--max_steps={max_steps}\",\n",
        "            \"--max_seq_length=512\",\n",
        "            f\"--learning_rate={learning_rate}\",\n",
        "            f\"--precision_mode={finetuning_precision_mode}\",\n",
        "            f\"--template={template}\",\n",
        "            f\"--huggingface_access_token={HF_TOKEN}\",\n",
        "        ],\n",
        "        environment_variables={\"WANDB_DISABLED\": True},\n",
        "        replica_count=replica_count,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=ACCELERATOR_TYPE,\n",
        "        accelerator_count=accelerator_count,\n",
        "        boot_disk_size_gb=500,\n",
        "        service_account=SERVICE_ACCOUNT,\n",
        "    )\n",
        "\n",
        "    print(\"LoRA adapter was saved in: \", lora_output_dir)\n",
        "    print(\"Trained and merged models were saved in: \", merged_model_output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qmHW6m8xG_4U"
      },
      "outputs": [],
      "source": [
        "# @title Deploy\n",
        "# @markdown This section uploads the model to Model Registry and deploys it on the Endpoint. It takes 15 minutes to 1 hour to finish.\n",
        "# @markdown Please click \"Show Code\" to see more details.\n",
        "\n",
        "if LOAD_MODEL_FROM == \"Kaggle\":\n",
        "    print(\n",
        "        \"Skipped: Cannot load model from Kaggle, which is only supported in the KerasNLP section.\"\n",
        "    )\n",
        "else:\n",
        "    print(\"Deploying models in: \", merged_model_output_dir)\n",
        "\n",
        "    # Please finds Vertex AI prediction supported accelerators and regions in [here](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute).\n",
        "    # Sets 1 L4 (24G) to deploy Gemma models.\n",
        "    machine_type = \"g2-standard-12\"\n",
        "    accelerator_type = \"NVIDIA_L4\"\n",
        "    accelerator_count = 1\n",
        "\n",
        "    check_quota(\n",
        "        project_id=PROJECT_ID,\n",
        "        region=REGION,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        is_for_training=False,\n",
        "    )\n",
        "\n",
        "    # Note that a larger max_model_len will require more GPU memory.\n",
        "    max_model_len = 2048\n",
        "\n",
        "    model, endpoint = deploy_model_vllm(\n",
        "        model_name=get_job_name_with_datetime(prefix=\"gemma-vllm-serve\"),\n",
        "        base_model_id=f\"google/{MODEL_ID}\",\n",
        "        model_id=merged_model_output_dir,\n",
        "        service_account=SERVICE_ACCOUNT,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        max_model_len=max_model_len,\n",
        "    )\n",
        "    print(\"endpoint_name:\", endpoint.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2UYUNn60G_4U"
      },
      "outputs": [],
      "source": [
        "# @title Predict\n",
        "# @markdown Once deployment succeeds, you can send requests to the endpoint with text prompts.\n",
        "\n",
        "# @markdown Here we use an example from the [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) to show the finetuning outcome:\n",
        "\n",
        "# @markdown ```\n",
        "# @markdown ### Human: How would the Future of AI in 10 Years look?### Assistant: Predicting the future is always a challenging task, but here are some possible ways that AI could evolve over the next 10 years: Continued advancements in deep learning: Deep learning has been one of the main drivers of recent AI breakthroughs, and we can expect continued advancements in this area. This may include improvements to existing algorithms, as well as the development of new architectures that are better suited to specific types of data and tasks. Increased use of AI in healthcare: AI has the potential to revolutionize healthcare, by improving the accuracy of diagnoses, developing new treatments, and personalizing patient care. We can expect to see continued investment in this area, with more healthcare providers and researchers using AI to improve patient outcomes. Greater automation in the workplace: Automation is already transforming many industries, and AI is likely to play an increasingly important role in this process. We can expect to see more jobs being automated, as well as the development of new types of jobs that require a combination of human and machine skills. More natural and intuitive interactions with technology: As AI becomes more advanced, we can expect to see more natural and intuitive ways of interacting with technology. This may include voice and gesture recognition, as well as more sophisticated chatbots and virtual assistants. Increased focus on ethical considerations: As AI becomes more powerful, there will be a growing need to consider its ethical implications. This may include issues such as bias in AI algorithms, the impact of automation on employment, and the use of AI in surveillance and policing. Overall, the future of AI in 10 years is likely to be shaped by a combination of technological advancements, societal changes, and ethical considerations. While there are many exciting possibilities for AI in the future, it will be important to carefully consider its potential impact on society and to work towards ensuring that its benefits are shared fairly and equitably.\n",
        "# @markdown ```\n",
        "\n",
        "# @markdown Additionally, you can moderate the generated text with Vertex AI. See [Moderate text documentation](https://cloud.google.com/natural-language/docs/moderating-text) for more details.\n",
        "\n",
        "# Loads an existing endpoint instance using the endpoint name:\n",
        "# - Using `endpoint_name = endpoint.name` allows us to get the\n",
        "#   endpoint name of the endpoint `endpoint` created in the cell\n",
        "#   above.\n",
        "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
        "#   an existing endpoint with the ID 1234567890123456789.\n",
        "# You may uncomment the code below to load an existing endpoint.\n",
        "\n",
        "# endpoint_name = \"\"  # @param {type:\"string\"}\n",
        "# aip_endpoint_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
        "# )\n",
        "# endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
        "\n",
        "if LOAD_MODEL_FROM == \"Kaggle\":\n",
        "    print(\n",
        "        \"Skipped: Cannot load model from Kaggle, which is only supported in the KerasNLP section.\"\n",
        "    )\n",
        "else:\n",
        "    prompt = \"How would the Future of AI in 10 Years look?\"  # @param {type: \"string\"}\n",
        "    max_tokens = 128  # @param {type:\"integer\"}\n",
        "    temperature = 1.0  # @param {type:\"number\"}\n",
        "    top_p = 0.9  # @param {type:\"number\"}\n",
        "    top_k = 1  # @param {type:\"integer\"}\n",
        "\n",
        "    # Overides max_tokens and top_k parameters during inferences.\n",
        "    # If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`,\n",
        "    # you can reduce the max length, such as set max_tokens as 20.\n",
        "    instances = [\n",
        "        {\n",
        "            \"prompt\": f\"### Human: {prompt}### Assistant: \",\n",
        "            \"max_tokens\": max_tokens,\n",
        "            \"temperature\": temperature,\n",
        "            \"top_p\": top_p,\n",
        "            \"top_k\": top_k,\n",
        "        },\n",
        "    ]\n",
        "    response = endpoint.predict(instances=instances)\n",
        "\n",
        "    for prediction in response.predictions:\n",
        "        print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qujA6HkYyL5Z"
      },
      "source": [
        "## Finetune with KerasNLP PEFT and Deploy with HexLLM on TPUs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xyfeOl1HyL5Z"
      },
      "outputs": [],
      "source": [
        "# @title Set Dataset\n",
        "\n",
        "# @markdown For the training dataset, you can\n",
        "# @markdown  - either use a pre-built TensorFlow\n",
        "# @markdown    dataset such as this [imdb_reviews](https://www.tensorflow.org/datasets/catalog/imdb_reviews) dataset\n",
        "# @markdown  - or your own dataset in [JSONL format](https://jsonlines.org/examples/) such as this [databricks-dolly-15K](https://huggingface.co/datasets/databricks/databricks-dolly-15k/blob/main/databricks-dolly-15k.jsonl) JSONL file. [See license information here.](https://huggingface.co/datasets/databricks/databricks-dolly-15k#licenseattribution)\n",
        "\n",
        "# @markdown Whether you use a TensorFlow dataset or a JSONL dataset, each data-item\n",
        "# @markdown will be in the form of a dictionary containing multiple key-value pairs. For example,\n",
        "# @markdown [the `imdb_reviews` data-item dictionary](https://www.tensorflow.org/datasets/catalog/imdb_reviews#imdb_reviewsplain_text_default_config)\n",
        "# @markdown contains keys `text` and `label` and [the `databricks-dolly-15k` data-item dictionary](https://huggingface.co/datasets/databricks/databricks-dolly-15k) contains\n",
        "# @markdown keys `instruction`, `context`, `response`, and  `category`.\n",
        "# @markdown However, the Gemma model only takes a single string as a training example. To arbitrarily select and combine\n",
        "# @markdown multiple key-values into a single training string, you can set a `template` variable in the next section.\n",
        "# @markdown For example, for the [`databricks-dolly-15k`](https://huggingface.co/datasets/databricks/databricks-dolly-15k), you can set the\n",
        "# @markdown `template` as `Instruction: {instruction} Response: {response}`\n",
        "# @markdown which will then automatically fill each `instruction` and `response` key-values into this string template\n",
        "# @markdown and generate a single training string which will look like:\n",
        "\n",
        "# @markdown ```\n",
        "# @markdown  Instruction: Why can camels survive for long without water? Response: Camels use the fat in their humps to keep them filled with energy and hydration for long periods of time.\n",
        "# @markdown  ```\n",
        "\n",
        "# @markdown And for the [imdb_reviews](https://www.tensorflow.org/datasets/catalog/imdb_reviews#imdb_reviewsplain_text_default_config) you can set the `template` as `{text}`\n",
        "# @markdown which will then select each `text` key-value as a single training string which will look like:\n",
        "\n",
        "# @markdown ```\n",
        "# @markdown I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.\n",
        "# @markdown ```\n",
        "\n",
        "# @markdown Set a template suitable for the selected dataset whether TensorFlow Dataset or JSONL format. The following value is\n",
        "# @markdown set for the `databricks-dolly-15k` dataset.\n",
        "template = \"Instruction: {instruction} Response: {response}\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ### Fill only one of the sections below:\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown For example `imdb_reviews`.\n",
        "\n",
        "# The TensorFlow dataset name.\n",
        "tfds_dataset_name = \"\"  # @param {type:\"string\"}\n",
        "# The dataset split to use.\n",
        "tfds_dataset_split = \"train\"  # @param {type:\"string\"}\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown or\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown `jsonl_dataset_file` can be your Cloud Storage path\n",
        "# @markdown such as `<BUCKET_URI>/<path-to-your-jsonl-file>` or a link to an online JSONL file\n",
        "# @markdown in which case the code here will download and then copy the file to `BUCKET_URI`.\n",
        "# @markdown If you want to upload a JSONL dataset to [Google Cloud Storage](https://cloud.google.com/storage) by yourself, then follow the instructions on [Upload objects from a filesystem](https://cloud.google.com/storage/docs/uploading-objects). Then, set `jsonl_dataset_file` to the `gs://` URI to your JSONL file such as `gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl`.\n",
        "\n",
        "jsonl_dataset_file = \"https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl\"  # @param {type:\"string\"}\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown Click `Show code` to see more details.\n",
        "\n",
        "assert (\n",
        "    tfds_dataset_name or jsonl_dataset_file\n",
        "), \"Please fill in either `tfds_dataset_name` or `jsonl_dataset_file`.\"\n",
        "assert not (\n",
        "    tfds_dataset_name and jsonl_dataset_file\n",
        "), \"Please fill in only one of `tfds_dataset_name` or `jsonl_dataset_file`.\"\n",
        "\n",
        "# Download the JSONL dataset.\n",
        "jsonl_dataset_uri_gcsfuse = \"\"\n",
        "if jsonl_dataset_file:\n",
        "    if jsonl_dataset_file.startswith(\"gs://\"):\n",
        "        # Using cloud storage location.\n",
        "        jsonl_dataset_uri = jsonl_dataset_file\n",
        "    else:\n",
        "        # Download the file and copy to cloud storage.\n",
        "        !wget -O dataset.jsonl $jsonl_dataset_file\n",
        "        jsonl_dataset_uri = f\"{BUCKET_URI}/dataset.jsonl\"\n",
        "        print(\"Copying dataset.jsonl to \", jsonl_dataset_uri)\n",
        "        !gsutil cp dataset.jsonl $jsonl_dataset_uri\n",
        "        print(\"JSONL url copied to: \", jsonl_dataset_uri)\n",
        "    jsonl_dataset_uri_gcsfuse = jsonl_dataset_uri.replace(\"gs://\", \"/gcs/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "s1bR_nk_yL5a"
      },
      "outputs": [],
      "source": [
        "# @title Finetune\n",
        "# @markdown Use the Vertex AI SDK to create and run the custom training jobs.\n",
        "# @markdown The training job uses TPU V3 8 cores and takes around 10 mins to\n",
        "# @markdown finish once it starts running.\n",
        "# @markdown Click `View backing custom job` link in the output of this cell to follow training job progress.\n",
        "# @markdown **Note that to make the training run faster, only a subset of dataset (2000 examples) is used here during fine tuning and the fine tuning runs for just one epoch. To improve the performance of the model, use more training samples, fine tune for more epochs and experiment with increasing the LoRA rank.**\n",
        "# @markdown Click `Show code` to see more details.\n",
        "\n",
        "if LOAD_MODEL_FROM != \"Kaggle\":\n",
        "    print(\"Skipped: Expect to load model from Kaggle, got\", LOAD_MODEL_FROM)\n",
        "else:\n",
        "    # The Gemma base model.\n",
        "    KAGGLE_MODEL_ID = \"gemma_2b_en\"  # @param[\"gemma_2b_en\", \"gemma_instruct_2b_en\", \"gemma_7b_en\", \"gemma_instruct_7b_en\", \"gemma_1.1_instruct_2b_en\", \"gemma_1.1_instruct_7b_en\"] {isTemplate:true}\n",
        "    # @markdown Set `num_train_subset_samples` as `-1` to use all the training samples.\n",
        "    num_train_subset_samples = 2000  # @param {type:\"integer\"}\n",
        "    # Number of train epochs.\n",
        "    num_epochs = 1  # @param{type:\"integer\"}\n",
        "    # Learning rate.\n",
        "    learning_rate = 5e-5  # @param{type:\"number\"}\n",
        "    # Weight decay.\n",
        "    weight_decay = 0.01  # @param{type:\"number\"}\n",
        "    # Input sequence length. It determines the memory required by the model.\n",
        "    input_sequence_length = 512  # @param{type:\"integer\"}\n",
        "    # LoRA rank.\n",
        "    lora_rank = 4  # @param{type:\"integer\"}\n",
        "    # Batch size for training.\n",
        "    train_batch_size = 2  # @param{type:\"integer\"}\n",
        "    # The KerasNLP checkpoint filename.\n",
        "    # Note: Do not add folder name here.\n",
        "    checkpoint_filename = \"fine_tuned.weights.h5\"  # @param{type:\"string\"}\n",
        "\n",
        "    # Worker pool spec.\n",
        "    machine_type = \"cloud-tpu\"\n",
        "    # NOTE: The models have been test only with 8 cores.\n",
        "    accelerator_type = \"TPU_V3\"\n",
        "    # Number of TPU cores.\n",
        "    accelerator_count = 8\n",
        "    # Set model parallelism related parameters for 8 cores.\n",
        "    model_parallel_batch_dim = 1\n",
        "    model_parallel_model_dim = 8\n",
        "\n",
        "    replica_count = 1\n",
        "\n",
        "    check_quota(\n",
        "        project_id=PROJECT_ID,\n",
        "        region=REGION,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        is_for_training=True,\n",
        "    )\n",
        "\n",
        "    # Setup training job.\n",
        "    job_name = get_job_name_with_datetime(\"gemma-keras-lora-train\")\n",
        "\n",
        "    # Pass training arguments and launch job.\n",
        "    train_job = aiplatform.CustomContainerTrainingJob(\n",
        "        display_name=job_name,\n",
        "        container_uri=KERAS_TRAIN_DOCKER_URI,\n",
        "    )\n",
        "\n",
        "    # Create a GCS folder to save the finetuned model.\n",
        "    output_folder = os.path.join(BUCKET_URI, job_name)\n",
        "    output_folder_gcsfuse = output_folder.replace(\"gs://\", \"/gcs/\")\n",
        "\n",
        "    train_job.run(\n",
        "        args=[\n",
        "            f\"--model_type={KAGGLE_MODEL_ID}\",\n",
        "            f\"--num_epochs={num_epochs}\",\n",
        "            f\"--learning_rate={learning_rate}\",\n",
        "            f\"--weight_decay={weight_decay}\",\n",
        "            f\"--input_sequence_length={input_sequence_length}\",\n",
        "            f\"--lora_rank={lora_rank}\",\n",
        "            f\"--model_parallel_batch_dim={model_parallel_batch_dim}\",\n",
        "            f\"--model_parallel_model_dim={model_parallel_model_dim}\",\n",
        "            f\"--tfds_dataset_name={tfds_dataset_name}\",\n",
        "            f\"--tfds_dataset_split={tfds_dataset_split}\",\n",
        "            f\"--jsonl_dataset_file={jsonl_dataset_uri_gcsfuse}\",\n",
        "            f\"--template={template}\",\n",
        "            f\"--train_batch_size={train_batch_size}\",\n",
        "            f\"--num_train_subset_samples={num_train_subset_samples}\",\n",
        "            f\"--output_folder={output_folder_gcsfuse}\",\n",
        "            f\"--checkpoint_filename={checkpoint_filename}\",\n",
        "        ],\n",
        "        environment_variables={\n",
        "            \"KAGGLE_USERNAME\": KAGGLE_USERNAME,\n",
        "            \"KAGGLE_KEY\": KAGGLE_KEY,\n",
        "        },\n",
        "        replica_count=replica_count,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        service_account=SERVICE_ACCOUNT,\n",
        "    )\n",
        "\n",
        "    print(\"Trained model is saved in: \", output_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "h2xEEcgsAQsb"
      },
      "outputs": [],
      "source": [
        "# @title Convert model\n",
        "# @markdown Convert the KerasNLP model checkpoint to Hex-LLM format.\n",
        "# @markdown  Use the Vertex AI SDK to create and run the custom job.\n",
        "# @markdown Click `View backing custom job` link in the output of this cell to follow job progress.\n",
        "# @markdown  The jobs takes around 6 mins to finish.\n",
        "# @markdown  Click `Show code` to see more details.\n",
        "\n",
        "if LOAD_MODEL_FROM != \"Kaggle\":\n",
        "    print(\"Skipped: Expect to load model from Kaggle, got\", LOAD_MODEL_FROM)\n",
        "else:\n",
        "    model_type_to_size = {\n",
        "        \"gemma_2b_en\": \"2b\",\n",
        "        \"gemma_instruct_2b_en\": \"2b\",\n",
        "        \"gemma_7b_en\": \"7b\",\n",
        "        \"gemma_instruct_7b_en\": \"7b\",\n",
        "        \"gemma_1.1_instruct_2b_en\": \"2b\",\n",
        "        \"gemma_1.1_instruct_7b_en\": \"7b\",\n",
        "    }\n",
        "    size = model_type_to_size[KAGGLE_MODEL_ID]\n",
        "    # NOTE: The Hex-LLM serving code looks for model type tag in\n",
        "    # the checkpoint filename.\n",
        "    if \"_2b_\" in KAGGLE_MODEL_ID:\n",
        "        model_type_to_file_suffix = \"_gemma-2b.ckpt\"\n",
        "    else:\n",
        "        assert (\n",
        "            \"_7b_\" in KAGGLE_MODEL_ID\n",
        "        ), \"KAGGLE_MODEL_ID should contain `_2b_` or `_7b_`.\"\n",
        "        model_type_to_file_suffix = \"_gemma-7b.ckpt\"\n",
        "    hexllm_checkpoint_file = \"finetuned_hexllm\" + model_type_to_file_suffix\n",
        "\n",
        "    # Worker pool spec.\n",
        "    machine_type = \"n1-highmem-16\"\n",
        "\n",
        "    replica_count = 1\n",
        "\n",
        "    # Setup training job.\n",
        "    job_name = get_job_name_with_datetime(\"gemma-keras-model-conversion\")\n",
        "\n",
        "    # Pass training arguments and launch job.\n",
        "    conversion_job = aiplatform.CustomContainerTrainingJob(\n",
        "        display_name=job_name,\n",
        "        container_uri=KERAS_MODEL_CONVERSION_DOCKER_URI,\n",
        "    )\n",
        "\n",
        "    conversion_job.run(\n",
        "        args=[\n",
        "            f\"--weights_file={output_folder_gcsfuse}/{checkpoint_filename}\",\n",
        "            f\"--size={size}\",\n",
        "            f\"--output_file={output_folder_gcsfuse}/{hexllm_checkpoint_file}\",\n",
        "        ],\n",
        "        environment_variables={\n",
        "            \"KAGGLE_USERNAME\": KAGGLE_USERNAME,\n",
        "            \"KAGGLE_KEY\": KAGGLE_KEY,\n",
        "        },\n",
        "        replica_count=replica_count,\n",
        "        machine_type=machine_type,\n",
        "        service_account=SERVICE_ACCOUNT,\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        \"Converted Hexllm checkpoint is saved in: \",\n",
        "        output_folder + \"/\" + hexllm_checkpoint_file,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VJkzxF9tyL5a"
      },
      "outputs": [],
      "source": [
        "# @title Deploy\n",
        "# @markdown This section uploads the model to Model Registry and deploys it on the Endpoint. It takes 15 minutes to 1 hour to finish.\n",
        "# @markdown **Hex-LLM** is a **H**igh-**E**fficiency **L**arge **L**anguage **M**odel (LLM) TPU serving solution built with **XLA**, which is being developed by Google Cloud. This notebook uses TPU v5e machines. Click `Show code` to see more details.\n",
        "\n",
        "# @markdown Region to deploy the model on TPU.\n",
        "TPU_DEPLOYMENT_REGION = \"us-west1\"  # @param [\"us-west1\"] {isTemplate:true}\n",
        "\n",
        "if LOAD_MODEL_FROM != \"Kaggle\":\n",
        "    print(\"Skipped: Expect to load model from Kaggle, got\", LOAD_MODEL_FROM)\n",
        "else:\n",
        "    if \"2b\" in KAGGLE_MODEL_ID:\n",
        "        # Sets ct5lp-hightpu-1t (1 TPU chip) to deploy Gemma 2B models.\n",
        "        machine_type = \"ct5lp-hightpu-1t\"\n",
        "        accelerator_type = \"TPU_V5e\"\n",
        "        # Note: 1 TPU-V5e chip has only 1 core.\n",
        "        accelerator_count = 1\n",
        "    else:\n",
        "        # Sets ct5lp-hightpu-4t (4 TPU chips) to deploy Gemma 7B models.\n",
        "        machine_type = \"ct5lp-hightpu-4t\"\n",
        "        accelerator_type = \"TPU_V5e\"\n",
        "        # Note: 1 TPU-V5e chip has only 1 core.\n",
        "        accelerator_count = 4\n",
        "\n",
        "    check_quota(\n",
        "        project_id=PROJECT_ID,\n",
        "        region=REGION,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        is_for_training=False,\n",
        "    )\n",
        "\n",
        "    # Note that a larger max_num_batched_tokens will require more TPU memory.\n",
        "    max_num_batched_tokens = 11264\n",
        "    # Multiple of tokens for padding alignment. A higher value can reduce\n",
        "    # re-compilation but can also increase the waste in computation.\n",
        "    tokens_pad_multiple = 1024\n",
        "    # Multiple of sequences for padding alignment. A higher value can reduce\n",
        "    # re-compilation but can also increase the waste in computation.\n",
        "    seqs_pad_multiple = 32\n",
        "\n",
        "    print(\"Using model from: \", output_folder)\n",
        "    model, endpoint = deploy_model_hexllm(\n",
        "        model_name=get_job_name_with_datetime(prefix=\"gemma-serve-hexllm\"),\n",
        "        base_model_id=f\"google/{KAGGLE_MODEL_ID}\",\n",
        "        model_id=output_folder,\n",
        "        service_account=SERVICE_ACCOUNT,\n",
        "        machine_type=machine_type,\n",
        "        max_num_batched_tokens=max_num_batched_tokens,\n",
        "        tokens_pad_multiple=tokens_pad_multiple,\n",
        "        seqs_pad_multiple=seqs_pad_multiple,\n",
        "    )\n",
        "    print(\"endpoint_name:\", endpoint.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lgJPw3e9yL5a"
      },
      "outputs": [],
      "source": [
        "# @title Predict\n",
        "# @markdown Once deployment succeeds, you can send requests to the endpoint with text prompts based on your `template`.\n",
        "\n",
        "# @markdown Additionally, you can moderate the generated text with Vertex AI. See [Moderate text documentation](https://cloud.google.com/natural-language/docs/moderating-text) for more details.\n",
        "\n",
        "# Loads an existing endpoint instance using the endpoint name:\n",
        "# - Using `endpoint_name = endpoint.name` allows us to get the\n",
        "#   endpoint name of the endpoint `endpoint` created in the cell\n",
        "#   above.\n",
        "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
        "#   an existing endpoint with the ID 1234567890123456789.\n",
        "# You may uncomment the code below to load an existing endpoint.\n",
        "\n",
        "# endpoint_name = \"\"  # @param {type:\"string\"}\n",
        "# aip_endpoint_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
        "# )\n",
        "# endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
        "\n",
        "# If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`,\n",
        "# you can reduce the max length, such as set max_tokens as 20.\n",
        "\n",
        "# @markdown **Note:** The following input corresponds to the default `template` set for the `databricks-dolly-15k` which uses `instruction` and `response` keys.\n",
        "# @markdown   If you modify the `template` or use another dataset, then modify the `prompt` accordingly. For example for the `imdb_reviews` dataset  where `template = \"{text}\"`, set `prompt = \"Inception is \"`.\n",
        "\n",
        "if LOAD_MODEL_FROM != \"Kaggle\":\n",
        "    print(\"Skipped: Expect to load model from Kaggle, got\", LOAD_MODEL_FROM)\n",
        "else:\n",
        "    prompt = \"Instruction: What should I do on a trip to Europe? Response: \"  # @param {type: \"string\"}\n",
        "    # Overides max_tokens and top_k parameters during inferences.\n",
        "    # If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`,\n",
        "    # you can reduce the max length, such as set max_tokens as 20.\n",
        "    max_tokens = 50  # @param {type:\"integer\"}\n",
        "    temperature = 1.0  # @param {type:\"number\"}\n",
        "    top_p = 1.0  # @param {type:\"number\"}\n",
        "    top_k = 1.0  # @param {type:\"number\"}\n",
        "\n",
        "    # @markdown **Note that the first few prompts will take longer to execute.**\n",
        "\n",
        "    instances = [\n",
        "        {\n",
        "            \"prompt\": prompt,\n",
        "            \"max_tokens\": max_tokens,\n",
        "            \"temperature\": temperature,\n",
        "            \"top_p\": top_p,\n",
        "            \"top_k\": top_k,\n",
        "        },\n",
        "    ]\n",
        "    prediction_response = endpoint.predict(instances=instances)\n",
        "\n",
        "    for prediction in prediction_response.predictions:\n",
        "        print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af21a3cff1e0"
      },
      "source": [
        "## Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "911406c1561e"
      },
      "outputs": [],
      "source": [
        "# Delete the train job.\n",
        "train_job.delete()\n",
        "\n",
        "# Delete the conversion job.\n",
        "if conversion_job:\n",
        "    conversion_job.delete()\n",
        "\n",
        "# Undeploy model and delete endpoint.\n",
        "endpoint.delete(force=True)\n",
        "\n",
        "# Delete models.\n",
        "model.delete()\n",
        "\n",
        "# Delete Cloud Storage bucket that was created.\n",
        "if BUCKET_URI == f\"gs://{PROJECT_ID}-tmp-{now}\":\n",
        "    ! gsutil -m rm -r $BUCKET_URI"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_gemma_finetuning_on_vertex.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

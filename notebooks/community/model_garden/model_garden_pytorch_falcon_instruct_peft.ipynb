{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d9bbf86da5e"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Vertex AI Model Garden - Falcon Instruct (PEFT)\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_falcon_instruct_peft.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_falcon_instruct_peft.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_pytorch_falcon_instruct_peft.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "Open in Vertex AI Workbench\n",
        "    </a> (A Python-3 GPU notebook is recommended)\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates running inferences locally with prebuilt Falcon Instruct models, deploying prebuilt Falcon Instruct models, finetuning and deploying Falcon Instruct models with performance efficient finetuning libraries ([PEFT](https://github.com/huggingface/peft)), quantizing and deploying Falcon Instruct models with [GPTQ](https://arxiv.org/abs/2210.17323), and evaluating PEFT-finetuned Falcon Instruct models in Vertex AI.\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Run inferences locally on prebuilt Falcon Instruct models\n",
        "- Deploy prebuilt Falcon Instruct models\n",
        "- Finetune and deploy Falcon Instruct models with PEFT\n",
        "- Quantize and deploy Falcon Instruct models with GPTQ\n",
        "- Evaluate PEFT-finetuned Falcon Instruct models\n",
        "\n",
        "| Models | LoRA |\n",
        "| :- | :- |\n",
        "| [tiiuae/falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct) | Y |\n",
        "| [tiiuae/falcon-40b-instruct](https://huggingface.co/tiiuae/falcon-40b-instruct) | Y |\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "264c07757582"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "**NOTE**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands.\n",
        "\n",
        "Running inferences locally with Falcon Instruct models requires a GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioensNKM8ned"
      },
      "source": [
        "### Colab only\n",
        "Run the following commands for Colab and skip this section if you are using Workbench."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2707b02ef5df"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    ! pip3 install --upgrade google-cloud-aiplatform\n",
        "    ! pip3 install google-cloud-language==2.10.0\n",
        "    from google.colab import auth as google_auth\n",
        "\n",
        "    google_auth.authenticate_user()\n",
        "\n",
        "    # Restart the notebook kernel after installs.\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)\n",
        "! pip3 install transformers==4.31.0\n",
        "! pip3 install einops==0.6.1\n",
        "! pip3 install accelerate==0.21.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb7adab99e41"
      },
      "source": [
        "### Setup Google Cloud project\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "1. [Enable the Vertex AI API, Compute Engine API, and Cloud Natural Language API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component,language.googleapis.com).\n",
        "\n",
        "1. [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs.\n",
        "\n",
        "1. [Create a service account](https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console) with `Vertex AI User` and `Storage Object Admin` roles for deploying fine tuned model to Vertex AI endpoint."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c460088b873"
      },
      "source": [
        "Set the following variables for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`) should be located in the specified region (`REGION`). Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "855d6b96f291"
      },
      "outputs": [],
      "source": [
        "# Cloud project id.\n",
        "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# The region you want to launch jobs in.\n",
        "REGION = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# The Cloud Storage bucket for storing experiments output with gs:// prefix.\n",
        "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
        "\n",
        "! gcloud config set project $PROJECT_ID\n",
        "\n",
        "import os\n",
        "\n",
        "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
        "EXPERIMENT_BUCKET = os.path.join(BUCKET_URI, \"peft\")\n",
        "DATA_BUCKET = os.path.join(EXPERIMENT_BUCKET, \"data\")\n",
        "MODEL_BUCKET = os.path.join(EXPERIMENT_BUCKET, \"model\")\n",
        "\n",
        "# The service account looks like:\n",
        "# '@.iam.gserviceaccount.com'\n",
        "# Please go to https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console\n",
        "# and create service account with `Vertex AI User` and `Storage Object Admin` roles.\n",
        "# The service account for deploying fine tuned model.\n",
        "SERVICE_ACCOUNT = \"\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e828eb320337"
      },
      "source": [
        "### Initialize Vertex AI API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12cd25839741"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cc825514deb"
      },
      "source": [
        "### Define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b42bd4fa2b2d"
      },
      "outputs": [],
      "source": [
        "# The pre-built training, serving and evaluation docker images.\n",
        "TRAIN_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-train:20231207_0936_RC00\"\n",
        "PREDICTION_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-serve:20231129_0948_RC00\"\n",
        "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20231127_0916_RC00\"\n",
        "VLLM_GPTQ_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:gptq\"\n",
        "EVAL_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-lm-evaluation-harness:20231011_0934_RC00\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c250872074f"
      },
      "source": [
        "### Define common functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "354da31189dc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "from typing import Tuple\n",
        "\n",
        "from google.cloud import aiplatform, language\n",
        "\n",
        "\n",
        "def get_job_name_with_datetime(prefix: str) -> str:\n",
        "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
        "    jobs in Vertex AI.\n",
        "    \"\"\"\n",
        "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
        "\n",
        "\n",
        "def deploy_model(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    finetuned_lora_model_path: str,\n",
        "    service_account: str,\n",
        "    task: str,\n",
        "    machine_type: str = \"n1-standard-8\",\n",
        "    accelerator_type: str = \"NVIDIA_TESLA_V100\",\n",
        "    accelerator_count: int = 1,\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys trained models into Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
        "    serving_env = {\n",
        "        \"MODEL_ID\": model_id,\n",
        "        \"TASK\": task,\n",
        "    }\n",
        "    if finetuned_lora_model_path:\n",
        "        serving_env[\"FINETUNED_LORA_MODEL_PATH\"] = finetuned_lora_model_path\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=PREDICTION_DOCKER_URI,\n",
        "        serving_container_ports=[7080],\n",
        "        serving_container_predict_route=\"/predictions/peft_serving\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "        serving_container_environment_variables=serving_env,\n",
        "    )\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=service_account,\n",
        "    )\n",
        "    return model, endpoint\n",
        "\n",
        "\n",
        "def deploy_model_vllm(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    service_account: str,\n",
        "    machine_type: str = \"n1-standard-8\",\n",
        "    accelerator_type: str = \"NVIDIA_TESLA_V100\",\n",
        "    accelerator_count: int = 1,\n",
        "    quantization_method: str = \"\",\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
        "\n",
        "    vllm_args = [\n",
        "        \"--host=0.0.0.0\",\n",
        "        \"--port=7080\",\n",
        "        f\"--model={model_id}\",\n",
        "        f\"--tensor-parallel-size={accelerator_count}\",\n",
        "        \"--swap-space=16\",\n",
        "        \"--gpu-memory-utilization=0.9\",\n",
        "        \"--disable-log-stats\",\n",
        "        \"--dtype=float16\",\n",
        "        \"--trust-remote-code\",\n",
        "    ]\n",
        "    if quantization_method:\n",
        "        vllm_args.append(f\"--quantization={quantization_method}\")\n",
        "    if quantization_method == \"gptq\":\n",
        "        vllm_docker_uri = VLLM_GPTQ_DOCKER_URI\n",
        "    else:\n",
        "        vllm_docker_uri = VLLM_DOCKER_URI\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=vllm_docker_uri,\n",
        "        serving_container_command=[\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
        "        serving_container_args=vllm_args,\n",
        "        serving_container_ports=[7080],\n",
        "        serving_container_predict_route=\"/generate\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "    )\n",
        "\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=service_account,\n",
        "    )\n",
        "    return model, endpoint\n",
        "\n",
        "\n",
        "def moderate_text(text: str) -> language.ModerateTextResponse:\n",
        "    \"\"\"Calls Vertex AI APIs to analyze text moderations.\"\"\"\n",
        "    client = language.LanguageServiceClient()\n",
        "    document = language.Document(\n",
        "        content=text,\n",
        "        type_=language.Document.Type.PLAIN_TEXT,\n",
        "    )\n",
        "    return client.moderate_text(document=document)\n",
        "\n",
        "\n",
        "def show_text_moderation(text: str, response: language.ModerateTextResponse) -> None:\n",
        "    \"\"\"Shows text moderation results.\"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    def confidence(category: language.ClassificationCategory) -> float:\n",
        "        return category.confidence\n",
        "\n",
        "    columns = [\"category\", \"confidence\"]\n",
        "    categories = sorted(response.moderation_categories, key=confidence, reverse=True)\n",
        "    data = ((category.name, category.confidence) for category in categories)\n",
        "    df = pd.DataFrame(columns=columns, data=data)\n",
        "\n",
        "    print(f\"Text analyzed:\\n{text}\")\n",
        "    print(df.to_markdown(index=False, tablefmt=\"presto\", floatfmt=\".0%\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06e73cb3f412"
      },
      "source": [
        "## Run inferences locally with prebuilt Falcon Instruct models\n",
        "\n",
        "You will need at least 16GB of memory to swiftly run inference with Falcon-7B-Instruct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ea64305957f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model = \"tiiuae/falcon-7b-instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "sequences = pipeline(\n",
        "    \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Girafatron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n",
        "    max_length=200,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8neJc8CnDDpu"
      },
      "source": [
        "## Deploy prebuilt Falcon Instruct models\n",
        "\n",
        "This section deploys prebuilt Falcon Instruct models on the Endpoint. The model deployment step will take 15 minutes to 40 minutes to complete.\n",
        "\n",
        "The peak GPU memory usages for [tiiuae/falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct), and [tiiuae/falcon-40b-instruct](https://huggingface.co/tiiuae/falcon-40b-instruct) are ~15.5G and ~84G separately with the default settings. Please adjust the machine type, accelerator type and accelerator count accordingly. We use V100 in deployments as an example. Note that V100 serving generally offers better throughput and latency performance than L4 serving, while L4 serving is generally more cost efficient than V100 serving. The serving efficiency of V100 and L4 GPUs is inferior to that of A100 GPUs, but V100 and L4 GPUs are nevertheless good serving solutions if you do not have A100 quota."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MjaORIIFDVu"
      },
      "source": [
        "Set the prebuilt model id."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8OiHHNNE_wj"
      },
      "outputs": [],
      "source": [
        "prebuilt_model_id = \"tiiuae/falcon-7b-instruct\"  # @param [\"tiiuae/falcon-7b-instruct\", \"tiiuae/falcon-40b-instruct\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHFW7yvjaVFV"
      },
      "source": [
        "We use the PEFT serving images to deploy prebuilt Falcon Instruct models, by setting finetuning LoRA model paths as empty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uak1pyEeExYM"
      },
      "outputs": [],
      "source": [
        "# Find Vertex AI supported accelerators and regions in:\n",
        "#  https://cloud.google.com/vertex-ai/docs/predictions/configure-compute\n",
        "\n",
        "# Sets V100 (16G) to deploy tiiuae/falcon-7b-instruct or tiiuae/falcon-40b-instruct.\n",
        "# If A100 is not available, you may deploy tiiuae/falcon-40b-instruct with\n",
        "#  multiple V100s. Please keep in mind that the efficiency of serving with\n",
        "#  multiple V100s is inferior to that of serving with A100s.\n",
        "# Compared with L4, V100 serving can have better throughput and latency.\n",
        "machine_type = \"n1-standard-8\"\n",
        "accelerator_type = \"NVIDIA_TESLA_V100\"\n",
        "accelerator_count = 1  # for tiiuae/falcon-7b-instruct\n",
        "# accelerator_count = 8  # for tiiuae/falcon-40b-instruct\n",
        "\n",
        "# Sets L4 (24G) to deploy tiiuae/falcon-7b-instruct or tiiuae/falcon-40b-instruct.\n",
        "# If A100 is not available, you may deploy tiiuae/falcon-40b-instruct with\n",
        "#  multiple L4s. Please keep in mind that the efficiency of serving with\n",
        "#  multiple L4s is inferior to that of serving with A100s.\n",
        "# Compared with V100, L4 serving can be more cost efficient.\n",
        "\n",
        "# For tiiuae/falcon-7b-instruct.\n",
        "# machine_type = \"g2-standard-8\"\n",
        "# accelerator_type = \"NVIDIA_L4\"\n",
        "# accelerator_count = 1\n",
        "\n",
        "# For tiiuae/falcon-40b-instruct.\n",
        "# machine_type = \"g2-standard-48\"\n",
        "# accelerator_type = \"NVIDIA_L4\"\n",
        "# accelerator_count = 4\n",
        "\n",
        "# Sets A100 (40G) to deploy tiiuae/falcon-7b-instruct or tiiuae/falcon-40b-instruct.\n",
        "# machine_type = \"a2-highgpu-1g\"\n",
        "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
        "# accelerator_count = 1  # for tiiuae/falcon-7b-instruct\n",
        "# accelerator_count = 4  # for tiiuae/falcon-40b-instruct\n",
        "\n",
        "# Sets A100 (80G) to deploy falcon-40b-instruct models for faster inferences.\n",
        "# machine_type = \"a2-ultragpu-1g\"\n",
        "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
        "# accelerator_count = 2\n",
        "\n",
        "model_without_peft, endpoint_without_peft = deploy_model(\n",
        "    model_name=get_job_name_with_datetime(prefix=\"falcon-instruct-serve\"),\n",
        "    model_id=prebuilt_model_id,\n",
        "    finetuned_lora_model_path=\"\",  # This will avoid override finetuning models.\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    task=\"instruct-lora\",\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        ")\n",
        "print(\"endpoint_name:\", endpoint_without_peft.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGKIjgmDFRW2"
      },
      "source": [
        "NOTE: The prebuilt model weights will be downloaded on the fly from the original location after the deployment succeeds. Thus, an additional 10-30 minutes of waiting time is needed **after** the above model deployment step succeeds and before you can run the next step below. Otherwise you might see a `ServiceUnavailable: 503 502:Bad Gateway` error when you send requests to the endpoint.\n",
        "\n",
        "Once deployment succeeds, you can send requests to the endpoint with text prompts.\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "Human: What is a car?\n",
        "Assistant:  A car, or a motor car, is a road-connected human-transportation system used to move people or goods from one place to another. The term also encompasses a wide range of vehicles, including motorboats, trains, and aircrafts. Cars typically have four wheels, a cabin for passengers, and an engine or motor. They have been around since the early 19th century and are now one of the most popular forms of transportation, used for daily commuting, shopping, and other purposes.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDHsCOqvFYBi"
      },
      "outputs": [],
      "source": [
        "# Loads an existing endpoint instance using the endpoint name:\n",
        "# - Using `endpoint_name = endpoint_without_peft.name` allows us to get the\n",
        "#   endpoint name of the endpoint `endpoint_without_peft` created in the cell\n",
        "#   above.\n",
        "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
        "#   an existing endpoint with the ID 1234567890123456789.\n",
        "# You may uncomment the code below to load an existing endpoint.\n",
        "\n",
        "# endpoint_name = endpoint_without_peft.name\n",
        "# # endpoint_name = \"\"  # @param {type:\"string\"}\n",
        "# aip_endpoint_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
        "# )\n",
        "# endpoint_without_peft = aiplatform.Endpoint(aip_endpoint_name)\n",
        "\n",
        "instances = [\n",
        "    {\n",
        "        \"prompt\": \"What is a car?\",\n",
        "        \"max_tokens\": 50,\n",
        "        \"temperature\": 1.0,\n",
        "        \"top_p\": 1.0,\n",
        "        \"top_k\": 10,\n",
        "    },\n",
        "]\n",
        "response = endpoint_without_peft.predict(instances=instances)\n",
        "\n",
        "for prediction in response.predictions:\n",
        "    print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e70e3519ff8b"
      },
      "source": [
        "## Finetune and deploy Falcon Instruct models with PEFT\n",
        "\n",
        "This section demonstrates how to finetune and deploy Falcon Instruct models with PEFT LoRA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qCrm_kJH5cz"
      },
      "source": [
        "Set the base model id."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3UBLiYrM3sU"
      },
      "outputs": [],
      "source": [
        "model_id = \"tiiuae/falcon-7b-instruct\"  # @param [\"tiiuae/falcon-7b-instruct\", \"tiiuae/falcon-40b-instruct\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWGwJHqI7LMs"
      },
      "source": [
        "### Finetune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKEYoRfiHDVv"
      },
      "source": [
        "Use the Vertex AI SDK to create and run the custom training jobs with Vertex AI Model Garden training images.\n",
        "\n",
        "This example uses the dataset [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco). You can either use a [dataset from huggingface](https://huggingface.co/datasets) or a custom JSONL dataset in [Vertex text model dataset format](https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-supervised#dataset-format) stored in Cloud Storage. The `template` parameter is optional.\n",
        "\n",
        "The peak GPU memory usages are ~11G and ~34G for finetuning LoRA models for [tiiuae/falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct), and [tiiuae/falcon-40b-instruct](https://huggingface.co/tiiuae/falcon-40b-instruct) separately with default training parameters and the example dataset. Falcon-7b-instruct can be finetuned on 1 P100/V100, and falcon-40b-instruct can be finetuned on 1 A100 (40G)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d272b9f416a9"
      },
      "source": [
        "#### [Optional] Finetune with a custom dataset\n",
        "\n",
        "To use a custom dataset, you should supply a `gs://` URI to a JSONL file in [Vertex text model dataset format](https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-supervised#dataset-format) in the `dataset_name` below.\n",
        "\n",
        "For example, here is one data point from the sample dataset `gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl`:\n",
        "\n",
        "```json\n",
        "{\"input_text\":\"TRANSCRIPT: \\nREASON FOR EVALUATION:,\\n\\n LABEL:\",\"output_text\":\"Chiropractic\"}\n",
        "```\n",
        "\n",
        "To use this sample dataset that contains `input_text` and `output_text` fields, set `dataset_name` to `gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl` and `template` to `vertex_sample`. For advanced usage with custom datatset fields, see [the template example](https://github.com/tloen/alpaca-lora/blob/main/templates/alpaca.json) and supply your own JSON template as `gs://` URIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65467b361315"
      },
      "outputs": [],
      "source": [
        "# Huggingface dataset name or gs:// URI to a custom JSONL dataset.\n",
        "dataset_name = \"timdettmers/openassistant-guanaco\"  # @param {type:\"string\"}\n",
        "# Optional. Template name or gs:// URI to a custom template.\n",
        "template = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# Worker pool spec.\n",
        "# Find Vertex AI supported accelerators and regions in:\n",
        "#  https://cloud.google.com/vertex-ai/docs/training/configure-compute\n",
        "\n",
        "# Uses V100 (16G) to finetune falcon-7b-instruct.\n",
        "machine_type = \"n1-standard-8\"\n",
        "accelerator_type = \"NVIDIA_TESLA_V100\"\n",
        "accelerator_count = 1\n",
        "\n",
        "# Uses L4 (24G) to finetune falcon-7b-instruct.\n",
        "# machine_type = \"g2-standard-8\"\n",
        "# accelerator_type = \"NVIDIA_L4\"\n",
        "# accelerator_count = 1\n",
        "\n",
        "# Uses V100 (16G) to finetune falcon-40b-instruct.\n",
        "# machine_type = \"n1-standard-16\"\n",
        "# accelerator_type = \"NVIDIA_TESLA_V100\"\n",
        "# accelerator_count = 4\n",
        "\n",
        "# Uses L4 (24G) to finetune falcon-40b-instruct.\n",
        "# machine_type = \"g2-standard-24\"\n",
        "# accelerator_type = \"NVIDIA_L4\"\n",
        "# accelerator_count = 2\n",
        "\n",
        "# Uses A100 (40G) to finetune falcon-40b-instruct.\n",
        "# machine_type = \"a2-highgpu-1g\"\n",
        "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
        "# accelerator_count = 1\n",
        "\n",
        "replica_count = 1\n",
        "\n",
        "# Setup training job.\n",
        "job_name = get_job_name_with_datetime(\"falcon-instruct-lora-train\")\n",
        "train_job = aiplatform.CustomContainerTrainingJob(\n",
        "    display_name=job_name,\n",
        "    container_uri=TRAIN_DOCKER_URI,\n",
        ")\n",
        "output_dir = os.path.join(MODEL_BUCKET, job_name)\n",
        "output_dir_gcsfuse = output_dir.replace(\"gs://\", \"/gcs/\")\n",
        "\n",
        "# Pass training arguments and launch job.\n",
        "max_steps = 10\n",
        "train_job.run(\n",
        "    args=[\n",
        "        \"--task=instruct-lora\",\n",
        "        f\"--pretrained_model_id={model_id}\",\n",
        "        f\"--dataset_name={dataset_name}\",\n",
        "        f\"--output_dir={output_dir_gcsfuse}\",\n",
        "        \"--lora_rank=64\",\n",
        "        \"--lora_alpha=16\",\n",
        "        \"--lora_dropout=0.1\",\n",
        "        \"--warmup_ratio=0.03\",\n",
        "        f\"--max_steps={max_steps}\",\n",
        "        \"--max_seq_length=512\",\n",
        "        \"--learning_rate=2e-4\",\n",
        "        f\"--template={template}\",\n",
        "    ],\n",
        "    replica_count=replica_count,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    boot_disk_size_gb=500,\n",
        ")\n",
        "\n",
        "print(\"Trained models were saved in: \", output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqmCtkGnhDmp"
      },
      "source": [
        "### Deploy\n",
        "This section uploads the model to Model Registry and deploys it on the Endpoint.\n",
        "\n",
        "The model deployment step will take 15 minutes to 40 minutes to complete.\n",
        "\n",
        "The peak GPU memory usages for [tiiuae/falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct), and [tiiuae/falcon-40b-instruct](https://huggingface.co/tiiuae/falcon-40b-instruct) with LoRA weights are ~15.5G and ~84G separately with the default settings. Please adjust the machine type, accelerator type and accelerator count accordingly. We use V100 in deployments as an example. Note that V100 serving generally offers better throughput and latency performance than L4 serving, while L4 serving is generally more cost efficient than V100 serving. The serving efficiency of V100 and L4 GPUs is inferior to that of A100 GPUs, but V100 and L4 GPUs are nevertheless good serving solutions if you do not have A100 quota."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bf55e38815dc"
      },
      "outputs": [],
      "source": [
        "# Find Vertex AI supported accelerators and regions in:\n",
        "#  https://cloud.google.com/vertex-ai/docs/predictions/configure-compute\n",
        "\n",
        "# Sets V100 (16G) to deploy tiiuae/falcon-7b-instruct or tiiuae/falcon-40b-instruct.\n",
        "# If A100 is not available, you may deploy tiiuae/falcon-40b-instruct with\n",
        "#  multiple V100s. Please keep in mind that the efficiency of serving with\n",
        "#  multiple V100s is inferior to that of serving with A100s.\n",
        "# Compared with L4, V100 serving can have better throughput and latency.\n",
        "machine_type = \"n1-standard-8\"\n",
        "accelerator_type = \"NVIDIA_TESLA_V100\"\n",
        "accelerator_count = 1  # for tiiuae/falcon-7b-instruct\n",
        "# accelerator_count = 8  # for tiiuae/falcon-40b-instruct\n",
        "\n",
        "# Sets L4 (24G) to deploy tiiuae/falcon-7b-instruct or tiiuae/falcon-40b-instruct.\n",
        "# If A100 is not available, you may deploy tiiuae/falcon-40b-instruct with\n",
        "#  multiple L4s. Please keep in mind that the efficiency of serving with\n",
        "#  multiple L4s is inferior to that of serving with A100s.\n",
        "# Compared with V100, L4 serving can be more cost efficient.\n",
        "\n",
        "# For tiiuae/falcon-7b-instruct.\n",
        "# machine_type = \"g2-standard-8\"\n",
        "# accelerator_type = \"NVIDIA_L4\"\n",
        "# accelerator_count = 1\n",
        "\n",
        "# For tiiuae/falcon-40b-instruct.\n",
        "# machine_type = \"g2-standard-48\"\n",
        "# accelerator_type = \"NVIDIA_L4\"\n",
        "# accelerator_count = 4\n",
        "\n",
        "# Sets A100 (40G) to deploy tiiuae/falcon-7b-instruct or tiiuae/falcon-40b-instruct.\n",
        "# machine_type = \"a2-highgpu-1g\"\n",
        "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
        "# accelerator_count = 1  # for tiiuae/falcon-7b-instruct\n",
        "# accelerator_count = 4  # for tiiuae/falcon-40b-instruct\n",
        "\n",
        "# Sets A100 (80G) to deploy falcon-40b-instruct models for faster inferences.\n",
        "# machine_type = \"a2-ultragpu-1g\"\n",
        "# accelerator_type = \"NVIDIA_TESLA_A100_80GB\"\n",
        "# accelerator_count = 2\n",
        "\n",
        "model_with_peft, endpoint_with_peft = deploy_model(\n",
        "    model_name=get_job_name_with_datetime(prefix=\"falcon-instruct-peft-serve\"),\n",
        "    model_id=model_id,\n",
        "    finetuned_lora_model_path=os.path.join(output_dir, \"checkpoint-\" + str(max_steps)),\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    task=\"instruct-lora\",\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        ")\n",
        "print(\"endpoint_name:\", endpoint_with_peft.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80b3fd2ace09"
      },
      "source": [
        "NOTE: After the deployment succeeds, the base model weights will be downloaded one the fly from the original location and LoRA model weights will be downloaded from the GCS bucket used in training above. Thus, an additional 10-30 minutes of waiting time is needed **after** the above model deployment step succeeds and before you can run the next step below. Otherwise you might see a `ServiceUnavailable: 503 502:Bad Gateway` error when you send requests to the endpoint.\n",
        "\n",
        "Once deployment succeeds, you can send requests to the endpoint with text prompts.\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "Human: What is a car?\n",
        "Assistant:  A car, or a motor car, is a road-connected human-transportation system used to move people or goods from one place to another. The term also encompasses a wide range of vehicles, including motorboats, trains, and aircrafts. Cars typically have four wheels, a cabin for passengers, and an engine or motor. They have been around since the early 19th century and are now one of the most popular forms of transportation, used for daily commuting, shopping, and other purposes.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ab04da3ec9a"
      },
      "outputs": [],
      "source": [
        "# Loads an existing endpoint instance using the endpoint name:\n",
        "# - Using `endpoint_name = endpoint_with_peft.name` allows us to get the\n",
        "#   endpoint name of the endpoint `endpoint_with_peft` created in the cell\n",
        "#   above.\n",
        "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
        "#   an existing endpoint with the ID 1234567890123456789.\n",
        "# You may uncomment the code below to load an existing endpoint.\n",
        "\n",
        "# endpoint_name = endpoint_with_peft.name\n",
        "# # endpoint_name = \"\"  # @param {type:\"string\"}\n",
        "# aip_endpoint_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
        "# )\n",
        "# endpoint_with_peft = aiplatform.Endpoint(aip_endpoint_name)\n",
        "\n",
        "instances = [\n",
        "    {\n",
        "        \"prompt\": \"What is a car?\",\n",
        "        \"max_tokens\": 50,\n",
        "        \"temperature\": 1.0,\n",
        "        \"top_p\": 1.0,\n",
        "        \"top_k\": 10,\n",
        "    },\n",
        "]\n",
        "response = endpoint_with_peft.predict(instances=instances)\n",
        "\n",
        "for prediction in response.predictions:\n",
        "    print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcbApnK4iyyu"
      },
      "source": [
        "## Quantize and deploy Falcon Instruct models\n",
        "\n",
        "This section demonstrates post-training quantization of Falcon Instruct models with Vertex Custom Job. Quantization reduces the memory required by a model while attempting to retain the same performance. Read more about GPTQ in the following publication: [GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers](https://arxiv.org/abs/2210.17323)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt9pFQV-jG1v"
      },
      "source": [
        "### Deploy pre-quantized models with Google Cloud Text Moderation\n",
        "Many GPTQ-quantized models are provided [here](https://huggingface.co/TheBloke?search_models=-gptq).\n",
        "\n",
        "This section uploads the model to Model Registry and deploys it on the Endpoint.\n",
        "\n",
        "The model deployment step will take 15 minutes to 1 hour to complete, depending on the model sizes.\n",
        "\n",
        "Notice that deploying a quantized requires much less GPU. We can deploy a quantized 40B model with only two L4s instead of four."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SFje2ifjNIx"
      },
      "outputs": [],
      "source": [
        "quantized_model_id = \"TheBloke/Falcon-7B-Instruct-GPTQ\"  # @param [\"TheBloke/Falcon-7B-Instruct-GPTQ\", \"TheBloke/falcon-40b-instruct-GPTQ\"]\n",
        "\n",
        "quantization_method = \"gptq\"\n",
        "\n",
        "# Finds Vertex AI prediction supported accelerators and regions in\n",
        "# https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
        "\n",
        "# Sets 1 L4 (24G) to deploy Falcon Instruct 7B model.\n",
        "machine_type = \"g2-standard-8\"\n",
        "accelerator_type = \"NVIDIA_L4\"\n",
        "accelerator_count = 1\n",
        "\n",
        "# Sets 2 L4's (24G) to deploy Falcon Instruct 40B model.\n",
        "# machine_type = \"g2-standard-24\"\n",
        "# accelerator_type = \"NVIDIA_L4\"\n",
        "# accelerator_count = 2\n",
        "\n",
        "model_prequantized_vllm, endpoint_prequantized_vllm = deploy_model_vllm(\n",
        "    model_name=get_job_name_with_datetime(\n",
        "        prefix=\"falcon-instruct-serve-vllm-prequantized\"\n",
        "    ),\n",
        "    model_id=quantized_model_id,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    quantization_method=quantization_method,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vpslc04szHW"
      },
      "source": [
        "NOTE: After the deployment succeeds, the model weights will be downloaded on the fly. Thus additional 10 ~ 40 minutes (depending on the model sizes) of waiting time is needed **after** the above model deployment step succeeds and before you run the next step below. Otherwise you might see a `ServiceUnavailable: 503 502:Bad Gateway` error when you send requests to the endpoint.\n",
        "\n",
        "Once deployment succeeds, you can send requests to the endpoint with text prompts.\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "Human: What is a car?\n",
        "Assistant:  A car, or a motor car, is a road-connected human-transportation system used to move people or goods from one place to another. The term also encompasses a wide range of vehicles, including motorboats, trains, and aircrafts. Cars typically have four wheels, a cabin for passengers, and an engine or motor. They have been around since the early 19th century and are now one of the most popular forms of transportation, used for daily commuting, shopping, and other purposes.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPFMjyMEs0qt"
      },
      "outputs": [],
      "source": [
        "# Loads an existing endpoint instance using the endpoint name:\n",
        "# - Using `endpoint_name = endpoint_prequantized_vllm.name` allows us to get the\n",
        "#   endpoint name of the endpoint `endpoint_prequantized_vllm` created in the cell\n",
        "#   above.\n",
        "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
        "#   an existing endpoint with the ID 1234567890123456789.\n",
        "# You may uncomment the code below to load an existing endpoint.\n",
        "\n",
        "# endpoint_name = endpoint_prequantized_vllm.name\n",
        "# # endpoint_name = \"\"  # @param {type:\"string\"}\n",
        "# aip_endpoint_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
        "# )\n",
        "# endpoint_prequantized_vllm = aiplatform.Endpoint(aip_endpoint_name)\n",
        "\n",
        "\n",
        "# Overides max_length and top_k parameters during inferences.\n",
        "# If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`,\n",
        "# you can reduce the max length, such as set max_length as 20.\n",
        "# If you are using L4 GPUs to serve Falcon Instruct 40B models, you should set\n",
        "# max_length to around 1,000 tokens or fewer. If you need longer generated\n",
        "# sequences, please file a request with Vertex to allowlist your project for a\n",
        "# longer timeout threshold with Vertex endpoints.\n",
        "instances = [\n",
        "    {\n",
        "        \"prompt\": \"What is a car?\",\n",
        "        \"max_tokens\": 50,\n",
        "        \"temperature\": 1.0,\n",
        "        \"top_p\": 1.0,\n",
        "        \"top_k\": 10,\n",
        "    },\n",
        "]\n",
        "response = endpoint_prequantized_vllm.predict(instances=instances)\n",
        "\n",
        "for prediction in response.predictions:\n",
        "    print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0Xjw3Gxs_Lv"
      },
      "source": [
        "Text moderation analyzes a document against a list of safety attributes, which include \"harmful categories\" and topics that may be considered sensitive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GR2a8rzvs_kk"
      },
      "outputs": [],
      "source": [
        "for generated_text in response.predictions:\n",
        "    # Send a request to the API.\n",
        "    response = moderate_text(generated_text)\n",
        "    # Show the results.\n",
        "    show_text_moderation(generated_text, response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApFQMgVKmUbD"
      },
      "source": [
        "### Quantize Falcon Instruct models\n",
        "\n",
        "Quantization reduces the amount of GPU required to serve a model by reducing the bit precision of the weights while minimizing drop in performance. Serving quantized models on VLLM requires models to be quantized to 4 bits. It is recommended to first search if a model has already been quantized and made publicly available: [GPTQ](https://huggingface.co/TheBloke?search_models=-gptq).\n",
        "\n",
        "Quantizing models with GPTQ will take around 40 minutes for Falcon Instruct 7B using 1 NVIDIA_L4 GPU and will take around 4 hours for Falcon Instruct 40B using 4 NVIDIA_L4 GPU.\n",
        "\n",
        "Finetuned models can also be quantized, so long as the LoRA weights are merged with the base model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz8V8oNkxdEZ"
      },
      "source": [
        "Set the base model id."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oeyvnj8BxX8K"
      },
      "outputs": [],
      "source": [
        "model_id = \"tiiuae/falcon-7b-instruct\"  # @param [\"tiiuae/falcon-7b-instruct\", \"tiiuae/falcon-40b-instruct\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEquWSCkmrJ6"
      },
      "outputs": [],
      "source": [
        "# Set up quantization job.\n",
        "\n",
        "# Set `finetuned_model_path` to the GCS path of the merged finetuned model\n",
        "# from the section above, if not set, the base model will be quantized.\n",
        "finetuned_model_path = \"\"  # @param {type:\"string\"}\n",
        "if finetuned_model_path:\n",
        "    prequantized_model_path = finetuned_model_path\n",
        "else:\n",
        "    prequantized_model_path = model_id\n",
        "\n",
        "quantization_method = \"gptq\"\n",
        "quantization_job_name = get_job_name_with_datetime(\n",
        "    f\"falcon-instruct-{quantization_method}-quantize\"\n",
        ")\n",
        "\n",
        "quantization_output_dir = os.path.join(MODEL_BUCKET, quantization_job_name)\n",
        "quantization_output_dir_gcsfuse = quantization_output_dir.replace(\"gs://\", \"/gcs/\")\n",
        "\n",
        "# Worker pool spec.\n",
        "\n",
        "# Set 1 L4 (24G) for quantizing 7b model.\n",
        "machine_type = \"g2-standard-32\"\n",
        "accelerator_type = \"NVIDIA_L4\"\n",
        "accelerator_count = 1\n",
        "\n",
        "# Set 4 L4 (24G) for quantizing 40b model.\n",
        "# machine_type = \"g2-standard-48\"\n",
        "# accelerator_type = \"NVIDIA_L4\"\n",
        "# accelerator_count = 4\n",
        "\n",
        "# Quantization parameters.\n",
        "quantization_precision_mode = \"4bit\"\n",
        "\n",
        "# The original datasets used in GPTQ paper.\n",
        "gptq_dataset_name = \"c4\"  # @param [\"wikitext2\",\"c4\",\"c4-new\",\"ptb\",\"ptb-new\"]\n",
        "group_size = -1\n",
        "damp_percent = 0.1\n",
        "desc_act = True\n",
        "quantization_args = [\n",
        "    \"--task=quantize-model\",\n",
        "    f\"--quantization_method={quantization_method}\",\n",
        "    f\"--pretrained_model_id={model_id}\",\n",
        "    f\"--quantization_precision_mode={quantization_precision_mode}\",\n",
        "    f\"--quantization_output_dir={quantization_output_dir_gcsfuse}\",\n",
        "    f\"--quantization_dataset_name={gptq_dataset_name}\",\n",
        "    f\"--group_size={group_size}\",\n",
        "    f\"--damp_percent={damp_percent}\",\n",
        "    f\"--desc_act={desc_act}\",\n",
        "    \"--cache_examples_on_gpu=False\",\n",
        "]\n",
        "\n",
        "# Pass quantization arguments and launch job.\n",
        "worker_pool_specs = [\n",
        "    {\n",
        "        \"machine_spec\": {\n",
        "            \"machine_type\": machine_type,\n",
        "            \"accelerator_type\": accelerator_type,\n",
        "            \"accelerator_count\": accelerator_count,\n",
        "        },\n",
        "        \"replica_count\": 1,\n",
        "        \"disk_spec\": {\n",
        "            \"boot_disk_type\": \"pd-ssd\",\n",
        "            \"boot_disk_size_gb\": 500,\n",
        "        },\n",
        "        \"container_spec\": {\n",
        "            \"image_uri\": TRAIN_DOCKER_URI,\n",
        "            \"env\": [\n",
        "                {\n",
        "                    \"name\": \"PYTORCH_CUDA_ALLOC_CONF\",\n",
        "                    \"value\": \"max_split_size_mb:32\",\n",
        "                },\n",
        "            ],\n",
        "            \"command\": [],\n",
        "            \"args\": quantization_args,\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"Quantizing {prequantized_model_path}.\")\n",
        "quantize_job = aiplatform.CustomJob(\n",
        "    display_name=quantization_job_name,\n",
        "    project=PROJECT_ID,\n",
        "    worker_pool_specs=worker_pool_specs,\n",
        "    staging_bucket=STAGING_BUCKET,\n",
        ")\n",
        "quantize_job.run()\n",
        "\n",
        "print(\"Quantized models were saved in: \", quantization_output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7GIC4AYrG0E"
      },
      "source": [
        "### Deploy quantized models with Google Cloud Text Moderation\n",
        "This section uploads the model to Model Registry and deploys it on the Endpoint.\n",
        "\n",
        "The model deployment step will take 15 minutes to 1 hour to complete, depending on the model sizes.\n",
        "\n",
        "Notice that deploying a quantized model requires much less GPU. We can deploy a quantized 40B model with only two L4 GPUs instead of four."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SpJ_lbatIf7"
      },
      "outputs": [],
      "source": [
        "# Finds Vertex AI prediction supported accelerators and regions in\n",
        "# https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
        "\n",
        "# Sets 1 L4 (24G) to deploy Falcon Instruct 7B.\n",
        "machine_type = \"g2-standard-8\"\n",
        "accelerator_type = \"NVIDIA_L4\"\n",
        "accelerator_count = 1\n",
        "\n",
        "# Sets 2 L4's (24G) to deploy Falcon Instruct 70B models.\n",
        "# machine_type = \"g2-standard-24\"\n",
        "# accelerator_type = \"NVIDIA_L4\"\n",
        "# accelerator_count = 2\n",
        "\n",
        "model_quantized_vllm, endpoint_quantized_vllm = deploy_model_vllm(\n",
        "    model_name=get_job_name_with_datetime(\n",
        "        prefix=\"falcon-instruct-serve-vllm-quantized\"\n",
        "    ),\n",
        "    model_id=quantization_output_dir,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    quantization_method=quantization_method,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dzq7aM6ctPD9"
      },
      "source": [
        "NOTE: After the deployment succeeds, the model weights will be downloaded on the fly. Thus additional 10 ~ 40 minutes (depending on the model sizes) of waiting time is needed **after** the above model deployment step succeeds and before you run the next step below. Otherwise you might see a `ServiceUnavailable: 503 502:Bad Gateway` error when you send requests to the endpoint.\n",
        "\n",
        "Once deployment succeeds, you can send requests to the endpoint with text prompts.\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "Human: What is a car?\n",
        "Assistant:  A car, or a motor car, is a road-connected human-transportation system used to move people or goods from one place to another. The term also encompasses a wide range of vehicles, including motorboats, trains, and aircrafts. Cars typically have four wheels, a cabin for passengers, and an engine or motor. They have been around since the early 19th century and are now one of the most popular forms of transportation, used for daily commuting, shopping, and other purposes.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twm_awnMtQa2"
      },
      "outputs": [],
      "source": [
        "# Loads an existing endpoint instance using the endpoint name:\n",
        "# - Using `endpoint_name = endpoint_quantized_vllm.name` allows us to get the\n",
        "#   endpoint name of the endpoint `endpoint_quantized_vllm` created in the cell\n",
        "#   above.\n",
        "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
        "#   an existing endpoint with the ID 1234567890123456789.\n",
        "# You may uncomment the code below to load an existing endpoint.\n",
        "\n",
        "# endpoint_name = endpoint_quantized_vllm.name\n",
        "# # endpoint_name = \"\"  # @param {type:\"string\"}\n",
        "# aip_endpoint_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
        "# )\n",
        "# endpoint_quantized_vllm = aiplatform.Endpoint(aip_endpoint_name)\n",
        "\n",
        "\n",
        "# Overides max_length and top_k parameters during inferences.\n",
        "# If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`,\n",
        "# you can reduce the max length, such as set max_length as 20.\n",
        "# If you are using L4 GPUs to serve Falcon Instruct 40B models, you should set\n",
        "# max_length to around 1,000 tokens or fewer. If you need longer generated\n",
        "# sequences, please file a request with Vertex to allowlist your project for a\n",
        "# longer timeout threshold with Vertex endpoints.\n",
        "instances = [\n",
        "    {\n",
        "        \"prompt\": \"What is a car?\",\n",
        "        \"max_tokens\": 50,\n",
        "        \"temperature\": 1.0,\n",
        "        \"top_p\": 1.0,\n",
        "        \"top_k\": 10,\n",
        "    },\n",
        "]\n",
        "response = endpoint_quantized_vllm.predict(instances=instances)\n",
        "\n",
        "for prediction in response.predictions:\n",
        "    print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeUE4K_CteAj"
      },
      "source": [
        "Text moderation analyzes a document against a list of safety attributes, which include \"harmful categories\" and topics that may be considered sensitive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAoVzA69teui"
      },
      "outputs": [],
      "source": [
        "for generated_text in response.predictions:\n",
        "    # Send a request to the API.\n",
        "    response = moderate_text(generated_text)\n",
        "    # Show the results.\n",
        "    show_text_moderation(generated_text, response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ffffac5b20a"
      },
      "source": [
        "## Evaluate PEFT-finetuned Falcon Instruct models\n",
        "\n",
        "This section demonstrates how to evaluate the Falcon Instruct models fintuned with PEFT LoRA using EleutherAI's [Language Model Evaluation Harness (lm-evaluation-harness)](https://github.com/EleutherAI/lm-evaluation-harness) with Vertex CustomJob. Please reference the peak GPU memory usgaes for serving and adjust the machine type, accelerator type and accelerator count accordingly.\n",
        "\n",
        "This example uses the dataset [TruthfulQA](https://arxiv.org/abs/2109.07958). All supported tasks are listed in [this task table](https://github.com/EleutherAI/lm-evaluation-harness/blob/master/docs/task_table.md)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "078b4d178624"
      },
      "outputs": [],
      "source": [
        "eval_dataset = \"truthfulqa_mc\"  # @param {type:\"string\"}\n",
        "\n",
        "# Worker pool spec.\n",
        "# Find Vertex AI supported accelerators and regions in:\n",
        "#  https://cloud.google.com/vertex-ai/docs/training/configure-compute\n",
        "\n",
        "# Sets V100 (16G) to evaluate tiiuae/falcon-7b-instruct or tiiuae/falcon-40b-instruct.\n",
        "# If A100 is not available, you may evaluate tiiuae/falcon-40b-instruct with\n",
        "#  multiple V100s. Please keep in mind that the efficiency of evaluating with\n",
        "#  multiple V100s is inferior to that of evaluating with A100s.\n",
        "# Compared with L4, V100 inference can have better throughput and latency.\n",
        "machine_type = \"n1-standard-8\"\n",
        "accelerator_type = \"NVIDIA_TESLA_V100\"\n",
        "accelerator_count = 1  # for tiiuae/falcon-7b-instruct\n",
        "# accelerator_count = 8  # for tiiuae/falcon-40b-instruct\n",
        "\n",
        "# Sets L4 (24G) to evaluate tiiuae/falcon-7b-instruct or tiiuae/falcon-40b-instruct.\n",
        "# If A100 is not available, you may evaluate tiiuae/falcon-40b-instruct with\n",
        "#  multiple L4s. Please keep in mind that the efficiency of evaluating with\n",
        "#  multiple L4s is inferior to that of evaluating with A100s.\n",
        "# Compared with V100, L4 inference can be more cost efficient.\n",
        "\n",
        "# For tiiuae/falcon-7b-instruct.\n",
        "# machine_type = \"g2-standard-8\"\n",
        "# accelerator_type = \"NVIDIA_L4\"\n",
        "# accelerator_count = 1\n",
        "\n",
        "# For tiiuae/falcon-40b-instruct.\n",
        "# machine_type = \"g2-standard-48\"\n",
        "# accelerator_type = \"NVIDIA_L4\"\n",
        "# accelerator_count = 4\n",
        "\n",
        "# Sets A100 (40G) to evaluate tiiuae/falcon-7b-instruct or tiiuae/falcon-40b-instruct.\n",
        "# machine_type = \"a2-highgpu-1g\"\n",
        "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
        "# accelerator_count = 1  # for tiiuae/falcon-7b-instruct\n",
        "# accelerator_count = 4  # for tiiuae/falcon-40b-instruct\n",
        "\n",
        "# Sets A100 (80G) to evaluate falcon-40b-instruct models for faster inferences.\n",
        "# machine_type = \"a2-ultragpu-1g\"\n",
        "# accelerator_type = \"NVIDIA_TESLA_A100_80GB\"\n",
        "# accelerator_count = 2\n",
        "\n",
        "replica_count = 1\n",
        "\n",
        "# Setup evaluation job.\n",
        "job_name = get_job_name_with_datetime(prefix=\"falcon-instruct-peft-eval\")\n",
        "eval_output_dir = os.path.join(MODEL_BUCKET, job_name)\n",
        "eval_output_dir_gcsfuse = eval_output_dir.replace(\"gs://\", \"/gcs/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0t0RBixIw0P"
      },
      "outputs": [],
      "source": [
        "# Prepare evaluation command that runs the evaluation harness.\n",
        "# Set `trust_remote_code = True` because evaluating the model requires\n",
        "# executing code from the model repository.\n",
        "# Set `use_accelerate = True` to enable evaluation across multiple GPUs.\n",
        "eval_command = [\n",
        "    \"python\",\n",
        "    \"main.py\",\n",
        "    \"--model\",\n",
        "    \"hf-causal-experimental\",\n",
        "    \"--model_args\",\n",
        "    f\"pretrained={model_id},peft={output_dir_gcsfuse},trust_remote_code=True,use_accelerate=True,device_map_option=auto\",\n",
        "    \"--tasks\",\n",
        "    f\"{eval_dataset}\",\n",
        "    \"--output_path\",\n",
        "    f\"{eval_output_dir_gcsfuse}\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afd9305771ba"
      },
      "source": [
        "### Submit evaluation CustomJob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "602413896b58"
      },
      "outputs": [],
      "source": [
        "# Pass evaluation arguments and launch job.\n",
        "worker_pool_specs = [\n",
        "    {\n",
        "        \"machine_spec\": {\n",
        "            \"machine_type\": machine_type,\n",
        "            \"accelerator_type\": accelerator_type,\n",
        "            \"accelerator_count\": accelerator_count,\n",
        "        },\n",
        "        \"replica_count\": replica_count,\n",
        "        \"disk_spec\": {\n",
        "            \"boot_disk_size_gb\": 500,\n",
        "        },\n",
        "        \"container_spec\": {\n",
        "            \"image_uri\": EVAL_DOCKER_URI,\n",
        "            \"command\": eval_command,\n",
        "            \"args\": [],\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "eval_job = aiplatform.CustomJob(\n",
        "    display_name=job_name,\n",
        "    worker_pool_specs=worker_pool_specs,\n",
        "    base_output_dir=eval_output_dir,\n",
        ")\n",
        "\n",
        "eval_job.run()\n",
        "\n",
        "print(\"Evaluation results were saved in:\", eval_output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de47e182a37e"
      },
      "source": [
        "### Fetch and print evaluation results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f15ed6d375a"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "from google.cloud import storage\n",
        "\n",
        "# Fetch evaluation results.\n",
        "storage_client = storage.Client()\n",
        "BUCKET_NAME = BUCKET_URI.split(\"gs://\")[1]\n",
        "bucket = storage_client.get_bucket(BUCKET_NAME)\n",
        "RESULT_FILE_PATH = eval_output_dir[len(BUCKET_URI) + 1 :]\n",
        "blob = bucket.blob(RESULT_FILE_PATH)\n",
        "raw_result = blob.download_as_string()\n",
        "\n",
        "# Print evaluation results.\n",
        "result = json.loads(raw_result)\n",
        "result_formatted = json.dumps(result, indent=2)\n",
        "print(f\"Evaluation result:\\n{result_formatted}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af21a3cff1e0"
      },
      "source": [
        "## Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "911406c1561e"
      },
      "outputs": [],
      "source": [
        "# Delete custom train, quantization, and evaluation jobs.\n",
        "train_job.delete()\n",
        "quantize_job.delete()\n",
        "eval_job.delete()\n",
        "\n",
        "# Undeploy models and delete endpoints.\n",
        "endpoint_without_peft.delete(force=True)\n",
        "endpoint_with_peft.delete(force=True)\n",
        "endpoint_prequantized_vllm.delete(force=True)\n",
        "endpoint_quantized_vllm.delete(force=True)\n",
        "\n",
        "# Delete models.\n",
        "model_without_peft.delete()\n",
        "model_with_peft.delete()\n",
        "model_prequantized_vllm.delete()\n",
        "model_quantized_vllm.delete()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_pytorch_falcon_instruct_peft.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

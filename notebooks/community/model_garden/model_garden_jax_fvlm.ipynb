{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Vertex AI Model Garden - JAX F-VLM\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_jax_f_vlm.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_jax_f_vlm.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>                                                                                               <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_jax_f_vlm.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24743cf4a1e1"
      },
      "source": [
        "**_NOTE_**: This notebook has been tested in the following environment:\n",
        "\n",
        "* Python version = 3.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates serving a [JAX F-VLM model](https://github.com/google-research/google-research/tree/master/fvlm) for [open-vocabulary object detection](https://arxiv.org/abs/2209.15639) task and deploying them on Vertex AI for online prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d975e698c9a4"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to:\n",
        "\n",
        "- Upload the model to [Model Registry](https://cloud.google.com/vertex-ai/docs/model-registry/introduction).\n",
        "- Deploy the model on [Endpoint](https://cloud.google.com/vertex-ai/docs/predictions/using-private-endpoints).\n",
        "- Run online predictions for image classification.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services and resources:\n",
        "\n",
        "- Vertex AI Model Registry\n",
        "- Vertex AI Online Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08d289fa873f"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "This notebook uses the following prediction image as an example:\n",
        "\n",
        "Image: https://pixabay.com/nl/photos/het-fruit-eten-citroen-limoen-3134631/\n",
        "\n",
        "Creative Commons License: https://pixabay.com/nl/service/terms/\n",
        "\n",
        "You can use your own custom prediction image as well as by modifying the `DEMO_IMAGE_PATH` variable in this notebook below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aed92deeb4a0"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7EUnXsZhAGF"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Install the following packages required to execute this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b4ef9b72d43"
      },
      "outputs": [],
      "source": [
        "# Install the packages.\n",
        "! pip3 install --upgrade google-cloud-aiplatform\n",
        "# Get F-VLM repository by using svn to avoid downloading entire google-research repository.\n",
        "! apt install subversion\n",
        "! rm -rf ./fvlm\n",
        "! svn export -r 59152 https://github.com/google-research/google-research/trunk/fvlm\n",
        "# Note: The following libraries are pinned down versions of:\n",
        "# https://github.com/google-research/google-research/blob/master/fvlm/requirements.txt\n",
        "! pip3 install tensorflow==2.12.0\n",
        "! pip3 install numpy==1.23.5\n",
        "! pip3 install jax==0.4.14\n",
        "! pip3 install jaxlib==0.4.14+cuda11.cudnn86\n",
        "! pip3 install flax==0.7.1\n",
        "! pip3 install torch==2.0.1+cu118\n",
        "! pip3 install torchvision==0.15.2+cu118\n",
        "! pip3 install opencv-python==4.7.0.72\n",
        "! pip3 install tqdm==4.65.0\n",
        "! pip3 install git+https://github.com/openai/CLIP.git@a1d071733d7111c9c014f024669f959182114e33\n",
        "! pip3 install Pillow==9.5.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9q83As4G2Yn"
      },
      "source": [
        "Download the F-VLM checkpoints into the `fvlm/checkpoints` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXVI6s57FPyg"
      },
      "outputs": [],
      "source": [
        "%cd fvlm/checkpoints\n",
        "! ./download.sh\n",
        "%cd ../../"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58707a750154"
      },
      "source": [
        "### Colab only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f200f10a1da3"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages.\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF1j6f9HApxa"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "1. [Enable the Vertex AI API and Compute Engine API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component).\n",
        "\n",
        "1. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WReHDGG5g0XY"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, try the following:\n",
        "* Run `gcloud config list`.\n",
        "* Run `gcloud projects list`.\n",
        "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM1iC_MfAts1"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twgKk-LsLmX3"
      },
      "outputs": [],
      "source": [
        "REGION = \"us-central1\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBCra4QMA2wR"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74ccc9e52986"
      },
      "source": [
        "**1. Vertex AI Workbench**\n",
        "* Do nothing as you are already authenticated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de775a3773ba"
      },
      "source": [
        "**2. Local JupyterLab instance, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "254614fa0c46"
      },
      "outputs": [],
      "source": [
        "# ! gcloud auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef21552ccea8"
      },
      "source": [
        "**3. Colab, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "603adbbf0532"
      },
      "outputs": [],
      "source": [
        "# from google.colab import auth\n",
        "# auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6b2ccc891ed"
      },
      "source": [
        "**4. Service account or other**\n",
        "* See how to grant Cloud Storage permissions to your service account at https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgPO1eR3CYjk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "Create a storage bucket to store intermediate artifacts such as datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzGDU7TWdts_"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EcIXiGsCePi"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIq7R4HZCfIc"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "960505627ddf"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import functools\n",
        "import os\n",
        "import sys\n",
        "from io import BytesIO\n",
        "\n",
        "import jax\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "sys.path.append(\"./fvlm\")\n",
        "import inputs\n",
        "import jax_clip\n",
        "import utils\n",
        "from google.cloud import aiplatform\n",
        "from google.protobuf import json_format\n",
        "from google.protobuf.struct_pb2 import Value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vS1hQiGuLmX4"
      },
      "outputs": [],
      "source": [
        "staging_bucket = os.path.join(BUCKET_URI, \"jax_fvlm_staging\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=staging_bucket)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cc825514deb"
      },
      "source": [
        "### Define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b42bd4fa2b2d"
      },
      "outputs": [],
      "source": [
        "# The pre-built prediction docker image.\n",
        "OPTIMIZED_TF_RUNTIME_IMAGE_URI = (\n",
        "    \"us-docker.pkg.dev/vertex-ai-restricted/prediction/tf_opt-cpu.nightly:latest\"\n",
        ")\n",
        "# The local path to the F-VLM folder.\n",
        "F_VLM_FOLDER = \"./fvlm\"\n",
        "# The F-VLM model to use. Choose between 'resnet_50', 'resnet_50x4', or 'resnet_50x16'.\n",
        "MODEL = \"resnet_50\"\n",
        "# The list of object categories to detect. For example: \"person, car, oven\".\n",
        "CATEGORIES = [\n",
        "    \"kiwi\",\n",
        "    \"orange\",\n",
        "    \"lemon\",\n",
        "    \"blackberry\",\n",
        "    \"pine cone\",\n",
        "    \"red orange\",\n",
        "    \"table\",\n",
        "    \"spoon\",\n",
        "    \"pine needles\",\n",
        "    \"seed\",\n",
        "]\n",
        "# An upper bound on the number of classes.\n",
        "MAX_NUM_CLS = 91\n",
        "# The max number of boxes to draw on the output image.\n",
        "MAX_BOXES_TO_DRAW = 25\n",
        "# The minimum score required to draw a detected object.\n",
        "MIN_SCORE_THRESH = 0.2  # @param {type:\"slider\", min:0, max:0.9, step:0.05}\n",
        "# The local path to the output image.\n",
        "OUTPUT_IMAGE_PATH = \"./output.jpg\"\n",
        "# The original F-VLM SavedModel folder which takes image and text embeddings as inputs.\n",
        "SAVED_MODEL_DIR = f'{F_VLM_FOLDER}/checkpoints/{MODEL.replace(\"resnet_\",\"r\")}'\n",
        "# The converted SavedModel folder which takes jpeg bytes and text-embeddings bytes as inputs.\n",
        "CONVERTED_SAVED_MODEL_DIR = \"./converted_saved_model\"\n",
        "# The Cloud Storage location for the converted SavedModel.\n",
        "GCS_CONVERTED_SAVED_MODEL_DIR = f\"{BUCKET_URI}/fvlm_saved_model\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c250872074f"
      },
      "source": [
        "### Define common functions\n",
        "\n",
        "This section defines functions for:\n",
        "\n",
        "- Loading and converting input image into the required prediction format.\n",
        "- Visualization of detection outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcYUGwr-AJGY"
      },
      "outputs": [],
      "source": [
        "def convert_numpy_array_to_byte_string_via_tf_tensor(np_array):\n",
        "    \"\"\"Serializes a numpy array to tensor bytes.\"\"\"\n",
        "    tensor_array = tf.convert_to_tensor(np_array)\n",
        "    tensor_byte_string = tf.io.serialize_tensor(tensor_array)\n",
        "    return tensor_byte_string.numpy()\n",
        "\n",
        "\n",
        "def generate_text_embeddings(categories):\n",
        "    \"\"\"Generates text embeddings in numpy format from object categories.\"\"\"\n",
        "    clip_text_fn = jax_clip.get_clip_text_fn(MODEL)\n",
        "    class_clip_features = []\n",
        "    print(\"Computing custom category text embeddings.\")\n",
        "    for cls_name in tqdm.tqdm(categories, total=len(categories)):\n",
        "        cls_feat = clip_text_fn(cls_name)\n",
        "        class_clip_features.append(cls_feat)\n",
        "    text_embeddings = np.concatenate(class_clip_features, axis=0)\n",
        "    embed_path = (\n",
        "        f'{F_VLM_FOLDER}/data/{MODEL.replace(\"resnet_\", \"r\")}_bg_empty_embed.npy'\n",
        "    )\n",
        "    background_embedding, empty_embeddings = np.load(embed_path)\n",
        "    background_embedding = background_embedding[np.newaxis, Ellipsis]\n",
        "    empty_embeddings = empty_embeddings[np.newaxis, Ellipsis]\n",
        "    tile_empty_embeddings = np.tile(\n",
        "        empty_embeddings, (MAX_NUM_CLS - len(categories) - 1, 1)\n",
        "    )\n",
        "    # Concatenate 'background' and 'empty' embeddings.\n",
        "    text_embeddings = np.concatenate(\n",
        "        (background_embedding, text_embeddings, tile_empty_embeddings), axis=0\n",
        "    )\n",
        "    return text_embeddings\n",
        "\n",
        "\n",
        "def get_jpeg_bytes(local_image_path, new_width=-1):\n",
        "    \"\"\"Returns jpeg bytes given an image path and resizes if required.\"\"\"\n",
        "    image = Image.open(local_image_path)\n",
        "    if new_width <= 0:\n",
        "        new_image = image\n",
        "    else:\n",
        "        width, height = image.size\n",
        "        print(\"original input image size: \", width, \" , \", height)\n",
        "        new_height = int(height * new_width / width)\n",
        "        print(\"new input image size: \", new_width, \" , \", new_height)\n",
        "        new_image = image.resize((new_width, new_height))\n",
        "    buffered = BytesIO()\n",
        "    new_image.save(buffered, format=\"JPEG\")\n",
        "    return buffered.getvalue()\n",
        "\n",
        "\n",
        "def generate_prediction_output_image(\n",
        "    input_image_path, prediction_output, output_image_path\n",
        "):\n",
        "    \"\"\"Generates prediction output image with detected objects and bounding boxes.\"\"\"\n",
        "    # Generate tensors from prediction outputs.\n",
        "    prediction_output_tensor = {}\n",
        "    for key, val in prediction_output.items():\n",
        "        prediction_output_tensor[key] = tf.expand_dims(\n",
        "            tf.convert_to_tensor(val), axis=0\n",
        "        )\n",
        "    prediction_output_tensor[\"num_detections\"] = tf.cast(\n",
        "        prediction_output_tensor[\"num_detections\"], tf.int32\n",
        "    )\n",
        "    # Generate image embeddings for the input image.\n",
        "    with open(input_image_path, \"rb\") as f:\n",
        "        np_image = np.array(Image.open(f))\n",
        "    parser_fn = inputs.get_maskrcnn_parser()\n",
        "    data = parser_fn({\"image\": np_image, \"source_id\": np.array([0])})\n",
        "    np_data = jax.tree_map(lambda x: x.numpy()[np.newaxis, Ellipsis], data)\n",
        "    image_embeddings = np_data.pop(\"images\")\n",
        "    labels = np_data.pop(\"labels\")\n",
        "    # Generate visualization.\n",
        "    print(\"Preparing visualization.\")\n",
        "    categories = CATEGORIES\n",
        "    id_mapping = {(i + 1): c for i, c in enumerate(categories)}\n",
        "    id_mapping[0] = \"background\"\n",
        "    for k in range(len(categories) + 2, MAX_NUM_CLS):\n",
        "        id_mapping[k] = \"empty\"\n",
        "    category_index = inputs.get_category_index(id_mapping)\n",
        "    maskrcnn_visualizer_fn = functools.partial(\n",
        "        utils.visualize_boxes_and_labels_on_image_array,\n",
        "        category_index=category_index,\n",
        "        use_normalized_coordinates=False,\n",
        "        max_boxes_to_draw=MAX_BOXES_TO_DRAW,\n",
        "        min_score_thresh=MIN_SCORE_THRESH,\n",
        "        skip_labels=False,\n",
        "    )\n",
        "    vis_image = utils.visualize_instance_segmentations(\n",
        "        prediction_output_tensor,\n",
        "        image_embeddings,\n",
        "        labels[\"image_info\"],\n",
        "        maskrcnn_visualizer_fn,\n",
        "    )\n",
        "    pil_vis_image = Image.fromarray(vis_image, mode=\"RGB\")\n",
        "    pil_vis_image.save(output_image_path)\n",
        "    print(\"Completed saving the output image at: \", output_image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayNrua2txk0B"
      },
      "source": [
        "# Convert F-VLM SavedModel to support smaller input size\n",
        "\n",
        "The F-VLM SavedModel takes image embeddings and text embeddings as input. But you can not send these inputs directly for Vertex AI Online Prediction because there is a limit of 1.5 MB on the prediction request size. So you will first convert the SavedModel format to take jpeg bytes and text-embeddings bytes as an input instead. This modified input format will meet the 1.5 MB limit requirement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef-svu2Ix1OA"
      },
      "outputs": [],
      "source": [
        "def preprocess_jpeg_byte_string(tensor_byte_string):\n",
        "    \"\"\"Converts jpeg bytes to image embeddings as an input for the original F-VLM SavedModel.\"\"\"\n",
        "    decoded_image_tensor = tf.io.decode_jpeg(tensor_byte_string, channels=3)\n",
        "    parser_fn = inputs.get_maskrcnn_parser()\n",
        "    parser_output = parser_fn({\"image\": decoded_image_tensor})\n",
        "    image_embeddings_tensor = parser_output[\"images\"]\n",
        "    return image_embeddings_tensor\n",
        "\n",
        "\n",
        "def preprocess_text_embeddings_byte_string(tensor_byte_string):\n",
        "    \"\"\"Converts text-embeddings bytes to text-embeddings as an input for the original F-VLM SavedModel.\"\"\"\n",
        "    return tf.io.parse_tensor(tensor_byte_string, tf.float32)\n",
        "\n",
        "\n",
        "def get_serve_fn(model):\n",
        "    \"\"\"Creates a serving function for the modified SavedModel which takes jpeg bytes and text-embeddings bytes as an input.\"\"\"\n",
        "\n",
        "    @tf.function(\n",
        "        input_signature=[\n",
        "            tf.TensorSpec([None], tf.string),\n",
        "            tf.TensorSpec([None], tf.string),\n",
        "        ]\n",
        "    )\n",
        "    def serve_fn(image_jpeg_bytes_inputs, text_embeddings_bytes_inputs):\n",
        "        image_embeddings_tensor = tf.map_fn(\n",
        "            preprocess_jpeg_byte_string, image_jpeg_bytes_inputs, dtype=tf.bfloat16\n",
        "        )\n",
        "        text_embeddings_tensor = tf.map_fn(\n",
        "            preprocess_text_embeddings_byte_string,\n",
        "            text_embeddings_bytes_inputs,\n",
        "            dtype=tf.float32,\n",
        "        )\n",
        "        return model({\"image\": image_embeddings_tensor, \"text\": text_embeddings_tensor})\n",
        "\n",
        "    return serve_fn\n",
        "\n",
        "\n",
        "! rm -rf {CONVERTED_SAVED_MODEL_DIR}\n",
        "model = tf.saved_model.load(SAVED_MODEL_DIR)\n",
        "signatures = {\n",
        "    \"serving_default\": get_serve_fn(model=model).get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None], dtype=tf.string), tf.TensorSpec([None], tf.string)\n",
        "    )\n",
        "}\n",
        "tf.saved_model.save(model, CONVERTED_SAVED_MODEL_DIR, signatures=signatures)\n",
        "print(\"Saved the converted SavedModel to directory: \", CONVERTED_SAVED_MODEL_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cJQEETi1jsg"
      },
      "source": [
        "Copy the local converted TF SavedModel to Cloud Storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hlTWKxh11dF"
      },
      "outputs": [],
      "source": [
        "! gsutil -m rm -R -f {GCS_CONVERTED_SAVED_MODEL_DIR}\n",
        "! gsutil -m cp -R {CONVERTED_SAVED_MODEL_DIR} {GCS_CONVERTED_SAVED_MODEL_DIR}\n",
        "! gsutil ls {GCS_CONVERTED_SAVED_MODEL_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iILhhP3TfO8B"
      },
      "source": [
        "## Run online prediction\n",
        "Run online prediction with the converted TF SavedModel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExIyCnKf3a94"
      },
      "source": [
        "Upload TF SavedModel and deploy it to an endpoint for prediction. This step can take up to 15 minutes to finish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0xYDT0BxP0W"
      },
      "outputs": [],
      "source": [
        "jax_fvlm_model = aiplatform.Model.upload(\n",
        "    display_name=\"jax_fvlm\",\n",
        "    artifact_uri=GCS_CONVERTED_SAVED_MODEL_DIR,\n",
        "    serving_container_image_uri=OPTIMIZED_TF_RUNTIME_IMAGE_URI,\n",
        "    serving_container_args=[],\n",
        "    location=REGION,\n",
        ")\n",
        "\n",
        "jax_fvlm_endpoint = jax_fvlm_model.deploy(\n",
        "    deployed_model_display_name=\"jax_vlm_deployed\",\n",
        "    traffic_split={\"0\": 100},\n",
        "    machine_type=\"n1-highmem-16\",\n",
        "    min_replica_count=1,\n",
        "    max_replica_count=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w99wNhz_3ruV"
      },
      "source": [
        "Prepare input prediction image.\n",
        "\n",
        "Note: You can modify the input image as required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Itg0k1s30t3"
      },
      "outputs": [],
      "source": [
        "# Local path to the prediction image.\n",
        "DEMO_IMAGE_PATH = \"./prediction_image.jpg\"\n",
        "# Download the prediction image.\n",
        "! wget -O {DEMO_IMAGE_PATH} https://cdn.pixabay.com/photo/2018/02/06/12/37/fruit-3134631_1280.jpg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1Q7AbmJ4QxZ"
      },
      "source": [
        "Prepare jpeg bytes and text-embeddings bytes inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxj4Xv_DhHXj"
      },
      "outputs": [],
      "source": [
        "image_jpeg_bytes_inputs = get_jpeg_bytes(\n",
        "    local_image_path=DEMO_IMAGE_PATH, new_width=1024\n",
        ")\n",
        "text_embeddings = generate_text_embeddings(categories=CATEGORIES)\n",
        "text_embeddings_bytes_inputs = convert_numpy_array_to_byte_string_via_tf_tensor(\n",
        "    text_embeddings\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys88lEkK4XDp"
      },
      "source": [
        "Use base-64 encoding followed by UTF-8 decoding to package the bytes inputs and then send them to the endpoint for prediction. The Vertex AI Prediction service will automatically convert these input strings back to bytes based on the `b64` keyword.\n",
        "\n",
        "**Note: The first prediction can take up to 2 minutes due to one time JIT compilation of the model. This may cause a timeout error below. If you get a timeout error, then wait for 2 minutes and run the prediction again. You will not get the timeout error after that.**\n",
        "The subsequent predictions take 4 seconds to finish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mj4sqTAG4sU5"
      },
      "outputs": [],
      "source": [
        "instances_list = [\n",
        "    {\n",
        "        \"image_jpeg_bytes_inputs\": {\n",
        "            \"b64\": base64.b64encode(image_jpeg_bytes_inputs).decode(\"utf-8\")\n",
        "        },\n",
        "        \"text_embeddings_bytes_inputs\": {\n",
        "            \"b64\": base64.b64encode(text_embeddings_bytes_inputs).decode(\"utf-8\")\n",
        "        },\n",
        "    }\n",
        "]\n",
        "instances = [json_format.ParseDict(s, Value()) for s in instances_list]\n",
        "prediction_output = jax_fvlm_endpoint.predict(instances=instances).predictions[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3qC-MrN6SEs"
      },
      "source": [
        "Generate output image with predicted bounding boxes, labels, and probabilities. The output image will be saved to `OUTPUT_IMAGE_PATH`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnPY2MFN6fL6"
      },
      "outputs": [],
      "source": [
        "generate_prediction_output_image(\n",
        "    input_image_path=DEMO_IMAGE_PATH,\n",
        "    prediction_output=prediction_output,\n",
        "    output_image_path=OUTPUT_IMAGE_PATH,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx_vKniMq9ZX"
      },
      "outputs": [],
      "source": [
        "# Delete endpoint resource.\n",
        "jax_fvlm_endpoint.delete(force=True)\n",
        "\n",
        "# Delete model resource.\n",
        "jax_fvlm_model.delete()\n",
        "\n",
        "# Delete Cloud Storage objects that were created.\n",
        "delete_bucket = False\n",
        "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
        "    ! gsutil -m rm -r $BUCKET_URI"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_jax_fvlm.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SgQ6t5bqZVlH"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Vertex AI Model Garden - Serve Llama 3.1 with vLLM\n",
        "## A Comprehensive Deployment Tutorial\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_vllm_text_only_tutorial.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_vllm_text_only_tutorial.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Large Language Models (LLMs) like Llama 3.1 have emerged as powerful tools, capable of generating creative text, translating languages, and answering questions in an informative way. However, harnessing their full potential often hinges on efficient and scalable deployment strategies. This is where **vLLM** steps in, fundamentally transforming the landscape of LLM serving.\n",
        "\n",
        "Imagine an orchestra: each instrument (or query) requires attention, and a traditional server struggles to coordinate them all simultaneously, leading to delays and a cacophony of frustrated users. vLLM, on the other hand, acts as a masterful conductor, efficiently orchestrating requests to maximize throughput and minimize latency.\n",
        "\n",
        "**vLLM** is more than an optimization; it's a paradigm shift. This open-source library revolutionizes LLM serving by:\n",
        "\n",
        "*   **Democratizing Access:** Enabling efficient and cost-effective deployment of powerful models like Llama 3.1, even on limited hardware. Think of it as making a Formula 1 racing engine accessible to your everyday sedan, boosting its performance far beyond expectations.\n",
        "\n",
        "*   **Boosting Throughput:** Maximizing the number of requests handled per second, ensuring responsive and seamless user experiences. vLLM transforms your LLM from a single lane road to a multi-lane superhighway, capable of handling significantly increased traffic.\n",
        "\n",
        "*   **Enabling Dynamic Customization:** Supporting dynamic LoRA (Low-Rank Adaptation), allowing you to change the model behavior on-the-fly without the need for retraining the entire model. This is akin to having interchangeable lenses for a camera, each optimized for different shooting conditions, enabling you to adapt the model to specific tasks with unparalleled precision.\n",
        "\n",
        "### Google Vertex AI vLLM Customizations: Enhancing Performance and Integration\n",
        "\n",
        "The vLLM implementation within Google Vertex AI Model Garden is not merely a direct integration of the open-source library. Vertex AI maintains a customized and optimized version of vLLM, specifically tailored to enhance performance, reliability, and seamless integration within the Google Cloud ecosystem. These customizations provide tangible benefits for users deploying LLMs on Vertex AI.\n",
        "\n",
        "Key Vertex AI vLLM customizations include:\n",
        "\n",
        "*   **Performance Optimizations:**\n",
        "    *   **Parallel Downloading from Google Cloud Storage (GCS):** Significantly accelerates model loading and deployment times by enabling parallel data retrieval from GCS, reducing latency and improving startup speed.\n",
        "*   **Feature Enhancements:**\n",
        "    *   **Dynamic LoRA with Enhanced Caching and GCS Support:** Extends dynamic LoRA capabilities with local disk caching mechanisms and robust error handling, alongside support for loading LoRA weights directly from GCS paths and signed URLs, simplifying management and deployment of customized models.\n",
        "    *   **Llama 3.1/3.2 Function Calling Parsing:** Implements specialized parsing for Llama 3.1/3.2 function calling, improving the robustness in parsing.\n",
        "    *   **Host memory prefix caching:** The external vLLM only supports GPU memory prefix caching.\n",
        "    *   **Speculative decoding:** This is an existing vLLM feature, but Vertex ran experiments to find high-performing model setups.\n",
        "*   **Vertex AI Ecosystem Integration:**\n",
        "    *   **Vertex Prediction Input/Output Format Support:** Ensures seamless compatibility with Vertex AI prediction input and output formats, simplifying data handling and integration with other Vertex AI services.\n",
        "    *   **Vertex Environment Variable Awareness:** Respects and leverages Vertex AI environment variables (AIP_*) for configuration and resource management, streamlining deployment and ensuring consistent behavior within the Vertex AI environment.\n",
        "    *   **Enhanced Error Handling and Robustness:** Implements comprehensive error handling, input/output validation, and server termination mechanisms to ensure stability, reliability, and seamless operation within the managed Vertex AI environment.\n",
        "    *   **Nginx Server for Scalability:** Integrates an Nginx server on top of the vLLM server, facilitating the deployment of multiple replicas and enhancing scalability and high availability of the serving infrastructure.\n",
        "\n",
        "These Vertex AI-specific customizations, while often transparent to the end-user, are crucial for delivering a production-ready, highly optimized, and seamlessly integrated vLLM serving experience within the Google Cloud ecosystem. By leveraging these enhancements, users can maximize the performance and efficiency of their Llama 3.1 deployments on Vertex AI Model Garden.\n",
        "\n",
        "## Objectives\n",
        "\n",
        "This tutorial will guide you through the deployment process on Google Vertex AI Model Garden, showcasing how to harness the power of vLLM to unlock the full potential of Llama 3.1. We'll delve into:\n",
        "\n",
        "1.  **vLLM's Core Innovations:** A high-level overview of the key technologies that drive vLLM's exceptional performance, including Paged Attention, Continuous Batching, and Optimized CUDA Kernels. Think of these as the secret ingredients that give vLLM its edge.\n",
        "\n",
        "2.  **Vertex AI Model Garden: Your LLM Launchpad:** Discover how Google Vertex AI Model Garden provides a curated collection of pre-built Llama 3.1 models optimized for vLLM, simplifying deployment and eliminating complex configuration steps. This is your fast track to LLM success.\n",
        "\n",
        "3.  **Streamlined Model Deployment:** Step-by-step instructions on deploying Llama 3.1 models with both standard and optimized vLLM configurations, demonstrating how to select the best approach for your specific needs. We'll cover all the essentials, from selecting the appropriate hardware to configuring the deployment environment.\n",
        "\n",
        "4.  **Performance Optimization Techniques:** Learn how to leverage dynamic LoRA adapters to fine-tune model behavior for specific tasks and optimize resource utilization to minimize costs. This is where you'll learn how to squeeze every last drop of performance out of your LLM deployment.\n",
        "\n",
        "This tutorial caters to users of all levels, from beginners eager to explore the world of LLMs to experienced practitioners seeking to optimize their existing deployments. Let's embark on this journey together and unlock the transformative power of Llama 3.1 with vLLM!\n",
        "\n",
        "### Deployment Options\n",
        "This notebook proceeds with the following deployment options.\n",
        "\n",
        "1.  **Deploy prebuilt Llama 3.1 8B Instruct with standard vLLM and Fast Deployment:**  \n",
        "This option prioritizes rapid deployment and exploration on a `a2-ultragpu-1g` machine with Fast Deployment in `us-central1`. Fast Deployment is used for initial validation.\n",
        "\n",
        "2.  **Deploy prebuilt Llama 3.1 8B, 70B and 405B with standard vLLM:**  \n",
        "This approach leverages the NVIDIA_L4 and NVIDIA_H100_80GB for general usage and demonstration of the implementation.\n",
        "\n",
        "3.  **Deploy prebuilt Llama 3.1 8B and 70B with optimized vLLM:**  \n",
        "This provides optimized deployment of the Llama 3.1 models. Our general recommendation is that the optimized vLLM serving container can provide better serving performance in some cases and it is preferable to use it.This demonstrates how to take LLMs to the highest standard for performance and throughput.\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial utilizes billable components of Google Cloud, including:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Refer to [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage pricing](https://cloud.google.com/storage/pricing) to estimate costs based on your projected usage. The [Pricing Calculator](https://cloud.google.com/products/calculator/) is available to generate a more detailed cost breakdown."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaRQpIo49y4p"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "Before you begin executing this notebook, please review the following prerequisites. We guide you through the process of executing these steps and preparing your environment to serve Llama 3.1:\n",
        "\n",
        "1. A Google Cloud Platform (GCP) project with billing enabled.\n",
        "2. Request Quota.\n",
        "3. The Google Cloud SDK (gcloud) installed and configured.\n",
        "4. The Vertex AI API and Compute Engine API are enabled.\n",
        "5. A Cloud Storage bucket is created for storing experiment outputs.\n",
        "\n",
        "## Request for quota\n",
        "\n",
        "Before deploying Llama 3.1 for serving, it's critical to ensure you have sufficient GPU quota in your Google Cloud project. Quota represents the maximum amount of resources your project can utilize and is essential for successful deployment.\n",
        "\n",
        "**Machine and Accelerator Types:**\n",
        "\n",
        "*   **L4 GPUs:** For serving, a minimum of **1 L4 GPU** is required in the `us-central1` region. To verify your current L4 GPU quota in `us-central1`, click [here](https://console.cloud.google.com/iam-admin/quotas?location=us-central1&metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_l4_gpus). If your quota is insufficient, you will need to request an increase using [these instructions](https://cloud.google.com/docs/quotas/view-manage#viewing_your_quota_console).\n",
        "*   **Alternative GPUs (A100 80GB & H100 80GB):** You can also run Llama 3.1 predictions on **A100 80GB** or **H100 80GB** GPUs. However, this requires that you have sufficient quota in the chosen region.  It is CRITICAL to verify your quota for these GPUs using the following links:\n",
        "    *   **Nvidia A100 80GB Quota:** [Click here](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_a100_80gb_gpus)\n",
        "    *   **Nvidia H100 80GB Quota:** [Click here](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_h100_gpus)\n",
        "\n",
        "**Important Considerations for Quota:**\n",
        "\n",
        "*   **Quota Regions:** Quotas are region-specific. The links provided are for general quota viewing, but you must ensure your selected region has available resources. For example, the links above direct you to see A100 and H100 quota in *any region*. You may be planning on serving in `us-central1` but your quota may not allow for it.\n",
        "*   **Quota Types:** The provided links direct you to *serving* quota. If your goal is *finetuning*, you will need to follow the finetuning specific instructions. Serving and finetuning quotas are different. This notebook focuses on serving\n",
        "*   **Requesting Quota Increases:** If your existing quota is insufficient for your deployment, you'll need to request a quota increase via the Google Cloud console. The link provided earlier provides instructions on how to navigate the request process. This typically involves submitting a request with your requirements and may require review.\n",
        "*   **Preemptible vs. Non-preemptible GPUs:** There are often different quotas for preemptible (cost-effective but interruptible) and non-preemptible GPUs. If you are using dynamic workload scheduler for finetuning and using H100s, make sure you have the correct type of quota. The links for the H100s are all for preemptible GPUs.\n",
        "\n",
        "By carefully checking and managing your GPU quota, you can ensure a smooth and successful deployment of Llama 3.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YXFGIp1l-qtT"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown 2. **[Optional]** [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs. Set the `BUCKET_URI` for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`) should be located in the same region as where the notebook was launched. Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\"). If not set, a unique GCS bucket will be created instead.\n",
        "\n",
        "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown 3. **[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment. If you want to run predictions with A100 80GB or H100 GPUs, we recommend using the regions listed below.\n",
        "\n",
        "REGION = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown > | Machine Type | Accelerator Type | Recommended Regions |\n",
        "# @markdown | ----------- | ----------- | ----------- |\n",
        "# @markdown | a2-ultragpu-1g | 1 NVIDIA_A100_80GB | us-central1, us-east4, europe-west4, asia-southeast1, us-east4 |\n",
        "# @markdown | a3-highgpu-2g | 2 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "# @markdown | a3-highgpu-4g | 4 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "# @markdown | a3-highgpu-8g | 8 NVIDIA_H100_80GB | us-central1, us-east5, europe-west4, us-west1, asia-southeast1 |\n",
        "\n",
        "# Import the necessary packages\n",
        "\n",
        "# Clone the vertex-ai-samples repository.\n",
        "! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
        "\n",
        "import datetime\n",
        "import importlib\n",
        "import os\n",
        "import uuid\n",
        "from typing import Tuple\n",
        "\n",
        "import requests\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "common_util = importlib.import_module(\n",
        "    \"vertex-ai-samples.notebooks.community.model_garden.docker_source_codes.notebook_util.common_util\"\n",
        ")\n",
        "\n",
        "models, endpoints = {}, {}\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "if not REGION:\n",
        "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
        "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
        "\n",
        "# Cloud Storage bucket for storing the experiment artifacts.\n",
        "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
        "# prefer using your own GCS bucket, change the value yourself below.\n",
        "now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "\n",
        "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
        "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}-{str(uuid.uuid4())[:4]}\"\n",
        "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "    ! gcloud storage buckets create --location={REGION} {BUCKET_URI}\n",
        "else:\n",
        "    assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
        "    shell_output = ! gcloud storage ls --full --buckets {BUCKET_NAME} | grep \"Location Constraint:\" | sed \"s/Location Constraint://\"\n",        "    bucket_region = shell_output[0].strip().lower()\n",
        "    if bucket_region != REGION:\n",
        "        raise ValueError(\n",
        "            \"Bucket region %s is different from notebook region %s\"\n",
        "            % (bucket_region, REGION)\n",
        "        )\n",
        "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
        "\n",
        "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
        "MODEL_BUCKET = os.path.join(BUCKET_URI, \"llama3_1\")\n",
        "\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "# Gets the default SERVICE_ACCOUNT.\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
        "\n",
        "\n",
        "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
        "! gcloud storage buckets add-iam-policy-binding $BUCKET_NAME --member=serviceAccount:{SERVICE_ACCOUNT} --role=roles/storage.admin\n",
        "\n",
        "! gcloud config set project $PROJECT_ID\n",
        "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/storage.admin\"\n",
        "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/aiplatform.user\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfV_ICRID_Gw"
      },
      "source": [
        "### Using Hugging Face with Meta Llama 3.1 and vLLM for Efficient Model Serving\n",
        "\n",
        "Metaâ€™s Llama 3.1 collection provide a range of multilingual large language models (LLMs) designed for high-quality text generation across various use cases. These models are pretrained and instruction-tuned, excelling in tasks like multilingual dialogue, summarization, and agentic retrieval. For efficient deployment of these models, the vLLM library offers an open source streamlined serving environment with optimizations for latency, memory efficiency, and scalability.\n",
        "\n",
        "The Llama 3.1 is a **gated** model. You need to agree to share your contact information to access the model.\n",
        "\n",
        "<img src=\"https://cloud.google.com/static/vertex-ai/generative-ai/docs/open-models/vllm/images/meta_llama_agreement.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZgf3RR7EV5x"
      },
      "source": [
        "### HuggingFace User Access Tokens\n",
        "\n",
        "This tutorial requires a read access token from the Hugging Face Hub to access the necessary resources. Please follow these steps to set up your authentication:\n",
        "\n",
        " #### Generate a Read Access Token:\n",
        "- Navigate to your [Hugging Face account settings](https://huggingface.co/settings/tokens).\n",
        "- Create a new token, assign it the Read role, and save the token securely.\n",
        "\n",
        "<img src=\"https://cloud.google.com/static/vertex-ai/generative-ai/docs/open-models/vllm/images/access_token_settings_link.png\">\n",
        "\n",
        "#### Utilize the Token:\n",
        "Use the generated token to authenticate and access public or private repositories as needed for the tutorial.\n",
        "This setup ensures you have the appropriate level of access without unnecessary permissions. These practices enhance security and prevent accidental token exposure. For more information on setting up access tokens, visit this [page](https://huggingface.co/docs/hub/en/security-tokens).\n",
        "\n",
        "Make sure to enable Read access. Once the access token has been created, Please be cautious with your Hugging Face access token. Avoid sharing or exposing it publicly or online. When you set your token as an environment variable during deployment, it remains private to your project. Vertex AI ensures its security by preventing other users from accessing your models and endpoints.\n",
        "\n",
        "<img src=\"https://cloud.google.com/static/vertex-ai/generative-ai/docs/open-models/vllm/images/manage_access_token.png\">\n",
        "\n",
        "For more information on protecting your access token, please refer to the [Hugging Face Access Tokens - Best Practices](https://huggingface.co/docs/hub/en/security-tokens#best-practices)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJDqpx-w6y1b"
      },
      "source": [
        "## Deploy prebuilt Llama 3.1 8B Instruct with standard vLLM and Fast Deployment\n",
        "\n",
        "Before diving into the code, let's understand the arguments provided to the `fast_deploy` function and the subsequent `aiplatform.Model.upload` and `model.deploy` calls within it. These arguments configure how the model is uploaded to Vertex AI and deployed for serving.\n",
        "\n",
        "**Arguments Explained:**\n",
        "\n",
        "**`fast_deploy` function arguments:**\n",
        "\n",
        "*   `publisher` *(str)*: The name of the model publisher. This indicates the organization or entity that provides the model (e.g., \"meta\").\n",
        "*   `publisher_model_id` *(str)*: The identifier of the model within the publisher's catalog (e.g., \"llama3_1\").\n",
        "*   `version_id` *(str)*: The specific version of the model to be deployed (e.g., \"llama-3.1-8b-instruct\"). This allows for tracking and selecting particular model iterations.\n",
        "\n",
        "**Arguments for `aiplatform.Model.upload` (inside `fast_deploy`):**\n",
        "\n",
        "*   `display_name` *(str)*: The name that will be shown in the Vertex AI Model Registry. This helps identify the uploaded model in the Vertex AI console. It's automatically set based on the model from the model garden.\n",
        "*   `serving_container_image_uri` *(str)*: The location of the container image that will be used for serving the model. Typically this would be a pre-built image from a container registry that includes the necessary libraries and tools to run the model.\n",
        "*    `serving_container_args` *(list)*: Command-line arguments passed to the container when it starts, if needed by the containerized application.\n",
        "*   `serving_container_environment_variables` *(dict)*: Environment variables that are set within the serving container. This is commonly used to configure model parameters, resource limits, and other operational settings. These are equivalent to text-generation-inference environment variables, which are in turn equivalent to the arguments for the text-generation-launcher. The Hugging Face DLCs for TGI also capture the `AIP_` environment variables as specified in the [Vertex AI Documentation](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#environment-variables).\n",
        "*   `serving_container_ports` *(list of int)*: The port number where the Vertex AI endpoint will be exposed within the container. By default, this is 8080.\n",
        "*   `serving_container_predict_route` *(str)*: The route used for the prediction endpoint in the container.\n",
        "*   `serving_container_health_route` *(str)*: The route used for the health check endpoint in the container.\n",
        "*  `serving_container_shared_memory_size_mb` *(int)*: Sets the shared memory size in megabytes for the container.\n",
        "*   `serving_container_deployment_timeout` *(int)*: Time in seconds before the deployment request will timeout.\n",
        "*   `model_garden_source_model_name` *(str)*: The full path to the source model that is being deployed. This information is useful for tracking lineage of the model.\n",
        "\n",
        "**Arguments for `model.deploy` (inside `fast_deploy`):**\n",
        "\n",
        "*   `endpoint` *(aiplatform.Endpoint)*: The Vertex AI endpoint where the model will be deployed.\n",
        "*   `machine_type` *(str)*: The type of virtual machine to use for prediction (e.g., `n1-standard-8`, `a2-highgpu-1g`).\n",
        "*   `accelerator_type` *(str)*: The type of GPU accelerator to use for prediction (e.g., `NVIDIA_TESLA_A100`, `NVIDIA_L4`).\n",
        "*   `accelerator_count` *(int)*: The number of GPU accelerators to use for prediction.\n",
        "*   `deploy_request_timeout` *(int)*: The maximum time in seconds to wait for the deployment request to complete.\n",
        "*   `disable_container_logging` *(bool)*: A flag to disable container logging.\n",
        "*    `fast_tryout_enabled` *(bool)*: A flag to enable a faster tryout deployment of the endpoint.\n",
        "*   `system_labels` *(dict)*:  Labels to attach to the deployed model. The code adds two specific labels here, \"DEPLOY_SOURCE\" to \"notebook\" and  \"NOTEBOOK_NAME\" with the name of the current notebook.\n",
        "\n",
        "**Key Concepts and Considerations:**\n",
        "\n",
        "*   **Containerization:** The model is packaged into a container image, ensuring consistency across different environments. The `serving_container_image_uri` specifies the location of this container image.\n",
        "*   **Environment Variables:** These allow you to customize the serving behavior of your model without modifying the container image itself. This is very important for configuring specific model settings, and passing credentials.\n",
        "*   **Resource Allocation:** The `machine_type`, `accelerator_type`, and `accelerator_count` arguments control the compute resources dedicated to your deployed model.\n",
        "*   **Endpoint:** The endpoint is the access point for making predictions using your deployed model.\n",
        "*   **Model Garden:** This model is being deployed from Google's Model Garden, which includes pre-trained models that can be deployed in Vertex AI.\n",
        "*   **Dedicated Endpoint:** The `dedicated_endpoint_enabled=True` argument in `aiplatform.Endpoint.create()`  forces the endpoint to be dedicated which is required for fast deployments.\n",
        "\n",
        "By carefully configuring these arguments, you can tailor your model deployment to meet your specific performance, cost, and operational requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HyYES4HB5EHm"
      },
      "outputs": [],
      "source": [
        "# @title Fast Deployment\n",
        "\n",
        "# @markdown This section demonstrates how to use the Fast Deployment feature.\n",
        "\n",
        "# @markdown The Fast Deployment feature prioritizes speed for model exploration, making it ideal for initial testing and experimentation. For sensitive data or production workloads, use the Standard environment for enhanced security and stability. It is enabled by setting `fast_tryout_enabled` to `True`.\n",
        "\n",
        "# @markdown This is useful for quick experiments. Not for production workloads. Only available for most popular models and machine types.\n",
        "\n",
        "API_ENDPOINT = f\"{REGION}-aiplatform.googleapis.com\"\n",
        "\n",
        "\n",
        "def fast_deploy(\n",
        "    publisher: str, publisher_model_id: str, version_id: str\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    url = f\"https://{API_ENDPOINT}/v1/publishers/{publisher}/models/{publisher_model_id}@{version_id}\"\n",
        "    access_token = ! gcloud auth print-access-token\n",
        "    access_token = access_token[0]\n",
        "\n",
        "    response = requests.get(\n",
        "        url,\n",
        "        headers={\n",
        "            \"Authorization\": f\"Bearer {access_token}\",\n",
        "            \"Content-Type\": \"application/json\",\n",
        "        },\n",
        "    )\n",
        "    response.raise_for_status()\n",
        "    response = response.json()\n",
        "    if (\n",
        "        len(\n",
        "            response.get(\"supportedActions\", {})\n",
        "            .get(\"multiDeployVertex\", {})\n",
        "            .get(\"multiDeployVertex\", {})\n",
        "        )\n",
        "        == 0\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            f\"No supportedActions.multiDeployVertex found in {REGION}. You can skip this section or try a different region.\"\n",
        "        )\n",
        "    deploy_configs = response[\"supportedActions\"][\"multiDeployVertex\"][\n",
        "        \"multiDeployVertex\"\n",
        "    ]\n",
        "    fast_deploy_config = [\n",
        "        config\n",
        "        for config in deploy_configs\n",
        "        if config[\"deployMetadata\"]\n",
        "        .get(\"labels\", {})\n",
        "        .get(\"show-faster-deployment-option\")\n",
        "        == \"true\"\n",
        "    ]\n",
        "    if fast_deploy_config:\n",
        "        fast_deploy_config = fast_deploy_config[0]\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            f\"No Fast Deployment config found in {REGION}. You can skip this section or try a different region.\"\n",
        "        )\n",
        "\n",
        "    container_spec = fast_deploy_config[\"containerSpec\"]\n",
        "    machine_spec = fast_deploy_config[\"dedicatedResources\"][\"machineSpec\"]\n",
        "    machine_type = machine_spec[\"machineType\"]\n",
        "    accelerator_type = machine_spec[\"acceleratorType\"]\n",
        "    accelerator_count = machine_spec[\"acceleratorCount\"]\n",
        "    env = {item[\"name\"]: item[\"value\"] for item in container_spec.get(\"env\", [])}\n",
        "    if \"DEPLOY_SOURCE\" in env:\n",
        "        del env[\"DEPLOY_SOURCE\"]\n",
        "    port = container_spec.get(\"ports\", [{}])[0].get(\"containerPort\")\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=fast_deploy_config.get(\"modelDisplayName\"),\n",
        "        serving_container_image_uri=container_spec.get(\"imageUri\"),\n",
        "        serving_container_args=container_spec.get(\"args\"),\n",
        "        serving_container_environment_variables=env,\n",
        "        serving_container_ports=[port],\n",
        "        serving_container_predict_route=container_spec.get(\"predictRoute\"),\n",
        "        serving_container_health_route=container_spec.get(\"healthRoute\"),\n",
        "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
        "        serving_container_deployment_timeout=7200,\n",
        "        model_garden_source_model_name=f\"publishers/{publisher}/models/{publisher_model_id}\",\n",
        "    )\n",
        "    endpoint = aiplatform.Endpoint.create(\n",
        "        display_name=model.name + \"-endpoint\",\n",
        "        dedicated_endpoint_enabled=True,\n",
        "    )\n",
        "    print(\n",
        "        f\"Deploying {model.name} on {machine_type} with {accelerator_count} {accelerator_type} GPU(s).\"\n",
        "    )\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        deploy_request_timeout=1800,\n",
        "        disable_container_logging=True,\n",
        "        fast_tryout_enabled=True,\n",
        "        system_labels={\n",
        "            \"DEPLOY_SOURCE\": \"notebook\",\n",
        "            \"NOTEBOOK_NAME\": \"model_garden_pytorch_llama3_1_deployment.ipynb\",\n",
        "        },\n",
        "    )\n",
        "    print(\"endpoint_name:\", endpoint.name)\n",
        "    return model, endpoint\n",
        "\n",
        "\n",
        "# @markdown The Llama 3.1 8B Instruct model will be deployed to a dedicated endpoint on an `a2-ultragpu-1g` machine with Fast Deployment.\n",
        "# @markdown **Currently, the Fast Deployment is only supported in the `us-central1` region.**\n",
        "\n",
        "use_dedicated_endpoint = True  # Fast Deployment only supports dedicated endpoints.\n",
        "models[\"vllm_fast\"], endpoints[\"vllm_fast\"] = fast_deploy(\n",
        "    \"meta\", \"llama3_1\", \"llama-3.1-8b-instruct\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jryOnazQ6y1b"
      },
      "outputs": [],
      "source": [
        "# @title Raw predict\n",
        "\n",
        "# @markdown Once deployment succeeds, you can send requests to the endpoint with text prompts. Sampling parameters supported by vLLM can be found [here](https://docs.vllm.ai/en/latest/dev/sampling_params.html).\n",
        "\n",
        "# @markdown <img src=\"https://cloud.google.com/static/vertex-ai/generative-ai/docs/open-models/vllm/images/vertexai_llama3_1_endpoint.png\">\n",
        "\n",
        "# @markdown Example:\n",
        "\n",
        "# @markdown ```\n",
        "# @markdown Human: What is a car?\n",
        "# @markdown Assistant:  A car, or a motor car, is a road-connected human-transportation system used to move people or goods from one place to another. The term also encompasses a wide range of vehicles, including motorboats, trains, and aircrafts. Cars typically have four wheels, a cabin for passengers, and an engine or motor. They have been around since the early 19th century and are now one of the most popular forms of transportation, used for daily commuting, shopping, and other purposes.\n",
        "# @markdown ```\n",
        "# @markdown Additionally, you can moderate the generated text with Vertex AI. See [Moderate text documentation](https://cloud.google.com/natural-language/docs/moderating-text) for more details.\n",
        "\n",
        "# Loads an existing endpoint instance using the endpoint name:\n",
        "# - Using `endpoint_name = endpoint.name` allows us to get the\n",
        "#   endpoint name of the endpoint `endpoint` created in the cell\n",
        "#   above.\n",
        "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
        "#   an existing endpoint with the ID 1234567890123456789.\n",
        "# You may uncomment the code below to load an existing endpoint.\n",
        "\n",
        "# endpoint_name = \"\"  # @param {type:\"string\"}\n",
        "# aip_endpoint_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
        "# )\n",
        "# endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
        "\n",
        "prompt = \"What is a car?\"  # @param {type: \"string\"}\n",
        "# @markdown If you encounter an issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, by lowering `max_tokens`.\n",
        "max_tokens = 50  # @param {type:\"integer\"}\n",
        "temperature = 1.0  # @param {type:\"number\"}\n",
        "top_p = 1.0  # @param {type:\"number\"}\n",
        "top_k = 1  # @param {type:\"integer\"}\n",
        "# @markdown Set `raw_response` to `True` to obtain the raw model output. Set `raw_response` to `False` to apply additional formatting in the structure of `\"Prompt:\\n{prompt.strip()}\\nOutput:\\n{output}\"`.\n",
        "raw_response = False  # @param {type:\"boolean\"}\n",
        "\n",
        "# Overrides parameters for inferences.\n",
        "instances = [\n",
        "    {\n",
        "        \"prompt\": prompt,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": top_p,\n",
        "        \"top_k\": top_k,\n",
        "        \"raw_response\": raw_response,\n",
        "    },\n",
        "]\n",
        "response = endpoints[\"vllm_fast\"].predict(\n",
        "    instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
        ")\n",
        "\n",
        "for prediction in response.predictions:\n",
        "    print(prediction)\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4hInljZi6y1b"
      },
      "outputs": [],
      "source": [
        "# @title Chat completion\n",
        "\n",
        "DEDICATED_ENDPOINT_DNS = endpoints[\"vllm_fast\"].gca_resource.dedicated_endpoint_dns\n",
        "ENDPOINT_RESOURCE_NAME = \"projects/{}/locations/{}/endpoints/{}\".format(\n",
        "    PROJECT_ID, REGION, endpoints[\"vllm_fast\"].name\n",
        ")\n",
        "\n",
        "# @title Chat Completions Inference\n",
        "\n",
        "# @markdown Once deployment succeeds, you can send requests to the endpoint using the OpenAI SDK.\n",
        "\n",
        "# @markdown First you will need to install the SDK and some auth-related dependencies.\n",
        "\n",
        "! pip install -qU openai google-auth requests\n",
        "\n",
        "# @markdown Next fill out some request parameters:\n",
        "\n",
        "user_message = \"How is your day going?\"  # @param {type: \"string\"}\n",
        "# @markdown If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, such as set `max_tokens` as 20.\n",
        "max_tokens = 50  # @param {type: \"integer\"}\n",
        "temperature = 1.0  # @param {type: \"number\"}\n",
        "\n",
        "# @markdown Now we can send a request.\n",
        "\n",
        "import google.auth\n",
        "import openai\n",
        "\n",
        "creds, project = google.auth.default()\n",
        "auth_req = google.auth.transport.requests.Request()\n",
        "creds.refresh(auth_req)\n",
        "\n",
        "BASE_URL = (\n",
        "    f\"https://{REGION}-aiplatform.googleapis.com/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
        ")\n",
        "try:\n",
        "    if use_dedicated_endpoint:\n",
        "        BASE_URL = f\"https://{DEDICATED_ENDPOINT_DNS}/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "client = openai.OpenAI(base_url=BASE_URL, api_key=creds.token)\n",
        "\n",
        "model_response = client.chat.completions.create(\n",
        "    model=\"\",\n",
        "    messages=[{\"role\": \"user\", \"content\": user_message}],\n",
        "    temperature=temperature,\n",
        "    max_tokens=max_tokens,\n",
        ")\n",
        "print(model_response)\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-XybZjtgF9M"
      },
      "source": [
        "## Deploy prebuilt Llama 3.1 8B, 70B and 405B with standard vLLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8OiHHNNE_wj"
      },
      "outputs": [],
      "source": [
        "# @title Deploy\n",
        "\n",
        "# @markdown This section uploads prebuilt Llama 3.1 models to Model Registry and deploys it to a Vertex AI Endpoint. It takes 15 minutes to 1 hour to finish depending on the size of the model.\n",
        "\n",
        "# @markdown NVIDIA_L4 GPUs are used for demonstration. The serving efficiency of L4 GPUs is inferior to that of H100 GPUs, but L4 GPUs are nevertheless good serving solutions if you do not have H100 quota.\n",
        "\n",
        "# @markdown Recommended Serving Configurations: This example recommends using A100 80G or H100 GPUs for optimal serving efficiency and performance. These GPUs are now readily available and are the preferred options for deploying these models.\n",
        "\n",
        "# @markdown Set the model to deploy.\n",
        "\n",
        "base_model_name = \"Meta-Llama-3.1-8B\"  # @param [\"Meta-Llama-3.1-8B\", \"Meta-Llama-3.1-8B-Instruct\", \"Meta-Llama-3.1-70B\", \"Meta-Llama-3.1-70B-Instruct\", \"Meta-Llama-3.1-405B-FP8\", \"Meta-Llama-3.1-405B-Instruct-FP8\"] {isTemplate:true}\n",
        "model_id = os.path.join(VERTEX_AI_MODEL_GARDEN_LLAMA_3_1, base_model_name)\n",
        "ENABLE_DYNAMIC_LORA = True  # @param {type:\"boolean\", isTemplate:true}\n",
        "hf_model_id = \"meta-llama/\" + base_model_name\n",
        "\n",
        "# The pre-built serving docker images.\n",
        "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20241210_0916_RC00\"\n",
        "\n",
        "# @markdown Set use_dedicated_endpoint to False if you don't want to use [dedicated endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment#create-dedicated-endpoint).\n",
        "use_dedicated_endpoint = True  # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown Find Vertex AI prediction supported accelerators and regions at https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
        "if \"8b\" in base_model_name.lower():\n",
        "    accelerator_type = \"NVIDIA_L4\"\n",
        "    machine_type = \"g2-standard-12\"\n",
        "    accelerator_count = 1\n",
        "    max_loras = 5\n",
        "elif \"70b\" in base_model_name.lower():\n",
        "    accelerator_type = \"NVIDIA_H100_80GB\"\n",
        "    machine_type = \"a3-highgpu-4g\"\n",
        "    accelerator_count = 4\n",
        "    max_loras = 1\n",
        "elif \"405b\" in base_model_name.lower():\n",
        "    accelerator_type = \"NVIDIA_H100_80GB\"\n",
        "    machine_type = \"a3-highgpu-8g\"\n",
        "    accelerator_count = 8\n",
        "    max_loras = 1\n",
        "else:\n",
        "    raise ValueError(\n",
        "        f\"Recommended GPU setting not found for: {accelerator_type} and {base_model_name}.\"\n",
        "    )\n",
        "\n",
        "common_util.check_quota(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    is_for_training=False,\n",
        ")\n",
        "\n",
        "gpu_memory_utilization = 0.95\n",
        "max_model_len = 8192  # Maximum context length.\n",
        "\n",
        "\n",
        "# Enable automatic prefix caching using GPU HBM\n",
        "enable_prefix_cache = True\n",
        "# Setting this value >0 will use the idle host memory for a second-tier prefix kv\n",
        "# cache beneath the HBM cache. It only has effect if enable_prefix_cache=True.\n",
        "# The range of this value: [0, 1)\n",
        "# Setting host_prefix_kv_cache_utilization_target to 0 will disable the host memory prefix kv cache.\n",
        "host_prefix_kv_cache_utilization_target = 0.7\n",
        "\n",
        "\n",
        "def deploy_model_vllm(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    service_account: str,\n",
        "    base_model_id: str = None,\n",
        "    machine_type: str = \"g2-standard-8\",\n",
        "    accelerator_type: str = \"NVIDIA_L4\",\n",
        "    accelerator_count: int = 1,\n",
        "    gpu_memory_utilization: float = 0.9,\n",
        "    max_model_len: int = 4096,\n",
        "    dtype: str = \"auto\",\n",
        "    enable_trust_remote_code: bool = False,\n",
        "    enforce_eager: bool = False,\n",
        "    enable_lora: bool = False,\n",
        "    enable_chunked_prefill: bool = False,\n",
        "    enable_prefix_cache: bool = False,\n",
        "    host_prefix_kv_cache_utilization_target: float = 0.0,\n",
        "    max_loras: int = 1,\n",
        "    max_cpu_loras: int = 8,\n",
        "    use_dedicated_endpoint: bool = False,\n",
        "    max_num_seqs: int = 256,\n",
        "    model_type: str = None,\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(\n",
        "        display_name=f\"{model_name}-endpoint\",\n",
        "        dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
        "    )\n",
        "\n",
        "    if not base_model_id:\n",
        "        base_model_id = model_id\n",
        "\n",
        "    # See https://docs.vllm.ai/en/latest/models/engine_args.html for a list of possible arguments with descriptions.\n",
        "    vllm_args = [\n",
        "        \"python\",\n",
        "        \"-m\",\n",
        "        \"vllm.entrypoints.api_server\",\n",
        "        \"--host=0.0.0.0\",\n",
        "        \"--port=8080\",\n",
        "        f\"--model={model_id}\",\n",
        "        f\"--tensor-parallel-size={accelerator_count}\",\n",
        "        \"--swap-space=16\",\n",
        "        f\"--gpu-memory-utilization={gpu_memory_utilization}\",\n",
        "        f\"--max-model-len={max_model_len}\",\n",
        "        f\"--dtype={dtype}\",\n",
        "        f\"--max-loras={max_loras}\",\n",
        "        f\"--max-cpu-loras={max_cpu_loras}\",\n",
        "        f\"--max-num-seqs={max_num_seqs}\",\n",
        "        \"--disable-log-stats\",\n",
        "    ]\n",
        "\n",
        "    if enable_trust_remote_code:\n",
        "        vllm_args.append(\"--trust-remote-code\")\n",
        "\n",
        "    if enforce_eager:\n",
        "        vllm_args.append(\"--enforce-eager\")\n",
        "\n",
        "    if enable_lora:\n",
        "        vllm_args.append(\"--enable-lora\")\n",
        "\n",
        "    if enable_chunked_prefill:\n",
        "        vllm_args.append(\"--enable-chunked-prefill\")\n",
        "\n",
        "    if enable_prefix_cache:\n",
        "        vllm_args.append(\"--enable-prefix-caching\")\n",
        "\n",
        "    if 0 < host_prefix_kv_cache_utilization_target < 1:\n",
        "        vllm_args.append(\n",
        "            f\"--host-prefix-kv-cache-utilization-target={host_prefix_kv_cache_utilization_target}\"\n",
        "        )\n",
        "\n",
        "    if model_type:\n",
        "        vllm_args.append(f\"--model-type={model_type}\")\n",
        "\n",
        "    env_vars = {\n",
        "        \"MODEL_ID\": base_model_id,\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "    }\n",
        "\n",
        "    # HF_TOKEN is not a compulsory field and may not be defined.\n",
        "    try:\n",
        "        if HF_TOKEN:\n",
        "            env_vars[\"HF_TOKEN\"] = HF_TOKEN\n",
        "    except NameError:\n",
        "        pass\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
        "        serving_container_args=vllm_args,\n",
        "        serving_container_ports=[8080],\n",
        "        serving_container_predict_route=\"/generate\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "        serving_container_environment_variables=env_vars,\n",
        "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
        "        serving_container_deployment_timeout=7200,\n",
        "    )\n",
        "    print(\n",
        "        f\"Deploying {model_name} on {machine_type} with {accelerator_count} {accelerator_type} GPU(s).\"\n",
        "    )\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=service_account,\n",
        "        system_labels={\n",
        "            \"NOTEBOOK_NAME\": \"model_garden_pytorch_llama3_1_deployment.ipynb\",\n",
        "        },\n",
        "    )\n",
        "    print(\"endpoint_name:\", endpoint.name)\n",
        "\n",
        "    return model, endpoint\n",
        "\n",
        "\n",
        "models[\"vllm_gpu\"], endpoints[\"vllm_gpu\"] = deploy_model_vllm(\n",
        "    model_name=common_util.get_job_name_with_datetime(prefix=\"llama3_1-serve\"),\n",
        "    model_id=model_id,\n",
        "    base_model_id=hf_model_id,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    gpu_memory_utilization=gpu_memory_utilization,\n",
        "    max_model_len=max_model_len,\n",
        "    max_loras=max_loras,\n",
        "    enforce_eager=True,\n",
        "    enable_lora=ENABLE_DYNAMIC_LORA,\n",
        "    enable_chunked_prefill=not ENABLE_DYNAMIC_LORA,\n",
        "    enable_prefix_cache=enable_prefix_cache,\n",
        "    host_prefix_kv_cache_utilization_target=host_prefix_kv_cache_utilization_target,\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        ")\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rDHsCOqvFYBi"
      },
      "outputs": [],
      "source": [
        "# @title Raw predict\n",
        "\n",
        "# @markdown Once deployment succeeds, you can send requests to the endpoint with text prompts. Sampling parameters supported by vLLM can be found [here](https://docs.vllm.ai/en/latest/dev/sampling_params.html).\n",
        "\n",
        "# @markdown Example:\n",
        "\n",
        "# @markdown ```\n",
        "# @markdown Human: What is a car?\n",
        "# @markdown Assistant:  A car, or a motor car, is a road-connected human-transportation system used to move people or goods from one place to another.\n",
        "# @markdown ```\n",
        "\n",
        "# @markdown Optionally, you can apply LoRA weights to prediction. Set `lora_id` to be either a GCS URI or a HuggingFace repo containing the LoRA weight.\n",
        "\n",
        "# @markdown Additionally, you can moderate the generated text with Vertex AI. See [Moderate text documentation](https://cloud.google.com/natural-language/docs/moderating-text) for more details.\n",
        "\n",
        "prompt = \"What is a car?\"  # @param {type: \"string\"}\n",
        "# @markdown If you encounter an issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, by lowering `max_tokens`.\n",
        "max_tokens = 50  # @param {type:\"integer\"}\n",
        "temperature = 1.0  # @param {type:\"number\"}\n",
        "top_p = 1.0  # @param {type:\"number\"}\n",
        "top_k = 1  # @param {type:\"integer\"}\n",
        "# @markdown Set `raw_response` to `True` to obtain the raw model output. Set `raw_response` to `False` to apply additional formatting in the structure of `\"Prompt:\\n{prompt.strip()}\\nOutput:\\n{output}\"`.\n",
        "raw_response = False  # @param {type:\"boolean\"}\n",
        "lora_id = \"\"  # @param {type:\"string\", isTemplate: true}\n",
        "\n",
        "# Overrides parameters for inferences.\n",
        "instance = {\n",
        "    \"prompt\": prompt,\n",
        "    \"max_tokens\": max_tokens,\n",
        "    \"temperature\": temperature,\n",
        "    \"top_p\": top_p,\n",
        "    \"top_k\": top_k,\n",
        "    \"raw_response\": raw_response,\n",
        "}\n",
        "if lora_id:\n",
        "    instance[\"dynamic-lora\"] = lora_id\n",
        "instances = [instance]\n",
        "response = endpoints[\"vllm_gpu\"].predict(\n",
        "    instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
        ")\n",
        "\n",
        "for prediction in response.predictions:\n",
        "    print(prediction)\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LSG9ITWTbTb7"
      },
      "outputs": [],
      "source": [
        "# @title Chat completion\n",
        "\n",
        "if use_dedicated_endpoint:\n",
        "    DEDICATED_ENDPOINT_DNS = endpoints[\"vllm_gpu\"].gca_resource.dedicated_endpoint_dns\n",
        "ENDPOINT_RESOURCE_NAME = \"projects/{}/locations/{}/endpoints/{}\".format(\n",
        "    PROJECT_ID, REGION, endpoints[\"vllm_gpu\"].name\n",
        ")\n",
        "\n",
        "# @title Chat Completions Inference\n",
        "\n",
        "# @markdown Once deployment succeeds, you can send requests to the endpoint using the OpenAI SDK.\n",
        "\n",
        "# @markdown First you will need to install the SDK and some auth-related dependencies.\n",
        "\n",
        "! pip install -qU openai google-auth requests\n",
        "\n",
        "# @markdown Next fill out some request parameters:\n",
        "\n",
        "user_message = \"How is your day going?\"  # @param {type: \"string\"}\n",
        "# @markdown If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, such as set `max_tokens` as 20.\n",
        "max_tokens = 50  # @param {type: \"integer\"}\n",
        "temperature = 1.0  # @param {type: \"number\"}\n",
        "\n",
        "# @markdown Now we can send a request.\n",
        "\n",
        "import google.auth\n",
        "import openai\n",
        "\n",
        "creds, project = google.auth.default()\n",
        "auth_req = google.auth.transport.requests.Request()\n",
        "creds.refresh(auth_req)\n",
        "\n",
        "BASE_URL = (\n",
        "    f\"https://{REGION}-aiplatform.googleapis.com/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
        ")\n",
        "try:\n",
        "    if use_dedicated_endpoint:\n",
        "        BASE_URL = f\"https://{DEDICATED_ENDPOINT_DNS}/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "client = openai.OpenAI(base_url=BASE_URL, api_key=creds.token)\n",
        "\n",
        "model_response = client.chat.completions.create(\n",
        "    model=\"\",\n",
        "    messages=[{\"role\": \"user\", \"content\": user_message}],\n",
        "    temperature=temperature,\n",
        "    max_tokens=max_tokens,\n",
        ")\n",
        "print(model_response)\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZRvRJZgz1ua"
      },
      "source": [
        "## Deploy prebuilt Llama 3.1 8B and 70B with optimized vLLM\n",
        "\n",
        "The optimized vLLM serving container can provide better serving performance in some cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5DJwoySkz1ub"
      },
      "outputs": [],
      "source": [
        "# @title Deploy\n",
        "\n",
        "# @markdown This section uploads prebuilt Llama 3.1 models to Model Registry and deploys it to a Vertex AI Endpoint. It takes 15 minutes to 1 hour to finish depending on the size of the model.\n",
        "\n",
        "# @markdown NVIDIA_L4 GPUs are used for demonstration. The serving efficiency of L4 GPUs is inferior to that of H100 GPUs, but L4 GPUs are nevertheless good serving solutions if you do not have H100 quota.\n",
        "\n",
        "# @markdown Recommended Serving Configurations: This example recommends using A100 80G or H100 GPUs for optimal serving efficiency and performance. These GPUs are now readily available and are the preferred options for deploying these models.\n",
        "\n",
        "# @markdown Set the model to deploy.\n",
        "\n",
        "base_model_name = \"Meta-Llama-3.1-8B\"  # @param [\"Meta-Llama-3.1-8B\", \"Meta-Llama-3.1-8B-Instruct\", \"Meta-Llama-3.1-70B\", \"Meta-Llama-3.1-70B-Instruct\"] {isTemplate:true}\n",
        "model_id = os.path.join(VERTEX_AI_MODEL_GARDEN_LLAMA_3_1, base_model_name)\n",
        "hf_model_id = \"meta-llama/\" + base_model_name\n",
        "\n",
        "# The pre-built serving docker images.\n",
        "OPTIMIZED_VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai-restricted/vertex-vision-model-garden-dockers/pytorch-vllm-optimized-serve:20241029_0835_RC00\"\n",
        "\n",
        "# @markdown Find Vertex AI prediction supported accelerators and regions at https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
        "if \"8b\" in base_model_name.lower():\n",
        "    accelerator_type = \"NVIDIA_L4\"\n",
        "    machine_type = \"g2-standard-12\"\n",
        "    accelerator_count = 1\n",
        "elif \"70b\" in base_model_name.lower():\n",
        "    accelerator_type = \"NVIDIA_H100_80GB\"\n",
        "    machine_type = \"a3-highgpu-8g\"\n",
        "    accelerator_count = 8\n",
        "else:\n",
        "    raise ValueError(\n",
        "        f\"Recommended GPU setting not found for: {accelerator_type} and {base_model_name}.\"\n",
        "    )\n",
        "\n",
        "common_util.check_quota(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    is_for_training=False,\n",
        ")\n",
        "\n",
        "max_model_len = 8192  # Maximum context length.\n",
        "\n",
        "\n",
        "def deploy_model_optimized_vllm(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    service_account: str,\n",
        "    base_model_id: str = None,\n",
        "    machine_type: str = \"g2-standard-12\",\n",
        "    accelerator_type: str = \"NVIDIA_L4\",\n",
        "    accelerator_count: int = 1,\n",
        "    max_model_len: int = 4096,\n",
        "    enable_trust_remote_code: bool = False,\n",
        "    use_dedicated_endpoint: bool = False,\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys models with optimized vLLM into Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(\n",
        "        display_name=f\"{model_name}-endpoint\",\n",
        "        dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
        "    )\n",
        "\n",
        "    if not base_model_id:\n",
        "        base_model_id = model_id\n",
        "\n",
        "    vllm_args = [\n",
        "        \"python\",\n",
        "        \"-m\",\n",
        "        \"vllm.entrypoints.api_server\",\n",
        "        \"--host=0.0.0.0\",\n",
        "        \"--port=8080\",\n",
        "        f\"--model={model_id}\",\n",
        "        f\"--tensor-parallel-size={accelerator_count}\",\n",
        "        \"--swap-space=16\",\n",
        "        f\"--max-model-len={max_model_len}\",\n",
        "        \"--disable-log-stats\",\n",
        "    ]\n",
        "\n",
        "    if enable_trust_remote_code:\n",
        "        vllm_args.append(\"--trust-remote-code\")\n",
        "\n",
        "    env_vars = {\n",
        "        \"MODEL_ID\": base_model_id,\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "    }\n",
        "\n",
        "    # HF_TOKEN is not a compulsory field and may not be defined.\n",
        "    try:\n",
        "        if HF_TOKEN:\n",
        "            env_vars[\"HF_TOKEN\"] = HF_TOKEN\n",
        "    except NameError:\n",
        "        pass\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=OPTIMIZED_VLLM_DOCKER_URI,\n",
        "        serving_container_args=vllm_args,\n",
        "        serving_container_ports=[8080],\n",
        "        serving_container_predict_route=\"/generate\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "        serving_container_environment_variables=env_vars,\n",
        "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
        "        serving_container_deployment_timeout=7200,\n",
        "    )\n",
        "    print(\n",
        "        f\"Deploying {model_name} on {machine_type} with {accelerator_count} {accelerator_type} GPU(s).\"\n",
        "    )\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=service_account,\n",
        "        system_labels={\n",
        "            \"NOTEBOOK_NAME\": \"model_garden_pytorch_llama3_1_deployment.ipynb\",\n",
        "        },\n",
        "    )\n",
        "    print(\"endpoint_name:\", endpoint.name)\n",
        "\n",
        "    return model, endpoint\n",
        "\n",
        "\n",
        "(\n",
        "    models[\"optimized_vllm_gpu\"],\n",
        "    endpoints[\"optimized_vllm_gpu\"],\n",
        ") = deploy_model_optimized_vllm(\n",
        "    model_name=common_util.get_job_name_with_datetime(prefix=\"llama3_1-serve\"),\n",
        "    model_id=model_id,\n",
        "    base_model_id=hf_model_id,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    max_model_len=max_model_len,\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        ")\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "D2Tp3HU5z1ub"
      },
      "outputs": [],
      "source": [
        "# @title Raw predict\n",
        "\n",
        "# @markdown Once deployment succeeds, you can send requests to the endpoint with text prompts. Sampling parameters supported by vLLM can be found [here](https://docs.vllm.ai/en/latest/dev/sampling_params.html).\n",
        "\n",
        "# @markdown Example:\n",
        "\n",
        "# @markdown ```\n",
        "# @markdown Human: What is a car?\n",
        "# @markdown Assistant:  A car, or a motor car, is a road-connected human-transportation system used to move people or goods from one place to another. The term also encompasses a wide range of vehicles, including motorboats, trains, and aircrafts. Cars typically have four wheels, a cabin for passengers, and an engine or motor. They have been around since the early 19th century and are now one of the most popular forms of transportation, used for daily commuting, shopping, and other purposes.\n",
        "# @markdown ```\n",
        "# @markdown Additionally, you can moderate the generated text with Vertex AI. See [Moderate text documentation](https://cloud.google.com/natural-language/docs/moderating-text) for more details.\n",
        "\n",
        "# Loads an existing endpoint instance using the endpoint name:\n",
        "# - Using `endpoint_name = endpoint.name` allows us to get the\n",
        "#   endpoint name of the endpoint `endpoint` created in the cell\n",
        "#   above.\n",
        "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
        "#   an existing endpoint with the ID 1234567890123456789.\n",
        "# You may uncomment the code below to load an existing endpoint.\n",
        "\n",
        "# endpoint_name = \"\"  # @param {type:\"string\"}\n",
        "# aip_endpoint_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
        "# )\n",
        "# endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
        "\n",
        "prompt = \"What is a car?\"  # @param {type: \"string\"}\n",
        "max_tokens = 50  # @param {type:\"integer\"}\n",
        "temperature = 1.0  # @param {type:\"number\"}\n",
        "top_p = 1.0  # @param {type:\"number\"}\n",
        "top_k = 1  # @param {type:\"integer\"}\n",
        "raw_response = True  # @param {type:\"boolean\"}\n",
        "\n",
        "# Overrides parameters for inferences.\n",
        "instances = [\n",
        "    {\n",
        "        \"prompt\": prompt,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": top_p,\n",
        "        \"top_k\": top_k,\n",
        "        \"raw_response\": raw_response,\n",
        "    },\n",
        "]\n",
        "response = endpoints[\"optimized_vllm_gpu\"].predict(\n",
        "    instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
        ")\n",
        "\n",
        "for prediction in response.predictions:\n",
        "    print(prediction)\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Tz_JwXbKz1ub"
      },
      "outputs": [],
      "source": [
        "# @title Chat completion\n",
        "\n",
        "ENDPOINT_RESOURCE_NAME = \"projects/{}/locations/{}/endpoints/{}\".format(\n",
        "    PROJECT_ID, REGION, endpoints[\"optimized_vllm_gpu\"].name\n",
        ")\n",
        "\n",
        "# @title Chat Completions Inference\n",
        "\n",
        "# @markdown Once deployment succeeds, you can send requests to the endpoint using the OpenAI SDK.\n",
        "\n",
        "# @markdown First you will need to install the SDK and some auth-related dependencies.\n",
        "\n",
        "! pip install -qU openai google-auth requests\n",
        "\n",
        "# @markdown Next fill out some request parameters:\n",
        "\n",
        "user_message = \"How is your day going?\"  # @param {type: \"string\"}\n",
        "# @markdown If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, such as set `max_tokens` as 20.\n",
        "max_tokens = 50  # @param {type: \"integer\"}\n",
        "temperature = 1.0  # @param {type: \"number\"}\n",
        "\n",
        "# @markdown Now we can send a request.\n",
        "\n",
        "import google.auth\n",
        "import openai\n",
        "\n",
        "creds, project = google.auth.default()\n",
        "auth_req = google.auth.transport.requests.Request()\n",
        "creds.refresh(auth_req)\n",
        "\n",
        "BASE_URL = (\n",
        "    f\"https://{REGION}-aiplatform.googleapis.com/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
        ")\n",
        "try:\n",
        "    if use_dedicated_endpoint:\n",
        "        BASE_URL = f\"https://{DEDICATED_ENDPOINT_DNS}/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "client = openai.OpenAI(base_url=BASE_URL, api_key=creds.token)\n",
        "\n",
        "model_response = client.chat.completions.create(\n",
        "    model=\"\",\n",
        "    messages=[{\"role\": \"user\", \"content\": user_message}],\n",
        "    temperature=temperature,\n",
        "    max_tokens=max_tokens,\n",
        ")\n",
        "print(model_response)\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JETd33jIDcjm"
      },
      "source": [
        "## Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "911406c1561e"
      },
      "outputs": [],
      "source": [
        "# @title Delete the models and endpoints\n",
        "\n",
        "# @markdown  Delete the experiment models and endpoints to recycle the resources\n",
        "# @markdown  and avoid unnecessary continuous charges that may incur.\n",
        "\n",
        "# Undeploy model and delete endpoint.\n",
        "for endpoint in endpoints.values():\n",
        "    endpoint.delete(force=True)\n",
        "\n",
        "# Delete models.\n",
        "for model in models.values():\n",
        "    model.delete()\n",
        "\n",
        "delete_bucket = False  # @param {type:\"boolean\"}\n",
        "if delete_bucket:\n",
        "    ! gcloud storage rm --recursive $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FytZFLIOqoeF"
      },
      "source": [
        "## Debugging Common Bugs and Issues"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gIYEc7mn3OC"
      },
      "source": [
        "### HuggingFace Token Needed\n",
        "\n",
        "<img src=\"https://cloud.google.com/static/vertex-ai/generative-ai/docs/open-models/vllm/images/hf_token_needed.png\">\n",
        "\n",
        "The screenshot displays a log entry in Google Cloud's Log Explorer showing an error message related to accessing the Meta LLaMA-3.2-11B-Vision model hosted on Hugging Face. The error indicates that access to the model is restricted, requiring authentication to proceed. The message specifically states, \"Cannot access gated repo for URL,\" highlighting that the model is gated and requires proper authentication credentials to be accessed. This log entry can help troubleshoot authentication issues when working with restricted resources in external repositories.\n",
        "\n",
        "To resolve this issue, verify the permissions of your HuggingFace access token. Copy the latest token and deploy a new endpoint."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMkvHsiEnjGu"
      },
      "source": [
        "### Chat Template Needed\n",
        "\n",
        "<img src=\"https://cloud.google.com/static/vertex-ai/generative-ai/docs/open-models/vllm/images/chat_template_needed.png\">\n",
        "\n",
        "This screenshot shows a log entry in Google Cloud's Log Explorer, where a ValueError occurs due to a missing chat template in the transformers library version 4.44. The error message indicates that the default chat template is no longer allowed, and a custom chat template must be provided if the tokenizer does not define one. This error highlights a recent change in the library requiring explicit definition of a chat template, useful for debugging issues when deploying chat-based applications.\n",
        "\n",
        "To bypass this, make sure you use an Instruct model during deployment as shown in the above sections. In the  case of Llava models for example, you can provide a chat template."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-t02XHHm5C7"
      },
      "source": [
        "### Model Max Seq Len\n",
        "\n",
        "<img src=\"https://cloud.google.com/static/vertex-ai/generative-ai/docs/open-models/vllm/images/model_max_seq_len.png\">\n",
        "\n",
        "ValueError: The model's max seq len (4096) is larger than the maximum number of tokens that can be stored in KV cache (2256). Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.\n",
        "\n",
        "To resolve this problem, set max_model_len 2048, which is less than 2256. Another resolution for this issue is to use more or larger GPUs. `tensor-parallel-size` will need to be set appropriately if opting to use more GPUs."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_vllm_text_only_tutorial.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

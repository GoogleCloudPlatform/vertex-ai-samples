{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SgQ6t5bqZVlH"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Vertex AI Model Garden - DeepSeek (Deployment)\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/instances\">\n",
        "      <img alt=\"Workbench logo\" src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" width=\"32px\"><br> Run in Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_pytorch_deepseek_deployment.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_deepseek_deployment.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates serving DeepSeek models with [vLLM](https://github.com/vllm-project/vllm), [SGLang](https://github.com/sgl-project/sglang), or [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM). [DeepSeek-V3](https://huggingface.co/deepseek-ai/DeepSeek-V3) is a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. [DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1) is one of the first-generation reasoning models introduced by DeepSeek and offers performance comparable to OpenAI-o1 across math, code, and reasoning tasks.\n",
        "\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Deploy DeepSeek-V3 and DeepSeek-R1 largest variants with vLLM, SGLang, or TensorRT-LLM on GPU using single-host and multi-host serving, and [Spot VMs](https://cloud.google.com/compute/docs/instances/spot) (Optional). Multi-host GPU serving is a preview feature.\n",
        "\n",
        "### File a bug\n",
        "\n",
        "File a bug on [GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/issues/new) if you encounter any issue with the notebook.\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "264c07757582"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ax7zWynUDcjk"
      },
      "outputs": [],
      "source": [
        "# @title Request for quota\n",
        "\n",
        "# @markdown To deploy the largest variants of the DeepSeek models, you need 1 host of 8 x H200 machine, or 2 hosts of 8 x H100 machines (which gives a total of 16 x H100s). Check that you have sufficient quota:\n",
        "# @markdown - For Spot VM quota, check [`CustomModelServingPreemptibleH100GPUsPerProjectPerRegion`](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_preemptible_nvidia_h100_gpus). H200 GPUs are currently not available in Spot VM quota.\n",
        "# @markdown - For regular VM quota, check [`CustomModelServingH200GPUsPerProjectPerRegion`](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_h200_gpus) and [`CustomModelServingH100GPUsPerProjectPerRegion`](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_h100_gpus).\n",
        "#\n",
        "# @markdown If you don't have sufficient quota, request for quota following the instructions at [\"Request a higher quota\"](https://cloud.google.com/docs/quota/view-manage#requesting_higher_quota)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YXFGIp1l-qtT"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown 2. **[Optional]** Set region. If left unchanged, the region defaults to us-east4 for using H200 GPUs.\n",
        "\n",
        "REGION = \"us-east4\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown 3. If you want to run predictions with H100 GPUs, we recommend using the regions listed below. **NOTE:** Make sure you have associated quota in selected regions. Click the links to see your current quota for Spot VM H100 GPUs: [`CustomModelServingPreemptibleH100GPUsPerProjectPerRegion`](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_preemptible_nvidia_h100_gpus) and regular VM H100s: [`CustomModelServingH100GPUsPerProjectPerRegion`](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_h100_gpus)..\n",
        "\n",
        "# @markdown > | Machine Type | Accelerator Type | Recommended Regions |\n",
        "# @markdown | ----------- | ----------- | ----------- |\n",
        "# @markdown | a3-highgpu-8g (Spot VM) | 8 NVIDIA_H100_80GB | us-central1, europe-west4, asia-southeast1 |\n",
        "# @markdown | a3-highgpu-8g (regular VM) | 8 NVIDIA_H100_80GB | us-central1, europe-west4, us-west1, asia-southeast1 |\n",
        "\n",
        "# Upgrade Vertex AI SDK.\n",
        "! pip3 install --upgrade --quiet 'google-cloud-aiplatform==1.93.1'\n",
        "\n",
        "# Import the necessary packages\n",
        "import importlib\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from typing import Tuple\n",
        "\n",
        "import requests\n",
        "from google import auth\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "# Upgrade Vertex AI SDK.\n",
        "if os.environ.get(\"VERTEX_PRODUCT\") != \"COLAB_ENTERPRISE\":\n",
        "    ! pip install --upgrade tensorflow\n",
        "! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
        "\n",
        "common_util = importlib.import_module(\n",
        "    \"vertex-ai-samples.community-content.vertex_model_garden.model_oss.notebook_util.common_util\"\n",
        ")\n",
        "\n",
        "\n",
        "def check_quota(\n",
        "    project_id: str,\n",
        "    region: str,\n",
        "    resource_id: str,\n",
        "    accelerator_count: int,\n",
        "):\n",
        "    \"\"\"Checks if the project and the region has the required quota.\"\"\"\n",
        "    quota = common_util.get_quota(project_id, region, resource_id)\n",
        "    quota_request_instruction = (\n",
        "        \"Either use \"\n",
        "        \"a different region or request additional quota. Follow \"\n",
        "        \"instructions here \"\n",
        "        \"https://cloud.google.com/docs/quotas/view-manage#requesting_higher_quota\"\n",
        "        \" to check quota in a region or request additional quota for \"\n",
        "        \"your project.\"\n",
        "    )\n",
        "    if quota == -1:\n",
        "        raise ValueError(\n",
        "            f\"Quota not found for: {resource_id} in {region}.\"\n",
        "            f\" {quota_request_instruction}\"\n",
        "        )\n",
        "    if quota < accelerator_count:\n",
        "        raise ValueError(\n",
        "            f\"Quota not enough for {resource_id} in {region}: {quota} <\"\n",
        "            f\" {accelerator_count}. {quota_request_instruction}\"\n",
        "        )\n",
        "\n",
        "\n",
        "LABEL = \"vllm_gpu\"\n",
        "models, endpoints = {}, {}\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "if not REGION:\n",
        "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "! gcloud config set project $PROJECT_ID\n",
        "\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(\n",
        "    project=PROJECT_ID,\n",
        "    location=REGION,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-XybZjtgF9M"
      },
      "source": [
        "## Deploy DeepSeek-V3 and DeepSeek-R1 with vLLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "E8OiHHNNE_wj"
      },
      "outputs": [],
      "source": [
        "# @title Set the model variants\n",
        "\n",
        "# @markdown It's recommended to use the region selected by the deployment button on the model card. If the deployment button is not available, it's recommended to stay with the default region of the notebook.\n",
        "\n",
        "# @markdown Multi-host GPU serving is a preview feature.\n",
        "\n",
        "# @markdown Set the model to deploy.\n",
        "\n",
        "base_model_name = \"DeepSeek-R1\"  # @param [\"DeepSeek-V3\", \"DeepSeek-V3-Base\", \"DeepSeek-V3-0324\", \"DeepSeek-R1\", \"DeepSeek-R1-0528\"] {isTemplate:true}\n",
        "model_id = \"deepseek-ai/\" + base_model_name\n",
        "hf_model_id = model_id\n",
        "if \"R1\" in model_id:\n",
        "    model_user_id = \"deepseek-r1\"\n",
        "    model_id = f\"gs://vertex-model-garden-restricted-us/{model_id}\"\n",
        "else:\n",
        "    model_user_id = \"deepseek-v3\"\n",
        "\n",
        "PUBLISHER_MODEL_NAME = (\n",
        "    f\"publishers/deepseek-ai/models/{model_user_id}@{base_model_name.lower()}\"\n",
        ")\n",
        "\n",
        "# @markdown Set use_dedicated_endpoint to False if you don't want to use [dedicated endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment#create-dedicated-endpoint). Note that [dedicated endpoint does not support VPC Service Controls](https://cloud.google.com/vertex-ai/docs/predictions/choose-endpoint-type), uncheck the box if you are using VPC-SC.\n",
        "use_dedicated_endpoint = True  # @param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "acd75fc92341"
      },
      "outputs": [],
      "source": [
        "# @title Deploy with customized configs\n",
        "\n",
        "# @markdown This section uploads DeepSeek models to Model Registry and deploys them to a Vertex Prediction Endpoint. It takes ~1 hour to finish.\n",
        "\n",
        "# @markdown The following vLLM container version has been validated. The version will be continuously updated to incorporate latest optimizations and features.\n",
        "# The pre-built serving docker image for vLLM past v0.7.3, https://github.com/vllm-project/vllm/commit/f6bb18fd9a19e5e4fb1991339638fc666d06b27a.\n",
        "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20250312_0916_RC01\"\n",
        "\n",
        "# @markdown Choose whether to use a [Spot VM](https://cloud.google.com/compute/docs/instances/spot) for the deployment.\n",
        "is_spot = False  # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown Find Vertex AI prediction supported accelerators and regions at https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
        "accelerator_type = \"NVIDIA_H200_141GB\"  # @param [\"NVIDIA_H200_141GB\", \"NVIDIA_H100_80GB\"] {isTemplate:true}\n",
        "accelerator_count = 8\n",
        "if accelerator_type == \"NVIDIA_H200_141GB\":\n",
        "    machine_type = \"a3-ultragpu-8g\"\n",
        "    multihost_gpu_node_count = 1\n",
        "    if is_spot:\n",
        "        raise ValueError(\"H200 GPUs are currently not available in Spot VM quota.\")\n",
        "    else:\n",
        "        resource_id = \"custom_model_serving_nvidia_h200_gpus\"\n",
        "else:\n",
        "    machine_type = \"a3-highgpu-8g\"\n",
        "    multihost_gpu_node_count = 2\n",
        "    if is_spot:\n",
        "        resource_id = \"custom_model_serving_preemptible_nvidia_h100_gpus\"\n",
        "    else:\n",
        "        resource_id = \"custom_model_serving_nvidia_h100_gpus\"\n",
        "\n",
        "check_quota(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    resource_id=resource_id,\n",
        "    accelerator_count=int(accelerator_count * multihost_gpu_node_count),\n",
        ")\n",
        "\n",
        "if accelerator_type == \"NVIDIA_H200_141GB\":\n",
        "    # @markdown With a single host of 8 x H200s, speculative decoding with MTP and a context length of 8192 are supported in the specified configuration. The configuration has been validated for stability and performance.\n",
        "    pipeline_parallel_size = 1\n",
        "    gpu_memory_utilization = 0.75\n",
        "    max_model_len = 8192  # Maximum context length.\n",
        "    enable_chunked_prefill = False\n",
        "    max_num_seqs = 64\n",
        "    kv_cache_dtype = \"auto\"\n",
        "    num_speculative_tokens = 3\n",
        "    speculative_draft_tensor_parallel_size = 8\n",
        "else:\n",
        "    # @markdown With 2 hosts of 8 x H100s, chunked prefill and a context length of 163840 are supported in the specified configuration. The configuration has been validated for stability and performance.\n",
        "    pipeline_parallel_size = 2\n",
        "    gpu_memory_utilization = 0.82\n",
        "    max_model_len = 163840  # Maximum context length.\n",
        "    enable_chunked_prefill = True\n",
        "    max_num_seqs = 64\n",
        "    kv_cache_dtype = \"auto\"\n",
        "    num_speculative_tokens = None\n",
        "    speculative_draft_tensor_parallel_size = None\n",
        "\n",
        "\n",
        "# # The pre-built serving docker image and configuration for vLLM v0.7.2.\n",
        "# VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20250304_0916_RC01\"\n",
        "# accelerator_type = \"NVIDIA_H100_80GB\"\n",
        "# accelerator_count = 8\n",
        "# machine_type = \"a3-highgpu-8g\"\n",
        "# multihost_gpu_node_count = 2\n",
        "# pipeline_parallel_size = 2\n",
        "# gpu_memory_utilization = 0.8\n",
        "# max_model_len = 4096  # Maximum context length.\n",
        "# enable_chunked_prefill = False\n",
        "# max_num_seqs = 64\n",
        "# kv_cache_dtype = \"auto\"\n",
        "# num_speculative_tokens = None\n",
        "# speculative_draft_tensor_parallel_size = None\n",
        "\n",
        "# # The pre-built serving docker image and configuration for vLLM v0.6.6.post1.\n",
        "# VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20250130_0916_RC01\"\n",
        "# accelerator_type = \"NVIDIA_H100_80GB\"\n",
        "# accelerator_count = 8\n",
        "# machine_type = \"a3-highgpu-8g\"\n",
        "# multihost_gpu_node_count = 2\n",
        "# pipeline_parallel_size = 1\n",
        "# gpu_memory_utilization = 0.9\n",
        "# max_model_len = 32768  # Maximum context length.\n",
        "# enable_chunked_prefill = False\n",
        "# max_num_seqs = 128\n",
        "# kv_cache_dtype = \"fp8\"\n",
        "# num_speculative_tokens = None\n",
        "# speculative_draft_tensor_parallel_size = None\n",
        "\n",
        "\n",
        "# Enable automatic prefix caching using GPU HBM\n",
        "enable_prefix_cache = False\n",
        "# Setting this value >0 will use the idle host memory for a second-tier prefix kv\n",
        "# cache beneath the HBM cache. It only has effect if enable_prefix_cache=True.\n",
        "# The range of this value: [0, 1)\n",
        "# Setting host_prefix_kv_cache_utilization_target to 0 will disable the host memory prefix kv cache.\n",
        "host_prefix_kv_cache_utilization_target = 0\n",
        "\n",
        "# @markdown To enable the auto-scaling in deployment, you can set the following options:\n",
        "\n",
        "min_replica_count = 1  # @param {type:\"integer\"}\n",
        "max_replica_count = 1  # @param {type:\"integer\"}\n",
        "required_replica_count = 1  # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown Set the target of GPU duty cycle or CPU usage between 1 and 100 for auto-scaling.\n",
        "autoscale_by_gpu_duty_cycle_target = 0  # @param {type:\"integer\"}\n",
        "autoscale_by_cpu_usage_target = 0  # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown Note: GPU duty cycle is not the most accurate metric for scaling workloads. More advanced auto-scaling metrics are coming soon. See [the public doc](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#autoscaling) for more details.\n",
        "\n",
        "\n",
        "def deploy_model_vllm_multihost_spec_decode(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    publisher: str,\n",
        "    publisher_model_id: str,\n",
        "    service_account: str = None,\n",
        "    base_model_id: str = None,\n",
        "    machine_type: str = \"g2-standard-8\",\n",
        "    accelerator_type: str = \"NVIDIA_L4\",\n",
        "    accelerator_count: int = 1,\n",
        "    multihost_gpu_node_count: int = 1,\n",
        "    pipeline_parallel_size: int = 1,\n",
        "    gpu_memory_utilization: float = 0.9,\n",
        "    max_model_len: int = 4096,\n",
        "    dtype: str = \"auto\",\n",
        "    kv_cache_dtype: str = \"auto\",\n",
        "    enable_trust_remote_code: bool = False,\n",
        "    enforce_eager: bool = False,\n",
        "    enable_lora: bool = False,\n",
        "    enable_chunked_prefill: bool = False,\n",
        "    enable_prefix_cache: bool = False,\n",
        "    host_prefix_kv_cache_utilization_target: float = 0.0,\n",
        "    max_loras: int = 1,\n",
        "    max_cpu_loras: int = 8,\n",
        "    use_dedicated_endpoint: bool = False,\n",
        "    max_num_seqs: int = 256,\n",
        "    num_speculative_tokens: int = None,\n",
        "    speculative_draft_tensor_parallel_size: int = None,\n",
        "    model_type: str = None,\n",
        "    min_replica_count: int = 1,\n",
        "    max_replica_count: int = 1,\n",
        "    required_replica_count: int = 1,\n",
        "    autoscale_by_gpu_duty_cycle_target: int = 0,\n",
        "    autoscale_by_cpu_usage_target: int = 0,\n",
        "    is_spot: bool = True,\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(\n",
        "        display_name=f\"{model_name}-endpoint\",\n",
        "        dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
        "    )\n",
        "\n",
        "    if not base_model_id:\n",
        "        base_model_id = model_id\n",
        "\n",
        "    # See https://docs.vllm.ai/en/latest/models/engine_args.html for a list of possible arguments with descriptions.\n",
        "    vllm_args = [\n",
        "        \"python\",\n",
        "        \"-m\",\n",
        "        \"vllm.entrypoints.api_server\",\n",
        "        \"--host=0.0.0.0\",\n",
        "        \"--port=8080\",\n",
        "        f\"--model={model_id}\",\n",
        "        f\"--tensor-parallel-size={int(accelerator_count * multihost_gpu_node_count / pipeline_parallel_size)}\",\n",
        "        f\"--pipeline-parallel-size={pipeline_parallel_size}\",\n",
        "        \"--swap-space=16\",\n",
        "        f\"--gpu-memory-utilization={gpu_memory_utilization}\",\n",
        "        f\"--max-model-len={max_model_len}\",\n",
        "        f\"--dtype={dtype}\",\n",
        "        f\"--kv-cache-dtype={kv_cache_dtype}\",\n",
        "        f\"--max-loras={max_loras}\",\n",
        "        f\"--max-cpu-loras={max_cpu_loras}\",\n",
        "        f\"--max-num-seqs={max_num_seqs}\",\n",
        "        \"--disable-log-requests\",\n",
        "    ]\n",
        "\n",
        "    if multihost_gpu_node_count > 1:\n",
        "        vllm_args = [\"/vllm-workspace/ray_launcher.sh\"] + vllm_args\n",
        "\n",
        "    if enable_trust_remote_code:\n",
        "        vllm_args.append(\"--trust-remote-code\")\n",
        "\n",
        "    if enforce_eager:\n",
        "        vllm_args.append(\"--enforce-eager\")\n",
        "\n",
        "    if enable_lora:\n",
        "        vllm_args.append(\"--enable-lora\")\n",
        "\n",
        "    if enable_chunked_prefill:\n",
        "        vllm_args.append(\"--enable-chunked-prefill\")\n",
        "\n",
        "    if enable_prefix_cache:\n",
        "        vllm_args.append(\"--enable-prefix-caching\")\n",
        "\n",
        "    if 0 < host_prefix_kv_cache_utilization_target < 1:\n",
        "        vllm_args.append(\n",
        "            f\"--host-prefix-kv-cache-utilization-target={host_prefix_kv_cache_utilization_target}\"\n",
        "        )\n",
        "\n",
        "    if num_speculative_tokens is not None:\n",
        "        vllm_args.append(f\"--num-speculative-tokens={num_speculative_tokens}\")\n",
        "\n",
        "    if speculative_draft_tensor_parallel_size is not None:\n",
        "        vllm_args.append(\n",
        "            f\"--speculative-draft-tensor-parallel-size={speculative_draft_tensor_parallel_size}\"\n",
        "        )\n",
        "\n",
        "    if model_type:\n",
        "        vllm_args.append(f\"--model-type={model_type}\")\n",
        "\n",
        "    env_vars = {\n",
        "        \"MODEL_ID\": base_model_id,\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "    }\n",
        "\n",
        "    # HF_TOKEN is not a compulsory field and may not be defined.\n",
        "    try:\n",
        "        if HF_TOKEN:\n",
        "            env_vars[\"HF_TOKEN\"] = HF_TOKEN\n",
        "    except NameError:\n",
        "        pass\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
        "        serving_container_args=vllm_args,\n",
        "        serving_container_ports=[8080],\n",
        "        serving_container_predict_route=\"/generate\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "        serving_container_environment_variables=env_vars,\n",
        "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
        "        serving_container_deployment_timeout=7200,\n",
        "        model_garden_source_model_name=(\n",
        "            f\"publishers/{publisher}/models/{publisher_model_id}\"\n",
        "        ),\n",
        "    )\n",
        "    print(\n",
        "        f\"Deploying {model_name} on {machine_type} with {int(accelerator_count * multihost_gpu_node_count)} {accelerator_type} GPU(s).\"\n",
        "    )\n",
        "\n",
        "    creds, _ = auth.default()\n",
        "    auth_req = auth.transport.requests.Request()\n",
        "    creds.refresh(auth_req)\n",
        "\n",
        "    url = f\"https://{REGION}-aiplatform.googleapis.com/ui/projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint.name}:deployModel\"\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {creds.token}\",\n",
        "    }\n",
        "    data = {\n",
        "        \"deployedModel\": {\n",
        "            \"model\": model.resource_name,\n",
        "            \"displayName\": model_name,\n",
        "            \"dedicatedResources\": {\n",
        "                \"machineSpec\": {\n",
        "                    \"machineType\": machine_type,\n",
        "                    \"multihostGpuNodeCount\": multihost_gpu_node_count,\n",
        "                    \"acceleratorType\": accelerator_type,\n",
        "                    \"acceleratorCount\": accelerator_count,\n",
        "                },\n",
        "                \"minReplicaCount\": min_replica_count,\n",
        "                \"requiredReplicaCount\": required_replica_count,\n",
        "                \"maxReplicaCount\": max_replica_count,\n",
        "            },\n",
        "            \"system_labels\": {\n",
        "                \"NOTEBOOK_NAME\": \"model_garden_pytorch_deepseek_deployment.ipynb\",\n",
        "                \"NOTEBOOK_ENVIRONMENT\": common_util.get_deploy_source(),\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "    if service_account:\n",
        "        data[\"deployedModel\"][\"serviceAccount\"] = service_account\n",
        "    if is_spot:\n",
        "        data[\"deployedModel\"][\"dedicatedResources\"][\"spot\"] = True\n",
        "    if autoscale_by_gpu_duty_cycle_target > 0 or autoscale_by_cpu_usage_target > 0:\n",
        "        data[\"deployedModel\"][\"dedicatedResources\"][\"autoscalingMetricSpecs\"] = []\n",
        "        if autoscale_by_gpu_duty_cycle_target > 0:\n",
        "            data[\"deployedModel\"][\"dedicatedResources\"][\n",
        "                \"autoscalingMetricSpecs\"\n",
        "            ].append(\n",
        "                {\n",
        "                    \"metricName\": \"aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle\",\n",
        "                    \"target\": autoscale_by_gpu_duty_cycle_target,\n",
        "                }\n",
        "            )\n",
        "        if autoscale_by_cpu_usage_target > 0:\n",
        "            data[\"deployedModel\"][\"dedicatedResources\"][\n",
        "                \"autoscalingMetricSpecs\"\n",
        "            ].append(\n",
        "                {\n",
        "                    \"metricName\": \"aiplatform.googleapis.com/prediction/online/cpu/utilization\",\n",
        "                    \"target\": autoscale_by_cpu_usage_target,\n",
        "                }\n",
        "            )\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "    print(f\"Deploy Model response: {response.json()}\")\n",
        "    if response.status_code != 200 or \"name\" not in response.json():\n",
        "        raise ValueError(f\"Failed to deploy model: {response.text}\")\n",
        "    common_util.poll_and_wait(response.json()[\"name\"], REGION, 7200)\n",
        "    print(\"endpoint_name:\", endpoint.name)\n",
        "\n",
        "    return model, endpoint\n",
        "\n",
        "\n",
        "models[\"vllm_gpu\"], endpoints[\"vllm_gpu\"] = deploy_model_vllm_multihost_spec_decode(\n",
        "    model_name=common_util.get_job_name_with_datetime(prefix=\"deepseek-serve\"),\n",
        "    model_id=model_id,\n",
        "    publisher=\"deepseek-ai\",\n",
        "    publisher_model_id=(\"deepseek-v3\" if \"V3\" in model_id else \"deepseek-r1\"),\n",
        "    base_model_id=hf_model_id,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    multihost_gpu_node_count=multihost_gpu_node_count,\n",
        "    pipeline_parallel_size=pipeline_parallel_size,\n",
        "    gpu_memory_utilization=gpu_memory_utilization,\n",
        "    max_model_len=max_model_len,\n",
        "    max_num_seqs=max_num_seqs,\n",
        "    kv_cache_dtype=kv_cache_dtype,\n",
        "    enable_trust_remote_code=True,\n",
        "    enforce_eager=False,\n",
        "    enable_lora=False,\n",
        "    enable_chunked_prefill=enable_chunked_prefill,\n",
        "    num_speculative_tokens=num_speculative_tokens,\n",
        "    speculative_draft_tensor_parallel_size=speculative_draft_tensor_parallel_size,\n",
        "    enable_prefix_cache=enable_prefix_cache,\n",
        "    host_prefix_kv_cache_utilization_target=host_prefix_kv_cache_utilization_target,\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        "    min_replica_count=min_replica_count,\n",
        "    max_replica_count=max_replica_count,\n",
        "    required_replica_count=required_replica_count,\n",
        "    autoscale_by_gpu_duty_cycle_target=autoscale_by_gpu_duty_cycle_target,\n",
        "    autoscale_by_cpu_usage_target=autoscale_by_cpu_usage_target,\n",
        "    is_spot=is_spot,\n",
        ")\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rDHsCOqvFYBi"
      },
      "outputs": [],
      "source": [
        "# @title Raw predict\n",
        "\n",
        "# @markdown Once deployment succeeds, you can send requests to the endpoint with text prompts. Sampling parameters supported by vLLM can be found [here](https://docs.vllm.ai/en/latest/dev/sampling_params.html).\n",
        "\n",
        "# @markdown Example:\n",
        "\n",
        "# @markdown ```\n",
        "# @markdown User: What is the best way to diagnose and fix a flickering light in my house?\n",
        "# @markdown Assistant: Okay, so I need to figure out how to diagnose and fix a flickering light in my house. Hmm, where do I start? Let's think. First, I remember that flickering lights can be caused by various issues. Maybe the bulb is loose? That's a common problem. Let me start with the simplest things first.\n",
        "# @markdown ```\n",
        "# @markdown Additionally, you can moderate the generated text with Vertex AI. See [Moderate text documentation](https://cloud.google.com/natural-language/docs/moderating-text) for more details.\n",
        "\n",
        "# Loads an existing endpoint instance using the endpoint name:\n",
        "# - Using `endpoint_name = endpoint.name` allows us to get the\n",
        "#   endpoint name of the endpoint `endpoint` created in the cell\n",
        "#   above.\n",
        "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
        "#   an existing endpoint with the ID 1234567890123456789.\n",
        "# You may uncomment the code below to load an existing endpoint.\n",
        "\n",
        "# endpoint_name = \"\"  # @param {type:\"string\"}\n",
        "# aip_endpoint_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
        "# )\n",
        "# endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
        "\n",
        "# @markdown A chat template formatted prompt for the DeepSeek-R1 model is shown below as an example.\n",
        "# @markdown A chat template formatted prompt for the DeepSeek-V3 model would be: \"<｜begin▁of▁sentence｜><｜User｜>What is the best way to diagnose and fix a flickering light in my house?<｜Assistant｜>\\n\"\n",
        "prompt = \"<｜begin▁of▁sentence｜><｜User｜>What is the best way to diagnose and fix a flickering light in my house?<｜Assistant｜><think>\\n\"  # @param {type: \"string\"}\n",
        "# @markdown If you encounter an issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, by lowering `max_tokens`.\n",
        "max_tokens = 1024  # @param {type:\"integer\"}\n",
        "temperature = 0.6  # @param {type:\"number\"}\n",
        "top_p = 0.95  # @param {type:\"number\"}\n",
        "# @markdown Set `raw_response` to `True` to obtain the raw model output. Set `raw_response` to `False` to apply additional formatting in the structure of `\"Prompt:\\n{prompt.strip()}\\nOutput:\\n{output}\"`.\n",
        "raw_response = True  # @param {type:\"boolean\"}\n",
        "\n",
        "# Overrides parameters for inferences.\n",
        "instances = [\n",
        "    {\n",
        "        \"prompt\": prompt,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": top_p,\n",
        "        \"raw_response\": raw_response,\n",
        "    },\n",
        "]\n",
        "response = endpoints[\"vllm_gpu\"].predict(\n",
        "    instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
        ")\n",
        "\n",
        "for prediction in response.predictions:\n",
        "    print(prediction)\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LSG9ITWTbTb7"
      },
      "outputs": [],
      "source": [
        "# @title Chat completion\n",
        "\n",
        "if use_dedicated_endpoint:\n",
        "    DEDICATED_ENDPOINT_DNS = endpoints[\"vllm_gpu\"].gca_resource.dedicated_endpoint_dns\n",
        "ENDPOINT_RESOURCE_NAME = endpoints[\"vllm_gpu\"].resource_name\n",
        "\n",
        "# @markdown Because the DeepSeek-R1 model generates detailed reasoning steps, the output is expected to be long. We recommend using streaming for a better generation experience.\n",
        "# @title Chat Completions Inference\n",
        "\n",
        "# @markdown Once deployment succeeds, you can send requests to the endpoint using the OpenAI SDK.\n",
        "\n",
        "# @markdown First you will need to install the SDK and some auth-related dependencies.\n",
        "\n",
        "! pip install -qU openai google-auth requests\n",
        "\n",
        "# @markdown Next fill out some request parameters:\n",
        "\n",
        "user_message = \"How is your day going?\"  # @param {type: \"string\"}\n",
        "# @markdown If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, such as set `max_tokens` as 20.\n",
        "max_tokens = 50  # @param {type: \"integer\"}\n",
        "temperature = 1.0  # @param {type: \"number\"}\n",
        "stream = False  # @param {type: \"boolean\"}\n",
        "\n",
        "# @markdown Now we can send a request.\n",
        "\n",
        "import google.auth\n",
        "import openai\n",
        "\n",
        "creds, project = google.auth.default()\n",
        "auth_req = google.auth.transport.requests.Request()\n",
        "creds.refresh(auth_req)\n",
        "\n",
        "BASE_URL = (\n",
        "    f\"https://{REGION}-aiplatform.googleapis.com/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
        ")\n",
        "try:\n",
        "    if use_dedicated_endpoint:\n",
        "        BASE_URL = f\"https://{DEDICATED_ENDPOINT_DNS}/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "client = openai.OpenAI(base_url=BASE_URL, api_key=creds.token)\n",
        "\n",
        "model_response = client.chat.completions.create(\n",
        "    model=\"\",\n",
        "    messages=[{\"role\": \"user\", \"content\": user_message}],\n",
        "    temperature=temperature,\n",
        "    max_tokens=max_tokens,\n",
        "    stream=stream,\n",
        ")\n",
        "\n",
        "if stream:\n",
        "    usage = None\n",
        "    contents = []\n",
        "    for chunk in model_response:\n",
        "        if chunk.usage is not None:\n",
        "            usage = chunk.usage\n",
        "            continue\n",
        "        print(chunk.choices[0].delta.content, end=\"\")\n",
        "        contents.append(chunk.choices[0].delta.content)\n",
        "    print(f\"\\n\\n{usage}\")\n",
        "else:\n",
        "    print(model_response)\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zJJDmldn7rw"
      },
      "source": [
        "## Deploy DeepSeek-V3 and DeepSeek-R1 with SGLang"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_3Swj3pxn7rw"
      },
      "outputs": [],
      "source": [
        "# @title Deploy\n",
        "\n",
        "# @markdown This section uploads DeepSeek models to Model Registry and deploys them to a Vertex Prediction Endpoint. It takes ~1 hour to finish.\n",
        "\n",
        "# @markdown It's recommended to use the region selected by the deployment button on the model card. If the deployment button is not available, it's recommended to stay with the default region of the notebook.\n",
        "\n",
        "# @markdown Multi-host GPU serving is a preview feature.\n",
        "\n",
        "# @markdown Set the model to deploy.\n",
        "\n",
        "base_model_name = \"DeepSeek-R1\"  # @param [\"DeepSeek-V3\", \"DeepSeek-V3-Base\", \"DeepSeek-V3-0324\", \"DeepSeek-R1\", \"DeepSeek-R1-0528\"] {isTemplate:true}\n",
        "model_id = \"deepseek-ai/\" + base_model_name\n",
        "hf_model_id = model_id\n",
        "\n",
        "# The pre-built serving docker images.\n",
        "SGLANG_DOCKER_URI = \"us-docker.pkg.dev/deeplearning-platform-release/vertex-model-garden/sglang-serve.cu124.0-4.ubuntu2204.py310:20250427-1800-rc0\"\n",
        "\n",
        "# @markdown Choose whether to use a [Spot VM](https://cloud.google.com/compute/docs/instances/spot) for the deployment.\n",
        "is_spot = False  # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown Set use_dedicated_endpoint to False if you don't want to use [dedicated endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment#create-dedicated-endpoint). Note that [dedicated endpoint does not support VPC Service Controls](https://cloud.google.com/vertex-ai/docs/predictions/choose-endpoint-type), uncheck the box if you are using VPC-SC.\n",
        "use_dedicated_endpoint = True  # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown Find Vertex AI prediction supported accelerators and regions at https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
        "accelerator_type = \"NVIDIA_H200_141GB\"  # @param [\"NVIDIA_H200_141GB\", \"NVIDIA_H100_80GB\"] {isTemplate:true}\n",
        "accelerator_count = 8\n",
        "if accelerator_type == \"NVIDIA_H200_141GB\":\n",
        "    machine_type = \"a3-ultragpu-8g\"\n",
        "    multihost_gpu_node_count = 1\n",
        "    if is_spot:\n",
        "        raise ValueError(\"H200 GPUs are currently not available in Spot VM quota.\")\n",
        "    else:\n",
        "        resource_id = \"custom_model_serving_nvidia_h200_gpus\"\n",
        "else:\n",
        "    machine_type = \"a3-highgpu-8g\"\n",
        "    multihost_gpu_node_count = 2\n",
        "    if is_spot:\n",
        "        resource_id = \"custom_model_serving_preemptible_nvidia_h100_gpus\"\n",
        "    else:\n",
        "        resource_id = \"custom_model_serving_nvidia_h100_gpus\"\n",
        "\n",
        "check_quota(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    resource_id=resource_id,\n",
        "    accelerator_count=int(accelerator_count * multihost_gpu_node_count),\n",
        ")\n",
        "\n",
        "# @markdown The maximum context length 163840 is supported in the following configurations.\n",
        "# @markdown These configuration has been validated for stability and performance.\n",
        "# @markdown 1. Low latency: This profile optimizes for low latency on small batches of incoming requests.\n",
        "# @markdown 2. High throughput: This profile optimizes for high throughput on large batches of incoming requests.\n",
        "profile = \"Low latency\"  # @param [\"Low latency\", \"High throughput\"] {isTemplate:true}\n",
        "\n",
        "# Set this value to the expected number of concurrent requests.\n",
        "torch_compile_max_bs = 4\n",
        "\n",
        "if profile == \"Low latency\":\n",
        "    enable_torch_compile = True\n",
        "    disable_cuda_graph = False\n",
        "    if base_model_name not in (\"DeepSeek-V3\", \"DeepSeek-V3-0324\", \"DeepSeek-R1\"):\n",
        "        speculative_algorithm = None\n",
        "        speculative_draft_model_path = \"\"\n",
        "        print(\n",
        "            f\"No speculative draft model is available for {base_model_name}. Performance will be degraded.\"\n",
        "        )\n",
        "    else:\n",
        "        speculative_algorithm = \"EAGLE\"\n",
        "        speculative_draft_model_path = f\"lmsys/{base_model_name}-NextN\"\n",
        "    enable_jit_deepgemm = True\n",
        "    enable_dp_attention = False\n",
        "    dp_size = 1\n",
        "else:\n",
        "    enable_torch_compile = False\n",
        "    disable_cuda_graph = False\n",
        "    speculative_algorithm = None\n",
        "    speculative_draft_model_path = \"\"\n",
        "    enable_jit_deepgemm = True\n",
        "    enable_dp_attention = True\n",
        "    dp_size = 8\n",
        "\n",
        "\n",
        "def poll_operation(op_name: str) -> bool:  # noqa: F811\n",
        "    creds, _ = auth.default()\n",
        "    auth_req = auth.transport.requests.Request()\n",
        "    creds.refresh(auth_req)\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {creds.token}\",\n",
        "    }\n",
        "    get_resp = requests.get(\n",
        "        f\"https://{REGION}-aiplatform.googleapis.com/ui/{op_name}\",\n",
        "        headers=headers,\n",
        "    )\n",
        "    opjs = get_resp.json()\n",
        "    if \"error\" in opjs:\n",
        "        raise ValueError(f\"Operation failed: {opjs['error']}\")\n",
        "    return opjs.get(\"done\", False)\n",
        "\n",
        "\n",
        "def poll_and_wait(op_name: str, total_wait: int, interval: int = 60):  # noqa: F811\n",
        "    waited = 0\n",
        "    while not poll_operation(op_name):\n",
        "        if waited > total_wait:\n",
        "            raise TimeoutError(\"Operation timed out\")\n",
        "        print(\n",
        "            f\"\\rStill waiting for operation... Waited time in second: {waited:<6}\",\n",
        "            end=\"\",\n",
        "            flush=True,\n",
        "        )\n",
        "        waited += interval\n",
        "        time.sleep(interval)\n",
        "\n",
        "\n",
        "def deploy_model_sglang_multihost(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    publisher: str,\n",
        "    publisher_model_id: str,\n",
        "    service_account: str = \"\",\n",
        "    base_model_id: str = \"\",\n",
        "    machine_type: str = \"g2-standard-8\",\n",
        "    accelerator_type: str = \"NVIDIA_L4\",\n",
        "    accelerator_count: int = 1,\n",
        "    multihost_gpu_node_count: int = 1,\n",
        "    gpu_memory_utilization: float | None = None,\n",
        "    context_length: int | None = None,\n",
        "    dtype: str | None = None,\n",
        "    enable_trust_remote_code: bool = False,\n",
        "    enable_torch_compile: bool = False,\n",
        "    torch_compile_max_bs: int | None = None,\n",
        "    attention_backend: str = \"\",\n",
        "    enable_flashinfer_mla: bool = False,\n",
        "    disable_cuda_graph: bool = False,\n",
        "    speculative_algorithm: str | None = None,\n",
        "    speculative_draft_model_path: str = \"\",\n",
        "    speculative_num_steps: int = 3,\n",
        "    speculative_eagle_topk: int = 1,\n",
        "    speculative_num_draft_tokens: int = 4,\n",
        "    enable_jit_deepgemm: bool = False,\n",
        "    enable_dp_attention: bool = False,\n",
        "    dp_size: int = 1,\n",
        "    use_dedicated_endpoint: bool = False,\n",
        "    max_num_seqs: int | None = None,\n",
        "    is_spot: bool = True,\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys trained models with SGLang into Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(\n",
        "        display_name=f\"{model_name}-endpoint\",\n",
        "        dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
        "    )\n",
        "\n",
        "    if not base_model_id:\n",
        "        base_model_id = model_id\n",
        "\n",
        "    # See https://docs.sglang.ai/backend/server_arguments.html for a list of possible arguments with descriptions.\n",
        "    sglang_args = [\n",
        "        f\"--model={model_id}\",\n",
        "        f\"--tp={accelerator_count * multihost_gpu_node_count}\",\n",
        "        f\"--dp={dp_size}\",\n",
        "    ]\n",
        "\n",
        "    if context_length:\n",
        "        sglang_args.append(f\"--context-length={context_length}\")\n",
        "\n",
        "    if gpu_memory_utilization:\n",
        "        sglang_args.append(f\"--mem-fraction-static={gpu_memory_utilization}\")\n",
        "\n",
        "    if max_num_seqs:\n",
        "        sglang_args.append(f\"--max-running-requests={max_num_seqs}\")\n",
        "\n",
        "    if dtype:\n",
        "        sglang_args.append(f\"--dtype={dtype}\")\n",
        "\n",
        "    if enable_trust_remote_code:\n",
        "        sglang_args.append(\"--trust-remote-code\")\n",
        "\n",
        "    if enable_torch_compile:\n",
        "        sglang_args.append(\"--enable-torch-compile\")\n",
        "        if torch_compile_max_bs:\n",
        "            sglang_args.append(f\"--torch-compile-max-bs={torch_compile_max_bs}\")\n",
        "\n",
        "    if attention_backend:\n",
        "        sglang_args.append(f\"--attention-backend={attention_backend}\")\n",
        "\n",
        "    if enable_flashinfer_mla:\n",
        "        sglang_args.append(\"--enable-flashinfer-mla\")\n",
        "\n",
        "    if disable_cuda_graph:\n",
        "        sglang_args.append(\"--disable-cuda-graph\")\n",
        "\n",
        "    if speculative_algorithm:\n",
        "        sglang_args.append(f\"--speculative-algorithm={speculative_algorithm}\")\n",
        "        sglang_args.append(\n",
        "            f\"--speculative-draft-model-path={speculative_draft_model_path}\"\n",
        "        )\n",
        "        sglang_args.append(f\"--speculative-num-steps={speculative_num_steps}\")\n",
        "        sglang_args.append(f\"--speculative-eagle-topk={speculative_eagle_topk}\")\n",
        "        sglang_args.append(\n",
        "            f\"--speculative-num-draft-tokens={speculative_num_draft_tokens}\"\n",
        "        )\n",
        "\n",
        "    if enable_dp_attention:\n",
        "        sglang_args.append(\"--enable-dp-attention\")\n",
        "\n",
        "    env_vars = {\n",
        "        \"MODEL_ID\": base_model_id,\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "    }\n",
        "\n",
        "    if enable_jit_deepgemm:\n",
        "        env_vars[\"SGL_ENABLE_JIT_DEEPGEMM\"] = \"1\"\n",
        "\n",
        "    # HF_TOKEN is not a compulsory field and may not be defined.\n",
        "    try:\n",
        "        if HF_TOKEN:\n",
        "            env_vars[\"HF_TOKEN\"] = HF_TOKEN\n",
        "    except NameError:\n",
        "        pass\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=SGLANG_DOCKER_URI,\n",
        "        serving_container_args=sglang_args,\n",
        "        serving_container_ports=[30000],\n",
        "        serving_container_predict_route=\"/vertex_generate\",\n",
        "        serving_container_health_route=\"/health\",\n",
        "        serving_container_environment_variables=env_vars,\n",
        "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
        "        serving_container_deployment_timeout=7200,\n",
        "        model_garden_source_model_name=(\n",
        "            f\"publishers/{publisher}/models/{publisher_model_id}\"\n",
        "        ),\n",
        "    )\n",
        "    print(\n",
        "        f\"Deploying {model_name} on {machine_type} with {int(accelerator_count * multihost_gpu_node_count)} {accelerator_type} GPU(s).\"\n",
        "    )\n",
        "\n",
        "    creds, _ = auth.default()\n",
        "    auth_req = auth.transport.requests.Request()\n",
        "    creds.refresh(auth_req)\n",
        "\n",
        "    url = f\"https://{REGION}-aiplatform.googleapis.com/ui/projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint.name}:deployModel\"\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {creds.token}\",\n",
        "    }\n",
        "    data = {\n",
        "        \"deployedModel\": {\n",
        "            \"model\": model.resource_name,\n",
        "            \"displayName\": model_name,\n",
        "            \"dedicatedResources\": {\n",
        "                \"machineSpec\": {\n",
        "                    \"machineType\": machine_type,\n",
        "                    \"multihostGpuNodeCount\": multihost_gpu_node_count,\n",
        "                    \"acceleratorType\": accelerator_type,\n",
        "                    \"acceleratorCount\": accelerator_count,\n",
        "                },\n",
        "                \"minReplicaCount\": 1,\n",
        "                \"maxReplicaCount\": 1,\n",
        "            },\n",
        "            \"system_labels\": {\n",
        "                \"NOTEBOOK_NAME\": \"model_garden_pytorch_deepseek_deployment.ipynb\",\n",
        "                \"NOTEBOOK_ENVIRONMENT\": common_util.get_deploy_source(),\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "    if service_account:\n",
        "        data[\"deployedModel\"][\"serviceAccount\"] = service_account\n",
        "    if is_spot:\n",
        "        data[\"deployedModel\"][\"dedicatedResources\"][\"spot\"] = True\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "    print(f\"Deploy Model response: {response.json()}\")\n",
        "    if response.status_code != 200 or \"name\" not in response.json():\n",
        "        raise ValueError(f\"Failed to deploy model: {response.text}\")\n",
        "    poll_and_wait(response.json()[\"name\"], 7200)\n",
        "    print(\"endpoint_name:\", endpoint.name)\n",
        "\n",
        "    return model, endpoint\n",
        "\n",
        "\n",
        "models[\"sglang_gpu\"], endpoints[\"sglang_gpu\"] = deploy_model_sglang_multihost(\n",
        "    model_name=common_util.get_job_name_with_datetime(prefix=\"deepseek-serve\"),\n",
        "    model_id=model_id,\n",
        "    publisher=\"deepseek-ai\",\n",
        "    publisher_model_id=(\"deepseek-v3\" if \"V3\" in model_id else \"deepseek-r1\"),\n",
        "    base_model_id=hf_model_id,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    multihost_gpu_node_count=multihost_gpu_node_count,\n",
        "    enable_trust_remote_code=True,\n",
        "    enable_torch_compile=enable_torch_compile,\n",
        "    torch_compile_max_bs=torch_compile_max_bs,\n",
        "    attention_backend=\"fa3\",\n",
        "    disable_cuda_graph=disable_cuda_graph,\n",
        "    speculative_algorithm=speculative_algorithm,\n",
        "    speculative_draft_model_path=speculative_draft_model_path,\n",
        "    enable_jit_deepgemm=enable_jit_deepgemm,\n",
        "    enable_dp_attention=enable_dp_attention,\n",
        "    dp_size=dp_size,\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        "    is_spot=is_spot,\n",
        ")\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AGVPzwHkn7rw"
      },
      "outputs": [],
      "source": [
        "# @title Raw predict\n",
        "\n",
        "\n",
        "# @markdown Once deployment succeeds, you can send requests to the endpoint with text prompts. Sampling parameters supported by SGLang can be found [here](https://docs.sglang.ai/backend/sampling_params.html).\n",
        "\n",
        "# @markdown Example:\n",
        "\n",
        "# @markdown ```\n",
        "# @markdown User: What is the best way to diagnose and fix a flickering light in my house?\n",
        "# @markdown Assistant: Okay, so I need to figure out how to diagnose and fix a flickering light in my house. Hmm, where do I start? Let's think. First, I remember that flickering lights can be caused by various issues. Maybe the bulb is loose? That's a common problem. Let me start with the simplest things first.\n",
        "# @markdown ```\n",
        "# @markdown Additionally, you can moderate the generated text with Vertex AI. See [Moderate text documentation](https://cloud.google.com/natural-language/docs/moderating-text) for more details.\n",
        "\n",
        "# Loads an existing endpoint instance using the endpoint name:\n",
        "# - Using `endpoint_name = endpoint.name` allows us to get the\n",
        "#   endpoint name of the endpoint `endpoint` created in the cell\n",
        "#   above.\n",
        "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
        "#   an existing endpoint with the ID 1234567890123456789.\n",
        "# You may uncomment the code below to load an existing endpoint.\n",
        "\n",
        "# endpoint_name = \"\"  # @param {type:\"string\"}\n",
        "# aip_endpoint_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
        "# )\n",
        "# endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
        "\n",
        "# @markdown A chat template formatted prompt for the DeepSeek-V3 model is shown below as an example.\n",
        "prompt = \"<｜begin▁of▁sentence｜><｜User｜>What is the best way to diagnose and fix a flickering light in my house?<｜Assistant｜>\"  # @param {type: \"string\"}\n",
        "# @markdown For the DeepSeek-R1 model, `<think>` should be appended to the prompt, as shown below.\n",
        "if model_id.lower().endswith(\"deepseek-r1\"):\n",
        "    prompt += \"<think>\\n\"\n",
        "\n",
        "# @markdown If you encounter an issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, by lowering `max_new_tokens`.\n",
        "max_new_tokens = 1024  # @param {type:\"integer\"}\n",
        "temperature = 0.6  # @param {type:\"number\"}\n",
        "top_p = 0.95  # @param {type:\"number\"}\n",
        "\n",
        "# Overrides parameters for inferences.\n",
        "instances = [{\"text\": prompt}]\n",
        "parameters = {\n",
        "    \"sampling_params\": {\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": top_p,\n",
        "    }\n",
        "}\n",
        "response = endpoints[\"sglang_gpu\"].predict(\n",
        "    instances=instances,\n",
        "    parameters=parameters,\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        ")\n",
        "\n",
        "for prediction in response.predictions:\n",
        "    print(prediction)\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZauMzfXJzAKZ"
      },
      "outputs": [],
      "source": [
        "# @title Chat completion\n",
        "\n",
        "if use_dedicated_endpoint:\n",
        "    DEDICATED_ENDPOINT_DNS = endpoints[\"sglang_gpu\"].gca_resource.dedicated_endpoint_dns\n",
        "ENDPOINT_RESOURCE_NAME = endpoints[\"sglang_gpu\"].resource_name\n",
        "\n",
        "# @markdown Because the DeepSeek-R1 / DeepSeek-V3 model generates detailed reasoning steps, the output is expected to be long. We recommend using streaming for a better generation experience.\n",
        "# @title Chat Completions Inference\n",
        "\n",
        "# @markdown Once deployment succeeds, you can send requests to the endpoint using the OpenAI SDK.\n",
        "\n",
        "# @markdown First you will need to install the SDK and some auth-related dependencies.\n",
        "\n",
        "! pip install -qU openai google-auth requests\n",
        "\n",
        "# @markdown Next fill out some request parameters:\n",
        "\n",
        "user_message = \"How is your day going?\"  # @param {type: \"string\"}\n",
        "# @markdown If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, such as set `max_tokens` as 20.\n",
        "max_tokens = 50  # @param {type: \"integer\"}\n",
        "temperature = 1.0  # @param {type: \"number\"}\n",
        "stream = False  # @param {type: \"boolean\"}\n",
        "\n",
        "# @markdown Now we can send a request.\n",
        "\n",
        "import google.auth\n",
        "import openai\n",
        "\n",
        "creds, project = google.auth.default()\n",
        "auth_req = google.auth.transport.requests.Request()\n",
        "creds.refresh(auth_req)\n",
        "\n",
        "BASE_URL = (\n",
        "    f\"https://{REGION}-aiplatform.googleapis.com/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
        ")\n",
        "try:\n",
        "    if use_dedicated_endpoint:\n",
        "        BASE_URL = f\"https://{DEDICATED_ENDPOINT_DNS}/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "client = openai.OpenAI(base_url=BASE_URL, api_key=creds.token)\n",
        "\n",
        "model_response = client.chat.completions.create(\n",
        "    model=\"\",\n",
        "    messages=[{\"role\": \"user\", \"content\": user_message}],\n",
        "    temperature=temperature,\n",
        "    max_tokens=max_tokens,\n",
        "    stream=stream,\n",
        ")\n",
        "\n",
        "if stream:\n",
        "    usage = None\n",
        "    contents = []\n",
        "    for chunk in model_response:\n",
        "        if chunk.usage is not None:\n",
        "            usage = chunk.usage\n",
        "            continue\n",
        "        print(chunk.choices[0].delta.content, end=\"\")\n",
        "        contents.append(chunk.choices[0].delta.content)\n",
        "    print(f\"\\n\\n{usage}\")\n",
        "else:\n",
        "    print(model_response)\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyg_QzpQD0DJ"
      },
      "source": [
        "## Deploy DeepSeek-V3 and DeepSeek-R1 with TensorRT-LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wfoBFRv_D4mB"
      },
      "outputs": [],
      "source": [
        "# @title Deploy\n",
        "\n",
        "# @markdown This section uploads DeepSeek models to Model Registry and deploys them to a Vertex Prediction Endpoint. It takes ~1 hour to finish.\n",
        "\n",
        "# @markdown It's recommended to use the region selected by the deployment button on the model card. If the deployment button is not available, it's recommended to stay with the default region of the notebook.\n",
        "\n",
        "# @markdown Set the model to deploy.\n",
        "\n",
        "base_model_name = \"DeepSeek-R1\"  # @param [\"DeepSeek-V3\", \"DeepSeek-V3-Base\", \"DeepSeek-V3-0324\", \"DeepSeek-R1\", \"DeepSeek-R1-0528\"] {isTemplate:true}\n",
        "model_id = \"deepseek-ai/\" + base_model_name\n",
        "hf_model_id = model_id\n",
        "\n",
        "# The pre-built serving docker images.\n",
        "TRTLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/tensorrt-llm.cu128.0-18.ubuntu2404.py312:deepseek\"\n",
        "\n",
        "# @markdown Choose whether to use a [Spot VM](https://cloud.google.com/compute/docs/instances/spot) for the deployment.\n",
        "is_spot = False  # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown Set use_dedicated_endpoint to False if you don't want to use [dedicated endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment#create-dedicated-endpoint). Note that [dedicated endpoint does not support VPC Service Controls](https://cloud.google.com/vertex-ai/docs/predictions/choose-endpoint-type), uncheck the box if you are using VPC-SC.\n",
        "use_dedicated_endpoint = True  # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown Find Vertex AI prediction supported accelerators and regions at https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
        "trtllm_accelerator_type = \"NVIDIA_H200_141GB\"  # @param [\"NVIDIA_H200_141GB\"] {isTemplate:true}\n",
        "accelerator_count = 8\n",
        "trtllm_region = \"us-east4\"  # @param [\"us-east4\"] {isTemplate:true}\n",
        "if trtllm_accelerator_type == \"NVIDIA_H200_141GB\":\n",
        "    machine_type = \"a3-ultragpu-8g\"\n",
        "    multihost_gpu_node_count = 1\n",
        "    resource_id = \"custom_model_serving_nvidia_h200_gpus\"\n",
        "else:\n",
        "    raise ValueError(\"Only NVIDIA_H200_141GB is supported for DeepSeek-R1.\")\n",
        "\n",
        "check_quota(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=trtllm_region,\n",
        "    resource_id=resource_id,\n",
        "    accelerator_count=int(accelerator_count * multihost_gpu_node_count),\n",
        ")\n",
        "\n",
        "# 18K context length. This is the maximum supported by the current version of TensorRT-LLM on DeepSeek V3/R1 models.\n",
        "MAX_INPUT_LEN = 18000\n",
        "MAX_MODEL_LEN = 18000\n",
        "MAX_NUM_SEQS = 128\n",
        "GPU_MEMORY_UTILIZATION = 0.55\n",
        "\n",
        "\n",
        "def poll_operation(op_name: str, trtllm_region: str) -> bool:  # noqa: F811\n",
        "    creds, _ = auth.default()\n",
        "    auth_req = auth.transport.requests.Request()\n",
        "    creds.refresh(auth_req)\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {creds.token}\",\n",
        "    }\n",
        "    get_resp = requests.get(\n",
        "        f\"https://{trtllm_region}-aiplatform.googleapis.com/ui/{op_name}\",\n",
        "        headers=headers,\n",
        "    )\n",
        "    opjs = get_resp.json()\n",
        "    if \"error\" in opjs:\n",
        "        raise ValueError(f\"Operation failed: {opjs['error']}\")\n",
        "    return opjs.get(\"done\", False)\n",
        "\n",
        "\n",
        "def poll_and_wait_trtllm(\n",
        "    op_name: str, total_wait: int, trtllm_region: str, interval: int = 60\n",
        "):  # noqa: F811\n",
        "    waited = 0\n",
        "    while not poll_operation(op_name, trtllm_region):\n",
        "        if waited > total_wait:\n",
        "            raise TimeoutError(\"Operation timed out\")\n",
        "        print(\n",
        "            f\"\\rStill waiting for operation... Waited time in second: {waited:<6}\",\n",
        "            end=\"\",\n",
        "            flush=True,\n",
        "        )\n",
        "        waited += interval\n",
        "        time.sleep(interval)\n",
        "\n",
        "\n",
        "def deploy_model_tensorrt_llm_multihost(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    publisher: str,\n",
        "    publisher_model_id: str,\n",
        "    service_account: str = \"\",\n",
        "    base_model_id: str = \"\",\n",
        "    machine_type: str = \"g2-standard-8\",\n",
        "    accelerator_type: str = \"NVIDIA_L4\",\n",
        "    accelerator_count: int = 1,\n",
        "    multihost_gpu_node_count: int = 1,\n",
        "    gpu_memory_utilization: float | None = None,\n",
        "    max_input_len: int | None = None,\n",
        "    max_model_len: int | None = None,\n",
        "    max_num_seqs: int | None = None,\n",
        "    enable_trust_remote_code: bool = False,\n",
        "    enable_chunked_prefill: bool = False,\n",
        "    use_dedicated_endpoint: bool = False,\n",
        "    is_spot: bool = True,\n",
        "    trtllm_region: str = REGION,\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys trained models with TensorRT-LLM on Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(\n",
        "        display_name=f\"{model_name}-endpoint\",\n",
        "        location=trtllm_region,\n",
        "        dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
        "    )\n",
        "\n",
        "    if not base_model_id:\n",
        "        base_model_id = model_id\n",
        "\n",
        "    container_args = [\n",
        "        \"python\",\n",
        "        \"api_server.py\",\n",
        "        \"--host=0.0.0.0\",\n",
        "        \"--port=7080\",\n",
        "        f\"--model={model_id}\",\n",
        "        f\"--tensor-parallel-size={accelerator_count * multihost_gpu_node_count}\",\n",
        "    ]\n",
        "\n",
        "    if gpu_memory_utilization:\n",
        "        container_args.append(f\"--gpu-memory-utilization={gpu_memory_utilization}\")\n",
        "\n",
        "    if max_input_len:\n",
        "        container_args.append(f\"--max-input-len={max_input_len}\")\n",
        "\n",
        "    if max_model_len:\n",
        "        container_args.append(f\"--max-model-len={max_model_len}\")\n",
        "\n",
        "    if max_num_seqs:\n",
        "        container_args.append(f\"--max-num-seqs={max_num_seqs}\")\n",
        "\n",
        "    if enable_trust_remote_code:\n",
        "        container_args.append(\"--trust-remote-code=True\")\n",
        "\n",
        "    if enable_chunked_prefill:\n",
        "        container_args.append(\"--enable-chunked-prefill=True\")\n",
        "\n",
        "    env_vars = {\n",
        "        \"MODEL_ID\": base_model_id,\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "    }\n",
        "\n",
        "    # HF_TOKEN is not a compulsory field and may not be defined.\n",
        "    try:\n",
        "        if HF_TOKEN:\n",
        "            env_vars[\"HF_TOKEN\"] = HF_TOKEN\n",
        "    except NameError:\n",
        "        pass\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        location=trtllm_region,\n",
        "        serving_container_image_uri=TRTLLM_DOCKER_URI,\n",
        "        serving_container_args=container_args,\n",
        "        serving_container_ports=[7080],\n",
        "        serving_container_predict_route=\"/v1/chat/completions\",\n",
        "        serving_container_health_route=\"/health\",\n",
        "        serving_container_environment_variables=env_vars,\n",
        "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
        "        serving_container_deployment_timeout=7200,\n",
        "        model_garden_source_model_name=(\n",
        "            f\"publishers/{publisher}/models/{publisher_model_id}\"\n",
        "        ),\n",
        "    )\n",
        "    print(\n",
        "        f\"Deploying {model_name} on {machine_type} with {int(accelerator_count * multihost_gpu_node_count)} {accelerator_type} GPU(s).\"\n",
        "    )\n",
        "\n",
        "    creds, _ = auth.default()\n",
        "    auth_req = auth.transport.requests.Request()\n",
        "    creds.refresh(auth_req)\n",
        "\n",
        "    url = f\"https://{trtllm_region}-aiplatform.googleapis.com/ui/projects/{PROJECT_ID}/locations/{trtllm_region}/endpoints/{endpoint.name}:deployModel\"\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {creds.token}\",\n",
        "    }\n",
        "    data = {\n",
        "        \"deployedModel\": {\n",
        "            \"model\": model.resource_name,\n",
        "            \"displayName\": model_name,\n",
        "            \"dedicatedResources\": {\n",
        "                \"machineSpec\": {\n",
        "                    \"machineType\": machine_type,\n",
        "                    \"multihostGpuNodeCount\": multihost_gpu_node_count,\n",
        "                    \"acceleratorType\": accelerator_type,\n",
        "                    \"acceleratorCount\": accelerator_count,\n",
        "                },\n",
        "                \"minReplicaCount\": 1,\n",
        "                \"maxReplicaCount\": 1,\n",
        "            },\n",
        "            \"system_labels\": {\n",
        "                \"NOTEBOOK_NAME\": \"model_garden_pytorch_deepseek_deployment.ipynb\",\n",
        "                \"NOTEBOOK_ENVIRONMENT\": common_util.get_deploy_source(),\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "    if service_account:\n",
        "        data[\"deployedModel\"][\"serviceAccount\"] = service_account\n",
        "    if is_spot:\n",
        "        data[\"deployedModel\"][\"dedicatedResources\"][\"spot\"] = True\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "    print(f\"Deploy Model response: {response.json()}\")\n",
        "    if response.status_code != 200 or \"name\" not in response.json():\n",
        "        raise ValueError(f\"Failed to deploy model: {response.text}\")\n",
        "    poll_and_wait_trtllm(response.json()[\"name\"], 7200, trtllm_region)\n",
        "    print(\"endpoint_name:\", endpoint.name)\n",
        "\n",
        "    return model, endpoint\n",
        "\n",
        "\n",
        "models[\"trtllm_gpu\"], endpoints[\"trtllm_gpu\"] = deploy_model_tensorrt_llm_multihost(\n",
        "    model_name=common_util.get_job_name_with_datetime(prefix=\"deepseek-serve\"),\n",
        "    model_id=model_id,\n",
        "    publisher=\"deepseek-ai\",\n",
        "    publisher_model_id=(\"deepseek-v3\" if \"V3\" in model_id else \"deepseek-r1\"),\n",
        "    base_model_id=hf_model_id,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=trtllm_accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    multihost_gpu_node_count=multihost_gpu_node_count,\n",
        "    gpu_memory_utilization=GPU_MEMORY_UTILIZATION,\n",
        "    max_input_len=MAX_INPUT_LEN,\n",
        "    max_model_len=MAX_MODEL_LEN,\n",
        "    max_num_seqs=MAX_NUM_SEQS,\n",
        "    enable_trust_remote_code=True,\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        "    is_spot=is_spot,\n",
        "    trtllm_region=trtllm_region,\n",
        ")\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sfnEWPRGFDGD"
      },
      "outputs": [],
      "source": [
        "# @title Raw predict\n",
        "# @markdown Once deployment succeeds, you can send requests to the endpoint using `:rawPredict`. The OpenAI Client chat completions support is coming soon.\n",
        "endpoints[\"trtllm_gpu\"] = aiplatform.Endpoint(endpoints[\"trtllm_gpu\"].resource_name)\n",
        "\n",
        "# @markdown Fill out some request parameters:\n",
        "user_message = \"How is your day going?\"  # @param {type: \"string\"}\n",
        "# @markdown If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, such as set `max_tokens` as 20.\n",
        "max_tokens = 50  # @param {type: \"integer\"}\n",
        "temperature = 1.0  # @param {type: \"number\"}\n",
        "\n",
        "# @markdown Now we can send a request.\n",
        "\n",
        "response = endpoints[\"trtllm_gpu\"].raw_predict(\n",
        "    body=json.dumps(\n",
        "        {\n",
        "            \"model\": \"\",\n",
        "            \"messages\": [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": user_message,\n",
        "                }\n",
        "            ],\n",
        "            \"max_tokens\": max_tokens,\n",
        "            \"temperature\": temperature,\n",
        "        }\n",
        "    ),\n",
        "    headers={\"Content-Type\": \"application/json\"},\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        ")\n",
        "print(response.json()[\"choices\"][0][\"message\"][\"content\"])\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JETd33jIDcjm"
      },
      "source": [
        "## Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "911406c1561e"
      },
      "outputs": [],
      "source": [
        "# @title Delete the models and endpoints\n",
        "\n",
        "# @markdown  Delete the experiment models and endpoints to recycle the resources\n",
        "# @markdown  and avoid unnecessary continuous charges that may incur.\n",
        "\n",
        "# Undeploy model and delete endpoint.\n",
        "for endpoint in endpoints.values():\n",
        "    endpoint.delete(force=True)\n",
        "\n",
        "# Delete models.\n",
        "for model in models.values():\n",
        "    model.delete()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_pytorch_deepseek_deployment.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

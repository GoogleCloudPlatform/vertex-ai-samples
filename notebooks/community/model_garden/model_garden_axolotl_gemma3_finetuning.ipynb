{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1jg2qBjVb4yf"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAeljAi7b4yg"
      },
      "source": [
        "# Vertex AI Model Garden - Fine-tune Gemma3 model with Axolotl\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_axolotl_gemma3_finetuning.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_axolotl_gemma3_finetuning.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5NXBxyjf1xs"
      },
      "source": [
        "## Overview\n",
        "This notebook demonstrates fine-tuning Gemma3 model using [Axolotl](https://github.com/axolotl-ai-cloud/axolotl). Axolotl streamlines AI model fine-tuning by providing a wide range of training recipes and supporting multiple configurations and architectures.\n",
        "\n",
        "The notebook shows two modes of running the fine-tuning:\n",
        "- Local fine-tuning using the Enterprise Colab runtime.\n",
        "- Fine-tuning on cloud using the Vertex AI training.\n",
        "\n",
        "Both of these modes can be run independent of each other and the local fine-tuning is optional.\n",
        "\n",
        "Local fine-tuning with the Enterprise Colab runtime has the following advantages:\n",
        "- **Debugging**: Use Enterprise Colab runtime to debug axolotl fine-tuning. This can be more efficient because debugging on the Vertex AI training involves waiting for resources to be provisioned, which can add delays. Also it is easier to debug on Enterprise Colab runtime compared to Vertex AI training.\n",
        "- **Sanity check for flags**: Use Enterprise Colab runtime to do sanity check for Axolotl flags before running it on Vertex AI training directly.\n",
        "- **Quick experimentations**: Use Enterprise Colab runtime to do quick experimentations with Axolotl flags.\n",
        "\n",
        "Once the local fine-tuning is verified, the Vertex AI training is the recommended way to run the fine-tuning. Vertex AI training has several advantages, including:\n",
        "- **Running multiple training jobs in parallel**: This can be useful for hyperparameter tuning or running experiments with different datasets etc.\n",
        "- **Availability of Higher-end GPUs**: Vertex AI training provides access to higher-end GPUs like the H100, which can be crucial if you encounter out-of-memory (OOM) errors.\n",
        "- **[DWS support](https://cloud.google.com/vertex-ai/docs/training/schedule-jobs-dws)**: DWS makes Vertex AI training more cost-effective, and easier to manage, especially in scenarios where GPU availability is a concern.\n",
        "Refer to [this documentation](https://cloud.google.com/vertex-ai/docs/training/overview#vertexi-ai-operationalizes-training-at-scale) for more details on Vertex AI training advantages.\n",
        "\n",
        "### Objective\n",
        "- Train Gemma3 model using Axolotl in local Enterprise Colab runtime.\n",
        "- Run local prediction with Enterprise Colab runtime for the trained model.\n",
        "- Train Gemma3 model using Axolotl with Vertex AI Training.\n",
        "- Deploy the trained model on Vertex AI and run predictions on cloud.\n",
        "\n",
        "### Resources required\n",
        "The table below outlines the recommended machine specifications for different parts of the notebook to function correctly. Note that machine types with higher VRAM than recommended can also be used.\n",
        "> | Model | Local Finetuning | Vertex AI Finetuning | Vertex AI Deployment |\n",
        "| ----------- | ----------- | ----------- | ----------- |\n",
        "| google/gemma-3-1b-it | a2-highgpu-4g | a2-ultragpu-8g or a3-highgpu-8g | g2-standard-12 |\n",
        "| google/gemma-3-1b-pt | a2-highgpu-4g | a2-ultragpu-8g or a3-highgpu-8g | a3-highgpu-2g |\n",
        "| google/gemma-3-4b-it | a2-highgpu-4g | a2-ultragpu-8g or a3-highgpu-8g | a3-highgpu-2g |\n",
        "| google/gemma-3-4b-pt | a2-highgpu-4g | a2-ultragpu-8g or a3-highgpu-8g | a3-highgpu-2g |\n",
        "| google/gemma-3-12b-it | a2-highgpu-4g | a2-ultragpu-8g or a3-highgpu-8g | a3-highgpu-2g |\n",
        "| google/gemma-3-12b-pt | a2-highgpu-4g | a2-ultragpu-8g or a3-highgpu-8g | a3-highgpu-2g |\n",
        "| google/gemma-3-27b-it | a2-highgpu-4g | a2-ultragpu-8g or a3-highgpu-8g | a3-highgpu-4g |\n",
        "| google/gemma-3-27b-pt | a2-highgpu-4g | a2-ultragpu-8g or a3-highgpu-8g | a3-highgpu-4g |\n",
        "\n",
        "Learn more about machine types by following [this doc](https://cloud.google.com/vertex-ai/docs/training/configure-compute#specifying_gpus).\n",
        "\n",
        "### File a bug\n",
        "\n",
        "File a bug on [GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/issues/new) if you encounter any issue with the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq8JgAE4BQTj"
      },
      "source": [
        "## [Optional for Vertex AI fine-tuning] Setup Colab Runtime\n",
        "**To run local fine-tuning you must connect to GPU runtime. This is optional for fine-tuning using Vertex AI. If you are already connected to a GPU runtime then you can skip this step.**\n",
        "\n",
        "**The following sections perform the setup for NVIDIA_TESLA_A100 GPU. All Gemma3 Axolotl configs work with NVIDIA_TESLA_A100 machine with 4 GPUs.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ggq03Jf5nNFp"
      },
      "outputs": [],
      "source": [
        "# @title Create runtime\n",
        "# @markdown This cell creates a local GPU runtime.\n",
        "# @markdown **If you have already created a runtime previously, then you can skip this cell.** Optionally, read [this](https://cloud.google.com/colab/docs/create-runtime) to learn how to manually create a runtime.\n",
        "# @markdown This cell can take up to 5 minutes to run.\n",
        "\n",
        "import os\n",
        "import uuid\n",
        "import re\n",
        "import subprocess\n",
        "\n",
        "RUNTIME_PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "RUNTIME_REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "RUNTIME_ACCELERATOR_TYPE = \"NVIDIA_TESLA_A100\"  # @param {type:\"string\"}\n",
        "RUNTIME_ACCELERATOR_COUNT = \"4\"  # @param [1, 2, 4, 8, 16]\n",
        "RUNTIME_ACCELERATOR_COUNT = int(RUNTIME_ACCELERATOR_COUNT)\n",
        "\n",
        "if not RUNTIME_ACCELERATOR_TYPE:\n",
        "  print(\"Warning: No accelerator type specified. Skipping runtime creation.\")\n",
        "  try:\n",
        "    subprocess.check_output(\"nvidia-smi\")\n",
        "    print(\"Nvidia GPU detected!\")\n",
        "  except Exception:\n",
        "    print(\"Warning: Nvidia GPU not detected. Use GPU runtime for local fine-tuning.\")\n",
        "else:\n",
        "  if RUNTIME_ACCELERATOR_TYPE == \"NVIDIA_TESLA_A100\" and RUNTIME_ACCELERATOR_COUNT != 16:\n",
        "    RUNTIME_MACHINE_TYPE = f\"a2-highgpu-{RUNTIME_ACCELERATOR_COUNT}g\"\n",
        "  elif RUNTIME_ACCELERATOR_TYPE == \"NVIDIA_TESLA_A100\" and RUNTIME_ACCELERATOR_COUNT == 16:\n",
        "    RUNTIME_MACHINE_TYPE = \"a2-megagpu-16g\"\n",
        "  else:\n",
        "    raise ValueError(f\"Invalid GPU type {RUNTIME_ACCELERATOR_TYPE}, and count {RUNTIME_ACCELERATOR_COUNT} combination.\")\n",
        "\n",
        "  print(f\"Machine type: {RUNTIME_MACHINE_TYPE}\")\n",
        "\n",
        "  uuid = uuid.uuid4()\n",
        "  RUNTIME_DISPLAY_NAME = f\"axolotl-{RUNTIME_ACCELERATOR_TYPE}-{RUNTIME_ACCELERATOR_COUNT}-{uuid}\"\n",
        "  print(f\"Creating runtime with display name: {RUNTIME_DISPLAY_NAME}\")\n",
        "\n",
        "  # create runtime template\n",
        "  shell_output = ! gcloud colab runtime-templates create --display-name=$RUNTIME_DISPLAY_NAME \\\n",
        "    --project=$RUNTIME_PROJECT_ID --region=$RUNTIME_REGION \\\n",
        "    --machine-type=$RUNTIME_MACHINE_TYPE --accelerator-type=$RUNTIME_ACCELERATOR_TYPE \\\n",
        "    --accelerator-count=$RUNTIME_ACCELERATOR_COUNT --disk-type=PD_BALANCED\n",
        "  shell_output = \"\\n\".join(shell_output)\n",
        "  print(shell_output)\n",
        "  RUNTIME_TEMPLATE_ID = re.search(r\"projects/.*/locations/.*/notebookRuntimeTemplates/(\\d+)\", shell_output).group(1)\n",
        "\n",
        "  # create runtime\n",
        "  shell_output = ! gcloud colab runtimes create --display-name=$RUNTIME_DISPLAY_NAME \\\n",
        "    --runtime-template=$RUNTIME_TEMPLATE_ID --project=$RUNTIME_PROJECT_ID \\\n",
        "    --region=$RUNTIME_REGION\n",
        "  shell_output = \"\\n\".join(shell_output)\n",
        "  print(shell_output)\n",
        "  RUNTIME_ID = re.search(r\"projects/.*/locations/.*/notebookRuntimes/(\\d+)\", shell_output).group(1)\n",
        "\n",
        "  # start runtime\n",
        "  ! gcloud colab runtimes start $RUNTIME_ID --project=$RUNTIME_PROJECT_ID --region=$RUNTIME_REGION\n",
        "\n",
        "  print(f\"Runtime: {RUNTIME_ID} created successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGDpB0NnnNFp"
      },
      "source": [
        "### Connect to runtime manually\n",
        "Although the previous step created a runtime, you still have to **connect manually to this runtime by following [the instructions here](https://cloud.google.com/colab/docs/connect-to-runtime).** This also applies if you want to use a previously created GPU runtime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kik-beBAnNFq"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XeJdEWe9nNFq"
      },
      "outputs": [],
      "source": [
        "# @title [Optional for Vertex AI fine-tuning] Setup Pytorch for local fine-tuning\n",
        "# @markdown **Note: This section must be run before performing local fine-tuning.** This section installs correct pytorch dependency needed for the Axolotl local run.\n",
        "import os\n",
        "\n",
        "os.environ[\"TORCH_CUDA_ARCH_LIST\"] = \"7.0 7.5 8.0 8.6 9.0+PTX\"\n",
        "! pip install torch==2.4.1 torchvision\n",
        "os.environ[\"PYTORCH_INSTALLATION\"] = \"done\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_cAAzli5nNFq"
      },
      "outputs": [],
      "source": [
        "# @title Import utility packages for fine-tuning\n",
        "\n",
        "# Upgrade Vertex AI SDK.\n",
        "! pip3 install --upgrade --quiet 'google-cloud-aiplatform==1.97.0'\n",
        "\n",
        "# Import the necessary packages.\n",
        "! rm -rf vertex-ai-samples && git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
        "! cd vertex-ai-samples\n",
        "\n",
        "# Import the necessary packages.\n",
        "\n",
        "import datetime\n",
        "import importlib\n",
        "import os\n",
        "import pathlib\n",
        "import time\n",
        "import uuid\n",
        "from typing import Tuple\n",
        "\n",
        "import requests\n",
        "import yaml\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "common_util = importlib.import_module(\n",
        "    \"vertex-ai-samples.community-content.vertex_model_garden.model_oss.notebook_util.common_util\"\n",
        ")\n",
        "\n",
        "\n",
        "def run_cmd_and_check_output(\n",
        "    cmd: list[str], env: dict[str, str] = None, input: str = \"\", cwd: str = None\n",
        "):\n",
        "    \"\"\"Runs the given command and raises exception if the command fails.\"\"\"\n",
        "    with subprocess.Popen(\n",
        "        cmd,\n",
        "        stdin=subprocess.PIPE,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True,\n",
        "        bufsize=1,\n",
        "        env=env,\n",
        "        cwd=cwd,\n",
        "    ) as p:\n",
        "        if input:\n",
        "            p.stdin.write(input)\n",
        "            p.stdin.flush()\n",
        "        p.stdin.close()\n",
        "        for line in p.stdout:\n",
        "            print(line, end=\"\", flush=True)\n",
        "    if p.returncode:\n",
        "        raise ValueError(\n",
        "            f\"Command '{' '.join(cmd)}' execution failed with return code {p.returncode}\"\n",
        "        )\n",
        "\n",
        "\n",
        "train_job = None\n",
        "models, endpoints = {}, {}\n",
        "HF_TOKEN = \"\"\n",
        "WORKING_DIR = os.getcwd()\n",
        "print(f\"Current working directory for notebook: {WORKING_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "k32BrMnWnNFq"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown 2. For finetuning using Vertex AI, we will use Dynamic Workload Scheduler. Learn more about Dynamic workload scheduler [here](https://cloud.google.com/vertex-ai/docs/training/schedule-jobs-dws). For Dynamic Workload Scheduler, check the [us-central1](https://console.cloud.google.com/iam-admin/quotas?location=us-central1&metric=aiplatform.googleapis.com%2Fcustom_model_training_preemptible_nvidia_h100_gpus) or [europe-west4](https://console.cloud.google.com/iam-admin/quotas?location=europe-west4&metric=aiplatform.googleapis.com%2Fcustom_model_training_preemptible_nvidia_h100_gpus) quota for Nvidia H100 GPUs, [us-central1](https://console.cloud.google.com/iam-admin/quotas?location=us-central1&metric=aiplatform.googleapis.com%2Fcustom_model_training_preemptible_nvidia_a100_80gb_gpus) quota for Nvidia A100 80GB GPUs. If you do not have enough GPUs, then you can follow [these instructions](https://cloud.google.com/docs/quotas/view-manage#viewing_your_quota_console) to request quota.\n",
        "\n",
        "# @markdown 3. For serving, **[click here](https://console.cloud.google.com/iam-admin/quotas?location=us-central1&metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_l4_gpus)** to check if your project already has the required L4 GPUs in the us-central1 region. If you need more L4 GPUs for your project, then you can follow [these instructions](https://cloud.google.com/docs/quotas/view-manage#viewing_your_quota_console) to request more. Alternatively, if you want to run predictions with A100 80GB or H100 GPUs, we recommend using the regions listed below. **NOTE:** Make sure you have associated quota in selected regions. Click the links to see your current quota for each GPU type: [Nvidia A100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_a100_80gb_gpus), [Nvidia H100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_h100_gpus).\n",
        "\n",
        "# @markdown > | Machine Type | Accelerator Type | Recommended Regions |\n",
        "# @markdown | ----------- | ----------- | ----------- |\n",
        "# @markdown | a2-ultragpu-1g | 1 NVIDIA_A100_80GB | us-central1, us-east4, europe-west4, asia-southeast1, us-east4 |\n",
        "# @markdown | a3-highgpu-2g | 2 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "# @markdown | a3-highgpu-4g | 4 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "# @markdown | a3-highgpu-8g | 8 NVIDIA_H100_80GB | us-central1, us-east5, europe-west4, us-west1, asia-southeast1 |\n",
        "\n",
        "# @markdown 4. **[Optional]** [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs. Set the BUCKET_URI for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`) should be located in the same region as where the notebook was launched. Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\"). If not set, a unique GCS bucket will be created instead.\n",
        "\n",
        "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown 5. **[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment.\n",
        "\n",
        "REGION = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "if not REGION:\n",
        "    if not os.environ.get(\"GOOGLE_CLOUD_REGION\"):\n",
        "        raise ValueError(\n",
        "            \"REGION must be set. See\"\n",
        "            \" https://cloud.google.com/vertex-ai/docs/general/locations for\"\n",
        "            \" available cloud locations.\"\n",
        "        )\n",
        "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
        "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
        "\n",
        "# Cloud Storage bucket for storing the experiment artifacts.\n",
        "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
        "# prefer using your own GCS bucket, change the value yourself below.\n",
        "now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "\n",
        "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
        "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}-{str(uuid.uuid4())[:4]}\"\n",
        "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
        "else:\n",
        "    assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
        "    shell_output = ! gsutil ls -Lb {BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
        "    bucket_region = shell_output[0].strip().lower()\n",
        "    if bucket_region != REGION:\n",
        "        raise ValueError(\n",
        "            \"Bucket region %s is different from notebook region %s\"\n",
        "            % (bucket_region, REGION)\n",
        "        )\n",
        "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
        "\n",
        "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
        "MODEL_BUCKET = os.path.join(BUCKET_URI, \"axolotl\")\n",
        "\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "# Gets the default SERVICE_ACCOUNT.\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
        "\n",
        "\n",
        "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
        "\n",
        "! gcloud config set project $PROJECT_ID\n",
        "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/storage.admin\"\n",
        "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/aiplatform.user\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7AfA9UsnNFq"
      },
      "source": [
        "## Finetune with Axolotl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5GzmhZdLIjTI"
      },
      "outputs": [],
      "source": [
        "# @title Set model to fine-tune\n",
        "# @markdown Note: This overrides Axolotl's `base_model` flag.\n",
        "HF_MODEL_ID = \"google/gemma-3-1b-it\"  # @param [\"google/gemma-3-1b-it\", \"google/gemma-3-4b-it\", \"google/gemma-3-12b-it\", \"google/gemma-3-27b-it\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_iTQ4nSlODZJ"
      },
      "outputs": [],
      "source": [
        "# @title Set Axolotl config\n",
        "\n",
        "# @markdown Axolotl is designed to work with YAML config files that contain everything you need to preprocess a dataset, train or fine-tune a model, run model inference or evaluation, and much more.\n",
        "# @markdown The Gemma3 Axolotl configs are taken from [examples directory](https://github.com/axolotl-ai-cloud/axolotl/tree/6ba5c0ed2c42a0e069b28c83646ee5a2a6904430/examples/gemma3).\n",
        "\n",
        "# @markdown Suggestion for Gemma3 Axolotl configs:\n",
        "# @markdown > | Model | Recommended Axolotl Config |\n",
        "# @markdown | ----------- | ----------- |\n",
        "# @markdown | google/gemma-3-1b-it | examples/gemma3/gemma-3-1b-qlora.yml |\n",
        "# @markdown | google/gemma-3-4b-it | examples/gemma3/gemma-3-4b-qlora.yml |\n",
        "# @markdown | google/gemma-3-12b-it | examples/gemma3/gemma-3-4b-qlora.yml |\n",
        "# @markdown | google/gemma-3-27b-it | examples/gemma3/gemma-3-4b-qlora.yml |\n",
        "# @markdown | google/gemma-3-4b-it | examples/gemma3/gemma-3-4b-vision-qlora.yml |\n",
        "# @markdown | google/gemma-3-12b-it | examples/gemma3/gemma-3-4b-vision-qlora.yml |\n",
        "# @markdown | google/gemma-3-27b-it | examples/gemma3/gemma-3-4b-vision-qlora.yml |\n",
        "\n",
        "# @markdown You can also customize the Axolotl config as per your requirements. To use a custom Axolotl config you can use `LOCAL` or `GCS` source option below.\n",
        "# @markdown Alternatively, you can specify github axolotl config and override flags using `Setup Axolotl Flags` section below.\n",
        "\n",
        "# @markdown 1. Set Axolotl config source.<br>\n",
        "# @markdown For **GITHUB** as source, you can explore different Axolotl configurations in the [examples directory](https://github.com/axolotl-ai-cloud/axolotl/tree/6ba5c0ed2c42a0e069b28c83646ee5a2a6904430/examples). For `GITHUB` source, `AXOLOTL_CONFIG_PATH` should start with `examples/`. e.g. \"examples/gemma3/gemma-3-1b-qlora.yml\".<br>\n",
        "# @markdown For **LOCAL** as source, create Axolotl config yaml file and specify correct path below. Note that, the local file will be copied to GCS bucket before running Vertex AI training job. For `LOCAL` source, `AXOLOTL_CONFIG_PATH` should be a absolute path of the config file, e.g. /content/lora.yml.<br>\n",
        "# @markdown For **GCS** as source, specify the GCS URI to the Axolotl config file. Make sure the file is accessible to service account used in the notebook. For `GCS` source, `AXOLOTL_CONFIG_PATH` should be a complete GCS URI of the config file, e.g. gs://bucket/path/to/config/file.yml.\n",
        "\n",
        "AXOLOTL_SOURCE = \"GITHUB\"  # @param [\"GITHUB\", \"LOCAL\", \"GCS\"]\n",
        "\n",
        "# @markdown 2. Set the Axolotl config file path.\n",
        "AXOLOTL_CONFIG_PATH = \"examples/gemma3/gemma-3-1b-qlora.yml\"  # @param [\"examples/gemma3/gemma-3-1b-qlora.yml\", \"examples/gemma3/gemma-3-4b-qlora.yml\", \"examples/gemma3/gemma-3-4b-vision-qlora.yml\"] {allow-input: true}\n",
        "\n",
        "assert AXOLOTL_CONFIG_PATH, \"AXOLOTL_CONFIG_PATH must be set.\"\n",
        "\n",
        "if AXOLOTL_SOURCE == \"GITHUB\":\n",
        "    assert AXOLOTL_CONFIG_PATH.startswith(\n",
        "        \"examples/\"\n",
        "    ), \"AXOLOTL_CONFIG_PATH must start with examples/ for GITHUB source.\"\n",
        "    github_url = f\"https://github.com/axolotl-ai-cloud/axolotl/raw/6ba5c0ed2c42a0e069b28c83646ee5a2a6904430/{AXOLOTL_CONFIG_PATH}\"\n",
        "    r = requests.get(github_url)\n",
        "    axolotl_config = r.content.decode(\"utf-8\")\n",
        "    axolotl_config = yaml.safe_load(axolotl_config)\n",
        "elif AXOLOTL_SOURCE == \"LOCAL\":\n",
        "    config_path = pathlib.Path(AXOLOTL_CONFIG_PATH)\n",
        "    assert config_path.exists(), \"AXOLOTL_CONFIG_PATH must exist for LOCAL source.\"\n",
        "    file_content = config_path.read_text()\n",
        "    axolotl_config = yaml.safe_load(file_content)\n",
        "elif AXOLOTL_SOURCE == \"GCS\":\n",
        "    local_path = pathlib.Path(f\"{WORKING_DIR}/tmp/axolotl_config.yml\")\n",
        "    common_util.download_gcs_file_to_local(AXOLOTL_CONFIG_PATH, local_path.absolute())\n",
        "    file_content = local_path.read_text()\n",
        "    axolotl_config = yaml.safe_load(file_content)\n",
        "    AXOLOTL_CONFIG_PATH = common_util.gcs_fuse_path(AXOLOTL_CONFIG_PATH)\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported AXOLOTL_SOURCE: {AXOLOTL_SOURCE}\")\n",
        "\n",
        "OUTPUT_GCS_URI = MODEL_BUCKET\n",
        "\n",
        "if not OUTPUT_GCS_URI.startswith(\"gs://\"):\n",
        "    OUTPUT_GCS_URI = f\"gs://{OUTPUT_GCS_URI}\"\n",
        "\n",
        "output_sub_dir = (\n",
        "    AXOLOTL_CONFIG_PATH.replace(\"/\", \"_\").replace(\".yaml\", \"\").replace(\".yml\", \"\")\n",
        ")\n",
        "BASE_AXOLOTL_OUTPUT_GCS_URI = f\"{OUTPUT_GCS_URI}/{output_sub_dir}/axolotl_output\"\n",
        "BASE_AXOLOTL_OUTPUT_DIR = common_util.gcs_fuse_path(BASE_AXOLOTL_OUTPUT_GCS_URI)\n",
        "\n",
        "# Placeholders for dataset settings.\n",
        "datasets = []\n",
        "test_datasets = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RTRV22DVJuMz"
      },
      "outputs": [],
      "source": [
        "# @title Setup HF token\n",
        "HF_TOKEN = \"\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4bp13RSoODZJ"
      },
      "outputs": [],
      "source": [
        "# @title **[Optional]** Setup dataset\n",
        "\n",
        "# @markdown This section configures the dataset used for fine-tuning.\n",
        "\n",
        "# @markdown **Note: If you don't fill any of the dataset options given below, then the dataset used will be the one defined in the Axolotl config file.** You have two options to configure the dataset:\n",
        "\n",
        "# @markdown **1. Use a Hugging Face Dataset**\n",
        "# @markdown   - Requires specifying the dataset name and type.\n",
        "\n",
        "# @markdown **2. Load from Google Cloud Storage (GCS)**\n",
        "# @markdown   - Requires specifying the bucket name, dataset type, file type, and paths to training/test splits.\n",
        "\n",
        "# @markdown **Choose ONE of the following options:**\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown **Option 1: Hugging Face**\n",
        "\n",
        "# @markdown **Hugging Face Dataset Name:**\n",
        "HF_DATASET = \"\"  # @param {type:\"string\", placeholder: \"e.g. timdettmers/openassistant-guanaco\"}\n",
        "# @markdown **Set the dataset type:** Refer to [Axolotl config file](https://github.com/axolotl-ai-cloud/axolotl/blob/6ba5c0ed2c42a0e069b28c83646ee5a2a6904430/docs/config.qmd#L102) for more details.\n",
        "HF_DATASET_TYPE = \"\"  # @param {type:\"string\", placeholder: \"e.g. completion\"}\n",
        "if HF_DATASET:\n",
        "    assert HF_DATASET_TYPE, \"HF_DATASET_TYPE must be set if HF_DATASET is set.\"\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown **Option 2: GCS**\n",
        "\n",
        "# @markdown **Bucket Name:**\n",
        "DATASET_BUCKET_NAME = \"\"  # @param {type:\"string\"}\n",
        "# @markdown **Dataset Type:** Refer to the [Axolotl config file](https://github.com/axolotl-ai-cloud/axolotl/blob/6ba5c0ed2c42a0e069b28c83646ee5a2a6904430/docs/config.qmd#L102) for more details.\n",
        "DATASET_TYPE = \"\"  # @param {type:\"string\"}\n",
        "# @markdown **File Type**. Refer to the [Axolotl config file](https://github.com/axolotl-ai-cloud/axolotl/blob/6ba5c0ed2c42a0e069b28c83646ee5a2a6904430/docs/config.qmd#L103).\n",
        "FILE_TYPE = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown **Path to Training Data (relative to bucket):**\n",
        "TRAIN_DATAFILES_PATH = \"\"  # @param {type:\"string\"}\n",
        "# @markdown **[Optional] Path to Test Data (relative to bucket):**\n",
        "# @markdown To use a dedicated validation set, provide the file path. Otherwise, the training data will be split to create a validation set.\n",
        "TEST_DATAFILES_PATH = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "if DATASET_BUCKET_NAME:\n",
        "    assert (\n",
        "        TRAIN_DATAFILES_PATH\n",
        "    ), \"TRAIN_DATAFILES_PATH must be set if DATASET_BUCKET_NAME is set.\"\n",
        "    assert DATASET_TYPE, \"DATASET_TYPE must be set if DATASET_BUCKET_NAME is set.\"\n",
        "    assert FILE_TYPE, \"FILE_TYPE must be set if DATASET_BUCKET_NAME is set.\"\n",
        "\n",
        "assert not (\n",
        "    HF_DATASET and DATASET_BUCKET_NAME\n",
        "), \"Only one of HF_DATASET or DATASET_BUCKET_NAME can be set.\"\n",
        "\n",
        "if DATASET_BUCKET_NAME:\n",
        "    paths = TRAIN_DATAFILES_PATH.split(\",\")\n",
        "    dataset = {\n",
        "        \"path\": f\"/gcs/{DATASET_BUCKET_NAME}/\",\n",
        "        \"type\": DATASET_TYPE,\n",
        "        \"data_files\": [],\n",
        "        \"ds_type\": FILE_TYPE,\n",
        "    }\n",
        "    for path in paths:\n",
        "        if path.startswith(\"/\"):\n",
        "            path = path[1:]\n",
        "        dataset[\"data_files\"].append(f\"/gcs/{DATASET_BUCKET_NAME}/{path}\")\n",
        "        dataset[\"split\"] = \"train\"\n",
        "    datasets.append(dataset)\n",
        "\n",
        "if TEST_DATAFILES_PATH:\n",
        "    paths = TEST_DATAFILES_PATH.split(\",\")\n",
        "    dataset = {\n",
        "        \"path\": f\"/gcs/{DATASET_BUCKET_NAME}/\",\n",
        "        \"type\": DATASET_TYPE,\n",
        "        \"data_files\": [],\n",
        "        \"ds_type\": FILE_TYPE,\n",
        "    }\n",
        "    for path in paths:\n",
        "        if path.startswith(\"/\"):\n",
        "            path = path[1:]\n",
        "        dataset[\"data_files\"].append(f\"/gcs/{DATASET_BUCKET_NAME}/{path}\")\n",
        "        dataset[\"split\"] = \"train\"\n",
        "    test_datasets.append(dataset)\n",
        "\n",
        "if HF_DATASET:\n",
        "    datasets.append({\"path\": HF_DATASET, \"type\": HF_DATASET_TYPE})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3SOjc9q8ODZJ"
      },
      "outputs": [],
      "source": [
        "# @title Setup Axolotl Flags\n",
        "# @markdown This section configures additional Axolotl flags. You can explore different Axolotl flags in the [Axolotl config file](https://github.com/axolotl-ai-cloud/axolotl/blob/6ba5c0ed2c42a0e069b28c83646ee5a2a6904430/docs/config.qmd).\n",
        "\n",
        "# @markdown **To avoid OOM, you can reduce sequence length.** This can be done by setting `sequence_len` flag to some smaller value. But reducing sequence length might also reduce the fine-tuned model's quality.\n",
        "# @markdown **Another alternative to avoid OOM is to use higher memory GPU.** It is recommended to use Vertex AI training for higher memory GPUs like A100 and H100. Vertex AI training offers greater availability of high-end GPUs.\n",
        "\n",
        "# @markdown **Training can take a long time (20+ hours) to complete depending on the model, dataset and axololt config.** You can reduce the training time by reducing the max training steps. This can be done by setting `max_steps` flag to some smaller value. Note that, this might also reduce the fine-tuned model's quality.\n",
        "\n",
        "# @markdown For example, if you want to run only single step of training, then you can set `[\"--use-tensorboard=True\", \"--max_steps=1\"]` in the `axolotl_flag_overrides` to achieve that.\n",
        "\n",
        "axolotl_flag_overrides = [\"--use-tensorboard=True\"]  # @param {type:\"raw\"}\n",
        "assert type(axolotl_flag_overrides) is list, \"axolotl_flag_overrides must be a list.\"\n",
        "\n",
        "axolotl_flag_overrides.append(f\"--base_model={HF_MODEL_ID}\")\n",
        "\n",
        "\n",
        "# Check if duplicate flags are passed.\n",
        "flags_seen = set()\n",
        "for flag in axolotl_flag_overrides:\n",
        "    if flag in flags_seen:\n",
        "        raise ValueError(f\"Duplicate flag: {flag}\")\n",
        "    flags_seen.add(flag)\n",
        "\n",
        "base_model = axolotl_config[\"base_model\"]\n",
        "for overrides in axolotl_flag_overrides:\n",
        "    if overrides.startswith(\"--base_model=\"):\n",
        "        base_model = overrides.split(\"=\")[1]\n",
        "        break\n",
        "publisher = base_model.split(\"/\")[0]\n",
        "model_id = base_model.split(\"/\")[1]\n",
        "model_id = model_id.replace(\".\", \"-\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrZ8eQJKODZJ"
      },
      "source": [
        "### Finetune with Local Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aQ0v6wsyODZJ"
      },
      "outputs": [],
      "source": [
        "# @title Install Axolotl And gscfuse\n",
        "# @markdown 1. Check machine type\n",
        "import subprocess\n",
        "\n",
        "try:\n",
        "    subprocess.check_output(\"nvidia-smi\")\n",
        "    print(\"Nvidia GPU detected!\")\n",
        "except Exception:\n",
        "    raise ValueError(\"Nvidia GPU not detected. Use GPU runtime for local fine-tuning.\")\n",
        "\n",
        "# @markdown 2. Check if correct pytorch is installed.\n",
        "if \"PYTORCH_INSTALLATION\" not in os.environ:\n",
        "    raise ValueError(\n",
        "        \"pytorch is not installed. Install it from `Setup Pytorch for local fine-tuning` section of the notebook.\"\n",
        "    )\n",
        "\n",
        "# @markdown 3. Install Axolotl\n",
        "! rm -rf axolotl\n",
        "! git clone https://github.com/axolotl-ai-cloud/axolotl.git\n",
        "! cd axolotl && git reset --hard 6ba5c0ed2c42a0e069b28c83646ee5a2a6904430\n",
        "! pip3 install packaging ninja\n",
        "! cd axolotl && pip3 install --no-build-isolation -e '.[flash-attn,deepspeed,llmcompressor,ring-flash-attn,optimizers]'\n",
        "! cd axolotl && python scripts/unsloth_install.py | sh\n",
        "! cd axolotl && python scripts/cutcrossentropy_install.py | sh\n",
        "\n",
        "# @markdown 4. Install gscfuse\n",
        "! apt-get install gcsfuse -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GREqaOn1ODZJ"
      },
      "outputs": [],
      "source": [
        "# @title Run Local fine-tuning\n",
        "# @markdown This section runs the Axolotl training locally (i.e. colab runtime).\n",
        "# @markdown **Note: This section can take a long time to run. You can reduce the training time by reducing the max training steps as mentioned in `Setup Axolotl Flags` section.**\n",
        "# @markdown Model trained using Axolotl will be saved in the GCS bucket with the help of gscfuse.\n",
        "\n",
        "# @markdown 1. Run gscfuse so that Axolotl can store the training output in the GCS bucket.\n",
        "! mkdir -p /gcs/\n",
        "! gcsfuse /gcs\n",
        "\n",
        "# @markdown 2. Set up huggingface cache dir and access token.\n",
        "os.environ[\"HF_HOME\"] = f\"{WORKING_DIR}/hf\"\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "\n",
        "# @markdown 3. Run Axolotl training.\n",
        "\n",
        "local_config_path = AXOLOTL_CONFIG_PATH\n",
        "if AXOLOTL_SOURCE == \"GITHUB\":\n",
        "    local_config_path = f\"{WORKING_DIR}/axolotl/{AXOLOTL_CONFIG_PATH}\"\n",
        "finetuning_time = time.time_ns()\n",
        "AXOLOTL_OUTPUT_GCS_URI = (\n",
        "    f\"{BASE_AXOLOTL_OUTPUT_GCS_URI}/local/time_ns_{finetuning_time}\"\n",
        ")\n",
        "AXOLOTL_OUTPUT_DIR = common_util.gcs_fuse_path(AXOLOTL_OUTPUT_GCS_URI)\n",
        "axolotl_args = f\" --output-dir={AXOLOTL_OUTPUT_DIR}\"\n",
        "if len(datasets) > 0:\n",
        "    axolotl_args += f' --datasets=\"{datasets}\"'\n",
        "if len(test_datasets) > 0:\n",
        "    axolotl_args += f' --test-datasets=\"{test_datasets}\"'\n",
        "    axolotl_args += \" --val-set-size=0\"\n",
        "additional_flags = \" \".join(axolotl_flag_overrides)\n",
        "axolotl_args += f\" {additional_flags}\"\n",
        "! accelerate launch -m axolotl.cli.train $axolotl_args $local_config_path\n",
        "\n",
        "# @markdown 4. Check the output in the bucket.\n",
        "! gsutil ls $AXOLOTL_OUTPUT_GCS_URI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "eD_66aBjODZJ"
      },
      "outputs": [],
      "source": [
        "# @title Run Local inference\n",
        "# @markdown This section performs inference using the finetuned model.\n",
        "# @markdown There are two options for inference:\n",
        "# @markdown 1. Gradio: This option provides a URL for the playground to test the model.\n",
        "# @markdown 2. CLI: This is option outputs the inference results in the console.\n",
        "\n",
        "INFERENCE_METHOD = \"gradio\"  # @param [\"gradio\", \"cli\"]\n",
        "\n",
        "# @markdown **Note: `CLI_PROMPT` will be only used if `INFERENCE_METHOD` is `cli`.**\n",
        "CLI_PROMPT = \"What is car?\"  # @param {type:\"string\"}\n",
        "\n",
        "if INFERENCE_METHOD == \"gradio\":\n",
        "    ! cd axolotl && export CUDA_VISIBLE_DEVICES=0 && axolotl inference --base-model=$base_model $local_config_path --lora-model-dir=$AXOLOTL_OUTPUT_DIR --gradio\n",
        "elif INFERENCE_METHOD == \"cli\":\n",
        "    assert CLI_PROMPT, \"CLI_PROMPT must be set if INFERENCE_METHOD is 'cli'.\"\n",
        "    env = os.environ.copy()\n",
        "    env[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "    cmd = [\n",
        "        \"axolotl\",\n",
        "        \"inference\",\n",
        "        local_config_path,\n",
        "        f\"--base-model={base_model}\",\n",
        "        f\"--lora-model-dir={AXOLOTL_OUTPUT_DIR}\",\n",
        "    ]\n",
        "    run_cmd_and_check_output(cmd, env, f\"{CLI_PROMPT}\\x04\", f\"{WORKING_DIR}/axolotl/\")\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported inference method: {INFERENCE_METHOD}\")\n",
        "\n",
        "\n",
        "# @markdown For Gradio, after running the cell, a public URL ([\"https://*.gradio.live\"](#)) will appear in the cell output. The playground is available in a separate browser tab when you click the URL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VEGLvAlnODZK"
      },
      "outputs": [],
      "source": [
        "# @title Create merged model\n",
        "# @markdown This section merges the finetuned adapter with the base model.\n",
        "\n",
        "if (\n",
        "    \"adapter\" in axolotl_config\n",
        "    and axolotl_config[\"adapter\"] != \"lora\"\n",
        "    and axolotl_config[\"adapter\"] != \"qlora\"\n",
        "):\n",
        "    raise ValueError(\"This cell is only needed for lora and qlora.\")\n",
        "\n",
        "# @markdown 1. Run Axolotl merge. **Note: Based on model size, this step can take 5-20 minutes to complete.**\n",
        "cmd = [\n",
        "    \"python3\",\n",
        "    \"-m\",\n",
        "    \"axolotl.cli.merge_lora\",\n",
        "    f\"--base-model={base_model}\",\n",
        "    f\"--output-dir={AXOLOTL_OUTPUT_DIR}\",\n",
        "    local_config_path,\n",
        "]\n",
        "run_cmd_and_check_output(cmd, None, None, f\"{WORKING_DIR}/axolotl/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H7V8fEjODZK"
      },
      "source": [
        "### Finetune with Vertex AI Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0XqH_Mvelhon"
      },
      "outputs": [],
      "source": [
        "# @title Vertex AI fine-tuning job\n",
        "# @markdown This section runs the Axolotl training using Vertex AI training job.\n",
        "# @markdown **Note: This section can take a long time to run. You can reduce the training time by reducing the max training steps as mentioned in `Setup Axolotl Flags` section.**\n",
        "# @markdown Refer to [Axolotl config](https://axolotl-ai-cloud.github.io/axolotl/docs/config.html) to override additional Axolotl flags.\n",
        "\n",
        "from google.cloud.aiplatform.compat.types import \\\n",
        "    custom_job as gca_custom_job_compat\n",
        "\n",
        "# @markdown Acceletor type to use for training.\n",
        "training_accelerator_type = \"NVIDIA_H100_80GB\"  # @param [\"NVIDIA_H100_80GB\", \"NVIDIA_A100_80GB\"]\n",
        "\n",
        "\n",
        "replica_count = 1\n",
        "repo = \"us-docker.pkg.dev/vertex-ai\"\n",
        "per_node_accelerator_count = 8\n",
        "boot_disk_size_gb = 500\n",
        "dws_kwargs = {\n",
        "    \"max_wait_duration\": 1800,  # 30 minutes\n",
        "    \"scheduling_strategy\": gca_custom_job_compat.Scheduling.Strategy.FLEX_START,\n",
        "}\n",
        "is_dynamic_workload_scheduler = True\n",
        "if training_accelerator_type == \"NVIDIA_A100_80GB\":\n",
        "    training_machine_type = \"a2-ultragpu-8g\"\n",
        "elif training_accelerator_type == \"NVIDIA_H100_80GB\":\n",
        "    training_machine_type = \"a3-highgpu-8g\"\n",
        "    boot_disk_size_gb = 2000\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported accelerator type: {training_accelerator_type}\")\n",
        "\n",
        "TRAIN_DOCKER_URI = (\n",
        "    f\"{repo}/vertex-vision-model-garden-dockers/axolotl-train-dws:20250515-1800-rc0\"\n",
        ")\n",
        "\n",
        "common_util.check_quota(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    accelerator_type=training_accelerator_type,\n",
        "    accelerator_count=per_node_accelerator_count * replica_count,\n",
        "    is_for_training=True,\n",
        "    is_restricted_image=False,\n",
        "    is_dynamic_workload_scheduler=is_dynamic_workload_scheduler,\n",
        ")\n",
        "\n",
        "vertex_ai_config_path = AXOLOTL_CONFIG_PATH\n",
        "# Copy the config file to the bucket.\n",
        "if AXOLOTL_SOURCE == \"LOCAL\":\n",
        "    ! gsutil -m cp $AXOLOTL_CONFIG_PATH $MODEL_BUCKET/config/\n",
        "    vertex_ai_config_path = f\"{common_util.gcs_fuse_path(MODEL_BUCKET)}/config/{pathlib.Path(AXOLOTL_CONFIG_PATH).name}\"\n",
        "\n",
        "job_name = common_util.get_job_name_with_datetime(\"axolotl-train\")\n",
        "AXOLOTL_OUTPUT_GCS_URI = f\"{BASE_AXOLOTL_OUTPUT_GCS_URI}/{job_name}\"\n",
        "AXOLOTL_OUTPUT_DIR = f\"{BASE_AXOLOTL_OUTPUT_DIR}/{job_name}\"\n",
        "\n",
        "TRAINING_JOB_OUTPUT_DIR = f\"{AXOLOTL_OUTPUT_GCS_URI}/training_job_output\"\n",
        "\n",
        "# Set Axolotl flags.\n",
        "axolotl_config_overwrites = []\n",
        "axolotl_config_overwrites.append(f\"--output_dir={AXOLOTL_OUTPUT_DIR}\")\n",
        "if len(datasets) > 0:\n",
        "    axolotl_config_overwrites.append(f\"--datasets={datasets}\")\n",
        "if len(test_datasets) > 0:\n",
        "    axolotl_config_overwrites.append(f\"--test_datasets={test_datasets}\")\n",
        "    axolotl_config_overwrites.append(\"--val_set_size=0\")\n",
        "axolotl_config_overwrites += axolotl_flag_overrides\n",
        "\n",
        "train_job_args = []\n",
        "train_job_args.append(f\"--axolotl_config_path={vertex_ai_config_path}\")\n",
        "train_job_args += axolotl_config_overwrites\n",
        "if HF_TOKEN:\n",
        "    train_job_args.append(f\"--huggingface_access_token={HF_TOKEN}\")\n",
        "\n",
        "job_name = common_util.get_job_name_with_datetime(\"axolotl-train\")\n",
        "\n",
        "# Add labels for the finetuning job.\n",
        "labels = {\n",
        "    \"mg-source\": \"notebook\",\n",
        "    \"mg-notebook-name\": \"model_garden_axolotl_gemma3_finetuning.ipynb\".split(\".\")[0],\n",
        "}\n",
        "\n",
        "model_name = AXOLOTL_CONFIG_PATH.split(\"/\")[1]\n",
        "labels[\"mg-tune\"] = f\"publishers-{publisher}-models-{model_name}\".lower()\n",
        "labels[\"versioned-mg-tune\"] = f\"{labels['mg-tune']}-{model_id}\".lower()\n",
        "labels[\"versioned-mg-tune\"] = labels[\"versioned-mg-tune\"][\n",
        "    : min(len(labels[\"versioned-mg-tune\"]), 63)\n",
        "]\n",
        "\n",
        "\n",
        "# Pass training arguments and launch job.\n",
        "train_job = aiplatform.CustomContainerTrainingJob(\n",
        "    display_name=job_name,\n",
        "    container_uri=TRAIN_DOCKER_URI,\n",
        "    labels=labels,\n",
        ")\n",
        "\n",
        "# Run Vertex AI job.\n",
        "print(\"Running training job with args:\")\n",
        "print(\" \\\\\\n\".join(train_job_args))\n",
        "train_job.run(\n",
        "    args=train_job_args,\n",
        "    replica_count=replica_count,\n",
        "    machine_type=training_machine_type,\n",
        "    accelerator_type=training_accelerator_type,\n",
        "    accelerator_count=per_node_accelerator_count,\n",
        "    boot_disk_size_gb=boot_disk_size_gb,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    base_output_dir=TRAINING_JOB_OUTPUT_DIR,\n",
        "    sync=False,  # Non-blocking call to run.\n",
        "    **dws_kwargs,\n",
        ")\n",
        "\n",
        "# Wait until resource has been created.\n",
        "train_job.wait_for_resource_creation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15BI63V2IIha"
      },
      "source": [
        "### Run TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RcSJinPWXy9_"
      },
      "outputs": [],
      "source": [
        "# @markdown This section shows how to launch TensorBoard in a [Cloud Shell](https://cloud.google.com/shell/docs).\n",
        "# @markdown 1. Click the Cloud Shell icon(![terminal](https://github.com/google/material-design-icons/blob/master/png/action/terminal/materialicons/24dp/1x/baseline_terminal_black_24dp.png?raw=true)) on the top right to open the Cloud Shell.\n",
        "# @markdown 2. Copy the `tensorboard` command shown below by running this cell.\n",
        "# @markdown 3. Paste and run the command in the Cloud Shell to launch TensorBoard.\n",
        "# @markdown 4. Once the command runs (You may have to click `Authorize` if prompted), click the link starting with `http://localhost`.\n",
        "\n",
        "# @markdown Note: You may need to wait around 10 minutes after the job starts in order for the TensorBoard logs to be written to the GCS bucket.\n",
        "print(f\"Command to copy: tensorboard --logdir {AXOLOTL_OUTPUT_GCS_URI}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO1BYNnfXy9_"
      },
      "source": [
        "## Deploy using vLLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Up326e7kXy9_"
      },
      "outputs": [],
      "source": [
        "# @markdown 1. Wait for the training job to finish.\n",
        "if train_job and train_job.end_time is None:\n",
        "    print(\"Waiting for the training job to finish...\")\n",
        "    train_job.wait()\n",
        "    print(\"The training job has finished.\")\n",
        "\n",
        "# @markdown 2. Set up VLLM docker URI and model gcs uri.\n",
        "\n",
        "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20250312_0916_RC01\"\n",
        "VLLM_MODEL_GCS_URI = AXOLOTL_OUTPUT_GCS_URI\n",
        "\n",
        "if \"adapter\" in axolotl_config and (\n",
        "    axolotl_config[\"adapter\"] == \"lora\" or axolotl_config[\"adapter\"] == \"qlora\"\n",
        "):\n",
        "    VLLM_MODEL_GCS_URI = f\"{AXOLOTL_OUTPUT_GCS_URI}/merged\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulsUk-xkXy9_"
      },
      "source": [
        "### Create model endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KSW-gb6nXy9_"
      },
      "outputs": [],
      "source": [
        "# @markdown This section uploads the model to Model Registry and deploys it on the Endpoint. It takes 15 minutes to 1 hour to finish.\n",
        "# @markdown 1. Set the machine type and accelerator type.\n",
        "# @markdown Find Vertex AI prediction supported accelerators and regions [here](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute).\n",
        "\n",
        "if \"1b\" in HF_MODEL_ID:\n",
        "    machine_type = \"g2-standard-12\"\n",
        "    accelerator_type = \"NVIDIA_L4\"\n",
        "    per_node_accelerator_count = 1\n",
        "elif \"4b\" in HF_MODEL_ID or \"12b\" in HF_MODEL_ID:\n",
        "    accelerator_type = \"NVIDIA_H100_80GB\"\n",
        "    machine_type = \"a3-highgpu-2g\"\n",
        "    per_node_accelerator_count = 2\n",
        "elif \"27b\" in HF_MODEL_ID:\n",
        "    accelerator_type = \"NVIDIA_H100_80GB\"\n",
        "    machine_type = \"a3-highgpu-4g\"\n",
        "    per_node_accelerator_count = 4\n",
        "else:\n",
        "    raise ValueError(\n",
        "        f\"Recommended machine settings not found for model: {HF_MODEL_ID}.\"\n",
        "    )\n",
        "\n",
        "common_util.check_quota(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=per_node_accelerator_count,\n",
        "    is_for_training=False,\n",
        ")\n",
        "\n",
        "# @markdown Set `use_dedicated_endpoint` to False if you don't want to use [dedicated endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment#create-dedicated-endpoint).\n",
        "use_dedicated_endpoint = True  # @param {type:\"boolean\"}\n",
        "\n",
        "gpu_memory_utilization = 0.95\n",
        "max_model_len = 131072\n",
        "\n",
        "\n",
        "def deploy_model_vllm(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    publisher: str,\n",
        "    publisher_model_id: str,\n",
        "    service_account: str,\n",
        "    base_model_id: str = None,\n",
        "    machine_type: str = \"g2-standard-8\",\n",
        "    accelerator_type: str = \"NVIDIA_L4\",\n",
        "    accelerator_count: int = 1,\n",
        "    gpu_memory_utilization: float = 0.9,\n",
        "    max_model_len: int = 4096,\n",
        "    dtype: str = \"auto\",\n",
        "    enable_trust_remote_code: bool = False,\n",
        "    enforce_eager: bool = False,\n",
        "    enable_lora: bool = False,\n",
        "    enable_chunked_prefill: bool = False,\n",
        "    enable_prefix_cache: bool = False,\n",
        "    host_prefix_kv_cache_utilization_target: float = 0.0,\n",
        "    max_loras: int = 1,\n",
        "    max_cpu_loras: int = 8,\n",
        "    use_dedicated_endpoint: bool = False,\n",
        "    max_num_seqs: int = 256,\n",
        "    model_type: str = None,\n",
        "    enable_llama_tool_parser: bool = False,\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(\n",
        "        display_name=f\"{model_name}-endpoint\",\n",
        "        dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
        "    )\n",
        "\n",
        "    if not base_model_id:\n",
        "        base_model_id = model_id\n",
        "\n",
        "    # See https://docs.vllm.ai/en/latest/models/engine_args.html for a list of possible arguments with descriptions.\n",
        "    vllm_args = [\n",
        "        \"python\",\n",
        "        \"-m\",\n",
        "        \"vllm.entrypoints.api_server\",\n",
        "        \"--host=0.0.0.0\",\n",
        "        \"--port=8080\",\n",
        "        f\"--model={model_id}\",\n",
        "        f\"--tensor-parallel-size={accelerator_count}\",\n",
        "        \"--swap-space=16\",\n",
        "        f\"--gpu-memory-utilization={gpu_memory_utilization}\",\n",
        "        f\"--max-model-len={max_model_len}\",\n",
        "        f\"--dtype={dtype}\",\n",
        "        f\"--max-loras={max_loras}\",\n",
        "        f\"--max-cpu-loras={max_cpu_loras}\",\n",
        "        f\"--max-num-seqs={max_num_seqs}\",\n",
        "        \"--disable-log-stats\",\n",
        "    ]\n",
        "\n",
        "    if enable_trust_remote_code:\n",
        "        vllm_args.append(\"--trust-remote-code\")\n",
        "\n",
        "    if enforce_eager:\n",
        "        vllm_args.append(\"--enforce-eager\")\n",
        "\n",
        "    if enable_lora:\n",
        "        vllm_args.append(\"--enable-lora\")\n",
        "\n",
        "    if enable_chunked_prefill:\n",
        "        vllm_args.append(\"--enable-chunked-prefill\")\n",
        "\n",
        "    if enable_prefix_cache:\n",
        "        vllm_args.append(\"--enable-prefix-caching\")\n",
        "\n",
        "    if 0 < host_prefix_kv_cache_utilization_target < 1:\n",
        "        vllm_args.append(\n",
        "            f\"--host-prefix-kv-cache-utilization-target={host_prefix_kv_cache_utilization_target}\"\n",
        "        )\n",
        "\n",
        "    if model_type:\n",
        "        vllm_args.append(f\"--model-type={model_type}\")\n",
        "\n",
        "    if enable_llama_tool_parser:\n",
        "        vllm_args.append(\"--enable-auto-tool-choice\")\n",
        "        vllm_args.append(\"--tool-call-parser=vertex-llama-3\")\n",
        "\n",
        "    env_vars = {\n",
        "        \"MODEL_ID\": base_model_id,\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "    }\n",
        "\n",
        "    # HF_TOKEN is not a compulsory field and may not be defined.\n",
        "    try:\n",
        "        if HF_TOKEN:\n",
        "            env_vars[\"HF_TOKEN\"] = HF_TOKEN\n",
        "    except NameError:\n",
        "        pass\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
        "        serving_container_args=vllm_args,\n",
        "        serving_container_ports=[8080],\n",
        "        serving_container_predict_route=\"/generate\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "        serving_container_environment_variables=env_vars,\n",
        "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
        "        serving_container_deployment_timeout=7200,\n",
        "        model_garden_source_model_name=(\n",
        "            f\"publishers/{publisher}/models/{publisher_model_id}\"\n",
        "        ),\n",
        "    )\n",
        "    print(\n",
        "        f\"Deploying {model_name} on {machine_type} with {accelerator_count} {accelerator_type} GPU(s).\"\n",
        "    )\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=service_account,\n",
        "        system_labels={\n",
        "            \"NOTEBOOK_NAME\": \"model_garden_axolotl_gemma3_finetuning.ipynb\",\n",
        "            \"NOTEBOOK_ENVIRONMENT\": common_util.get_deploy_source(),\n",
        "        },\n",
        "    )\n",
        "    print(\"endpoint_name:\", endpoint.name)\n",
        "\n",
        "    return model, endpoint\n",
        "\n",
        "\n",
        "models[\"vllm_gpu\"], endpoints[\"vllm_gpu\"] = deploy_model_vllm(\n",
        "    model_name=common_util.get_job_name_with_datetime(prefix=\"axolotl-vllm-serve\"),\n",
        "    publisher=publisher.lower(),\n",
        "    publisher_model_id=model_id.lower(),\n",
        "    model_id=VLLM_MODEL_GCS_URI,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=per_node_accelerator_count,\n",
        "    gpu_memory_utilization=gpu_memory_utilization,\n",
        "    max_model_len=max_model_len,\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO76I0p3Xy9_"
      },
      "source": [
        "### Perform Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kC9Apto7Xy9_"
      },
      "outputs": [],
      "source": [
        "def predict_vllm(\n",
        "    prompt: str,\n",
        "    max_tokens: int,\n",
        "    temperature: float,\n",
        "    top_p: float,\n",
        "    top_k: int,\n",
        "    raw_response: bool,\n",
        "    lora_weight: str = \"\",\n",
        "):\n",
        "    # Parameters for inference.\n",
        "    instance = {\n",
        "        \"prompt\": prompt,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": top_p,\n",
        "        \"top_k\": top_k,\n",
        "        \"raw_response\": raw_response,\n",
        "    }\n",
        "    if lora_weight:\n",
        "        instance[\"dynamic-lora\"] = lora_weight\n",
        "    instances = [instance]\n",
        "    response = endpoints[\"vllm_gpu\"].predict(\n",
        "        instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
        "    )\n",
        "\n",
        "    for prediction in response.predictions:\n",
        "        print(prediction)\n",
        "\n",
        "\n",
        "# @markdown Once deployment succeeds, you can send requests to the endpoint with text prompts. Sampling parameters supported by vLLM can be found [here](https://docs.vllm.ai/en/latest/dev/sampling_params.html).\n",
        "\n",
        "# Loads an existing endpoint instance using the endpoint name:\n",
        "# - Using `endpoint_name = endpoint.name` allows us to get the\n",
        "#   endpoint name of the endpoint `endpoint` created in the cell\n",
        "#   above.\n",
        "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
        "#   an existing endpoint with the ID 1234567890123456789.\n",
        "# You may uncomment the code below to load an existing endpoint.\n",
        "\n",
        "# endpoint_name = \"\"  # @param {type:\"string\"}\n",
        "# aip_endpoint_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
        "# )\n",
        "# endpoints[\"vllm_gpu\"] = aiplatform.Endpoint(aip_endpoint_name)\n",
        "\n",
        "prompt = \"Write a function to list n Fibonacci numbers in Python.\"  # @param {type: \"string\"}\n",
        "max_tokens = 500  # @param {type:\"integer\"}\n",
        "temperature = 1.0  # @param {type:\"number\"}\n",
        "top_p = 1.0  # @param {type:\"number\"}\n",
        "top_k = 1  # @param {type:\"integer\"}\n",
        "# @markdown Set `raw_response` to `True` to obtain the raw model output. Set `raw_response` to `False` to apply additional formatting in the structure of `\"Prompt:\\n{prompt.strip()}\\nOutput:\\n{output}\"`.\n",
        "raw_response = True  # @param {type:\"boolean\"}\n",
        "\n",
        "instances = [\n",
        "    {\n",
        "        \"prompt\": prompt,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": top_p,\n",
        "        \"top_k\": top_k,\n",
        "        \"raw_response\": raw_response,\n",
        "    },\n",
        "]\n",
        "response = endpoints[\"vllm_gpu\"].predict(\n",
        "    instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
        ")\n",
        "\n",
        "# \"<|file_separator|>\" is the end of the file token.\n",
        "for prediction in response.predictions:\n",
        "    print(prediction.split(\"<|file_separator|>\")[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2grDDphYx4zI"
      },
      "source": [
        "## Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_Pqw3TsF2uG4"
      },
      "outputs": [],
      "source": [
        "# @markdown Delete the training job.\n",
        "\n",
        "if train_job:\n",
        "    train_job.delete()\n",
        "\n",
        "# @markdown  Delete the experiment models and endpoints to recycle the resources\n",
        "# @markdown  and avoid unnecessary continuous charges that may incur.\n",
        "\n",
        "# Undeploy model and delete endpoint.\n",
        "for endpoint in endpoints.values():\n",
        "    endpoint.delete(force=True)\n",
        "\n",
        "# Delete models.\n",
        "for model in models.values():\n",
        "    model.delete()\n",
        "\n",
        "delete_bucket = False  # @param {type:\"boolean\"}\n",
        "if delete_bucket:\n",
        "    ! gsutil -m rm -r $BUCKET_NAME"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_axolotl_gemma3_finetuning.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

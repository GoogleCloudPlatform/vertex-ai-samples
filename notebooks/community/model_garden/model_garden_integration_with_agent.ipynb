{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "20qcPG1PmFUM"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXYOa1odnikj"
      },
      "source": [
        "# Vertex AI Model Garden Integration With Agents\n",
        "\n",
        "\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\n",
        "  \u003ctd style=\"text-align: center\"\u003e\n",
        "    \u003ca href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_integration_with_agent.ipynb\"\u003e\n",
        "      \u003cimg alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"\u003e\u003cbr\u003e Run in Colab Enterprise\n",
        "    \u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd style=\"text-align: center\"\u003e\n",
        "    \u003ca href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_integration_with_agent.ipynb\"\u003e\n",
        "      \u003cimg alt=\"GitHub logo\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\"\u003e\u003cbr\u003e View on GitHub\n",
        "    \u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbDI9ag4oR4C"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates how to build an agent with a Model Garden open model deployed through Vertex endpoint and [Google Agent Development Kit](https://google.github.io/adk-docs/) (ADK) and [Agent Engine](https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/quickstart). The agent can automatically call function tools like `get_weather` and `get_current_time`.\n",
        "\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Deploy Vertex AI Model Garden OSS LLMs properly for agent integration\n",
        "- Test deployed endpoints\n",
        "- Integrate model garden endpoints with ADK (example: qwen3)\n",
        "- Integrate model garden endpoints with Agent Engine (example: qwen3, llama3, llama4, deepseek-r1)\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQJWRopioSKT"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_jmxcIZoSxU",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown 2. **[Optional]** [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs. Set the BUCKET_URI for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`) should be located in the same region as where the notebook was launched. Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\"). If not set, a unique GCS bucket will be created instead.\n",
        "\n",
        "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown 3. **[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment.\n",
        "\n",
        "REGION = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown 4. If you want to run predictions with A100 80GB or H100 GPUs, we recommend using the regions listed below. **NOTE:** Make sure you have associated quota in selected regions. Click the links to see your current quota for each GPU type: [Nvidia A100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_a100_80gb_gpus), [Nvidia H100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_h100_gpus). You can request for quota following the instructions at [\"Request a higher quota\"](https://cloud.google.com/docs/quota/view-manage#requesting_higher_quota).\n",
        "\n",
        "# @markdown \u003e | Machine Type | Accelerator Type | Recommended Regions |\n",
        "# @markdown | ----------- | ----------- | ----------- |\n",
        "# @markdown | a2-ultragpu-1g | 1 NVIDIA_A100_80GB | us-central1, us-east4, europe-west4, asia-southeast1, us-east4 |\n",
        "# @markdown | a3-highgpu-2g | 2 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "# @markdown | a3-highgpu-4g | 4 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "# @markdown | a3-highgpu-8g | 8 NVIDIA_H100_80GB | us-central1, europe-west4, us-west1, asia-southeast1 |\n",
        "\n",
        "# Upgrade Vertex AI SDK.\n",
        "! pip3 install --upgrade --quiet 'google-cloud-aiplatform\u003e=1.84.0'\n",
        "! pip3 install -qU openai google-auth requests\n",
        "! pip3 install --upgrade --quiet \\\n",
        "    \"google-cloud-aiplatform[agent_engines,langchain]\" \\\n",
        "    cloudpickle==3.0.0 \\\n",
        "    langchain-google-community \\\n",
        "    pydantic==2.10.6 \\\n",
        "    requests \\\n",
        "    langchain-openai\n",
        "\n",
        "# Import the necessary packages\n",
        "import datetime\n",
        "import importlib\n",
        "import os\n",
        "import uuid\n",
        "from typing import Tuple\n",
        "import zoneinfo\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "import google.auth\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from vertexai.preview import reasoning_engines\n",
        "from vertexai import agent_engines\n",
        "import vertexai\n",
        "\n",
        "\n",
        "if not os.path.exists(\"./vertex-ai-samples\"):\n",
        "  ! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
        "\n",
        "models, endpoints = {}, {}\n",
        "\n",
        "common_util = importlib.import_module(\n",
        "    \"vertex-ai-samples.community-content.vertex_model_garden.model_oss.notebook_util.common_util\"\n",
        ")\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "if not REGION:\n",
        "    if not os.environ.get(\"GOOGLE_CLOUD_REGION\"):\n",
        "        raise ValueError(\n",
        "            \"REGION must be set. See\"\n",
        "            \" https://cloud.google.com/vertex-ai/docs/general/locations for\"\n",
        "            \" available cloud locations.\"\n",
        "        )\n",
        "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
        "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
        "\n",
        "# Cloud Storage bucket for storing the experiment artifacts.\n",
        "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
        "# prefer using your own GCS bucket, change the value yourself below.\n",
        "now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "\n",
        "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
        "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}-{str(uuid.uuid4())[:4]}\"\n",
        "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
        "else:\n",
        "    assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
        "    shell_output = ! gsutil ls -Lb {BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
        "    bucket_region = shell_output[0].strip().lower()\n",
        "    if bucket_region != REGION:\n",
        "        raise ValueError(\n",
        "            \"Bucket region %s is different from notebook region %s\"\n",
        "            % (bucket_region, REGION)\n",
        "        )\n",
        "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)\n",
        "vertexai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)\n",
        "\n",
        "# Gets the default SERVICE_ACCOUNT.\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
        "\n",
        "\n",
        "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
        "\n",
        "! gcloud config set project $PROJECT_ID\n",
        "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/storage.admin\"\n",
        "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/aiplatform.user\"\n",
        "\n",
        "from etils import epath\n",
        "TUTORIAL_DIR = epath.Path(\"vmg_adk_agent_tutorial\")\n",
        "BUILD_DIR = TUTORIAL_DIR / \"build\"\n",
        "BUILD_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "REPOSITORY_NAME = \"vertex-vision-model-garden-dockers\"\n",
        "\n",
        "!gcloud artifacts repositories create $REPOSITORY_NAME \\\n",
        "      --repository-format=docker \\\n",
        "      --location=$REGION \\\n",
        "      --project=$PROJECT_ID\n",
        "\n",
        "! gcloud auth configure-docker $REGION-docker.pkg.dev --quiet\n",
        "\n",
        "\n",
        "# Utils for tool calls.\n",
        "from langchain.agents.format_scratchpad.tools import format_to_tool_messages\n",
        "from langchain.memory import ChatMessageHistory\n",
        "from langchain_core import prompts\n",
        "from langchain_core.tools import render_text_description\n",
        "from vertexai.preview.generative_models import ToolConfig\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Any, Dict, List\n",
        "import json\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def get_weather(city: str) -\u003e str:\n",
        "  \"\"\"Simulates a web search. Use it get information on weather.\n",
        "\n",
        "  Args:\n",
        "      city: A string containing the location to get weather information for.\n",
        "\n",
        "  Returns:\n",
        "      A string with the simulated weather information for the queried city.\n",
        "  \"\"\"\n",
        "  if \"sf\" in city.lower() or \"san francisco\" in city.lower():\n",
        "    return \"It's 70 degrees and foggy.\"\n",
        "  return \"It's 80 degrees and sunny.\"\n",
        "\n",
        "\n",
        "@tool\n",
        "def get_current_time(city: str) -\u003e str:\n",
        "  \"\"\"Simulates getting the current time for a city.\n",
        "\n",
        "  Args:\n",
        "      city: The name of the city to get the current time for.\n",
        "\n",
        "  Returns:\n",
        "      A string with the current time information.\n",
        "  \"\"\"\n",
        "  if \"sf\" in city.lower() or \"san francisco\" in city.lower():\n",
        "    tz_identifier = \"America/Los_Angeles\"\n",
        "  else:\n",
        "    return f\"Sorry, I don't have timezone information for city: {city}.\"\n",
        "\n",
        "  tz = zoneinfo.ZoneInfo(tz_identifier)\n",
        "  now = datetime.datetime.now(tz)\n",
        "  return (\n",
        "      f\"The current time for city {city} is\"\n",
        "      f\" {now.strftime('%Y-%m-%d %H:%M:%S %Z%z')}\"\n",
        "  )\n",
        "\n",
        "tools = [get_weather, get_current_time]\n",
        "\n",
        "# Define prompt template\n",
        "\n",
        "rendered_tools = render_text_description(tools)\n",
        "system_prompt_with_tools = f\"\"\"You are an assistant that has access to the\n",
        "following set of tools. Here are the names and descriptions for each tool:\n",
        "\n",
        "    {rendered_tools}\n",
        "\n",
        "Given the user input, if the results need to call tools, please append the tool call results to the response, in the format of a JSON blob.\n",
        "The tool call results should be the name and input of the tool to use, and return your response as a JSON blob with 'name' and 'arguments' keys. The `arguments` should be a dictionary, with keys corresponding to the argument names and the values corresponding to the requested values.\n",
        "\n",
        "The json body must be in the format as:\n",
        "\n",
        "```json\n",
        "tool names and args.\n",
        "```\n",
        "The examples of json body are:\n",
        "\n",
        "```json\n",
        "{{\n",
        "  'name': 'get_weather',\n",
        "  'arguments': {{\n",
        "    'city': 'SF'\n",
        "  }}\n",
        "}}\n",
        "```\n",
        "\n",
        "Please make the response as reasonable as possible.\n",
        "\n",
        "The input is as below.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "system_prompt_without_tools = \"\"\"You are an assistant.\n",
        "There might be JSON blob in the response.\n",
        "Please remove the JSON blob and make the response as reasonable as possible.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class ToolCallRequest(BaseModel):\n",
        "  \"\"\"A typed dict that shows the inputs into the invoke_tool function.\"\"\"\n",
        "\n",
        "  name: str = Field(description=\"The name of the tool to call.\")\n",
        "  arguments: Dict[str, Any] = Field(\n",
        "      description=\"The arguments to pass to the tool.\"\n",
        "  )\n",
        "\n",
        "\n",
        "def call_tools(model_output: str = None) -\u003e List[Any]:\n",
        "  \"\"\"Execute the tool calls.\"\"\"\n",
        "  if not model_output:\n",
        "    return []\n",
        "  tool_map = {tool.name: tool for tool in tools}\n",
        "  parser = JsonOutputParser(pydantic_object=ToolCallRequest)\n",
        "  format_tool_calls = []\n",
        "  try:\n",
        "    tool_calls = parser.parse(model_output)\n",
        "    if not tool_calls:\n",
        "      return \"\"\n",
        "    if not isinstance(tool_calls, list):\n",
        "      tool_calls = [tool_calls]\n",
        "    for tool_call in tool_calls:\n",
        "      tool_name = tool_call[\"name\"]\n",
        "      tool_arguments = tool_call[\"arguments\"]\n",
        "      if tool_name not in tool_map:\n",
        "        continue\n",
        "\n",
        "      tool_function = tool_map[tool_name]\n",
        "      tool_result = tool_function.invoke(tool_call[\"arguments\"])\n",
        "      format_tool_calls.append({\n",
        "          \"name\": tool_name,\n",
        "          \"arguments\": tool_arguments,\n",
        "          \"result\": str(tool_result),\n",
        "      })\n",
        "  except Exception as ex:  # pylint: disable=broad-except\n",
        "    print(str(ex))\n",
        "\n",
        "  return format_tool_calls\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploy And Test OSS LLM Models For Agents"
      ],
      "metadata": {
        "id": "de6hEdbjNvWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Deploy OSS LLMs\n",
        "# @markdown This section will show how to deploy OSS LLMs properly for agent integration.\n",
        "\n",
        "# @markdown Note that, 1) if models support tool calls, then the deployment should enable tool calls.\n",
        "\n",
        "# @markdown - If the models are deployed with *vllm*,\n",
        "# @markdown the deployment should specify settings like `--enable-auto-tool-choice`\n",
        "# @markdown and `--tool-call-parser=hermes`.\n",
        "\n",
        "# @markdown - If the models are deployed with *sglang*, the deployment should specify\n",
        "# @markdown setting like `--tool-call-parser=qwen25`.\n",
        "# @markdown Refer to tool calls in\n",
        "# @markdown [vllm](https://docs.vllm.ai/en/stable/features/tool_calling.html)\n",
        "# @markdown and [sglang](https://docs.sglang.ai/backend/function_calling.html) for more details.\n",
        "\n",
        "# @markdown 2) agent engine supports both traditional and dedicated endpoints,\n",
        "# @markdown but ADK integration does not support dedicated points yet. You can\n",
        "# @markdown also refer to deployment examples for [qwen3](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_qwen3_deployment.ipynb),\n",
        "# @markdown [llama4](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_llama4_deployment.ipynb)\n",
        "# @markdown and [deepseek-r1](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_deepseek_deployment.ipynb).\n",
        "# @markdown For simplicity, we deploy models with traditional endpoints by default here.\n",
        "\n",
        "MODEL_ID = \"Qwen3-32B\"  # @param [\"Qwen3-32B\", \"llama-4-scout-17b-16e-instruct\", \"llama-3.3-70b-instruct\", \"DeepSeek-R1-Distill-Llama-70B\", \"gemma-3-27b-it\"] {isTemplate: true}\n",
        "accelerator_type = \"NVIDIA_H100_80GB\"  # @param [\"NVIDIA_L4\", \"NVIDIA_H100_80GB\"] {isTemplate: true}\n",
        "\n",
        "if \"Qwen\" in MODEL_ID:\n",
        "  publisher_model_name = f\"publishers/qwen/models/qwen3@{MODEL_ID.lower()}\"\n",
        "\n",
        "  if accelerator_type == \"NVIDIA_L4\":\n",
        "    accelerator_count = 4\n",
        "    # Sets machine type to g2-standard-48 for 4 L4's\n",
        "    machine_type = \"g2-standard-48\"\n",
        "  elif accelerator_type == \"NVIDIA_H100_80GB\":\n",
        "    accelerator_count = 2\n",
        "    machine_type = \"a3-highgpu-2g\"\n",
        "  else:\n",
        "    raise ValueError(\n",
        "        \"Recommended machine settings not found for accelerator type: %s\"\n",
        "        % accelerator_type\n",
        "    )\n",
        "elif \"llama-4\" in MODEL_ID:\n",
        "  publisher_model_name = f\"publishers/meta/models/llama4@{MODEL_ID.lower()}\"\n",
        "  if accelerator_type == \"NVIDIA_H100_80GB\":\n",
        "    accelerator_count = 8\n",
        "    machine_type = \"a3-highgpu-8g\"\n",
        "  else:\n",
        "    raise ValueError(\n",
        "        \"Recommended machine settings not found for accelerator type: %s\"\n",
        "        % accelerator_type\n",
        "    )\n",
        "elif \"llama-3\" in MODEL_ID:\n",
        "  publisher_model_name = f\"publishers/meta/models/llama3-3@{MODEL_ID.lower()}\"\n",
        "  if accelerator_type == \"NVIDIA_H100_80GB\":\n",
        "    accelerator_count = 4\n",
        "    machine_type = \"a3-highgpu-4g\"\n",
        "  else:\n",
        "    raise ValueError(\n",
        "        \"Recommended machine settings not found for accelerator type: %s\"\n",
        "        % accelerator_type\n",
        "    )\n",
        "elif \"DeepSeek-R1\" in MODEL_ID:\n",
        "  publisher_model_name = (\n",
        "      f\"publishers/deepseek-ai/models/deepseek-r1@{MODEL_ID.lower()}\"\n",
        "  )\n",
        "  if accelerator_type == \"NVIDIA_L4\":\n",
        "    accelerator_count = 8\n",
        "    # Sets machine type to g2-standard-96 for 8 L4's\n",
        "    machine_type = \"g2-standard-96\"\n",
        "  elif accelerator_type == \"NVIDIA_H100_80GB\":\n",
        "    accelerator_count = 4\n",
        "    machine_type = \"a3-highgpu-4g\"\n",
        "  else:\n",
        "    raise ValueError(\n",
        "        \"Recommended machine settings not found for accelerator type: %s\"\n",
        "        % accelerator_type\n",
        "    )\n",
        "elif \"gemma-3\" in MODEL_ID:\n",
        "  publisher_model_name = f\"publishers/google/models/gemma3@{MODEL_ID.lower()}\"\n",
        "  if accelerator_type == \"NVIDIA_L4\":\n",
        "    accelerator_count = 4\n",
        "    # Sets machine type to g2-standard-48 for 4 L4's\n",
        "    machine_type = \"g2-standard-48\"\n",
        "  elif accelerator_type == \"NVIDIA_H100_80GB\":\n",
        "    accelerator_count = 2\n",
        "    machine_type = \"a3-highgpu-2g\"\n",
        "  else:\n",
        "    raise ValueError(\n",
        "        \"Recommended machine settings not found for accelerator type: %s\"\n",
        "        % accelerator_type\n",
        "    )\n",
        "else:\n",
        "  raise ValueError(\"Unsupported model: %s\" % MODEL_ID)\n",
        "\n",
        "from vertexai.preview import model_garden\n",
        "\n",
        "model = model_garden.OpenModel(publisher_model_name)\n",
        "\n",
        "if \"Qwen3-32B\" == MODEL_ID:\n",
        "  container_spec = model.list_deploy_options()[0].container_spec\n",
        "  updated_args = container_spec.args[:-2] + [\n",
        "      f\"--tp={accelerator_count}\",\n",
        "      \"--tool-call-parser=qwen25\",\n",
        "  ]\n",
        "  container_spec.args = updated_args\n",
        "  accept_eula = False\n",
        "elif \"llama-4-scout-17b-16e-instruct\" == MODEL_ID:\n",
        "  container_spec = model.list_deploy_options()[1].container_spec\n",
        "  container_spec.image_uri = \"us-docker.pkg.dev/deeplearning-platform-release/vertex-model-garden/sglang-serve.cu124.0-4.ubuntu2204.py310:20250515-1800-rc0\"\n",
        "  updated_args = container_spec.args[:-1] + [\n",
        "      f\"--tp={accelerator_count}\",\n",
        "      \"--tool-call-parser=pythonic\",\n",
        "  ]\n",
        "  container_spec.args = updated_args\n",
        "  accept_eula = True\n",
        "elif \"llama-3.3-70b-instruct\" == MODEL_ID:\n",
        "  container_spec = model.list_deploy_options()[0].container_spec\n",
        "  accept_eula = True\n",
        "elif \"DeepSeek-R1\" in MODEL_ID:\n",
        "  container_spec = model.list_deploy_options()[0].container_spec\n",
        "  updated_args = container_spec.args[:-1] + [f\"--tp={accelerator_count}\"]\n",
        "  container_spec.args = updated_args\n",
        "  accept_eula = False\n",
        "elif \"gemma-3-27b-it\" == MODEL_ID:\n",
        "  container_spec = model.list_deploy_options()[0].container_spec\n",
        "  updated_args = []\n",
        "  for arg in container_spec.args:\n",
        "    if arg.startswith(\"--tensor-parallel-size\"):\n",
        "      updated_args.append(f\"--tensor-parallel-size={accelerator_count}\")\n",
        "    else:\n",
        "      updated_args.append(arg)\n",
        "  container_spec.args = updated_args\n",
        "\n",
        "  accept_eula = True\n",
        "else:\n",
        "  raise ValueError(\"Unsupported model: %s\" % MODEL_ID)\n",
        "\n",
        "print(\"The container spec are:\")\n",
        "print(container_spec)\n",
        "\n",
        "print(\"Start to check quota for the deployment.\")\n",
        "common_util.check_quota(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    is_for_training=False,\n",
        ")\n",
        "print(\"Finished to check quota for the deployment.\")\n",
        "\n",
        "print(\"Start to deploy models to endpoints.\")\n",
        "endpoint = model.deploy(\n",
        "    serving_container_spec=container_spec,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    use_dedicated_endpoint=False,\n",
        "    spot=False,\n",
        "    deploy_request_timeout=deploy_request_timeout,\n",
        "    accept_eula=False,\n",
        ")\n",
        "print(\"Finished to deploy models to endpoints.\")\n",
        "# @markdown After endpoints are deployed successfully, you get the endpoint\n",
        "# @markdown resource name with the format as\n",
        "# @markdown `projects/{PROJECT_ID}/locations/{REGION}/endpoints/{ENDPOINT_ID}`.\n",
        "# @markdown The endpoint resource name will be used in local predictions and\n",
        "# @markdown integration with ADK below.\n",
        "endpoint_resource_name = endpoint.resource_name\n",
        "print(\"The deployed endpoint resource name is:\")\n",
        "print(endpoint_resource_name)\n",
        "# @markdown Click \"Show Code\" to see more details."
      ],
      "metadata": {
        "id": "gbfFZLT5KkUV",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Test The Endpoint\n",
        "# endpoint_resource_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{ENDPOINT_NAME}\"\n",
        "# )\n",
        "endpoint = aiplatform.Endpoint(endpoint_resource_name)\n",
        "\n",
        "location = endpoint_resource_name.split(\"/\")[3]\n",
        "if endpoint.gca_resource.dedicated_endpoint_enabled:\n",
        "  base_url = f\"https://{endpoint.gca_resource.dedicated_endpoint_dns}/v1beta1/{endpoint.resource_name}\"\n",
        "else:\n",
        "  base_url = f\"https://{location}-aiplatform.googleapis.com/v1beta1/{endpoint.resource_name}\"\n",
        "\n",
        "# @markdown Predict locally with some requests.\n",
        "\n",
        "user_message = \"How is your day going?\"  # @param {type: \"string\"}\n",
        "# @markdown If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, such as set `max_tokens` as 20.\n",
        "max_tokens = 1000  # @param {type: \"integer\"}\n",
        "temperature = 1.0  # @param {type: \"number\"}\n",
        "stream = True  # @param {type: \"boolean\"}\n",
        "\n",
        "import google.auth\n",
        "import openai\n",
        "\n",
        "creds, project = google.auth.default()\n",
        "auth_req = google.auth.transport.requests.Request()\n",
        "creds.refresh(auth_req)\n",
        "\n",
        "client = openai.OpenAI(base_url=base_url, api_key=creds.token)\n",
        "\n",
        "model_response = client.chat.completions.create(\n",
        "    model=\"\",\n",
        "    messages=[{\"role\": \"user\", \"content\": user_message}],\n",
        "    temperature=temperature,\n",
        "    max_tokens=max_tokens,\n",
        "    stream=stream,\n",
        ")\n",
        "\n",
        "if stream:\n",
        "  usage = None\n",
        "  contents = []\n",
        "  for chunk in model_response:\n",
        "    if chunk.usage is not None:\n",
        "      usage = chunk.usage\n",
        "      continue\n",
        "    print(chunk.choices[0].delta.content, end=\"\")\n",
        "    contents.append(chunk.choices[0].delta.content)\n",
        "  print(f\"\\n\\n{usage}\")\n",
        "else:\n",
        "  print(model_response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "_MNVrfJomCd0",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Integrate OSS LLMs With ADK"
      ],
      "metadata": {
        "id": "T9UiFwbiowOV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aa4e1-6FvRAP",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Build Agent Web App Dockers With VMG Endpoints\n",
        "# @markdown The section will create required python and docker files first, and\n",
        "# @markdown then build the dockers with cloud build.\n",
        "# @markdown Note, if a model (e.g.: qwen3) is deployed properly with enabling\n",
        "# @markdown tools, then the integration with ADK can work properly.\n",
        "\n",
        "# @markdown 1. Create `agent.py` by loading VMG endpoints and example tool functions.\n",
        "agent_app = '''\n",
        "\"\"\"This is a sample agent for model garden agents.\"\"\"\n",
        "\n",
        "import datetime\n",
        "import os\n",
        "import re\n",
        "import zoneinfo\n",
        "\n",
        "from google.adk.agents import LlmAgent\n",
        "from google.adk.models.lite_llm import LiteLlm\n",
        "import google.auth\n",
        "\n",
        "_MODEL_GARDEN_ENDPOINT_REGEX = r\"projects\\/.+\\/locations\\/.+\\/endpoints\\/.+\"\n",
        "\n",
        "\n",
        "def get_weather(city: str) -\u003e str:\n",
        "  \"\"\"Simulates a web search. Use it get information on weather.\n",
        "\n",
        "  Args:\n",
        "      city: A string containing the location to get weather information for.\n",
        "\n",
        "  Returns:\n",
        "      A string with the simulated weather information for the queried city.\n",
        "  \"\"\"\n",
        "  if \"sf\" in city.lower() or \"san francisco\" in city.lower():\n",
        "    return \"It's 70 degrees and foggy.\"\n",
        "  return \"It's 80 degrees and sunny.\"\n",
        "\n",
        "\n",
        "def get_current_time(city: str) -\u003e str:\n",
        "  \"\"\"Simulates getting the current time for a city.\n",
        "\n",
        "  Args:\n",
        "      city: The name of the city to get the current time for.\n",
        "\n",
        "  Returns:\n",
        "      A string with the current time information.\n",
        "  \"\"\"\n",
        "  if \"sf\" in city.lower() or \"san francisco\" in city.lower():\n",
        "    tz_identifier = \"America/Los_Angeles\"\n",
        "  else:\n",
        "    return f\"Sorry, I don't have timezone information for city: {city}.\"\n",
        "\n",
        "  tz = zoneinfo.ZoneInfo(tz_identifier)\n",
        "  now = datetime.datetime.now(tz)\n",
        "  return (\n",
        "      f\"The current time for city {city} is\"\n",
        "      f\" {now.strftime('%Y-%m-%d %H:%M:%S %Z%z')}\"\n",
        "  )\n",
        "\n",
        "\n",
        "def _get_auth_headers() -\u003e dict[str, str]:\n",
        "  \"\"\"Gets the auth headers for the model garden endpoint.\"\"\"\n",
        "  creds, _ = google.auth.default(\n",
        "      scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n",
        "  )\n",
        "  auth_req = google.auth.transport.requests.Request()\n",
        "  creds.refresh(auth_req)\n",
        "  return {\n",
        "      \"Content-Type\": \"application/json\",\n",
        "      \"Authorization\": f\"Bearer {creds.token}\",\n",
        "  }\n",
        "\n",
        "\n",
        "def _setup_model_garden_endpoint():\n",
        "  \"\"\"Sets up the model garden endpoint.\"\"\"\n",
        "  endpoint = os.environ.get(\"GOOGLE_MODEL_GARDEN_ENDPOINT\", \"\")\n",
        "\n",
        "  if not re.compile(_MODEL_GARDEN_ENDPOINT_REGEX).fullmatch(endpoint):\n",
        "    raise ValueError(\n",
        "        f\"Invalid model garden endpoint: {endpoint}. Please use the format\"\n",
        "        \" projects/{project}/locations/{location}/endpoints/{endpoint}.\"\n",
        "    )\n",
        "  endpoint_parts = endpoint.split(\"/\")\n",
        "  os.environ.setdefault(\"GOOGLE_GENAI_USE_VERTEXAI\", \"True\")\n",
        "  os.environ[\"VERTEXAI_PROJECT\"] = endpoint_parts[1]\n",
        "  os.environ[\"VERTEXAI_LOCATION\"] = endpoint_parts[3]\n",
        "  os.environ[\"LITELLM_LOG\"] = \"DEBUG\"\n",
        "  return f\"vertex_ai/openai/{endpoint_parts[5]}\"\n",
        "\n",
        "\n",
        "auth_headers = _get_auth_headers()\n",
        "model = _setup_model_garden_endpoint()\n",
        "\n",
        "print(\"The current model is: {model}\")\n",
        "\n",
        "root_agent = LlmAgent(\n",
        "    name=\"root_agent\",\n",
        "    model=LiteLlm(\n",
        "        model=model,\n",
        "        extra_headers=auth_headers,\n",
        "    ),\n",
        "    instruction=(\n",
        "        \"You are a helpful AI assistant designed to provide accurate and useful\"\n",
        "        \" information. Please output the tool callings with json format if\"\n",
        "        \" exists.\"\n",
        "    ),\n",
        "    description=\"Retrieves the weather and current time using specific tools.\",\n",
        "    tools=[get_weather, get_current_time],\n",
        ")\n",
        "'''\n",
        "with BUILD_DIR.joinpath(\"agent.py\").open(\"w\") as f:\n",
        "  f.write(agent_app)\n",
        "\n",
        "# @markdown 2. Create `__init__.py` to load agent.py for ADK apps.\n",
        "initialize = \"\"\"\n",
        "from . import agent\n",
        "\"\"\"\n",
        "with BUILD_DIR.joinpath(\"__init__.py\").open(\"w\") as f:\n",
        "  f.write(initialize)\n",
        "\n",
        "# @markdown 3. Create `Dockerfile` to build agent app dockers.\n",
        "dockerfile_content = \"\"\"\n",
        "FROM python:3.11-slim\n",
        "WORKDIR /app\n",
        "RUN adduser --disabled-password --gecos \"\" myuser\n",
        "RUN chown -R myuser:myuser /app\n",
        "USER myuser\n",
        "ENV PATH=\"/home/myuser/.local/bin:$PATH\"\n",
        "RUN pip install \\\n",
        "  google-adk~=0.4.0 \\\n",
        "  google-cloud-logging~=3.11.4 \\\n",
        "  opentelemetry-exporter-gcp-trace~=1.9.0 \\\n",
        "  google-cloud-aiplatform[evaluation,agent-engines]~=1.88.0 \\\n",
        "  litellm~=1.66.2\n",
        "\n",
        "COPY agent.py \"/app/agents/model_garden_agents/\"\n",
        "COPY __init__.py \"/app/agents/model_garden_agents/\"\n",
        "ENV GOOGLE_MODEL_GARDEN_ENDPOINT YOUR_ENDPOINT\n",
        "EXPOSE 8000\n",
        "CMD adk web --port=8000 --trace_to_cloud \"/app/agents\"\n",
        "\"\"\"\n",
        "with BUILD_DIR.joinpath(\"Dockerfile\").open(\"w\") as f:\n",
        "  f.write(dockerfile_content)\n",
        "\n",
        "# @markdown 4. Build agent web app dockers.\n",
        "VMG_AGENT_UI_CONTAINER_IMAGE_URI = (\n",
        "    f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY_NAME}/vmg-adk-ui\"\n",
        ")\n",
        "! gcloud builds submit --tag $VMG_AGENT_UI_CONTAINER_IMAGE_URI --project $PROJECT_ID --machine-type e2-highcpu-32 $BUILD_DIR\n",
        "print(\"The agent UI docker is :\")\n",
        "print(VMG_AGENT_UI_CONTAINER_IMAGE_URI)\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Deploy Agent Web App Dockers To Cloud Run\n",
        "\n",
        "! gcloud run deploy vmg-agent-ui-1 \\\n",
        "    --port 8000 \\\n",
        "    --image=\"{VMG_AGENT_UI_CONTAINER_IMAGE_URI}\" \\\n",
        "    --region=\"{REGION}\" \\\n",
        "    --platform=managed \\\n",
        "    --allow-unauthenticated \\\n",
        "    --memory=1024Mi \\\n",
        "    --set-env-vars=\"GOOGLE_MODEL_GARDEN_ENDPOINT={endpoint_resource_name}\"\n",
        "# @markdown Click \"Show Code\" to see more details."
      ],
      "metadata": {
        "id": "L2AGmQZmuVam",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Test With Web App\n",
        "# @markdown After the deployment, there will be a service URL. You can click\n",
        "# @markdown the service URL and interact with the agent web app. The agent web\n",
        "# @markdown is a built-in development UI in [ADK](https://github.com/google/adk-python?tab=readme-ov-file)\n",
        "# @markdown to help you test, evaluate, debug, and showcase your agent(s).\n",
        "\n",
        "# @markdown ![ADK WEB UI](https://raw.githubusercontent.com/google/adk-python/main/assets/adk-web-dev-ui-function-call.png)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "d5WGF_jG0CEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Integrate OSS LLMs With Agent Engine"
      ],
      "metadata": {
        "id": "ZI9NZvpPQcgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Build Agents With Endpoints And Agent Engine\n",
        "\n",
        "# @markdown This section will build agents using deployed endpoints above and agent engine.\n",
        "# @markdown If you already have an existing endpoint, which has the format as\n",
        "# @markdown `projects/{PROJECT_ID}/locations/{REGION}/endpoints/{ENDPOINT_NAME}`,\n",
        "# @markdown you can load the endpoint by `endpoint = aiplatform.Endpoint(endpoint_resource_name)`.\n",
        "\n",
        "# endpoint_resource_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{ENDPOINT_NAME}\"\n",
        "# )\n",
        "\n",
        "endpoint = aiplatform.Endpoint(endpoint_resource_name)\n",
        "location = endpoint_resource_name.split(\"/\")[3]\n",
        "if endpoint.gca_resource.dedicated_endpoint_enabled:\n",
        "  base_url = f\"https://{endpoint.gca_resource.dedicated_endpoint_dns}/v1beta1/{endpoint.resource_name}\"\n",
        "else:\n",
        "  base_url = f\"https://{location}-aiplatform.googleapis.com/v1beta1/{endpoint.resource_name}\"\n",
        "\n",
        "\n",
        "def model_builder(\n",
        "    *,\n",
        "    model_name: str,\n",
        "    model_kwargs=None,\n",
        "    project: str,  # Specified via vertexai.init\n",
        "    location: str,  # Specified via vertexai.init\n",
        "    **kwargs,\n",
        "):\n",
        "  # Note: the credential expires after 1 hour by default.\n",
        "  # After expiration, it must be refreshed.\n",
        "  creds, _ = google.auth.default(\n",
        "      scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n",
        "  )\n",
        "  auth_req = google.auth.transport.requests.Request()\n",
        "  creds.refresh(auth_req)\n",
        "\n",
        "  if model_kwargs is None:\n",
        "    model_kwargs = {}\n",
        "\n",
        "  return ChatOpenAI(\n",
        "      model=\"\",\n",
        "      base_url=base_url,\n",
        "      api_key=creds.token,\n",
        "      **model_kwargs,\n",
        "  )\n",
        "\n",
        "\n",
        "# @markdown Use the following parameters to generate different answers:\n",
        "# @markdown *   `max_tokens` to control the max tokens of the response\n",
        "# @markdown *   `temperature` to control the randomness of the response\n",
        "\n",
        "max_tokens = 1000  # @param {type:\"number\"}\n",
        "temperature = 1.0  # @param {type:\"number\"}\n",
        "\n",
        "\n",
        "prompt = {\n",
        "    \"system_prompt\": lambda x: x[\"system_prompt\"],\n",
        "    \"history\": lambda x: x[\"history\"],\n",
        "    \"input\": lambda x: x[\"input\"],\n",
        "    \"ai_prompt\": lambda x: x[\"ai_prompt\"],\n",
        "    \"agent_scratchpad\": lambda x: format_to_tool_messages(\n",
        "        x[\"intermediate_steps\"]\n",
        "    ),\n",
        "} | prompts.ChatPromptTemplate.from_messages([\n",
        "    prompts.MessagesPlaceholder(variable_name=\"system_prompt\"),\n",
        "    prompts.MessagesPlaceholder(variable_name=\"history\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "    prompts.MessagesPlaceholder(variable_name=\"ai_prompt\"),\n",
        "    prompts.MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "])\n",
        "\n",
        "# Initialize session history\n",
        "store = {}\n",
        "\n",
        "\n",
        "def get_session_history(session_id: str):\n",
        "  if session_id not in store:\n",
        "    store[session_id] = ChatMessageHistory()\n",
        "  return store[session_id]\n",
        "\n",
        "\n",
        "agent = agent_engines.LangchainAgent(\n",
        "    prompt=prompt,\n",
        "    model=\"\",  # Required.\n",
        "    chat_history=get_session_history,\n",
        "    model_builder=model_builder,  # Required.\n",
        "    model_kwargs={\n",
        "        \"temperature\": temperature,  # Optional.\n",
        "        \"max_tokens\": max_tokens,  # Optional.\n",
        "        \"extra_body\": {},\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8A_DhwDrQbOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Test The Agent Locally\n",
        "# @markdown You can check the agent result before deploying to Vertex AI.\n",
        "\n",
        "\n",
        "def test_agent(running_agent, query, session_id):\n",
        "  response = running_agent.query(\n",
        "      input={\n",
        "          \"system_prompt\": [(\"system\", system_prompt_with_tools)],\n",
        "          \"input\": query,\n",
        "          \"ai_prompt\": [],\n",
        "      },\n",
        "      config={\"configurable\": {\"session_id\": session_id}},\n",
        "  )\n",
        "  tool_results = call_tools(response[\"output\"])\n",
        "\n",
        "  if tool_results:\n",
        "    temporal_result = response[\"output\"] + json.dumps(tool_results)\n",
        "    response = running_agent.query(\n",
        "        input={\n",
        "            \"system_prompt\": [(\"system\", system_prompt_without_tools)],\n",
        "            \"ai_prompt\": [(\"ai\", temporal_result)],\n",
        "            \"input\": query,\n",
        "        },\n",
        "        config={\"configurable\": {\"session_id\": session_id}},\n",
        "    )\n",
        "\n",
        "  print(response[\"output\"])\n",
        "\n",
        "\n",
        "query = \"What is the weather and current time in SF?\"  # @param {type:\"string\"}\n",
        "session_id = \"demo\"  # @param {type:\"string\"}\n",
        "\n",
        "test_agent(\n",
        "    running_agent=agent,\n",
        "    query=query,\n",
        "    session_id=session_id,\n",
        ")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "tsdsFCe00IGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Deploy Agent On Vertex AI\n",
        "\n",
        "# @markdown This section will deploy the agent on Vertex AI.\n",
        "# @markdown The supported regions for agent engine are listed [here](https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/overview#supported-regions).\n",
        "region_to_deploy_agent = \"us-central1\"  # @param {type:\"string\"}\n",
        "vertexai.init(\n",
        "    project=PROJECT_ID,\n",
        "    location=region_to_deploy_agent,\n",
        "    staging_bucket=BUCKET_URI,\n",
        ")\n",
        "\n",
        "remote_agent = agent_engines.create(\n",
        "    agent,\n",
        "    requirements=[\n",
        "        \"google-cloud-aiplatform[langchain,agent_engines]\",\n",
        "        \"cloudpickle==3.0.0\",\n",
        "        \"pydantic==2.10.6\",\n",
        "        \"requests\",\n",
        "        \"langchain-openai\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "# @markdown After the deployment, you can get the agent resource name with the format\n",
        "# @markdown as `projects/{PROJECT_ID}/locations/{REGION}/endpoints/{RESOURCE_ID}`.\n",
        "# @markdown The agent resource name will be used below.\n",
        "\n",
        "remote_agent_resource_name = remote_agent.resource_name\n",
        "print(\n",
        "    \"The deployed remote agent resource name is: \", remote_agent_resource_name\n",
        ")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KLQreX5X0v6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Test The Remote Agent\n",
        "\n",
        "# remote_agent_resource_name = f\"projects/{PROJECT}/locations/{REGION}/reasoningEngines/{RESOURCE_ID}\"\n",
        "\n",
        "remote_agent = agent_engines.get(remote_agent_resource_name)\n",
        "\n",
        "query = \"What is the weather in SF?\"  # @param {type:\"string\"}\n",
        "session_id = \"demo\"  # @param {type:\"string\"}\n",
        "\n",
        "test_agent(\n",
        "    running_agent=remote_agent,\n",
        "    query=query,\n",
        "    session_id=session_id,\n",
        ")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5XnQ8uBNWpOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAelDidov5AW"
      },
      "source": [
        "## Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SeZCFo5v7z-"
      },
      "outputs": [],
      "source": [
        "# @markdown  Delete the experiment resources to avoid unnecessary continuous\n",
        "# @markdown  charges that may incur.\n",
        "\n",
        "delete_endpoint = False # @param {type:\"boolean\"}\n",
        "delete_artifact_registry = False # @param {type:\"boolean\"}\n",
        "delete_tutorial_folder = False # @param {type:\"boolean\"}\n",
        "delete_agent_engine = False  # @param {type:\"boolean\"}\n",
        "\n",
        "if delete_endpoint:\n",
        "  # Undeploy model and delete endpoint.\n",
        "  endpoint.delete(force=True)\n",
        "\n",
        "if delete_artifact_registry:\n",
        "    ! gcloud artifacts repositories delete $REPOSITORY_NAME \\\n",
        "          --repository-format=docker \\\n",
        "          --location=$REGION \\\n",
        "          --project=$PROJECT_ID\n",
        "\n",
        "if delete_tutorial_folder:\n",
        "    import shutil\n",
        "    shutil.rmtree(TUTORIAL_DIR)\n",
        "\n",
        "if delete_agent_engine:\n",
        "    remote_agent.delete()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_integration_with_agent.ipynb",
      "toc_visible": true,
      "provenance": [],
      "collapsed_sections": [
        "tAelDidov5AW"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

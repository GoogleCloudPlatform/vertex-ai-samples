{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TirJ-SGQseby"
      },
      "source": [
        "# Vertex AI Model Garden Keras YOLOv8\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_keras_yolov8.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_keras_yolov8.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_keras_yolov8.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwGLvtIeECLK"
      },
      "source": [
        "**_NOTE_**: This notebook has been tested in the following environment:\n",
        "\n",
        "* Python version = 3.9\n",
        "\n",
        "You can open this notebook directly in Colab, or create [google managed](https://cloud.google.com/vertex-ai/docs/workbench/managed/create-instance) or [user managed](https://cloud.google.com/vertex-ai/docs/workbench/user-managed/create-new) Workbench instances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates how to use [Keras YOLOv8](https://keras.io/api/keras_cv/models/tasks/yolo_v8_detector/) in Vertex AI Model Garden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z9r_mBmDeYh"
      },
      "source": [
        "### Objective\n",
        "\n",
        "* Run local inferences for pretrained or customized models\n",
        "\n",
        "* Deploy pretrained or customized models in Google Cloud Vertex AI\n",
        "\n",
        "* Finetune models in Google Cloud Vertex AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEnkHABrDijz"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af989c0e437d"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "\n",
        "The dataset used for this tutorial is the Salads category of the [OpenImages dataset](https://www.tensorflow.org/datasets/catalog/open_images_v4) from [TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/overview). This dataset does not require any feature engineering. The version of the dataset you will use in this tutorial is stored in a public Cloud Storage bucket. The trained model predicts the bounding box locations and corresponding type of salad items in an image from a class of five items: Salad, Seafood, Tomato, Baked Goods, or Cheese."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z__i0w0lCAsW"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Install the following packages required to execute this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jvqs-ehKlaYh"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    # Configs for Colab notebooks.\n",
        "    ! pip3 install --upgrade --quiet google-cloud-aiplatform\n",
        "\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)\n",
        "\n",
        "    from google.colab import auth as google_auth\n",
        "\n",
        "    google_auth.authenticate_user()\n",
        "\n",
        "# Configs for all notebooks.\n",
        "! pip3 install --quiet keras-cv==0.6.1\n",
        "! pip3 install --quiet keras-core==0.1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEukV6uRk_S3"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "1. [Enable the Vertex AI API and Compute Engine API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component).\n",
        "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
        "\n",
        "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF1j6f9HApxa"
      },
      "source": [
        "### Set your project, region and buckets\n",
        "\n",
        "**If you don't know your project ID**, try the following:\n",
        "* Run `gcloud config list`.\n",
        "* Run `gcloud projects list`.\n",
        "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)\n",
        "\n",
        "You can change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations).\n",
        "\n",
        "You can create a storage bucket to store intermediate artifacts such as datasets, trained models etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjNCFxq0JxlA"
      },
      "outputs": [],
      "source": [
        "# The project and bucket are for experiments below.\n",
        "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "! gcloud config set project $PROJECT_ID\n",
        "\n",
        "# The form for BUCKET_URI is gs://<bucket-name>.\n",
        "BUCKET_URI = \"\"  # @param {type:\"string\"}\n",
        "REGION = \"us-central1\"  # @param {type: \"string\"}\n",
        "\n",
        "import os\n",
        "\n",
        "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
        "MODEL_BUCKET = os.path.join(STAGING_BUCKET, \"keras_yolov8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDjp76aaLZY9"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uv7-iDKLbO0"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZFPe_GezXg8"
      },
      "source": [
        "### Define constants and common functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcYUGwr-AJGY"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import io\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Union\n",
        "\n",
        "import keras_cv\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import yaml\n",
        "from google.protobuf import json_format\n",
        "from google.protobuf.struct_pb2 import Value\n",
        "from keras_cv import visualization\n",
        "from PIL import Image\n",
        "\n",
        "TRAIN_MACHINE_TYPE = \"n1-highmem-16\"\n",
        "TRAIN_ACCELERATOR_TYPE = \"NVIDIA_TESLA_V100\"\n",
        "TRAIN_NUM_GPU = 2\n",
        "TRAIN_CONTAINER_URI = (\n",
        "    \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/keras-yolov8-train\"\n",
        ")\n",
        "TRAINING_JOB_PREFIX = \"train_yolov8\"\n",
        "\n",
        "UPLOAD_JOB_PREFIX = \"upload_yolov8\"\n",
        "DEPLOY_JOB_PREFIX = \"deploy_yolov8\"\n",
        "SERVING_CONTAINER_URI = (\n",
        "    \"us-docker.pkg.dev/vertex-ai-restricted/prediction/tf_opt-gpu.2-12:latest\"\n",
        ")\n",
        "SERVING_ACCELERATOR_TYPE = \"NVIDIA_TESLA_T4\"\n",
        "SERVING_MACHINE_TYPE = \"n1-standard-4\"\n",
        "SERVING_CONTAINER_ARGS = [\"--allow_precompilation\", \"--allow_compression\"]\n",
        "\n",
        "RESOLUTION = 512\n",
        "\n",
        "\n",
        "def get_job_name_with_datetime(prefix: str):\n",
        "    \"\"\"Generates a job name with date time when triggering training or deployment\n",
        "    jobs in Vertex AI.\n",
        "    \"\"\"\n",
        "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
        "\n",
        "\n",
        "def load_img(path):\n",
        "    \"\"\"Reads image from path and return PIL.Image instance.\"\"\"\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    return Image.fromarray(np.uint8(img)).convert(\"RGB\")\n",
        "\n",
        "\n",
        "def decode_image(image_str_tensor: tf.string) -> tf.float32:\n",
        "    \"\"\"Converts and resizes image bytes to image tensor.\"\"\"\n",
        "    image = tf.io.decode_image(image_str_tensor, 3, expand_animations=False)\n",
        "    image = tf.image.resize(image, (RESOLUTION, RESOLUTION))\n",
        "    return image\n",
        "\n",
        "\n",
        "def get_label_map(label_map_yaml_filepath):\n",
        "    \"\"\"Returns class id to label mapping given a filepath to the label map.\"\"\"\n",
        "    with tf.io.gfile.GFile(label_map_yaml_filepath, \"rb\") as input_file:\n",
        "        label_map = yaml.safe_load(input_file.read())[\"label_map\"]\n",
        "    return label_map\n",
        "\n",
        "\n",
        "def get_prediction_instances(test_filepath, new_width=-1):\n",
        "    \"\"\"Generate instance from image path to pass to Vertex AI Endpoint for prediction.\"\"\"\n",
        "    if new_width <= 0:\n",
        "        with tf.io.gfile.GFile(test_filepath, \"rb\") as input_file:\n",
        "            encoded_string = base64.b64encode(input_file.read()).decode(\"utf-8\")\n",
        "    else:\n",
        "        img = load_img(test_filepath)\n",
        "        width, height = img.size\n",
        "        print(\"original input image size: \", width, \" , \", height)\n",
        "        new_height = int(height * new_width / width)\n",
        "        new_img = img.resize((new_width, new_height))\n",
        "        print(\"resized input image size: \", new_width, \" , \", new_height)\n",
        "        buffered = io.BytesIO()\n",
        "        new_img.save(buffered, format=\"JPEG\")\n",
        "        encoded_string = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "\n",
        "    instances = [\n",
        "        {\n",
        "            \"encoded_image\": {\"b64\": encoded_string},\n",
        "        }\n",
        "    ]\n",
        "    return instances\n",
        "\n",
        "\n",
        "def predict_custom_trained_model(\n",
        "    project: str,\n",
        "    endpoint_id: str,\n",
        "    instances: Union[Dict, List[Dict]],\n",
        "    location: str = \"us-central1\",\n",
        "):\n",
        "    # The AI Platform services require regional API endpoints.\n",
        "    client_options = {\"api_endpoint\": f\"{location}-aiplatform.googleapis.com\"}\n",
        "    # Initialize client that will be used to create and send requests.\n",
        "    # This client only needs to be created once, and can be reused for multiple requests.\n",
        "    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
        "    parameters_dict = {}\n",
        "    parameters = json_format.ParseDict(parameters_dict, Value())\n",
        "    endpoint = client.endpoint_path(\n",
        "        project=project, location=location, endpoint=endpoint_id\n",
        "    )\n",
        "    response = client.predict(\n",
        "        endpoint=endpoint, instances=instances, parameters=parameters\n",
        "    )\n",
        "    return response.predictions, response.deployed_model_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epo-RHXzcBBT"
      },
      "source": [
        "## Run local inferences with pretrained model\n",
        "\n",
        "This section shows how to run inferences locally with YOLOv8-M pretrained on PascalVOC 2012 object detection task, which consists of 20 classes.\n",
        "\n",
        "Load image from Cloud Storage and decode as Tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zsa9vnBHhvO"
      },
      "outputs": [],
      "source": [
        "test_filepath = \"\"  # @param {type:\"string\"}\n",
        "img_bytes = tf.io.read_file(test_filepath)\n",
        "image = tf.expand_dims(decode_image(img_bytes), axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wC-pSYR0jjU"
      },
      "source": [
        "Load model pretrained on PascalVOC 2012."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nvPEly_4Vm6"
      },
      "outputs": [],
      "source": [
        "model = keras_cv.models.YOLOV8Detector.from_preset(\n",
        "    \"yolo_v8_m_pascalvoc\",\n",
        "    bounding_box_format=\"xywh\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrijGrxT0lvC"
      },
      "source": [
        "Then run inferences and visualize results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65yEa4N0xcTS"
      },
      "outputs": [],
      "source": [
        "decoded = model.predict(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8-X3gA5xV_l"
      },
      "outputs": [],
      "source": [
        "# Classes in PascalVOC 2012 dataset.\n",
        "class_ids = [\n",
        "    \"Aeroplane\",\n",
        "    \"Bicycle\",\n",
        "    \"Bird\",\n",
        "    \"Boat\",\n",
        "    \"Bottle\",\n",
        "    \"Bus\",\n",
        "    \"Car\",\n",
        "    \"Cat\",\n",
        "    \"Chair\",\n",
        "    \"Cow\",\n",
        "    \"Dining Table\",\n",
        "    \"Dog\",\n",
        "    \"Horse\",\n",
        "    \"Motorbike\",\n",
        "    \"Person\",\n",
        "    \"Potted Plant\",\n",
        "    \"Sheep\",\n",
        "    \"Sofa\",\n",
        "    \"Train\",\n",
        "    \"Tvmonitor\",\n",
        "    \"Total\",\n",
        "]\n",
        "class_mapping = dict(zip(range(len(class_ids)), class_ids))\n",
        "\n",
        "visualization.plot_bounding_box_gallery(\n",
        "    image,\n",
        "    value_range=(0, 255),\n",
        "    rows=1,\n",
        "    cols=1,\n",
        "    y_pred=decoded,\n",
        "    scale=5,\n",
        "    font_scale=0.7,\n",
        "    bounding_box_format=\"xywh\",\n",
        "    class_mapping=class_mapping,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB_xY9ipr7ZU"
      },
      "source": [
        "## Finetune models\n",
        "This section shows how to finetune the Keras YOLOv8 model with training dockers and then deploy to Vertex AI Endpoint resource. The accepted dataset format is a CSV formatted as it would for [AutoML Image Object Detection](https://cloud.google.com/vertex-ai/docs/image-data/object-detection/prepare-data#input-files), without an `ML_USE` column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkNc7jyq1js1"
      },
      "outputs": [],
      "source": [
        "input_csv_path = \"gs://cloud-samples-data/vision/salads.csv\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee7Hzq8O5jgF"
      },
      "source": [
        "### Start training jobs\n",
        "The following code block shows some of the possible hyperparameters that can be set. The settings are for demonstration purposes only. Parameters such as `batch_size`, `learning_rate`, and `epochs` be overridden when used. `backbone` must be one of the following:\n",
        "* `yolo_v8_xs_backbone`\n",
        "* `yolo_v8_s_backbone`\n",
        "* `yolo_v8_m_backbone`\n",
        "* `yolo_v8_l_backbone`\n",
        "* `yolo_v8_xl_backbone`\n",
        "* `yolo_v8_xs_backbone_coco`\n",
        "* `yolo_v8_s_backbone_coco`\n",
        "* `yolo_v8_m_backbone_coco`\n",
        "* `yolo_v8_l_backbone_coco`\n",
        "* `yolo_v8_xl_backbone_coco`\n",
        "\n",
        "If looking for a preset with pretrained weights, choose one of `yolo_v8_xs_backbone_coco`, `yolo_v8_s_backbone_coco`, `yolo_v8_m_backbone_coco`, `yolo_v8_l_backbone_coco`, `yolo_v8_xl_backbone_coco`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riG_qUokg0XZ"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "epochs = 10\n",
        "learning_rate = 0.0005\n",
        "fpn_depth = 3\n",
        "confidence_threshold = 0.02\n",
        "iou_threshold = 0.3\n",
        "backbone = \"yolo_v8_xl_backbone_coco\"\n",
        "\n",
        "train_job_name = get_job_name_with_datetime(TRAINING_JOB_PREFIX)\n",
        "model_dir = os.path.join(MODEL_BUCKET, train_job_name)\n",
        "worker_pool_specs = [\n",
        "    {\n",
        "        \"machine_spec\": {\n",
        "            \"machine_type\": TRAIN_MACHINE_TYPE,\n",
        "            \"accelerator_type\": TRAIN_ACCELERATOR_TYPE,\n",
        "            \"accelerator_count\": TRAIN_NUM_GPU,\n",
        "        },\n",
        "        \"replica_count\": 1,\n",
        "        \"disk_spec\": {\n",
        "            \"boot_disk_type\": \"pd-ssd\",\n",
        "            \"boot_disk_size_gb\": 500,\n",
        "        },\n",
        "        \"container_spec\": {\n",
        "            \"image_uri\": TRAIN_CONTAINER_URI,\n",
        "            \"command\": [],\n",
        "            \"env\": [\n",
        "                {\n",
        "                    \"name\": \"RESOLUTION\",\n",
        "                    \"value\": f\"{RESOLUTION}\",\n",
        "                },\n",
        "            ],\n",
        "            \"args\": [\n",
        "                f\"--input_csv_path={input_csv_path}\",\n",
        "                f\"--output_model_dir={model_dir}\",\n",
        "                f\"--epochs={epochs}\",\n",
        "                f\"--pretrained_backbone={backbone}\",\n",
        "                f\"--fpn_depth={fpn_depth}\",\n",
        "                f\"--learning_rate={learning_rate}\",\n",
        "                f\"--confidence_threshold={confidence_threshold}\",\n",
        "                f\"--iou_threshold={iou_threshold}\",\n",
        "            ],\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "train_job = aiplatform.CustomJob(\n",
        "    display_name=train_job_name,\n",
        "    project=PROJECT_ID,\n",
        "    worker_pool_specs=worker_pool_specs,\n",
        "    staging_bucket=STAGING_BUCKET,\n",
        ")\n",
        "\n",
        "train_job.run()\n",
        "\n",
        "print(\"The trained model is saved in: \", model_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KBJ0ySVYX47"
      },
      "source": [
        "### Prediction\n",
        "This section shows how to deploy and make online predictions with the model.\n",
        "\n",
        "1. Upload and deploy models\n",
        "2. Run predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6rUSSKmYZJ6"
      },
      "outputs": [],
      "source": [
        "upload_job_name = get_job_name_with_datetime(UPLOAD_JOB_PREFIX)\n",
        "\n",
        "serving_env = {\n",
        "    \"MODEL_ID\": \"keras-yolov8\",\n",
        "}\n",
        "\n",
        "model = aiplatform.Model.upload(\n",
        "    display_name=upload_job_name,\n",
        "    artifact_uri=model_dir,\n",
        "    serving_container_image_uri=SERVING_CONTAINER_URI,\n",
        "    serving_container_args=SERVING_CONTAINER_ARGS,\n",
        "    serving_container_environment_variables=serving_env,\n",
        ")\n",
        "\n",
        "print(\"The uploaded model name is: \", upload_job_name)\n",
        "\n",
        "deploy_model_name = get_job_name_with_datetime(DEPLOY_JOB_PREFIX)\n",
        "\n",
        "endpoint = model.deploy(\n",
        "    deployed_model_display_name=deploy_model_name,\n",
        "    machine_type=SERVING_MACHINE_TYPE,\n",
        "    traffic_split={\"0\": 100},\n",
        "    accelerator_type=SERVING_ACCELERATOR_TYPE,\n",
        "    accelerator_count=1,\n",
        "    min_replica_count=1,\n",
        "    max_replica_count=1,\n",
        ")\n",
        "print(\"The deployed job name is: \", deploy_model_name)\n",
        "\n",
        "endpoint_id = endpoint.name\n",
        "print(\"endpoint id is: \", endpoint_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a879effaf402"
      },
      "source": [
        "Load image from Cloud Storage, resize, and encode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDznWEMmbwj4"
      },
      "outputs": [],
      "source": [
        "test_filepath = \"gs://cloud-ml-data/img/openimage/1302/4677521502_6f2767039c_o.jpg\"  # @param {type:\"string\"}\n",
        "image_bytes = tf.io.read_file(test_filepath)\n",
        "image_resized = tf.expand_dims(decode_image(image_bytes), axis=0)\n",
        "\n",
        "instances = get_prediction_instances(test_filepath, new_width=640)\n",
        "\n",
        "predictions, _ = predict_custom_trained_model(\n",
        "    project=PROJECT_ID, location=REGION, endpoint_id=endpoint_id, instances=instances\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14e889492871"
      },
      "source": [
        "Run online predictions using the endpoint and visualize the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bx1cW0IdXqp"
      },
      "outputs": [],
      "source": [
        "predictions_dict = {\n",
        "    \"boxes\": tf.expand_dims(predictions[0][\"boxes\"], axis=0),\n",
        "    \"classes\": tf.expand_dims(predictions[0][\"classes\"], axis=0),\n",
        "    \"confidence\": tf.expand_dims(predictions[0][\"confidence\"], axis=0),\n",
        "    \"num_detections\": predictions[0][\"num_detections\"],\n",
        "}\n",
        "\n",
        "label_map = get_label_map(os.path.join(model_dir, \"label_map.yaml\"))\n",
        "\n",
        "visualization.plot_bounding_box_gallery(\n",
        "    image_resized,\n",
        "    value_range=(0, 255),\n",
        "    rows=1,\n",
        "    cols=1,\n",
        "    y_pred=predictions_dict,\n",
        "    scale=5,\n",
        "    font_scale=0.7,\n",
        "    bounding_box_format=\"xywh\",\n",
        "    class_mapping=label_map,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkH2nrpdp4sp"
      },
      "source": [
        "### Clean up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ax6vQVZhp9pR"
      },
      "outputs": [],
      "source": [
        "# Deletes custom train jobs.\n",
        "train_job.delete()\n",
        "# Undeploys models and deletes endpoints.\n",
        "endpoint.delete(force=True)\n",
        "# Deletes models.\n",
        "model.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dijQDiZWegt"
      },
      "source": [
        "## References\n",
        "\n",
        "- [Efficient Object Detection with YOLOV8 and KerasCV](https://keras.io/examples/vision/yolov8/)\n",
        "- [Keras YOLOv8 API Documentation](https://keras.io/api/keras_cv/models/tasks/yolo_v8_detector/)\n",
        "- [Keras YOLOv8 Backbones](https://keras.io/api/keras_cv/models/backbones/yolo_v8/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_keras_yolov8.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-e1HpvsDh34Q"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5o1Ggr5h34U"
      },
      "source": [
        "# Vertex AI Model Garden - ZipNeRF (Pytorch) Notebook\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_zipnerf.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_zipnerf.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_pytorch_zipnerf.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "Open in Vertex AI Workbench\n",
        "    </a> (A Python-3 CPU notebook is recommended)\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-SERmqUh34V"
      },
      "source": [
        "**_NOTE_**: This notebook has been tested in the following environment:\n",
        "\n",
        "* Python version = 3.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6QmW0Doh34W"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates a [pytorch implementation](https://github.com/SuLvXiangXin/zipnerf-pytorch) of [Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields](https://jonbarron.info/zipnerf/) for rendering Neural Radiance Fields (NeRFs) more efficiently. It is primarily aimed at addressing some of the limitations of traditional NeRF techniques, which, while powerful for creating detailed 3D models from 2D images, can be computationally intensive and slow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkSMThcKh34W"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to:\n",
        "\n",
        "- Use [COLMAP](https://colmap.github.io/) to perform Structure from Motion (SfM), a technique that estimates the three-dimensional structure of a scene from a series of two-dimensional images.\n",
        "- Calibrate, train and render NERF scenes using [Vertex AI custom jobs](https://cloud.google.com/vertex-ai/docs/samples/aiplatform-create-custom-job-sample).\n",
        "- Render a video along a custom camera path using a series of keyframe photos.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services and resources:\n",
        "\n",
        "- Vertex AI Training\n",
        "- Vertex AI Custom Job"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myi4N60Xh34W"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beDl8cg1RNmb"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0WVDnCSh34W"
      },
      "source": [
        "### Installation\n",
        "\n",
        "Install the following packages required to execute this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SarHdTe6h34X"
      },
      "outputs": [],
      "source": [
        "! pip install --upgrade pip\n",
        "! pip install google-cloud-aiplatform==1.38.1\n",
        "! pip install google-cloud-storage==2.14.0\n",
        "! pip install wget==3.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqAlUIpTh34X"
      },
      "source": [
        "### Before you begin\n",
        "\n",
        "**NOTE**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXesIn5Ph34X"
      },
      "source": [
        "### Colab only\n",
        "Run the following commands for Colab and skip this section if you are using Workbench."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxEFcySJh34Y"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    ! pip3 install --upgrade google-cloud-aiplatform\n",
        "    from google.colab import auth as google_auth\n",
        "\n",
        "    google_auth.authenticate_user()\n",
        "    # Install gdown for downloading example training images.\n",
        "    ! pip3 install gdown\n",
        "\n",
        "    # Restart the notebook kernel after installs.\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKwm8ghIh34Y"
      },
      "source": [
        "### Setup Google Cloud project\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "1. [Enable the Vertex AI API and Compute Engine API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component).\n",
        "\n",
        "1. [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs.\n",
        "\n",
        "1. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuydf63fh34Z"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below.\n",
        "\n",
        "**1. Vertex AI Workbench**\n",
        "* Do nothing as you are already authenticated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfXC5K20WuSw"
      },
      "source": [
        "**2. Local JupyterLab instance, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGAUWRyvWzjP"
      },
      "outputs": [],
      "source": [
        "# ! gcloud auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G5q1KaUh34Y"
      },
      "source": [
        "### Set your project parameters\n",
        "\n",
        "**If you don't know your project ID**, try the following:\n",
        "* Run `gcloud config list`.\n",
        "* Run `gcloud projects list`.\n",
        "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)\n",
        "\n",
        "You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations).\n",
        "\n",
        "Create a storage bucket to store intermediate artifacts such as datasets. Recommended pattern: `gs://cloudnerf-{PROJECT_ID}-unique`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qayv5ifRh34Y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
        "REGION = \"us-central1\"  # @param {type: \"string\"}\n",
        "\n",
        "# Enter the name of the bucket without gs://\n",
        "BUCKET_NAME = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "# Set the project id.\n",
        "! gcloud config set project {PROJECT_ID}\n",
        "\n",
        "# Create the bucket if it doesn't already exist.\n",
        "BUCKET_URI = os.path.join(\"gs://\", BUCKET_NAME)\n",
        "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSDay9zNh34a"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZseCTgCBh34a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "staging_bucket = os.path.join(BUCKET_URI, \"zipnerf_staging\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=staging_bucket)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUVTFBqgh34a"
      },
      "source": [
        "### Define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOqMkJTeh34a"
      },
      "outputs": [],
      "source": [
        "# The pre-built calibration docker image.\n",
        "CALIBRATION_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-cloudnerf-calibrate:latest\"\n",
        "# The pre-built training docker image.\n",
        "TRAINING_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-cloudnerf-train:latest\"\n",
        "# The pre-built rendering docker image.\n",
        "RENDERING_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-cloudnerf-render:latest\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWs4aw0nh34a"
      },
      "source": [
        "### Define common functions\n",
        "\n",
        "This section defines functions for:\n",
        "\n",
        "- Custom job naming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cq9oIwnDh34a"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "from datetime import datetime\n",
        "from typing import Any, List\n",
        "\n",
        "IMAGE_EXTENSIONS = (\".png\", \".jpg\", \".jpeg\", \".gif\", \".bmp\")\n",
        "GCS_API_ENDPOINT = \"https://storage.cloud.google.com/\"\n",
        "\n",
        "\n",
        "def get_job_name_with_datetime(prefix: str) -> str:\n",
        "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
        "    jobs in Vertex AI.\n",
        "    \"\"\"\n",
        "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
        "\n",
        "\n",
        "def get_mp4_video_link(mp4_rendering_path: str) -> str:\n",
        "    # Define the gsutil command.\n",
        "    command = f\"gsutil ls {mp4_rendering_path}\"\n",
        "\n",
        "    # Run the command and capture the output.\n",
        "    try:\n",
        "        result = subprocess.check_output(command, shell=True, text=True)\n",
        "        # Split the result by newlines to get a list of files.\n",
        "        file_list = result.strip().split(\"\\n\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        file_list = []\n",
        "    mp4_video_link = file_list[0].replace(\"gs://\", GCS_API_ENDPOINT)\n",
        "    return mp4_video_link\n",
        "\n",
        "\n",
        "def write_keyframe_list_to_gcs(\n",
        "    bucket_path: str, output_gcs_file: str, max_files: int = 10\n",
        ") -> List[Any]:\n",
        "    # Get the list of files in the GCS bucket.\n",
        "    cmd = f\"gsutil ls {bucket_path}\"\n",
        "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "\n",
        "    if result.returncode != 0:\n",
        "        print(\"Error listing GCS bucket:\", result.stderr)\n",
        "        return []\n",
        "\n",
        "    # Filter for image files and extract file names.\n",
        "    files = result.stdout.splitlines()\n",
        "    image_files = [\n",
        "        os.path.basename(f) for f in files if f.lower().endswith(IMAGE_EXTENSIONS)\n",
        "    ]\n",
        "\n",
        "    output_file = \"out.txt\"\n",
        "    with open(output_file, \"w\") as file:\n",
        "        for name in image_files[:max_files]:\n",
        "            file.write(name + \"\\n\")\n",
        "\n",
        "    cmd = f\"gsutil cp {output_file} {output_gcs_file}\"\n",
        "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "\n",
        "    if result.returncode != 0:\n",
        "        print(\"Error listing GCS bucket:\", result.stderr)\n",
        "        return []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Umz2XgIeh34a"
      },
      "source": [
        "## Prepare dataset\n",
        "It is necessary to prepare the dataset and store it on Cloud Storage. The following example illustrates the process for the bicycle scene in the [mipnerf360](https://jonbarron.info/mipnerf360/) dataset. For the sake of convenience, each scene in the mipnerf360 has been provided as a separate dataset with its own unique download link. Mip-NeRF 360 dataset contains the following 7 scenes:\n",
        "\n",
        "- [`bicycle`](http://storage.googleapis.com/gresearch/refraw360/bicycle.zip)\n",
        "- [`bonsai`](http://storage.googleapis.com/gresearch/refraw360/bonsai.zip)\n",
        "- [`counter`](http://storage.googleapis.com/gresearch/refraw360/counter.zip)\n",
        "- [`garden`](http://storage.googleapis.com/gresearch/refraw360/garden.zip)\n",
        "- [`kitchen`](http://storage.googleapis.com/gresearch/refraw360/kitchen.zip)\n",
        "- [`room`](http://storage.googleapis.com/gresearch/refraw360/room.zip)\n",
        "- [`stump`](http://storage.googleapis.com/gresearch/refraw360/stump.zip)\n",
        "\n",
        "Each scene comes preprocessed with COLMAP information so the calibration step in the following section is optional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBhvKerXh34a"
      },
      "outputs": [],
      "source": [
        "local_mipnerf_data_directory = \"./mipnerf360_dataset\"  # @param {type:\"string\"}\n",
        "MIPNERF_DATA_GCS_PATH = os.path.join(BUCKET_URI, \"mipnerf360_dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThoLj-HDh34a"
      },
      "outputs": [],
      "source": [
        "# Download the bicycle scene data to a local directory.\n",
        "! rm -rf $local_mipnerf_data_directory\n",
        "! mkdir -p $local_mipnerf_data_directory\n",
        "! wget -P $local_mipnerf_data_directory http://storage.googleapis.com/gresearch/refraw360/bicycle.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hP8IsiqCh34a"
      },
      "outputs": [],
      "source": [
        "# Unzip the mipnerf360 dataset.\n",
        "! unzip $local_mipnerf_data_directory/bicycle.zip -d $local_mipnerf_data_directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZ4C5kuyh34a"
      },
      "outputs": [],
      "source": [
        "# Move mipnerf360 data from local directory to Cloud Storage.\n",
        "# This step takes a few minutes to finish.\n",
        "! gsutil -m cp -R $local_mipnerf_data_directory/* $MIPNERF_DATA_GCS_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdWHe-RmSEn6"
      },
      "source": [
        "## NERF pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtcVGao6h34f"
      },
      "source": [
        "### Camera pose estimation\n",
        "As mentioned above, all the scenes in the Mip-NeRF 360 dataset have been preprocessed with colmap information. However, in order to demonstrate how to run the pipeline end-to-end on your own data, we will use the `bicycle` scene to estimate the camera poses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFkzzGq0h34f"
      },
      "outputs": [],
      "source": [
        "# Folder containing all the images of the bicycle scene.\n",
        "INPUT_IMAGES_FOLDER = f\"{MIPNERF_DATA_GCS_PATH}/bicycle/images\"  # @param {type:\"string\"}\n",
        "\n",
        "# Folder for storing experiment outputs for calibration, training and rendering.\n",
        "OUTPUT_FOLDER = f\"{MIPNERF_DATA_GCS_PATH}/exp/bicycle\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-rEdcdah34f"
      },
      "source": [
        "Once data and experiment paths have been configured, run the custom job below.\n",
        "\n",
        "The following parameters are required:\n",
        "\n",
        "* `use_gpu`: Whether to use GPU or not.\n",
        "* `gcs_dataset_path`: Path to image folder in GCS dataset.\n",
        "* `gcs_experiment_path`: GCS path for storing experiment outputs.\n",
        "* `camera`: Type of camera used. `OPENCV` for perspective, `OPENCV_FISHEYE` for fisheye.\n",
        "\n",
        "The custom job will run on the images in the `gcs_dataset_path` folder and store the colmap outputs in the `gcs_experiment_path/data` folder.\n",
        "\n",
        "On the scenes in this current dataset, this step takes about 30 minutes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1evctm2h34g"
      },
      "outputs": [],
      "source": [
        "# This job will run colmap camera pose estimation.\n",
        "data_calibration_job_name = get_job_name_with_datetime(\"colmap\")\n",
        "\n",
        "# Worker pool spec.\n",
        "machine_type = \"n1-highmem-64\"\n",
        "num_nodes = 1\n",
        "gpu_type = \"NVIDIA_TESLA_V100\"\n",
        "num_gpus = 8\n",
        "worker_pool_specs = [\n",
        "    {\n",
        "        \"machine_spec\": {\n",
        "            \"machine_type\": machine_type,\n",
        "            \"accelerator_type\": gpu_type,\n",
        "            \"accelerator_count\": num_gpus,\n",
        "        },\n",
        "        \"replica_count\": num_nodes,\n",
        "        \"container_spec\": {\n",
        "            \"image_uri\": CALIBRATION_DOCKER_URI,\n",
        "            \"args\": [\n",
        "                \"-use_gpu\",\n",
        "                \"1\",\n",
        "                \"-gcs_dataset_path\",\n",
        "                INPUT_IMAGES_FOLDER,\n",
        "                \"-gcs_experiment_path\",\n",
        "                OUTPUT_FOLDER,\n",
        "                \"-camera\",\n",
        "                \"OPENCV\",\n",
        "            ],\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "data_calibration_custom_job = aiplatform.CustomJob(\n",
        "    display_name=data_calibration_job_name,\n",
        "    project=PROJECT_ID,\n",
        "    worker_pool_specs=worker_pool_specs,\n",
        "    staging_bucket=staging_bucket,\n",
        ")\n",
        "\n",
        "data_calibration_custom_job.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3v-y3Sjh34g"
      },
      "source": [
        "### Training the ZipNeRF model\n",
        "Once the Colmap pose calibration is completed, we can run training.\n",
        "\n",
        "The following parameters are required:\n",
        "\n",
        "* `gcs_experiment_path`: GCS path for loading processed dataset and storing experiment outputs.\n",
        "* `gin_config_file`: Configuration file for ZipNeRF network. Currents options are:\n",
        "  * configs/360.gin: Configuration for 360 reconstruction.\n",
        "  * configs/360_glo.gin: Configuration for 360 reconstruction with [generative latent optimization]\n",
        "* `factor`: A factor of the downsampled images in the preprocessing step that affects the resolution or detail level of the training pixel ground truth and rendered images. A factor of 2 is recommended for indoor scenes and a factor of 4 for outdoor scenes. **The factor used in training must be the same for rendering.**\n",
        "\n",
        "The custom job will run on the images in the `gcs_experiment_path/data` colmap dataset and outputs in the checkpoints in `gcs_experiment_path/checkpoints` folder.\n",
        "\n",
        "Depending on the configuration, this step could take up to 3 hours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7ZWhSpjh34g"
      },
      "outputs": [],
      "source": [
        "# This job will run zipnerf training.\n",
        "\n",
        "# This is the nerf training job name. You will use it to load the checkpoints\n",
        "# in the rendering job for the current run.\n",
        "nerf_training_job_name = get_job_name_with_datetime(\"nerf_training\")\n",
        "\n",
        "GIN_CONFIG_FILE = \"configs/360.gin\"  # @param {type:\"string\"}\n",
        "FACTOR = \"4\"  # @param {type:\"string\"}\n",
        "\n",
        "# Worker pool spec.\n",
        "machine_type = \"n1-highmem-64\"\n",
        "num_nodes = 1\n",
        "gpu_type = \"NVIDIA_TESLA_V100\"\n",
        "num_gpus = 8\n",
        "worker_pool_specs = [\n",
        "    {\n",
        "        \"machine_spec\": {\n",
        "            \"machine_type\": machine_type,\n",
        "            \"accelerator_type\": gpu_type,\n",
        "            \"accelerator_count\": num_gpus,\n",
        "        },\n",
        "        \"replica_count\": num_nodes,\n",
        "        \"container_spec\": {\n",
        "            \"image_uri\": TRAINING_DOCKER_URI,\n",
        "            \"args\": [\n",
        "                \"-training_job_name\",\n",
        "                nerf_training_job_name,\n",
        "                \"-gcs_experiment_path\",\n",
        "                OUTPUT_FOLDER,\n",
        "                \"-gin_config_file\",\n",
        "                GIN_CONFIG_FILE,\n",
        "                \"-factor\",\n",
        "                FACTOR,\n",
        "            ],\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "nerf_training_custom_job = aiplatform.CustomJob(\n",
        "    display_name=nerf_training_job_name,\n",
        "    project=PROJECT_ID,\n",
        "    worker_pool_specs=worker_pool_specs,\n",
        "    staging_bucket=staging_bucket,\n",
        ")\n",
        "\n",
        "nerf_training_custom_job.run(enable_web_access=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdw6bih8h34g"
      },
      "source": [
        "### Rendering the ZipNeRF model (360)\n",
        "Once the Colmap pose calibration is completed, we can run training.\n",
        "\n",
        "The following parameters are required:\n",
        "\n",
        "* `gcs_experiment_path`: GCS path for loading processed dataset and storing experiment outputs.\n",
        "* `gin_config_file`: Configuration file for ZipNeRF network. Currents options are:\n",
        "  * configs/360.gin: Configuration for 360 reconstruction.\n",
        "  * configs/360_glo.gin: Configuration for 360 reconstruction with [generative latent optimization](https://www.researchgate.net/publication/318527851_Optimizing_the_Latent_Space_of_Generative_Networks).\n",
        "* `render_video_fps`: Frame rate of rendered video.\n",
        "* `render_path_frames`: Number of frames to render for a path.\n",
        "* `factor`: A factor of the downsampled images in the preprocessing step that affects the resolution or detail level of the training pixel ground truth and rendered images. A factor of 2 is recommended for indoor scenes and a factor of 4 for outdoor scenes. **The factor used in training must be the same for rendering.**\n",
        "\n",
        "The custom job will run on the images in the `gcs_experiment_path/data` colmap dataset and outputs in the checkpoints in `gcs_experiment_path/checkpoints` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lufsDWzh34g"
      },
      "outputs": [],
      "source": [
        "# This job will run zipnerf rendering.\n",
        "nerf_rendering_job_name = get_job_name_with_datetime(\"nerf_rendering\")\n",
        "\n",
        "RENDER_PATH_FRAMES = \"120\"  # @param {type:\"string\"}\n",
        "RENDER_VIDEO_FPS = \"30\"  # @param {type:\"string\"}\n",
        "\n",
        "# Worker pool spec.\n",
        "machine_type = \"n1-highmem-64\"\n",
        "num_nodes = 1\n",
        "gpu_type = \"NVIDIA_TESLA_V100\"\n",
        "num_gpus = 8\n",
        "worker_pool_specs = [\n",
        "    {\n",
        "        \"machine_spec\": {\n",
        "            \"machine_type\": machine_type,\n",
        "            \"accelerator_type\": gpu_type,\n",
        "            \"accelerator_count\": num_gpus,\n",
        "        },\n",
        "        \"replica_count\": num_nodes,\n",
        "        \"container_spec\": {\n",
        "            \"image_uri\": RENDERING_DOCKER_URI,\n",
        "            \"args\": [\n",
        "                \"-rendering_job_name\",\n",
        "                nerf_rendering_job_name,\n",
        "                \"-training_job_name\",\n",
        "                nerf_training_job_name,\n",
        "                \"-gcs_experiment_path\",\n",
        "                OUTPUT_FOLDER,\n",
        "                \"-gin_config_file\",\n",
        "                GIN_CONFIG_FILE,\n",
        "                \"-render_video_fps\",\n",
        "                RENDER_VIDEO_FPS,\n",
        "                \"-render_path_frames\",\n",
        "                RENDER_PATH_FRAMES,\n",
        "                \"-factor\",\n",
        "                FACTOR,\n",
        "            ],\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "nerf_rendering_custom_job = aiplatform.CustomJob(\n",
        "    display_name=nerf_rendering_job_name,\n",
        "    project=PROJECT_ID,\n",
        "    worker_pool_specs=worker_pool_specs,\n",
        "    staging_bucket=staging_bucket,\n",
        ")\n",
        "\n",
        "nerf_rendering_custom_job.run(enable_web_access=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88Zv38QBh34g"
      },
      "source": [
        "#### Show rendered video from GCS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifhDb9xeh34g"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Video\n",
        "\n",
        "MP4_RENDERING_PATH = f\"{OUTPUT_FOLDER}/render/{nerf_rendering_job_name}/*color.mp4\"\n",
        "mp4_video_link = get_mp4_video_link(MP4_RENDERING_PATH)\n",
        "Video(mp4_video_link)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyWwIU9JbS7x"
      },
      "source": [
        "### Rendering the ZipNeRF model (custom camera trajectory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DY5YPVqDh34g"
      },
      "source": [
        "#### Create keyframe file list for rendering custom camera trajectories.\n",
        "\n",
        "To create a custom camera trajectory in a Neural Radiance Field (NeRF) model using images from the same dataset used for training, you can generate a keyframe file list where each keyframe corresponds to the name of an image file stored in a Google Cloud Storage (GCS) bucket. This section will guide you through creating this keyframe file list.\n",
        "\n",
        "#### Step 1: Identifying keyframe images\n",
        "First, identify the images within your dataset that you want to use as keyframes. These images should ideally represent the significant views or angles that you want your camera trajectory to include.\n",
        "\n",
        "#### Step 2: Creating a list of image file names\n",
        "Access Your GCS Bucket: Navigate to your GCS bucket where the dataset is stored.\n",
        "\n",
        "Select Image Files: Choose the specific image files that you want to use as keyframes. Remember, these should be files used in training the NeRF model, as they will have corresponding camera parameters already defined.\n",
        "\n",
        "Compile File Names: Create a list of the file names (not the paths) of these selected images. Ensure that each file name is on a separate line. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uAT-uQ3h34g"
      },
      "outputs": [],
      "source": [
        "# This job will run zipnerf rendering.\n",
        "nerf_custom_rendering_job_name = get_job_name_with_datetime(\"nerf_custom_rendering\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gai5cc-bh34g"
      },
      "outputs": [],
      "source": [
        "# Example usage.\n",
        "KEYFRAME_IMAGE_FILELIST = (\n",
        "    f\"{OUTPUT_FOLDER}/keyframe_list_{nerf_custom_rendering_job_name}.txt\"\n",
        ")\n",
        "max_files = 30  # Set this to the number of files you want\n",
        "write_keyframe_list_to_gcs(\n",
        "    INPUT_IMAGES_FOLDER, KEYFRAME_IMAGE_FILELIST, max_files=max_files\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQ6k_LdKh34h"
      },
      "source": [
        "#### Run rendering\n",
        "\n",
        "Once the training is completed, we can run rendering.\n",
        "\n",
        "The following parameters are required:\n",
        "\n",
        "* `gcs_experiment_path`: GCS path for loading processed dataset and storing experiment outputs.\n",
        "* `gin_config_file`: Configuration file for ZipNeRF network. Currents options are:\n",
        "  * configs/360.gin: Configuration for 360 reconstruction.\n",
        "  * configs/360_glo.gin: Configuration for 360 reconstruction with [generative latent optimization](https://www.researchgate.net/publication/318527851_Optimizing_the_Latent_Space_of_Generative_Networks).\n",
        "* `render_video_fps`: Frame rate of rendered video.\n",
        "* `factor`: A factor of the downsampled images in the preprocessing step that affects the resolution or detail level of the training pixel ground truth and rendered images. A factor of 2 is recommended for indoor scenes and a factor of 4 for outdoor scenes. **The factor used in training must be the same for rendering.**\n",
        "* `keyframe_image_list`: List of image filename, one per line, for rendering custom camera path.\n",
        "\n",
        "With keyframes, an interpolated path is generated. This path represents a smoothly contoured spline that interconnects the specified keyframe camera poses. The process utilizes a configuration variable, `render_spline_n_interp`, which is preset to a default value of 30. As a result, the finalized interpolated path comprises a total of `render_spline_n_interp` * (n - 1) poses. In the specific scenario under discussion, the config.render_spline_n_interp is configured to 30. **With an input of 30 keyframes, the calculation yields a total of 30 * 29, amounting to 870 poses**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hi0oIbyZh34h"
      },
      "outputs": [],
      "source": [
        "# This job will run zipnerf rendering.\n",
        "# Worker pool spec.\n",
        "machine_type = \"n1-highmem-64\"\n",
        "num_nodes = 1\n",
        "gpu_type = \"NVIDIA_TESLA_V100\"\n",
        "num_gpus = 8\n",
        "worker_pool_specs = [\n",
        "    {\n",
        "        \"machine_spec\": {\n",
        "            \"machine_type\": machine_type,\n",
        "            \"accelerator_type\": gpu_type,\n",
        "            \"accelerator_count\": num_gpus,\n",
        "        },\n",
        "        \"replica_count\": num_nodes,\n",
        "        \"container_spec\": {\n",
        "            \"image_uri\": RENDERING_DOCKER_URI,\n",
        "            \"args\": [\n",
        "                \"-rendering_job_name\",\n",
        "                nerf_custom_rendering_job_name,\n",
        "                \"-training_job_name\",\n",
        "                nerf_training_job_name,\n",
        "                \"-gcs_experiment_path\",\n",
        "                OUTPUT_FOLDER,\n",
        "                \"-gin_config_file\",\n",
        "                GIN_CONFIG_FILE,\n",
        "                \"-render_video_fps\",\n",
        "                RENDER_VIDEO_FPS,\n",
        "                \"-factor\",\n",
        "                FACTOR,\n",
        "                \"-gcs_keyframes_file\",\n",
        "                KEYFRAME_IMAGE_FILELIST,\n",
        "                \"-render_path_frames\",\n",
        "                RENDER_PATH_FRAMES,\n",
        "            ],\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "nerf_custom_rendering_custom_job = aiplatform.CustomJob(\n",
        "    display_name=nerf_custom_rendering_job_name,\n",
        "    project=PROJECT_ID,\n",
        "    worker_pool_specs=worker_pool_specs,\n",
        "    staging_bucket=staging_bucket,\n",
        ")\n",
        "\n",
        "nerf_custom_rendering_custom_job.run(enable_web_access=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQHDoqn1h34h"
      },
      "source": [
        "#### Show rendered video from GCS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeup-oLAh34h"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Video\n",
        "\n",
        "MP4_RENDERING_PATH = (\n",
        "    f\"{OUTPUT_FOLDER}/render/{nerf_custom_rendering_job_name}/*color.mp4\"\n",
        ")\n",
        "mp4_video_link = get_mp4_video_link(MP4_RENDERING_PATH)\n",
        "Video(mp4_video_link)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsUREZ6jNmDP"
      },
      "source": [
        "## Clean up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9K-sK6INmDP"
      },
      "outputs": [],
      "source": [
        "# Delete pose estimation, training and rendering custom jobs.\n",
        "if data_calibration_custom_job.list(\n",
        "    filter=f'display_name=\"{data_calibration_job_name}\"'\n",
        "):\n",
        "    data_calibration_custom_job.delete()\n",
        "if nerf_training_custom_job.list(filter=f'display_name=\"{nerf_training_job_name}\"'):\n",
        "    nerf_training_custom_job.delete()\n",
        "if nerf_rendering_custom_job.list(filter=f'display_name=\"{nerf_rendering_job_name}\"'):\n",
        "    nerf_rendering_custom_job.delete()\n",
        "if nerf_custom_rendering_custom_job.list(\n",
        "    filter=f'display_name=\"{nerf_custom_rendering_job_name}\"'\n",
        "):\n",
        "    nerf_custom_rendering_custom_job.delete()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_pytorch_zipnerf.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

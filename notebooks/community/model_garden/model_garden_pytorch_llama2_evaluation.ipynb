{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d9bbf86da5e"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Vertex AI Model Garden - LLaMA 2 (Evaluation)\n",
        "\n",
        "\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\n",
        "  \u003ctd style=\"text-align: center\"\u003e\n",
        "    \u003ca href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_pytorch_llama2_evaluation.ipynb\"\u003e\n",
        "      \u003cimg alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"\u003e\u003cbr\u003e Run in Colab Enterprise\n",
        "    \u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd style=\"text-align: center\"\u003e\n",
        "    \u003ca href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_llama2_evaluation.ipynb\"\u003e\n",
        "      \u003cimg alt=\"GitHub logo\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" width=\"32px\"\u003e\u003cbr\u003e View on GitHub\n",
        "    \u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates downloading prebuilt [LLaMA 2 models](https://huggingface.co/meta-llama), evaluating LLaMA 2 models with popular benchmark datasets through Vertex CustomJobs using [EleutherAI's evaluation harness](https://github.com/EleutherAI/lm-evaluation-harness) and running\n",
        "[automatic side-by-side evaluation](https://cloud.google.com/vertex-ai/docs/generative-ai/models/side-by-side-eval).\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Download prebuilt LLaMA 2 models\n",
        "- Evaluate the LLaMA 2 models on any of the benchmark datasets\n",
        "- Run bulk inference job\n",
        "- Run automatic side by side (autoSxS) evaluation job\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "264c07757582"
      },
      "source": [
        "## Run the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "855d6b96f291"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown 2. [Optional] [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing\n",
        "# @markdown experiment outputs. Set the BUCKET_URI for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`)\n",
        "# @markdown should be located in the same region as where the notebook was launched. Note that a multi-region bucket (eg. \"us\") is\n",
        "# @markdown not considered a match for a single region covered by the multi-region range (eg. \"us-central1\").\n",
        "# @markdown If not set, a unique GCS bucket will be created instead.\n",
        "\n",
        "# Import the necessary packages\n",
        "! pip3 install --upgrade --quiet google-cloud-aiplatform google-cloud-pipeline-components\n",
        "\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "from typing import Dict\n",
        "\n",
        "import pandas as pd\n",
        "from google.cloud import aiplatform, storage\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
        "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
        "\n",
        "# Cloud Storage bucket for storing the experiment artifacts.\n",
        "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
        "# prefer using your own GCS bucket, change the value yourself below.\n",
        "now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
        "\n",
        "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
        "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}\"\n",
        "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
        "else:\n",
        "    assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
        "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "    shell_output = ! gsutil ls -Lb {BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
        "    bucket_region = shell_output[0].strip().lower()\n",
        "    if bucket_region != REGION:\n",
        "        raise ValueError(\n",
        "            \"Bucket region %s is different from notebook region %s\"\n",
        "            % (bucket_region, REGION)\n",
        "        )\n",
        "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
        "\n",
        "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
        "EXPERIMENT_BUCKET = os.path.join(BUCKET_URI, \"llama2\")\n",
        "BASE_MODEL_BUCKET = os.path.join(EXPERIMENT_BUCKET, \"base_model\")\n",
        "MODEL_BUCKET = os.path.join(EXPERIMENT_BUCKET, \"model\")\n",
        "OUTPUT_BUCKET_A = os.path.join(EXPERIMENT_BUCKET, \"output_a\")\n",
        "OUTPUT_BUCKET_B = os.path.join(EXPERIMENT_BUCKET, \"output_b\")\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "# Gets the default SERVICE_ACCOUNT.\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
        "\n",
        "\n",
        "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
        "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
        "\n",
        "! gcloud config set project $PROJECT_ID\n",
        "\n",
        "# The evaluation and the bulk inference docker images.\n",
        "EVAL_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-lm-evaluation-harness:20231011_0934_RC00\"\n",
        "BULK_INFERRER_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-bulk-inferrer:20240708_1042_RC00\"\n",
        "\n",
        "\n",
        "def get_job_name_with_datetime(prefix: str) -\u003e str:\n",
        "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
        "    jobs in Vertex AI.\n",
        "    \"\"\"\n",
        "    return prefix + datetime.now().strftime(\"-%Y%m%d%H%M%S\")\n",
        "\n",
        "\n",
        "def preprocess(\n",
        "    output_prediction_a: Dict[str, str], output_prediction_b: Dict[str, str]\n",
        ") -\u003e Dict[str, str]:\n",
        "    \"\"\"Preprocesses the output predictions of model a and model b.\n",
        "\n",
        "    It takes the output predictions of bulk inference job of the model a and\n",
        "    model b and merges into one jsonl file.\n",
        "\n",
        "    Args:\n",
        "      output_prediction_a:\n",
        "        Output json file which contains the predictions of the model a.\n",
        "      output_prediction_b:\n",
        "        Output json file which contains the predictions of the model b.\n",
        "\n",
        "    Returns:\n",
        "      Merged jsonl file.\n",
        "    \"\"\"\n",
        "    # Get the outputs of prediction of to the dataframe.\n",
        "    df1 = pd.read_json(output_prediction_a, lines=True)\n",
        "    df2 = pd.read_json(output_prediction_b, lines=True)\n",
        "\n",
        "    # Rename the columns and merge the dataframes based on the input column.\n",
        "    df1 = df1.rename(columns={index_column: \"inputs\", \"prediction\": \"pred_a\"})\n",
        "    df2 = df2.rename(columns={index_column: \"inputs\", \"prediction\": \"pred_b\"})\n",
        "\n",
        "    df1[\"inputs\"] = df1[\"inputs\"].apply(lambda d: d[\"inputs_pretokenized\"])\n",
        "    df2[\"inputs\"] = df2[\"inputs\"].apply(lambda d: d[\"inputs_pretokenized\"])\n",
        "\n",
        "    result = pd.merge(df1, df2, on=index_column)\n",
        "\n",
        "    # Convert the dataframe to result.jsonl file.\n",
        "    return result.to_json(\"result.jsonl\", orient=\"records\", lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vNUYdFAeNnq2"
      },
      "outputs": [],
      "source": [
        "# @title Access pretrained LLaMA 2 models\n",
        "\n",
        "# @markdown The original models from Meta are converted into the HuggingFace format for serving in Vertex AI.\n",
        "\n",
        "# @markdown Accept the model agreement to access the models:\n",
        "# @markdown 1. Open the [LLaMA 2 model card](https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama2).\n",
        "# @markdown 2. Review and accept the agreement in the pop-up window on the model card page. If you have previously accepted the model agreement, there will not be a pop-up window on the model card page and this step is not needed.\n",
        "# @markdown 3. A Cloud Storage bucket (starting with ‘gs://’) containing LLaMA 2 pretrained and finetuned models will be shared under the “Documentation” section and its “Get started” subsection.\n",
        "\n",
        "# This path will be shared once click the agreement in Code LLaMA model card\n",
        "# as described in the `Access pretrained Code LLaMA models` section.\n",
        "VERTEX_AI_MODEL_GARDEN_LLAMA2 = \"\"  # @param {type:\"string\"}\n",
        "assert (\n",
        "    VERTEX_AI_MODEL_GARDEN_LLAMA2\n",
        "), \"Please click the agreement of LLaMA2 in Vertex AI Model Garden, and get the GCS path of LLaMA2 model artifacts.\"\n",
        "print(\n",
        "    \"Copy LLaMA2 model artifacts from\",\n",
        "    VERTEX_AI_MODEL_GARDEN_LLAMA2,\n",
        "    \"to \",\n",
        "    BASE_MODEL_BUCKET,\n",
        ")\n",
        "! gsutil -m cp -R $VERTEX_AI_MODEL_GARDEN_LLAMA2/* $BASE_MODEL_BUCKET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "V5AQpnzQS3j6"
      },
      "outputs": [],
      "source": [
        "# @title Evaluate LLaMA 2 models\n",
        "# @markdown This section demonstrates evaluation of LLaMA 2 models using EleutherAI's [Language Model Evaluation Harness (lm-evaluation-harness)](https://github.com/EleutherAI/lm-evaluation-harness) with Vertex Custom Job.\n",
        "\n",
        "# @markdown This example uses the dataset [TruthfulQA](https://arxiv.org/abs/2109.07958).\n",
        "# @markdown All the supported tasks are listed in [this task table](https://github.com/EleutherAI/lm-evaluation-harness/blob/master/docs/task_table.md).\n",
        "\n",
        "# set the base model id\n",
        "base_model_name = \"llama2-7b-chat-hf\"  # @param [\"llama2-7b-hf\", \"llama2-7b-chat-hf\", \"llama2-13b-hf\", \"llama2-13b-chat-hf\", \"llama2-70b-hf\", \"llama2-70b-chat-hf\"]\n",
        "base_model_id = os.path.join(BASE_MODEL_BUCKET, base_model_name)\n",
        "\n",
        "# Set the machine_type, accelerator_type, accelerator_count and benchmark dataset.\n",
        "eval_dataset = \"truthfulqa_mc\"  # @param [\"truthfulqa_mc\", \"boolq\", \"gsm8k\", \"hellaswag\", \"natural_questions\", \"openai_humaneval\", \"openbookqa\", \"quac\", \"trivia_qa\", \"winograde\"]\n",
        "\n",
        "# Worker pool spec.\n",
        "# Find Vertex AI supported accelerators and regions in:\n",
        "#  https://cloud.google.com/vertex-ai/docs/training/configure-compute\n",
        "\n",
        "if base_model_name == \"llama2-7b-hf\":\n",
        "    # Sets 1 (24G) to evaluate LLaMA2 7B models.\n",
        "    machine_type = \"g2-standard-16\"\n",
        "    accelerator_type = \"NVIDIA_L4\"\n",
        "    accelerator_count = 1\n",
        "elif base_model_name == \"llama2-7b-chat-hf\":\n",
        "    # Sets 1 L4 (24G) to evaluate LLaMA2 7B models.\n",
        "    machine_type = \"g2-standard-16\"\n",
        "    accelerator_type = \"NVIDIA_L4\"\n",
        "    accelerator_count = 1\n",
        "elif base_model_name == \"llama2-13b-hf\":\n",
        "    # Sets 2 L4 (24G) to evaluate LLaMA2 13B models.\n",
        "    machine_type = \"g2-standard-24\"\n",
        "    accelerator_type = \"NVIDIA_L4\"\n",
        "    accelerator_count = 2\n",
        "elif base_model_name == \"llama2-13b-chat-hf\":\n",
        "    # Sets 2 L4 (24G) to evaluate LLaMA2 13B models.\n",
        "    machine_type = \"g2-standard-24\"\n",
        "    accelerator_type = \"NVIDIA_L4\"\n",
        "    accelerator_count = 2\n",
        "elif base_model_name == \"llama2-70b-hf\":\n",
        "    # Sets 8 L4 (24G) to evaluate LLaMA2 70B models.\n",
        "    machine_type = \"g2-standard-96\"\n",
        "    accelerator_type = \"NVIDIA_L4\"\n",
        "    accelerator_count = 8\n",
        "elif base_model_name == \"llama2-70b-chat-hf\":\n",
        "    # Sets 8 L4 (24G) to evaluate LLaMA2 70B models.\n",
        "    machine_type = \"g2-standard-96\"\n",
        "    accelerator_type = \"NVIDIA_L4\"\n",
        "    accelerator_count = 8\n",
        "\n",
        "replica_count = 1\n",
        "\n",
        "job_name = get_job_name_with_datetime(prefix=\"llama2-eval\")\n",
        "eval_output_dir = os.path.join(MODEL_BUCKET, job_name)\n",
        "eval_output_dir_gcsfuse = eval_output_dir.replace(\"gs://\", \"/gcs/\")\n",
        "base_model_id_gcsfuse = base_model_id.replace(\"gs://\", \"/gcs/\")\n",
        "\n",
        "# @markdown  To evaluate a PEFT-finetuned model, enter the PEFT output directory below.\n",
        "# @markdown  Otherwise, leave it empty.\n",
        "\n",
        "# @markdown  See the [finetuning notebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_llama2_peft_finetuning.ipynb) for more details:\n",
        "\n",
        "peft_output_dir = \"\"  # @param {type:\"string\"}\n",
        "peft_output_dir_gcsfuse = peft_output_dir.replace(\"gs://\", \"/gcs/\")\n",
        "\n",
        "# Prepare evaluation command that runs the evaluation harness.\n",
        "# Set `trust_remote_code = True` because evaluating the model requires\n",
        "# executing code from the model repository.\n",
        "# Set `use_accelerate = True` to enable evaluation across multiple GPUs.\n",
        "eval_command = [\n",
        "    \"python\",\n",
        "    \"main.py\",\n",
        "    \"--model\",\n",
        "    \"hf-causal-experimental\",\n",
        "    \"--tasks\",\n",
        "    f\"{eval_dataset}\",\n",
        "    \"--output_path\",\n",
        "    f\"{eval_output_dir_gcsfuse}\",\n",
        "]\n",
        "\n",
        "if peft_output_dir_gcsfuse:\n",
        "    eval_command += [\n",
        "        \"--model_args\",\n",
        "        f\"pretrained={base_model_id_gcsfuse},peft={peft_output_dir_gcsfuse},trust_remote_code=True,use_accelerate=True,device_map_option=auto\",\n",
        "    ]\n",
        "else:\n",
        "    eval_command += [\n",
        "        \"--model_args\",\n",
        "        f\"pretrained={base_model_id_gcsfuse},trust_remote_code=True,use_accelerate=True,device_map_option=auto\",\n",
        "    ]\n",
        "\n",
        "# Run the evaluation job.\n",
        "worker_pool_specs = [\n",
        "    {\n",
        "        \"machine_spec\": {\n",
        "            \"machine_type\": machine_type,\n",
        "            \"accelerator_type\": accelerator_type,\n",
        "            \"accelerator_count\": accelerator_count,\n",
        "        },\n",
        "        \"replica_count\": replica_count,\n",
        "        \"disk_spec\": {\n",
        "            \"boot_disk_size_gb\": 500,\n",
        "        },\n",
        "        \"container_spec\": {\n",
        "            \"image_uri\": EVAL_DOCKER_URI,\n",
        "            \"command\": eval_command,\n",
        "            \"args\": [],\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "eval_job = aiplatform.CustomJob(\n",
        "    display_name=job_name,\n",
        "    worker_pool_specs=worker_pool_specs,\n",
        "    base_output_dir=eval_output_dir,\n",
        ")\n",
        "\n",
        "eval_job.run()\n",
        "\n",
        "print(\"Evaluation results were saved in:\", eval_output_dir)\n",
        "\n",
        "# Fetch evaluation results.\n",
        "storage_client = storage.Client()\n",
        "BUCKET_NAME = BUCKET_URI.replace(\"gs://\", \"\")\n",
        "bucket = storage_client.get_bucket(BUCKET_NAME)\n",
        "RESULT_FILE_PATH = eval_output_dir[len(BUCKET_URI) + 1 :]\n",
        "blob = bucket.blob(RESULT_FILE_PATH)\n",
        "raw_result = blob.download_as_string()\n",
        "\n",
        "# Print evaluation results.\n",
        "result = json.loads(raw_result)\n",
        "result_formatted = json.dumps(result, indent=2)\n",
        "print(f\"Evaluation result:\\n{result_formatted}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3Vm13jGd4aU"
      },
      "source": [
        "### Bulk Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yyW4BJDr9t-O"
      },
      "outputs": [],
      "source": [
        "# @title [Optional] Generate `input_dataset` for the bulk inference job\n",
        "# @markdown Note: For experimentation, we request that users provide only a few prompts.\n",
        "\n",
        "# @markdown For demonstration, a publicly available [TensorFlow dataset](https://www.tensorflow.org/datasets/catalog/reddit) is used. This dataset contains preprocessed posts from the Reddit dataset.\n",
        "TEST_DATASET = \"gs://vertex-ai/generative-ai/rlhf/text_small/reddit_tfds/val/shard-00000-of-00001.jsonl\"  # @param {type:\"string\"}\n",
        "NUM_EXAMPLES = 50  # @param {type:\"integer\"}\n",
        "\n",
        "# Load dataset and modify.\n",
        "df = pd.read_json(TEST_DATASET, lines=True)\n",
        "examples = df.head(NUM_EXAMPLES)\n",
        "\n",
        "# Upload new dataset to GCS.\n",
        "examples.to_json(\"data.json\", orient=\"records\", lines=True)\n",
        "! gsutil cp data.json $BUCKET_URI/temp/data.jsonl\n",
        "DATASET = f\"{BUCKET_URI}/temp/data.jsonl\"\n",
        "\n",
        "print(f\"{NUM_EXAMPLES} examples written to {DATASET}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "X-yl8yOptBvH"
      },
      "outputs": [],
      "source": [
        "# @title Set up bulk inference job\n",
        "\n",
        "# @markdown Run the bulk inference custom job to generate offline predictions.\n",
        "# @markdown You can perform bulk inference using a base model or LoRA finetuned model.\n",
        "\n",
        "# Setup bulk inference job.\n",
        "model_a = \"llama2-7b-hf\"  # @param [\"llama2-7b-hf\", \"llama2-7b-chat-hf\", \"llama2-13b-hf\", \"llama2-13b-chat-hf\",  \"llama2-70b-hf\", \"llama2-70b-chat-hf\"]\n",
        "model_b = \"llama2-13b-hf\"  # @param [\"llama2-7b-hf\", \"llama2-7b-chat-hf\", \"llama2-13b-hf\", \"llama2-13b-chat-hf\",  \"llama2-70b-hf\", \"llama2-70b-chat-hf\"]\n",
        "\n",
        "# @markdown **Required Parameters**\n",
        "\n",
        "# @markdown `input_dataset` : Path to JSONL file containing the input dataset.\n",
        "\n",
        "# @markdown `index_column` : The column which distinguishes unique evaluation examples.\n",
        "\n",
        "# @markdown `input_text` : Indexing key for inputs.\n",
        "\n",
        "# @markdown `output_prediction_a`: Path to JSONL file which will contain output predictions of model a.\n",
        "\n",
        "# @markdown `output_prediction_b`: Path to JSONL file which will contain output predictions of model b.\n",
        "\n",
        "input_dataset = f\"{BUCKET_URI}/temp/data.jsonl\"  # @param {type:\"string\"}\n",
        "index_column = \"inputs\"  # @param {type:\"string\"}\n",
        "input_text = \"input_text\"  # @param {type:\"string\"}\n",
        "\n",
        "output_prediction_a = \"\"  # @param {type:\"string\"}\n",
        "output_prediction_b = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "# @markdown **Optional Parameters** : Provide the path to the LoRA finetuned models.\n",
        "\n",
        "# @markdown `lora_path_a` : Path to finetuned LoRA adapter of model a.\n",
        "\n",
        "# @markdown `lora_path_b` : Path to finetuned LoRA adapter of model b.\n",
        "\n",
        "lora_path_a = \"\"  # @param {type:\"string\"}\n",
        "lora_path_b = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "if model_a == model_b:\n",
        "    raise ValueError(\"Select different models to run AutoSxS evaluation.\")\n",
        "\n",
        "if model_a in [\"llama2-7b-hf\", \"llama2-7b-chat-hf\"]:\n",
        "    machine_type_a = \"g2-standard-16\"\n",
        "    accelerator_type_a = \"NVIDIA_L4\"\n",
        "    accelerator_count_a = 1\n",
        "elif model_a in [\"llama2-13b-hf\", \"llama2-13b-chat-hf\"]:\n",
        "    machine_type_a = \"g2-standard-24\"\n",
        "    accelerator_type_a = \"NVIDIA_L4\"\n",
        "    accelerator_count_a = 2\n",
        "elif model_a in [\"llama2-70b-hf\", \"llama2-70b-chat-hf\"]:\n",
        "    machine_type_a = \"g2-standard-96\"\n",
        "    accelerator_type_a = \"NVIDIA_L4\"\n",
        "    accelerator_count_a = 8\n",
        "\n",
        "if model_b in [\"llama2-7b-hf\", \"llama2-7b-chat-hf\"]:\n",
        "    machine_type_b = \"g2-standard-16\"\n",
        "    accelerator_type_b = \"NVIDIA_L4\"\n",
        "    accelerator_count_b = 1\n",
        "elif model_b in [\"llama2-13b-hf\", \"llama2-13b-chat-hf\"]:\n",
        "    machine_type_b = \"g2-standard-24\"\n",
        "    accelerator_type_b = \"NVIDIA_L4\"\n",
        "    accelerator_count_b = 2\n",
        "elif model_b in [\"llama2-70b-hf\", \"llama2-70b-chat-hf\"]:\n",
        "    machine_type_b = \"g2-standard-96\"\n",
        "    accelerator_type_b = \"NVIDIA_L4\"\n",
        "    accelerator_count_b = 8\n",
        "\n",
        "replica_count = 1\n",
        "\n",
        "bulk_infer_job_name = get_job_name_with_datetime(prefix=\"bulk-infer\")\n",
        "eval_output_dir = os.path.join(MODEL_BUCKET, bulk_infer_job_name)\n",
        "\n",
        "# Maximum encoder/prefix length. Inputs will be padded or truncated to match this length.\n",
        "input_seq_length = 50\n",
        "\n",
        "# Maximum number of decoder steps. Outputs will be at most this length.\n",
        "targets_seq_length = 50\n",
        "\n",
        "model_id_a = os.path.join(BASE_MODEL_BUCKET, model_a)\n",
        "model_id_b = os.path.join(BASE_MODEL_BUCKET, model_b)\n",
        "\n",
        "worker_pool_specs_base_model_a = [\n",
        "    {\n",
        "        \"machine_spec\": {\n",
        "            \"machine_type\": machine_type_a,\n",
        "            \"accelerator_type\": accelerator_type_a,\n",
        "            \"accelerator_count\": accelerator_count_a,\n",
        "        },\n",
        "        \"replica_count\": replica_count,\n",
        "        \"disk_spec\": {\n",
        "            \"boot_disk_type\": \"pd-ssd\",\n",
        "            \"boot_disk_size_gb\": 500,\n",
        "        },\n",
        "        \"container_spec\": {\n",
        "            \"image_uri\": BULK_INFERRER_DOCKER_URI,\n",
        "            \"args\": [\n",
        "                f\"--large_model_reference={model_id_a}\",\n",
        "                f\"--input_model={lora_path_a}\",\n",
        "                f\"--input_dataset={input_dataset}\",\n",
        "                \"--dataset_split=empty\",\n",
        "                f\"--output_prediction={output_prediction_a}\",\n",
        "                f\"--output_prediction_gcs_path={OUTPUT_BUCKET_A}\",\n",
        "                f\"--inputs_sequence_length={input_seq_length}\",\n",
        "                f\"--targets_sequence_length={targets_seq_length}\",\n",
        "                f\"--inputs_key={input_text}\",\n",
        "            ],\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "worker_pool_specs_base_model_b = [\n",
        "    {\n",
        "        \"machine_spec\": {\n",
        "            \"machine_type\": machine_type_b,\n",
        "            \"accelerator_type\": accelerator_type_b,\n",
        "            \"accelerator_count\": accelerator_count_b,\n",
        "        },\n",
        "        \"replica_count\": replica_count,\n",
        "        \"disk_spec\": {\n",
        "            \"boot_disk_size_gb\": 500,\n",
        "        },\n",
        "        \"container_spec\": {\n",
        "            \"image_uri\": BULK_INFERRER_DOCKER_URI,\n",
        "            \"args\": [\n",
        "                f\"--large_model_reference={model_id_b}\",\n",
        "                f\"--input_model={lora_path_b}\",\n",
        "                f\"--input_dataset={input_dataset}\",\n",
        "                \"--dataset_split=empty\",\n",
        "                f\"--output_prediction={output_prediction_b}\",\n",
        "                f\"--output_prediction_gcs_path={OUTPUT_BUCKET_B}\",\n",
        "                f\"--inputs_sequence_length={input_seq_length}\",\n",
        "                f\"--targets_sequence_length={targets_seq_length}\",\n",
        "                f\"--inputs_key={input_text}\",\n",
        "            ],\n",
        "        },\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "x1sPbM1-eEx7"
      },
      "outputs": [],
      "source": [
        "# @title Run bulk inference job for Model A\n",
        "\n",
        "bulk_inferrer_a = aiplatform.CustomJob(\n",
        "    display_name=get_job_name_with_datetime(prefix=\"bulk-infer-a\"),\n",
        "    worker_pool_specs=worker_pool_specs_base_model_a,\n",
        "    base_output_dir=os.path.join(OUTPUT_BUCKET_A, bulk_infer_job_name),\n",
        ")\n",
        "\n",
        "bulk_inferrer_a.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_WzQmq2yeHSg"
      },
      "outputs": [],
      "source": [
        "# @title Run bulk inference job for Model B\n",
        "\n",
        "bulk_inferrer_b = aiplatform.CustomJob(\n",
        "    display_name=get_job_name_with_datetime(prefix=\"bulk-infer-b\"),\n",
        "    worker_pool_specs=worker_pool_specs_base_model_b,\n",
        "    base_output_dir=os.path.join(OUTPUT_BUCKET_B, bulk_infer_job_name),\n",
        ")\n",
        "\n",
        "bulk_inferrer_b.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pX-sEhReKdB"
      },
      "source": [
        "### AutoSxS Job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hE_8SlrUGrSP"
      },
      "outputs": [],
      "source": [
        "# @title Compile AutoSxS pipeline\n",
        "\n",
        "from google_cloud_pipeline_components.preview import model_evaluation\n",
        "from kfp import compiler\n",
        "\n",
        "template_uri = \"pipeline.yaml\"\n",
        "compiler.Compiler().compile(\n",
        "    pipeline_func=model_evaluation.autosxs_pipeline,\n",
        "    package_path=template_uri,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VRBrm4d3eLtV"
      },
      "outputs": [],
      "source": [
        "# @title Run AutoSxS pipeline job\n",
        "\n",
        "# @markdown Automatic side-by-side (AutoSxS) is a model-assisted evaluation tool that compares two large language models (LLMs) side by side.\n",
        "# @markdown In order to run AutoSxS, we need to define a `autosxs_pipeline` job with the following parameters. More details of the AutoSxS pipeline configuration can be found [here](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-2.9.0/api/preview/model_evaluation.html#preview.model_evaluation.autosxs_pipeline).\n",
        "\n",
        "\n",
        "# Preprocess the output json files and copy to the GCS bucket\n",
        "preprocess(output_prediction_a, output_prediction_b)\n",
        "PREDS = f\"{BUCKET_URI}/temp/preds/\"\n",
        "! gsutil cp result.jsonl $PREDS\n",
        "\n",
        "# @title AutoSxS Job\n",
        "autosxs_job_name = get_job_name_with_datetime(prefix=\"autosxs\")\n",
        "\n",
        "# @markdown AutoSxS supports evaluating models for summarization and question-answering tasks.\n",
        "\"\"\"\n",
        "Evaluation task in the form {task}@{version}. Task can be one of\n",
        "[summarization, question_answer].\n",
        "version is an integer with three digits or 'latest'.\n",
        "Ex: summarization@001 or question_answer@latest\n",
        "\"\"\"\n",
        "task_name = \"summarization@001\"  # @param [\"question_answer@latest\", \"summarization@001\"]\n",
        "\n",
        "\n",
        "parameters = {\n",
        "    \"evaluation_dataset\": f\"{BUCKET_URI}/temp/preds/result.jsonl\",\n",
        "    \"id_columns\": [\"inputs\"],\n",
        "    \"autorater_prompt_parameters\": {\n",
        "        \"inference_context\": {\"column\": index_column},\n",
        "        \"inference_instruction\": {\"template\": \"{{ default_instruction }}\"},\n",
        "    },\n",
        "    \"response_column_a\": \"pred_a\",\n",
        "    \"response_column_b\": \"pred_b\",\n",
        "    \"task\": task_name,\n",
        "}\n",
        "\n",
        "autosxs_job = aiplatform.PipelineJob(\n",
        "    job_id=autosxs_job_name,\n",
        "    display_name=autosxs_job_name,\n",
        "    pipeline_root=os.path.join(BUCKET_URI, autosxs_job_name),\n",
        "    template_path=template_uri,\n",
        "    parameter_values=parameters,\n",
        "    enable_caching=False,\n",
        ")\n",
        "autosxs_job.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gAXCY0hueRBF"
      },
      "outputs": [],
      "source": [
        "# @title Get autorater judgements\n",
        "# @markdown Autorater is a language model which compares the quality of two model responses based on a pre-defined criteria.\n",
        "# @markdown More details on the autorater can be found [here](https://cloud.google.com/vertex-ai/generative-ai/docs/models/side-by-side-eval#autorater)\n",
        "\n",
        "for details in autosxs_job.task_details:\n",
        "    if details.task_name == \"online-evaluation-pairwise\":\n",
        "        break\n",
        "\n",
        "# Judgments\n",
        "judgments_uri = details.outputs[\"judgments\"].artifacts[0].uri\n",
        "judgments_df = pd.read_json(judgments_uri, lines=True)\n",
        "judgments_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5NYV1FKjIeD6"
      },
      "outputs": [],
      "source": [
        "# @title Get win-rate\n",
        "# @markdown Win rate is the percentage of the time the autorater has decided that a particular model had a better response.\n",
        "\n",
        "for details in autosxs_job.task_details:\n",
        "    if details.task_name == \"model-evaluation-text-generation-pairwise\":\n",
        "        break\n",
        "pd.DataFrame([details.outputs[\"autosxs_metrics\"].artifacts[0].metadata])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJuMq31DeWwO"
      },
      "source": [
        "### Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "911406c1561e"
      },
      "outputs": [],
      "source": [
        "# @title Clean up\n",
        "# @markdown  Delete the jobs to recycle the resources and avoid unnecessary continouous charges that may incur.\n",
        "\n",
        "eval_job.delete()\n",
        "bulk_inferrer_a.delete()\n",
        "bulk_inferrer_b.delete()\n",
        "autosxs_job.delete()\n",
        "\n",
        "# Delete Cloud Storage objects that were created\n",
        "delete_bucket = False  # @param {type: \"boolean\"}\n",
        "if delete_bucket:\n",
        "    ! gsutil -m rm -r $EXPERIMENT_BUCKET"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_pytorch_llama2_evaluation.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

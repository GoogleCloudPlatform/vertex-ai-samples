{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7d9bbf86da5e"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Vertex AI Model Garden - WeatherNext Forecasting\n",
        "\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\n",
        "  \u003ctd style=\"text-align: center\"\u003e\n",
        "    \u003ca href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_weather_prediction_on_vertex.ipynb\"\u003e\n",
        "      \u003cimg alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"\u003e\u003cbr\u003e Run in Colab Enterprise\n",
        "    \u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd style=\"text-align: center\"\u003e\n",
        "    \u003ca href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_weather_prediction_on_vertex.ipynb\"\u003e\n",
        "      \u003cimg alt=\"GitHub logo\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\"\u003e\u003cbr\u003e View on GitHub\n",
        "    \u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates running forecasts with [WeatherNext Graph](https://www.science.org/doi/10.1126/science.adi2336) and [WeatherNext Gen](https://arxiv.org/abs/2312.15796) models on TPU using Vertex Model Garden.\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Config Gen/Graph models with example data\n",
        "- Run Gen/Graph model forecasts\n",
        "- Visualize forecasting results\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "264c07757582"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### Request For TPU Quota\n",
        "\n",
        "By default, the quota for TPU training [Custom model training TPU v5e cores per region](https://console.cloud.google.com/iam-admin/quotas?location=us-central1\u0026metric=aiplatform.googleapis.com%2Fcustom_model_training_tpu_v5e) is 0. TPU quota is only available in `us-west1`, `us-west4`, `us-central1`. You can request for higher TPU quota following the instructions at [\"Request a higher quota\"](https://cloud.google.com/docs/quota/view-manage#requesting_higher_quota). It is suggested to request at least 4 v5e to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2707b02ef5df",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project\n",
        "\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown 2. **[Optional]** [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs. Set the BUCKET_URI for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`) should be located in the same region as where the notebook was launched. Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\"). If not set, a unique GCS bucket will be created instead.\n",
        "\n",
        "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown 3. **[Optional]** TPU is only available in `us-west1`, `us-west4`, `us-central1`.\n",
        "\n",
        "# REGION = \"\"  # @param {type:\"string\"}\n",
        "REGION = \"us-central1\" # @param [\"us-central1\", \"us-west1\", \"us-west4\"]\n",
        "\n",
        "# Import the necessary packages\n",
        "import datetime\n",
        "import importlib\n",
        "import os\n",
        "import uuid\n",
        "from typing import Tuple, List\n",
        "import glob\n",
        "from google.cloud import aiplatform, storage\n",
        "\n",
        "# Upgrade Vertex AI SDK.\n",
        "! pip3 install --upgrade --quiet 'google-cloud-aiplatform\u003e=1.64.0'\n",
        "if not os.path.exists(\"./vertex-ai-samples\"):\n",
        "  ! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
        "! pip3 uninstall --quiet -y xarray\n",
        "! pip3 install --quiet xarray[complete]\n",
        "\n",
        "# Import model garden utils.\n",
        "common_util = importlib.import_module(\n",
        "    \"vertex-ai-samples.community-content.vertex_model_garden.model_oss.notebook_util.common_util\"\n",
        ")\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "if not REGION:\n",
        "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
        "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
        "\n",
        "# Cloud Storage bucket for storing the experiment artifacts.\n",
        "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
        "# prefer using your own GCS bucket, change the value yourself below.\n",
        "now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "\n",
        "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
        "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}-{str(uuid.uuid4())[:4]}\"\n",
        "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
        "else:\n",
        "    assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
        "    shell_output = ! gsutil ls -Lb {BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
        "    bucket_region = shell_output[0].strip().lower()\n",
        "    if bucket_region != REGION:\n",
        "        raise ValueError(\n",
        "            \"Bucket region %s is different from notebook region %s\"\n",
        "            % (bucket_region, REGION)\n",
        "        )\n",
        "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
        "\n",
        "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "# Gets the default SERVICE_ACCOUNT.\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
        "\n",
        "\n",
        "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
        "! gcloud config set project $PROJECT_ID\n",
        "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/storage.admin\"\n",
        "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/aiplatform.user\"\n",
        "\n",
        "\n",
        "# Utility functions for vertex jobs.\n",
        "def get_job_name_with_datetime(prefix: str) -\u003e str:\n",
        "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
        "    jobs in Vertex AI.\n",
        "    \"\"\"\n",
        "    return prefix + datetime.datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
        "\n",
        "def get_bucket_and_blob_name(filepath):\n",
        "    # The gcs path is of the form gs:///\n",
        "    gs_suffix = filepath.split(\"gs://\", 1)[1]\n",
        "    return tuple(gs_suffix.split(\"/\", 1))\n",
        "\n",
        "def upload_local_dir_to_gcs(local_dir_path, gcs_dir_path):\n",
        "    \"\"\"Uploads files in a local directory to a GCS directory.\"\"\"\n",
        "    client = storage.Client()\n",
        "    bucket_name = gcs_dir_path.split(\"/\")[2]\n",
        "    bucket = client.get_bucket(bucket_name)\n",
        "    for local_file in glob.glob(local_dir_path + \"/**\"):\n",
        "        if not os.path.isfile(local_file):\n",
        "            continue\n",
        "        filename = local_file[1 + len(local_dir_path) :]\n",
        "        gcs_file_path = os.path.join(gcs_dir_path, filename)\n",
        "        _, blob_name = get_bucket_and_blob_name(gcs_file_path)\n",
        "        blob = bucket.blob(blob_name)\n",
        "        blob.upload_from_filename(local_file)\n",
        "        print(\"Copied {} to {}.\".format(local_file, gcs_file_path))\n",
        "\n",
        "def download_gcs_blob_as_json(gcs_file_path):\n",
        "    \"\"\"Download GCS blob and convert it to json.\"\"\"\n",
        "    client = storage.Client()\n",
        "    bucket_name, blob_name = get_bucket_and_blob_name(gcs_file_path)\n",
        "    bucket = client.get_bucket(bucket_name)\n",
        "    blob = bucket.blob(blob_name)\n",
        "\n",
        "    return json.loads(blob.download_as_bytes())\n",
        "\n",
        "# Utility functions for prediction visualization.\n",
        "import matplotlib\n",
        "import xarray\n",
        "from typing import Optional\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import math\n",
        "from IPython.display import HTML\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "print(xarray.backends.list_engines())\n",
        "\n",
        "def get_existing_demo_step(\n",
        "    num_forecast_steps: int,\n",
        "    model_type: str = \"gen_small\",\n",
        "    ) -\u003e str:\n",
        "  # The demo data only supports some steps, and we can only run the predictions\n",
        "  # if num_forecast_steps is smaller than or equal to the maximum supported setps.\n",
        "  if model_type == \"gen_small\":\n",
        "    # The max supported steps are obtained from gs://dm_graphcast/gencast/dataset.\n",
        "    supported_demo_steps = [1, 4, 12, 20, 30]\n",
        "  elif model_type == \"graph_small\":\n",
        "    # The max supported steps are obtained from gs://dm_graphcast/graphcast/dataset.\n",
        "    supported_demo_steps = [1, 4, 12, 20, 40]\n",
        "  elif model_type == \"graph_operational\":\n",
        "    # The max supported steps are obtained from gs://dm_graphcast/graphcast/dataset.\n",
        "    supported_demo_steps = [1, 4, 12]\n",
        "  else:\n",
        "    raise ValueError(\"Invalid model_type.\")\n",
        "\n",
        "  # Find the proper demo data for forecasting.\n",
        "  found_supported_step = supported_demo_steps[-1]\n",
        "  for i, supported_step in enumerate(supported_demo_steps):\n",
        "    if num_forecast_steps \u003c= supported_step:\n",
        "      found_supported_step = supported_step\n",
        "      break\n",
        "  if num_forecast_steps \u003e found_supported_step:\n",
        "    raise ValueError(f\"Supported demo steps for {model_type} in gs://dm_graphcast are {supported_demo_steps}. {num_forecast_steps} is too large, and could not find proper demo data.\")\n",
        "  return found_supported_step\n",
        "\n",
        "def get_suggested_machines(\n",
        "    num_forecast_steps: int,\n",
        "    model_type: str = \"gen_small\",) -\u003e Tuple[str, str, int]:\n",
        "    if model_type == \"gen_small\":\n",
        "      if num_forecast_steps \u003c= 16:\n",
        "        machine_type = \"ct5lp-hightpu-4t\"\n",
        "        tpu_topology = \"2x2\"\n",
        "        accelerator_count = 4\n",
        "      else:\n",
        "        machine_type = \"ct5lp-hightpu-8t\"\n",
        "        tpu_topology = \"2x4\"\n",
        "        accelerator_count = 8\n",
        "    else:\n",
        "      if num_forecast_steps \u003c= 16:\n",
        "        machine_type = \"ct5lp-hightpu-1t\"\n",
        "        tpu_topology = \"1x1\"\n",
        "        accelerator_count = 1\n",
        "      else:\n",
        "        machine_type = \"ct5lp-hightpu-4t\"\n",
        "        tpu_topology = \"2x2\"\n",
        "        accelerator_count = 4\n",
        "    return machine_type, tpu_topology, accelerator_count\n",
        "\n",
        "def select(\n",
        "    data: xarray.Dataset,\n",
        "    variable: str,\n",
        "    level: Optional[int] = None,\n",
        "    max_steps: Optional[int] = None\n",
        "    ) -\u003e xarray.Dataset:\n",
        "  data = data[variable]\n",
        "  if \"batch\" in data.dims:\n",
        "    data = data.isel(batch=0)\n",
        "  if max_steps is not None and \"time\" in data.sizes and max_steps \u003c data.sizes[\"time\"]:\n",
        "    data = data.isel(time=range(0, max_steps))\n",
        "  if level is not None and \"level\" in data.coords:\n",
        "    data = data.sel(level=level)\n",
        "  return data\n",
        "\n",
        "def scale(\n",
        "    data: xarray.Dataset,\n",
        "    center: Optional[float] = None,\n",
        "    robust: bool = False,\n",
        "    ) -\u003e tuple[xarray.Dataset, matplotlib.colors.Normalize, str]:\n",
        "  vmin = np.nanpercentile(data, (2 if robust else 0))\n",
        "  vmax = np.nanpercentile(data, (98 if robust else 100))\n",
        "  if center is not None:\n",
        "    diff = max(vmax - center, center - vmin)\n",
        "    vmin = center - diff\n",
        "    vmax = center + diff\n",
        "  return (data, matplotlib.colors.Normalize(vmin, vmax),\n",
        "          (\"RdBu_r\" if center is not None else \"viridis\"))\n",
        "\n",
        "def plot_data(\n",
        "    data: dict[str, xarray.Dataset],\n",
        "    fig_title: str,\n",
        "    plot_size: float = 5,\n",
        "    robust: bool = False,\n",
        "    cols: int = 4\n",
        "    ) -\u003e tuple[xarray.Dataset, matplotlib.colors.Normalize, str]:\n",
        "\n",
        "  first_data = next(iter(data.values()))[0]\n",
        "  max_steps = first_data.sizes.get(\"time\", 1)\n",
        "  assert all(max_steps == d.sizes.get(\"time\", 1) for d, _, _ in data.values())\n",
        "\n",
        "  cols = min(cols, len(data))\n",
        "  rows = math.ceil(len(data) / cols)\n",
        "  figure = plt.figure(figsize=(plot_size * 2 * cols,\n",
        "                               plot_size * rows))\n",
        "  figure.suptitle(fig_title, fontsize=16)\n",
        "  figure.subplots_adjust(wspace=0, hspace=0)\n",
        "  figure.tight_layout()\n",
        "\n",
        "  images = []\n",
        "  for i, (title, (plot_data, norm, cmap)) in enumerate(data.items()):\n",
        "    ax = figure.add_subplot(rows, cols, i+1)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_title(title)\n",
        "    im = ax.imshow(\n",
        "        plot_data.isel(time=0, missing_dims=\"ignore\"), norm=norm,\n",
        "        origin=\"lower\", cmap=cmap)\n",
        "    plt.colorbar(\n",
        "        mappable=im,\n",
        "        ax=ax,\n",
        "        orientation=\"vertical\",\n",
        "        pad=0.02,\n",
        "        aspect=16,\n",
        "        shrink=0.75,\n",
        "        cmap=cmap,\n",
        "        extend=(\"both\" if robust else \"neither\"))\n",
        "    images.append(im)\n",
        "\n",
        "  def update(frame):\n",
        "    if \"time\" in first_data.dims:\n",
        "      td = datetime.timedelta(microseconds=first_data[\"time\"][frame].item() / 1000)\n",
        "      figure.suptitle(f\"{fig_title}, {td}\", fontsize=16)\n",
        "    else:\n",
        "      figure.suptitle(fig_title, fontsize=16)\n",
        "    for im, (plot_data, norm, cmap) in zip(images, data.values()):\n",
        "      im.set_data(plot_data.isel(time=frame, missing_dims=\"ignore\"))\n",
        "\n",
        "  ani = animation.FuncAnimation(\n",
        "      fig=figure, func=update, frames=max_steps, interval=250)\n",
        "  plt.close(figure.number)\n",
        "  return HTML(ani.to_jshtml())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forecasts and Visualizations"
      ],
      "metadata": {
        "id": "qQIfscWiCC6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Configure Models\n",
        "# @markdown You can config WeatherNext models with *data_type*, *model_type* and *num_forecast_steps*.\n",
        "\n",
        "output_dir = f\"{BUCKET_URI}/science\"\n",
        "accelerator_type = \"TPU_V5e\"\n",
        "\n",
        "# @markdown All demo data are from the public gcs bucket `gs://dm_graphcast`.\n",
        "# @markdown You can prepare your own data similarly to demo data for forecasting.\n",
        "# @markdown The demo data for gen_small is from the date 2019-03-29 with resolution 1.0.\n",
        "# @markdown The demo data for graph_small is from the date 2022-01-01 with resolution 1.0.\n",
        "# @markdown The demo data for graph_operational is from the date 2022-01-01 with resolution 0.25.\n",
        "\n",
        "model_type = \"graph_operational\" # @param [\"gen_small\", \"graph_small\", \"graph_operational\"]\n",
        "\n",
        "\n",
        "num_forecast_steps = 10 # @param {type:\"integer\"}\n",
        "# @markdown *num_forecast_steps* will specific the number of forecast steps, which will indicate the forcasting time combined with model leading time.\n",
        "# @markdown Assuming num_forecast_steps=4, and the leading time is 6 hours, then the results will contain forecasts with 6 hours, 12 hours, 18 hours and 24 hours.\n",
        "# @markdown WeatherNext Gen and Graph models support leading time as 12 hours and 6 hours separately.\n",
        "# @markdown num_forecast_steps will be truncated to the maximum of allowed values if it is beyond. Maximum of num_forecast_steps for Weather Gen and Graph models are 30 and 40 separately.\n",
        "machine_type, tpu_topology, accelerator_count = get_suggested_machines(num_forecast_steps, model_type)\n",
        "\n",
        "SCIENCE_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai-restricted/vertex-vision-model-garden-dockers/science-serve.tpu.0-1.debian12.py310:20250331_0715_RC03\"\n",
        "data_storage_dir = \"gs://dm_graphcast\"\n",
        "existing_demo_step = get_existing_demo_step(num_forecast_steps, model_type)\n",
        "if model_type == \"gen_small\":\n",
        "  input_file = f\"{data_storage_dir}/gencast/dataset/source-era5_date-2019-03-29_res-1.0_levels-13_steps-{existing_demo_step:02d}.nc\"\n",
        "  # num_ensemble_samples (WeatherNext Gen models only) specified the number of ensembling samples per step.\n",
        "  num_ensemble_samples = 8\n",
        "  parameters = {\"num_forecast_steps\": num_forecast_steps, \"num_ensemble_samples\": num_ensemble_samples}\n",
        "elif model_type == \"graph_small\":\n",
        "  input_file = f\"{data_storage_dir}/graphcast/dataset/source-era5_date-2022-01-01_res-1.0_levels-13_steps-{existing_demo_step:02d}.nc\"\n",
        "  parameters = {\"num_forecast_steps\": num_forecast_steps}\n",
        "elif model_type == \"graph_operational\":\n",
        "  input_file = f\"{data_storage_dir}/graphcast/dataset/source-hres_date-2022-01-01_res-0.25_levels-13_steps-{existing_demo_step:02d}.nc\"\n",
        "  parameters = {\"num_forecast_steps\": num_forecast_steps}\n",
        "else:\n",
        "  raise ValueError(\"Invalid example_type.\")\n",
        "\n",
        "instances = [\n",
        "    {\n",
        "        \"input_file\": input_file,\n",
        "        \"output_dir\": output_dir,\n",
        "        \"parameters\": parameters,\n",
        "    }\n",
        "]\n",
        "\n",
        "# @markdown Refer to more details in https://github.com/google-deepmind/graphcast.\n",
        "\n",
        "print(f\"machine_type is {machine_type}.\")\n",
        "print(f\"tpu_topology is {tpu_topology}.\")\n",
        "print(f\"SCIENCE_DOCKER_URI is {SCIENCE_DOCKER_URI}.\")\n",
        "print(f\"The prediction instances: {instances}\")\n"
      ],
      "metadata": {
        "id": "Hf_7MhTIMSJp",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run Forecasts\n",
        "# @markdown This section will create vertex jobs to run forecasts.\n",
        "# @markdown It usually takes a couple of minutes to finish.\n",
        "# @markdown Click on the generated link in the output to see your run in the Cloud Console.\n",
        "\n",
        "print(\"Check if there are enough quota.\")\n",
        "common_util.check_quota(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    is_for_training=True,\n",
        ")\n",
        "\n",
        "print(\"Generate custom job inputs and outputs.\")\n",
        "input_jsonl_name = f\"custom_{model_type}_input.jsonl\"\n",
        "output_jsonl_name = f\"custom_{model_type}_output.jsonl\"\n",
        "\n",
        "# Convert and write JSON object to file.\n",
        "os.makedirs(\"bath_prediction_input\", exist_ok=True)\n",
        "\n",
        "with open(f\"bath_prediction_input/{input_jsonl_name}\", \"w\") as outfile:\n",
        "    for item in instances:\n",
        "        json_str = json.dumps(item)\n",
        "        outfile.write(json_str)\n",
        "        outfile.write(\"\\n\")\n",
        "\n",
        "upload_local_dir_to_gcs(\n",
        "    \"bath_prediction_input\", output_dir\n",
        ")\n",
        "\n",
        "JOB_NAME = get_job_name_with_datetime(prefix=f\"jax_{model_type}\")\n",
        "\n",
        "input_jsonl = f\"{output_dir}/{input_jsonl_name}\"\n",
        "output_jsonl = f\"{output_dir}/{output_jsonl_name}\"\n",
        "\n",
        "docker_args_list = [\n",
        "    \"python3\",\n",
        "    \"./gdm_science/batch_prediction.py\",\n",
        "    f\"--model_type={model_type}\",\n",
        "    f\"--input_jsonl={input_jsonl}\",\n",
        "    f\"--output_jsonl={output_jsonl}\"\n",
        "]\n",
        "\n",
        "print(f\"The input json file will be {input_jsonl}.\")\n",
        "print(f\"The output json file will be {output_jsonl}.\")\n",
        "print(f\"The docker args list is {docker_args_list}.\")\n",
        "print(f\"JOB_NAME is {JOB_NAME}.\")\n",
        "\n",
        "labels = {\n",
        "    \"mg-source\": \"notebook\",\n",
        "    \"mg-notebook-name\": \"model_garden_weather_prediction_on_vertex.ipynb\".split(\".\")[0],\n",
        "    \"mg-tune\": f\"publishers-google-models-{model_type}\".lower(),\n",
        "    \"versioned-mg-tune\": f\"publishers-google-models-{model_type}\".lower(),\n",
        "}\n",
        "\n",
        "job = aiplatform.CustomContainerTrainingJob(\n",
        "    display_name=JOB_NAME,\n",
        "    container_uri=SCIENCE_DOCKER_URI,\n",
        "    labels=labels,\n",
        ")\n",
        "\n",
        "job.run(\n",
        "    args=docker_args_list,\n",
        "    base_output_dir=f\"{BUCKET_URI}\",\n",
        "    replica_count=1,\n",
        "    machine_type=machine_type,\n",
        "    tpu_topology=tpu_topology,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        ")"
      ],
      "metadata": {
        "id": "LSejOqyVUvdF",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  Visualize Forecasts\n",
        "# @markdown For simplicity, we only pick the one forecast result for visualization.\n",
        "# @markdown You can also visualize previous forecasts by setting `prediction_data_path`.\n",
        "prediction_json = download_gcs_blob_as_json(output_jsonl)\n",
        "prediction_data_path = json.loads(prediction_json[0])[\"predictions\"] # @param\n",
        "\n",
        "# @markdown  (Optional) The sample to visualize if there are multiple samples.\n",
        "sample = 0 # @param\n",
        "\n",
        "print(prediction_data_path)\n",
        "predictions = xarray.open_zarr(prediction_data_path)\n",
        "\n",
        "plot_size = 7\n",
        "variable = \"2m_temperature\"\n",
        "level = None\n",
        "steps = predictions.dims[\"time\"]\n",
        "print(\"steps=\", steps)\n",
        "if \"sample\" in predictions:\n",
        "  print(\"sample=\", len(predictions[\"sample\"]))\n",
        "  # Visualize one sample if there are many.\n",
        "  visualized_data = predictions.isel(sample=sample)\n",
        "else:\n",
        "  visualized_data = predictions\n",
        "\n",
        "data = {\n",
        "    \" \": scale(select(visualized_data, variable, level, steps), robust=True),\n",
        "}\n",
        "\n",
        "fig_title = variable\n",
        "if \"level\" in predictions[variable].coords:\n",
        "  fig_title += f\" at {level} hPa\"\n",
        "\n",
        "plot_data(data, fig_title, plot_size, robust=True)"
      ],
      "metadata": {
        "id": "zP3b2vGDgvMN",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean Up Resources"
      ],
      "metadata": {
        "id": "fgVnRY2tCzjt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "911406c1561e"
      },
      "outputs": [],
      "source": [
        "# @title Delete Temporal Buckets\n",
        "\n",
        "delete_bucket = True  # @param {type:\"boolean\"}\n",
        "if delete_bucket:\n",
        "    ! gsutil -m rm -r $BUCKET_URI\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_weather_prediction_on_vertex.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

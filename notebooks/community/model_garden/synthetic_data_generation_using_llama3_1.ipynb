{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoEDALsivNDl"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCo1J-gNwJM2"
      },
      "source": [
        "# Vertex AI Model Garden - Synthetic Data Generation using Llama 3.1\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fsynthetic_data_generation_using_llama3_1.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/synthetic_data_generation_using_llama3_1.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FlmJqqHyYwm"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates generating synthetic data using the [Llama 3.1 405B service API](https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama3-405b-instruct-maas).\n",
        "\n",
        "\n",
        "### Objective\n",
        "\n",
        "Leverage the Llama 3.1 405B service API to gnerate synthetic data. The framework is based on [Snowfakery](https://snowfakery.readthedocs.io/en/latest/) which is itself based on [Faker](https://faker.readthedocs.io/en/master/). It requires the expected outputs to be codified in a YAML file per Snowfakery specs, detailing all the required fields and their respective data generation strategies.\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlelO-pw5xaT"
      },
      "source": [
        "## Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjA_UYD25_1w"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project\n",
        "\n",
        "!pip install --upgrade --user -q openai snowfakery==3.6.2 wikipedia-api==0.6.0\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "# Define project information\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
        "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
        "\n",
        "# Cloud Storage bucket for storing the experiment artifacts.\n",
        "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
        "# prefer using your own GCS bucket, change the value yourself below.\n",
        "now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
        "\n",
        "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
        "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}\"\n",
        "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
        "else:\n",
        "    assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
        "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "    shell_output = ! gsutil ls -Lb {BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
        "    bucket_region = shell_output[0].strip().lower()\n",
        "    if bucket_region != REGION:\n",
        "        raise ValueError(\n",
        "            \"Bucket region %s is different from notebook region %s\"\n",
        "            % (bucket_region, REGION)\n",
        "        )\n",
        "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
        "\n",
        "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
        "MODEL_BUCKET = os.path.join(BUCKET_URI, \"llama_3_1\")\n",
        "\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user(project_id=PROJECT_ID)\n",
        "\n",
        "\n",
        "import google.auth\n",
        "\n",
        "# Programmatically get an access token\n",
        "creds, _ = google.auth.default(\n",
        "    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n",
        ")\n",
        "auth_req = google.auth.transport.requests.Request()\n",
        "creds.refresh(auth_req)\n",
        "# Note: the credential lives for 1 hour by default (https://cloud.google.com/docs/authentication/token-types#at-lifetime); after expiration, it must be refreshed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yl8gDtmA75hD"
      },
      "outputs": [],
      "source": [
        "# @title Creating Plugins and Prompts\n",
        "\n",
        "# @markdown The following cells create the 2 custom plugins we need for this use case along with the needed prompts.\n",
        "\n",
        "import logging\n",
        "import sys\n",
        "import types\n",
        "from io import StringIO\n",
        "\n",
        "import jinja2\n",
        "import openai\n",
        "import wikipediaapi\n",
        "from snowfakery import generate_data\n",
        "from snowfakery.plugins import SnowfakeryPlugin\n",
        "\n",
        "MODEL_ID = \"meta/llama3-405b-instruct-maas\"\n",
        "ENDPOINT = \"aiplatform.googleapis.com\"\n",
        "\n",
        "# Pass the Vertex endpoint and authentication to the OpenAI SDK\n",
        "client = openai.OpenAI(\n",
        "    base_url=f\"https://us-central1-{ENDPOINT}/v1beta1/projects/{PROJECT_ID}/locations/{REGION}/endpoints/openapi\",\n",
        "    api_key=creds.token,\n",
        ")\n",
        "\n",
        "\n",
        "class SyntheticDataGeneration:\n",
        "    \"\"\"\n",
        "    Implements all the extra functionality needed for this use-case\n",
        "    \"\"\"\n",
        "\n",
        "    # The first plugin allows us to interact with the Llama 3.1 405B service API.\n",
        "    class Plugins(types.ModuleType):\n",
        "        \"\"\"\n",
        "        Provides the plugins needed to extend Snowfakery\n",
        "        \"\"\"\n",
        "\n",
        "        class Llama3(SnowfakeryPlugin):\n",
        "            \"\"\"\n",
        "            Plugin for interacting with Llama3 API service.\n",
        "            \"\"\"\n",
        "\n",
        "            class Functions:\n",
        "                \"\"\"\n",
        "                Functions to implement field / object level data generation\n",
        "                \"\"\"\n",
        "\n",
        "                def fill_prompt(self, prompt_name: str, **kwargs) -> str:\n",
        "                    \"\"\"\n",
        "                    Returns a formatted prompt\n",
        "                    \"\"\"\n",
        "                    return (\n",
        "                        jinja2.Environment(\n",
        "                            loader=jinja2.FileSystemLoader(searchpath=\"./\")\n",
        "                        )\n",
        "                        .get_template(prompt_name)\n",
        "                        .render(**kwargs)\n",
        "                    )\n",
        "\n",
        "                def generate(\n",
        "                    self,\n",
        "                    prompt_name: str,\n",
        "                    model=\"Llama3\",\n",
        "                    temperature=0.9,\n",
        "                    top_p=1,\n",
        "                    **kwargs,\n",
        "                ) -> str | None:\n",
        "                    \"\"\"\n",
        "                    A wrapper around Llama3 plugin\n",
        "                    \"\"\"\n",
        "                    prompt = self.fill_prompt(prompt_name, **kwargs)\n",
        "                    try:\n",
        "                        response = client.chat.completions.create(\n",
        "                            model=MODEL_ID,\n",
        "                            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                            temperature=temperature,\n",
        "                            top_p=top_p,\n",
        "                        )\n",
        "                        return response.choices[0].message.content\n",
        "                    except Exception as e:\n",
        "                        logging.trace(\n",
        "                            (\n",
        "                                \"Unable to generate text using %s.\\n\"\n",
        "                                \"Prepared Prompt: \\n%s\\n\\nError: %s\"\n",
        "                            ),\n",
        "                            prompt_name,\n",
        "                            prompt,\n",
        "                            e,\n",
        "                        )\n",
        "                        return None\n",
        "\n",
        "        # The second plugin gives us the ability to interact with Wikipedia and fetch the contents for a given page.\n",
        "        class Wikipedia(SnowfakeryPlugin):\n",
        "            \"\"\"\n",
        "            Plugin for interacting with Wikipedia.\n",
        "            \"\"\"\n",
        "\n",
        "            class Functions:\n",
        "                \"\"\"\n",
        "                Implements a single function to fetch a Wikipedia page\n",
        "                \"\"\"\n",
        "\n",
        "                def get_page(self, title: str):\n",
        "                    \"\"\"\n",
        "                    Returns the title, URL and sections of the given wikipedia page\n",
        "                    \"\"\"\n",
        "                    logging.info(\"Parsing Wikipedia Page %s\", title)\n",
        "                    page = wikipediaapi.Wikipedia(\n",
        "                        \"Snowfakery (example@google.com)\", \"en\"\n",
        "                    ).page(title)\n",
        "                    results = {\"sections\": {}, \"title\": page.title, \"url\": page.fullurl}\n",
        "                    sections = [(s.title, s) for s in page.sections]\n",
        "                    while sections:\n",
        "                        sec_title, sec_obj = sections.pop()\n",
        "                        if sec_title in [\n",
        "                            \"External links\",\n",
        "                            \"References\",\n",
        "                            \"See also\",\n",
        "                            \"Further reading\",\n",
        "                        ]:\n",
        "                            continue\n",
        "                        if sec_obj.text:\n",
        "                            results[\"sections\"][sec_title] = sec_obj.text\n",
        "                        for sub_sec in sec_obj.sections:\n",
        "                            sections.append((f\"{sec_title} - {sub_sec.title}\", sub_sec))\n",
        "                    logging.info(\"Parsing Wikipedia Page %s Complete\", title)\n",
        "                    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSryene29Dan"
      },
      "outputs": [],
      "source": [
        "# @title Making plugins discoverable\n",
        "\n",
        "# @markdown We add the created class to sys.modules to ensure Snowfakery can find them and import them as modules as needed.\n",
        "\n",
        "sys.modules[\"SyntheticDataGeneration.Plugins\"] = SyntheticDataGeneration.Plugins(\n",
        "    name=\"SyntheticDataGeneration.Plugins\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJaPNlBi9bJr"
      },
      "outputs": [],
      "source": [
        "# @title Creating Prompt Templates\n",
        "\n",
        "%%writefile blog_generator.jinja\n",
        "You are an expert content creator who writes detailed, factual blogs.\n",
        "You have been asked to write a blog about {{idea_title}}.\n",
        "To get you started, you have also been given the following context about the topic:\n",
        "\n",
        "{{idea_body}}\n",
        "\n",
        "Ensure the blog that you write is interesting,detailed and factual.\n",
        "Take a deep breath and start writing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbOYKhD5-Bht"
      },
      "outputs": [],
      "source": [
        "%%writefile comment_generator.jinja\n",
        "You are {{first_name}} {{last_name}}. You are {{age}} years old. You are interested in {{interests}}. You work at {{organization}} as a {{profession}}.\n",
        "You came across the following article:\n",
        "\n",
        "{{blog_title}}\n",
        "\n",
        "{{blog_body}}\n",
        "\n",
        "Present your thoughts and feelings about the article in a short comment.\n",
        "\n",
        "Comment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPe-UoD_-Ilo"
      },
      "outputs": [],
      "source": [
        "# @title Creating the Recipe\n",
        "\n",
        "# @markdown In order to generate synthetic data, the schema of the synthetic data must be defined first. This is done by creating a recipe in a YAML format as demonstrated below, more details on writing recipes can be found [here](https://snowfakery.readthedocs.io/en/latest/#central-concepts).\n",
        "\n",
        "recipe = \"\"\"\n",
        "- plugin: SyntheticDataGeneration.Plugins.Wikipedia\n",
        "- plugin: SyntheticDataGeneration.Plugins.Llama3\n",
        "- option: wiki_title\n",
        "- var: __seed\n",
        "  value:\n",
        "    - Wikipedia.get_page :\n",
        "      title : ${{wiki_title}}\n",
        "\n",
        "- object : users\n",
        "  count : ${{random_number(min=100, max=500)}}\n",
        "  fields :\n",
        "    first_name : ${{fake.FirstName}}\n",
        "    last_name : ${{fake.FirstName}}\n",
        "    age:\n",
        "      random_number:\n",
        "        min: 18\n",
        "        max: 95\n",
        "    email : ${{fake.Email}}\n",
        "    phone : ${{fake.PhoneNumber}}\n",
        "    interests : ${{fake.Bs}}\n",
        "    postal_code : ${{fake.Postalcode}}\n",
        "    organization : ${{fake.Company}}\n",
        "    profession : ${{fake.Job}}\n",
        "\n",
        "- object : seeds\n",
        "  fields :\n",
        "    title : ${{__seed['title']}}\n",
        "    url : ${{__seed['url']}}\n",
        "    section_count : ${{__seed['sections'] | length}}\n",
        "\n",
        "  friends:\n",
        "    - object : blog_ideas\n",
        "      count : ${{seeds.section_count}}\n",
        "      fields :\n",
        "        seed_id : ${{seeds.id}}\n",
        "        section : ${{(__seed.sections.keys() | list)[child_index]}}\n",
        "        body : ${{__seed.sections[section]}}\n",
        "\n",
        "      friends:\n",
        "        - object : blog_posts\n",
        "          fields :\n",
        "            blog_idea_id : ${{blog_ideas.id}}\n",
        "            title : ${{seeds.title}} - ${{blog_ideas.section}}\n",
        "            body :\n",
        "              - Llama3.generate:\n",
        "                prompt_name : blog_generator.jinja\n",
        "                idea_title : ${{title}}\n",
        "                idea_body : ${{blog_ideas.body}}\n",
        "            author : Llama3\n",
        "\n",
        "          friends:\n",
        "            - object : blog_post_comments\n",
        "              fields :\n",
        "                blog_post_id : ${{blog_posts.id}}\n",
        "                author_id :\n",
        "                  random_reference : users\n",
        "                author_email : ${{author_id.email}}\n",
        "                comment :\n",
        "                  - Llama3.generate:\n",
        "                    prompt_name : comment_generator.jinja\n",
        "                    first_name : ${{author_id.first_name}}\n",
        "                    last_name : ${{author_id.last_name}}\n",
        "                    age : ${{author_id.age}}\n",
        "                    interests : ${{author_id.interests}}\n",
        "                    organization : ${{author_id.organization}}\n",
        "                    profession : ${{author_id.profession}}\n",
        "                    blog_title : ${{blog_posts.title}}\n",
        "                    blog_body : ${{blog_posts.body | truncate(1000)}}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTEQU-u--ei7"
      },
      "outputs": [],
      "source": [
        "# @title Generating Data\n",
        "\n",
        "generate_data(\n",
        "    StringIO(recipe),\n",
        "    output_format=\"csv\",\n",
        "    output_folder=\"outputs\",\n",
        "    user_options={\"wiki_title\": \"Python_(programming_language)\"},\n",
        ")\n",
        "\n",
        "# @markdown Results The synthetic data has been generated and stored as CSV files in the `outputs` folder."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "synthetic_data_generation_using_llama3_1.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

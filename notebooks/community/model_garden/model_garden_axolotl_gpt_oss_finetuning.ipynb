{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1jg2qBjVb4yf"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAeljAi7b4yg"
      },
      "source": [
        "# Vertex AI Model Garden - Fine-tune gpt-oss models with Axolotl\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_axolotl_gpt_oss_finetuning.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_axolotl_gpt_oss_finetuning.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5NXBxyjf1xs"
      },
      "source": [
        "## Overview\n",
        "This notebook demonstrates fine-tuning gpt-oss model using [Axolotl](https://github.com/axolotl-ai-cloud/axolotl). Axolotl streamlines AI model fine-tuning by providing a wide range of training recipes and supporting multiple configurations and architectures.\n",
        "\n",
        "### Objective\n",
        "- Train gpt-oss model using Axolotl with Vertex AI Training.\n",
        "- Deploy the trained model on Vertex AI and run predictions on Google Cloud.\n",
        "\n",
        "### Resources required\n",
        "The table below outlines the recommended machine specifications for different parts of the notebook to function correctly. Note that machine types with higher VRAM than recommended can also be used.\n",
        "> | Model | Vertex AI Finetuning | Vertex AI Deployment |\n",
        "| ----------- | ----------- | ----------- |\n",
        "| openai/gpt-oss-20b | a2-ultragpu-8g or a3-highgpu-8g | a3-highgpu-1g |\n",
        "\n",
        "Learn more about machine types by following [this doc](https://cloud.google.com/vertex-ai/docs/training/configure-compute#specifying_gpus).\n",
        "\n",
        "### File a bug\n",
        "\n",
        "File a bug on [GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/issues/new) if you encounter any issue with the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kik-beBAnNFq"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aDnYedjSIzVj"
      },
      "outputs": [],
      "source": [
        "# @title Import utility packages for fine-tuning\n",
        "\n",
        "# Upgrade Vertex AI SDK.\n",
        "! pip3 install --upgrade --quiet 'google-cloud-aiplatform==1.103.0'\n",
        "\n",
        "# Import the necessary packages.\n",
        "! rm -rf vertex-ai-samples && git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
        "! cd vertex-ai-samples\n",
        "\n",
        "# Import the necessary packages.\n",
        "\n",
        "import datetime\n",
        "import importlib\n",
        "import os\n",
        "import pathlib\n",
        "import time\n",
        "import uuid\n",
        "from typing import Tuple\n",
        "\n",
        "import requests\n",
        "import yaml\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "common_util = importlib.import_module(\n",
        "    \"vertex-ai-samples.notebooks.community.model_garden.docker_source_codes.notebook_util.common_util\"\n",
        ")\n",
        "\n",
        "\n",
        "def run_cmd_and_check_output(\n",
        "    cmd: list[str], env: dict[str, str] = None, input: str = \"\", cwd: str = None\n",
        "):\n",
        "    \"\"\"Runs the given command and raises exception if the command fails.\"\"\"\n",
        "    with subprocess.Popen(\n",
        "        cmd,\n",
        "        stdin=subprocess.PIPE,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True,\n",
        "        bufsize=1,\n",
        "        env=env,\n",
        "        cwd=cwd,\n",
        "    ) as p:\n",
        "        if input:\n",
        "            p.stdin.write(input)\n",
        "            p.stdin.flush()\n",
        "        p.stdin.close()\n",
        "        for line in p.stdout:\n",
        "            print(line, end=\"\", flush=True)\n",
        "    if p.returncode:\n",
        "        raise ValueError(\n",
        "            f\"Command '{' '.join(cmd)}' execution failed with return code {p.returncode}\"\n",
        "        )\n",
        "\n",
        "\n",
        "train_job = None\n",
        "models, endpoints = {}, {}\n",
        "HF_TOKEN = \"\"\n",
        "WORKING_DIR = os.getcwd()\n",
        "print(f\"Current working directory for notebook: {WORKING_DIR}\")\n",
        "from google import auth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "k32BrMnWnNFq"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown 2. For finetuning using Vertex AI, we will use Dynamic Workload Scheduler. Learn more about Dynamic workload scheduler [here](https://cloud.google.com/vertex-ai/docs/training/schedule-jobs-dws). For Dynamic Workload Scheduler, check the [us-central1](https://console.cloud.google.com/iam-admin/quotas?location=us-central1&metric=aiplatform.googleapis.com%2Fcustom_model_training_preemptible_nvidia_h100_gpus) or [europe-west4](https://console.cloud.google.com/iam-admin/quotas?location=europe-west4&metric=aiplatform.googleapis.com%2Fcustom_model_training_preemptible_nvidia_h100_gpus) quota for Nvidia H100 GPUs, [us-central1](https://console.cloud.google.com/iam-admin/quotas?location=us-central1&metric=aiplatform.googleapis.com%2Fcustom_model_training_preemptible_nvidia_a100_80gb_gpus) quota for Nvidia A100 80GB GPUs. If you do not have enough GPUs, then you can follow [these instructions](https://cloud.google.com/docs/quotas/view-manage#viewing_your_quota_console) to request quota.\n",
        "\n",
        "# @markdown 3. For serving, **[click here](https://console.cloud.google.com/iam-admin/quotas?location=us-central1&metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_l4_gpus)** to check if your project already has the required L4 GPUs in the us-central1 region. If you need more L4 GPUs for your project, then you can follow [these instructions](https://cloud.google.com/docs/quotas/view-manage#viewing_your_quota_console) to request more. Alternatively, if you want to run predictions with A100 80GB or H100 GPUs, we recommend using the regions listed below. **NOTE:** Make sure you have associated quota in selected regions. Click the links to see your current quota for each GPU type: [Nvidia A100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_a100_80gb_gpus), [Nvidia H100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_h100_gpus).\n",
        "\n",
        "# @markdown > | Machine Type | Accelerator Type | Recommended Regions |\n",
        "# @markdown | ----------- | ----------- | ----------- |\n",
        "# @markdown | a2-ultragpu-1g | 1 NVIDIA_A100_80GB | us-central1, us-east4, europe-west4, asia-southeast1, us-east4 |\n",
        "# @markdown | a3-highgpu-2g | 2 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "# @markdown | a3-highgpu-4g | 4 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "# @markdown | a3-highgpu-8g | 8 NVIDIA_H100_80GB | us-central1, us-east5, europe-west4, us-west1, asia-southeast1 |\n",
        "\n",
        "# @markdown 4. **[Optional]** [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs. Set the BUCKET_URI for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`) should be located in the same region as where the notebook was launched. Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\"). If not set, a unique GCS bucket will be created instead.\n",
        "\n",
        "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown 5. **[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment.\n",
        "\n",
        "REGION = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "if not REGION:\n",
        "    if not os.environ.get(\"GOOGLE_CLOUD_REGION\"):\n",
        "        raise ValueError(\n",
        "            \"REGION must be set. See\"\n",
        "            \" https://cloud.google.com/vertex-ai/docs/general/locations for\"\n",
        "            \" available cloud locations.\"\n",
        "        )\n",
        "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
        "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
        "\n",
        "# Cloud Storage bucket for storing the experiment artifacts.\n",
        "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
        "# prefer using your own GCS bucket, change the value yourself below.\n",
        "now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "\n",
        "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
        "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}-{str(uuid.uuid4())[:4]}\"\n",
        "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "    ! gcloud storage buckets create --location={REGION} {BUCKET_URI}\n",        "else:\n",
        "    assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
        "    shell_output = ! gcloud storage ls --full --buckets {BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",        "    bucket_region = shell_output[0].strip().lower()\n",
        "    if bucket_region != REGION:\n",
        "        raise ValueError(\n",
        "            \"Bucket region %s is different from notebook region %s\"\n",
        "            % (bucket_region, REGION)\n",
        "        )\n",
        "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
        "\n",
        "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
        "MODEL_BUCKET = os.path.join(BUCKET_URI, \"axolotl\")\n",
        "\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "# Gets the default SERVICE_ACCOUNT.\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
        "\n",
        "\n",
        "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
        "! # Note: Migrating scripts using gsutil iam ch is more complex than get or set. You need to replace the single iam ch command with a series of gcloud storage bucket add-iam-policy-binding and/or gcloud storage bucket remove-iam-policy-binding commands, or replicate the read-modify-write loop.\n! gcloud storage buckets add-iam-policy-binding $BUCKET_NAME --member=serviceAccount:{SERVICE_ACCOUNT} --role=roles/storage.admin\n",        "\n",
        "! gcloud config set project $PROJECT_ID\n",
        "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/storage.admin\"\n",
        "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/aiplatform.user\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7AfA9UsnNFq"
      },
      "source": [
        "## Finetune with Axolotl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5GzmhZdLIjTI"
      },
      "outputs": [],
      "source": [
        "# @title Set model to fine-tune\n",
        "# @markdown Note: This overrides Axolotl's `base_model` flag.\n",
        "HF_MODEL_ID = \"openai/gpt-oss-20b\"  # @param [\"openai/gpt-oss-20b\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_iTQ4nSlODZJ"
      },
      "outputs": [],
      "source": [
        "# @title Set Axolotl config\n",
        "\n",
        "# @markdown Axolotl is designed to work with YAML config files that contain everything you need to preprocess a dataset, train or fine-tune a model, run model inference or evaluation, and much more.\n",
        "# @markdown The gpt-oss Axolotl configs are taken from [examples directory](https://github.com/axolotl-ai-cloud/axolotl/blob/ba3dba3e4f6fbe845b0249f517c3bff88d898e22/examples/gpt-oss).\n",
        "\n",
        "# @markdown Suggestion for gpt-oss Axolotl configs:\n",
        "# @markdown > | Model | Recommended Axolotl Config |\n",
        "# @markdown | ----------- | ----------- |\n",
        "# @markdown | openai/gpt-oss-20b | examples/gpt-oss/gpt-oss-20b-sft-lora-singlegpu.yaml |\n",
        "\n",
        "\n",
        "# @markdown You can also customize the Axolotl config as per your requirements. To use a custom Axolotl config you can use `LOCAL` or `GCS` source option below.\n",
        "# @markdown Alternatively, you can specify github axolotl config and override flags using `Setup Axolotl Flags` section below.\n",
        "\n",
        "# @markdown 1. Set Axolotl config source.<br>\n",
        "# @markdown For **GITHUB** as source, you can explore different Axolotl configurations in the [examples directory](https://github.com/axolotl-ai-cloud/axolotl/tree/6ba5c0ed2c42a0e069b28c83646ee5a2a6904430/examples). For `GITHUB` source, `AXOLOTL_CONFIG_PATH` should start with `examples/`. e.g. \"examples/gpt-oss/gpt-oss-20b-sft-lora-singlegpu.yaml\".<br>\n",
        "# @markdown For **LOCAL** as source, create Axolotl config yaml file and specify correct path below. Note that, the local file will be copied to GCS bucket before running Vertex AI training job. For `LOCAL` source, `AXOLOTL_CONFIG_PATH` should be a absolute path of the config file, e.g. /content/lora.yml.<br>\n",
        "# @markdown For **GCS** as source, specify the GCS URI to the Axolotl config file. Make sure the file is accessible to service account used in the notebook. For `GCS` source, `AXOLOTL_CONFIG_PATH` should be a complete GCS URI of the config file, e.g. gs://bucket/path/to/config/file.yml.\n",
        "\n",
        "AXOLOTL_SOURCE = \"GITHUB\"  # @param [\"GITHUB\", \"LOCAL\", \"GCS\"]\n",
        "\n",
        "# @markdown 2. Set the Axolotl config file path.\n",
        "AXOLOTL_CONFIG_PATH = \"examples/gpt-oss/gpt-oss-20b-sft-lora-singlegpu.yaml\"  # @param [\"examples/gpt-oss/gpt-oss-20b-sft-lora-singlegpu.yaml\"] {allow-input: true}\n",
        "\n",
        "assert AXOLOTL_CONFIG_PATH, \"AXOLOTL_CONFIG_PATH must be set.\"\n",
        "\n",
        "if AXOLOTL_SOURCE == \"GITHUB\":\n",
        "    assert AXOLOTL_CONFIG_PATH.startswith(\n",
        "        \"examples/\"\n",
        "    ), \"AXOLOTL_CONFIG_PATH must start with examples/ for GITHUB source.\"\n",
        "    github_url = f\"https://github.com/axolotl-ai-cloud/axolotl/raw/9d5c95db6f4d883252fdb1183e82d0b354ff76a2/{AXOLOTL_CONFIG_PATH}\"\n",
        "    r = requests.get(github_url)\n",
        "    axolotl_config = r.content.decode(\"utf-8\")\n",
        "    axolotl_config = yaml.safe_load(axolotl_config)\n",
        "elif AXOLOTL_SOURCE == \"LOCAL\":\n",
        "    config_path = pathlib.Path(AXOLOTL_CONFIG_PATH)\n",
        "    assert config_path.exists(), \"AXOLOTL_CONFIG_PATH must exist for LOCAL source.\"\n",
        "    file_content = config_path.read_text()\n",
        "    axolotl_config = yaml.safe_load(file_content)\n",
        "elif AXOLOTL_SOURCE == \"GCS\":\n",
        "    local_path = pathlib.Path(f\"{WORKING_DIR}/tmp/axolotl_config.yml\")\n",
        "    common_util.download_gcs_file_to_local(AXOLOTL_CONFIG_PATH, local_path.absolute())\n",
        "    file_content = local_path.read_text()\n",
        "    axolotl_config = yaml.safe_load(file_content)\n",
        "    AXOLOTL_CONFIG_PATH = common_util.gcs_fuse_path(AXOLOTL_CONFIG_PATH)\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported AXOLOTL_SOURCE: {AXOLOTL_SOURCE}\")\n",
        "\n",
        "OUTPUT_GCS_URI = MODEL_BUCKET\n",
        "\n",
        "if not OUTPUT_GCS_URI.startswith(\"gs://\"):\n",
        "    OUTPUT_GCS_URI = f\"gs://{OUTPUT_GCS_URI}\"\n",
        "\n",
        "output_sub_dir = (\n",
        "    AXOLOTL_CONFIG_PATH.replace(\"/\", \"_\").replace(\".yaml\", \"\").replace(\".yml\", \"\")\n",
        ")\n",
        "BASE_AXOLOTL_OUTPUT_GCS_URI = f\"{OUTPUT_GCS_URI}/{output_sub_dir}/axolotl_output\"\n",
        "BASE_AXOLOTL_OUTPUT_DIR = common_util.gcs_fuse_path(BASE_AXOLOTL_OUTPUT_GCS_URI)\n",
        "\n",
        "# Placeholders for dataset settings.\n",
        "datasets = []\n",
        "test_datasets = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RTRV22DVJuMz"
      },
      "outputs": [],
      "source": [
        "# @title Setup HF token\n",
        "HF_TOKEN = \"\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4bp13RSoODZJ"
      },
      "outputs": [],
      "source": [
        "# @title **[Optional]** Setup dataset\n",
        "\n",
        "# @markdown This section configures the dataset used for fine-tuning.\n",
        "\n",
        "# @markdown **Note: If you don't fill any of the dataset options given below, then the dataset used will be the one defined in the Axolotl config file.** You have two options to configure the dataset:\n",
        "\n",
        "# @markdown **1. Use a Hugging Face Dataset**\n",
        "# @markdown   - Requires specifying the dataset name and type.\n",
        "\n",
        "# @markdown **2. Load from Google Cloud Storage (GCS)**\n",
        "# @markdown   - Requires specifying the bucket name, dataset type, file type, and paths to training/test splits.\n",
        "\n",
        "# @markdown **Choose ONE of the following options:**\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown **Option 1: Hugging Face**\n",
        "\n",
        "# @markdown **Hugging Face Dataset Name:**\n",
        "HF_DATASET = \"\"  # @param {type:\"string\", placeholder: \"e.g. trl-lib/chatbot_arena_completions\"}\n",
        "# @markdown **Set the dataset type:** Refer to [Axolotl config file](https://github.com/axolotl-ai-cloud/axolotl/blob/6ba5c0ed2c42a0e069b28c83646ee5a2a6904430/docs/config.qmd#L140) for more details.\n",
        "HF_DATASET_TYPE = \"\"  # @param {type:\"string\", placeholder: \"e.g. chat_template\"}\n",
        "if HF_DATASET:\n",
        "    assert HF_DATASET_TYPE, \"HF_DATASET_TYPE must be set if HF_DATASET is set.\"\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown **Option 2: GCS**\n",
        "\n",
        "# @markdown **Path to Training Data :**\n",
        "\n",
        "# @markdown E.g. `gs://cloud-samples-data/vertex-ai/model-garden/datasets/vertex-sample-chat-train.jsonl`\n",
        "TRAIN_DATASET_PATH = \"\"  # @param {type:\"string\"}\n",
        "# @markdown **File Type**. Refer to the [Axolotl config file](https://github.com/axolotl-ai-cloud/axolotl/blob/6ba5c0ed2c42a0e069b28c83646ee5a2a6904430/docs/config.qmd#L103).\n",
        "FILE_TYPE = \"\"  # @param {type:\"string\", placeholder: \"e.g. json\"}\n",
        "# @markdown **Messages Column**. Refer to the [Axolotl config file](https://github.com/axolotl-ai-cloud/axolotl/blob/6ba5c0ed2c42a0e069b28c83646ee5a2a6904430/docs/config.qmd#L155).\n",
        "MESSAGES_COLUMN = \"messages\"  # @param {type:\"string\", placeholder: \"e.g. messages\"}\n",
        "\n",
        "# @markdown **[Optional] Path to Test Data :**\n",
        "# @markdown To use a dedicated validation set, provide the file path. Otherwise, the training data will be split to create a validation set.\n",
        "\n",
        "# @markdown E.g. `gs://cloud-samples-data/vertex-ai/model-garden/datasets/vertex-sample-chat-validation.jsonl`\n",
        "TEST_DATASET_PATH = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "if TRAIN_DATASET_PATH:\n",
        "    assert FILE_TYPE, \"FILE_TYPE must be set if TRAIN_DATASET_PATH is set.\"\n",
        "\n",
        "if TEST_DATASET_PATH:\n",
        "    assert (\n",
        "        TRAIN_DATASET_PATH\n",
        "    ), \"TRAIN_DATASET_PATH must be set if TEST_DATASET_PATH is set.\"\n",
        "\n",
        "assert not (\n",
        "    HF_DATASET and TRAIN_DATASET_PATH\n",
        "), \"Only one of HF_DATASET or TRAIN_DATASET_PATH can be set.\"\n",
        "\n",
        "datasets = []\n",
        "test_datasets = []\n",
        "\n",
        "if TRAIN_DATASET_PATH:\n",
        "    dataset = {\n",
        "        \"path\": TRAIN_DATASET_PATH,\n",
        "        \"ds_type\": FILE_TYPE,\n",
        "        \"type\": \"chat_template\",\n",
        "        \"field_messages\": MESSAGES_COLUMN,\n",
        "    }\n",
        "    datasets.append(dataset)\n",
        "\n",
        "if TEST_DATASET_PATH:\n",
        "    dataset = {\n",
        "        \"path\": TEST_DATASET_PATH,\n",
        "        \"ds_type\": FILE_TYPE,\n",
        "        \"type\": \"chat_template\",\n",
        "        \"split\": \"train\",\n",
        "        \"field_messages\": MESSAGES_COLUMN,\n",
        "    }\n",
        "    test_datasets.append(dataset)\n",
        "\n",
        "if HF_DATASET:\n",
        "    datasets.append({\"path\": HF_DATASET, \"type\": HF_DATASET_TYPE})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3SOjc9q8ODZJ"
      },
      "outputs": [],
      "source": [
        "# @title Setup Axolotl Flags\n",
        "# @markdown This section configures additional Axolotl flags. You can explore different Axolotl flags in the [Axolotl config file](https://github.com/axolotl-ai-cloud/axolotl/blob/6ba5c0ed2c42a0e069b28c83646ee5a2a6904430/docs/config.qmd).\n",
        "\n",
        "# @markdown **To avoid OOM, you can reduce sequence length.** This can be done by setting `sequence_len` flag to some smaller value. But reducing sequence length might also reduce the fine-tuned model's quality.\n",
        "# @markdown **Another alternative to avoid OOM is to use higher memory GPU.** It is recommended to use Vertex AI training for higher memory GPUs like A100 and H100. Vertex AI training offers greater availability of high-end GPUs.\n",
        "\n",
        "# @markdown **Training can take a long time (20+ hours) to complete depending on the model, dataset and axololt config.** You can reduce the training time by reducing the max training steps. This can be done by setting `max_steps` flag to some smaller value. Note that, this might also reduce the fine-tuned model's quality.\n",
        "\n",
        "# @markdown For example, if you want to run only single step of training, then you can set `[\"--use-tensorboard=True\", \"--max_steps=1\"]` in the `axolotl_flag_overrides` to achieve that.\n",
        "\n",
        "axolotl_flag_overrides = [\"--use-tensorboard=True\"]  # @param {type:\"raw\"}\n",
        "assert type(axolotl_flag_overrides) is list, \"axolotl_flag_overrides must be a list.\"\n",
        "\n",
        "axolotl_flag_overrides.append(f\"--base_model={HF_MODEL_ID}\")\n",
        "\n",
        "\n",
        "# Check if duplicate flags are passed.\n",
        "flags_seen = set()\n",
        "for flag in axolotl_flag_overrides:\n",
        "    if flag in flags_seen:\n",
        "        raise ValueError(f\"Duplicate flag: {flag}\")\n",
        "    flags_seen.add(flag)\n",
        "\n",
        "base_model = axolotl_config[\"base_model\"]\n",
        "for overrides in axolotl_flag_overrides:\n",
        "    if overrides.startswith(\"--base_model=\"):\n",
        "        base_model = overrides.split(\"=\")[1]\n",
        "        break\n",
        "publisher = base_model.split(\"/\")[0]\n",
        "model_id = base_model.split(\"/\")[1]\n",
        "model_id = model_id.replace(\".\", \"-\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H7V8fEjODZK"
      },
      "source": [
        "### Finetune with Vertex AI Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0XqH_Mvelhon"
      },
      "outputs": [],
      "source": [
        "# @title Vertex AI fine-tuning job\n",
        "# @markdown This section runs the Axolotl training using Vertex AI training job.\n",
        "# @markdown **Note: This section can take a long time to run. You can reduce the training time by reducing the max training steps as mentioned in `Setup Axolotl Flags` section.**\n",
        "# @markdown Refer to [Axolotl config](https://axolotl-ai-cloud.github.io/axolotl/docs/config.html) to override additional Axolotl flags.\n",
        "\n",
        "from google.cloud.aiplatform.compat.types import \\\n",
        "    custom_job as gca_custom_job_compat\n",
        "\n",
        "# @markdown Acceletor type to use for training.\n",
        "training_accelerator_type = \"NVIDIA_H100_80GB\"  # @param [\"NVIDIA_H100_80GB\", \"NVIDIA_A100_80GB\"]\n",
        "\n",
        "\n",
        "replica_count = 1\n",
        "repo = \"us-docker.pkg.dev/vertex-ai\"\n",
        "per_node_accelerator_count = 8\n",
        "boot_disk_size_gb = 500\n",
        "dws_kwargs = {\n",
        "    \"max_wait_duration\": 5400,  # 90 minutes\n",
        "    \"scheduling_strategy\": gca_custom_job_compat.Scheduling.Strategy.FLEX_START,\n",
        "}\n",
        "is_dynamic_workload_scheduler = True\n",
        "if training_accelerator_type == \"NVIDIA_A100_80GB\":\n",
        "    training_machine_type = \"a2-ultragpu-8g\"\n",
        "elif training_accelerator_type == \"NVIDIA_H100_80GB\":\n",
        "    training_machine_type = \"a3-highgpu-8g\"\n",
        "    boot_disk_size_gb = 2000\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported accelerator type: {training_accelerator_type}\")\n",
        "\n",
        "TRAIN_DOCKER_URI = (\n",
        "    f\"{repo}/vertex-vision-model-garden-dockers/axolotl-train-dws:20250812-1800-rc1\"\n",
        ")\n",
        "\n",
        "common_util.check_quota(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    accelerator_type=training_accelerator_type,\n",
        "    accelerator_count=per_node_accelerator_count * replica_count,\n",
        "    is_for_training=True,\n",
        "    is_restricted_image=False,\n",
        "    is_dynamic_workload_scheduler=is_dynamic_workload_scheduler,\n",
        ")\n",
        "\n",
        "vertex_ai_config_path = AXOLOTL_CONFIG_PATH\n",
        "# Copy the config file to the bucket.\n",
        "if AXOLOTL_SOURCE == \"LOCAL\":\n",
        "    ! gcloud storage cp $AXOLOTL_CONFIG_PATH $MODEL_BUCKET/config/\n",        "    vertex_ai_config_path = f\"{common_util.gcs_fuse_path(MODEL_BUCKET)}/config/{pathlib.Path(AXOLOTL_CONFIG_PATH).name}\"\n",
        "\n",
        "job_name = common_util.get_job_name_with_datetime(\"axolotl-train\")\n",
        "AXOLOTL_OUTPUT_GCS_URI = f\"{BASE_AXOLOTL_OUTPUT_GCS_URI}/{job_name}\"\n",
        "AXOLOTL_OUTPUT_DIR = f\"{BASE_AXOLOTL_OUTPUT_DIR}/{job_name}\"\n",
        "\n",
        "TRAINING_JOB_OUTPUT_DIR = f\"{AXOLOTL_OUTPUT_GCS_URI}/training_job_output\"\n",
        "\n",
        "# Set Axolotl flags.\n",
        "axolotl_config_overwrites = []\n",
        "axolotl_config_overwrites.append(f\"--output_dir={AXOLOTL_OUTPUT_DIR}\")\n",
        "if len(datasets) > 0:\n",
        "    axolotl_config_overwrites.append(f\"--datasets={datasets}\")\n",
        "if len(test_datasets) > 0:\n",
        "    axolotl_config_overwrites.append(f\"--test_datasets={test_datasets}\")\n",
        "    axolotl_config_overwrites.append(\"--val_set_size=0\")\n",
        "axolotl_config_overwrites += axolotl_flag_overrides\n",
        "\n",
        "train_job_args = []\n",
        "train_job_args.append(f\"--axolotl_config_path={vertex_ai_config_path}\")\n",
        "train_job_args += axolotl_config_overwrites\n",
        "if HF_TOKEN:\n",
        "    train_job_args.append(f\"--huggingface_access_token={HF_TOKEN}\")\n",
        "\n",
        "job_name = common_util.get_job_name_with_datetime(\"axolotl-train\")\n",
        "\n",
        "# Add labels for the finetuning job.\n",
        "labels = {\n",
        "    \"mg-source\": \"notebook\",\n",
        "    \"mg-notebook-name\": \"model_garden_axolotl_gpt_oss_finetuning.ipynb\".split(\".\")[0],\n",
        "}\n",
        "\n",
        "model_name = AXOLOTL_CONFIG_PATH.split(\"/\")[1]\n",
        "labels[\"mg-tune\"] = f\"publishers-{publisher}-models-{model_name}\".lower()\n",
        "labels[\"versioned-mg-tune\"] = f\"{labels['mg-tune']}-{model_id}\".lower()\n",
        "labels[\"versioned-mg-tune\"] = labels[\"versioned-mg-tune\"][\n",
        "    : min(len(labels[\"versioned-mg-tune\"]), 63)\n",
        "]\n",
        "\n",
        "\n",
        "# Pass training arguments and launch job.\n",
        "train_job = aiplatform.CustomContainerTrainingJob(\n",
        "    display_name=job_name,\n",
        "    container_uri=TRAIN_DOCKER_URI,\n",
        "    labels=labels,\n",
        ")\n",
        "\n",
        "# Run Vertex AI job.\n",
        "print(\"Running training job with args:\")\n",
        "print(\" \\\\\\n\".join(train_job_args))\n",
        "train_job.run(\n",
        "    args=train_job_args,\n",
        "    replica_count=replica_count,\n",
        "    machine_type=training_machine_type,\n",
        "    accelerator_type=training_accelerator_type,\n",
        "    accelerator_count=per_node_accelerator_count,\n",
        "    boot_disk_size_gb=boot_disk_size_gb,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    base_output_dir=TRAINING_JOB_OUTPUT_DIR,\n",
        "    sync=False,  # Non-blocking call to run.\n",
        "    **dws_kwargs,\n",
        ")\n",
        "\n",
        "# Wait until resource has been created.\n",
        "train_job.wait_for_resource_creation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15BI63V2IIha"
      },
      "source": [
        "### Run TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RcSJinPWXy9_"
      },
      "outputs": [],
      "source": [
        "# @markdown This section shows how to launch TensorBoard in a [Cloud Shell](https://cloud.google.com/shell/docs).\n",
        "# @markdown 1. Click the Cloud Shell icon(![terminal](https://github.com/google/material-design-icons/blob/master/png/action/terminal/materialicons/24dp/1x/baseline_terminal_black_24dp.png?raw=true)) on the top right to open the Cloud Shell.\n",
        "# @markdown 2. Copy the `tensorboard` command shown below by running this cell.\n",
        "# @markdown 3. Paste and run the command in the Cloud Shell to launch TensorBoard.\n",
        "# @markdown 4. Once the command runs (You may have to click `Authorize` if prompted), click the link starting with `http://localhost`.\n",
        "\n",
        "# @markdown Note: You may need to wait around 10 minutes after the job starts in order for the TensorBoard logs to be written to the GCS bucket.\n",
        "print(f\"Command to copy: tensorboard --logdir {AXOLOTL_OUTPUT_GCS_URI}/node-0/runs/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO1BYNnfXy9_"
      },
      "source": [
        "## Deploy using SGLang"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Up326e7kXy9_"
      },
      "outputs": [],
      "source": [
        "# @markdown 1. Wait for the training job to finish.\n",
        "if train_job and train_job.end_time is None:\n",
        "    print(\"Waiting for the training job to finish...\")\n",
        "    train_job.wait()\n",
        "    print(\"The training job has finished.\")\n",
        "\n",
        "# @markdown 2. Set up SGLang docker URI and model gcs uri.\n",
        "\n",
        "\n",
        "SGLANG_DOCKER_URI = \"us-docker.pkg.dev/deeplearning-platform-release/vertex-model-garden/sglang-serve.cu124.0-4.ubuntu2204.py310:model-garden.sglang-0-4-release_20250810.00_p0\"\n",
        "SGLANG_MODEL_GCS_URI = AXOLOTL_OUTPUT_GCS_URI\n",
        "\n",
        "if \"adapter\" in axolotl_config and (\n",
        "    axolotl_config[\"adapter\"] == \"lora\" or axolotl_config[\"adapter\"] == \"qlora\"\n",
        "):\n",
        "    SGLANG_MODEL_GCS_URI = f\"{AXOLOTL_OUTPUT_GCS_URI}/node-0/merged\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulsUk-xkXy9_"
      },
      "source": [
        "### Create model endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KSW-gb6nXy9_"
      },
      "outputs": [],
      "source": [
        "# @markdown This section uploads the model to Model Registry and deploys it on the Endpoint. It takes 15 minutes to 1 hour to finish.\n",
        "# @markdown 1. Set the machine type and accelerator type.\n",
        "# @markdown Find Vertex AI prediction supported accelerators and regions [here](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute).\n",
        "\n",
        "if \"20b\" in HF_MODEL_ID:\n",
        "    accelerator_type = \"NVIDIA_H100_80GB\"\n",
        "    machine_type = \"a3-highgpu-1g\"\n",
        "    per_node_accelerator_count = 1\n",
        "else:\n",
        "    raise ValueError(\n",
        "        f\"Recommended machine settings not found for model: {HF_MODEL_ID}.\"\n",
        "    )\n",
        "\n",
        "common_util.check_quota(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=per_node_accelerator_count,\n",
        "    is_for_training=False,\n",
        ")\n",
        "\n",
        "# @markdown Set `use_dedicated_endpoint` to False if you don't want to use [dedicated endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment#create-dedicated-endpoint).\n",
        "use_dedicated_endpoint = True  # @param {type:\"boolean\"}\n",
        "\n",
        "gpu_memory_utilization = 0.95\n",
        "max_model_len = 131072\n",
        "if \"1b\" in HF_MODEL_ID:\n",
        "    max_model_len = 32768\n",
        "\n",
        "\n",
        "def poll_operation(op_name: str) -> bool:  # noqa: F811\n",
        "    creds, _ = auth.default()\n",
        "    auth_req = auth.transport.requests.Request()\n",
        "    creds.refresh(auth_req)\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {creds.token}\",\n",
        "    }\n",
        "    get_resp = requests.get(\n",
        "        f\"https://{REGION}-aiplatform.googleapis.com/ui/{op_name}\",\n",
        "        headers=headers,\n",
        "    )\n",
        "    opjs = get_resp.json()\n",
        "    if \"error\" in opjs:\n",
        "        raise ValueError(f\"Operation failed: {opjs['error']}\")\n",
        "    return opjs.get(\"done\", False)\n",
        "\n",
        "\n",
        "def poll_and_wait(op_name: str, total_wait: int, interval: int = 60):  # noqa: F811\n",
        "    waited = 0\n",
        "    while not poll_operation(op_name):\n",
        "        if waited > total_wait:\n",
        "            raise TimeoutError(\"Operation timed out\")\n",
        "        print(\n",
        "            f\"\\rStill waiting for operation... Waited time in second: {waited:<6}\",\n",
        "            end=\"\",\n",
        "            flush=True,\n",
        "        )\n",
        "        waited += interval\n",
        "        time.sleep(interval)\n",
        "\n",
        "\n",
        "def deploy_model_sglang_multihost(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    publisher: str,\n",
        "    publisher_model_id: str,\n",
        "    service_account: str = \"\",\n",
        "    base_model_id: str = \"\",\n",
        "    machine_type: str = \"g2-standard-8\",\n",
        "    accelerator_type: str = \"NVIDIA_L4\",\n",
        "    accelerator_count: int = 1,\n",
        "    multihost_gpu_node_count: int = 1,\n",
        "    gpu_memory_utilization: float | None = None,\n",
        "    context_length: int | None = None,\n",
        "    dtype: str | None = None,\n",
        "    quantization: str | None = None,\n",
        "    enable_trust_remote_code: bool = False,\n",
        "    enable_torch_compile: bool = False,\n",
        "    torch_compile_max_bs: int | None = None,\n",
        "    attention_backend: str = \"\",\n",
        "    enable_flashinfer_mla: bool = False,\n",
        "    disable_cuda_graph: bool = False,\n",
        "    speculative_algorithm: str | None = None,\n",
        "    speculative_draft_model_path: str = \"\",\n",
        "    speculative_num_steps: int = 3,\n",
        "    speculative_eagle_topk: int = 1,\n",
        "    speculative_num_draft_tokens: int = 4,\n",
        "    enable_jit_deepgemm: bool = False,\n",
        "    enable_dp_attention: bool = False,\n",
        "    dp_size: int = 1,\n",
        "    enable_multimodal: bool = False,\n",
        "    use_dedicated_endpoint: bool = False,\n",
        "    max_num_seqs: int | None = None,\n",
        "    is_spot: bool = True,\n",
        "    tool_call_parser: str | None = None,\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys trained models with SGLang into Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(\n",
        "        display_name=f\"{model_name}-endpoint\",\n",
        "        dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
        "    )\n",
        "\n",
        "    if not base_model_id:\n",
        "        base_model_id = model_id\n",
        "\n",
        "    # See https://docs.sglang.ai/backend/server_arguments.html for a list of possible arguments with descriptions.\n",
        "    sglang_args = [\n",
        "        f\"--model={model_id}\",\n",
        "        f\"--tp={accelerator_count * multihost_gpu_node_count}\",\n",
        "        f\"--dp={dp_size}\",\n",
        "    ]\n",
        "\n",
        "    if context_length:\n",
        "        sglang_args.append(f\"--context-length={context_length}\")\n",
        "\n",
        "    if gpu_memory_utilization:\n",
        "        sglang_args.append(f\"--mem-fraction-static={gpu_memory_utilization}\")\n",
        "\n",
        "    if max_num_seqs:\n",
        "        sglang_args.append(f\"--max-running-requests={max_num_seqs}\")\n",
        "\n",
        "    if dtype:\n",
        "        sglang_args.append(f\"--dtype={dtype}\")\n",
        "\n",
        "    if quantization:\n",
        "        sglang_args.append(f\"--quantization={quantization}\")\n",
        "\n",
        "    if enable_trust_remote_code:\n",
        "        sglang_args.append(\"--trust-remote-code\")\n",
        "\n",
        "    if enable_torch_compile:\n",
        "        sglang_args.append(\"--enable-torch-compile\")\n",
        "        if torch_compile_max_bs:\n",
        "            sglang_args.append(f\"--torch-compile-max-bs={torch_compile_max_bs}\")\n",
        "\n",
        "    if attention_backend:\n",
        "        sglang_args.append(f\"--attention-backend={attention_backend}\")\n",
        "\n",
        "    if enable_flashinfer_mla:\n",
        "        sglang_args.append(\"--enable-flashinfer-mla\")\n",
        "\n",
        "    if disable_cuda_graph:\n",
        "        sglang_args.append(\"--disable-cuda-graph\")\n",
        "\n",
        "    if speculative_algorithm:\n",
        "        sglang_args.append(f\"--speculative-algorithm={speculative_algorithm}\")\n",
        "        sglang_args.append(\n",
        "            f\"--speculative-draft-model-path={speculative_draft_model_path}\"\n",
        "        )\n",
        "        sglang_args.append(f\"--speculative-num-steps={speculative_num_steps}\")\n",
        "        sglang_args.append(f\"--speculative-eagle-topk={speculative_eagle_topk}\")\n",
        "        sglang_args.append(\n",
        "            f\"--speculative-num-draft-tokens={speculative_num_draft_tokens}\"\n",
        "        )\n",
        "\n",
        "    if enable_dp_attention:\n",
        "        sglang_args.append(\"--enable-dp-attention\")\n",
        "\n",
        "    if enable_multimodal:\n",
        "        sglang_args.append(\"--enable-multimodal\")\n",
        "\n",
        "    if tool_call_parser:\n",
        "        sglang_args.append(f\"--tool-call-parser={tool_call_parser}\")\n",
        "\n",
        "    env_vars = {\n",
        "        \"MODEL_ID\": base_model_id,\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "    }\n",
        "\n",
        "    if enable_jit_deepgemm:\n",
        "        env_vars[\"SGL_ENABLE_JIT_DEEPGEMM\"] = \"1\"\n",
        "\n",
        "    # HF_TOKEN is not a compulsory field and may not be defined.\n",
        "    try:\n",
        "        if HF_TOKEN:\n",
        "            env_vars[\"HF_TOKEN\"] = HF_TOKEN\n",
        "    except NameError:\n",
        "        pass\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=SGLANG_DOCKER_URI,\n",
        "        serving_container_args=sglang_args,\n",
        "        serving_container_ports=[30000],\n",
        "        serving_container_predict_route=\"/vertex_generate\",\n",
        "        serving_container_health_route=\"/health\",\n",
        "        serving_container_environment_variables=env_vars,\n",
        "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
        "        serving_container_deployment_timeout=7200,\n",
        "        model_garden_source_model_name=(\n",
        "            f\"publishers/{publisher}/models/{publisher_model_id}\"\n",
        "        ),\n",
        "    )\n",
        "    print(\n",
        "        f\"Deploying {model_name} on {machine_type} with {int(accelerator_count * multihost_gpu_node_count)} {accelerator_type} GPU(s).\"\n",
        "    )\n",
        "\n",
        "    creds, _ = auth.default()\n",
        "    auth_req = auth.transport.requests.Request()\n",
        "    creds.refresh(auth_req)\n",
        "\n",
        "    url = f\"https://{REGION}-aiplatform.googleapis.com/ui/projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint.name}:deployModel\"\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {creds.token}\",\n",
        "    }\n",
        "    data = {\n",
        "        \"deployedModel\": {\n",
        "            \"model\": model.resource_name,\n",
        "            \"displayName\": model_name,\n",
        "            \"dedicatedResources\": {\n",
        "                \"machineSpec\": {\n",
        "                    \"machineType\": machine_type,\n",
        "                    \"multihostGpuNodeCount\": multihost_gpu_node_count,\n",
        "                    \"acceleratorType\": accelerator_type,\n",
        "                    \"acceleratorCount\": accelerator_count,\n",
        "                },\n",
        "                \"minReplicaCount\": 1,\n",
        "                \"maxReplicaCount\": 1,\n",
        "            },\n",
        "            \"system_labels\": {\n",
        "                \"NOTEBOOK_NAME\": \"model_garden_axolotl_gpt_oss_finetuning.ipynb\",\n",
        "                \"NOTEBOOK_ENVIRONMENT\": common_util.get_deploy_source(),\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "    if service_account:\n",
        "        data[\"deployedModel\"][\"serviceAccount\"] = service_account\n",
        "    if is_spot:\n",
        "        data[\"deployedModel\"][\"dedicatedResources\"][\"spot\"] = True\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "    print(f\"Deploy Model response: {response.json()}\")\n",
        "    if response.status_code != 200 or \"name\" not in response.json():\n",
        "        raise ValueError(f\"Failed to deploy model: {response.text}\")\n",
        "    poll_and_wait(response.json()[\"name\"], 7200)\n",
        "    print(\"endpoint_name:\", endpoint.name)\n",
        "\n",
        "    return model, endpoint\n",
        "\n",
        "\n",
        "models[\"sglang_gpu\"], endpoints[\"sglang_gpu\"] = deploy_model_sglang_multihost(\n",
        "    model_name=common_util.get_job_name_with_datetime(prefix=\"axolotl-sglang-serve\"),\n",
        "    model_id=SGLANG_MODEL_GCS_URI,\n",
        "    publisher=publisher.lower(),\n",
        "    publisher_model_id=model_id.lower(),\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=per_node_accelerator_count,\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO76I0p3Xy9_"
      },
      "source": [
        "### Perform Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kC9Apto7Xy9_"
      },
      "outputs": [],
      "source": [
        "# @title Raw predict\n",
        "\n",
        "\n",
        "# @markdown Once deployment succeeds, you can send requests to the endpoint with text prompts. Sampling parameters supported by SGLang can be found [here](https://docs.sglang.ai/backend/sampling_params.html).\n",
        "\n",
        "# @markdown Example:\n",
        "\n",
        "# @markdown ```\n",
        "# @markdown User: What is the best way to diagnose and fix a flickering light in my house?\n",
        "# @markdown Assistant: Okay, so I need to figure out how to diagnose and fix a flickering light in my house. Hmm, where do I start? Let's think. First, I remember that flickering lights can be caused by various issues. Maybe the bulb is loose? That's a common problem. Let me start with the simplest things first.\n",
        "# @markdown ```\n",
        "# @markdown Additionally, you can moderate the generated text with Vertex AI. See [Moderate text documentation](https://cloud.google.com/natural-language/docs/moderating-text) for more details.\n",
        "\n",
        "# Loads an existing endpoint instance using the endpoint name:\n",
        "# - Using `endpoint_name = endpoint.name` allows us to get the\n",
        "#   endpoint name of the endpoint `endpoint` created in the cell\n",
        "#   above.\n",
        "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
        "#   an existing endpoint with the ID 1234567890123456789.\n",
        "# You may uncomment the code below to load an existing endpoint.\n",
        "\n",
        "# endpoint_name = \"\"  # @param {type:\"string\"}\n",
        "# aip_endpoint_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
        "# )\n",
        "# endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
        "\n",
        "prompt = \"What is a car?\"  # @param {type: \"string\"}\n",
        "# @markdown If you encounter an issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, by lowering `max_tokens`.\n",
        "max_new_tokens = 1024  # @param {type:\"integer\"}\n",
        "temperature = 0.6  # @param {type:\"number\"}\n",
        "top_p = 0.95  # @param {type:\"number\"}\n",
        "\n",
        "# Overrides parameters for inferences.\n",
        "instances = [{\"text\": prompt}]\n",
        "parameters = {\n",
        "    \"sampling_params\": {\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": top_p,\n",
        "    }\n",
        "}\n",
        "\n",
        "response = endpoints[\"sglang_gpu\"].predict(\n",
        "    instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
        ")\n",
        "\n",
        "for prediction in response.predictions:\n",
        "    print(prediction)\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2grDDphYx4zI"
      },
      "source": [
        "## Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_Pqw3TsF2uG4"
      },
      "outputs": [],
      "source": [
        "# @markdown Delete the training job.\n",
        "\n",
        "if train_job:\n",
        "    train_job.delete()\n",
        "\n",
        "# @markdown  Delete the experiment models and endpoints to recycle the resources\n",
        "# @markdown  and avoid unnecessary continuous charges that may incur.\n",
        "\n",
        "# Undeploy model and delete endpoint.\n",
        "for endpoint in endpoints.values():\n",
        "    endpoint.delete(force=True)\n",
        "\n",
        "# Delete models.\n",
        "for model in models.values():\n",
        "    model.delete()\n",
        "\n",
        "delete_bucket = False  # @param {type:\"boolean\"}\n",
        "if delete_bucket:\n",
        "    ! gcloud storage rm --recursive $BUCKET_NAME"      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_axolotl_gpt_oss_finetuning.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

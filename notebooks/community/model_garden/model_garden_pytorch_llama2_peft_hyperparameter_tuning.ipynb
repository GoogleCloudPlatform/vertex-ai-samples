{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d9bbf86da5e"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Vertex AI Model Garden - LLaMA2 (PEFT Hyperparameter Tuning)\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_pytorch_llama2_peft_hyperparameter_tuning.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_llama2_peft_hyperparameter_tuning.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates downloading [LLaMA2 models](https://huggingface.co/meta-llama), and tuning LLaMA2 model hyperparameters with parameter efficient finetuning libraries ([PEFT](https://github.com/huggingface/peft)) on Vertex AI.\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Download prebuilt LLaMA2 models.\n",
        "- Tune LLaMA2 model hyperparameters with Vertex AI SDK.\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "264c07757582"
      },
      "source": [
        "## Run the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IoPsYDwDdFBf"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown 2. [Optional] [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs. Set the BUCKET_URI for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`) should be located in the same region as where the notebook was launched. Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\"). If not set, a unique GCS bucket will be created instead.\n",
        "# Import the necessary packages\n",
        "import os\n",
        "from datetime import datetime\n",
        "from typing import Tuple\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
        "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
        "\n",
        "# Cloud Storage bucket for storing the experiment artifacts.\n",
        "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
        "# prefer using your own GCS bucket, please change the value yourself below.\n",
        "now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
        "\n",
        "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
        "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}\"\n",
        "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
        "else:\n",
        "    assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
        "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "    shell_output = ! gsutil ls -Lb {BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
        "    bucket_region = shell_output[0].strip().lower()\n",
        "    if bucket_region != REGION:\n",
        "        raise ValueError(\n",
        "            \"Bucket region %s is different from notebook region %s\"\n",
        "            % (bucket_region, REGION)\n",
        "        )\n",
        "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
        "\n",
        "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
        "EXPERIMENT_BUCKET = os.path.join(BUCKET_URI, \"peft\")\n",
        "MODEL_BUCKET = os.path.join(BUCKET_URI, \"llama2\")\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "# Gets the default BUCKET_URI and SERVICE_ACCOUNT if they were not specified by the user.\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
        "\n",
        "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
        "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
        "\n",
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "esW92m2vEdOd"
      },
      "outputs": [],
      "source": [
        "# @title Access LLaMA2 models on Vertex AI for GPU based serving\n",
        "# @markdown The original models from Meta are converted into the Hugging Face format for serving in Vertex AI.\n",
        "# @markdown Accept the model agreement to access the models:\n",
        "# @markdown 1. Open the [LLaMA2 model card](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/139) from [Vertex AI Model Garden](https://cloud.google.com/model-garden).\n",
        "# @markdown 2. Review and accept the agreement in the pop-up window on the model card page. If you have previously accepted the model agreement, there will not be a pop-up window on the model card page and this step is not needed.\n",
        "# @markdown 3. A Cloud Storage bucket (starting with `gs://`) containing LLaMA 2 pretrained and finetuned models will be shared under the “Documentation” section and its “Get started” subsection.\n",
        "\n",
        "\n",
        "VERTEX_AI_MODEL_GARDEN_LLAMA2 = \"\"  # @param {type:\"string\", isTemplate:true}\n",
        "assert (\n",
        "    VERTEX_AI_MODEL_GARDEN_LLAMA2\n",
        "), \"Please click the agreement of LLaMA2 in Vertex AI Model Garden, and get the GCS path of LLaMA2 model artifacts.\"\n",
        "print(\n",
        "    \"Copying LLaMA2 model artifacts from\",\n",
        "    VERTEX_AI_MODEL_GARDEN_LLAMA2,\n",
        "    \"to\",\n",
        "    MODEL_BUCKET,\n",
        ")\n",
        "\n",
        "! gsutil -m cp -R $VERTEX_AI_MODEL_GARDEN_LLAMA2/* $MODEL_BUCKET\n",
        "\n",
        "# The pre-built serving and training docker images.\n",
        "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240326_0916_RC00\"\n",
        "TRAIN_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-train:20240321_0936_RC00\"\n",
        "\n",
        "\n",
        "def get_job_name_with_datetime(prefix: str) -> str:\n",
        "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
        "    jobs in Vertex AI.\n",
        "    \"\"\"\n",
        "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
        "\n",
        "\n",
        "def deploy_model_vllm(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    service_account: str,\n",
        "    machine_type: str = \"g2-standard-8\",\n",
        "    accelerator_type: str = \"NVIDIA_L4\",\n",
        "    accelerator_count: int = 1,\n",
        "    max_model_len: int = 4096,\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
        "\n",
        "    vllm_args = [\n",
        "        \"--host=0.0.0.0\",\n",
        "        \"--port=7080\",\n",
        "        f\"--tensor-parallel-size={accelerator_count}\",\n",
        "        \"--swap-space=16\",\n",
        "        \"--gpu-memory-utilization=0.8\",\n",
        "        f\"--max-model-len={max_model_len}\",\n",
        "        \"--max-num-batched-tokens=4096\",\n",
        "        \"--disable-log-stats\",\n",
        "    ]\n",
        "\n",
        "    env_vars = {\"MODEL_ID\": model_id}\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
        "        serving_container_command=[\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
        "        serving_container_args=vllm_args,\n",
        "        serving_container_ports=[7080],\n",
        "        serving_container_predict_route=\"/generate\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "        serving_container_environment_variables=env_vars,\n",
        "        artifact_uri=model_id,\n",
        "    )\n",
        "    print(\n",
        "        f\"Deploying {model_name} on {machine_type} with {accelerator_count} {accelerator_type} GPU(s).\"\n",
        "    )\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=service_account,\n",
        "    )\n",
        "    print(\"endpoint_name:\", endpoint.name)\n",
        "\n",
        "    print(\"To load this existing endpoint from a different session:\")\n",
        "    print(\"from google.cloud import aiplatform\")\n",
        "    print(\n",
        "        f'endpoint = aiplatform.Endpoint(\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint.name}\")'\n",
        "    )\n",
        "    return model, endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Ax6GlzzMk-sc"
      },
      "outputs": [],
      "source": [
        "# @title Set training dataset\n",
        "\n",
        "# @markdown This notebook uses [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) dataset as an example.\n",
        "# @markdown You can set `dataset_name` to any existing [Hugging Face dataset](https://huggingface.co/datasets) name, and set `instruct_column_in_dataset` to the name of the dataset column containing training data. The [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) has only one column `text`, and therefore we set `instruct_column_in_dataset` to `text` in this notebook.\n",
        "\n",
        "# @markdown #### (Optional) Prepare a custom JSONL dataset for finetuning\n",
        "\n",
        "# @markdown You can prepare a JSONL file where each line is a valid JSON string as your custom training dataset. For example, here is one line from the [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) dataset:\n",
        "# @markdown ```\n",
        "# @markdown {\"text\": \"### Human: Hola### Assistant: \\u00a1Hola! \\u00bfEn qu\\u00e9 puedo ayudarte hoy?\"}\n",
        "# @markdown ```\n",
        "\n",
        "# @markdown The JSON object has a key `text`, which should match `instruct_column_in_dataset`; The value should be one training data point, i.e. a string. After you prepared your JSONL file, you can either upload it to [Hugging Face datasets](https://huggingface.co/datasets) or [Google Cloud Storage](https://cloud.google.com/storage).\n",
        "\n",
        "# @markdown - To upload a JSONL dataset to [Hugging Face datasets](https://huggingface.co/datasets), follow the instructions on [Uploading Datasets](https://huggingface.co/docs/hub/en/datasets-adding). Then, set `dataset_name` to the name of your newly created dataset on Hugging Face.\n",
        "\n",
        "# @markdown - To upload a JSONL dataset to [Google Cloud Storage](https://cloud.google.com/storage), follow the instructions on [Upload objects from a filesystem](https://cloud.google.com/storage/docs/uploading-objects). Then, set `dataset_name` to the `gs://` URI to your JSONL file. For example: `gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl`.\n",
        "\n",
        "# @markdown Optionally update the `instruct_column_in_dataset` field below if your JSON objects use a key other than the default `text`.\n",
        "\n",
        "# @markdown #### (Optional) Format your data with custom JSON template\n",
        "\n",
        "# @markdown Sometimes, your dataset might have multiple text columns and you want to construct the training data with a template. You can prepare a JSON template in the following format:\n",
        "\n",
        "# @markdown ```\n",
        "# @markdown {\n",
        "# @markdown   \"description\": \"A short template for vertex sample dataset.\",\n",
        "# @markdown   \"prompt_input\": \"{input_text}{output_text}\",\n",
        "# @markdown   \"prompt_no_input\": \"{input_text}{output_text}\"\n",
        "# @markdown }\n",
        "# @markdown ```\n",
        "\n",
        "# @markdown As an example, the template above can be used to format the following training data (this line comes from `gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl`):\n",
        "\n",
        "# @markdown ```\n",
        "# @markdown {\"input_text\":\"TRANSCRIPT: \\nREASON FOR EVALUATION:,\\n\\n LABEL:\",\"output_text\":\"Chiropractic\"}\n",
        "# @markdown ```\n",
        "\n",
        "# @markdown This example template simply concatenates `input_text` with `output_text`. You can set `template` to `vertex_sample` to try out this built-in template with the dataset `gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl`, or build more complicated JSON templates such as [the alpaca example](https://github.com/tloen/alpaca-lora/blob/main/templates/alpaca.json). To use your own JSON template, please [upload it to Google Cloud Storage](https://cloud.google.com/storage/docs/uploading-objects) and put the `gs://` URI in the `template` field below. Leave `instruct_column_in_dataset` as `text`.\n",
        "\n",
        "# Hugging Face dataset name or gs:// URI to a custom JSONL dataset.\n",
        "dataset_name = \"timdettmers/openassistant-guanaco\"  # @param {type:\"string\"}\n",
        "\n",
        "# Name of the dataset column containing training text input.\n",
        "instruct_column_in_dataset = \"text\"  # @param {type:\"string\"}\n",
        "\n",
        "# Optional. Template name or gs:// URI to a custom template.\n",
        "template = \"\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-Sh_hqAil8ck"
      },
      "outputs": [],
      "source": [
        "# @title Set evaluation dataset\n",
        "\n",
        "# @markdown To obtain a model with better performance on some specific tasks, you might want to run hyperparameter tuning with a custom evaluation dataset. The hyperparameter tuning service will pick the model according to the evaluation dataset and the metrics you selected. You can use any of the following tasks as the `eval_task` in the input field below:\n",
        "\n",
        "# @markdown 1. The name of a [lm-evaluation-harness task](https://github.com/EleutherAI/lm-evaluation-harness/tree/big-refactor/lm_eval/tasks). Then, set `eval_mertric_name` to a supported metric name in the task config, such as [`acc_norm`](https://github.com/EleutherAI/lm-evaluation-harness/blob/967eb4fa90b80ba4e8cc7a2fd171f44f0e384808/lm_eval/tasks/arc/arc_easy.yaml#L19) in `arc_challenge`.\n",
        "\n",
        "# @markdown 2. `builtin_eval`. The built-in evaluation loop of the trainer will be used to evaluate the model instead of the [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) library.\n",
        "# @markdown You can supply any eval dataset in the same format as the training dataset by specifying the following parameters:\n",
        "# @markdown    - `eval_dataset_path`: Same format as `dataset_name` in the **Set training dataset** section.\n",
        "# @markdown    - `eval_column`: Same format as `instruct_column_in_dataset` in the **Set training dataset** section.\n",
        "# @markdown    - `eval_template`: Same format as `template` in the **Set training dataset** section\n",
        "\n",
        "eval_task = \"arc_challenge\"  # @param {type:\"string\"}\n",
        "eval_metric_name = \"acc_norm\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown The following fields are optional and only useful if you set `eval_task` to `builtin_eval`.\n",
        "eval_dataset_path = \"\"  # @param {type:\"string\"}\n",
        "eval_column = \"\"  # @param {type:\"string\"}\n",
        "eval_template = \"\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "e1289e21a9d3"
      },
      "outputs": [],
      "source": [
        "# @title Run hyperparameter tuning\n",
        "\n",
        "# @markdown This section demonstrates how to hyperparameter tune the LLaMA 2 models with PEFT LoRA.\n",
        "\n",
        "# @markdown You can use the Vertex AI SDK to create and run the [hyperparameter tuning job](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview) to obtain a better performance by experimenting with different hyperparameters.\n",
        "# @markdown You can customize the search space by extending the range of learning rates, adding other parameters such as LoRA rank, etc. Please refer to the [hyperparameter tuning documentation](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview) for more information.\n",
        "\n",
        "# @markdown The following 4bit QLoRA experiment results show the effectiveness of hyperparameter tuning evaluated on the ARC Challenge dataset (for reference only). Each model is fine-tuned for 1000 steps and evaluated on 10000 samples:\n",
        "\n",
        "# @markdown | Model      | Training time | Trials | Parallel trials | GPU  | ∆arc challenge | ∆hellaswag | ∆truthfulqa_mc | Cost        |\n",
        "# @markdown |------------|---------------|--------|-----------------|------|----------------|------------|----------------|-------------|\n",
        "# @markdown | Llama2-7b  | 2d 10hrs      | 8      | 1               | L4x1 | +0.73          | +1.61      | +5.34          | \\$49.5088    |\n",
        "# @markdown | Llama2-13b | 4d 8hrs       | 8      | 1               | L4x1 | +1.53          | +2.11      | +10.98         | \\$88.7744    |\n",
        "# @markdown | Llama2-70b | 6d 10hrs      | 8      | 2               | L4x4 | +0.77          | +0.93      | +10.63         | \\$1,312.5461 |\n",
        "\n",
        "# @markdown In this notebook, the model will be finetuned for 200 steps and evaluated on 1000 samples, running 2 hyperparameter tuning trials.\n",
        "# @markdown To customize the hyperparameter tuning to obtain better performance, click \"Show Code\" to see more details.\n",
        "\n",
        "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
        "\n",
        "# @markdown Set the base model id.\n",
        "base_model_id = \"llama2-7b-hf\"  # @param [\"llama2-7b-hf\", \"llama2-7b-chat-hf\", \"llama2-13b-hf\", \"llama2-13b-chat-hf\", \"llama2-70b-hf\", \"llama2-70b-chat-hf\"]\n",
        "model_id = os.path.join(MODEL_BUCKET, base_model_id)\n",
        "\n",
        "# @markdown Set the accelerator type.\n",
        "accelerator_type = \"NVIDIA_TESLA_V100\"  # @param[\"NVIDIA_TESLA_V100\", \"NVIDIA_L4\", \"NVIDIA_TESLA_A100\"]\n",
        "\n",
        "# Worker pool spec.\n",
        "# Find Vertex AI supported accelerators and regions in:\n",
        "# https://cloud.google.com/vertex-ai/docs/training/configure-compute\n",
        "machine_type = None\n",
        "if \"7b\" in model_id or \"13b\" in model_id:\n",
        "    if accelerator_type == \"NVIDIA_TESLA_A100\":\n",
        "        machine_type = \"a2-highgpu-1g\"\n",
        "        accelerator_count = 1\n",
        "    elif accelerator_type == \"NVIDIA_TESLA_V100\":\n",
        "        machine_type = \"n1-standard-8\"\n",
        "        accelerator_count = 1\n",
        "    elif accelerator_type == \"NVIDIA_L4\":\n",
        "        machine_type = \"g2-standard-8\"\n",
        "        accelerator_count = 1\n",
        "elif \"70b\" in model_id:\n",
        "    if accelerator_type == \"NVIDIA_TESLA_A100\":\n",
        "        machine_type = \"a2-highgpu-2g\"\n",
        "        accelerator_count = 2\n",
        "    elif accelerator_type == \"NVIDIA_TESLA_V100\":\n",
        "        machine_type = \"n1-highmem-64\"\n",
        "        accelerator_count = 8\n",
        "    elif accelerator_type == \"NVIDIA_L4\":\n",
        "        machine_type = \"g2-standard-48\"\n",
        "        accelerator_count = 4\n",
        "\n",
        "if machine_type is None:\n",
        "    raise ValueError(\n",
        "        f\"Recommended machine settings not found for: {accelerator_type}. To use another another accelerator, please edit this code block to set an appropriate `machine_type`, `accelerator_type`, and `accelerator_count` in worker_pool_specs.\"\n",
        "    )\n",
        "\n",
        "job_name = get_job_name_with_datetime(\"llama2-train\")\n",
        "output_dir = os.path.join(EXPERIMENT_BUCKET, job_name)\n",
        "hpt_precision_mode = \"4bit\"\n",
        "\n",
        "replica_count = 1\n",
        "\n",
        "# Runs 200 training steps.\n",
        "max_steps = 200  # @param {type: \"integer\"}\n",
        "per_device_train_batch_size = 4\n",
        "# Evaluates the model on 1000 examples.\n",
        "eval_limit = 1000\n",
        "# LoRA parameters.\n",
        "lora_rank = 16  # @param {type: \"integer\"}\n",
        "lora_alpha = 32\n",
        "lora_dropout = 0.05\n",
        "\n",
        "flags = {\n",
        "    \"learning_rate\": 1e-5,\n",
        "    \"precision_mode\": hpt_precision_mode,\n",
        "    \"task\": \"instruct-lora\",\n",
        "    \"pretrained_model_id\": model_id,\n",
        "    \"output_dir\": output_dir,\n",
        "    \"warmup_steps\": 10,\n",
        "    \"max_steps\": max_steps,\n",
        "    \"per_device_train_batch_size\": per_device_train_batch_size,\n",
        "    \"lora_rank\": lora_rank,\n",
        "    \"lora_alpha\": lora_alpha,\n",
        "    \"lora_dropout\": lora_dropout,\n",
        "    \"dataset_name\": dataset_name,\n",
        "    \"instruct_column_in_dataset\": instruct_column_in_dataset,\n",
        "    \"template\": template,\n",
        "    \"eval_steps\": max_steps + 1,  # Only evaluates in the end.\n",
        "    \"eval_tasks\": eval_task,\n",
        "    \"eval_metric_name\": eval_metric_name,\n",
        "    \"eval_limit\": eval_limit,\n",
        "    \"eval_dataset_path\": eval_dataset_path,\n",
        "    \"eval_column\": eval_column,\n",
        "    \"eval_template\": eval_template,\n",
        "}\n",
        "replica_count = 1\n",
        "\n",
        "worker_pool_specs = [\n",
        "    {\n",
        "        \"machine_spec\": {\n",
        "            \"machine_type\": machine_type,\n",
        "            \"accelerator_type\": accelerator_type,\n",
        "            \"accelerator_count\": accelerator_count,\n",
        "        },\n",
        "        \"replica_count\": replica_count,\n",
        "        \"container_spec\": {\n",
        "            \"image_uri\": TRAIN_DOCKER_URI,\n",
        "            \"args\": [\"--{}={}\".format(k, v) for k, v in flags.items()],\n",
        "        },\n",
        "    }\n",
        "]\n",
        "metric_spec = {\"model_performance\": \"maximize\"}\n",
        "parameter_spec = {\n",
        "    \"learning_rate\": hpt.DoubleParameterSpec(min=1e-5, max=1e-4, scale=\"linear\"),\n",
        "}\n",
        "train_job = aiplatform.CustomJob(\n",
        "    display_name=job_name,\n",
        "    worker_pool_specs=worker_pool_specs,\n",
        "    staging_bucket=STAGING_BUCKET,\n",
        ")\n",
        "\n",
        "train_hpt_job = aiplatform.HyperparameterTuningJob(\n",
        "    display_name=f\"{job_name}_hpt\",\n",
        "    custom_job=train_job,\n",
        "    metric_spec=metric_spec,\n",
        "    parameter_spec=parameter_spec,\n",
        "    max_trial_count=2,\n",
        "    parallel_trial_count=2,\n",
        ")\n",
        "\n",
        "train_hpt_job.run()\n",
        "\n",
        "print(\"The finetuned models of different trials can be found at: \", output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rlvrI-W38nFV"
      },
      "outputs": [],
      "source": [
        "# @title Merge LoRA weights\n",
        "\n",
        "# @markdown Find the best trial and run another job to merge the LoRA weights into the base model.\n",
        "\n",
        "best_trial_id = max(\n",
        "    train_hpt_job.trials, key=lambda trial: trial.final_measurement.metrics[0].value\n",
        ").id\n",
        "best_output_dir = os.path.join(\n",
        "    output_dir, f\"trial_{best_trial_id}\", f\"checkpoint-{max_steps}\"\n",
        ")\n",
        "print(f\"Best trial {best_trial_id} saved model in:\", best_output_dir)\n",
        "\n",
        "# Setup LoRA weights merge job.\n",
        "job_name = get_job_name_with_datetime(\"llama-lora-merge\")\n",
        "\n",
        "# Pass arguments and launch job.\n",
        "merge_job = aiplatform.CustomContainerTrainingJob(\n",
        "    display_name=job_name,\n",
        "    container_uri=TRAIN_DOCKER_URI,\n",
        ")\n",
        "merged_model_output_dir = os.path.join(EXPERIMENT_BUCKET, \"merged\")\n",
        "\n",
        "merge_job.run(\n",
        "    args=[\n",
        "        \"--task=merge-causal-language-model-lora\",\n",
        "        \"--merge_model_precision_mode=float16\",\n",
        "        f\"--pretrained_model_id={model_id}\",\n",
        "        f\"--finetuned_lora_model_dir={best_output_dir}\",\n",
        "        f\"--merge_base_and_lora_output_dir={merged_model_output_dir}\",\n",
        "    ],\n",
        "    environment_variables={\"WANDB_DISABLED\": True},\n",
        "    replica_count=1,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    boot_disk_size_gb=500,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        ")\n",
        "print(\"Trained and merged models were saved in: \", merged_model_output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "h0hGj09CuRFQ"
      },
      "outputs": [],
      "source": [
        "# @title Deploy\n",
        "# @markdown This section uploads the model to Model Registry and deploys it on the Endpoint. It takes 15 minutes to 1 hour to finish.\n",
        "# @markdown Please click \"Show code\" to see more details.\n",
        "\n",
        "print(\"Deploying models in: \", merged_model_output_dir)\n",
        "\n",
        "# The max_model_len must not exceed the model's context length.\n",
        "# A larger max_model_len will require more GPU memory.\n",
        "max_model_len = 2048\n",
        "# Worker pool spec.\n",
        "# Find Vertex AI supported accelerators and regions in:\n",
        "# https://cloud.google.com/vertex-ai/docs/training/configure-compute\n",
        "machine_type = None\n",
        "accelerator_type = \"NVIDIA_L4\"  # @param[\"NVIDIA_L4\", \"NVIDIA_TESLA_V100\", \"NVIDIA_TESLA_A100\"]\n",
        "\n",
        "if \"7b\" in model_id:\n",
        "    if accelerator_type == \"NVIDIA_TESLA_A100\":\n",
        "        machine_type = \"a2-highgpu-1g\"\n",
        "        accelerator_count = 1\n",
        "    elif accelerator_type == \"NVIDIA_TESLA_V100\":\n",
        "        machine_type = \"n1-standard-8\"\n",
        "        accelerator_count = 1\n",
        "    elif accelerator_type == \"NVIDIA_L4\":\n",
        "        machine_type = \"g2-standard-8\"\n",
        "        accelerator_count = 1\n",
        "elif \"13b\" in model_id:\n",
        "    if accelerator_type == \"NVIDIA_TESLA_A100\":\n",
        "        machine_type = \"a2-highgpu-1g\"\n",
        "        accelerator_count = 1\n",
        "    elif accelerator_type == \"NVIDIA_TESLA_V100\":\n",
        "        machine_type = \"n1-standard-16\"\n",
        "        accelerator_count = 2\n",
        "    elif accelerator_type == \"NVIDIA_L4\":\n",
        "        machine_type = \"g2-standard-24\"\n",
        "        accelerator_count = 2\n",
        "elif \"70b\" in model_id:\n",
        "    if accelerator_type == \"NVIDIA_TESLA_A100\":\n",
        "        machine_type = \"a2-highgpu-4g\"\n",
        "        accelerator_count = 4\n",
        "    elif accelerator_type == \"NVIDIA_L4\":\n",
        "        machine_type = \"g2-standard-96\"\n",
        "        accelerator_count = 8\n",
        "\n",
        "if machine_type is None:\n",
        "    raise ValueError(\n",
        "        f\"Recommended machine settings not found for: {accelerator_type}. To use another another accelerator, please edit this code block to pass in an appropriate `machine_type`, `accelerator_type`, and `accelerator_count` to the deploy_model_vllm function.\"\n",
        "    )\n",
        "\n",
        "model, endpoint = deploy_model_vllm(\n",
        "    model_name=get_job_name_with_datetime(prefix=\"llama-vllm-serve\"),\n",
        "    model_id=merged_model_output_dir,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    max_model_len=max_model_len,\n",
        ")\n",
        "print(\"endpoint_name:\", endpoint.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vgU_qYHNuy3w"
      },
      "outputs": [],
      "source": [
        "# @title Predict\n",
        "# @markdown Once deployment succeeds, you can send requests to the endpoint with text prompts.\n",
        "\n",
        "# @markdown Here we use an example from the [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) to show the finetuning outcome:\n",
        "\n",
        "# @markdown ```\n",
        "# @markdown ### Human: How would the Future of AI in 10 Years look?### Assistant: Predicting the future is always a challenging task, but here are some possible ways that AI could evolve over the next 10 years: Continued advancements in deep learning: Deep learning has been one of the main drivers of recent AI breakthroughs, and we can expect continued advancements in this area. This may include improvements to existing algorithms, as well as the development of new architectures that are better suited to specific types of data and tasks. Increased use of AI in healthcare: AI has the potential to revolutionize healthcare, by improving the accuracy of diagnoses, developing new treatments, and personalizing patient care. We can expect to see continued investment in this area, with more healthcare providers and researchers using AI to improve patient outcomes. Greater automation in the workplace: Automation is already transforming many industries, and AI is likely to play an increasingly important role in this process. We can expect to see more jobs being automated, as well as the development of new types of jobs that require a combination of human and machine skills. More natural and intuitive interactions with technology: As AI becomes more advanced, we can expect to see more natural and intuitive ways of interacting with technology. This may include voice and gesture recognition, as well as more sophisticated chatbots and virtual assistants. Increased focus on ethical considerations: As AI becomes more powerful, there will be a growing need to consider its ethical implications. This may include issues such as bias in AI algorithms, the impact of automation on employment, and the use of AI in surveillance and policing. Overall, the future of AI in 10 years is likely to be shaped by a combination of technological advancements, societal changes, and ethical considerations. While there are many exciting possibilities for AI in the future, it will be important to carefully consider its potential impact on society and to work towards ensuring that its benefits are shared fairly and equitably.\n",
        "# @markdown ```\n",
        "\n",
        "# @markdown Additionally, you can moderate the generated text with Vertex AI. See [Moderate text documentation](https://cloud.google.com/natural-language/docs/moderating-text) for more details.\n",
        "\n",
        "# Loads an existing endpoint instance using the endpoint name:\n",
        "# - Using `endpoint_name = endpoint.name` allows us to get the\n",
        "#   endpoint name of the endpoint `endpoint` created in the cell\n",
        "#   above.\n",
        "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
        "#   an existing endpoint with the ID 1234567890123456789.\n",
        "# You may uncomment the code below to load an existing endpoint.\n",
        "\n",
        "# endpoint_name = \"\"  # @param {type:\"string\"}\n",
        "# aip_endpoint_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
        "# )\n",
        "# endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
        "\n",
        "prompt = \"How would the Future of AI in 10 Years look?\"  # @param {type: \"string\"}\n",
        "max_tokens = 128  # @param {type:\"integer\"}\n",
        "temperature = 1.0  # @param {type:\"number\"}\n",
        "top_p = 0.9  # @param {type:\"number\"}\n",
        "top_k = 1  # @param {type:\"integer\"}\n",
        "\n",
        "# Overides max_tokens and top_k parameters during inferences.\n",
        "# If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`,\n",
        "# you can reduce the max length, such as set max_tokens as 20.\n",
        "instances = [\n",
        "    {\n",
        "        \"prompt\": f\"### Human: {prompt}### Assistant: \",\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": top_p,\n",
        "        \"top_k\": top_k,\n",
        "    },\n",
        "]\n",
        "response = endpoint.predict(instances=instances)\n",
        "\n",
        "for prediction in response.predictions:\n",
        "    print(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "911406c1561e"
      },
      "outputs": [],
      "source": [
        "# @title Clean up resources\n",
        "\n",
        "# @markdown Delete the experiment models and endpoints to recycle the resources\n",
        "# @markdown and avoid unnecessary continuous charges that may incur.\n",
        "\n",
        "if train_job._gca_resource.name:\n",
        "    # Training job is submitted.\n",
        "    train_job.delete()\n",
        "train_hpt_job.delete()\n",
        "\n",
        "# Undeploy model and delete endpoint.\n",
        "endpoint.delete(force=True)\n",
        "\n",
        "# Delete model.\n",
        "model.delete()\n",
        "\n",
        "# Delete Cloud Storage objects that were created.\n",
        "delete_bucket = False  # @param {type:\"boolean\"}\n",
        "if delete_bucket:\n",
        "    ! gsutil -m rm -r $BUCKET_URI"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_pytorch_llama2_peft_hyperparameter_tuning.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

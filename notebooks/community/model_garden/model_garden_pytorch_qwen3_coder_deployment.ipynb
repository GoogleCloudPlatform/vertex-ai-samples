{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SgQ6t5bqZVlH"
      },
      "outputs": [],
      "source": [
        "# Copyright 2026 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Vertex AI Model Garden - Qwen3-Coder (Deployment)\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_pytorch_qwen3_coder_deployment.ipynb\">\n",
        "      <img alt=\"Workbench logo\" src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" width=\"32px\"><br> Run in Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_pytorch_qwen3_coder_deployment.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_qwen3_coder_deployment.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates serving Qwen3-Coder models with [SGLang](https://github.com/sgl-project/sglang). \n",
        "\n",
        "[Qwen3-Coder](https://huggingface.co/collections/Qwen/qwen3-coder-687fc861e53c939e52d52d10) is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support.\n",
        "\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Deploy Qwen3-Coder with SGLang on GPU.\n",
        "\n",
        "### File a bug\n",
        "\n",
        "File a bug on [GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/issues/new) if you encounter any issue with the notebook.\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "264c07757582"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "M_T_hjR1-vQU"
      },
      "outputs": [],
      "source": [
        "# @title Upgrade Vertex AI SDK\n",
        "\n",
        "# @markdown After executing this cell, click \"RESTART SESSION\" if prompted in the output.\n",
        "\n",
        "! pip install --upgrade google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YXFGIp1l-qtT"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown 2. **[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment.\n",
        "\n",
        "REGION = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown 3. This model requires NVIDIA_H200_141GB gpus. **NOTE:** Make sure you have associated quota in selected regions. Click the links to see your current quota for each GPU type: [Nvidia H200 141GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_h200_141gb_gpus). You can request for quota following the instructions at [\"Request a higher quota\"](https://cloud.google.com/docs/quota/view-manage#requesting_higher_quota).\n",
        "\n",
        "# @markdown > | Machine Type | Accelerator Type | Recommended Regions |\n",
        "# @markdown | ----------- | ----------- | ----------- |\n",
        "# @markdown | a3-ultragpu-8g | 8 NVIDIA_H200_141GB | asia-south2, us-south1 |\n",
        "\n",
        "# Upgrade Vertex AI SDK.\n",
        "! pip3 install --upgrade --quiet 'google-cloud-aiplatform==1.103.0'\n",
        "\n",
        "# Import the necessary packages\n",
        "import importlib\n",
        "import os\n",
        "import time\n",
        "from typing import Tuple\n",
        "\n",
        "import requests\n",
        "from google import auth\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "if os.environ.get(\"VERTEX_PRODUCT\") != \"COLAB_ENTERPRISE\":\n",
        "    ! pip install --upgrade tensorflow\n",
        "! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
        "\n",
        "common_util = importlib.import_module(\n",
        "    \"vertex-ai-samples.notebooks.community.model_garden.docker_source_codes.notebook_util.common_util\"\n",
        ")\n",
        "\n",
        "LABEL = \"sglang_gpu\"\n",
        "models, endpoints = {}, {}\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "if not REGION:\n",
        "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "! gcloud config set project $PROJECT_ID\n",
        "\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(\n",
        "    project=PROJECT_ID,\n",
        "    location=REGION,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zJJDmldn7rw"
      },
      "source": [
        "## Deploy Qwen3 with SGLang"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_3Swj3pxn7rw"
      },
      "outputs": [],
      "source": [
        "# @title Select the model variants\n",
        "\n",
        "# @markdown Set the model to deploy.\n",
        "\n",
        "MODEL_ID = \"Qwen/Qwen3-Coder-480B-A35B-Instruct\"  # @param [\"Qwen/Qwen3-Coder-480B-A35B-Instruct\"] {isTemplate:true}\n",
        "\n",
        "version_id = MODEL_ID.split(\"/\")[-1]\n",
        "hf_model_id = MODEL_ID\n",
        "\n",
        "# The pre-built serving docker images.\n",
        "SGLANG_DOCKER_URI = \"us-docker.pkg.dev/deeplearning-platform-release/vertex-model-garden/sglang-serve.cu124.0-4.ubuntu2204.py310:model-garden.sglang-0-4-release_20250720.00_p0\"\n",
        "\n",
        "# @markdown Set use_dedicated_endpoint to False if you don't want to use [dedicated endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment#create-dedicated-endpoint). Note that [dedicated endpoint does not support VPC Service Controls](https://cloud.google.com/vertex-ai/docs/predictions/choose-endpoint-type), uncheck the box if you are using VPC-SC.\n",
        "use_dedicated_endpoint = True  # @param {type:\"boolean\"}\n",
        "\n",
        "accelerator_type = \"NVIDIA_H200_141GB\"\n",
        "machine_type = \"a3-ultragpu-8g\"\n",
        "accelerator_count = 8\n",
        "resource_id = \"custom_model_serving_nvidia_h200_gpus\"\n",
        "\n",
        "PUBLISHER_MODEL_NAME = f\"publishers/qwen/models/qwen3-coder@{version_id.lower()}\"\n",
        "\n",
        "print(\"Checking quota...\")\n",
        "common_util.check_quota(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    is_for_training=False,\n",
        ")\n",
        "print(\"Quota check completed.\")\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "omW0LaC8wWz5"
      },
      "outputs": [],
      "source": [
        "# @title [Option 1] Deploy with Model Garden SDK\n",
        "# @markdown Deploy with Gen AI model-centric SDK. This section uploads the prebuilt model to Model Registry and deploys it to a Vertex AI Endpoint. It takes 15 minutes to 1 hour to finish depending on the size of the model. See [use open models with Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-open-models) for documentation on other use cases.\n",
        "deploy_request_timeout = 1800  # 30 minutes\n",
        "from vertexai import model_garden\n",
        "\n",
        "model = model_garden.OpenModel(PUBLISHER_MODEL_NAME)\n",
        "endpoints[LABEL] = model.deploy(\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        "    spot=is_spot,\n",
        "    deploy_request_timeout=deploy_request_timeout,\n",
        "    accept_eula=False,\n",
        ")\n",
        "\n",
        "endpoint = endpoints[LABEL]\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3m-tDxgawYhU"
      },
      "outputs": [],
      "source": [
        "# @title [Option 2] Deploy with customized configs\n",
        "\n",
        "# @markdown This section uploads Qwen3 models to Model Registry and deploys them to a Vertex Prediction Endpoint. It takes ~1 hour to finish.\n",
        "\n",
        "# @markdown It's recommended to use the region selected by the deployment button on the model card. If the deployment button is not available, it's recommended to stay with the default region of the notebook.\n",
        "\n",
        "\n",
        "def poll_operation(op_name: str) -> bool:  # noqa: F811\n",
        "    creds, _ = auth.default()\n",
        "    auth_req = auth.transport.requests.Request()\n",
        "    creds.refresh(auth_req)\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {creds.token}\",\n",
        "    }\n",
        "    get_resp = requests.get(\n",
        "        f\"https://{REGION}-aiplatform.googleapis.com/ui/{op_name}\",\n",
        "        headers=headers,\n",
        "    )\n",
        "    opjs = get_resp.json()\n",
        "    if \"error\" in opjs:\n",
        "        raise ValueError(f\"Operation failed: {opjs['error']}\")\n",
        "    return opjs.get(\"done\", False)\n",
        "\n",
        "\n",
        "def poll_and_wait(op_name: str, total_wait: int, interval: int = 60):  # noqa: F811\n",
        "    waited = 0\n",
        "    while not poll_operation(op_name):\n",
        "        if waited > total_wait:\n",
        "            raise TimeoutError(\"Operation timed out\")\n",
        "        print(\n",
        "            f\"\\rStill waiting for operation... Waited time in second: {waited:<6}\",\n",
        "            end=\"\",\n",
        "            flush=True,\n",
        "        )\n",
        "        waited += interval\n",
        "        time.sleep(interval)\n",
        "\n",
        "\n",
        "def deploy_model_sglang_multihost(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    publisher: str,\n",
        "    publisher_model_id: str,\n",
        "    service_account: str = \"\",\n",
        "    base_model_id: str = \"\",\n",
        "    machine_type: str = \"g2-standard-8\",\n",
        "    accelerator_type: str = \"NVIDIA_L4\",\n",
        "    accelerator_count: int = 1,\n",
        "    multihost_gpu_node_count: int = 1,\n",
        "    gpu_memory_utilization: float | None = None,\n",
        "    context_length: int | None = None,\n",
        "    dtype: str | None = None,\n",
        "    quantization: str | None = None,\n",
        "    enable_trust_remote_code: bool = False,\n",
        "    enable_torch_compile: bool = False,\n",
        "    torch_compile_max_bs: int | None = None,\n",
        "    attention_backend: str = \"\",\n",
        "    enable_flashinfer_mla: bool = False,\n",
        "    disable_cuda_graph: bool = False,\n",
        "    speculative_algorithm: str | None = None,\n",
        "    speculative_draft_model_path: str = \"\",\n",
        "    speculative_num_steps: int = 3,\n",
        "    speculative_eagle_topk: int = 1,\n",
        "    speculative_num_draft_tokens: int = 4,\n",
        "    enable_jit_deepgemm: bool = False,\n",
        "    enable_dp_attention: bool = False,\n",
        "    dp_size: int = 1,\n",
        "    enable_multimodal: bool = False,\n",
        "    use_dedicated_endpoint: bool = False,\n",
        "    max_num_seqs: int | None = None,\n",
        "    is_spot: bool = True,\n",
        "    tool_call_parser: str | None = None,\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys trained models with SGLang into Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(\n",
        "        display_name=f\"{model_name}-endpoint\",\n",
        "        dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
        "    )\n",
        "\n",
        "    if not base_model_id:\n",
        "        base_model_id = model_id\n",
        "\n",
        "    # See https://docs.sglang.ai/backend/server_arguments.html for a list of possible arguments with descriptions.\n",
        "    sglang_args = [\n",
        "        f\"--model={model_id}\",\n",
        "        f\"--tp={accelerator_count * multihost_gpu_node_count}\",\n",
        "        f\"--dp={dp_size}\",\n",
        "    ]\n",
        "\n",
        "    if context_length:\n",
        "        sglang_args.append(f\"--context-length={context_length}\")\n",
        "\n",
        "    if gpu_memory_utilization:\n",
        "        sglang_args.append(f\"--mem-fraction-static={gpu_memory_utilization}\")\n",
        "\n",
        "    if max_num_seqs:\n",
        "        sglang_args.append(f\"--max-running-requests={max_num_seqs}\")\n",
        "\n",
        "    if dtype:\n",
        "        sglang_args.append(f\"--dtype={dtype}\")\n",
        "\n",
        "    if quantization:\n",
        "        sglang_args.append(f\"--quantization={quantization}\")\n",
        "\n",
        "    if enable_trust_remote_code:\n",
        "        sglang_args.append(\"--trust-remote-code\")\n",
        "\n",
        "    if enable_torch_compile:\n",
        "        sglang_args.append(\"--enable-torch-compile\")\n",
        "        if torch_compile_max_bs:\n",
        "            sglang_args.append(f\"--torch-compile-max-bs={torch_compile_max_bs}\")\n",
        "\n",
        "    if attention_backend:\n",
        "        sglang_args.append(f\"--attention-backend={attention_backend}\")\n",
        "\n",
        "    if enable_flashinfer_mla:\n",
        "        sglang_args.append(\"--enable-flashinfer-mla\")\n",
        "\n",
        "    if disable_cuda_graph:\n",
        "        sglang_args.append(\"--disable-cuda-graph\")\n",
        "\n",
        "    if speculative_algorithm:\n",
        "        sglang_args.append(f\"--speculative-algorithm={speculative_algorithm}\")\n",
        "        sglang_args.append(\n",
        "            f\"--speculative-draft-model-path={speculative_draft_model_path}\"\n",
        "        )\n",
        "        sglang_args.append(f\"--speculative-num-steps={speculative_num_steps}\")\n",
        "        sglang_args.append(f\"--speculative-eagle-topk={speculative_eagle_topk}\")\n",
        "        sglang_args.append(\n",
        "            f\"--speculative-num-draft-tokens={speculative_num_draft_tokens}\"\n",
        "        )\n",
        "\n",
        "    if enable_dp_attention:\n",
        "        sglang_args.append(\"--enable-dp-attention\")\n",
        "\n",
        "    if enable_multimodal:\n",
        "        sglang_args.append(\"--enable-multimodal\")\n",
        "\n",
        "    if tool_call_parser:\n",
        "        sglang_args.append(f\"--tool-call-parser={tool_call_parser}\")\n",
        "\n",
        "    env_vars = {\n",
        "        \"MODEL_ID\": base_model_id,\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "    }\n",
        "\n",
        "    if enable_jit_deepgemm:\n",
        "        env_vars[\"SGL_ENABLE_JIT_DEEPGEMM\"] = \"1\"\n",
        "\n",
        "    # HF_TOKEN is not a compulsory field and may not be defined.\n",
        "    try:\n",
        "        if HF_TOKEN:\n",
        "            env_vars[\"HF_TOKEN\"] = HF_TOKEN\n",
        "    except NameError:\n",
        "        pass\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=SGLANG_DOCKER_URI,\n",
        "        serving_container_args=sglang_args,\n",
        "        serving_container_ports=[30000],\n",
        "        serving_container_predict_route=\"/vertex_generate\",\n",
        "        serving_container_health_route=\"/health\",\n",
        "        serving_container_environment_variables=env_vars,\n",
        "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
        "        serving_container_deployment_timeout=7200,\n",
        "        model_garden_source_model_name=(\n",
        "            f\"publishers/{publisher}/models/{publisher_model_id}\"\n",
        "        ),\n",
        "    )\n",
        "    print(\n",
        "        f\"Deploying {model_name} on {machine_type} with {int(accelerator_count * multihost_gpu_node_count)} {accelerator_type} GPU(s).\"\n",
        "    )\n",
        "\n",
        "    creds, _ = auth.default()\n",
        "    auth_req = auth.transport.requests.Request()\n",
        "    creds.refresh(auth_req)\n",
        "\n",
        "    url = f\"https://{REGION}-aiplatform.googleapis.com/ui/projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint.name}:deployModel\"\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {creds.token}\",\n",
        "    }\n",
        "    data = {\n",
        "        \"deployedModel\": {\n",
        "            \"model\": model.resource_name,\n",
        "            \"displayName\": model_name,\n",
        "            \"dedicatedResources\": {\n",
        "                \"machineSpec\": {\n",
        "                    \"machineType\": machine_type,\n",
        "                    \"multihostGpuNodeCount\": multihost_gpu_node_count,\n",
        "                    \"acceleratorType\": accelerator_type,\n",
        "                    \"acceleratorCount\": accelerator_count,\n",
        "                },\n",
        "                \"minReplicaCount\": 1,\n",
        "                \"maxReplicaCount\": 1,\n",
        "            },\n",
        "            \"system_labels\": {\n",
        "                \"NOTEBOOK_NAME\": \"model_garden_pytorch_qwen3_coder_deployment.ipynb\",\n",
        "                \"NOTEBOOK_ENVIRONMENT\": common_util.get_deploy_source(),\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "    if service_account:\n",
        "        data[\"deployedModel\"][\"serviceAccount\"] = service_account\n",
        "    if is_spot:\n",
        "        data[\"deployedModel\"][\"dedicatedResources\"][\"spot\"] = True\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "    print(f\"Deploy Model response: {response.json()}\")\n",
        "    if response.status_code != 200 or \"name\" not in response.json():\n",
        "        raise ValueError(f\"Failed to deploy model: {response.text}\")\n",
        "    poll_and_wait(response.json()[\"name\"], 7200)\n",
        "    print(\"endpoint_name:\", endpoint.name)\n",
        "\n",
        "    return model, endpoint\n",
        "\n",
        "\n",
        "models[LABEL], endpoints[LABEL] = deploy_model_sglang_multihost(\n",
        "    model_name=common_util.get_job_name_with_datetime(prefix=version_id),\n",
        "    model_id=MODEL_ID,\n",
        "    publisher=\"qwen\",\n",
        "    publisher_model_id=\"qwen3-coder\",\n",
        "    base_model_id=hf_model_id,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        "    tool_call_parser=\"qwen25\",\n",
        ")\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AGVPzwHkn7rw"
      },
      "outputs": [],
      "source": [
        "# @title Raw predict\n",
        "\n",
        "\n",
        "# @markdown Once deployment succeeds, you can send requests to the endpoint with text prompts. Sampling parameters supported by SGLang can be found [here](https://docs.sglang.ai/backend/sampling_params.html).\n",
        "\n",
        "# @markdown Example:\n",
        "\n",
        "# @markdown ```\n",
        "# @markdown Write a quick sort algorithm in Python.\n",
        "# @markdown ```\n",
        "# @markdown Additionally, you can moderate the generated text with Vertex AI. See [Moderate text documentation](https://cloud.google.com/natural-language/docs/moderating-text) for more details.\n",
        "\n",
        "# Loads an existing endpoint instance using the endpoint name:\n",
        "# - Using `endpoint_name = endpoint.name` allows us to get the\n",
        "#   endpoint name of the endpoint `endpoint` created in the cell\n",
        "#   above.\n",
        "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
        "#   an existing endpoint with the ID 1234567890123456789.\n",
        "# You may uncomment the code below to load an existing endpoint.\n",
        "\n",
        "# endpoint_name = \"\"  # @param {type:\"string\"}\n",
        "# aip_endpoint_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
        "# )\n",
        "# endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
        "\n",
        "prompt = \"Write a quick sort algorithm in Python.\"  # @param {type: \"string\"}\n",
        "\n",
        "max_new_tokens = 32768  # @param {type:\"integer\"}\n",
        "temperature = 0.7  # @param {type:\"number\"}\n",
        "top_p = 0.8  # @param {type:\"number\"}\n",
        "top_k = 20  # @param {type:\"number\"}\n",
        "\n",
        "# Overrides parameters for inferences.\n",
        "instances = [{\"text\": prompt}]\n",
        "parameters = {\n",
        "    \"sampling_params\": {\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": top_p,\n",
        "        \"top_k\": top_k,\n",
        "    }\n",
        "}\n",
        "response = endpoints[\"sglang_gpu\"].predict(\n",
        "    instances=instances,\n",
        "    parameters=parameters,\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        ")\n",
        "\n",
        "for prediction in response.predictions:\n",
        "    print(prediction)\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JETd33jIDcjm"
      },
      "source": [
        "## Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "911406c1561e"
      },
      "outputs": [],
      "source": [
        "# @title Delete the models and endpoints\n",
        "\n",
        "# @markdown  Delete the experiment models and endpoints to recycle the resources\n",
        "# @markdown  and avoid unnecessary continuous charges that may incur.\n",
        "\n",
        "# Undeploy model and delete endpoint.\n",
        "for endpoint in endpoints.values():\n",
        "    endpoint.delete(force=True)\n",
        "\n",
        "# Delete models.\n",
        "for model in models.values():\n",
        "    model.delete()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_pytorch_qwen3_coder_deployment.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

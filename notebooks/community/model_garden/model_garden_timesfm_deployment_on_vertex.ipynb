{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbnQoeZABLr6"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlUIoPa6BB5Z"
      },
      "source": [
        "# Vertex AI Model Garden - TimesFM 1.0 (CPU/GPU Deployment)\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_timesfm_deployment_on_vertex.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_timesfm_deployment_on_vertex.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates deploying TimesFM 1.0 to a Vertex AI Endpoint and making online predictions for times series forecast.\n",
        "\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Deploy TimesFM 1.0 to a Vertex AI Endpoint.\n",
        "- Make predictions to the endpoint for times series forecast.\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8KqOx2DC3Yc"
      },
      "source": [
        "## Setup Google Cloud project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iNgqf6w1DCZF"
      },
      "outputs": [],
      "source": [
        "# @markdown ### **Prerequisites**\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "# @markdown 2. [Optional] [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets)\n",
        "# @markdown for storing experiment outputs. Set the BUCKET_URI for the\n",
        "# @markdown experiment environment. The specified Cloud Storage bucket\n",
        "# @markdown (`BUCKET_URI`) should be located in the same region as where the\n",
        "# @markdown notebook was launched. Note that a multi-region bucket (eg. \"us\") is\n",
        "# @markdown not considered a match for a single region covered by the\n",
        "# @markdown multi-region range (eg. \"us-central1\"). If not set, a unique GCS\n",
        "# @markdown bucket will be created instead.\n",
        "\n",
        "import json\n",
        "import os\n",
        "# Import the necessary packages\n",
        "from datetime import datetime\n",
        "from typing import Tuple\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "print(f\"Using this region: {REGION}\")\n",
        "\n",
        "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
        "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
        "\n",
        "# @markdown Cloud Storage bucket for storing the experiment artifacts.\n",
        "# @markdown A unique GCS bucket will be created for the purpose of this\n",
        "# @markdown notebook. If you prefer using your own GCS bucket, change the value\n",
        "# @markdown yourself below.\n",
        "now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
        "assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
        "\n",
        "# Create a unique GCS bucket for this notebook, if not specified by the user\n",
        "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
        "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}\"\n",
        "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
        "else:\n",
        "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "    shell_output = ! gsutil ls -Lb {BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
        "    bucket_region = shell_output[0].strip().lower()\n",
        "    if bucket_region != REGION:\n",
        "        raise ValueError(\n",
        "            f\"Bucket region {bucket_region} is different from notebook region\"\n",
        "            f\" {REGION}\"\n",
        "        )\n",
        "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
        "\n",
        "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
        "MODEL_BUCKET = os.path.join(BUCKET_URI, \"timesfm\")\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "# Set up default SERVICE_ACCOUNT\n",
        "SERVICE_ACCOUNT = None\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
        "\n",
        "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
        "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
        "\n",
        "\n",
        "# @markdown ### **Choose a prebuilt checkpoint**\n",
        "# @markdown Here we specify where to get the model checkpoint. TimesFM\n",
        "# @markdown pretrained checkpoints are by default saved under\n",
        "# @markdown `gs://vertex-model-garden-public-{region}/timesfm` and indexed by\n",
        "# @markdown the checkpoint version.\n",
        "\n",
        "VERTEX_AI_MODEL_GARDEN_TIMESFM = \"gs://vertex-model-garden-public-us/timesfm\"  # @param {type:\"string\", isTemplate:true} [\"gs://vertex-model-garden-public-us/timesfm\", \"gs://vertex-model-garden-public-eu/timesfm\", \"gs://vertex-model-garden-public-asia/timesfm\"]\n",
        "MODEL_VARIANT = \"timesfm-1.0-200m\"  # @param [\"timesfm-1.0-200m\"]\n",
        "\n",
        "print(\n",
        "    \"Copying TimesFM model artifacts from\",\n",
        "    f\"{VERTEX_AI_MODEL_GARDEN_TIMESFM}/{MODEL_VARIANT}\",\n",
        "    \"to\",\n",
        "    MODEL_BUCKET,\n",
        ")\n",
        "\n",
        "! gsutil -m cp -r -R $VERTEX_AI_MODEL_GARDEN_TIMESFM/$MODEL_VARIANT $MODEL_BUCKET\n",
        "\n",
        "model_path_prefix = MODEL_BUCKET\n",
        "\n",
        "# The pre-built serving docker images.\n",
        "SERVE_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/jax-timesfm-serve:20240528_1310_RC00\"\n",
        "\n",
        "\n",
        "def get_job_name_with_datetime(prefix: str) -> str:\n",
        "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
        "\n",
        "    jobs in Vertex AI.\n",
        "    \"\"\"\n",
        "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
        "\n",
        "\n",
        "def deploy_model(\n",
        "    model_name: str,\n",
        "    checkpoint_path: str,\n",
        "    horizon: str,\n",
        "    machine_type: str = \"g2-standard-4\",\n",
        "    accelerator_type: str = \"NVIDIA_L4\",\n",
        "    accelerator_count: int = 1,\n",
        "    deploy_source: str = \"notebook\",\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Create a Vertex AI Endpoint and deploy the specified model to the endpoint.\"\"\"\n",
        "    model_name_with_time = get_job_name_with_datetime(model_name)\n",
        "    endpoint = aiplatform.Endpoint.create(\n",
        "        display_name=f\"{model_name_with_time}-endpoint\",\n",
        "        credentials=aiplatform.initializer.global_config.credentials,\n",
        "    )\n",
        "\n",
        "    if accelerator_type == \"ACCELERATOR_TYPE_UNSPECIFIED\":\n",
        "        timesfm_backend = \"cpu\"\n",
        "        accelerator_type = None\n",
        "    elif accelerator_type.startswith(\"NVIDIA\"):\n",
        "        timesfm_backend = \"gpu\"\n",
        "    else:\n",
        "        timesfm_backend = \"tpu\"\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name_with_time,\n",
        "        artifact_uri=checkpoint_path,\n",
        "        serving_container_image_uri=SERVE_DOCKER_URI,\n",
        "        serving_container_ports=[8080],\n",
        "        serving_container_predict_route=\"/predict\",\n",
        "        serving_container_health_route=\"/health\",\n",
        "        serving_container_environment_variables={\n",
        "            \"DEPLOY_SOURCE\": deploy_source,\n",
        "            \"TIMESFM_HORIZON\": str(horizon),\n",
        "            \"TIMESFM_BACKEND\": timesfm_backend,\n",
        "        },\n",
        "        credentials=aiplatform.initializer.global_config.credentials,\n",
        "    )\n",
        "    print(\n",
        "        f\"Deploying {model_name_with_time} on {machine_type} with\"\n",
        "        f\" {accelerator_count} {accelerator_type} GPU(s).\"\n",
        "    )\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=SERVICE_ACCOUNT,\n",
        "        enable_access_logging=True,\n",
        "        min_replica_count=1,\n",
        "        sync=True,\n",
        "    )\n",
        "    return model, endpoint\n",
        "\n",
        "\n",
        "def get_quota(project_id: str, region: str, resource_id: str) -> int:\n",
        "    \"\"\"Returns the quota for a resource in a region.\n",
        "\n",
        "    Returns -1 if can not figure out the quota.\n",
        "    \"\"\"\n",
        "    service_endpoint = \"aiplatform.googleapis.com\"\n",
        "    quota_list_output = !gcloud alpha services quota list --service=$service_endpoint  --consumer=projects/$project_id --filter=\"$service_endpoint/$resource_id\" --format=json\n",
        "    # Use '.s' on the command output because it is an SList type.\n",
        "    quota_data = json.loads(quota_list_output.s)\n",
        "    if len(quota_data) == 0 or \"consumerQuotaLimits\" not in quota_data[0]:\n",
        "        return -1\n",
        "    if (\n",
        "        len(quota_data[0][\"consumerQuotaLimits\"]) == 0\n",
        "        or \"quotaBuckets\" not in quota_data[0][\"consumerQuotaLimits\"][0]\n",
        "    ):\n",
        "        return -1\n",
        "    all_regions_data = quota_data[0][\"consumerQuotaLimits\"][0][\"quotaBuckets\"]\n",
        "    for region_data in all_regions_data:\n",
        "        if (\n",
        "            region_data.get(\"dimensions\")\n",
        "            and region_data[\"dimensions\"][\"region\"] == region\n",
        "        ):\n",
        "            if \"effectiveLimit\" in region_data:\n",
        "                return int(region_data[\"effectiveLimit\"])\n",
        "            else:\n",
        "                return 0\n",
        "    return -1\n",
        "\n",
        "\n",
        "def get_resource_id(accelerator_type: str, is_for_training: bool) -> str:\n",
        "    \"\"\"Returns the resource id for a given accelerator type and the use case.\n",
        "\n",
        "    Args:\n",
        "      accelerator_type: The accelerator type.\n",
        "      is_for_training: Whether the resource is used for training. Set false for\n",
        "        serving use case.\n",
        "\n",
        "    Returns:\n",
        "      The resource id.\n",
        "    \"\"\"\n",
        "    training_accelerator_map = {\n",
        "        \"NVIDIA_TESLA_V100\": \"custom_model_training_nvidia_v100_gpus\",\n",
        "        \"NVIDIA_L4\": \"custom_model_training_nvidia_l4_gpus\",\n",
        "        \"NVIDIA_TESLA_A100\": \"custom_model_training_nvidia_a100_gpus\",\n",
        "        \"ACCELERATOR_TYPE_UNSPECIFIED\": \"custom_model_training_cpus\",\n",
        "    }\n",
        "    serving_accelerator_map = {\n",
        "        \"NVIDIA_TESLA_V100\": \"custom_model_serving_nvidia_v100_gpus\",\n",
        "        \"NVIDIA_L4\": \"custom_model_serving_nvidia_l4_gpus\",\n",
        "        \"NVIDIA_TESLA_A100\": \"custom_model_serving_nvidia_a100_gpus\",\n",
        "        \"ACCELERATOR_TYPE_UNSPECIFIED\": \"custom_model_serving_cpus\",\n",
        "    }\n",
        "    if is_for_training:\n",
        "        if accelerator_type in training_accelerator_map:\n",
        "            return training_accelerator_map[accelerator_type]\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Could not find accelerator type: {accelerator_type} for training.\"\n",
        "            )\n",
        "    else:\n",
        "        if accelerator_type in serving_accelerator_map:\n",
        "            return serving_accelerator_map[accelerator_type]\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Could not find accelerator type: {accelerator_type} for serving.\"\n",
        "            )\n",
        "\n",
        "\n",
        "def check_quota(\n",
        "    project_id: str,\n",
        "    region: str,\n",
        "    accelerator_type: str,\n",
        "    accelerator_count: int,\n",
        "    is_for_training: bool,\n",
        "):\n",
        "    \"\"\"Checks if the project and the region has the required quota.\"\"\"\n",
        "    resource_id = get_resource_id(accelerator_type, is_for_training)\n",
        "    quota = get_quota(project_id, region, resource_id)\n",
        "    quota_request_instruction = (\n",
        "        \"Either use \"\n",
        "        \"a different region or request additional quota. Follow \"\n",
        "        \"instructions here \"\n",
        "        \"https://cloud.google.com/docs/quotas/view-manage#requesting_higher_quota\"\n",
        "        \" to check quota in a region or request additional quota for \"\n",
        "        \"your project.\"\n",
        "    )\n",
        "    if quota == -1:\n",
        "        raise ValueError(\n",
        "            f\"\"\"Quota not found for: {resource_id} in {region}.\n",
        "            {quota_request_instruction}\"\"\"\n",
        "        )\n",
        "    if quota < accelerator_count:\n",
        "        raise ValueError(\n",
        "            f\"\"\"Quota not enough for {resource_id} in {region}:\n",
        "            {quota} < {accelerator_count}.\n",
        "            {quota_request_instruction}\"\"\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLTYNURLi6LQ"
      },
      "source": [
        "## Deploy TimesFM to a Vertex AI Endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UwyXUWD9i9fe"
      },
      "outputs": [],
      "source": [
        "# @markdown This section uploads the prebuilt TimesFM model to Model Registry\n",
        "# @markdown and deploys it to a Vertex AI Endpoint.\n",
        "# @markdown It takes **approximately 20 minutes** to deploy.\n",
        "\n",
        "# @markdown ### **Step 1: Set the checkpoint path**\n",
        "# @markdown Leave this blank to load the checkpoint we copied over earlier.\n",
        "# @markdown If you've brought your own checkpoint, specify its path here.\n",
        "# @markdown\n",
        "# @markdown **Note**: Most of the time you should leave it blank (as is)\n",
        "# @markdown when you've chosen to use a prebuilt checkpoint.\n",
        "# @markdown\n",
        "\n",
        "custom_timesfm_model_uri = \"gs://\"  # @param {type: \"string\"}\n",
        "\n",
        "if custom_timesfm_model_uri == \"gs://\" or not custom_timesfm_model_uri:\n",
        "    print(\"Deploying prebuilt TimesFM model. \")\n",
        "    checkpoint_path = model_path_prefix\n",
        "else:\n",
        "    print(\"Deploying custom TimesFM model.\")\n",
        "    checkpoint_path = custom_timesfm_model_uri\n",
        "print(f\"Loading checkpoint from {checkpoint_path}.\")\n",
        "\n",
        "# @markdown ### **Step 2: Choose the accelerator**\n",
        "# @markdown Select the accelerator type to use to deploy the model.\n",
        "# @markdown\n",
        "# @markdown **Note**: Most of the time you can go with CPU only. TimesFM is\n",
        "# @markdown fast even with the CPU backend. You can only consider GPU if you\n",
        "# @markdown need a dedicated endpoint to handle large queries per second.\n",
        "# @markdown\n",
        "# @markdown **Note**: After deployment, please take a look at the log to get\n",
        "# @markdown the model / enpoint that you can use in another session.\n",
        "# @markdown\n",
        "\n",
        "accelerator_type = \"NVIDIA_L4\"  # @param [\"CPU\", \"NVIDIA_L4\"]\n",
        "if accelerator_type == \"NVIDIA_L4\":\n",
        "    machine_type = \"g2-standard-4\"\n",
        "    accelerator_count = 1\n",
        "elif accelerator_type == \"CPU\":\n",
        "    accelerator_type = \"ACCELERATOR_TYPE_UNSPECIFIED\"\n",
        "    machine_type = \"n1-standard-8\"\n",
        "    accelerator_count = 0\n",
        "else:\n",
        "    raise ValueError(\n",
        "        f\"Recommended machine settings not found for: {accelerator_type}. To use\"\n",
        "        \" another another accelerator, edit this code block to pass in an\"\n",
        "        \" appropriate `machine_type`, `accelerator_type`, and\"\n",
        "        \" `accelerator_count` to the deploy_model function by clicking `Show\"\n",
        "        \" Code` and then modifying the code.\"\n",
        "    )\n",
        "\n",
        "if accelerator_type != \"ACCELERATOR_TYPE_UNSPECIFIED\":\n",
        "    check_quota(\n",
        "        project_id=PROJECT_ID,\n",
        "        region=REGION,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        is_for_training=False,\n",
        "    )\n",
        "\n",
        "print(\"Quota is OK.\")\n",
        "# @markdown If you want to use other accelerator types not listed above, please\n",
        "# @markdown check other Vertex AI prediction supported accelerators and regions\n",
        "# @markdown at https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
        "# @markdown You may need to manually set the `machine_type`, `accelerator_type`,\n",
        "# @markdown and `accelerator_count` in the code by clicking `Show code` first.\n",
        "\n",
        "# @markdown ### **Step 3: Set the forecast horizon**\n",
        "# @markdown We need to specify the forecast horizon TimesFM will be queried on\n",
        "# @markdown to compile its computation. The endpoint will always predict this\n",
        "# @markdown number of time points in the future, possibly after being rounded\n",
        "# @markdown up to the closest multiplier of the model output patch length.\n",
        "# @markdown Make sure to set it to the potential maximum for your usecase.\n",
        "horizon = 256  # @param {type:\"number\"}\n",
        "print(\"Creating endpoint.\")\n",
        "model, endpoint = deploy_model(\n",
        "    model_name=f\"timesfm-{MODEL_VARIANT}\",\n",
        "    checkpoint_path=checkpoint_path,\n",
        "    horizon=horizon,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzCdSZsilcEv"
      },
      "source": [
        "## Query TimesFM\n",
        "\n",
        "An endpoint prediction request looks like\n",
        "```python\n",
        "endpoint.predict(instances=[{\"input\": [...], \"freq\": 0}, ...])\n",
        "```\n",
        "\n",
        "Now we can query the endpoint to forecast on input time series. Let's first start with some sanity checks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dBIpE8eRarWL"
      },
      "outputs": [],
      "source": [
        "# @title Create a helper function to visulize forecasts.\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def visualize_forecast(\n",
        "    context: list[float],\n",
        "    horizon_mean: list[float],\n",
        "    ground_truth: list[float] | None = None,\n",
        "    horizon_lower: list[float] | None = None,\n",
        "    horizon_upper: list[float] | None = None,\n",
        "    ylabel: str | None = None,\n",
        "    title: str | None = None,\n",
        "):\n",
        "    plt_range = list(range(len(context) + len(horizon_mean)))\n",
        "    plt.figure(figsize=(5, 3))\n",
        "    plt.plot(\n",
        "        plt_range,\n",
        "        context + [np.nan for _ in horizon_mean],\n",
        "        color=\"tab:cyan\",\n",
        "        label=\"context\",\n",
        "    )\n",
        "    plt.plot(\n",
        "        plt_range,\n",
        "        [np.nan for _ in context] + horizon_mean,\n",
        "        color=\"tab:red\",\n",
        "        label=\"forecast\",\n",
        "    )\n",
        "    if ground_truth:\n",
        "        plt.plot(\n",
        "            list(range(len(context) + len(ground_truth))),\n",
        "            [np.nan for _ in context] + ground_truth,\n",
        "            color=\"tab:purple\",\n",
        "            label=\"ground truth\",\n",
        "        )\n",
        "    if horizon_upper and horizon_lower:\n",
        "        plt.plot(\n",
        "            plt_range,\n",
        "            [np.nan for _ in context] + horizon_upper,\n",
        "            color=\"tab:orange\",\n",
        "            linestyle=\"--\",\n",
        "            label=\"forecast, upper\",\n",
        "        )\n",
        "        plt.plot(\n",
        "            plt_range,\n",
        "            [np.nan for _ in context] + horizon_lower,\n",
        "            color=\"tab:orange\",\n",
        "            linestyle=\":\",\n",
        "            label=\"forecast, lower\",\n",
        "        )\n",
        "        plt.fill_between(\n",
        "            plt_range,\n",
        "            [np.nan for _ in context] + horizon_upper,\n",
        "            [np.nan for _ in context] + horizon_lower,\n",
        "            color=\"tab:orange\",\n",
        "            alpha=0.2,\n",
        "        )\n",
        "    if ylabel:\n",
        "        plt.ylabel(ylabel)\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    plt.xlabel(\"time\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gf-ND9VwWb9"
      },
      "source": [
        "### Sanity checks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CuUe-P0xgIuU"
      },
      "outputs": [],
      "source": [
        "# @markdown We first check TimesFM on some sinusoidals. Pay attention to how\n",
        "# @markdown we are calling the endpoints.\n",
        "# Prepare the context. Notice each of them has a different context length.\n",
        "# Note: this is strictly how the query should be structed:\n",
        "instances = [\n",
        "    {\"input\": np.sin(np.linspace(0, 20, 100)).tolist(), \"freq\": 0},\n",
        "    {\"input\": np.sin(np.linspace(0, 40, 500)).tolist(), \"freq\": 0},\n",
        "    {\n",
        "        \"input\": (\n",
        "            np.sin(np.linspace(0, 50, 300)) + np.sin(np.linspace(1, 71, 300)) * 0.5\n",
        "        ).tolist(),\n",
        "        \"freq\": 0,\n",
        "    },\n",
        "]\n",
        "\n",
        "# Query the endpoint.\n",
        "results = endpoint.predict(instances=instances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zJqLvZ0Qh-oe"
      },
      "outputs": [],
      "source": [
        "# @markdown Now we visualize the response. Make sure the model makes legit\n",
        "# @markdown forecasts on those curves, and we move on to real world data.\n",
        "\n",
        "# There's bunch of important stuff in the results. Here we focus on results[0]:\n",
        "# This is the TimesFM response.\n",
        "print(results[0][0].keys())\n",
        "visualize_forecast(\n",
        "    instances[0][\"input\"], results[0][0][\"point_forecast\"], title=\"Sinusoidal 1\"\n",
        ")\n",
        "visualize_forecast(\n",
        "    instances[1][\"input\"], results[0][1][\"point_forecast\"], title=\"Sinusoidal 2\"\n",
        ")\n",
        "visualize_forecast(\n",
        "    instances[2][\"input\"], results[0][2][\"point_forecast\"], title=\"Sinusoidal 3\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otoyBWG8vIfo"
      },
      "source": [
        "### Point forecast\n",
        "\n",
        "Let's use a real world dataset from Kaggle on the [daily temperatures in Delhi, India](https://www.kaggle.com/datasets/sumanthvrao/daily-climate-time-series-data/data). Make sure you've set the Kaggle credentials following [these instructions](https://github.com/Kaggle/kaggle-api/blob/main/docs/README.md#api-credentials).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsezQiCfnCwH"
      },
      "outputs": [],
      "source": [
        "# Install the dependencies.\n",
        "! pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-zztflGnMpH"
      },
      "outputs": [],
      "source": [
        "# Download and prepare the dataset\n",
        "! kaggle datasets download sumanthvrao/daily-climate-time-series-data\n",
        "! unzip /content/daily-climate-time-series-data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QREpLhPnxIR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"/content/DailyDelhiClimateTrain.csv\")\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msYISUQtojZo"
      },
      "outputs": [],
      "source": [
        "# We manually prepare 3 forecast tasks:\n",
        "# 1. Use day 0 - 199 to forecast day 200-299.\n",
        "# 2. Use day 300 - 599 to forecast day 600-699.\n",
        "# 3. Use day 700 - 1200 to forecast day 1200 - 1299.\n",
        "temperature = data.meantemp.to_list()\n",
        "inputs = [temperature[0:200], temperature[300:600], temperature[700:1200]]\n",
        "ground_truths = [\n",
        "    temperature[200:300],\n",
        "    temperature[600:700],\n",
        "    temperature[1200:1300],\n",
        "]\n",
        "response = endpoint.predict(\n",
        "    instances=[{\"input\": each_input, \"freq\": 0} for each_input in inputs]\n",
        ")\n",
        "response[0][0].keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy55NO_OqIKD"
      },
      "source": [
        "This `response` is structured that:\n",
        "* `response[0][i]` is the forecast result of the ith input inside `instances`.\n",
        "* `response[0][i]` has three keys:\n",
        " - `point_forecast`: the mean point forecast\n",
        " - `quantiles`: the schema of the quantile outputs\n",
        " - `quantile_forecast`: for each time stamp in the horizon this will be a list whose elements are the corresponding quantiles as denoted in the `quantiles` schema.\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWrOfE4A5vjo"
      },
      "outputs": [],
      "source": [
        "# Visualize the response\n",
        "for task_i in range(3):\n",
        "    visualize_forecast(\n",
        "        inputs[task_i],\n",
        "        response[0][task_i][\"point_forecast\"][:100],\n",
        "        ground_truth=ground_truths[0],\n",
        "        title=f\"Daily temperature in Delhi, India, Task {task_i+1}\",\n",
        "        ylabel=\"Temperature (°C)\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx8CvOjNvI0m"
      },
      "source": [
        "### Anomaly detection\n",
        "\n",
        "As of checkpoint TimesFM-1.0-200m, TimesFM is capable of outputing quantile forecasts as well. These are uncalibrated forecasts and are experimental. But please feel free to play with them to see what you can do with them.\n",
        "\n",
        "Here we show how these outputs can potentially serve as anomaly detectors, when we define the anomaly as something beyond a certain range of TimesFM forecasts. In this example we are drawing bands defined by the 30th and the 70th percentiles on the same tasks we did in the last section. Anything outside of the bands could be an \"anomaly\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab_NZ1hWiWwL"
      },
      "outputs": [],
      "source": [
        "# Visualize the response\n",
        "for task_i in range(3):\n",
        "    visualize_forecast(\n",
        "        inputs[task_i],\n",
        "        response[0][task_i][\"point_forecast\"][:100],\n",
        "        ground_truth=ground_truths[0],\n",
        "        horizon_lower=[x[3] for x in response[0][task_i][\"quantile_forecast\"]][:100],\n",
        "        horizon_upper=[x[7] for x in response[0][task_i][\"quantile_forecast\"]][:100],\n",
        "        title=f\"Daily temperature in Delhi, India, Task {task_i+1}\",\n",
        "        ylabel=\"Temperature (°C)\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jcS6Tb0pg1x"
      },
      "source": [
        "## Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5P_TAFcmlmdV"
      },
      "outputs": [],
      "source": [
        "# @title Releasing endpoint and model\n",
        "# @markdown Delete the experiment models and endpoints to recycle the resources\n",
        "# @markdown and avoid unnecessary continouous charges that may incur.\n",
        "\n",
        "# Undeploy model and delete endpoint.\n",
        "endpoint.delete(force=True)\n",
        "\n",
        "# Delete models.\n",
        "if model:\n",
        "    model.delete()\n",
        "\n",
        "delete_bucket = False  # @param {type:\"boolean\"}\n",
        "if delete_bucket:\n",
        "    ! gsutil -m rm -r $BUCKET_URI"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "2gf-ND9VwWb9",
        "otoyBWG8vIfo",
        "Tx8CvOjNvI0m",
        "0jcS6Tb0pg1x"
      ],
      "name": "model_garden_timesfm_deployment_on_vertex.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

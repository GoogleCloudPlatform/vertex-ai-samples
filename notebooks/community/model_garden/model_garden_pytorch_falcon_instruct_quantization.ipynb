{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d9bbf86da5e"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Vertex AI Model Garden - Falcon Instruct (Quantization)\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_pytorch_falcon_instruct_quantization.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_falcon_instruct_quantization.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates running inferences quantizing and deploying Falcon Instruct models with [GPTQ](https://arxiv.org/abs/2210.17323) Falcon Instruct models in Vertex AI.\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Quantize Falcon Instruct models using AutoGPTQ\n",
        "- Clean up the resources\n",
        "\n",
        "| Models | LoRA |\n",
        "| :- | :- |\n",
        "| [tiiuae/falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct) | Y |\n",
        "| [tiiuae/falcon-40b-instruct](https://huggingface.co/tiiuae/falcon-40b-instruct) | Y |\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "264c07757582"
      },
      "source": [
        "## Run the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "855d6b96f291"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown 2. [Optional] [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs. Set the BUCKET_URI for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`) should be located in the same region as where the notebook was launched. Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\"). If not set, a unique GCS bucket will be created instead.\n",
        "\n",
        "# Import the necessary packages\n",
        "import os\n",
        "from datetime import datetime\n",
        "from typing import Tuple\n",
        "\n",
        "from google.cloud import aiplatform, language\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
        "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
        "\n",
        "# Cloud Storage bucket for storing the experiment artifacts.\n",
        "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
        "# prefer using your own GCS bucket, please change the value yourself below.\n",
        "now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
        "assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
        "\n",
        "# Create a unique GCS bucket for this notebook, if not specified by the user.\n",
        "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
        "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}\"\n",
        "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
        "else:\n",
        "    shell_output = ! gsutil ls -Lb {BUCKET_URI} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
        "    bucket_region = shell_output[0].strip().lower()\n",
        "    if bucket_region != REGION:\n",
        "        raise ValueError(\n",
        "            \"Bucket region %s is different from notebook region %s\"\n",
        "            % (bucket_region, REGION)\n",
        "        )\n",
        "\n",
        "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
        "\n",
        "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
        "EXPERIMENT_BUCKET = os.path.join(BUCKET_URI, \"peft\")\n",
        "DATA_BUCKET = os.path.join(EXPERIMENT_BUCKET, \"data\")\n",
        "MODEL_BUCKET = os.path.join(EXPERIMENT_BUCKET, \"model\")\n",
        "\n",
        "# Set up default SERVICE_ACCOUNT\n",
        "SERVICE_ACCOUNT = None\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
        "\n",
        "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
        "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "# The pre-built training and serving docker images.\n",
        "TRAIN_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-train:20240409_0936_RC00\"\n",
        "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20231127_0916_RC00\"\n",
        "VLLM_GPTQ_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240126_0916_RC00\"\n",
        "\n",
        "\n",
        "def get_job_name_with_datetime(prefix: str) -> str:\n",
        "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
        "    jobs in Vertex AI.\n",
        "    \"\"\"\n",
        "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
        "\n",
        "\n",
        "def deploy_model(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    service_account: str,\n",
        "    machine_type: str = \"n1-standard-8\",\n",
        "    accelerator_type: str = \"NVIDIA_TESLA_V100\",\n",
        "    accelerator_count: int = 1,\n",
        "    quantization_method: str = \"\",\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
        "\n",
        "    vllm_args = [\n",
        "        \"--host=0.0.0.0\",\n",
        "        \"--port=7080\",\n",
        "        f\"--model={model_id}\",\n",
        "        f\"--tensor-parallel-size={accelerator_count}\",\n",
        "        \"--swap-space=16\",\n",
        "        \"--gpu-memory-utilization=0.9\",\n",
        "        \"--disable-log-stats\",\n",
        "        \"--dtype=float16\",\n",
        "        \"--trust-remote-code\",\n",
        "    ]\n",
        "    serving_env = {\n",
        "        \"MODEL_ID\": \"tiiuae/falcon-instruct-quantized\",\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "    }\n",
        "    if quantization_method:\n",
        "        vllm_args.append(f\"--quantization={quantization_method}\")\n",
        "    if quantization_method == \"gptq\":\n",
        "        vllm_docker_uri = VLLM_GPTQ_DOCKER_URI\n",
        "    else:\n",
        "        vllm_docker_uri = VLLM_DOCKER_URI\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=vllm_docker_uri,\n",
        "        serving_container_command=[\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
        "        serving_container_args=vllm_args,\n",
        "        serving_container_ports=[7080],\n",
        "        serving_container_predict_route=\"/generate\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "        serving_container_environment_variables=serving_env,\n",
        "    )\n",
        "\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=service_account,\n",
        "    )\n",
        "    return model, endpoint\n",
        "\n",
        "\n",
        "def moderate_text(text: str) -> language.ModerateTextResponse:\n",
        "    \"\"\"Calls Vertex AI APIs to analyze text moderations.\"\"\"\n",
        "    client = language.LanguageServiceClient()\n",
        "    document = language.Document(\n",
        "        content=text,\n",
        "        type_=language.Document.Type.PLAIN_TEXT,\n",
        "    )\n",
        "    return client.moderate_text(document=document)\n",
        "\n",
        "\n",
        "def show_text_moderation(text: str, response: language.ModerateTextResponse) -> None:\n",
        "    \"\"\"Shows text moderation results.\"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    def confidence(category: language.ClassificationCategory) -> float:\n",
        "        return category.confidence\n",
        "\n",
        "    columns = [\"category\", \"confidence\"]\n",
        "    categories = sorted(response.moderation_categories, key=confidence, reverse=True)\n",
        "    data = ((category.name, category.confidence) for category in categories)\n",
        "    df = pd.DataFrame(columns=columns, data=data)\n",
        "\n",
        "    print(f\"Text analyzed:\\n{text}\")\n",
        "    print(df.to_markdown(index=False, tablefmt=\"presto\", floatfmt=\".0%\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_SFje2ifjNIx"
      },
      "outputs": [],
      "source": [
        "# @title Deploy Quantized Falcon Instruct models  with Google Cloud Text Moderation\n",
        "\n",
        "# @markdown This section demonstrates post-training quantization of Falcon Instruct models with Vertex Custom Job. Quantization reduces the memory required by a model while attempting to retain the same performance. Read more about GPTQ in the following publication: [GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers](https://arxiv.org/abs/2210.17323).\n",
        "\n",
        "# @markdown Many GPTQ-quantized models are provided [here](https://huggingface.co/TheBloke?search_models=-gptq).\n",
        "\n",
        "# @markdown This section uploads the model to Model Registry and deploys it on the Endpoint.\n",
        "\n",
        "# @markdown The model deployment step will take 15 minutes to 1 hour to complete, depending on the model sizes.\n",
        "\n",
        "# @markdown Notice that deploying a quantized requires much less GPU. We can deploy a quantized 40B model with only two L4s instead of four.\n",
        "\n",
        "quantized_model_id = \"TheBloke/Falcon-7B-Instruct-GPTQ\"  # @param [\"TheBloke/Falcon-7B-Instruct-GPTQ\", \"TheBloke/falcon-40b-instruct-GPTQ\"]\n",
        "\n",
        "quantization_method = \"gptq\"\n",
        "\n",
        "# Finds Vertex AI prediction supported accelerators and regions in\n",
        "# https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
        "\n",
        "\n",
        "if \"7B\" in quantized_model_id:\n",
        "    # Sets 1 L4 (24G) to deploy Falcon Instruct 7B model.\n",
        "    machine_type = \"g2-standard-8\"\n",
        "    accelerator_type = \"NVIDIA_L4\"\n",
        "    accelerator_count = 1\n",
        "elif \"40b\" in quantized_model_id:\n",
        "    # Sets 2 L4's (24G) to deploy Falcon Instruct 40B model.\n",
        "    machine_type = \"g2-standard-24\"\n",
        "    accelerator_type = \"NVIDIA_L4\"\n",
        "    accelerator_count = 2\n",
        "else:\n",
        "    raise ValueError(f\"Recommended GPU setting not found for: {quantized_model_id}.\")\n",
        "\n",
        "model_prequantized_vllm, endpoint_prequantized_vllm = deploy_model(\n",
        "    model_name=get_job_name_with_datetime(\n",
        "        prefix=\"falcon-instruct-serve-vllm-prequantized\"\n",
        "    ),\n",
        "    model_id=quantized_model_id,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    quantization_method=quantization_method,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HPFMjyMEs0qt"
      },
      "outputs": [],
      "source": [
        "# @markdown NOTE: After the deployment succeeds, the model weights will be downloaded on the fly. Thus additional 10 ~ 40 minutes (depending on the model sizes) of waiting time is needed **after** the above model deployment step succeeds and before you run the next step below. Otherwise you might see a `ServiceUnavailable: 503 502:Bad Gateway` error when you send requests to the endpoint.\n",
        "\n",
        "# @markdown Once deployment succeeds, you can send requests to the endpoint with text prompts.\n",
        "\n",
        "# @markdown Example:\n",
        "\n",
        "# @markdown ```\n",
        "# @markdown Human: What is a car?\n",
        "# @markdown Assistant:  A car, or a motor car, is a road-connected human-transportation system used to move people or goods from one place to another. The term also encompasses a wide range of vehicles, including motorboats, trains, and aircrafts. Cars typically have four wheels, a cabin for passengers, and an engine or motor. They have been around since the early 19th century and are now one of the most popular forms of transportation, used for daily commuting, shopping, and other purposes.\n",
        "# @markdown ```\n",
        "\n",
        "# Loads an existing endpoint instance using the endpoint name:\n",
        "# - Using `endpoint_name = endpoint_prequantized_vllm.name` allows us to get the\n",
        "#   endpoint name of the endpoint `endpoint_prequantized_vllm` created in the cell\n",
        "#   above.\n",
        "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
        "#   an existing endpoint with the ID 1234567890123456789.\n",
        "# You may uncomment the code below to load an existing endpoint.\n",
        "\n",
        "# endpoint_name = endpoint_prequantized_vllm.name\n",
        "# # endpoint_name = \"\"  # @param {type:\"string\"}\n",
        "# aip_endpoint_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
        "# )\n",
        "# endpoint_prequantized_vllm = aiplatform.Endpoint(aip_endpoint_name)\n",
        "\n",
        "prompt = \"What is a car?\"  # @param {type: \"string\"}\n",
        "max_tokens = 50  # @param {type:\"integer\"}\n",
        "temperature = 1.0  # @param {type:\"number\"}\n",
        "top_p = 1.0  # @param {type:\"number\"}\n",
        "top_k = 10  # @param {type:\"integer\"}\n",
        "\n",
        "# Overides max_length and top_k parameters during inferences.\n",
        "# If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`,\n",
        "# you can reduce the max length, such as set max_length as 20.\n",
        "# If you are using L4 GPUs to serve Falcon Instruct 40B models, you should set\n",
        "# max_length to around 1,000 tokens or fewer. If you need longer generated\n",
        "# sequences, please file a request with Vertex to allowlist your project for a\n",
        "# longer timeout threshold with Vertex endpoints.\n",
        "instances = [\n",
        "    {\n",
        "        \"prompt\": \"What is a car?\",\n",
        "        \"max_tokens\": 50,\n",
        "        \"temperature\": 1.0,\n",
        "        \"top_p\": 1.0,\n",
        "        \"top_k\": 10,\n",
        "    },\n",
        "]\n",
        "response = endpoint_prequantized_vllm.predict(instances=instances)\n",
        "\n",
        "for prediction in response.predictions:\n",
        "    print(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GR2a8rzvs_kk"
      },
      "outputs": [],
      "source": [
        "# @markdown Text moderation analyzes a document against a list of safety attributes, which include \"harmful categories\" and topics that may be considered sensitive.\n",
        "\n",
        "for generated_text in response.predictions:\n",
        "    # Send a request to the API.\n",
        "    response = moderate_text(generated_text)\n",
        "    # Show the results.\n",
        "    show_text_moderation(generated_text, response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MEquWSCkmrJ6"
      },
      "outputs": [],
      "source": [
        "# @title Quantize Falcon Instruct models with GPTQ\n",
        "\n",
        "# @markdown Quantization reduces the amount of GPU required to serve a model by reducing the bit precision of the weights while minimizing drop in performance. Serving quantized models on VLLM requires models to be quantized to 4 bits. It is recommended to first search if a model has already been quantized and made publicly available: [GPTQ](https://huggingface.co/TheBloke?search_models=-gptq).\n",
        "\n",
        "# @markdown Quantizing models with GPTQ will take around 40 minutes for Falcon Instruct 7B using 1 NVIDIA_L4 GPU and will take around 4 hours for Falcon Instruct 40B using 4 NVIDIA_L4 GPU.\n",
        "\n",
        "# @markdown Finetuned models can also be quantized, so long as the LoRA weights are merged with the base model.\n",
        "\n",
        "# @markdown Set the base model id.\n",
        "base_model_id = \"tiiuae/falcon-7b-instruct\"  # @param [\"tiiuae/falcon-7b-instruct\", \"tiiuae/falcon-40b-instruct\"]\n",
        "\n",
        "# Set up quantization job.\n",
        "\n",
        "# Set `finetuned_model_path` to the GCS path of the merged finetuned model\n",
        "# from the section above, if not set, the base model will be quantized.\n",
        "finetuned_model_path = \"\"  # @param {type:\"string\"}\n",
        "if finetuned_model_path:\n",
        "    prequantized_model_path = finetuned_model_path\n",
        "else:\n",
        "    prequantized_model_path = base_model_id\n",
        "\n",
        "quantization_method = \"gptq\"\n",
        "quantization_job_name = get_job_name_with_datetime(\n",
        "    f\"falcon-instruct-{quantization_method}-quantize\"\n",
        ")\n",
        "\n",
        "quantization_output_dir = os.path.join(MODEL_BUCKET, quantization_job_name)\n",
        "quantization_output_dir_gcsfuse = quantization_output_dir.replace(\"gs://\", \"/gcs/\")\n",
        "\n",
        "# Worker pool spec.\n",
        "\n",
        "# Set 1 L4 (24G) for quantizing 7b model.\n",
        "machine_type = \"g2-standard-32\"\n",
        "accelerator_type = \"NVIDIA_L4\"\n",
        "accelerator_count = 1\n",
        "\n",
        "# Set 4 L4 (24G) for quantizing 40b model.\n",
        "# machine_type = \"g2-standard-48\"\n",
        "# accelerator_type = \"NVIDIA_L4\"\n",
        "# accelerator_count = 4\n",
        "\n",
        "# Quantization parameters.\n",
        "quantization_precision_mode = \"4bit\"\n",
        "\n",
        "# The original datasets used in GPTQ paper.\n",
        "gptq_dataset_name = \"c4\"  # @param [\"wikitext2\",\"c4\",\"c4-new\",\"ptb\",\"ptb-new\"]\n",
        "group_size = -1\n",
        "damp_percent = 0.1\n",
        "desc_act = True\n",
        "quantization_args = [\n",
        "    \"--task=quantize-model\",\n",
        "    f\"--quantization_method={quantization_method}\",\n",
        "    f\"--pretrained_model_id={base_model_id}\",\n",
        "    f\"--quantization_precision_mode={quantization_precision_mode}\",\n",
        "    f\"--quantization_output_dir={quantization_output_dir_gcsfuse}\",\n",
        "    f\"--quantization_dataset_name={gptq_dataset_name}\",\n",
        "    f\"--group_size={group_size}\",\n",
        "    f\"--damp_percent={damp_percent}\",\n",
        "    f\"--desc_act={desc_act}\",\n",
        "    \"--cache_examples_on_gpu=False\",\n",
        "]\n",
        "\n",
        "# Pass quantization arguments and launch job.\n",
        "worker_pool_specs = [\n",
        "    {\n",
        "        \"machine_spec\": {\n",
        "            \"machine_type\": machine_type,\n",
        "            \"accelerator_type\": accelerator_type,\n",
        "            \"accelerator_count\": accelerator_count,\n",
        "        },\n",
        "        \"replica_count\": 1,\n",
        "        \"disk_spec\": {\n",
        "            \"boot_disk_type\": \"pd-ssd\",\n",
        "            \"boot_disk_size_gb\": 500,\n",
        "        },\n",
        "        \"container_spec\": {\n",
        "            \"image_uri\": TRAIN_DOCKER_URI,\n",
        "            \"env\": [\n",
        "                {\n",
        "                    \"name\": \"PYTORCH_CUDA_ALLOC_CONF\",\n",
        "                    \"value\": \"max_split_size_mb:32\",\n",
        "                },\n",
        "            ],\n",
        "            \"command\": [],\n",
        "            \"args\": quantization_args,\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"Quantizing {prequantized_model_path}.\")\n",
        "quantize_job = aiplatform.CustomJob(\n",
        "    display_name=quantization_job_name,\n",
        "    project=PROJECT_ID,\n",
        "    worker_pool_specs=worker_pool_specs,\n",
        "    staging_bucket=STAGING_BUCKET,\n",
        ")\n",
        "quantize_job.run()\n",
        "\n",
        "print(\"Quantized models were saved in: \", quantization_output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0SpJ_lbatIf7"
      },
      "outputs": [],
      "source": [
        "# @title Deploy newly quantized models with Google Cloud Text Moderation\n",
        "\n",
        "# @markdown This section uploads the model to Model Registry and deploys it on the Endpoint.\n",
        "\n",
        "# @markdown The model deployment step will take 15 minutes to 1 hour to complete, depending on the model sizes.\n",
        "\n",
        "# @markdown Notice that deploying a quantized model requires much less GPU. We can deploy a quantized 40B model with only two L4 GPUs instead of four.\n",
        "\n",
        "# Finds Vertex AI prediction supported accelerators and regions in\n",
        "# https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
        "\n",
        "# Sets 1 L4 (24G) to deploy Falcon Instruct 7B.\n",
        "machine_type = \"g2-standard-8\"\n",
        "accelerator_type = \"NVIDIA_L4\"\n",
        "accelerator_count = 1\n",
        "\n",
        "# Sets 2 L4's (24G) to deploy Falcon Instruct 70B models.\n",
        "# machine_type = \"g2-standard-24\"\n",
        "# accelerator_type = \"NVIDIA_L4\"\n",
        "# accelerator_count = 2\n",
        "\n",
        "model_quantized_vllm, endpoint_quantized_vllm = deploy_model(\n",
        "    model_name=get_job_name_with_datetime(\n",
        "        prefix=\"falcon-instruct-serve-vllm-quantized\"\n",
        "    ),\n",
        "    model_id=quantization_output_dir,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    quantization_method=quantization_method,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "twm_awnMtQa2"
      },
      "outputs": [],
      "source": [
        "# @markdown NOTE: After the deployment succeeds, the model weights will be downloaded on the fly. Thus additional 10 ~ 40 minutes (depending on the model sizes) of waiting time is needed **after** the above model deployment step succeeds and before you run the next step below. Otherwise you might see a `ServiceUnavailable: 503 502:Bad Gateway` error when you send requests to the endpoint.\n",
        "\n",
        "# @markdown Once deployment succeeds, you can send requests to the endpoint with text prompts.\n",
        "\n",
        "# @markdown Example:\n",
        "\n",
        "# @markdown ```\n",
        "# @markdown Human: What is a car?\n",
        "# @markdown Assistant:  A car, or a motor car, is a road-connected human-transportation system used to move people or goods from one place to another. The term also encompasses a wide range of vehicles, including motorboats, trains, and aircrafts. Cars typically have four wheels, a cabin for passengers, and an engine or motor. They have been around since the early 19th century and are now one of the most popular forms of transportation, used for daily commuting, shopping, and other purposes.\n",
        "# @markdown ```\n",
        "\n",
        "# Loads an existing endpoint instance using the endpoint name:\n",
        "# - Using `endpoint_name = endpoint_quantized_vllm.name` allows us to get the\n",
        "#   endpoint name of the endpoint `endpoint_quantized_vllm` created in the cell\n",
        "#   above.\n",
        "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
        "#   an existing endpoint with the ID 1234567890123456789.\n",
        "# You may uncomment the code below to load an existing endpoint.\n",
        "\n",
        "# endpoint_name = endpoint_quantized_vllm.name\n",
        "# # endpoint_name = \"\"  # @param {type:\"string\"}\n",
        "# aip_endpoint_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
        "# )\n",
        "# endpoint_quantized_vllm = aiplatform.Endpoint(aip_endpoint_name)\n",
        "\n",
        "\n",
        "prompt = \"What is a car?\"  # @param {type: \"string\"}\n",
        "max_tokens = 50  # @param {type:\"integer\"}\n",
        "temperature = 1.0  # @param {type:\"number\"}\n",
        "top_p = 1.0  # @param {type:\"number\"}\n",
        "top_k = 10  # @param {type:\"integer\"}\n",
        "\n",
        "# Overides max_length and top_k parameters during inferences.\n",
        "# If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`,\n",
        "# you can reduce the max length, such as set max_length as 20.\n",
        "# If you are using L4 GPUs to serve Falcon Instruct 40B models, you should set\n",
        "# max_length to around 1,000 tokens or fewer. If you need longer generated\n",
        "# sequences, please file a request with Vertex to allowlist your project for a\n",
        "# longer timeout threshold with Vertex endpoints.\n",
        "instances = [\n",
        "    {\n",
        "        \"prompt\": \"What is a car?\",\n",
        "        \"max_tokens\": 50,\n",
        "        \"temperature\": 1.0,\n",
        "        \"top_p\": 1.0,\n",
        "        \"top_k\": 10,\n",
        "    },\n",
        "]\n",
        "response = endpoint_quantized_vllm.predict(instances=instances)\n",
        "\n",
        "for prediction in response.predictions:\n",
        "    print(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fAoVzA69teui"
      },
      "outputs": [],
      "source": [
        "# @markdown Text moderation analyzes a document against a list of safety attributes, which include \"harmful categories\" and topics that may be considered sensitive.\n",
        "\n",
        "for generated_text in response.predictions:\n",
        "    # Send a request to the API.\n",
        "    response = moderate_text(generated_text)\n",
        "    # Show the results.\n",
        "    show_text_moderation(generated_text, response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "911406c1561e"
      },
      "outputs": [],
      "source": [
        "# @title Clean up resources\n",
        "\n",
        "delete_bucket = False  # @param {type:\"boolean\"}\n",
        "if delete_bucket:\n",
        "    ! gsutil -m rm -r $BUCKET_URI\n",
        "    # Uncomment below to delete all artifacts\n",
        "    # !gsutil -m rm -r $STAGING_BUCKET $MODEL_BUCKET $EXPERIMENT_BUCKET $DATA_BUCKET\n",
        "\n",
        "# Delete custom train and quantization jobs.\n",
        "quantize_job.delete()\n",
        "\n",
        "# Undeploy models and delete endpoints.\n",
        "endpoint_prequantized_vllm.delete(force=True)\n",
        "endpoint_quantized_vllm.delete(force=True)\n",
        "\n",
        "# Delete models.\n",
        "model_prequantized_vllm.delete()\n",
        "model_quantized_vllm.delete()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_pytorch_falcon_instruct_quantization.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

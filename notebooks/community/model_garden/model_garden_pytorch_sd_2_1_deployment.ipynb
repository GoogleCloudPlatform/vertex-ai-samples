{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7d9bbf86da5e"
      },
      "outputs": [],
      "source": [
        "# Copyright 2026 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Vertex AI Model Garden - Stable Diffusion V2.1 Deployment\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_pytorch_sd_2_1_deployment.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_sd_2_1_deployment.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates deploying the Stable Diffusion [stabilityai/stable-diffusion-2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1)  model on Vertex AI for online prediction.\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Deploy the model to a [Vertex AI Endpoint resource](https://cloud.google.com/vertex-ai/docs/predictions/using-private-endpoints).\n",
        "- Run online predictions for text-to-image or text-guided-image-to-image.\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcFB_OmYsrR_"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2707b02ef5df"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown 2. **[Optional]** [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs. Set the BUCKET_URI for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`) should be located in the same region as where the notebook was launched. Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\"). If not set, a unique GCS bucket will be created instead.\n",
        "\n",
        "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown 3. **[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment.\n",
        "\n",
        "REGION = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown 4. If you want to run predictions with A100 80GB or H100 GPUs, we recommend using the regions listed below. **NOTE:** Make sure you have associated quota in selected regions. Click the links to see your current quota for each GPU type: [Nvidia A100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_a100_80gb_gpus), [Nvidia H100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_h100_gpus). You can request for quota following the instructions at [\"Request a higher quota\"](https://cloud.google.com/docs/quota/view-manage#requesting_higher_quota).\n",
        "\n",
        "# @markdown | Machine Type | Accelerator Type | Recommended Regions |\n",
        "# @markdown | ----------- | ----------- | ----------- |\n",
        "# @markdown | a2-ultragpu-1g | 1 NVIDIA_A100_80GB | us-central1, us-east4, europe-west4, asia-southeast1, us-east4 |\n",
        "# @markdown | a3-highgpu-2g | 2 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "# @markdown | a3-highgpu-4g | 4 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "# @markdown | a3-highgpu-8g | 8 NVIDIA_H100_80GB | us-central1, europe-west4, us-west1, asia-southeast1 |\n",
        "\n",
        "import datetime\n",
        "import importlib\n",
        "import math\n",
        "import os\n",
        "import uuid\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
        "\n",
        "common_util = importlib.import_module(\n",
        "    \"vertex-ai-samples.notebooks.community.model_garden.docker_source_codes.notebook_util.common_util\"\n",
        ")\n",
        "\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "if not REGION:\n",
        "    if not os.environ.get(\"GOOGLE_CLOUD_REGION\"):\n",
        "        raise ValueError(\n",
        "            \"REGION must be set. See\"\n",
        "            \" https://cloud.google.com/vertex-ai/docs/general/locations for\"\n",
        "            \" available cloud locations.\"\n",
        "        )\n",
        "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
        "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
        "\n",
        "# Cloud Storage bucket for storing the experiment artifacts.\n",
        "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
        "# prefer using your own GCS bucket, change the value yourself below.\n",
        "now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "\n",
        "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
        "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}-{str(uuid.uuid4())[:4]}\"\n",
        "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
        "else:\n",
        "    assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
        "    shell_output = ! gsutil ls -Lb {BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
        "    bucket_region = shell_output[0].strip().lower()\n",
        "    if bucket_region != REGION:\n",
        "        raise ValueError(\n",
        "            \"Bucket region %s is different from notebook region %s\"\n",
        "            % (bucket_region, REGION)\n",
        "        )\n",
        "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
        "\n",
        "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
        "MODEL_BUCKET = os.path.join(BUCKET_URI, \"stable_diffusion_v2_1\")\n",
        "\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "# Gets the default SERVICE_ACCOUNT.\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
        "\n",
        "\n",
        "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
        "\n",
        "! gcloud config set project $PROJECT_ID\n",
        "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/storage.admin\"\n",
        "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/aiplatform.user\"\n",
        "\n",
        "models, endpoints = {}, {}\n",
        "\n",
        "# The pre-built serving docker image. It contains serving scripts and models.\n",
        "TEXT_TO_IMAGE_DOCKER_URI = \"us-docker.pkg.dev/deeplearning-platform-release/vertex-model-garden/pytorch-inference.cu125.0-4.ubuntu2204.py310\"\n",
        "IMAGE_TO_IMAGE_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-diffusers-serve-opt:20240605_1400_RC00\"\n",
        "\n",
        "\n",
        "def deploy_model(model_id, task, accelerator_type, machine_type, accelerator_count=1):\n",
        "    \"\"\"Create a Vertex AI Endpoint and deploy the specified model to the endpoint.\"\"\"\n",
        "    common_util.check_quota(\n",
        "        project_id=PROJECT_ID,\n",
        "        region=REGION,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        is_for_training=False,\n",
        "    )\n",
        "\n",
        "    model_name = model_id\n",
        "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-{task}-endpoint\")\n",
        "    serving_env = {\n",
        "        \"MODEL_ID\": model_id,\n",
        "        \"TASK\": task,\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "    }\n",
        "\n",
        "    if task == \"image-to-image\":\n",
        "        model = aiplatform.Model.upload(\n",
        "            display_name=model_name,\n",
        "            serving_container_image_uri=IMAGE_TO_IMAGE_DOCKER_URI,\n",
        "            serving_container_ports=[7080],\n",
        "            serving_container_predict_route=\"/predictions/diffusers_serving\",\n",
        "            serving_container_health_route=\"/ping\",\n",
        "            serving_container_environment_variables=serving_env,\n",
        "            model_garden_source_model_name=\"publishers/stability-ai/models/stable-diffusion-2-1\",\n",
        "        )\n",
        "    else:\n",
        "        model = aiplatform.Model.upload(\n",
        "            display_name=model_name,\n",
        "            serving_container_image_uri=TEXT_TO_IMAGE_DOCKER_URI,\n",
        "            serving_container_ports=[7080],\n",
        "            serving_container_predict_route=\"/predict\",\n",
        "            serving_container_health_route=\"/health\",\n",
        "            serving_container_environment_variables=serving_env,\n",
        "            model_garden_source_model_name=\"publishers/stability-ai/models/stable-diffusion-2-1\",\n",
        "        )\n",
        "\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=SERVICE_ACCOUNT,\n",
        "        system_labels={\"NOTEBOOK_NAME\": \"model_garden_pytorch_sd_2_1_deployment.ipynb\"},\n",
        "    )\n",
        "    print(\"To load this existing endpoint from a different session:\")\n",
        "    print(\n",
        "        f'endpoint = aiplatform.Endpoint(\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint.name}\")'\n",
        "    )\n",
        "    return model, endpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7KVTWyUuFq3"
      },
      "source": [
        "## Deploy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3b9b98972f49"
      },
      "outputs": [],
      "source": [
        "# @title Deploy the Stable Diffusion model to Vertex\n",
        "\n",
        "# @markdown This section uploads the model to Model Registry and deploys it on the Endpoint. It takes ~15 minutes to finish.\n",
        "# @markdown Click \"Show Code\" to see more details.\n",
        "\n",
        "# @markdown `text-to-image` lets you send text prompts to the endpoint to generate images.\n",
        "\n",
        "# @markdown `image-to-image` lets you send text prompts and an initial image to the endpoint to\n",
        "# @markdown condition the generation of new images.\n",
        "\n",
        "\n",
        "model_id = \"stabilityai/stable-diffusion-2-1\"\n",
        "\n",
        "task = \"text-to-image\"  # @param [\"text-to-image\", \"image-to-image\"]\n",
        "accelerator_type = \"NVIDIA_L4\"  # @param [\"NVIDIA_L4\", \"NVIDIA_A100_80GB\"]\n",
        "\n",
        "if accelerator_type == \"NVIDIA_L4\":\n",
        "    machine_type = \"g2-standard-8\"\n",
        "elif accelerator_type == \"NVIDIA_A100_80GB\":\n",
        "    machine_type = \"a2-ultragpu-1g\"\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported accelerator type: {accelerator_type}\")\n",
        "\n",
        "common_util.check_quota(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=1,\n",
        "    is_for_training=False,\n",
        ")\n",
        "\n",
        "models[\"image_model\"], endpoints[\"image_model\"] = deploy_model(\n",
        "    model_id=model_id,\n",
        "    task=task,\n",
        "    accelerator_type=accelerator_type,\n",
        "    machine_type=machine_type,\n",
        ")\n",
        "print(\"endpoint_name:\", endpoints[\"image_model\"].name)\n",
        "\n",
        "# Loads an existing endpoint instance using the endpoint name:\n",
        "# - Using `endpoint_name = endpoint.name` allows us to get the\n",
        "#   endpoint name of the endpoint `endpoint` created in the cell\n",
        "#   above.\n",
        "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
        "#   an existing endpoint with the ID 1234567890123456789.\n",
        "# You may uncomment the code below to load an existing endpoint.\n",
        "\n",
        "# endpoint_name = \"\"  # @param {type:\"string\"}\n",
        "# aip_endpoint_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
        "# )\n",
        "# endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
        "\n",
        "# print(\"To load this existing endpoint from a different session:\")\n",
        "# print(\n",
        "#     f'endpoint = aiplatform.Endpoint(\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint.name}\")'\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US1TPVITuQej"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "c5efd3e1b065"
      },
      "outputs": [],
      "source": [
        "# @title Predict (text-to-image)\n",
        "\n",
        "# @markdown This section is only for sending predictions to an endpoint with the task `text-to-image`.\n",
        "\n",
        "# @markdown Once deployment succeeds, you can generate images by sending text prompts to the endpoint.\n",
        "\n",
        "# @markdown You can also batch send prompts by separating them with a comma.\n",
        "# @markdown You may adjust the parameters below to achieve best image quality.\n",
        "\n",
        "if task == \"text-to-image\":\n",
        "    comma_separated_prompt_list = \"A photo of an astronaut riding a horse on mars, A stone castle in a forest by the river\"  # @param {type: \"string\"}\n",
        "    prompt_list = [x.strip() for x in comma_separated_prompt_list.split(\",\")]\n",
        "    negative_prompt = \"\"  # @param {type: \"string\"}\n",
        "    height = 768  # @param {type:\"number\"}\n",
        "    width = 768  # @param {type:\"number\"}\n",
        "    num_inference_steps = 25  # @param {type:\"number\"}\n",
        "    guidance_scale = 7.5  # @param {type:\"number\"}\n",
        "\n",
        "    instances = [{\"text\": prompt} for prompt in prompt_list]\n",
        "    parameters = {\n",
        "        \"negative_prompt\": negative_prompt,\n",
        "        \"height\": height,\n",
        "        \"width\": width,\n",
        "        \"num_inference_steps\": num_inference_steps,\n",
        "        \"guidance_scale\": 7.5,\n",
        "    }\n",
        "\n",
        "    response = endpoints[\"image_model\"].predict(\n",
        "        instances=instances, parameters=parameters\n",
        "    )\n",
        "    images = [\n",
        "        common_util.base64_to_image(prediction.get(\"output\"))\n",
        "        for prediction in response.predictions\n",
        "    ]\n",
        "    display(common_util.image_grid(images, rows=math.ceil(len(images) ** 0.5)))\n",
        "else:\n",
        "    print(\n",
        "        \"To run `text-to-image` prediction, deploy the model with `text-to-image` task.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9xsy9FSZBft9"
      },
      "outputs": [],
      "source": [
        "# @title Predict with Dynamic LoRA (text-to-image)\n",
        "# @markdown You may specify a LoRA along with the request by setting `lora_id`. The LoRA will be loaded dynamically into the base model for the current prediction request. Note that this LoRA will not affect any subsequent requests, unless the same LoRA is specified in the request.\n",
        "\n",
        "# @markdown `lora_id` should be a Hugging Face id, or a GCS uri (with \"gs://\" prefix) to the LoRA directory.\n",
        "\n",
        "# @markdown Example request:\n",
        "\n",
        "# @markdown ```\n",
        "# @markdown {\n",
        "# @markdown   \"instances\": [{\"text\": \"papercut a red fox\"}],\n",
        "# @markdown   \"parameters\": {\n",
        "# @markdown     \"lora_id\": \"TheLastBen/Papercut_SDXL\"\n",
        "# @markdown   }\n",
        "# @markdown }\n",
        "# @markdown ```\n",
        "\n",
        "# Loads an existing endpoint instance using the endpoint name:\n",
        "# - Using `endpoint_name = endpoint.name` allows us to get the endpoint name of\n",
        "#   the endpoint `endpoint` created in the cell above.\n",
        "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
        "#   an existing endpoint with the ID 1234567890123456789.\n",
        "\n",
        "# You may uncomment the code below to load an existing endpoint.\n",
        "# endpoint_name = \"\"  # @param {type:\"string\"}\n",
        "# aip_endpoint_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
        "# )\n",
        "# endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
        "# print(\"Using this existing endpoint from a different session: {aip_endpoint_name}\")\n",
        "\n",
        "prompt = \"A painting from Michel Tolmer\"  # @param {type: \"string\"}\n",
        "lora_id = \"mathieuripert/tolmer-model\"  # @param {type: \"string\"}\n",
        "\n",
        "if task == \"text-to-image\":\n",
        "    instances = [{\"text\": prompt}]\n",
        "    parameters = {\"lora_id\": lora_id}\n",
        "\n",
        "    response = endpoints[\"image_model\"].predict(\n",
        "        instances=instances, parameters=parameters\n",
        "    )\n",
        "    images = [\n",
        "        common_util.base64_to_image(prediction.get(\"output\"))\n",
        "        for prediction in response.predictions\n",
        "    ]\n",
        "    display(common_util.image_grid(images, rows=math.ceil(len(images) ** 0.5)))\n",
        "else:\n",
        "    print(\n",
        "        \"To run `text-to-image` prediction, deploy the model with `text-to-image` task.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "52d4b6422fb7"
      },
      "outputs": [],
      "source": [
        "# @title Predict (text-guided image-to-image)\n",
        "\n",
        "# @markdown This section is only for sending predictions to an endpoint with the task `image-to-image`.\n",
        "\n",
        "# @markdown Once deployment succeeds, you can generate images by sending links to images and text prompts to the endpoint.\n",
        "\n",
        "if task == \"image-to-image\":\n",
        "    init_image_url = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"  # @param {type: \"string\"}\n",
        "    prompt = \"A fantasy landscape trending on artstation\"  # @param {type: \"string\"}\n",
        "    negative_prompt = \"\"  # @param {type: \"string\"}\n",
        "    height = 768  # @param {type:\"number\"}\n",
        "    width = 768  # @param {type:\"number\"}\n",
        "    num_inference_steps = 25  # @param {type:\"number\"}\n",
        "    guidance_scale = 7.5  # @param {type:\"number\"}\n",
        "\n",
        "    parameters = {\n",
        "        \"negative_prompt\": negative_prompt,\n",
        "        \"height\": height,\n",
        "        \"width\": width,\n",
        "        \"num_inference_steps\": num_inference_steps,\n",
        "        \"guidance_scale\": 7.5,\n",
        "    }\n",
        "\n",
        "    init_image = common_util.download_image(init_image_url)\n",
        "    display(init_image)\n",
        "\n",
        "    instances = [\n",
        "        {\n",
        "            \"prompt\": prompt,\n",
        "            \"negative_prompt\": negative_prompt,\n",
        "            \"image\": common_util.image_to_base64(init_image),\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    response = endpoints[\"image_model\"].predict(\n",
        "        instances=instances, parameters=parameters\n",
        "    )\n",
        "    images = [common_util.base64_to_image(image) for image in response.predictions]\n",
        "    display(common_util.image_grid(images, rows=math.ceil(len(images) ** 0.5)))\n",
        "else:\n",
        "    print(\n",
        "        \"To run `image-to-image` prediction, deploy the model with `image-to-image` task.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_aV8utGuWTO"
      },
      "source": [
        "## Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "86fec0dc3772"
      },
      "outputs": [],
      "source": [
        "# @title Delete the models and endpoints\n",
        "# @markdown  Delete the experiment models and endpoints to recycle the resources\n",
        "# @markdown  and avoid unnecessary continuous charges that may incur.\n",
        "\n",
        "# Undeploy model and delete endpoint.\n",
        "for endpoint in endpoints.values():\n",
        "    endpoint.delete(force=True)\n",
        "\n",
        "# Delete models.\n",
        "for model in models.values():\n",
        "    model.delete()\n",
        "\n",
        "delete_bucket = False  # @param {type:\"boolean\"}\n",
        "if delete_bucket:\n",
        "    ! gsutil -m rm -r $BUCKET_NAME"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_pytorch_sd_2_1_deployment.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2026 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TirJ-SGQseby"
      },
      "source": [
        "# Vertex AI Model Garden TFVision With Image Segmentation\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_tfvision_image_segmentation.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_tfvision_image_segmentation.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates how to use [TFVision](https://github.com/tensorflow/models/blob/master/official/vision/MODEL_GARDEN.md) in Vertex AI Model Garden.\n",
        "\n",
        "### Objective\n",
        "\n",
        "* Train new models\n",
        "  * Convert input data to training formats\n",
        "  * Create [hyperparameter tuning jobs](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview) to train new models\n",
        "  * Find and export best models\n",
        "\n",
        "* Test trained models\n",
        "  * Upload models to model registry\n",
        "  * Deploy uploaded models\n",
        "  * Run predictions\n",
        "\n",
        "* Cleanup resources\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEukV6uRk_S3"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9wExiMUxFk91"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown 2. For finetuning, **[click here](https://console.cloud.google.com/iam-admin/quotas?location=us-central1&metric=aiplatform.googleapis.com%2Frestricted_image_training_nvidia_a100_80gb_gpus)** to check if your project already has the required 8 Nvidia A100 80 GB GPUs in the us-central1 region. If yes, then run this notebook in the us-central1 region. If you do not have 8 Nvidia A100 80 GPUs or have more GPU requirements than this, then schedule your job with Nvidia H100 GPUs via Dynamic Workload Scheduler using [these instructions](https://cloud.google.com/vertex-ai/docs/training/schedule-jobs-dws). For Dynamic Workload Scheduler, check the [us-central1](https://console.cloud.google.com/iam-admin/quotas?location=us-central1&metric=aiplatform.googleapis.com%2Fcustom_model_training_preemptible_nvidia_h100_gpus) or [europe-west4](https://console.cloud.google.com/iam-admin/quotas?location=europe-west4&metric=aiplatform.googleapis.com%2Fcustom_model_training_preemptible_nvidia_h100_gpus) quota for Nvidia H100 GPUs. If you do not have enough GPUs, then you can follow [these instructions](https://cloud.google.com/docs/quotas/view-manage#viewing_your_quota_console) to request quota.\n",
        "\n",
        "# @markdown 3. For serving, **[click here](https://console.cloud.google.com/iam-admin/quotas?location=us-central1&metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_l4_gpus)** to check if your project already has the required 1 L4 GPU in the us-central1 region.  If yes, then run this notebook in the us-central1 region. If you need more L4 GPUs for your project, then you can follow [these instructions](https://cloud.google.com/docs/quotas/view-manage#viewing_your_quota_console) to request more. Alternatively, if you want to run predictions with A100 80GB or H100 GPUs, we recommend using the regions listed below. **NOTE:** Make sure you have associated quota in selected regions. Click the links to see your current quota for each GPU type: [Nvidia A100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_a100_80gb_gpus), [Nvidia H100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_h100_gpus).\n",
        "\n",
        "# @markdown > | Machine Type | Accelerator Type | Recommended Regions |\n",
        "# @markdown | ----------- | ----------- | ----------- |\n",
        "# @markdown | a2-ultragpu-1g | 1 NVIDIA_A100_80GB | us-central1, us-east4, europe-west4, asia-southeast1, us-east4 |\n",
        "# @markdown | a3-highgpu-2g | 2 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "# @markdown | a3-highgpu-4g | 4 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "# @markdown | a3-highgpu-8g | 8 NVIDIA_H100_80GB | us-central1, europe-west4, us-west1, asia-southeast1 |\n",
        "\n",
        "# @markdown 4. **[Optional]** [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs. Set the BUCKET_URI for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`) should be located in the same region as where the notebook was launched. Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\"). If not set, a unique GCS bucket will be created instead.\n",
        "\n",
        "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown 5. **[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment.\n",
        "\n",
        "REGION = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "import base64\n",
        "import datetime\n",
        "import importlib\n",
        "import json\n",
        "import os\n",
        "import subprocess\n",
        "import uuid\n",
        "from io import BytesIO\n",
        "from typing import Dict, List, Union\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import yaml\n",
        "from google.cloud import aiplatform\n",
        "from google.protobuf import json_format\n",
        "from google.protobuf.struct_pb2 import Value\n",
        "from PIL import Image\n",
        "\n",
        "common_util = importlib.import_module(\n",
        "    \"vertex-ai-samples.notebooks.community.model_garden.docker_source_codes.notebook_util.common_util\"\n",
        ")\n",
        "\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "if not REGION:\n",
        "    if not os.environ.get(\"GOOGLE_CLOUD_REGION\"):\n",
        "        raise ValueError(\n",
        "            \"REGION must be set. See\"\n",
        "            \" https://cloud.google.com/vertex-ai/docs/general/locations for\"\n",
        "            \" available cloud locations.\"\n",
        "        )\n",
        "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
        "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
        "\n",
        "# Cloud Storage bucket for storing the experiment artifacts.\n",
        "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
        "# prefer using your own GCS bucket, change the value yourself below.\n",
        "now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "\n",
        "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
        "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}-{str(uuid.uuid4())[:4]}\"\n",
        "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
        "else:\n",
        "    assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
        "    shell_output = ! gsutil ls -Lb {BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
        "    bucket_region = shell_output[0].strip().lower()\n",
        "    if bucket_region != REGION:\n",
        "        raise ValueError(\n",
        "            \"Bucket region %s is different from notebook region %s\"\n",
        "            % (bucket_region, REGION)\n",
        "        )\n",
        "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
        "\n",
        "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
        "MODEL_BUCKET = os.path.join(BUCKET_URI, \"image_segmentation\")\n",
        "\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "# Gets the default SERVICE_ACCOUNT.\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
        "\n",
        "\n",
        "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
        "\n",
        "! gcloud config set project $PROJECT_ID\n",
        "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/storage.admin\"\n",
        "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/aiplatform.user\"\n",
        "\n",
        "# Only regions prefixed by \"us\", \"asia\", or \"europe\" are supported.\n",
        "REGION_PREFIX = REGION.split(\"-\")[0]\n",
        "assert REGION_PREFIX in (\n",
        "    \"us\",\n",
        "    \"europe\",\n",
        "    \"asia\",\n",
        "), f'{REGION} is not supported. It must be prefixed by \"us\", \"asia\", or \"europe\".'\n",
        "\n",
        "# Download config files.\n",
        "CONFIG_DIR = os.path.join(BUCKET_URI, \"config\")\n",
        "! wget https://raw.githubusercontent.com/tensorflow/models/master/official/vision/configs/experiments/semantic_segmentation/deeplabv3plus_resnet101_cityscapes_gpu_multiworker_mirrored.yaml\n",
        "! gsutil cp deeplabv3plus_resnet101_cityscapes_gpu_multiworker_mirrored.yaml $CONFIG_DIR/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB_xY9ipr7ZU"
      },
      "source": [
        "## Train new models\n",
        "This section shows how to train new models.\n",
        "1. Convert input data to training formats.\n",
        "2. Create hyperparameter tuning jobs to train new models.\n",
        "3. Find and export best models.\n",
        "\n",
        "If you already trained models, kindly go to the section `Test trained models`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IndQ_m6ddUEM"
      },
      "outputs": [],
      "source": [
        "# @title Prepare input data for training\n",
        "\n",
        "# @markdown Prepare data in the format as described [here](https://cloud.google.com/vertex-ai/docs/image-data/classification/prepare-data), and then convert them to the training formats as below:\n",
        "# @markdown * `input_file_path`: The input file path in coco json formats.\n",
        "# @markdown * `split_ratio`: The proportion of data to split into train/validation/test.\n",
        "# @markdown * `num_shard`: The number of shards for train/validation/test.\n",
        "# @markdown * `output_dir`: The output directory, which will container prepared train/test/\n",
        "\n",
        "OBJECTIVE = \"isg\"\n",
        "\n",
        "# Data converter constants.\n",
        "DATA_CONVERTER_JOB_PREFIX = \"data_converter\"\n",
        "DATA_CONVERTER_CONTAINER = f\"{REGION_PREFIX}-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/data-converter\"\n",
        "DATA_CONVERTER_MACHINE_TYPE = \"n1-highmem-8\"\n",
        "\n",
        "# This job will convert input data as training format, with given split ratios\n",
        "# and number of shards on train/test/validation.\n",
        "data_converter_job_name = common_util.get_job_name_with_datetime(\n",
        "    DATA_CONVERTER_JOB_PREFIX + \"_\" + OBJECTIVE\n",
        ")\n",
        "\n",
        "input_file_path = \"\"  # @param {type:\"string\"}\n",
        "split_ratio = \"0.8,0.1,0.1\"  # @param\n",
        "num_shard = \"10,10,10\"  # @param\n",
        "data_converter_output_dir = os.path.join(BUCKET_URI, data_converter_job_name)\n",
        "\n",
        "worker_pool_specs = [\n",
        "    {\n",
        "        \"machine_spec\": {\n",
        "            \"machine_type\": DATA_CONVERTER_MACHINE_TYPE,\n",
        "        },\n",
        "        \"replica_count\": 1,\n",
        "        \"container_spec\": {\n",
        "            \"image_uri\": DATA_CONVERTER_CONTAINER,\n",
        "            \"command\": [],\n",
        "            \"args\": [\n",
        "                \"--input_file_path=%s\" % input_file_path,\n",
        "                \"--input_file_type=coco_json\",\n",
        "                \"--objective=%s\" % OBJECTIVE,\n",
        "                \"--num_shard=%s\" % num_shard,\n",
        "                \"--split_ratio=%s\" % split_ratio,\n",
        "                \"--output_dir=%s\" % data_converter_output_dir,\n",
        "            ],\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "data_converter_custom_job = aiplatform.CustomJob(\n",
        "    display_name=data_converter_job_name,\n",
        "    project=PROJECT_ID,\n",
        "    worker_pool_specs=worker_pool_specs,\n",
        "    staging_bucket=STAGING_BUCKET,\n",
        ")\n",
        "\n",
        "data_converter_custom_job.run()\n",
        "\n",
        "input_train_data_path = os.path.join(data_converter_output_dir, \"train.tfrecord*\")\n",
        "input_validation_data_path = os.path.join(data_converter_output_dir, \"val.tfrecord*\")\n",
        "label_map_path = os.path.join(data_converter_output_dir, \"label_map.yaml\")\n",
        "print(\"input_train_data_path for training: \", input_train_data_path)\n",
        "print(\"input_validation_data_path for training: \", input_validation_data_path)\n",
        "print(\"label_map_path for prediction: \", label_map_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "um_XKbmpTaHx"
      },
      "outputs": [],
      "source": [
        "# @title Define Hyperparameters tuning parameters\n",
        "\n",
        "# @markdown Create a Vertex AI custom job with hyperparameter tuning\n",
        "\n",
        "# @markdown You use the Vertex AI SDK to create and run the hyperparameter tuning job with Vertex AI Model Garden Training Dockers.\n",
        "\n",
        "# @markdown * `worker_pool_specs`: Dictionary specifying the machine type and Docker image. This example defines a single node cluster with one `n1-standard-4` machine with two `NVIDIA_TESLA_T4` GPUs.\n",
        "\n",
        "# @markdown * `parameter_spec`: Dictionary specifying the parameters to optimize. The dictionary key is the string assigned to the command line argument for each hyperparameter in your training application code, and the dictionary value is the parameter specification. The parameter specification includes the type, min/max values, and scale for the hyperparameter.\n",
        "\n",
        "# @markdown * `metric_spec`: Dictionary specifying the metric to optimize. The dictionary key is the `hyperparameter_metric_tag` that you set in your training application code, and the value is the optimization goal.\n",
        "\n",
        "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
        "\n",
        "# Training constants.\n",
        "TRAINING_JOB_PREFIX = \"train\"\n",
        "TRAIN_CONTAINER_URI = f\"{REGION_PREFIX}-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/tfvision-oss\"\n",
        "TRAIN_MACHINE_TYPE = \"n1-highmem-16\"\n",
        "TRAIN_ACCELERATOR_TYPE = \"NVIDIA_TESLA_V100\"\n",
        "TRAIN_NUM_GPU = 2\n",
        "TRAIN_DEEPLABV3PLUS_CONFIG = os.path.join(\n",
        "    CONFIG_DIR, \"deeplabv3plus_resnet101_cityscapes_gpu_multiworker_mirrored.yaml\"\n",
        ")\n",
        "\n",
        "\n",
        "def upload_checkpoint_to_gcs(checkpoint_url):\n",
        "    filename = os.path.basename(checkpoint_url)\n",
        "    checkpoint_name = filename.replace(\".tar.gz\", \"\")\n",
        "    print(\"Download checkpoint from\", checkpoint_url, \"and store to\", CHECKPOINT_BUCKET)\n",
        "    ! wget $checkpoint_url -O $filename\n",
        "    ! mkdir -p $checkpoint_name\n",
        "    ! tar -xvzf $filename -C $checkpoint_name\n",
        "\n",
        "    # Search for relative path to the checkpoint.\n",
        "    checkpoint_path = None\n",
        "    for root, dirs, files in os.walk(checkpoint_name):\n",
        "        for file in files:\n",
        "            if file.endswith(\".index\"):\n",
        "                checkpoint_path = os.path.join(root, os.path.splitext(file)[0])\n",
        "                checkpoint_path = os.path.relpath(checkpoint_path, checkpoint_name)\n",
        "                break\n",
        "\n",
        "    ! gsutil cp -r $checkpoint_name $CHECKPOINT_BUCKET/\n",
        "    checkpoint_uri = os.path.join(CHECKPOINT_BUCKET, checkpoint_name, checkpoint_path)\n",
        "    print(\"Checkpoint uploaded to\", checkpoint_uri)\n",
        "    return checkpoint_uri\n",
        "\n",
        "\n",
        "def get_label_map(label_map_yaml_filepath: str) -> Dict[int, str]:\n",
        "    \"\"\"Returns class id to label mapping given a filepath to the label map.\n",
        "\n",
        "    Args:\n",
        "      label_map_yaml_filepath: A string of label map yaml file path.\n",
        "\n",
        "    Returns:\n",
        "      A dictionary of class id to label mapping.\n",
        "    \"\"\"\n",
        "    label_map_filename = os.path.basename(label_map_yaml_filepath)\n",
        "    subprocess.check_output(\n",
        "        [\"gsutil\", \"cp\", label_map_yaml_filepath, label_map_filename],\n",
        "        stderr=subprocess.STDOUT,\n",
        "    )\n",
        "    with open(label_map_filename, \"rb\") as input_file:\n",
        "        label_map = yaml.safe_load(input_file.read())[\"label_map\"]\n",
        "        return label_map\n",
        "\n",
        "\n",
        "label_map = get_label_map(label_map_path)\n",
        "num_classes = len(label_map[\"label_map\"]) + 1\n",
        "\n",
        "# Input train and validation datasets can be found from the section above\n",
        "# `Convert input data for training`.\n",
        "# Set prepared datasets if exists.\n",
        "# input_train_data_path = ''\n",
        "# input_validation_data_path = ''\n",
        "\n",
        "# Refer to https://github.com/tensorflow/models/blob/master/official/vision/MODEL_GARDEN.md\n",
        "# for more model details.\n",
        "experiment = \"deeplabv3plus\"  # @param [\"deeplabv3plus\"]\n",
        "\n",
        "train_job_name = common_util.get_job_name_with_datetime(\n",
        "    TRAINING_JOB_PREFIX + \"_\" + OBJECTIVE\n",
        ")\n",
        "model_dir = os.path.join(BUCKET_URI, train_job_name)\n",
        "\n",
        "# The arguments here are mainly for test purposes. Kindly update them\n",
        "# to get better performances.\n",
        "experiment_container_args_dict = {\n",
        "    # deeplabv3plus experiment args.\n",
        "    \"deeplabv3plus\": {\n",
        "        \"experiment\": \"seg_deeplabv3plus_pascal\",\n",
        "        \"config_file\": TRAIN_DEEPLABV3PLUS_CONFIG,\n",
        "        \"input_train_data_path\": input_train_data_path,\n",
        "        \"input_validation_data_path\": input_validation_data_path,\n",
        "        \"objective\": OBJECTIVE,\n",
        "        \"model_dir\": model_dir,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"global_batch_size\": 2,\n",
        "        \"prefetch_buffer_size\": 12,\n",
        "        \"train_steps\": 500,\n",
        "        \"output_size\": \"1024,2048\",\n",
        "        \"init_checkpoint\": \"https://storage.googleapis.com/tf_model_garden/vision/deeplabv3plus/dilated-resnet-101-deeplabv3plus.tar.gz\",\n",
        "    }\n",
        "}\n",
        "experiment_container_args = experiment_container_args_dict[experiment]\n",
        "\n",
        "# Copy checkpoint to GCS bucket if specified.\n",
        "init_checkpoint = experiment_container_args.get(\"init_checkpoint\")\n",
        "if init_checkpoint:\n",
        "    experiment_container_args[\"init_checkpoint\"] = upload_checkpoint_to_gcs(\n",
        "        init_checkpoint\n",
        "    )\n",
        "\n",
        "worker_pool_specs = [\n",
        "    {\n",
        "        \"machine_spec\": {\n",
        "            \"machine_type\": TRAIN_MACHINE_TYPE,\n",
        "            \"accelerator_type\": TRAIN_ACCELERATOR_TYPE,\n",
        "            \"accelerator_count\": TRAIN_NUM_GPU,\n",
        "        },\n",
        "        \"replica_count\": 1,\n",
        "        \"container_spec\": {\n",
        "            \"image_uri\": TRAIN_CONTAINER_URI,\n",
        "            \"args\": [\n",
        "                \"--mode=train_and_eval\",\n",
        "            ]\n",
        "            + [\"--{}={}\".format(k, v) for k, v in experiment_container_args.items()],\n",
        "        },\n",
        "    },\n",
        "]\n",
        "\n",
        "metric_spec = {\"model_performance\": \"maximize\"}\n",
        "\n",
        "LEARNING_RATES = [0.001]\n",
        "# Models will be trained with each learning rate separately and max trial count is the number of learning rates.\n",
        "MAX_TRIAL_COUNT = len(LEARNING_RATES)\n",
        "parameter_spec = {\n",
        "    \"learning_rate\": hpt.DiscreteParameterSpec(values=LEARNING_RATES, scale=\"linear\"),\n",
        "}\n",
        "\n",
        "print(worker_pool_specs, metric_spec, parameter_spec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aec22792ee84"
      },
      "outputs": [],
      "source": [
        "# @title Run hyperparameter tuning jobs\n",
        "\n",
        "# @markdown * `max_trial_count`: Sets an upper bound on the number of trials the service will run. The recommended practice is to start with a smaller number of trials and get a sense of how impactful your chosen hyperparameters are before scaling up.\n",
        "\n",
        "# @markdown * `parallel_trial_count`:  If you use parallel trials, the service provisions multiple training processing clusters. The worker pool spec that you specify when creating the job is used for each individual training cluster.  Increasing the number of parallel trials reduces the amount of time the hyperparameter tuning job takes to run; however, it can reduce the effectiveness of the job overall. This is because the default tuning strategy uses results of previous trials to inform the assignment of values in subsequent trials.\n",
        "\n",
        "# @markdown * `search_algorithm`: The available search algorithms are grid, random, or default (None). The default option applies Bayesian optimization to search the space of possible hyperparameter values and is the recommended algorithm.\n",
        "\n",
        "# @markdown Click on the generated link in the output to see your run in the Cloud Console.\n",
        "\n",
        "\n",
        "common_util.check_quota(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    accelerator_type=TRAIN_ACCELERATOR_TYPE,\n",
        "    accelerator_count=TRAIN_NUM_GPU,\n",
        "    is_for_training=True,\n",
        ")\n",
        "\n",
        "# Add labels for the finetuning job.\n",
        "labels = {\n",
        "    \"mg-source\": \"notebook\",\n",
        "    \"mg-notebook-name\": \"model_garden_tfvision_image_segmentation.ipynb\".split(\".\")[0],\n",
        "}\n",
        "\n",
        "labels[\"mg-tune\"] = \"publishers-google-models-tfvision\"\n",
        "versioned_model_id = experiment.lower().replace(\"_\", \"-\")\n",
        "labels[\"versioned-mg-tune\"] = f\"{labels['mg-tune']}-{versioned_model_id}\"\n",
        "\n",
        "train_custom_job = aiplatform.CustomJob(\n",
        "    display_name=train_job_name,\n",
        "    project=PROJECT_ID,\n",
        "    worker_pool_specs=worker_pool_specs,\n",
        "    staging_bucket=STAGING_BUCKET,\n",
        "    labels=labels,\n",
        ")\n",
        "\n",
        "train_hpt_job = aiplatform.HyperparameterTuningJob(\n",
        "    display_name=train_job_name,\n",
        "    custom_job=train_custom_job,\n",
        "    metric_spec=metric_spec,\n",
        "    parameter_spec=parameter_spec,\n",
        "    max_trial_count=MAX_TRIAL_COUNT,\n",
        "    parallel_trial_count=1,\n",
        "    project=PROJECT_ID,\n",
        "    search_algorithm=None,\n",
        ")\n",
        "\n",
        "train_hpt_job.run()\n",
        "\n",
        "print(\"experiment is: \", experiment)\n",
        "print(\"model_dir is: \", model_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "09Rz1AYspK19"
      },
      "outputs": [],
      "source": [
        "# @title Export best models as TF Saved Model format\n",
        "\n",
        "# @markdown This section will export models from TF checkpoints to TF saved model format.\n",
        "\n",
        "# Evaluation constants.\n",
        "EVALUATION_METRIC = \"mean_iou\"\n",
        "\n",
        "# Export constants.\n",
        "EXPORT_JOB_PREFIX = \"export\"\n",
        "EXPORT_CONTAINER_URI = f\"{REGION_PREFIX}-docker.pkg.dev/vertex-ai-restricted/vertex-vision-model-garden-dockers/tfvision-model-export\"\n",
        "EXPORT_MACHINE_TYPE = \"n1-highmem-8\"\n",
        "\n",
        "\n",
        "def get_best_trial(model_dir, max_trial_count, evaluation_metric):\n",
        "    best_trial_dir = \"\"\n",
        "    best_trial_evaluation_results = {}\n",
        "    best_performance = -1\n",
        "\n",
        "    for i in range(max_trial_count):\n",
        "        current_trial = i + 1\n",
        "        current_trial_dir = os.path.join(model_dir, \"trial_\" + str(current_trial))\n",
        "        current_trial_best_ckpt_dir = os.path.join(current_trial_dir, \"best_ckpt\")\n",
        "        current_trial_best_ckpt_evaluation_filepath = os.path.join(\n",
        "            current_trial_best_ckpt_dir, \"info.json\"\n",
        "        )\n",
        "        ! gsutil cp $current_trial_best_ckpt_evaluation_filepath .\n",
        "        with open(\"info.json\", \"r\") as f:\n",
        "            eval_metric_results = json.load(f)\n",
        "            current_performance = eval_metric_results[evaluation_metric]\n",
        "            if current_performance > best_performance:\n",
        "                best_performance = current_performance\n",
        "                best_trial_dir = current_trial_dir\n",
        "                best_trial_evaluation_results = eval_metric_results\n",
        "    print(\"best_trial_dir: \", current_trial_best_ckpt_evaluation_filepath)\n",
        "    return best_trial_dir, best_trial_evaluation_results\n",
        "\n",
        "\n",
        "# model_dir is from the section above.\n",
        "best_trial_dir, best_trial_evaluation_results = get_best_trial(\n",
        "    model_dir, MAX_TRIAL_COUNT, EVALUATION_METRIC\n",
        ")\n",
        "print(\"best_trial_dir: \", best_trial_dir)\n",
        "print(\"best_trial_evaluation_results: \", best_trial_evaluation_results)\n",
        "\n",
        "worker_pool_specs = [\n",
        "    {\n",
        "        \"machine_spec\": {\n",
        "            \"machine_type\": EXPORT_MACHINE_TYPE,\n",
        "        },\n",
        "        \"replica_count\": 1,\n",
        "        \"container_spec\": {\n",
        "            \"image_uri\": EXPORT_CONTAINER_URI,\n",
        "            \"command\": [],\n",
        "            \"args\": [\n",
        "                \"--objective=%s\" % OBJECTIVE,\n",
        "                \"--experiment=%s\" % experiment_container_args[\"experiment\"],\n",
        "                \"--config_file=%s/params.yaml\" % best_trial_dir,\n",
        "                \"--checkpoint_path=%s/best_ckpt\" % best_trial_dir,\n",
        "                \"--export_dir=%s/best_model\" % model_dir,\n",
        "                \"--input_image_size=%s\" % experiment_container_args[\"output_size\"],\n",
        "            ],\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "model_export_name = common_util.get_job_name_with_datetime(\n",
        "    EXPORT_JOB_PREFIX + \"_\" + OBJECTIVE\n",
        ")\n",
        "model_export_custom_job = aiplatform.CustomJob(\n",
        "    display_name=model_export_name,\n",
        "    project=PROJECT_ID,\n",
        "    worker_pool_specs=worker_pool_specs,\n",
        "    staging_bucket=STAGING_BUCKET,\n",
        "    labels=labels,\n",
        ")\n",
        "\n",
        "model_export_custom_job.run()\n",
        "\n",
        "print(\"best model is saved to: \", os.path.join(model_dir, \"best_model\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0BGaofgsMsy"
      },
      "source": [
        "## Test trained models\n",
        "This section shows how to test with trained models.\n",
        "1. Upload and deploy models.\n",
        "2. Run predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NYuQowyZEtxK"
      },
      "outputs": [],
      "source": [
        "# @title Upload and deploy models\n",
        "\n",
        "# @markdown This section uploads and deploy models to model registry for online prediction. This example uses the exported best model from \"Train new models\" section.\n",
        "\n",
        "# Prediction constants.\n",
        "# You can deploy models with\n",
        "#   pre-build-dockers: https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers.\n",
        "#   and optimized tensorflow runtime dockers: https://cloud.google.com/vertex-ai/docs/predictions/optimized-tensorflow-runtime.\n",
        "# The example in this notebook uses optimized tensorflow runtime dockers.\n",
        "# You can adjust accelerator types and machine types to get faster predictions.\n",
        "\n",
        "PREDICTION_CONTAINER_URI = f\"{REGION_PREFIX}-docker.pkg.dev/vertex-ai-restricted/prediction/tf_opt-gpu.2-11:latest\"\n",
        "SERVING_CONTAINER_ARGS = [\"--allow_precompilation\", \"--allow_compression\"]\n",
        "PREDICTION_ACCELERATOR_TYPE = \"NVIDIA_TESLA_T4\"\n",
        "PREDICTION_MACHINE_TYPE = \"n1-standard-4\"\n",
        "UPLOAD_JOB_PREFIX = \"upload\"\n",
        "DEPLOY_JOB_PREFIX = \"deploy\"\n",
        "\n",
        "# model_dir is from the section above.\n",
        "trained_model_dir = os.path.join(model_dir, \"best_model/saved_model\")\n",
        "\n",
        "upload_job_name = common_util.get_job_name_with_datetime(\n",
        "    UPLOAD_JOB_PREFIX + \"_\" + OBJECTIVE\n",
        ")\n",
        "\n",
        "serving_env = {\n",
        "    \"MODEL_ID\": \"deeplabv3plus-cityscapes-20230315\",\n",
        "    \"DEPLOY_SOURCE\": \"notebook\",\n",
        "}\n",
        "\n",
        "models[\"model_isg\"] = aiplatform.Model.upload(\n",
        "    display_name=upload_job_name,\n",
        "    artifact_uri=trained_model_dir,\n",
        "    serving_container_image_uri=PREDICTION_CONTAINER_URI,\n",
        "    serving_container_args=SERVING_CONTAINER_ARGS,\n",
        "    serving_container_environment_variables=serving_env,\n",
        "    model_garden_source_model_name=\"publishers/google/models/imagesegmentation-deeplabv3\",\n",
        ")\n",
        "\n",
        "models[\"model_isg\"].wait()\n",
        "\n",
        "print(\"The uploaded model name is: \", upload_job_name)\n",
        "\n",
        "deploy_model_name = common_util.get_job_name_with_datetime(\n",
        "    DEPLOY_JOB_PREFIX + \"_\" + OBJECTIVE\n",
        ")\n",
        "print(\"The deployed job name is: \", deploy_model_name)\n",
        "\n",
        "common_util.check_quota(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    accelerator_type=PREDICTION_ACCELERATOR_TYPE,\n",
        "    accelerator_count=1,\n",
        "    is_for_training=False,\n",
        ")\n",
        "\n",
        "endpoints[\"endpoint_isg\"] = models[\"model_isg\"].deploy(\n",
        "    deployed_model_display_name=deploy_model_name,\n",
        "    machine_type=PREDICTION_MACHINE_TYPE,\n",
        "    traffic_split={\"0\": 100},\n",
        "    accelerator_type=PREDICTION_ACCELERATOR_TYPE,\n",
        "    accelerator_count=1,\n",
        "    min_replica_count=1,\n",
        "    max_replica_count=1,\n",
        "    system_labels={\"NOTEBOOK_NAME\": \"model_garden_tfvision_image_segmentation.ipynb\"},\n",
        ")\n",
        "\n",
        "endpoint_id = endpoints[\"endpoint_isg\"].name\n",
        "print(\"endpoint id is: \", endpoint_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vbIW9me1F2RY"
      },
      "outputs": [],
      "source": [
        "# @title Run predictions\n",
        "\n",
        "# @markdown Once deployment succeeds, you can send image to the endpoint for online prediction.\n",
        "\n",
        "# @markdown `test_filepath`: gcs uri to the test image file. The uri should start with \"gs://\".\n",
        "\n",
        "\n",
        "def load_img(path):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    return Image.fromarray(np.uint8(img)).convert(\"RGB\")\n",
        "\n",
        "\n",
        "def get_prediction_instances(test_filepath: str, new_width: int = -1) -> Any:\n",
        "    \"\"\"Generate instance from image path to pass to Vertex AI Endpoint for prediction.\n",
        "\n",
        "    Args:\n",
        "      test_filepath: A string of test image path.\n",
        "      new_width: An integer of new image width.\n",
        "\n",
        "    Returns:\n",
        "      A list of instances.\n",
        "    \"\"\"\n",
        "    if new_width <= 0:\n",
        "        test_file = os.path.basename(test_filepath)\n",
        "        subprocess.check_output(\n",
        "            [\"gsutil\", \"cp\", test_filepath, test_file], stderr=subprocess.STDOUT\n",
        "        )\n",
        "        with open(test_file, \"rb\") as input_file:\n",
        "            encoded_string = base64.b64encode(input_file.read()).decode(\"utf-8\")\n",
        "    else:\n",
        "        img = common_util.load_img(test_filepath)\n",
        "        width, height = img.size\n",
        "        print(\"original input image size: \", width, \" , \", height)\n",
        "        new_height = int(height * new_width / width)\n",
        "        new_img = img.resize((new_width, new_height))\n",
        "        print(\"resized input image size: \", new_width, \" , \", new_height)\n",
        "        buffered = io.BytesIO()\n",
        "        new_img.save(buffered, format=\"JPEG\")\n",
        "        encoded_string = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "\n",
        "        instances = [\n",
        "            {\n",
        "                \"encoded_image\": {\"b64\": encoded_string},\n",
        "            }\n",
        "        ]\n",
        "        return instances\n",
        "\n",
        "\n",
        "def predict_custom_trained_model(\n",
        "    project: str,\n",
        "    endpoint_id: str,\n",
        "    instances: Union[Dict, List[Dict]],\n",
        "    location: str = \"us-central1\",\n",
        "):\n",
        "    # The AI Platform services require regional API endpoints.\n",
        "    client_options = {\"api_endpoint\": f\"{location}-aiplatform.googleapis.com\"}\n",
        "    # Initialize client that will be used to create and send requests.\n",
        "    # This client only needs to be created once, and can be reused for multiple requests.\n",
        "    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
        "    parameters_dict = {}\n",
        "    parameters = json_format.ParseDict(parameters_dict, Value())\n",
        "    endpoint = client.endpoint_path(\n",
        "        project=project, location=location, endpoint=endpoint_id\n",
        "    )\n",
        "    response = client.predict(\n",
        "        endpoint=endpoint, instances=instances, parameters=parameters\n",
        "    )\n",
        "    return response.predictions, response.deployed_model_id\n",
        "\n",
        "\n",
        "def create_coco_stuff_label_colormap():\n",
        "    \"\"\"Creates a label colormap used in COCO-Stuff segmentation benchmark.\n",
        "\n",
        "    Returns:\n",
        "      A colormap for visualizing segmentation results.\n",
        "    \"\"\"\n",
        "    return np.asarray(\n",
        "        [\n",
        "            [54, 178, 118],\n",
        "            [0, 85, 178],\n",
        "            [150, 178, 22],\n",
        "            [107, 0, 0],\n",
        "            [0, 0, 89],\n",
        "            [0, 117, 178],\n",
        "            [47, 178, 124],\n",
        "            [178, 116, 0],\n",
        "            [0, 0, 178],\n",
        "            [79, 178, 92],\n",
        "            [134, 0, 0],\n",
        "            [22, 178, 150],\n",
        "            [178, 87, 0],\n",
        "            [178, 146, 0],\n",
        "            [0, 5, 178],\n",
        "            [0, 0, 125],\n",
        "            [0, 53, 178],\n",
        "            [0, 132, 178],\n",
        "            [111, 178, 60],\n",
        "            [178, 131, 0],\n",
        "            [0, 29, 178],\n",
        "            [178, 109, 0],\n",
        "            [178, 35, 0],\n",
        "            [0, 148, 178],\n",
        "            [9, 172, 163],\n",
        "            [0, 0, 178],\n",
        "            [178, 124, 0],\n",
        "            [178, 102, 0],\n",
        "            [0, 156, 175],\n",
        "            [178, 43, 0],\n",
        "            [0, 0, 170],\n",
        "            [178, 94, 0],\n",
        "            [0, 0, 134],\n",
        "            [67, 178, 105],\n",
        "            [99, 178, 73],\n",
        "            [0, 37, 178],\n",
        "            [86, 178, 86],\n",
        "            [15, 178, 156],\n",
        "            [0, 0, 152],\n",
        "            [178, 21, 0],\n",
        "            [0, 124, 178],\n",
        "            [0, 61, 178],\n",
        "            [178, 50, 0],\n",
        "            [0, 109, 178],\n",
        "            [137, 178, 35],\n",
        "            [0, 13, 178],\n",
        "            [0, 101, 178],\n",
        "            [0, 0, 116],\n",
        "            [0, 45, 178],\n",
        "            [41, 178, 131],\n",
        "            [0, 0, 161],\n",
        "            [178, 72, 0],\n",
        "            [0, 0, 143],\n",
        "            [116, 0, 0],\n",
        "            [28, 178, 143],\n",
        "            [170, 6, 0],\n",
        "            [156, 178, 15],\n",
        "            [89, 0, 0],\n",
        "            [143, 178, 28],\n",
        "            [73, 178, 99],\n",
        "            [118, 178, 54],\n",
        "            [92, 178, 79],\n",
        "            [152, 0, 0],\n",
        "            [178, 153, 0],\n",
        "            [98, 0, 0],\n",
        "            [178, 65, 0],\n",
        "            [60, 178, 111],\n",
        "            [169, 175, 3],\n",
        "            [105, 178, 67],\n",
        "            [178, 13, 0],\n",
        "            [163, 178, 9],\n",
        "            [3, 164, 169],\n",
        "            [125, 0, 0],\n",
        "            [175, 168, 0],\n",
        "            [178, 138, 0],\n",
        "            [178, 28, 0],\n",
        "            [35, 178, 137],\n",
        "            [0, 140, 178],\n",
        "            [0, 0, 98],\n",
        "            [131, 178, 41],\n",
        "            [0, 77, 178],\n",
        "            [0, 0, 107],\n",
        "            [0, 93, 178],\n",
        "            [143, 0, 0],\n",
        "            [178, 58, 0],\n",
        "            [161, 0, 0],\n",
        "            [0, 69, 178],\n",
        "            [178, 160, 0],\n",
        "            [178, 80, 0],\n",
        "            [0, 21, 178],\n",
        "            [124, 178, 47],\n",
        "            [255, 214, 0],\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "def parse_segmentation_prediction(prediction):\n",
        "    score_bytes = prediction[\"score_bytes\"]\n",
        "    score_image_grayscale = Image.open(\n",
        "        BytesIO(base64.b64decode(dict(score_bytes)[\"b64\"]))\n",
        "    )\n",
        "    category_bytes = prediction[\"category_bytes\"]\n",
        "    category_image_grayscale = Image.open(\n",
        "        BytesIO(base64.b64decode(dict(category_bytes)[\"b64\"]))\n",
        "    )\n",
        "\n",
        "    # Visualize category images.\n",
        "    color_map = create_coco_stuff_label_colormap()\n",
        "    category_image_grayscale_np = np.array(category_image_grayscale)\n",
        "    rendered_image_shape = category_image_grayscale_np.shape + (3,)\n",
        "    category_image_color_np = np.zeros(rendered_image_shape, dtype=np.uint8)\n",
        "    unique_labels = np.unique(category_image_grayscale_np)\n",
        "    for label in unique_labels:\n",
        "        if label == 0:\n",
        "            continue\n",
        "        category_image_color_np[category_image_grayscale_np == label] = color_map[\n",
        "            label % len(color_map)\n",
        "        ]\n",
        "    category_image_color = Image.fromarray(category_image_color_np)\n",
        "\n",
        "    return score_image_grayscale, category_image_color\n",
        "\n",
        "\n",
        "def display_image(original_image, category_image_color, score_image_grayscale):\n",
        "    _, axarr = plt.subplots(1, 3, figsize=(20, 15))\n",
        "    axarr[0].imshow(original_image)\n",
        "    axarr[1].imshow(category_image_color)\n",
        "    axarr[2].imshow(score_image_grayscale.convert(\"RGB\"))\n",
        "\n",
        "\n",
        "# The test image file path.\n",
        "test_filepath = \"\"  # @param {type:\"string\"}\n",
        "score_threshold = 0.5  # @param {type:\"number\"}\n",
        "# If the input image is too large, we will resize it for prediction.\n",
        "instances = get_prediction_instances(test_filepath, new_width=1000)\n",
        "\n",
        "# The label map file was generated from the section above (`Convert input data for training`).\n",
        "label_map = get_label_map(label_map_path)[\"label_map\"]\n",
        "\n",
        "predictions, _ = predict_custom_trained_model(\n",
        "    project=PROJECT_ID, location=REGION, endpoint_id=endpoint_id, instances=instances\n",
        ")\n",
        "\n",
        "score_image_grayscale, category_image_color = parse_segmentation_prediction(\n",
        "    dict(predictions[0])\n",
        ")\n",
        "display_image(\n",
        "    load_img(test_filepath), category_image_color, score_image_grayscale.convert(\"RGB\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkH2nrpdp4sp"
      },
      "source": [
        "## Clean up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Ax6vQVZhp9pR"
      },
      "outputs": [],
      "source": [
        "# @title Clean up training jobs and buckets\n",
        "# @markdown  Delete the experiment models and endpoints to recycle the resources\n",
        "# @markdown  and avoid unnecessary continuous charges that may incur.\n",
        "\n",
        "# Undeploy model and delete endpoint.\n",
        "for endpoint in endpoints.values():\n",
        "    endpoint.delete(force=True)\n",
        "\n",
        "# Delete models.\n",
        "for model in models.values():\n",
        "    model.delete()\n",
        "\n",
        "delete_bucket = False  # @param {type:\"boolean\"}\n",
        "if delete_bucket:\n",
        "    ! gsutil -m rm -r $BUCKET_NAME\n",
        "\n",
        "# Delete custom and hpt jobs.\n",
        "if data_converter_custom_job.list(filter=f'display_name=\"{data_converter_job_name}\"'):\n",
        "    data_converter_custom_job.delete()\n",
        "if train_hpt_job.list(filter=f'display_name=\"{train_job_name}\"'):\n",
        "    train_hpt_job.delete()\n",
        "if model_export_custom_job.list(filter=f'display_name=\"{model_export_name}\"'):\n",
        "    model_export_custom_job.delete()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_tfvision_image_segmentation.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDfHfeR-5L5v"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aexF6b9b5L5v"
      },
      "source": [
        "# Vertex AI Model Garden - Hugging Face Pytorch Local Inference\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_pytorch_huggingface_local_inference.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_huggingface_local_inference.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eG5dQMC5L5v"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates how to run Hugging Face inference pipeline locally in a Colab notebook. \n",
        "\n",
        "### Objective\n",
        "\n",
        "- Run inference in a local pipeline.\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Colab Enterprise\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloab Enterprise pricing](https://cloud.google.com/colab/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3ADoR_P5L5v"
      },
      "source": [
        "# Run the examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "s4R_DGcf5L5v"
      },
      "outputs": [],
      "source": [
        "# @title Check if the Colab VM has GPU\n",
        "\n",
        "# @markdown **Important:** This notebook requires a GPU runtime to function correctly.\n",
        "# @markdown The default Colab runtime does not have a GPU and will not work. Follow instructions below to create a GPU runtime:\n",
        "# @markdown   1. [Create a runtime template](https://cloud.google.com/vertex-ai/docs/colab/create-runtime-template#create)\n",
        "# @markdown   2. [Create a runtime](https://cloud.google.com/vertex-ai/docs/colab/create-runtime#create) \\\n",
        "# @markdown\n",
        "# @markdown Once you have created a GPU runtime, you can use this notebook to run local inference.\n",
        "\n",
        "import subprocess\n",
        "\n",
        "if subprocess.run(\"nvidia-smi\").returncode:\n",
        "    raise RuntimeError(\n",
        "        \"Cannot communicate with GPU. Make sure you are using a GPU Colab runtime. \"\n",
        "        \"Go to the Runtimes menu and select/create a runtime with GPUs.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JUOsrmpx5L5v"
      },
      "outputs": [],
      "source": [
        "# @title Prepare the virtual environment\n",
        "\n",
        "# @markdown Run this section to install required packages for the virtual environment.\n",
        "\n",
        "! pip install --upgrade pip\n",
        "! pip install diffusers~=0.30.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "G-NNpcBL5L5v"
      },
      "outputs": [],
      "source": [
        "# @title Run inference for `text-to-image` task\n",
        "\n",
        "# @markdown Text-to-image generates an image from a text description, which is also known as a `prompt`.\n",
        "\n",
        "# @markdown This example runs [black-forest-labs/FLUX.1-schnell](https://huggingface.co/black-forest-labs/FLUX.1-schnell) model with Diffusers [FluxPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/flux). Note that this model requires at least 24GB GPU memory. `g2-standard-24` machine type is recommended.\n",
        "import torch\n",
        "from diffusers import FluxPipeline\n",
        "\n",
        "model_id = \"black-forest-labs/FLUX.1-schnell\"\n",
        "pipe = FluxPipeline.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"balanced\",\n",
        "    max_memory={0: \"20GB\", 1: \"20GB\"},\n",
        ")\n",
        "\n",
        "prompt = \"A cat holding a sign that says hello world\"  # @param {type:\"string\"}\n",
        "image = pipe(prompt, num_inference_steps=4, guidance_scale=0.0).images[0]\n",
        "\n",
        "display(image)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_pytorch_huggingface_local_inference.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

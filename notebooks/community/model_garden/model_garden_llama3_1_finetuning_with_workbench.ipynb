{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5YEniAr2q1fG"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Vertex AI Model Garden - Llama 3.1 Finetuning with customized container\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/instances\">\n",
        "      <img alt=\"Workbench logo\" src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" width=\"32px\"><br> Run in Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_llama3_1_finetuning_with_workbench.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_llama3_1_finetuning_with_workbench.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates -\n",
        "- Making changes to existing finetuning code.\n",
        "- Finetuning Llama 3.1 models with this modified code.\n",
        "- Deploying Llama 3.1 models.\n",
        "\n",
        "All of the examples in this notebook use parameter efficient finetuning methods [PEFT (LoRA)](https://github.com/huggingface/peft) to reduce training and storage costs. LoRA (Low-Rank Adaptation) is one approach of Parameter Efficient FineTuning (PEFT), where pretrained model weights are frozen and rank decomposition matrices representing the change in model weights are trained during finetuning. Read more about LoRA in the following publication: [Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L. and Chen, W., 2021. Lora: Low-rank adaptation of large language models. *arXiv preprint arXiv:2106.09685*](https://arxiv.org/abs/2106.09685).\n",
        "\n",
        "After finetuning, we can deploy models on Vertex with GPU.\n",
        "\n",
        "**It is advised to use the Workbench for this notebook.**\n",
        "\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Customize docker container code and rebuild the container.\n",
        "- Finetune Llama 3.1 models in local environment using docker run.\n",
        "- Finetune Llama 3.1 models with Vertex AI Custom Training Jobs.\n",
        "- Run local predictions for finetuned Llama 3.1 models.\n",
        "- Deploy finetuned Llama 3.1 models on Vertex AI Prediction and send prediction requests.\n",
        "\n",
        "### File a bug\n",
        "\n",
        "File a bug on [GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/issues/new) if you encounter any issue with the notebook.\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "264c07757582"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb7RPFuS3mvD"
      },
      "source": [
        "### Create workbench local environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkKLZRF73mvD"
      },
      "source": [
        "For finetuning using workbench local environment, **[click here](https://cloud.google.com/vertex-ai/docs/workbench/instances/create#console)** to create a workbench instance. We need 4 Nvidia A100 GPUs or 8 Nvidia A100 80 GB GPUs or 8 Nvidia H100 80 GB GPUs for training the 8b model. **Note that for 70b and 405b models 8 Nvidia A100 80 GB GPUs or 8 Nvidia H100 80 GB GPUs are required.**\n",
        "Follow [this](https://cloud.google.com/vertex-ai/docs/workbench/instances/create-euc-instance#create-instance) to link service account with workbench instance.\n",
        "\n",
        "Check following links to see if there is enough quota available to create workbench instance: [A100 GPU Quota](https://console.cloud.google.com/iam-admin/quotas?location=us-central1&metric=compute.googleapis.com%2Fnvidia_a100_gpus), [A100 80GB GPU Quota](https://console.cloud.google.com/iam-admin/quotas?location=us-central1&metric=compute.googleapis.com%2Fnvidia_a100_80gb_gpus), [H100 80GB GPU Quota](https://console.cloud.google.com/iam-admin/quotas?location=us-central1&metric=compute.googleapis.com%2Fgpus_per_gpu_family)\n",
        "\n",
        "Refer to [vertex AI Workbench instances locations](https://cloud.google.com/vertex-ai/docs/general/locations#instances) for the workbench instance availability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saDLCxT6rI0c"
      },
      "source": [
        "### Install Python Packages for Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "855d6b96f291"
      },
      "outputs": [],
      "source": [
        "! pip install --quiet google-cloud-aiplatform\n",
        "! pip install --quiet gcsfs==2024.3.1\n",
        "! pip install --quiet accelerate==0.31.0\n",
        "! pip install --quiet transformers==4.43.1\n",
        "! pip install --quiet datasets==2.19.2\n",
        "! pip install --quiet tensorflow==2.18.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0IoZB_DKga5"
      },
      "source": [
        "### Import the necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wb-TeIY-Kga5"
      },
      "outputs": [],
      "source": [
        "! rm -rf vertex-ai-samples && git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
        "! cd vertex-ai-samples && git reset --hard dd333b8fdd7dd22e8902a963fb8269885eac49ee\n",
        "\n",
        "import datetime\n",
        "import importlib\n",
        "import os\n",
        "import uuid\n",
        "from typing import Tuple\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "from google.cloud.aiplatform.compat.types import \\\n",
        "    custom_job as gca_custom_job_compat\n",
        "\n",
        "common_util = importlib.import_module(\n",
        "    \"vertex-ai-samples.community-content.vertex_model_garden.model_oss.notebook_util.common_util\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9mBvuBurI0c"
      },
      "source": [
        "### Setup Google Cloud project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kwN4_T3rI0c"
      },
      "source": [
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "2. For finetuning using Vertex AI training, schedule your job with cs via Dynamic Workload Scheduler using [these instructions](https://cloud.google.com/vertex-ai/docs/training/schedule-jobs-dws). For Dynamic Workload Scheduler, check Nvidia Tesla A100 quota in [us-central1](https://console.cloud.google.com/iam-admin/quotas?location=us-central1&metric=aiplatform.googleapis.com%2Fcustom_model_training_preemptible_nvidia_a100_gpus) or [europe-west4](https://console.cloud.google.com/iam-admin/quotas?location=europe-west4&metric=aiplatform.googleapis.com%2Fcustom_model_training_preemptible_nvidia_a100_gpus. Check the [us-central1](https://console.cloud.google.com/iam-admin/quotas?location=us-central1&metric=aiplatform.googleapis.com%2Fcustom_model_training_preemptible_nvidia_a100_80gb_gpus) or [europe-west4](https://console.cloud.google.com/iam-admin/quotas?location=europe-west4&metric=aiplatform.googleapis.com%2Fcustom_model_training_preemptible_nvidia_a100_80gb_gpus) quota for Nvidia A100 80GB GPUs. Check the [us-central1](https://console.cloud.google.com/iam-admin/quotas?location=us-central1&metric=aiplatform.googleapis.com%2Fcustom_model_training_preemptible_nvidia_h100_gpus) or [europe-west4](https://console.cloud.google.com/iam-admin/quotas?location=europe-west4&metric=aiplatform.googleapis.com%2Fcustom_model_training_preemptible_nvidia_h100_gpus) quota for Nvidia H100 GPUs. If you do not have enough GPUs, then you can follow [these instructions](https://cloud.google.com/docs/quotas/view-manage#viewing_your_quota_console) to request quota. **Note: 8 Nvidia Tesla A100 GPUs can only be used to run 8b and 70b parameter models. For 405b parameter model, 8 Nvidia A100 80 GB or 8 Nvidia H100 80 GB GPUs are required**.\n",
        "\n",
        "3. For serving, **[click here](https://console.cloud.google.com/iam-admin/quotas?location=us-central1&metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_l4_gpus)** to check if your project already has the required 1 L4 GPU in the us-central1 region.  If yes, then run this notebook in the us-central1 region. If you need more L4 GPUs for your project, then you can follow [these instructions](https://cloud.google.com/docs/quotas/view-manage#viewing_your_quota_console) to request more. Alternatively, if you want to run predictions with A100 80GB or H100 GPUs, we recommend using the regions listed below. **NOTE:** Make sure you have associated quota in selected regions. Click the links to see your current quota for each GPU type: [Nvidia A100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_a100_80gb_gpus), [Nvidia H100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_h100_gpus).\n",
        "\n",
        "> | Machine Type | Accelerator Type | Recommended Regions |\n",
        "| ----------- | ----------- | ----------- |\n",
        "| a2-ultragpu-1g | 1 NVIDIA_A100_80GB | us-central1, us-east4, europe-west4, asia-southeast1, us-east4 |\n",
        "| a3-highgpu-2g | 2 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "| a3-highgpu-4g | 4 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "| a3-highgpu-8g | 8 NVIDIA_H100_80GB | us-central1, us-east5, europe-west4, us-west1, asia-southeast1 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcIzOjH9Kga5"
      },
      "source": [
        "Set region."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0jkF1pVFKga5"
      },
      "outputs": [],
      "source": [
        "REGION = \"\"  # @param {type:\\\"string\\\"}\"\n",
        "assert REGION, \"Region must be specified.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmpFgm3hrI0c"
      },
      "source": [
        "**[Optional]** [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs. Set the BUCKET_URI for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`) should be located in the same region as where the notebook was launched. Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\"). If not set, a unique GCS bucket will be created instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ocGkSSYxrI0c"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = \"\"  # @param {type:\\\"string\\\"}\"\n",
        "if BUCKET_URI and not BUCKET_URI.startswith(\"gs://\"):\n",
        "    BUCKET_URI = \"gs://\" + BUCKET_URI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "05_d3h9fKga5"
      },
      "outputs": [],
      "source": [
        "train_job = None\n",
        "models, endpoints = {}, {}\n",
        "\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "if not REGION:\n",
        "    if not os.environ.get(\"GOOGLE_CLOUD_REGION\"):\n",
        "        raise ValueError(\n",
        "            \"REGION must be set. See\"\n",
        "            \" https://cloud.google.com/vertex-ai/docs/general/locations for\"\n",
        "            \" available cloud locations.\"\n",
        "        )\n",
        "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
        "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
        "\n",
        "# Cloud Storage bucket for storing the experiment artifacts.\n",
        "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
        "# prefer using your own GCS bucket, change the value yourself below.\n",
        "now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "\n",
        "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
        "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}-{str(uuid.uuid4())[:4]}\"\n",
        "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
        "else:\n",
        "    assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
        "    shell_output = ! gsutil ls -Lb {BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
        "    bucket_region = shell_output[0].strip().lower()\n",
        "    if bucket_region != REGION:\n",
        "        raise ValueError(\n",
        "            \"Bucket region %s is different from notebook region %s\"\n",
        "            % (bucket_region, REGION)\n",
        "        )\n",
        "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
        "\n",
        "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
        "MODEL_BUCKET = os.path.join(BUCKET_URI, \"llama3_1\")\n",
        "\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "# Gets the default SERVICE_ACCOUNT.\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
        "\n",
        "\n",
        "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
        "\n",
        "! gcloud config set project $PROJECT_ID\n",
        "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/storage.admin\"\n",
        "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/aiplatform.user\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYI6Njr8rI0c"
      },
      "source": [
        "### Access Llama 3.1 models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMPlolGkrI0c"
      },
      "source": [
        "For GPU based finetuning and serving, choose between accessing Llama 3.1 models on [Hugging Face](https://huggingface.co/) or Vertex AI as described below.\n",
        "\n",
        "If you already obtained access to Llama 3.1 models on [Hugging Face](https://huggingface.co/), you can load models from there.\n",
        "Alternatively, you can also load the original Llama 3.1 models for finetuning and serving from Vertex AI after accepting the agreement.\n",
        "\n",
        "It is recommended to use \"Google Cloud\" for 405B model since it can be downloaded faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-Ngr8K5koIYE"
      },
      "outputs": [],
      "source": [
        "# Modify the following parameter based on the model source.\n",
        "LOAD_MODEL_FROM = \"Google Cloud\"  # @param [\"Google Cloud\", \"Hugging Face\"]\n",
        "HF_TOKEN = \"\"\n",
        "MODEL_BUCKET = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPmUsIgErI0c"
      },
      "source": [
        "#### Access Model from Google Cloud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBUh3K5qrI0c"
      },
      "source": [
        "The original models from Meta are converted into the Hugging Face format for serving in Vertex AI.\n",
        "Accept the model agreement to access the models:\n",
        "1. Open the [Llama 3.1 model card](https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama3_1) from [Vertex AI Model Garden](https://cloud.google.com/model-garden).\n",
        "2. Review and accept the agreement in the pop-up window on the model card page. If you have previously accepted the model agreement, there will not be a pop-up window on the model card page and this step is not needed.\n",
        "3. After accepting the agreement of Llama 3.1, a `gs://` URI containing Llama 3.1 pretrained and finetuned models will be shared.\n",
        "4. Paste the URI in the `MODEL_BUCKET` field below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "PmFyCuADrI0c"
      },
      "outputs": [],
      "source": [
        "MODEL_BUCKET = \"\"  # @param {type:\"string\"}\n",
        "if LOAD_MODEL_FROM == \"Google Cloud\":\n",
        "    assert (\n",
        "        MODEL_BUCKET\n",
        "    ), \"Click the agreement of Llama3.1 in Vertex AI Model Garden, and get the GCS path of the model artifacts.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnoAAGyfrI0c"
      },
      "source": [
        "#### Access Llama 3.1 models on Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_q9h-SArI0c"
      },
      "source": [
        "You must provide a Hugging Face User Access Token (read) to access the Llama 3.1 models. You can follow the [Hugging Face documentation](https://huggingface.co/docs/hub/en/security-tokens) to create a **read** access token and put it in the `HF_TOKEN` field below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9m0OhQ9brI0c"
      },
      "outputs": [],
      "source": [
        "HF_TOKEN = \"\"  # @param {type:\"string\"}\n",
        "if LOAD_MODEL_FROM == \"Hugging Face\":\n",
        "    assert (\n",
        "        HF_TOKEN\n",
        "    ), \"Provide a read HF_TOKEN to load models from Hugging Face, or select a different model source.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMBmQSxlkU5n"
      },
      "source": [
        "## Finetune with HuggingFace PEFT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CESFVl_ZrI0c"
      },
      "source": [
        "### Set Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6tTFEI9rI0c"
      },
      "source": [
        "Use the Vertex AI SDK to create and run the custom training jobs.\n",
        "\n",
        "This notebook uses [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) dataset as an example.\n",
        "You can set `dataset_name` to any existing [Hugging Face dataset](https://huggingface.co/datasets) name, and set `instruct_column_in_dataset` to the name of the dataset column containing training data. The [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) has only one column `text`, and therefore we set `instruct_column_in_dataset` to `text` in this notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WISejLC2rI0c"
      },
      "source": [
        "#### (Optional) Prepare a custom JSONL dataset for finetuning\n",
        "You can prepare a JSONL file where each line is a valid JSON string as your custom training dataset. For example, here is one line from the [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) dataset:\n",
        "```\n",
        "{\"text\": \"### Human: Hola### Assistant: \\u00a1Hola! \\u00bfEn qu\\u00e9 puedo ayudarte hoy?\"}\n",
        "```\n",
        "\n",
        "The JSON object has a key `text`, which should match `instruct_column_in_dataset`; The value should be one training data point, i.e. a string. After you prepared your JSONL file, you can either upload it to [Hugging Face datasets](https://huggingface.co/datasets) or [Google Cloud Storage](https://cloud.google.com/storage).\n",
        "- To upload a JSONL dataset to [Hugging Face datasets](https://huggingface.co/datasets), follow the instructions on [Uploading Datasets](https://huggingface.co/docs/hub/en/datasets-adding). Then, set `dataset_name` to the name of your newly created dataset on Hugging Face.\n",
        "- To upload a JSONL dataset to [Google Cloud Storage](https://cloud.google.com/storage), follow the instructions on [Upload objects from a filesystem](https://cloud.google.com/storage/docs/uploading-objects). Then, set `dataset_name` to the `gs://` URI to your JSONL file. For example: `gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl`.\n",
        "\n",
        "Optionally update the `instruct_column_in_dataset` field below if your JSON objects use a key other than the default `text`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnyVrCu2rI0c"
      },
      "source": [
        "#### (Optional) Format your data with custom JSON template\n",
        "\n",
        "Sometimes, your dataset might have multiple text columns and you want to construct the training data with a template. You can prepare a JSON template in the following format:\n",
        "```\n",
        "{\n",
        "  \"description\": \"Template used by Llama 3.1, accepting text-bison format.\",\n",
        "  \"source\": \"https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-text-models-supervised#dataset-format\",\n",
        "  \"prompt_input\": \"<|start_header_id|>user<|end_header_id|>\\n\\n{input_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{output_text}<|eot_id|>\",\n",
        "  \"instruction_separator\": \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
        "  \"response_separator\": \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "}\n",
        "```\n",
        "\n",
        "As an example, the template above can be used to format the following training data (this line comes from `gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl`):\n",
        "\n",
        "```\n",
        "{\"input_text\":\"TRANSCRIPT: \\nREASON FOR EVALUATION:,\\n\\n LABEL:\",\"output_text\":\"Chiropractic\"}\n",
        "```\n",
        "\n",
        "This example template simply concatenates `input_text` with `output_text` with some special tokens in between.\n",
        "\n",
        "To try such custom dataset, you can make the following changes:\n",
        "1. Set `template` to `llama3-text-bison`\n",
        "2. Set `train_dataset_name` to `gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl`\n",
        "3. Set `train_split_name` to `train`\n",
        "4. Set `eval_dataset_name` to `gs://cloud-samples-data/vertex-ai/model-evaluation/peft_eval_sample.jsonl`\n",
        "5. Set `eval_split_name` to `train` (**NOT** `test`)\n",
        "6. Set `instruct_column_in_dataset` as `input_text`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Wz78SovrrI0c"
      },
      "outputs": [],
      "source": [
        "# Template name or gs:// URI to a custom template.\n",
        "template = \"openassistant-guanaco\"  # @param {type:\"string\"}\n",
        "\n",
        "# Hugging Face dataset name or gs:// URI to a custom JSONL dataset.\n",
        "train_dataset_name = \"timdettmers/openassistant-guanaco\"  # @param {type:\"string\"}\n",
        "train_split_name = \"train\"  # @param {type:\"string\"}\n",
        "eval_dataset_name = \"timdettmers/openassistant-guanaco\"  # @param {type:\"string\"}\n",
        "eval_split_name = \"test\"  # @param {type:\"string\"}\n",
        "\n",
        "# Name of the dataset column containing training text input.\n",
        "instruct_column_in_dataset = \"text\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNg65esqrI0c"
      },
      "source": [
        "### Set model\n",
        "\n",
        "Select a model variant of Llama 3.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nmYE2jJJkU5n"
      },
      "outputs": [],
      "source": [
        "# valid base model ids\n",
        "supported_base_model_ids = [\n",
        "    \"meta-llama/Meta-Llama-3.1-8B\",\n",
        "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "    \"meta-llama/Meta-Llama-3.1-70B\",\n",
        "    \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
        "    \"meta-llama/Meta-Llama-3.1-405B\",\n",
        "    \"meta-llama/Meta-Llama-3.1-405B-Instruct\",\n",
        "]\n",
        "\n",
        "base_model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "assert base_model_id in supported_base_model_ids, \"Provide a valid base model id.\"\n",
        "\n",
        "if LOAD_MODEL_FROM == \"Google Cloud\":\n",
        "    pretrained_model_id = os.path.join(MODEL_BUCKET, base_model_id.split(\"/\")[-1])\n",
        "else:\n",
        "    pretrained_model_id = base_model_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4_WvOHFW8hB"
      },
      "source": [
        "### Modify Finetuning docker\n",
        "Here we will demonstrate how we can make changes to [existing finetuning code from GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/tree/dd333b8fdd7dd22e8902a963fb8269885eac49ee/community-content/vertex_model_garden/model_oss/peft). One can follow a similar process to customize finetuning code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuAb4wnYkU5n"
      },
      "source": [
        "#### Modify Trainer stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoKNzRIirI0c"
      },
      "source": [
        "The original code for `callbacks.py` is [here](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/f641e5d2213f27acb203af02e02ff66b2ef8b9ba/community-content/vertex_model_garden/model_oss/peft/train/vmg/callbacks.py). The `callbacks.py` file contains callbacks in TrainerStatsCallback which will be executed at the end of training step. Currently this contains information about gpu usage, gpu memory usage and training throughput stats.\n",
        "Here we will modify `callbacks.py` to add the TFLOPS stats to trainer stats. TFLOPS is a unit of measurement for a GPU's performance that indicates how many floating-point operations a processor can perform per second."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jJt33DP5kU5n"
      },
      "outputs": [],
      "source": [
        "%%writefile vertex-ai-samples/community-content/vertex_model_garden/model_oss/peft/train/vmg/callbacks.py\n",
        "\"\"\"Different trainer callbacks for PEFT Trainer.\"\"\"\n",
        "\n",
        "import time\n",
        "\n",
        "from absl import logging\n",
        "import accelerate\n",
        "from transformers import TrainingArguments\n",
        "from transformers.trainer_callback import TrainerCallback\n",
        "from transformers.trainer_callback import TrainerControl\n",
        "from transformers.trainer_callback import TrainerState\n",
        "\n",
        "from vertex_vision_model_garden_peft.train.vmg import utils\n",
        "\n",
        "\n",
        "class TrainerStatsCallback(TrainerCallback):\n",
        "  \"\"\"Trainer callback to report trainer stats.\"\"\"\n",
        "\n",
        "  def __init__(self, max_seq_length, filename=None):\n",
        "    self._max_seq_length = max_seq_length\n",
        "    self._filename = filename\n",
        "\n",
        "    self._partial_state = accelerate.PartialState()\n",
        "    self._start_time = float('nan')\n",
        "    self._prev_time = float('nan')\n",
        "    self._peak_mem = 0.0\n",
        "    self._avg_throughput = 0.0\n",
        "    self._avg_tflops_per_sec = 0.0\n",
        "\n",
        "  def on_step_end(\n",
        "      self,\n",
        "      args: TrainingArguments,\n",
        "      state: TrainerState,\n",
        "      control: TrainerControl,\n",
        "      **kwargs,\n",
        "  ):\n",
        "    if self._partial_state.is_main_process:\n",
        "      if state.global_step == 1:\n",
        "        self._prev_time = time.time()\n",
        "        delta_t = float('nan')\n",
        "        self._prev_tflops = state.total_flos / 1e12\n",
        "        tflops_per_sec = 0.0\n",
        "      else:\n",
        "        cur_time = time.time()\n",
        "        cur_tflops = state.total_flos / 1e12\n",
        "        tflops_per_sec = (cur_tflops - self._prev_tflops) / (\n",
        "            cur_time - self._prev_time\n",
        "        )\n",
        "        self._prev_tflops = cur_tflops\n",
        "        self._avg_tflops_per_sec += (\n",
        "            tflops_per_sec - self._avg_tflops_per_sec\n",
        "        ) / (state.global_step - 1)\n",
        "        delta_t = cur_time - self._prev_time\n",
        "        self._prev_time = cur_time\n",
        "        self._avg_throughput += (delta_t - self._avg_throughput) / (\n",
        "            state.global_step - 1\n",
        "        )\n",
        "\n",
        "      gpu_stats = utils.gpu_stats()\n",
        "      self._peak_mem = max(gpu_stats.total_mem, self._peak_mem)\n",
        "      logging.info(\n",
        "          'on_step_end: %s, throughput: %.2f s/it, flops: %.2f tflops/s',\n",
        "          utils.gpu_stats_str(gpu_stats),\n",
        "          delta_t,\n",
        "          tflops_per_sec\n",
        "      )\n",
        "\n",
        "  def on_train_begin(\n",
        "      self,\n",
        "      args: TrainingArguments,\n",
        "      state: TrainerState,\n",
        "      control: TrainerControl,\n",
        "      **kwargs,\n",
        "  ):\n",
        "    if self._partial_state.is_main_process:\n",
        "      self._start_time = time.time()\n",
        "      logging.info('on_train_begin: %s', utils.gpu_stats_str())\n",
        "\n",
        "  def on_train_end(\n",
        "      self,\n",
        "      args: TrainingArguments,\n",
        "      state: TrainerState,\n",
        "      control: TrainerControl,\n",
        "      **kwargs,\n",
        "  ):\n",
        "    if self._partial_state.is_main_process:\n",
        "      train_time = time.time() - self._start_time\n",
        "      logging.info(\n",
        "          'training time %.2f s, throughput: %.2f s/it, peak_mem: %.2f GB',\n",
        "          train_time,\n",
        "          self._avg_throughput,\n",
        "          self._peak_mem,\n",
        "      )\n",
        "      if self._filename:\n",
        "        with open(self._filename, 'a') as out_f:\n",
        "          out_f.write(\n",
        "              f'{self._max_seq_length/1024.0:.1f}k | {self._peak_mem:.2f} |'\n",
        "              f' {self._avg_throughput:.2f} | {self._avg_tflops_per_sec:.2f}\\n'\n",
        "          )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MqTX9zxrI0c"
      },
      "source": [
        "#### Build docker using gcloud build\n",
        "Here we will add `cloudbuild.yaml` file to build and push docker container using gcloud builds.\n",
        "**Note: gcloud docker build takes at least 15 mins to finish.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lqHkYzxjnoDY"
      },
      "outputs": [],
      "source": [
        "%%writefile vertex-ai-samples/community-content/vertex_model_garden/cloudbuild.yaml\n",
        "steps:\n",
        "- name: 'gcr.io/cloud-builders/docker'\n",
        "  script: |\n",
        "    docker build -t $_LOCATION-docker.pkg.dev/$_PROJECT_ID/$_REPO_NAME/$_DOCKER_IMAGE_NAME:$_TAG_NAME  -f model_oss/peft/train/vmg/dockerfile/train.Dockerfile .\n",
        "  automapSubstitutions: true\n",
        "images:\n",
        "- '$_LOCATION-docker.pkg.dev/$_PROJECT_ID/$_REPO_NAME/$_DOCKER_IMAGE_NAME:$_TAG_NAME'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_sV8LVeekU5o"
      },
      "outputs": [],
      "source": [
        "REPOSITORY = \"vmg-llama-repo\"\n",
        "\n",
        "TAG_NAME = \"tflops\"\n",
        "\n",
        "DOCKER_IMAGE_NAME = \"peft\"\n",
        "\n",
        "# 1. Create a repository.\n",
        "\n",
        "! gcloud artifacts repositories create {REPOSITORY} --repository-format=docker --location={REGION} --description=\"Docker repository\" --quiet\n",
        "\n",
        "! gcloud artifacts repositories list\n",
        "\n",
        "# 2. Configure authentication to your private repo.\n",
        "\n",
        "! gcloud auth configure-docker {REGION}-docker.pkg.dev --quiet\n",
        "\n",
        "# 3. Build the docker image.\n",
        "\n",
        "! cd vertex-ai-samples/community-content/vertex_model_garden && gcloud builds submit --region=us-central1 \\\n",
        "--substitutions=_LOCATION={REGION},_PROJECT_ID={PROJECT_ID},_REPO_NAME={REPOSITORY},_TAG_NAME={TAG_NAME},_DOCKER_IMAGE_NAME={DOCKER_IMAGE_NAME} --config cloudbuild.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEdCSZnMrI0c"
      },
      "source": [
        "### Set Finetuning Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RNxoBuLrI0c"
      },
      "source": [
        "**Note**:\n",
        "1. We recommend setting `finetuning_precision_mode` to `4bit` because it enables using fewer hardware resources for finetuning.\n",
        "2. If `max_steps > 0`, it takes precedence over `epochs`. One can set a small `max_steps` value to quickly check the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RCy_TGgAznR8"
      },
      "outputs": [],
      "source": [
        "TRAIN_DOCKER_URI = (\n",
        "    f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{DOCKER_IMAGE_NAME}:{TAG_NAME}\"\n",
        ")\n",
        "\n",
        "# Batch size for finetuning.\n",
        "per_device_train_batch_size = 1  # @param{type:\"integer\"}\n",
        "# Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\n",
        "gradient_accumulation_steps = 4  # @param{type:\"integer\"}\n",
        "# Maximum sequence length.\n",
        "max_seq_length = 4096  # @param{type:\"integer\"}\n",
        "# Setting a positive `max_steps` here will override `num_epochs`.\n",
        "max_steps = -1  # @param{type:\"integer\"}\n",
        "num_epochs = 1.0  # @param{type:\"number\"}\n",
        "# Learning rate.\n",
        "learning_rate = 5e-5  # @param{type:\"number\"}\n",
        "# The scheduler type to use.\n",
        "lr_scheduler_type = \"cosine\"  # @param{type:\"string\"}\n",
        "# LoRA parameters.\n",
        "lora_rank = 16  # @param{type:\"integer\"}\n",
        "lora_alpha = 32  # @param{type:\"integer\"}\n",
        "lora_dropout = 0.05  # @param{type:\"number\"}\n",
        "# gradient checkpointing for the current model (may be referred to as activation checkpointing or checkpoint activations in other frameworks).\n",
        "enable_gradient_checkpointing = True\n",
        "# Attention implementation to use in the model.\n",
        "attn_implementation = \"flash_attention_2\"\n",
        "# The optimizer for which to schedule the learning rate.\n",
        "optimizer = \"adamw_torch\"\n",
        "# Define the proportion of training to be dedicated to a linear warmup where learning rate gradually increases.\n",
        "warmup_ratio = \"0.01\"\n",
        "# The list or string of integrations to report the results and logs to.\n",
        "report_to = \"tensorboard\"\n",
        "# Number of updates steps before two checkpoint saves.\n",
        "save_steps = 10\n",
        "# Number of update steps between two logs.\n",
        "logging_steps = save_steps\n",
        "# Precision to use for training.\n",
        "train_precision = \"float16\"\n",
        "\n",
        "\n",
        "base_output_dir = os.path.join(STAGING_BUCKET, \"modified_peft\")\n",
        "# Create a GCS folder to store the LORA adapter.\n",
        "lora_output_dir = os.path.join(base_output_dir, \"adapter\")\n",
        "# Create a GCS folder to store the merged model with the base model and the\n",
        "# finetuned LORA adapter.\n",
        "merged_model_output_dir = os.path.join(base_output_dir, \"merged-model\")\n",
        "# Create a GCS folder to store the finetuned LORA adapter.\n",
        "final_checkpoint = os.path.join(lora_output_dir, \"checkpoint-final\")\n",
        "\n",
        "eval_args = [\n",
        "    f\"--eval_dataset_path={eval_dataset_name}\",\n",
        "    f\"--eval_column={instruct_column_in_dataset}\",\n",
        "    f\"--eval_template={template}\",\n",
        "    f\"--eval_split={eval_split_name}\",\n",
        "    f\"--eval_steps={save_steps}\",\n",
        "    \"--eval_tasks=builtin_eval\",\n",
        "    \"--eval_metric_name=loss\",\n",
        "]\n",
        "\n",
        "training_args = [\n",
        "    \"--task=instruct-lora\",\n",
        "    \"--completion_only=True\",\n",
        "    f\"--pretrained_model_id={pretrained_model_id}\",\n",
        "    f\"--dataset_name={train_dataset_name}\",\n",
        "    f\"--train_split_name={train_split_name}\",\n",
        "    f\"--instruct_column_in_dataset={instruct_column_in_dataset}\",\n",
        "    f\"--output_dir={lora_output_dir}\",\n",
        "    f\"--merge_base_and_lora_output_dir={merged_model_output_dir}\",\n",
        "    f\"--per_device_train_batch_size={per_device_train_batch_size}\",\n",
        "    f\"--gradient_accumulation_steps={gradient_accumulation_steps}\",\n",
        "    f\"--lora_rank={lora_rank}\",\n",
        "    f\"--lora_alpha={lora_alpha}\",\n",
        "    f\"--lora_dropout={lora_dropout}\",\n",
        "    f\"--max_steps={max_steps}\",\n",
        "    f\"--max_seq_length={max_seq_length}\",\n",
        "    f\"--learning_rate={learning_rate}\",\n",
        "    f\"--lr_scheduler_type={lr_scheduler_type}\",\n",
        "    f\"--train_precision={train_precision}\",\n",
        "    f\"--enable_gradient_checkpointing={enable_gradient_checkpointing}\",\n",
        "    f\"--num_epochs={num_epochs}\",\n",
        "    f\"--attn_implementation={attn_implementation}\",\n",
        "    f\"--optimizer={optimizer}\",\n",
        "    f\"--warmup_ratio={warmup_ratio}\",\n",
        "    f\"--report_to={report_to}\",\n",
        "    f\"--logging_output_dir={base_output_dir}\",\n",
        "    f\"--save_steps={save_steps}\",\n",
        "    f\"--logging_steps={logging_steps}\",\n",
        "    f\"--template={template}\",\n",
        "    f\"--huggingface_access_token={HF_TOKEN}\",\n",
        "] + eval_args"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65VOcDB01lf3"
      },
      "source": [
        "### Local Finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wB0AIwGW8hB"
      },
      "source": [
        "This section demonstrates how to finetune a Llama 3.1 model with the modified peft docker using local run. The cell below will output docker command which need to be run in the local terminal.\n",
        "We need Local environment with 8 Nvidia A100 80 GB GPUs or 8 Nvidia H100 80 GB GPUs to run the command successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MkuMj8Sv1lf3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "num_of_gpus = torch.cuda.device_count()\n",
        "\n",
        "if num_of_gpus == 4:\n",
        "    local_training_args = training_args + [\n",
        "        \"--config_file=vertex_vision_model_garden_peft/deepspeed_zero2_4gpu.yaml\"\n",
        "    ]\n",
        "elif num_of_gpus == 8:\n",
        "    local_training_args = training_args + [\n",
        "        \"--config_file=vertex_vision_model_garden_peft/llama_fsdp_8gpu.yaml\"\n",
        "    ]\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported number of GPUs for local training: {num_of_gpus}.\")\n",
        "\n",
        "args = \" \".join(local_training_args)\n",
        "print(\"Run peft training with the following command:\\n\")\n",
        "\n",
        "print(\n",
        "    f\"docker run --gpus=all --net=host --rm --shm-size=128gb {TRAIN_DOCKER_URI} {args}\\n\"\n",
        ")\n",
        "\n",
        "print(\"after running the command, check the following files for the training results:\")\n",
        "print(\"LoRA adapter will be saved in:\", lora_output_dir)\n",
        "print(\"Trained and merged models will be saved in:\", merged_model_output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4JBAO7njiNF"
      },
      "source": [
        "#### Verify added trainer stats\n",
        "Once training starts, you should be able to see logs with above changes printed like below:\n",
        "\"on_step_end: GPU memory: 26.50(occupied=15.17, unused=7.83, smi_diff=3.50) GB. Utilization: 48.00%, throughput: 12.08 s/it, flops: 463.39 tflops/s\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0doRxXku1lf3"
      },
      "source": [
        "### Vertex AI finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhPzKwrJW8hB"
      },
      "source": [
        "This section demonstrates how to finetune a Llama 3.1 model with the modified peft docker using Vertex AI run. **This section is expected to take at least 30 mins to finish.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdvRYbYd1lf3"
      },
      "source": [
        "#### Set machine configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1yjdPmbH1lf3"
      },
      "outputs": [],
      "source": [
        "# Set accelerator type for training job. Accelerator type must be one of the following: NVIDIA_A100_80GB, NVIDIA_H100_80GB\n",
        "accelerator_type = \"NVIDIA_A100_80GB\"  # @param [\"NVIDIA_TESLA_A100\", \"NVIDIA_A100_80GB\", \"NVIDIA_H100_80GB\"]\n",
        "# Set number of replicas to use for training.\n",
        "replica_count = 1\n",
        "\n",
        "# Worker pool spec.\n",
        "if accelerator_type == \"NVIDIA_TESLA_A100\":\n",
        "    per_node_accelerator_count = 8\n",
        "    machine_type = \"a2-highgpu-8g\"\n",
        "    boot_disk_size_gb = 500\n",
        "    dws_kwargs = {\n",
        "        \"max_wait_duration\": 1800,  # 30 minutes\n",
        "        \"scheduling_strategy\": gca_custom_job_compat.Scheduling.Strategy.FLEX_START,\n",
        "    }\n",
        "elif accelerator_type == \"NVIDIA_A100_80GB\":\n",
        "    per_node_accelerator_count = 8\n",
        "    machine_type = \"a2-ultragpu-8g\"\n",
        "    boot_disk_size_gb = 500\n",
        "    dws_kwargs = {\n",
        "        \"max_wait_duration\": 1800,  # 30 minutes\n",
        "        \"scheduling_strategy\": gca_custom_job_compat.Scheduling.Strategy.FLEX_START,\n",
        "    }\n",
        "elif accelerator_type == \"NVIDIA_H100_80GB\":\n",
        "    per_node_accelerator_count = 8\n",
        "    machine_type = \"a3-highgpu-8g\"\n",
        "    boot_disk_size_gb = 2000\n",
        "    dws_kwargs = {\n",
        "        \"max_wait_duration\": 1800,  # 30 minutes\n",
        "        \"scheduling_strategy\": gca_custom_job_compat.Scheduling.Strategy.FLEX_START,\n",
        "    }\n",
        "else:\n",
        "    raise ValueError(\n",
        "        f\"Recommended machine settings not found for: {accelerator_type}. To use another accelerator type, edit this code block to pass in an appropriate `machine_type`, `accelerator_type`, and `per_node_accelerator_count` to the deploy_model_vllm function by clicking `Show Code` and then modifying the code.\"\n",
        "    )\n",
        "\n",
        "if replica_count == 1:\n",
        "    config_file = \"vertex_vision_model_garden_peft/llama_fsdp_8gpu.yaml\"\n",
        "elif replica_count <= 4:\n",
        "    config_file = (\n",
        "        \"vertex_vision_model_garden_peft/\"\n",
        "        f\"llama_hsdp_{replica_count * per_node_accelerator_count}gpu.yaml\"\n",
        "    )\n",
        "else:\n",
        "    raise ValueError(\n",
        "        f\"Recommended config settings not found for replica_count: {replica_count}.\"\n",
        "    )\n",
        "\n",
        "common_util.check_quota(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=per_node_accelerator_count,\n",
        "    is_for_training=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-trU7da81lf3"
      },
      "source": [
        "#### Run training job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SYeI6kbE1lf3"
      },
      "outputs": [],
      "source": [
        "vertex_training_args = training_args + [f\"--config_file={config_file}\"]\n",
        "job_name = common_util.get_job_name_with_datetime(\"llama3_1-lora-train\")\n",
        "\n",
        "# Add labels for the finetuning job.\n",
        "labels = {\n",
        "    \"mg-source\": \"notebook\",\n",
        "    \"mg-notebook-name\": \"model_garden_llama3_1_finetuning_with_workbench.ipynb\".split(\n",
        "        \".\"\n",
        "    )[0],\n",
        "}\n",
        "labels[\"mg-tune\"] = \"publishers-meta-models-llama3-1\"\n",
        "versioned_model_id = base_model_id.split(\"/\")[1].lower().replace(\".\", \"-\")\n",
        "labels[\"versioned-mg-tune\"] = f\"{labels['mg-tune']}-{versioned_model_id}\"\n",
        "\n",
        "# Pass training arguments and launch job.\n",
        "train_job = aiplatform.CustomContainerTrainingJob(\n",
        "    display_name=job_name,\n",
        "    container_uri=TRAIN_DOCKER_URI,\n",
        "    labels=labels,\n",
        ")\n",
        "\n",
        "print(\"Running training job with args:\")\n",
        "print(\" \\\\\\n\".join(training_args))\n",
        "train_job.run(\n",
        "    args=vertex_training_args,\n",
        "    replica_count=replica_count,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=per_node_accelerator_count,\n",
        "    boot_disk_size_gb=boot_disk_size_gb,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    base_output_dir=base_output_dir,\n",
        "    **dws_kwargs,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sEi-9jXB-Zj"
      },
      "source": [
        "#### Verify added trainer stats\n",
        "Once training starts, you can go to above traning job link and open the logs. In the logs you should be able to see above changes printed like below:\n",
        "\"on_step_end: GPU memory: 26.50(occupied=15.17, unused=7.83, smi_diff=3.50) GB. Utilization: 48.00%, throughput: 12.08 s/it, flops: 463.39 tflops/s\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6yqKZQirI0c"
      },
      "source": [
        "## Deploy with vLLM on GPUs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0q_7FsC3uvsr"
      },
      "outputs": [],
      "source": [
        "# Wait until training job is finished.\n",
        "if train_job.end_time is None:\n",
        "    print(\"Waiting for the training job to finish...\")\n",
        "    train_job.wait()\n",
        "    print(\"The training job has finished.\")\n",
        "\n",
        "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240819_0916_RC00\"\n",
        "\n",
        "# Set vllm prediction arguments\n",
        "prompt = \"What is a car?\"\n",
        "# If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, such as set `max_tokens` as 20.\n",
        "max_tokens = 50\n",
        "temperature = 1.0\n",
        "top_p = 1.0\n",
        "top_k = 1\n",
        "raw_response = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH4yCSECuvsr"
      },
      "source": [
        "### Run Local Predictions\n",
        "This section outputs docker run command. This command can be run inside local terminal.\n",
        "Local environment is required to be at least L4 GPUs for 8x7B models and A100/H100 GPUs for 8x22B models to run the command successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dPLATBlOuvsr"
      },
      "outputs": [],
      "source": [
        "# Set Docker Arguments\n",
        "GPU_DEVICES = 0\n",
        "SOURCE_DIR = \"/home/jupyter\"\n",
        "CODE_DIR = \"/home/jupyter\"\n",
        "\n",
        "# Set vllm arguments\n",
        "model_id = pretrained_model_id\n",
        "accelerator_count = 1\n",
        "gpu_memory_utilization = 0.95\n",
        "max_model_len = 8192\n",
        "dtype = \"auto\"\n",
        "max_loras = 1\n",
        "max_cpu_loras = 8\n",
        "max_num_seqs = 256\n",
        "enable_trust_remote_code = False\n",
        "enforce_eager = False\n",
        "enable_lora = True\n",
        "model_type = None\n",
        "\n",
        "vllm_args = [\n",
        "    \"python\",\n",
        "    \"-m\",\n",
        "    \"vllm.entrypoints.api_server\",\n",
        "    \"--host=0.0.0.0\",\n",
        "    \"--port=8080\",\n",
        "    f\"--model={model_id}\",\n",
        "    f\"--tensor-parallel-size={accelerator_count}\",\n",
        "    \"--swap-space=16\",\n",
        "    f\"--gpu-memory-utilization={gpu_memory_utilization}\",\n",
        "    f\"--max-model-len={max_model_len}\",\n",
        "    f\"--dtype={dtype}\",\n",
        "    f\"--max-loras={max_loras}\",\n",
        "    f\"--max-cpu-loras={max_cpu_loras}\",\n",
        "    f\"--max-num-seqs={max_num_seqs}\",\n",
        "    \"--disable-log-stats\",\n",
        "]\n",
        "\n",
        "if enable_trust_remote_code:\n",
        "    vllm_args.append(\"--trust-remote-code\")\n",
        "\n",
        "if enforce_eager:\n",
        "    vllm_args.append(\"--enforce-eager\")\n",
        "\n",
        "if enable_lora:\n",
        "    vllm_args.append(\"--enable-lora\")\n",
        "\n",
        "if model_type:\n",
        "    vllm_args.append(f\"--model-type={model_type}\")\n",
        "\n",
        "docker_cmd_part = f\"docker run -t --rm --gpus=all --net=host --shm-size 32gb  --volume {SOURCE_DIR}:{CODE_DIR}  -p 7080:7080  -e NVIDIA_VISIBLE_DEVICES={GPU_DEVICES}\"\n",
        "if HF_TOKEN:\n",
        "    docker_cmd_part += f\" -e HF_TOKEN={HF_TOKEN}\"\n",
        "docker_args = \" \".join(vllm_args)\n",
        "cmd = f\"{docker_cmd_part} {VLLM_DOCKER_URI} {docker_args}\"\n",
        "\n",
        "print(f\"run below command in local terminal to start the container:\\n {cmd}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx3qlCv69SSg"
      },
      "source": [
        "Once above command has been run successfully, you will see the server running on 7080.\n",
        "You can use below cell to run predictions on the vllm server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MV4UgUmN9SSg"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "vllm_request_data = {\n",
        "    \"prompt\": prompt,\n",
        "    \"max_tokens\": max_tokens,\n",
        "    \"temperature\": temperature,\n",
        "    \"top_p\": top_p,\n",
        "    \"top_k\": top_k,\n",
        "    \"raw_response\": raw_response,\n",
        "}\n",
        "\n",
        "curl_data = json.dumps(vllm_request_data)\n",
        "cmd = f\"curl --header 'Content-Type: application/json'  --request POST  --data '{curl_data}'  http://localhost:7080/generate\"\n",
        "\n",
        "print(f\"run below command in local terminal to send request to the container:\\n {cmd}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiTNQ3tLrI0c"
      },
      "source": [
        "### Deploy with Vertex AI\n",
        "This section uploads the model to Model Registry and deploys it on the Endpoint. It takes 15 minutes to 1 hour to finish depending on the size of model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kMQH9asKznR8"
      },
      "outputs": [],
      "source": [
        "if train_job.end_time is None:\n",
        "    print(\"Waiting for the training job to finish...\")\n",
        "    train_job.wait()\n",
        "    print(\"The training job has finished.\")\n",
        "\n",
        "# @markdown Set `use_dedicated_endpoint` to False if you don't want to use [dedicated endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment#create-dedicated-endpoint).\n",
        "use_dedicated_endpoint = True  # @param {type:\"boolean\"}\n",
        "\n",
        "# Find Vertex AI prediction supported accelerators and regions [here](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute).\n",
        "if \"8b\" in base_model_id.lower():\n",
        "    machine_type = \"g2-standard-12\"\n",
        "    accelerator_type = \"NVIDIA_L4\"\n",
        "    per_node_accelerator_count = 1\n",
        "elif \"70b\" in base_model_id.lower():\n",
        "    machine_type = \"g2-standard-96\"\n",
        "    accelerator_type = \"NVIDIA_L4\"\n",
        "    per_node_accelerator_count = 8\n",
        "elif \"405b\" in base_model_id.lower():\n",
        "    machine_type = \"a3-highgpu-8g\"\n",
        "    accelerator_type = \"NVIDIA_H100_80GB\"\n",
        "    per_node_accelerator_count = 8\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported model ID or GCS path: {base_model_id}.\")\n",
        "\n",
        "\n",
        "def get_deploy_source() -> str:\n",
        "    \"\"\"Gets deploy_source string based on running environment.\"\"\"\n",
        "    vertex_product = os.environ.get(\"VERTEX_PRODUCT\", \"\")\n",
        "    if vertex_product == \"COLAB_ENTERPRISE\":\n",
        "        return \"notebook_colab_enterprise\"\n",
        "    elif vertex_product == \"WORKBENCH_INSTANCE\":\n",
        "        return \"notebook_workbench\"\n",
        "    else:\n",
        "        # Legacy workbench, legacy colab, or other custom environments.\n",
        "        return \"notebook_environment_unspecified\"\n",
        "\n",
        "\n",
        "def deploy_model_vllm(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    publisher: str,\n",
        "    publisher_model_id: str,\n",
        "    service_account: str,\n",
        "    base_model_id: str = None,\n",
        "    machine_type: str = \"g2-standard-8\",\n",
        "    accelerator_type: str = \"NVIDIA_L4\",\n",
        "    accelerator_count: int = 1,\n",
        "    gpu_memory_utilization: float = 0.9,\n",
        "    max_model_len: int = 4096,\n",
        "    dtype: str = \"auto\",\n",
        "    enable_trust_remote_code: bool = False,\n",
        "    enforce_eager: bool = False,\n",
        "    enable_lora: bool = False,\n",
        "    enable_chunked_prefill: bool = False,\n",
        "    enable_prefix_cache: bool = False,\n",
        "    host_prefix_kv_cache_utilization_target: float = 0.0,\n",
        "    max_loras: int = 1,\n",
        "    max_cpu_loras: int = 8,\n",
        "    use_dedicated_endpoint: bool = False,\n",
        "    max_num_seqs: int = 256,\n",
        "    model_type: str = None,\n",
        "    enable_llama_tool_parser: bool = False,\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(\n",
        "        display_name=f\"{model_name}-endpoint\",\n",
        "        dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
        "    )\n",
        "\n",
        "    if not base_model_id:\n",
        "        base_model_id = model_id\n",
        "\n",
        "    # See https://docs.vllm.ai/en/latest/models/engine_args.html for a list of possible arguments with descriptions.\n",
        "    vllm_args = [\n",
        "        \"python\",\n",
        "        \"-m\",\n",
        "        \"vllm.entrypoints.api_server\",\n",
        "        \"--host=0.0.0.0\",\n",
        "        \"--port=8080\",\n",
        "        f\"--model={model_id}\",\n",
        "        f\"--tensor-parallel-size={accelerator_count}\",\n",
        "        \"--swap-space=16\",\n",
        "        f\"--gpu-memory-utilization={gpu_memory_utilization}\",\n",
        "        f\"--max-model-len={max_model_len}\",\n",
        "        f\"--dtype={dtype}\",\n",
        "        f\"--max-loras={max_loras}\",\n",
        "        f\"--max-cpu-loras={max_cpu_loras}\",\n",
        "        f\"--max-num-seqs={max_num_seqs}\",\n",
        "        \"--disable-log-stats\",\n",
        "    ]\n",
        "\n",
        "    if enable_trust_remote_code:\n",
        "        vllm_args.append(\"--trust-remote-code\")\n",
        "\n",
        "    if enforce_eager:\n",
        "        vllm_args.append(\"--enforce-eager\")\n",
        "\n",
        "    if enable_lora:\n",
        "        vllm_args.append(\"--enable-lora\")\n",
        "\n",
        "    if enable_chunked_prefill:\n",
        "        vllm_args.append(\"--enable-chunked-prefill\")\n",
        "\n",
        "    if enable_prefix_cache:\n",
        "        vllm_args.append(\"--enable-prefix-caching\")\n",
        "\n",
        "    if 0 < host_prefix_kv_cache_utilization_target < 1:\n",
        "        vllm_args.append(\n",
        "            f\"--host-prefix-kv-cache-utilization-target={host_prefix_kv_cache_utilization_target}\"\n",
        "        )\n",
        "\n",
        "    if model_type:\n",
        "        vllm_args.append(f\"--model-type={model_type}\")\n",
        "\n",
        "    if enable_llama_tool_parser:\n",
        "        vllm_args.append(\"--enable-auto-tool-choice\")\n",
        "        vllm_args.append(\"--tool-call-parser=vertex-llama-3\")\n",
        "\n",
        "    env_vars = {\n",
        "        \"MODEL_ID\": base_model_id,\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "    }\n",
        "\n",
        "    # HF_TOKEN is not a compulsory field and may not be defined.\n",
        "    try:\n",
        "        if HF_TOKEN:\n",
        "            env_vars[\"HF_TOKEN\"] = HF_TOKEN\n",
        "    except NameError:\n",
        "        pass\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
        "        serving_container_args=vllm_args,\n",
        "        serving_container_ports=[8080],\n",
        "        serving_container_predict_route=\"/generate\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "        serving_container_environment_variables=env_vars,\n",
        "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
        "        serving_container_deployment_timeout=7200,\n",
        "        model_garden_source_model_name=(\n",
        "            f\"publishers/{publisher}/models/{publisher_model_id}\"\n",
        "        ),\n",
        "    )\n",
        "    print(\n",
        "        f\"Deploying {model_name} on {machine_type} with {accelerator_count} {accelerator_type} GPU(s).\"\n",
        "    )\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=service_account,\n",
        "        system_labels={\n",
        "            \"NOTEBOOK_NAME\": \"model_garden_llama3_1_finetuning_with_workbench.ipynb\",\n",
        "            \"NOTEBOOK_ENVIRONMENT\": get_deploy_source(),\n",
        "        },\n",
        "    )\n",
        "    print(\"endpoint_name:\", endpoint.name)\n",
        "\n",
        "    return model, endpoint\n",
        "\n",
        "\n",
        "# Use FP8 base model for 405B since original model does not fit.\n",
        "deploy_pretrained_model_id = pretrained_model_id\n",
        "if \"Meta-Llama-3.1-405B\" in deploy_pretrained_model_id:\n",
        "    deploy_pretrained_model_id += \"-FP8\"\n",
        "print(\"Deploying model in:\", deploy_pretrained_model_id)\n",
        "\n",
        "common_util.check_quota(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=per_node_accelerator_count,\n",
        "    is_for_training=False,\n",
        ")\n",
        "\n",
        "models[\"vllm_gpu\"], endpoints[\"vllm_gpu\"] = deploy_model_vllm(\n",
        "    model_name=common_util.get_job_name_with_datetime(prefix=\"llama3_1-vllm-serve\"),\n",
        "    model_id=deploy_pretrained_model_id,\n",
        "    publisher=\"meta\",\n",
        "    publisher_model_id=\"llama3_1\",\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=per_node_accelerator_count,\n",
        "    gpu_memory_utilization=gpu_memory_utilization,\n",
        "    max_model_len=max_model_len,\n",
        "    enable_lora=True,\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wo4qzRjXG6C4"
      },
      "source": [
        "Once deployment succeeds, you can send requests to the endpoint with text prompts. Sampling parameters supported by vLLM can be found [here](https://docs.vllm.ai/en/latest/dev/sampling_params.html).\n",
        "Additionally, you can moderate the generated text with Vertex AI. See [Moderate text documentation](https://cloud.google.com/natural-language/docs/moderating-text) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fLKpMietG6C4"
      },
      "outputs": [],
      "source": [
        "def predict_vllm(\n",
        "    prompt: str,\n",
        "    max_tokens: int,\n",
        "    temperature: float,\n",
        "    top_p: float,\n",
        "    top_k: int,\n",
        "    raw_response: bool,\n",
        "    lora_weight: str = \"\",\n",
        "):\n",
        "    # Parameters for inference.\n",
        "    instance = {\n",
        "        \"prompt\": prompt,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": top_p,\n",
        "        \"top_k\": top_k,\n",
        "        \"raw_response\": raw_response,\n",
        "    }\n",
        "    if lora_weight:\n",
        "        instance[\"dynamic-lora\"] = lora_weight\n",
        "    instances = [instance]\n",
        "    response = endpoints[\"vllm_gpu\"].predict(\n",
        "        instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
        "    )\n",
        "\n",
        "    for prediction in response.predictions:\n",
        "        print(prediction)\n",
        "\n",
        "\n",
        "predict_vllm(\n",
        "    prompt=prompt,\n",
        "    max_tokens=max_tokens,\n",
        "    temperature=temperature,\n",
        "    top_p=top_p,\n",
        "    top_k=top_k,\n",
        "    raw_response=raw_response,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlr6yT3IW8hB"
      },
      "source": [
        "## Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KjdIvxyDW8hB"
      },
      "outputs": [],
      "source": [
        "if train_job:\n",
        "    train_job.delete()\n",
        "\n",
        "# @markdown  Delete the experiment models and endpoints to recycle the resources\n",
        "# @markdown  and avoid unnecessary continuous charges that may incur.\n",
        "\n",
        "# Undeploy model and delete endpoint.\n",
        "for endpoint in endpoints.values():\n",
        "    endpoint.delete(force=True)\n",
        "\n",
        "# Delete models.\n",
        "for model in models.values():\n",
        "    model.delete()\n",
        "\n",
        "delete_bucket = False  # @param {type:\"boolean\"}\n",
        "if delete_bucket:\n",
        "    ! gsutil -m rm -r $BUCKET_NAME"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_llama3_1_finetuning_with_workbench.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ApNHZJmT2AMH"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfxrFG052AMI"
      },
      "source": [
        "# Vertex AI Model Garden - TIMM\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_pytorch_timm.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_timm.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76BCoQcm2AMJ"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates running local inference using the [timm](https://github.com/rwightman/pytorch-image-models) library, finetuning the PyTorch [timm models](https://github.com/huggingface/pytorch-image-models#models), and deploying the models on [Vertex AI](https://cloud.google.com/vertex-ai).\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Setup environment.\n",
        "- Run inference locally using the timm library.\n",
        "- Create a custom training job on Vertex AI to train or finetune a model.\n",
        "- Deploy the model on Vertex AI for online prediction.\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvg5rRWpd88Z"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cb041d87f50a"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown 2. For finetuning, **[click here](https://console.cloud.google.com/iam-admin/quotas?location=us-central1&metric=aiplatform.googleapis.com%2Frestricted_image_training_nvidia_a100_80gb_gpus)** to check if your project already has the required 8 Nvidia A100 80 GB GPUs in the us-central1 region. If yes, then run this notebook in the us-central1 region. If you do not have 8 Nvidia A100 80 GPUs or have more GPU requirements than this, then schedule your job with Nvidia H100 GPUs via Dynamic Workload Scheduler using [these instructions](https://cloud.google.com/vertex-ai/docs/training/schedule-jobs-dws). For Dynamic Workload Scheduler, check the [us-central1](https://console.cloud.google.com/iam-admin/quotas?location=us-central1&metric=aiplatform.googleapis.com%2Fcustom_model_training_preemptible_nvidia_h100_gpus) or [europe-west4](https://console.cloud.google.com/iam-admin/quotas?location=europe-west4&metric=aiplatform.googleapis.com%2Fcustom_model_training_preemptible_nvidia_h100_gpus) quota for Nvidia H100 GPUs. If you do not have enough GPUs, then you can follow [these instructions](https://cloud.google.com/docs/quotas/view-manage#viewing_your_quota_console) to request quota.\n",
        "\n",
        "# @markdown 3. For serving, **[click here](https://console.cloud.google.com/iam-admin/quotas?location=us-central1&metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_l4_gpus)** to check if your project already has the required 1 L4 GPU in the us-central1 region.  If yes, then run this notebook in the us-central1 region. If you need more L4 GPUs for your project, then you can follow [these instructions](https://cloud.google.com/docs/quotas/view-manage#viewing_your_quota_console) to request more. Alternatively, if you want to run predictions with A100 80GB or H100 GPUs, we recommend using the regions listed below. **NOTE:** Make sure you have associated quota in selected regions. Click the links to see your current quota for each GPU type: [Nvidia A100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_a100_80gb_gpus), [Nvidia H100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_h100_gpus).\n",
        "\n",
        "# @markdown > | Machine Type | Accelerator Type | Recommended Regions |\n",
        "# @markdown | ----------- | ----------- | ----------- |\n",
        "# @markdown | a2-ultragpu-1g | 1 NVIDIA_A100_80GB | us-central1, us-east4, europe-west4, asia-southeast1, us-east4 |\n",
        "# @markdown | a3-highgpu-2g | 2 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "# @markdown | a3-highgpu-4g | 4 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "# @markdown | a3-highgpu-8g | 8 NVIDIA_H100_80GB | us-central1, us-east5, europe-west4, us-west1, asia-southeast1 |\n",
        "\n",
        "# @markdown 4. **[Optional]** [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs. Set the BUCKET_URI for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`) should be located in the same region as where the notebook was launched. Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\"). If not set, a unique GCS bucket will be created instead.\n",
        "\n",
        "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown 5. **[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment.\n",
        "\n",
        "REGION = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "! pip3 install timm\n",
        "\n",
        "import base64\n",
        "import datetime\n",
        "import importlib\n",
        "import os\n",
        "import urllib\n",
        "import uuid\n",
        "\n",
        "import timm\n",
        "import torch\n",
        "from google.cloud import aiplatform\n",
        "from PIL import Image\n",
        "from timm.data import resolve_data_config\n",
        "from timm.data.transforms_factory import create_transform\n",
        "\n",
        "! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
        "\n",
        "common_util = importlib.import_module(\n",
        "    \"vertex-ai-samples.community-content.vertex_model_garden.model_oss.notebook_util.common_util\"\n",
        ")\n",
        "\n",
        "models, endpoints = {}, {}\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "if not REGION:\n",
        "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
        "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
        "\n",
        "# Cloud Storage bucket for storing the experiment artifacts.\n",
        "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
        "# prefer using your own GCS bucket, change the value yourself below.\n",
        "now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "\n",
        "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
        "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}-{str(uuid.uuid4())[:4]}\"\n",
        "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
        "else:\n",
        "    assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
        "    shell_output = ! gsutil ls -Lb {BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
        "    bucket_region = shell_output[0].strip().lower()\n",
        "    if bucket_region != REGION:\n",
        "        raise ValueError(\n",
        "            \"Bucket region %s is different from notebook region %s\"\n",
        "            % (bucket_region, REGION)\n",
        "        )\n",
        "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
        "\n",
        "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
        "MODEL_BUCKET = os.path.join(BUCKET_URI, \"timm\")\n",
        "\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "# Gets the default SERVICE_ACCOUNT.\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
        "\n",
        "\n",
        "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
        "\n",
        "! gcloud config set project $PROJECT_ID\n",
        "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/storage.admin\"\n",
        "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/aiplatform.user\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14a500024b7b"
      },
      "source": [
        "## Run local inference\n",
        "\n",
        "This section runs local inference on an image using a selected pre-trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8b6bbe928998"
      },
      "outputs": [],
      "source": [
        "# @title Load a pretrained model\n",
        "\n",
        "# @markdown `MODEL_NAME :` The model you want to train and serve.\n",
        "\n",
        "# We use a ViT model as the example.\n",
        "MODEL_NAME = \"vit_tiny_patch16_224\"  # @param [\"vit_tiny_patch16_224\", \"beit_base_patch16_224\", \"deit3_small_patch16_224\", \"efficientnet_b2\", \"mobilenetv2_100\", \"resnet50\", \"resnest50d\", \"convnext_base\", \"cspdarknet53\", \"inception_v4\"]\n",
        "\n",
        "model = timm.create_model(MODEL_NAME, pretrained=True)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6390302c9761"
      },
      "outputs": [],
      "source": [
        "# @title Preprocess the image\n",
        "\n",
        "config = resolve_data_config({}, model=model)\n",
        "transform = create_transform(**config)\n",
        "\n",
        "# @markdown This example downloads a test image from GitHub Pytorch sample images folder.\n",
        "\n",
        "SOURCE = \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown You can upload and use your own images by changing `IMAGE_FILENAME`.\n",
        "\n",
        "! wget $SOURCE -O test.jpg\n",
        "IMAGE_FILENAME = \"test.jpg\"  # @param {type:\"string\"}\n",
        "# @markdown  You can also copy over images stored in a GCS bucket by running the command - `! gsutil cp \"gs://path/to/image\" \"test.jpg\"`\n",
        "\n",
        "img = Image.open(IMAGE_FILENAME).convert(\"RGB\")\n",
        "tensor = transform(img).unsqueeze(0)  # transform and add batch dimension\n",
        "display(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0a7a666a0aef"
      },
      "outputs": [],
      "source": [
        "# @title Get the model predictions\n",
        "\n",
        "# @markdown This section gives the probability of the top 5 predictions.\n",
        "\n",
        "with torch.no_grad():\n",
        "    out = model(tensor)\n",
        "probabilities = torch.nn.functional.softmax(out[0], dim=0)\n",
        "print(probabilities.shape)\n",
        "\n",
        "url, filename = (\n",
        "    \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\",\n",
        "    \"imagenet_classes.txt\",\n",
        ")\n",
        "urllib.request.urlretrieve(url, filename)\n",
        "with open(\"imagenet_classes.txt\") as f:\n",
        "    categories = [s.strip() for s in f.readlines()]\n",
        "\n",
        "# Print top categories per image.\n",
        "top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
        "for i in range(top5_prob.size(0)):\n",
        "    print(categories[top5_catid[i]], top5_prob[i].item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4120d1585355"
      },
      "source": [
        "## Run training job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qy4gKksX2AMK"
      },
      "outputs": [],
      "source": [
        "# @title Training job\n",
        "\n",
        "# @markdown This section runs a regular training job on Vertex AI.\n",
        "\n",
        "# @markdown Before creating a training job, you need to prepare the dataset for training and evaluation.\n",
        "# @markdown For example, you can use [ImageNet-1K](https://huggingface.co/datasets/imagenet-1k).\n",
        "\n",
        "# @markdown If you want to create a hyperparameter tuning job instead, you can skip to the next section.\n",
        "\n",
        "# The prebuilt training docker uri.\n",
        "TRAIN_DOCKER_URI = (\n",
        "    \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-timm-train\"\n",
        ")\n",
        "\n",
        "# The path to data directory on Cloud Storage without gs:// prefix.\n",
        "# In the form of: <bucket-name>/path-to-data\n",
        "\n",
        "# @markdown - `GCS_DATA_DIR` - The GCS path of the directory which contains the training data and the evaluation data.\n",
        "GCS_DATA_DIR = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# Input and output path.\n",
        "data_dir = f\"/gcs/{GCS_DATA_DIR}\"\n",
        "output_dir = os.path.join(MODEL_BUCKET, \"output\")\n",
        "\n",
        "# Worker pool spec.\n",
        "# Single node with multiple GPUs.\n",
        "TRAINING_MACHINE_TYPE = \"n1-highmem-32\"\n",
        "NUM_NODES = 1\n",
        "TRAINING_ACCELERATOR_TYPE = \"NVIDIA_TESLA_P100\"  # @param {type:\"string\"}\n",
        "TRAINING_ACCELERATOR_COUNT = 4\n",
        "\n",
        "# Model specific config.\n",
        "job_name = f\"pytorch-{MODEL_NAME}\"\n",
        "batch_size = 32\n",
        "epochs = 2\n",
        "\n",
        "train_job = aiplatform.CustomContainerTrainingJob(\n",
        "    display_name=job_name,\n",
        "    container_uri=TRAIN_DOCKER_URI,\n",
        ")\n",
        "\n",
        "train_job.run(\n",
        "    args=[\n",
        "        \"--standalone\",\n",
        "        f\"--nnodes={NUM_NODES}\",\n",
        "        f\"--nproc_per_node={TRAINING_ACCELERATOR_COUNT}\",\n",
        "        \"train.py\",\n",
        "        data_dir,\n",
        "        f\"--model={MODEL_NAME}\",\n",
        "        \"--pretrained\",\n",
        "        f\"--output={output_dir}\",\n",
        "        f\"--batch-size={batch_size}\",\n",
        "        f\"--epochs={epochs}\",\n",
        "    ],\n",
        "    replica_count=num_nodes,\n",
        "    machine_type=TRAINING_MACHINE_TYPE,\n",
        "    accelerator_type=TRAINING_ACCELERATOR_TYPE,\n",
        "    accelerator_count=TRAINING_ACCELERATOR_COUNT,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m36nMhAxeCf"
      },
      "source": [
        "## Run hyperparameter tuning job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Hy_aCff_2AML"
      },
      "outputs": [],
      "source": [
        "# @title Hyperparameter tuning job\n",
        "\n",
        "# @markdown You can use a [hyperparameter tuning](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview) job to find the best configuration of your hyperparameters.\n",
        "\n",
        "# @markdown You can skip this section if you already trained a model in the previous section and do not want to tune the hyperparameters.\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
        "\n",
        "# Input and output path.\n",
        "data_dir = f\"/gcs/{GCS_DATA_DIR}\"\n",
        "output_dir = common_util.gcs_fuse_path(MODEL_BUCKET)\n",
        "\n",
        "# Model specific configurations.\n",
        "job_name = f\"pytorch-hp-{MODEL_NAME}\"\n",
        "batch_size = 32\n",
        "epochs = 2\n",
        "\n",
        "# Machine specs.\n",
        "HPT_MACHINE_TYPE = \"n1-highmem-16\"\n",
        "num_nodes = 1\n",
        "HPT_ACCELERATOR_TYPE = \"NVIDIA_TESLA_V100\"  # @param {type:\"string\"}\n",
        "HPT_ACCELERATOR_COUNT = 2\n",
        "\n",
        "# Worker pool specs.\n",
        "worker_pool_specs = [\n",
        "    {\n",
        "        \"machine_spec\": {\n",
        "            \"machine_type\": HPT_MACHINE_TYPE,\n",
        "            \"accelerator_type\": HPT_ACCELERATOR_TYPE,\n",
        "            \"accelerator_count\": HPT_ACCELERATOR_COUNT,\n",
        "        },\n",
        "        \"replica_count\": num_nodes,\n",
        "        \"container_spec\": {\n",
        "            \"image_uri\": TRAIN_DOCKER_URI,\n",
        "            \"args\": [\n",
        "                \"--standalone\",\n",
        "                f\"--nnodes={num_nodes}\",\n",
        "                f\"--nproc_per_node={HPT_ACCELERATOR_COUNT}\",\n",
        "                \"train.py\",\n",
        "                data_dir,\n",
        "                f\"--model={MODEL_NAME}\",\n",
        "                \"--pretrained\",\n",
        "                f\"--output={output_dir}\",\n",
        "                f\"--batch-size={batch_size}\",\n",
        "                f\"--epochs={epochs}\",\n",
        "            ],\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "# Hyperparameter job specs.\n",
        "metric_spec = {\"top1_accuracy\": \"maximize\"}\n",
        "parameter_spec = {\n",
        "    \"lr\": hpt.DoubleParameterSpec(min=0.001, max=0.05, scale=\"log\"),\n",
        "}\n",
        "max_trial_count = 2\n",
        "parallel_trial_count = 2\n",
        "\n",
        "# Check quota.\n",
        "common_util.check_quota(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    accelerator_type=HPT_ACCELERATOR_TYPE,\n",
        "    accelerator_count=HPT_ACCELERATOR_COUNT,\n",
        "    is_for_training=True,\n",
        ")\n",
        "\n",
        "# Launch jobs.\n",
        "training_job = aiplatform.CustomJob(\n",
        "    display_name=job_name, worker_pool_specs=worker_pool_specs\n",
        ")\n",
        "hp_job = aiplatform.HyperparameterTuningJob(\n",
        "    display_name=job_name,\n",
        "    custom_job=training_job,\n",
        "    metric_spec=metric_spec,\n",
        "    parameter_spec=parameter_spec,\n",
        "    max_trial_count=max_trial_count,\n",
        "    parallel_trial_count=parallel_trial_count,\n",
        ")\n",
        "\n",
        "hp_job.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAyRWwqW2AML"
      },
      "source": [
        "## Deploy model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jbNbg0yR2AML"
      },
      "outputs": [],
      "source": [
        "# @title Deploy\n",
        "\n",
        "# @markdown This section uploads the model to Model Registry and deploys it on an Endpoint resource.\n",
        "# @markdown This step will take ~15 minutes to complete.\n",
        "\n",
        "# @markdown The uploaded models and the endpoints can be managed in the [Model Registry](https://console.cloud.google.com/vertex-ai/models) and the [Endpoints](https://console.cloud.google.com/vertex-ai/endpoints) respectively.\n",
        "\n",
        "# The prebuilt serving docker uri.\n",
        "SERVE_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai-restricted/vertex-vision-model-garden-dockers/pytorch-timm-serve\"\n",
        "# The port number used by torchserve traffic.\n",
        "SERVE_PORT = 7080\n",
        "\n",
        "\n",
        "SERVING_MACHINE_TYPE = \"n1-standard-8\"\n",
        "SERVING_ACCELERATOR_TYPE = \"NVIDIA_TESLA_T4\"  # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "# @markdown - `MODEL_PT_PATH : ` The Cloud Storage path which contains the model checkpoint file(.pth extension).\n",
        "# @markdown - for e.g. - `gs://path_to_model_best.pth.tar`.\n",
        "MODEL_PT_PATH = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown - `INDEX_TO_NAME_FILE : ` [Optional] The Cloud Storage path to index_to_name.json, including gs:// prefix.\n",
        "# @markdown - for e.g. - `gs://path_to_index_to_name.json`\n",
        "INDEX_TO_NAME_FILE = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# Upload model.\n",
        "if INDEX_TO_NAME_FILE:\n",
        "    serving_env = {\n",
        "        \"MODEL_ID\": \"timm-mobilenetv2-100\",\n",
        "        \"MODEL_NAME\": MODEL_NAME,\n",
        "        \"MODEL_PT_PATH\": MODEL_PT_PATH,\n",
        "        \"INDEX_TO_NAME_FILE\": INDEX_TO_NAME_FILE,\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "    }\n",
        "else:\n",
        "    serving_env = {\n",
        "        \"MODEL_ID\": \"timm-mobilenetv2-100\",\n",
        "        \"MODEL_NAME\": MODEL_NAME,\n",
        "        \"MODEL_PT_PATH\": MODEL_PT_PATH,\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "    }\n",
        "\n",
        "models[\"timm-model\"] = aiplatform.Model.upload(\n",
        "    display_name=MODEL_NAME,\n",
        "    serving_container_image_uri=SERVE_DOCKER_URI,\n",
        "    serving_container_ports=[SERVE_PORT],\n",
        "    serving_container_predict_route=\"/predictions/timm_serving\",\n",
        "    serving_container_health_route=\"/ping\",\n",
        "    serving_container_environment_variables=serving_env,\n",
        ")\n",
        "# Or reuse a pre-uploaded model.\n",
        "# models[\"timm-model\"] = aiplatform.Model('projects/123456789/locations/us-central1/models/123456789@1')\n",
        "\n",
        "# Create an endpoint.\n",
        "endpoints[\"timm-endpoint\"] = aiplatform.Endpoint.create(\n",
        "    display_name=\"pytorch-timm-endpoint\"\n",
        ")\n",
        "# Or reuse a pre-created endpoint.\n",
        "# endpoints[\"timm-endpoint\"] = aiplatform.Endpoint('projects/123456789/locations/us-central1/endpoints/123456789')\n",
        "\n",
        "# Check quota.\n",
        "common_util.check_quota(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    accelerator_type=SERVING_ACCELERATOR_TYPE,\n",
        "    accelerator_count=1,\n",
        "    is_for_training=False,\n",
        ")\n",
        "\n",
        "# Deploy model to endpoint.\n",
        "models[\"timm-model\"].deploy(\n",
        "    endpoint=endpoints[\"timm-endpoint\"],\n",
        "    machine_type=SERVING_MACHINE_TYPE,\n",
        "    accelerator_type=SERVING_ACCELERATOR_TYPE,\n",
        "    accelerator_count=1,\n",
        "    traffic_percentage=100,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    system_labels={\"NOTEBOOK_NAME\": \"model_garden_pytorch_timm.ipynb\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zL2Qbm2x2AMM"
      },
      "outputs": [],
      "source": [
        "# @title Predict\n",
        "\n",
        "# You can get the deployed endpoint object by its resource name returned by Endpoint.create(). For example:\n",
        "# endpoints[\"timm-endpoint\"] = aiplatform.Endpoint('projects/816369962409/locations/us-central1/endpoints/8809168414485512192')\n",
        "\n",
        "# @markdown Upload an image along with its filename below.\n",
        "IMAGE_FILENAME = \"test.jpg\"  # @param {type:\"string\"}\n",
        "\n",
        "# Alternatively, uncomment the following line to download a cat image for demonstration.\n",
        "# ! wget http://images.cocodataset.org/val2017/000000039769.jpg -O test.jpg\n",
        "\n",
        "with open(IMAGE_FILENAME, \"rb\") as f:\n",
        "    image_b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "instances = [{\"data\": {\"b64\": image_b64}}]\n",
        "\n",
        "prediction = endpoints[\"timm-endpoint\"].predict(instances=instances)\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqSxSyT42AMM"
      },
      "source": [
        "## Clean Up Resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tMwdxiH-2AMM"
      },
      "outputs": [],
      "source": [
        "# @title Delete the job, model and endpoint\n",
        "\n",
        "# Delete the training job and hyperparameter tuning job.\n",
        "train_job.delete()\n",
        "hp_job.delete()\n",
        "\n",
        "# @markdown  Delete the experiment models and endpoints to recycle the resources\n",
        "# @markdown  and avoid unnecessary continuous charges that may incur.\n",
        "\n",
        "# Undeploy model and delete endpoint.\n",
        "for endpoint in endpoints.values():\n",
        "    endpoint.delete(force=True)\n",
        "\n",
        "# Delete models.\n",
        "for model in models.values():\n",
        "    model.delete()\n",
        "\n",
        "delete_bucket = False  # @param {type:\"boolean\"}\n",
        "if delete_bucket:\n",
        "    ! gsutil -m rm -r $BUCKET_NAME"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_pytorch_timm.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TirJ-SGQseby"
      },
      "source": [
        "# Vertex AI Model Garden MoViNet video action recognition\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_movinet_action_recognition.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_movinet_action_recognition.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates how to use [MoViNet](https://github.com/tensorflow/models/tree/master/official/projects/movinet) for video action recognition in Vertex AI Model Garden.\n",
        "\n",
        "### Objective\n",
        "\n",
        "* Train new models\n",
        "  * Convert input data to training formats\n",
        "  * Create [hyperparameter tuning jobs](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview) to train new models\n",
        "  * Find and export best models\n",
        "\n",
        "* Test trained models\n",
        "  * Upload models to the [Vertex AI Model Registry](https://cloud.google.com/vertex-ai/docs/model-registry/introduction)\n",
        "  * Run batch predictions\n",
        "\n",
        "* Clean up resources\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEukV6uRk_S3"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9wExiMUxFk91"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown 2. For finetuning, **[click here](https://console.cloud.google.com/iam-admin/quotas?location=us-central1&metric=aiplatform.googleapis.com%2Frestricted_image_training_nvidia_a100_80gb_gpus)** to check if your project already has the required 8 Nvidia A100 80 GB GPUs in the us-central1 region. If yes, then run this notebook in the us-central1 region. If you do not have 8 Nvidia A100 80 GPUs or have more GPU requirements than this, then schedule your job with Nvidia H100 GPUs via Dynamic Workload Scheduler using [these instructions](https://cloud.google.com/vertex-ai/docs/training/schedule-jobs-dws). For Dynamic Workload Scheduler, check the [us-central1](https://console.cloud.google.com/iam-admin/quotas?location=us-central1&metric=aiplatform.googleapis.com%2Fcustom_model_training_preemptible_nvidia_h100_gpus) or [europe-west4](https://console.cloud.google.com/iam-admin/quotas?location=europe-west4&metric=aiplatform.googleapis.com%2Fcustom_model_training_preemptible_nvidia_h100_gpus) quota for Nvidia H100 GPUs. If you do not have enough GPUs, then you can follow [these instructions](https://cloud.google.com/docs/quotas/view-manage#viewing_your_quota_console) to request quota.\n",
        "\n",
        "# @markdown 3. For serving, **[click here](https://console.cloud.google.com/iam-admin/quotas?location=us-central1&metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_l4_gpus)** to check if your project already has the required 1 L4 GPU in the us-central1 region.  If yes, then run this notebook in the us-central1 region. If you need more L4 GPUs for your project, then you can follow [these instructions](https://cloud.google.com/docs/quotas/view-manage#viewing_your_quota_console) to request more. Alternatively, if you want to run predictions with A100 80GB or H100 GPUs, we recommend using the regions listed below. **NOTE:** Make sure you have associated quota in selected regions. Click the links to see your current quota for each GPU type: [Nvidia A100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_a100_80gb_gpus), [Nvidia H100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_h100_gpus).\n",
        "\n",
        "# @markdown > | Machine Type | Accelerator Type | Recommended Regions |\n",
        "# @markdown | ----------- | ----------- | ----------- |\n",
        "# @markdown | a2-ultragpu-1g | 1 NVIDIA_A100_80GB | us-central1, us-east4, europe-west4, asia-southeast1, us-east4 |\n",
        "# @markdown | a3-highgpu-2g | 2 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "# @markdown | a3-highgpu-4g | 4 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "# @markdown | a3-highgpu-8g | 8 NVIDIA_H100_80GB | us-central1, europe-west4, us-west1, asia-southeast1 |\n",
        "\n",
        "# @markdown 4. **[Optional]** [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs. Set the BUCKET_URI for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`) should be located in the same region as where the notebook was launched. Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\"). If not set, a unique GCS bucket will be created instead.\n",
        "\n",
        "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown 5. **[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment.\n",
        "\n",
        "REGION = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
        "\n",
        "import datetime\n",
        "import importlib\n",
        "import json\n",
        "import os\n",
        "import subprocess\n",
        "import uuid\n",
        "from typing import Dict\n",
        "\n",
        "import yaml\n",
        "from google.cloud import aiplatform\n",
        "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
        "\n",
        "common_util = importlib.import_module(\n",
        "    \"vertex-ai-samples.community-content.vertex_model_garden.model_oss.notebook_util.common_util\"\n",
        ")\n",
        "\n",
        "models, endpoints = {}, {}\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "if not REGION:\n",
        "    if not os.environ.get(\"GOOGLE_CLOUD_REGION\"):\n",
        "        raise ValueError(\n",
        "            \"REGION must be set. See\"\n",
        "            \" https://cloud.google.com/vertex-ai/docs/general/locations for\"\n",
        "            \" available cloud locations.\"\n",
        "        )\n",
        "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
        "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
        "\n",
        "# Cloud Storage bucket for storing the experiment artifacts.\n",
        "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
        "# prefer using your own GCS bucket, change the value yourself below.\n",
        "now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "\n",
        "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
        "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}-{str(uuid.uuid4())[:4]}\"\n",
        "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
        "else:\n",
        "    assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
        "    shell_output = ! gsutil ls -Lb {BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
        "    bucket_region = shell_output[0].strip().lower()\n",
        "    if bucket_region != REGION:\n",
        "        raise ValueError(\n",
        "            \"Bucket region %s is different from notebook region %s\"\n",
        "            % (bucket_region, REGION)\n",
        "        )\n",
        "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
        "\n",
        "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
        "MODEL_BUCKET = os.path.join(BUCKET_URI, \"movinet_ar\")\n",
        "\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "# Gets the default SERVICE_ACCOUNT.\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
        "\n",
        "\n",
        "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
        "\n",
        "! gcloud config set project $PROJECT_ID\n",
        "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/storage.admin\"\n",
        "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/aiplatform.user\"\n",
        "\n",
        "# @markdown Kindly note Regions of \"us\", \"asia\", or \"europe\" are supported.\n",
        "\n",
        "REGION_PREFIX = REGION.split(\"-\")[0]\n",
        "assert REGION_PREFIX in (\n",
        "    \"us\",\n",
        "    \"europe\",\n",
        "    \"asia\",\n",
        "), f'{REGION} is not supported. It must be prefixed by \"us\", \"asia\", or \"europe\".'\n",
        "\n",
        "CHECKPOINT_BUCKET = os.path.join(BUCKET_URI, \"ckpt\")\n",
        "\n",
        "# Download config files.\n",
        "CONFIG_DIR = os.path.join(BUCKET_URI, \"config\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3--Ad5NUgH7"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3Ry1mw6AHLTy"
      },
      "outputs": [],
      "source": [
        "# @title Set the model for training\n",
        "\n",
        "# @markdown This section shows how to train new models.\n",
        "# @markdown 1. Convert input data to training formats\n",
        "# @markdown 2. Create hyperparameter tuning jobs to train new models\n",
        "# @markdown 3. Find and export best models\n",
        "\n",
        "# @markdown If you already trained models, kindly go to the section `Test Trained models`.\n",
        "\n",
        "# @markdown Select a model:\n",
        "# @markdown * `model_id`: MoViNet model variant ID, one of `a0`, `a1`, `a2`, `a3`, `a4`, `a5`. The model with a larger number requires more resources to train, and is expected to have a higher accuracy and latency. Here, we use `a3` for demonstration purpose. **`a0`, `a1`, and `a2` are not recommended for now as we are currently investigating some inference issues with them.**\n",
        "# @markdown * `model_mode`: MoViNet model type, either `base` or `stream`. The base model has a slightly higher accuracy, while the streaming model is optimized for streaming and faster CPU inference. See [official MoViNet docs](https://github.com/tensorflow/models/tree/master/official/projects/movinet) for more information.\n",
        "\n",
        "# @markdown **Note**: The prediction container only supports base model (non-streaming) for now. If you train a streaming model, you need to download the model and refer to the [MoViNet official guide](https://github.com/tensorflow/models/blob/master/official/projects/movinet/movinet_streaming_model_training_and_inference.ipynb) for running predictions locally.\n",
        "\n",
        "IMAGE_SIZES = {\n",
        "    \"a0\": 172,\n",
        "    \"a1\": 172,\n",
        "    \"a2\": 224,\n",
        "    \"a3\": 256,\n",
        "    \"a4\": 290,\n",
        "    \"a5\": 320,\n",
        "}\n",
        "\n",
        "model_id = \"a3\"  # @param [\"a0\", \"a1\", \"a2\", \"a3\", \"a4\", \"a5\"]\n",
        "model_mode = \"base\"  # @param [\"base\", \"stream\"]\n",
        "is_stream = model_mode == \"stream\"\n",
        "model_name = f\"movinet_{model_id}_{model_mode}\"\n",
        "image_size = IMAGE_SIZES[model_id]\n",
        "\n",
        "if is_stream:\n",
        "    export_container_args = {\n",
        "        \"conv_type\": \"2plus1d\",\n",
        "        \"se_type\": \"2plus3d\",\n",
        "        \"activation\": \"hard_swish\",\n",
        "        \"gating_activation\": \"hard_sigmoid\",\n",
        "        \"use_positional_encoding\": model_id in {\"a3\", \"a4\", \"a5\"},\n",
        "    }\n",
        "else:\n",
        "    export_container_args = {\n",
        "        \"conv_type\": \"3d\",\n",
        "        \"se_type\": \"3d\",\n",
        "        \"activation\": \"swish\",\n",
        "        \"gating_activation\": \"sigmoid\",\n",
        "        \"use_positional_encoding\": False,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IndQ_m6ddUEM"
      },
      "outputs": [],
      "source": [
        "# @title Prepare training data\n",
        "\n",
        "# @markdown Prepare data in the format as described [here](https://cloud.google.com/vertex-ai/docs/video-data/action-recognition/prepare-data), and then convert them to the training formats by running the cell below:\n",
        "\n",
        "# @markdown * `input_file_path`: The input file path to the prepared data.\n",
        "\n",
        "# @markdown * `input_file_type`: The input file type, such as `csv` or `jsonl`.\n",
        "\n",
        "# @markdown * `output_fps`: The sampling rate of the video; Frames per second.\n",
        "\n",
        "# @markdown * `num_frames`: Number of frame to sample around keyframe inputs.\n",
        "\n",
        "# @markdown * `min_duration_sec`: Minimum duration in seconds for sampling video clips around keyframe inputs. This is for validation purpose - an error will be thrown if there is not enough context around a keyframe.\n",
        "\n",
        "# @markdown * `pos_neg_ratio`: Sampling ratio between positive and negative segments. For example, a pos_neg_ratio of 0.5 samples 1 negative instance every 2 positive instances.\n",
        "\n",
        "# @markdown * `split_ratio`: Three comma separated floats indicating the proportion of data to split into train/validation/test. They must add up to 1.\n",
        "\n",
        "# @markdown * `num_shard`: Three comma separated integers indicating the shards for train/validation/test.\n",
        "\n",
        "# @markdown **Note**: For JSONL input, use `aiplatform.googleapis.com/ml_use` instead of `ml_use` as the JSON key for ML use in `dataItemResourceLabels`. This is to be consistent with other objectives.\n",
        "\n",
        "# This job will convert input data as training format, with given split ratios\n",
        "# and number of shards on train/test/validation.\n",
        "\n",
        "OBJECTIVE = \"var\"\n",
        "\n",
        "# Data converter constants.\n",
        "DATA_CONVERTER_JOB_PREFIX = \"data_converter\"\n",
        "DATA_CONVERTER_CONTAINER = f\"{REGION_PREFIX}-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/data-converter\"\n",
        "DATA_CONVERTER_MACHINE_TYPE = \"n1-highmem-8\"\n",
        "\n",
        "data_converter_job_name = common_util.get_job_name_with_datetime(\n",
        "    DATA_CONVERTER_JOB_PREFIX + \"_\" + OBJECTIVE\n",
        ")\n",
        "\n",
        "input_file_path = \"\"  # @param {type:\"string\"}\n",
        "input_file_type = \"csv\"  # @param [\"csv\", \"jsonl\"]\n",
        "output_fps = 10  # @param {type:\"integer\"}\n",
        "num_frames = 32  # @param {type:\"integer\"}\n",
        "min_duration_sec = 1.0  # @param {type:\"number\"}\n",
        "pos_neg_ratio = 1.0  # @param {type:\"number\"}\n",
        "split_ratio = \"0.8,0.1,0.1\"\n",
        "num_shard = \"10,10,10\"\n",
        "data_converter_output_dir = os.path.join(BUCKET_URI, data_converter_job_name)\n",
        "\n",
        "worker_pool_specs = [\n",
        "    {\n",
        "        \"machine_spec\": {\n",
        "            \"machine_type\": DATA_CONVERTER_MACHINE_TYPE,\n",
        "        },\n",
        "        \"replica_count\": 1,\n",
        "        \"container_spec\": {\n",
        "            \"image_uri\": DATA_CONVERTER_CONTAINER,\n",
        "            \"command\": [],\n",
        "            \"args\": [\n",
        "                \"--input_file_path=%s\" % input_file_path,\n",
        "                \"--input_file_type=%s\" % input_file_type,\n",
        "                \"--objective=%s\" % OBJECTIVE,\n",
        "                \"--num_shard=%s\" % num_shard,\n",
        "                \"--split_ratio=%s\" % split_ratio,\n",
        "                \"--output_dir=%s\" % data_converter_output_dir,\n",
        "                \"--output_fps=%d\" % output_fps,\n",
        "                \"--num_frames=%d\" % num_frames,\n",
        "                \"--min_duration_sec=%f\" % min_duration_sec,\n",
        "                \"--pos_neg_ratio=%f\" % pos_neg_ratio,\n",
        "                \"--output_shape=%d,%d\" % (image_size, image_size),\n",
        "            ],\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "data_converter_custom_job = aiplatform.CustomJob(\n",
        "    display_name=data_converter_job_name,\n",
        "    project=PROJECT_ID,\n",
        "    worker_pool_specs=worker_pool_specs,\n",
        "    staging_bucket=STAGING_BUCKET,\n",
        ")\n",
        "\n",
        "data_converter_custom_job.run(service_account=SERVICE_ACCOUNT)\n",
        "\n",
        "input_train_data_path = os.path.join(data_converter_output_dir, \"train.tfrecord*\")\n",
        "input_validation_data_path = os.path.join(data_converter_output_dir, \"val.tfrecord*\")\n",
        "label_map_path = os.path.join(data_converter_output_dir, \"label_map.yaml\")\n",
        "print(\"input_train_data_path for training: \", input_train_data_path)\n",
        "print(\"input_validation_data_path for training: \", input_validation_data_path)\n",
        "print(\"label_map_path for prediction: \", label_map_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "um_XKbmpTaHx"
      },
      "outputs": [],
      "source": [
        "# @title Hyperparameter tuning\n",
        "\n",
        "# @markdown Use the Vertex AI SDK to create and run the [hyperparameter tuning job](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview) with Vertex AI Model Garden training docker images.\n",
        "\n",
        "# @markdown #### Define the following specifications\n",
        "\n",
        "# @markdown * `worker_pool_specs`: A list of dictionaries specifying the machine type and docker image. This example defines a single node cluster with one `n1-highmem-32` machine with 8 `NVIDIA_TESLA_V100` GPUs.\n",
        "\n",
        "# @markdown   **Note**: We recommend using 8 GPUs for MoViNet-A2 and larger. Since loading video data requires a lot of GPU memory, it is recommended to experiment with a small batch size first.\n",
        "# @markdown * `parameter_spec`: Dictionary specifying the parameters to optimize. The dictionary key is the string assigned to the command line argument for each hyperparameter in your training application code, and the dictionary value is the parameter specification. The parameter specification includes the type, min/max values, and scale for the hyperparameter.\n",
        "# @markdown * `metric_spec`: Dictionary specifying the metric to optimize. The dictionary key is the `hyperparameter_metric_tag` that you set in your training application code, and the value is the optimization goal.\n",
        "# @markdown * `max_trial_count`: Sets an upper bound on the number of trials the service will run. The recommended practice is to start with a smaller number of trials and get a sense of how impactful your chosen hyperparameters are before scaling up.\n",
        "# @markdown * `parallel_trial_count`:  If you use parallel trials, the service provisions multiple training processing clusters. The worker pool spec that you specify when creating the job is used for each individual training cluster.  Increasing the number of parallel trials reduces the amount of time the hyperparameter tuning job takes to run; however, it can reduce the effectiveness of the job overall. This is because the default tuning strategy uses results of previous trials to inform the assignment of values in subsequent trials.\n",
        "# @markdown * `search_algorithm`: The available search algorithms are grid, random, or default (None). The default option applies Bayesian optimization to search the space of possible hyperparameter values and is the recommended algorithm.\n",
        "\n",
        "# @markdown Click on the generated link in the output to see your run in the Cloud Console.\n",
        "\n",
        "# Input train and validation datasets can be found from the section above\n",
        "# `Prepare input data for training`.\n",
        "# Or, set prepared datasets paths if already exist.\n",
        "# input_train_data_path = \"\"\n",
        "# input_validation_data_path = \"\"\n",
        "# label_map_path = \"\"\n",
        "\n",
        "# Training constants.\n",
        "TRAINING_JOB_PREFIX = \"train\"\n",
        "TRAIN_CONTAINER_URI = f\"{REGION_PREFIX}-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/movinet-train\"\n",
        "TRAIN_MACHINE_TYPE = \"n1-highmem-32\"\n",
        "TRAIN_ACCELERATOR_TYPE = \"NVIDIA_TESLA_V100\"\n",
        "TRAIN_NUM_GPU = 8\n",
        "\n",
        "\n",
        "def get_label_map(label_map_yaml_filepath: str) -> Dict[int, str]:\n",
        "    \"\"\"Returns class id to label mapping given a filepath to the label map.\n",
        "\n",
        "    Args:\n",
        "      label_map_yaml_filepath: A string of label map yaml file path.\n",
        "\n",
        "    Returns:\n",
        "      A dictionary of class id to label mapping.\n",
        "    \"\"\"\n",
        "    label_map_filename = os.path.basename(label_map_yaml_filepath)\n",
        "    subprocess.check_output(\n",
        "        [\"gsutil\", \"cp\", label_map_yaml_filepath, label_map_filename],\n",
        "        stderr=subprocess.STDOUT,\n",
        "    )\n",
        "    with open(label_map_filename, \"rb\") as input_file:\n",
        "        label_map = yaml.safe_load(input_file.read())[\"label_map\"]\n",
        "        num_classes = max(label_map.keys()) + 1\n",
        "        return label_map, num_classes\n",
        "\n",
        "\n",
        "def find_checkpoint_in_dir(directory: str) -> str:\n",
        "    \"\"\"Finds a checkpoint path relative to the directory.\"\"\"\n",
        "    for item in os.listdir(directory):\n",
        "        path = os.path.join(directory, item)\n",
        "        if os.path.isfile(path) and path.endswith(\".index\"):\n",
        "            return (\n",
        "                os.path.dirname(path)\n",
        "                + \"/\"\n",
        "                + os.path.splitext(os.path.basename(path))[0]\n",
        "            )\n",
        "        elif os.path.isdir(path):\n",
        "            result = find_checkpoint_in_dir(path)\n",
        "            if result:\n",
        "                return result\n",
        "    return None\n",
        "\n",
        "\n",
        "def upload_checkpoint_to_gcs(checkpoint_url: str) -> str:\n",
        "    \"\"\"Uploads a compressed .tar.gz checkpoint at the given URL to Cloud Storage.\"\"\"\n",
        "    filename = os.path.basename(checkpoint_url)\n",
        "    checkpoint_name = filename.replace(\".tar.gz\", \"\")\n",
        "    print(\"Download checkpoint from\", checkpoint_url, \"and store to\", CHECKPOINT_BUCKET)\n",
        "    ! wget $checkpoint_url -O $filename\n",
        "    ! mkdir -p $checkpoint_name\n",
        "    ! tar -xvzf $filename -C $checkpoint_name\n",
        "\n",
        "    checkpoint_path = find_checkpoint_in_dir(checkpoint_name)\n",
        "    checkpoint_path = os.path.relpath(checkpoint_path, checkpoint_name)\n",
        "\n",
        "    ! gsutil cp -r $checkpoint_name $CHECKPOINT_BUCKET/\n",
        "    checkpoint_uri = os.path.join(CHECKPOINT_BUCKET, checkpoint_name, checkpoint_path)\n",
        "    print(\"Checkpoint uploaded to\", checkpoint_uri)\n",
        "    return checkpoint_uri\n",
        "\n",
        "\n",
        "def upload_config_to_gcs(url: str) -> str:\n",
        "    \"\"\"Uploads a config file at the given URL to Cloud Storage.\"\"\"\n",
        "    filename = os.path.basename(url)\n",
        "    destination = os.path.join(CONFIG_DIR, filename)\n",
        "    print(\"Copy\", url, \"to\", destination)\n",
        "    ! wget \"$url\" -O \"$filename\"\n",
        "    ! gsutil cp \"$filename\" \"$destination\"\n",
        "    return destination\n",
        "\n",
        "\n",
        "train_job_name = common_util.get_job_name_with_datetime(\n",
        "    f\"{TRAINING_JOB_PREFIX}_{model_name}\"\n",
        ")\n",
        "model_dir = os.path.join(BUCKET_URI, train_job_name)\n",
        "label_map, num_classes = get_label_map(label_map_path)\n",
        "\n",
        "# Uploads pretained checkpoint to GCS bucket.\n",
        "init_checkpoint = f\"https://storage.googleapis.com/tf_model_garden/vision/movinet/{model_name}_with_backbone.tar.gz\"\n",
        "init_checkpoint = upload_checkpoint_to_gcs(init_checkpoint)\n",
        "\n",
        "# Uploads config file according to model_id and streaming options.\n",
        "config_file = f\"{model_id}_stream\" if is_stream else model_id\n",
        "config_file = f\"https://raw.githubusercontent.com/tensorflow/models/master/official/projects/movinet/configs/yaml/movinet_{config_file}_gpu.yaml\"\n",
        "config_file = upload_config_to_gcs(config_file)\n",
        "\n",
        "# The parameters here are mainly for demonstration purpose. Kindly update them\n",
        "# for better performance.\n",
        "trainer_args = {\n",
        "    \"experiment\": \"movinet_kinetics600\",\n",
        "    \"config_file\": config_file,\n",
        "    \"input_train_data_path\": input_train_data_path,\n",
        "    \"input_validation_data_path\": input_validation_data_path,\n",
        "    \"init_checkpoint\": init_checkpoint,\n",
        "    \"model_dir\": model_dir,\n",
        "    \"num_classes\": num_classes,\n",
        "    \"global_batch_size\": 16,\n",
        "    \"prefetch_buffer_size\": 16,\n",
        "    \"shuffle_buffer_size\": 32,\n",
        "    \"train_steps\": 10,\n",
        "}\n",
        "\n",
        "worker_pool_specs = [\n",
        "    {\n",
        "        \"machine_spec\": {\n",
        "            \"machine_type\": TRAIN_MACHINE_TYPE,\n",
        "            \"accelerator_type\": TRAIN_ACCELERATOR_TYPE,\n",
        "            # Each training job uses TRAIN_NUM_GPU GPUs.\n",
        "            \"accelerator_count\": TRAIN_NUM_GPU,\n",
        "        },\n",
        "        \"replica_count\": 1,\n",
        "        \"container_spec\": {\n",
        "            \"image_uri\": TRAIN_CONTAINER_URI,\n",
        "            \"args\": [\n",
        "                \"--mode=train_and_eval\",\n",
        "                \"--params_override=runtime.num_gpus=%d\" % TRAIN_NUM_GPU,\n",
        "            ]\n",
        "            + [\"--{}={}\".format(k, v) for k, v in trainer_args.items()],\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "metric_spec = {\"model_performance\": \"maximize\"}\n",
        "\n",
        "# These learning rates might not be optimal for your selected model type; To\n",
        "# tune learning rates, try hpt.DoubleParameterSpec with more trials.\n",
        "# LEARNING_RATES = [1e-3, 3e-3]\n",
        "LEARNING_RATES = [0.001]\n",
        "MAX_TRIAL_COUNT = len(LEARNING_RATES)\n",
        "parameter_spec = {\n",
        "    \"learning_rate\": hpt.DiscreteParameterSpec(values=LEARNING_RATES, scale=\"linear\"),\n",
        "}\n",
        "\n",
        "print(worker_pool_specs, metric_spec, parameter_spec)\n",
        "\n",
        "common_util.check_quota(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    accelerator_type=TRAIN_ACCELERATOR_TYPE,\n",
        "    accelerator_count=TRAIN_NUM_GPU,\n",
        "    is_for_training=True,\n",
        ")\n",
        "\n",
        "# Add labels for the finetuning job.\n",
        "labels = {\n",
        "    \"mg-source\": \"notebook\",\n",
        "    \"mg-notebook-name\": \"model_garden_movinet_action_recognition.ipynb\".split(\".\")[0],\n",
        "}\n",
        "\n",
        "labels[\"mg-tune\"] = \"publishers-google-models-movinet\"\n",
        "versioned_model_id = model_name.lower().replace(\"_\", \"-\")\n",
        "labels[\"versioned-mg-tune\"] = f\"{labels['mg-tune']}-{versioned_model_id}\"\n",
        "\n",
        "train_custom_job = aiplatform.CustomJob(\n",
        "    display_name=train_job_name,\n",
        "    project=PROJECT_ID,\n",
        "    worker_pool_specs=worker_pool_specs,\n",
        "    staging_bucket=STAGING_BUCKET,\n",
        "    labels=labels,\n",
        ")\n",
        "\n",
        "train_hpt_job = aiplatform.HyperparameterTuningJob(\n",
        "    display_name=train_job_name,\n",
        "    custom_job=train_custom_job,\n",
        "    metric_spec=metric_spec,\n",
        "    parameter_spec=parameter_spec,\n",
        "    max_trial_count=MAX_TRIAL_COUNT,\n",
        "    parallel_trial_count=1,\n",
        "    project=PROJECT_ID,\n",
        "    search_algorithm=None,\n",
        ")\n",
        "\n",
        "train_hpt_job.run(service_account=SERVICE_ACCOUNT)\n",
        "\n",
        "print(\"model_dir is:\", model_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "09Rz1AYspK19"
      },
      "outputs": [],
      "source": [
        "# @title Export model in Tensorflow SavedModel format\n",
        "\n",
        "# This job will export models from TF checkpoints to TF saved model format.\n",
        "# model_dir is from the section above.\n",
        "\n",
        "EVALUATION_METRIC = \"accuracy\"\n",
        "\n",
        "# Export constants.\n",
        "EXPORT_JOB_PREFIX = \"export\"\n",
        "EXPORT_CONTAINER_URI = f\"{REGION_PREFIX}-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/movinet-model-export\"\n",
        "EXPORT_MACHINE_TYPE = \"n1-highmem-8\"\n",
        "\n",
        "\n",
        "def get_best_trial(model_dir, max_trial_count, evaluation_metric):\n",
        "    best_trial_dir = \"\"\n",
        "    best_trial_evaluation_results = {}\n",
        "    best_performance = -1\n",
        "\n",
        "    for i in range(max_trial_count):\n",
        "        current_trial = i + 1\n",
        "        current_trial_dir = os.path.join(model_dir, \"trial_\" + str(current_trial))\n",
        "        current_trial_best_ckpt_dir = os.path.join(current_trial_dir, \"best_ckpt\")\n",
        "        current_trial_best_ckpt_evaluation_filepath = os.path.join(\n",
        "            current_trial_best_ckpt_dir, \"info.json\"\n",
        "        )\n",
        "        ! gsutil cp $current_trial_best_ckpt_evaluation_filepath .\n",
        "        with open(\"info.json\", \"r\") as f:\n",
        "            eval_metric_results = json.load(f)\n",
        "            current_performance = eval_metric_results[evaluation_metric]\n",
        "            if current_performance > best_performance:\n",
        "                best_performance = current_performance\n",
        "                best_trial_dir = current_trial_dir\n",
        "                best_trial_evaluation_results = eval_metric_results\n",
        "    print(\"best_trial_dir: \", current_trial_best_ckpt_evaluation_filepath)\n",
        "    return best_trial_dir, best_trial_evaluation_results\n",
        "\n",
        "\n",
        "def get_best_ckpt(checkpoint_dir: str) -> str:\n",
        "    \"\"\"Finds the best checkpoint path.\"\"\"\n",
        "    try:\n",
        "        checkpoint_files = (\n",
        "            subprocess.check_output([\"gsutil\", \"ls\", checkpoint_dir])\n",
        "            .decode(\"utf-8\")\n",
        "            .strip()\n",
        "        )\n",
        "        for file in checkpoint_files.splitlines():\n",
        "            if file.endswith(\".index\"):\n",
        "                return (\n",
        "                    os.path.dirname(file)\n",
        "                    + \"/\"\n",
        "                    + os.path.splitext(os.path.basename(file))[0]\n",
        "                )\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error listing checkpoints: {e}\")\n",
        "\n",
        "\n",
        "best_trial_dir, best_trial_evaluation_results = get_best_trial(\n",
        "    model_dir, MAX_TRIAL_COUNT, EVALUATION_METRIC\n",
        ")\n",
        "best_checkpoint_path = get_best_ckpt(f\"{best_trial_dir}/best_ckpt/\")\n",
        "print(\"best_trial_dir: \", best_trial_dir)\n",
        "print(\"best_trial_evaluation_results: \", best_trial_evaluation_results)\n",
        "print(\"best_checkpoint: \", best_checkpoint_path)\n",
        "\n",
        "container_args = {\n",
        "    \"export_path\": f\"{model_dir}/best_model\",\n",
        "    \"model_id\": model_id,\n",
        "    \"num_classes\": num_classes,\n",
        "    \"causal\": is_stream,\n",
        "    \"checkpoint_path\": best_checkpoint_path,\n",
        "    \"assert_checkpoint_objects_matched\": False,\n",
        "    **export_container_args,\n",
        "}\n",
        "\n",
        "worker_pool_specs = [\n",
        "    {\n",
        "        \"machine_spec\": {\n",
        "            \"machine_type\": EXPORT_MACHINE_TYPE,\n",
        "        },\n",
        "        \"replica_count\": 1,\n",
        "        \"container_spec\": {\n",
        "            \"image_uri\": EXPORT_CONTAINER_URI,\n",
        "            \"args\": [\"--{}={}\".format(k, v) for k, v in container_args.items()],\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "model_export_job_name = common_util.get_job_name_with_datetime(\n",
        "    EXPORT_JOB_PREFIX + \"_\" + OBJECTIVE\n",
        ")\n",
        "model_export_custom_job = aiplatform.CustomJob(\n",
        "    display_name=model_export_job_name,\n",
        "    project=PROJECT_ID,\n",
        "    worker_pool_specs=worker_pool_specs,\n",
        "    staging_bucket=STAGING_BUCKET,\n",
        ")\n",
        "\n",
        "model_export_custom_job.run(service_account=SERVICE_ACCOUNT)\n",
        "\n",
        "print(\"best model is saved to: \", container_args[\"export_path\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0BGaofgsMsy"
      },
      "source": [
        "## Upload\n",
        "This section shows the way to test with trained models.\n",
        "1. Upload and deploy models to the [Vertex AI Model Registry](https://cloud.google.com/vertex-ai/docs/model-registry/introduction).\n",
        "2. Run batch predictions.\n",
        "\n",
        "**Note:** The prediction container only works with the base model. If you trained a streaming model, download the model from the exported path and refer to the [MoViNet official guide](https://github.com/tensorflow/models/blob/master/official/projects/movinet/movinet_streaming_model_training_and_inference.ipynb) for running predictions locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NYuQowyZEtxK"
      },
      "outputs": [],
      "source": [
        "# @title Upload model to Vertex AI Model Registry\n",
        "\n",
        "# @markdown The following cell uploads the trained model to Vertex AI Model Registry. Skip it if you want to run batch predictions on an already uploaded model instead.\n",
        "\n",
        "# @markdown  **Configurable environment variables**\n",
        "# @markdown *  `MODEL_PATH`: Cloud Storage URI to the MoViNet model.\n",
        "# @markdown * `BATCH_SIZE`: Batch size for inference. Use a larger value to accelerate GPU prediction.\n",
        "# @markdown * `NUM_FRAMES`: Number of frames for a single prediction with the model.\n",
        "# @markdown * `FPS`: Video sampling frame per second.\n",
        "# @markdown * `OVERLAP_FRAMES`: Allowed overlapping frames between consecutive prediction windows. Set a smaller value for faster inference but less accurate.\n",
        "\n",
        "\n",
        "# Prediction constants.\n",
        "# You can adjust accelerator types and machine types to get faster predictions.\n",
        "PREDICTION_CONTAINER_URI = f\"{REGION_PREFIX}-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/movinet-serve\"\n",
        "PREDICTION_PORT = 8080\n",
        "PREDICTION_ACCELERATOR_COUNT = 1\n",
        "PREDICTION_ACCELERATOR_TYPE = \"NVIDIA_TESLA_T4\"\n",
        "PREDICTION_MACHINE_TYPE = \"n1-standard-4\"\n",
        "PREDICTION_JOB_PREFIX = \"predict\"\n",
        "\n",
        "serving_env = {\n",
        "    \"MODEL_ID\": \"tfvision-movinet-var\",\n",
        "    \"MODEL_PATH\": container_args[\"export_path\"],\n",
        "    \"BATCH_SIZE\": 1,\n",
        "    \"NUM_FRAMES\": 32,\n",
        "    \"FPS\": output_fps,\n",
        "    \"OVERLAP_FRAMES\": 24,\n",
        "    \"OBJECTIVE\": OBJECTIVE,\n",
        "    \"IMAGE_WIDTH\": image_size,\n",
        "    \"IMAGE_HEIGHT\": image_size,\n",
        "    \"DEPLOY_SOURCE\": \"notebook\",\n",
        "}\n",
        "\n",
        "models[\"model_var\"] = aiplatform.Model.upload(\n",
        "    display_name=model_name,\n",
        "    serving_container_image_uri=PREDICTION_CONTAINER_URI,\n",
        "    serving_container_ports=[PREDICTION_PORT],\n",
        "    serving_container_predict_route=\"/predict\",\n",
        "    serving_container_health_route=\"/ping\",\n",
        "    serving_container_environment_variables=serving_env,\n",
        ")\n",
        "\n",
        "models[\"model_var\"].wait()\n",
        "\n",
        "print(\"The uploaded model name is: \", model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SZsKGeS3x6S"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vbIW9me1F2RY"
      },
      "outputs": [],
      "source": [
        "# @title Run batch predictions\n",
        "\n",
        "# @markdown We will now run batch predictions with the trained MoViNet action recognition model with [Vertex AI Batch Prediction](https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions).\n",
        "\n",
        "# @markdown Prepare an input JSONL file where each line follows [this format](https://cloud.google.com/vertex-ai/docs/video-data/action-recognition/get-predictions?hl=en#input_data_requirements) and store it in a Cloud Storage bucket. The service account should have read access to the buckets containing the trained model and the input data. See [Service accounts overview](https://cloud.google.com/iam/docs/service-account-overview) for more information.\n",
        "\n",
        "# @markdown The [Vertex AI Batch Prediction](https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions) has a default timeout of 10 minutes. Therefore, make sure the input video clip is around 5 minutes at 5~10 FPS or you may experience a timeout error.\n",
        "\n",
        "# @markdown To use this model at a larger scale beyond this notebook demontration, you can try one of the following:\n",
        "# @markdown - Pull the serving docker image to a VM or a local machine and send prediction requests directly.\n",
        "# @markdown - To process more data concurrently, write a custom [DataFlow](https://cloud.google.com/dataflow) pipeline to send prediction requests to the movinet serving container.\n",
        "# @markdown - Divide videos into 5-minute clips and run batch prediction with a small batch size.\n",
        "\n",
        "# Path to the prediction input JSONL file.\n",
        "test_jsonl_path = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "predict_job_name = common_util.get_job_name_with_datetime(\n",
        "    f\"{PREDICTION_JOB_PREFIX}_{model_name}\"\n",
        ")\n",
        "predict_destination_prefix = os.path.join(STAGING_BUCKET, predict_job_name)\n",
        "\n",
        "batch_prediction_job = models[\"model_var\"].batch_predict(\n",
        "    job_display_name=predict_job_name,\n",
        "    gcs_source=test_jsonl_path,\n",
        "    gcs_destination_prefix=predict_destination_prefix,\n",
        "    machine_type=PREDICTION_MACHINE_TYPE,\n",
        "    accelerator_count=PREDICTION_ACCELERATOR_COUNT,\n",
        "    accelerator_type=PREDICTION_ACCELERATOR_TYPE,\n",
        "    max_replica_count=1,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        ")\n",
        "\n",
        "batch_prediction_job.wait()\n",
        "\n",
        "print(batch_prediction_job.display_name)\n",
        "print(batch_prediction_job.resource_name)\n",
        "print(batch_prediction_job.state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tdkW9e5B9OU1"
      },
      "outputs": [],
      "source": [
        "# @title Read the prediction response\n",
        "\n",
        "\n",
        "def print_response_instance(json_str: str, label_map: dict[int, str]):\n",
        "    \"\"\"Prints summary of a prediction JSON result from the model response.\"\"\"\n",
        "    json_obj = json.loads(json_str)\n",
        "    if \"prediction\" not in json_obj:\n",
        "        print(\"Error:\", json_str)\n",
        "        return\n",
        "    instance = json_obj[\"instance\"]\n",
        "    prediction = json_obj[\"prediction\"]\n",
        "    gcs_uri = instance[\"content\"]\n",
        "    time_start = instance.get(\"timeSegmentStart\", \"0.0s\")\n",
        "    time_end = instance.get(\"timeSegmentEnd\", \"Infinity\")\n",
        "    print(f\"---------- Predict {gcs_uri}, {time_start} to {time_end}:\")\n",
        "    for predicted in prediction:\n",
        "        time = predicted[\"timeSegmentStart\"]\n",
        "        label = label_map[predicted[\"label\"]]\n",
        "        confidence = predicted[\"confidence\"]\n",
        "        print(f\"At {time}, detected {label} with {confidence} confidence.\")\n",
        "\n",
        "\n",
        "# The label map file was generated from the section above (`Prepare input data for training`).\n",
        "\n",
        "dir_name = os.path.basename(predict_destination_prefix)\n",
        "! gsutil -m cp -R $predict_destination_prefix /tmp\n",
        "\n",
        "local_path = os.path.join(\"/tmp\", dir_name)\n",
        "file_paths = []\n",
        "for root, _, files in os.walk(local_path):\n",
        "    for file in files:\n",
        "        file_path = os.path.join(root, file)\n",
        "        file_paths.append(file_path)\n",
        "\n",
        "for file in file_paths:\n",
        "    with open(file, \"r\") as f:\n",
        "        for line in f:\n",
        "            print_response_instance(line, label_map)\n",
        "\n",
        "! rm -rf /tmp/$dir_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkH2nrpdp4sp"
      },
      "source": [
        "## Clean up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Ax6vQVZhp9pR"
      },
      "outputs": [],
      "source": [
        "# @title Clean up training jobs, models, endpoints and buckets\n",
        "\n",
        "# Delete custom and hpt jobs.\n",
        "if data_converter_custom_job.list(filter=f'display_name=\"{data_converter_job_name}\"'):\n",
        "    data_converter_custom_job.delete()\n",
        "if train_hpt_job.list(filter=f'display_name=\"{train_job_name}\"'):\n",
        "    train_hpt_job.delete()\n",
        "if model_export_custom_job.list(filter=f'display_name=\"{model_export_job_name}\"'):\n",
        "    model_export_custom_job.delete()\n",
        "if batch_prediction_job.list(filter=f'display_name=\"{predict_job_name}\"'):\n",
        "    batch_prediction_job.delete()\n",
        "\n",
        "# Delete models.\n",
        "for model in models.values():\n",
        "    model.delete()\n",
        "\n",
        "# @markdown Delete temporary GCS buckets.\n",
        "\n",
        "delete_bucket = False  # @param {type:\"boolean\"}\n",
        "if delete_bucket:\n",
        "    ! gsutil -m rm -r $BUCKET_NAME"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_movinet_action_recognition.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

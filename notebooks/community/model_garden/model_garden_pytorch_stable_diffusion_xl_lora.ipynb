{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d9bbf86da5e"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Vertex AI Model Garden - Stable Diffusion XL 1.0 - LoRA serving\n",
        "\n",
        "<table align=\"left\"><tbody><tr>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_pytorch_stable_diffusion_xl_lora.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_stable_diffusion_xl_lora.ipynb\">\n",
        "      <img src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" alt=\"GitHub logo\"><br>\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates how to download the popular LoRA (Low-Rank Adaptation) adapters from huggingface.co or civitai.com, and serve it together with the [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) model on Vertex AI for online prediction.\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Deploy the base model and the LoRA adapter to a [Vertex AI Endpoint resource](https://cloud.google.com/vertex-ai/docs/predictions/using-private-endpoints).\n",
        "- Run online predictions for text-to-image.\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "264c07757582"
      },
      "source": [
        "## Run the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2707b02ef5df"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown 2. **[Optional]** [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs. Set the BUCKET_URI for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`) should be located in the same region as where the notebook was launched. Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\"). If not set, a unique GCS bucket will be created instead.\n",
        "\n",
        "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown 3. **[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment.\n",
        "\n",
        "REGION = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown 4. If you want to run predictions with A100 80GB or H100 GPUs, we recommend using the regions listed below. **NOTE:** Make sure you have associated quota in selected regions. Click the links to see your current quota for each GPU type: [Nvidia A100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_a100_80gb_gpus), [Nvidia H100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_h100_gpus).\n",
        "\n",
        "# @markdown > | Machine Type | Accelerator Type | Recommended Regions |\n",
        "# @markdown | ----------- | ----------- | ----------- |\n",
        "# @markdown | a2-ultragpu-1g | 1 NVIDIA_A100_80GB | us-central1, us-east4, europe-west4, asia-southeast1, us-east4 |\n",
        "# @markdown | a3-highgpu-2g | 2 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "# @markdown | a3-highgpu-4g | 4 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "# @markdown | a3-highgpu-8g | 8 NVIDIA_H100_80GB | us-central1, us-east5, europe-west4, us-west1, asia-southeast1 |\n",
        "\n",
        "import base64\n",
        "import glob\n",
        "import importlib\n",
        "import os\n",
        "import sys\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "from io import BytesIO\n",
        "\n",
        "from google.cloud import aiplatform, storage\n",
        "from PIL import Image\n",
        "\n",
        "! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
        "\n",
        "common_util = importlib.import_module(\n",
        "    \"vertex-ai-samples.community-content.vertex_model_garden.model_oss.notebook_util.common_util\"\n",
        ")\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "if not REGION:\n",
        "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
        "\n",
        "# Cloud Storage bucket for storing the experiment artifacts.\n",
        "# If a custom gcs bucket uri is not provided, a unique GCS bucket will be\n",
        "# created for the purpose of this notebook.\n",
        "now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "\n",
        "# Create a unique GCS bucket for this notebook, if not specified by the user.\n",
        "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
        "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}-{str(uuid.uuid4())[:4]}\"\n",
        "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
        "else:\n",
        "    assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
        "    shell_output = ! gsutil ls -Lb {BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
        "    bucket_region = shell_output[0].strip().lower()\n",
        "    if bucket_region != REGION:\n",
        "        raise ValueError(\n",
        "            \"Bucket region %s is different from notebook region %s\"\n",
        "            % (bucket_region, REGION)\n",
        "        )\n",
        "\n",
        "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
        "\n",
        "# Set up the default SERVICE_ACCOUNT.\n",
        "SERVICE_ACCOUNT = None\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "\n",
        "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
        "\n",
        "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user(project_id=PROJECT_ID)\n",
        "\n",
        "# The pre-built serving docker image. It contains serving scripts and models.\n",
        "SERVE_DOCKER_URI = \"us-docker.pkg.dev/deeplearning-platform-release/vertex-model-garden/pytorch-inference.cu125.0-4.ubuntu2204.py310\"\n",
        "\n",
        "\n",
        "# Define common functions.\n",
        "def base64_to_image(image_str):\n",
        "    \"\"\"Convert base64 encoded string to an image.\"\"\"\n",
        "    image = Image.open(BytesIO(base64.b64decode(image_str)))\n",
        "    return image\n",
        "\n",
        "\n",
        "def image_grid(imgs, rows=2, cols=2):\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new(\n",
        "        mode=\"RGB\", size=(cols * w + 10 * cols, rows * h), color=(255, 255, 255)\n",
        "    )\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i % cols * w + 10 * i, i // cols * h))\n",
        "    return grid\n",
        "\n",
        "\n",
        "def deploy_model(\n",
        "    model_id, lora_id, accelerator_type, machine_type, accelerator_count=1\n",
        "):\n",
        "    common_util.check_quota(\n",
        "        project_id=PROJECT_ID,\n",
        "        region=REGION,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        is_for_training=False,\n",
        "    )\n",
        "\n",
        "    model_name = model_id\n",
        "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
        "    serving_env = {\n",
        "        \"MODEL_ID\": model_id,\n",
        "        \"LORA_ID\": lora_id,\n",
        "        \"TASK\": \"text-to-image-sdxl\",\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "    }\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=SERVE_DOCKER_URI,\n",
        "        serving_container_ports=[7080],\n",
        "        serving_container_predict_route=\"/predict\",\n",
        "        serving_container_health_route=\"/health\",\n",
        "        serving_container_environment_variables=serving_env,\n",
        "        model_garden_source_model_name=\"publishers/stability-ai/models/stable-diffusion-xl-base\",\n",
        "    )\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=SERVICE_ACCOUNT,\n",
        "        system_labels={\n",
        "            \"NOTEBOOK_NAME\": \"model_garden_pytorch_stable_diffusion_xl_lora.ipynb\"\n",
        "        },\n",
        "    )\n",
        "    return model, endpoint\n",
        "\n",
        "\n",
        "def get_bucket_and_blob_name(filepath):\n",
        "    # The gcs path is of the form gs://<bucket-name>/<blob-name>\n",
        "    gs_suffix = filepath.split(\"gs://\", 1)[1]\n",
        "    return tuple(gs_suffix.split(\"/\", 1))\n",
        "\n",
        "\n",
        "def upload_local_dir_to_gcs(local_dir_path, gcs_dir_path):\n",
        "    \"\"\"Uploads files in a local directory to a GCS directory.\"\"\"\n",
        "    client = storage.Client()\n",
        "    bucket_name = gcs_dir_path.split(\"/\")[2]\n",
        "    bucket = client.get_bucket(bucket_name)\n",
        "    for local_file in glob.glob(local_dir_path + \"/**\"):\n",
        "        if not os.path.isfile(local_file):\n",
        "            continue\n",
        "        filename = local_file[1 + len(local_dir_path) :]\n",
        "        gcs_file_path = os.path.join(gcs_dir_path, filename)\n",
        "        _, blob_name = get_bucket_and_blob_name(gcs_file_path)\n",
        "        blob = bucket.blob(blob_name)\n",
        "        blob.upload_from_filename(local_file)\n",
        "        print(\"Copied {} to {}.\".format(local_file, gcs_file_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "a89f8e7ea577"
      },
      "outputs": [],
      "source": [
        "# @title Upload and deploy model\n",
        "\n",
        "# @markdown This section uploads the model to Model Registry and deploys it to a Vertex AI Endpoint resource. The model deployment step will take ~30 minutes to complete.\n",
        "\n",
        "# @markdown In this example, we deploy the [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stable-diffusion-xl-base-1.0) model together with a LoRA.\n",
        "\n",
        "# @markdown Select one of the two preset LoRA options, or use your own LoRA.\n",
        "# @markdown * `Huggingface`: [TheLastBen/Papercut_SDXL](https://huggingface.co/TheLastBen/Papercut_SDXL)\n",
        "# @markdown * `Civitai`: [Pixel Art XL](https://civitai.com/models/120096/pixel-art-xl). This example downloads LoRA from Civitai, and uploads it to GCS bucket.\n",
        "# @markdown * `Use your own`: Set your own lora in the next section.\n",
        "\n",
        "PRESET_LORA = \"Huggingface\"  # @param [\"Huggingface\", \"Civitai\", \"Use your own\"]\n",
        "\n",
        "# @markdown [Optional] If you selected `Use your own`, set your lora source in `CUSTOM_LORA`. It can be one of the following:\n",
        "# @markdown * A [Hugging Face](https://huggingface.co) lora id\n",
        "# @markdown * A GCS uri (starting with \"gs://\")\n",
        "# @markdown * An http uri. In this case, we'll download the lora and upload it to GCS bucket.\n",
        "\n",
        "CUSTOM_LORA = \"\"  # @param {type: \"string\"}\n",
        "\n",
        "accelerator_type = \"NVIDIA_L4\"  # @param [\"NVIDIA_L4\", \"NVIDIA_A100_80GB\"]\n",
        "machine_type_map = {\n",
        "    \"NVIDIA_L4\": \"g2-standard-8\",\n",
        "    \"NVIDIA_A100_80GB\": \"a2-ultragpu-1g\",\n",
        "}\n",
        "\n",
        "if PRESET_LORA != \"Use your own\" and CUSTOM_LORA != \"\":\n",
        "    print(\n",
        "        f'Warning: PRESET_LORA [{PRESET_LORA}] is selected. CUSTOM_LORA will not be used.\\nIf you want to use CUSTOM_LORA, select \"Use your own\" in PRESET_LORA.\\n'\n",
        "    )\n",
        "\n",
        "if PRESET_LORA == \"Huggingface\":\n",
        "    lora_source = \"TheLastBen/Papercut_SDXL\"\n",
        "elif PRESET_LORA == \"Civitai\":\n",
        "    lora_source = \"https://civitai.com/api/download/models/135931\"\n",
        "else:\n",
        "    lora_source = CUSTOM_LORA\n",
        "\n",
        "if lora_source.startswith(\"http\"):\n",
        "    # Download a LoRA adapter first and save it to a GCS bucket.\n",
        "    ! rm -r /tmp/custom-lora\n",
        "    ! mkdir /tmp/custom-lora\n",
        "\n",
        "    destination_folder = \"/tmp/custom-lora\"\n",
        "    file_name = \"custom-lora.safetensors\"\n",
        "\n",
        "    target = f\"{destination_folder}/{file_name}\"\n",
        "\n",
        "    !gdown --fuzzy -O $target \"$lora_source\"\n",
        "    upload_local_dir_to_gcs(\"/tmp/custom-lora\", f\"{BUCKET_URI}/custom-lora\")\n",
        "    lora_id = f\"{BUCKET_URI}/custom-lora\"\n",
        "else:\n",
        "    # Juggingface id and gs uri can be used directly as `lora_id`\n",
        "    lora_id = lora_source\n",
        "\n",
        "model, endpoint = deploy_model(\n",
        "    model_id=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    lora_id=lora_id,\n",
        "    accelerator_type=accelerator_type,\n",
        "    machine_type=machine_type_map[accelerator_type],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2ee26660f87c"
      },
      "outputs": [],
      "source": [
        "# @title Predict\n",
        "\n",
        "# @markdown Once deployment succeeds, you can send a batch of text prompts to the endpoint to generated images.\n",
        "\n",
        "# @markdown **Note:** Some LoRA adaptors require explicit prompt to activate. For\n",
        "# @markdown example, the preset Hugging Face [TheLastBen/Papercut_SDXL](https://huggingface.co/TheLastBen/Papercut_SDXL)\n",
        "# @markdown Lora requires an explicit prompt `\"papercut\"`. In this example,\n",
        "# @markdown we'll automatically append the prompt for you.\n",
        "\n",
        "# @markdown Example:\n",
        "\n",
        "# @markdown ```\n",
        "# @markdown Prompt: A serious capybara at work, wearing a suit\n",
        "# @markdown ```\n",
        "\n",
        "# @markdown You may adjust the parameters below to achieve best image quality.\n",
        "\n",
        "prompt = \"A serious capybara at work, wearing a suit\"  # @param {type: \"string\"}\n",
        "negative_prompt = \"\"  # @param {type: \"string\"}\n",
        "height = 1024  # @param {type:\"integer\"}\n",
        "width = 1024  # @param {type:\"number\"}\n",
        "num_inference_steps = 25  # @param {type:\"number\"}\n",
        "guidance_scale = 2.5  # @param {type:\"number\"}\n",
        "\n",
        "if PRESET_LORA == \"Huggingface\" and \"papercut\" not in prompt:\n",
        "    prompt = \"papercut \" + prompt\n",
        "    print('Adding \"papercut\" to prompt')\n",
        "    print(\"prompt:\", prompt)\n",
        "\n",
        "instances = [{\"text\": prompt}]\n",
        "parameters = {\n",
        "    \"negative_prompt\": negative_prompt,\n",
        "    \"height\": height,\n",
        "    \"width\": width,\n",
        "    \"num_inference_steps\": num_inference_steps,\n",
        "    \"guidance_scale\": guidance_scale,\n",
        "}\n",
        "\n",
        "response = endpoint.predict(instances=instances, parameters=parameters)\n",
        "images = [\n",
        "    base64_to_image(prediction.get(\"output\")) for prediction in response.predictions\n",
        "]\n",
        "image_grid(images, rows=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "64050bd0158e"
      },
      "outputs": [],
      "source": [
        "# @title Clean up resources\n",
        "\n",
        "# @markdown  Delete the experiment models and endpoints to recycle the resources\n",
        "# @markdown  and avoid unnecessary continouous charges that may incur.\n",
        "\n",
        "try:\n",
        "    # Undeploy model and delete endpoint.\n",
        "    endpoint.delete(force=True)\n",
        "\n",
        "    # Delete model.\n",
        "    model.delete()\n",
        "\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "# Delete bucket.\n",
        "delete_bucket = False  # @param {type:\"boolean\"}\n",
        "if delete_bucket:\n",
        "    ! gsutil -m rm -r $BUCKET_NAME"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_pytorch_stable_diffusion_xl_lora.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJc36RtD90jd"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9EezHSo90jf"
      },
      "source": [
        "# Vertex AI Model Garden - Mixtral-8x7B (PEFT)\n",
        "\n",
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_mixtral_peft_tuning.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_mixtral_peft_tuning.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_pytorch_mixtral_peft_tuning.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>                                                                                               \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybMCVFh0_5R8"
      },
      "source": [
        "## Overview\n",
        "In this notebook you will learn how to fine tune Mixtral-8x7B with QLoRa.\n",
        "\n",
        "### Objective\n",
        "\n",
        "*   Fine tune and merge Mixtral-8x7B model with peft train docker image (maintained by Vertex AI Model Garden). Optionally run Hyperparameter tuning to find the best parameters.\n",
        "*   Deploy the finetuned model with vLLM docker image on a Vertex AI Endpoint\n",
        "*   Run inference on the deployed Vertex AI Endpoint\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoTMt4AXgYhr"
      },
      "source": [
        "## Step 0 - Initiatialise the notebook\n",
        "### Define some helper functions and variables:\n",
        "\n",
        "0. Define some variables and APIs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "I-OjzhpyMHsu"
      },
      "outputs": [],
      "source": [
        "# Cloud project id.\n",
        "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# The region you want to launch jobs in.\n",
        "REGION = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# The Cloud Storage bucket for storing experiments output.\n",
        "# Start with gs:// prefix, e.g. gs://foo_bucket.\n",
        "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
        "\n",
        "! gcloud config set project $PROJECT_ID\n",
        "! gcloud services enable language.googleapis.com\n",
        "\n",
        "import os\n",
        "\n",
        "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
        "EXPERIMENT_BUCKET = os.path.join(BUCKET_URI, \"peft\")\n",
        "DATA_BUCKET = os.path.join(EXPERIMENT_BUCKET, \"data\")\n",
        "MODEL_BUCKET = os.path.join(EXPERIMENT_BUCKET, \"model\")\n",
        "\n",
        "# The service account looks like:\n",
        "# '@.iam.gserviceaccount.com'\n",
        "# Please go to https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console\n",
        "# and create service account with `Vertex AI User` and `Storage Object Admin` roles.\n",
        "# The service account for deploying fine tuned model.\n",
        "SERVICE_ACCOUNT = \"\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgNendbgjSJ6"
      },
      "source": [
        "### Installation: *Vertex AI API*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mj-SxEcNjQNN"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okocadG3huSB"
      },
      "source": [
        "### Define *constants*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KsIcP7nqh1FL"
      },
      "outputs": [],
      "source": [
        "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240112_0916_RC00\"\n",
        "TRAIN_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-train:20240126_0936_RC00\"\n",
        "VLLM_GPTQ_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:gptq\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arrkM1d69rGK"
      },
      "source": [
        "### Define common functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qoa8GXFBiJ6I"
      },
      "source": [
        "1. Define model deployment functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Qxd5gsGhiKZ4"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "from typing import Tuple\n",
        "\n",
        "\n",
        "def create_name_with_datetime(prefix: str) -> str:\n",
        "    \"\"\"Creates a name with date time when triggering training or deployment\n",
        "    jobs in Vertex AI.\n",
        "    \"\"\"\n",
        "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
        "\n",
        "\n",
        "def deploy_model_vllm(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    service_account: str,\n",
        "    machine_type: str = \"n1-standard-8\",\n",
        "    accelerator_type: str = \"NVIDIA_TESLA_V100\",\n",
        "    accelerator_count: int = 1,\n",
        "    quantization_method: str = \"\",\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
        "\n",
        "    vllm_args = [\n",
        "        \"--host=0.0.0.0\",\n",
        "        \"--port=7080\",\n",
        "        f\"--model={model_id}\",\n",
        "        f\"--tensor-parallel-size={accelerator_count}\",\n",
        "        \"--swap-space=16\",\n",
        "        \"--gpu-memory-utilization=0.9\",\n",
        "        \"--max-num-batched-tokens=4096\",\n",
        "        \"--disable-log-stats\",\n",
        "    ]\n",
        "    if quantization_method:\n",
        "        vllm_args.append(f\"--quantization={quantization_method}\")\n",
        "    if quantization_method == \"gptq\":\n",
        "        vllm_docker_uri = VLLM_GPTQ_DOCKER_URI\n",
        "    else:\n",
        "        vllm_docker_uri = VLLM_DOCKER_URI\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=vllm_docker_uri,\n",
        "        serving_container_command=[\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
        "        serving_container_args=vllm_args,\n",
        "        serving_container_ports=[7080],\n",
        "        serving_container_predict_route=\"/generate\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "    )\n",
        "\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=service_account,\n",
        "    )\n",
        "    return model, endpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m06rH8cTrZof"
      },
      "source": [
        "## Step 1 - Fine tune Mixtral-8x7B model with PEFT\n",
        "This section demonstrates how to finetune the Mixtral-8x7B model, merge the finetuned LoRA adapter with the base model on Vertex AI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-131f-kFncq6"
      },
      "source": [
        "Set the base model id."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "89NX4-1jniGJ"
      },
      "outputs": [],
      "source": [
        "base_model_id = \"mistralai/Mixtral-8x7B\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIdjRx6tnyZz"
      },
      "source": [
        "### Finetune\n",
        "\n",
        "Use the Vertex AI SDK to create and run the custom training jobs with Vertex AI Model Garden training images.\n",
        "\n",
        "This example uses the dataset fredmo/vertexai-qna-500 , a small dataset containing questions and answers about GCP Vertex AI Documentation. You can either use a dataset from huggingface or a custom JSONL dataset in Vertex AI text model dataset format stored in Cloud Storage. The template parameter is optional.\n",
        "\n",
        "In order to make the finetuning efficient, you enabled quantization for loading pretrained models for finetuning LoRA models. Precision options include \"4bit\", \"8bit\", \"float16\" (default) and \"float32\", and the precision can be set via \"--precision_mode\".\n",
        "\n",
        "In this section, the finetuned LoRA adapter will be saved to a GCS bucket specified by the variable lora_adapter_dir below; and you merge the LoRa adapter with the base model, and save it to a separate GCS bucket specified by merged_model_output_dir below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paBvlvREn014"
      },
      "source": [
        "### Finetune with a custom dataset\n",
        "\n",
        "To use a custom dataset, you should supply a gs:// URI to a JSONL file in Vertex text model dataset format in the dataset_name below.\n",
        "\n",
        "For example, you can download the template file here https://github.com/thomaslemoullec/QLora_vLLM_Mistral7B/blob/main/vertexAI_q%26a_template.json and upload it to your bucket, then reference the gs:// URI.\n",
        "\n",
        "{\"input_text\":\"TRANSCRIPT: \\nREASON FOR EVALUATION:,\\n\\n LABEL:\",\"output_text\":\"Chiropractic\"}\n",
        "\n",
        "To use this sample dataset that contains input_text and output_text fields, set dataset_name to gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl and template to vertex_sample. For advanced usage with custom datatset fields, see the template example and supply your own JSON template as gs:// URIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "B7GUQgq5pkF0"
      },
      "outputs": [],
      "source": [
        "# Huggingface dataset name or gs:// URI to a custom JSONL dataset.\n",
        "dataset_name = \"fredmo/vertexai-qna-500\"  # @param {type:\"string\"}\n",
        "# Optional. Template name or gs:// URI to a custom template.\n",
        "template = \"vertex_sample\"  # @param {type:\"string\"}\n",
        "# Runs 10 training steps as a minimal example.\n",
        "max_steps = 10  # @param {type:\"integer\"}\n",
        "\n",
        "finetuning_precision_mode = \"4bit\"  # @param {type:\"string\"}\n",
        "\n",
        "# Worker pool spec for 4bit finetuning.\n",
        "# Finetunes Mixtral-8x7B with 4 V100 (16G).\n",
        "machine_type = \"n1-highmem-32\"\n",
        "accelerator_type = \"NVIDIA_TESLA_V100\"\n",
        "accelerator_count = 4\n",
        "\n",
        "# Finetunes Mixtral-8x7B with 2 L4 (24G).\n",
        "# machine_type = \"g2-standard-24\"\n",
        "# accelerator_type = \"NVIDIA_L4\"\n",
        "# accelerator_count = 2\n",
        "\n",
        "replica_count = 1\n",
        "\n",
        "\n",
        "# Setup training job.\n",
        "job_name = create_name_with_datetime(\"mixtral-lora-train\")\n",
        "train_job = aiplatform.CustomContainerTrainingJob(\n",
        "    display_name=job_name,\n",
        "    container_uri=TRAIN_DOCKER_URI,\n",
        ")\n",
        "\n",
        "# Create a GCS folder to store the LORA adapter.\n",
        "lora_adapter_dir = create_name_with_datetime(\"mixtral-lora-adapter\")\n",
        "lora_output_dir = os.path.join(MODEL_BUCKET, lora_adapter_dir)\n",
        "\n",
        "# Create a GCS folder to store the merged model with the base model and the\n",
        "# finetuned LORA adapter.\n",
        "merged_model_dir = create_name_with_datetime(\"mixtral-merged-model\")\n",
        "merged_model_output_dir = os.path.join(MODEL_BUCKET, merged_model_dir)\n",
        "\n",
        "# Pass training arguments and launch job.\n",
        "train_job.run(\n",
        "    args=[\n",
        "        \"--task=causal-language-modeling-lora\",\n",
        "        f\"--pretrained_model_id={base_model_id}\",\n",
        "        f\"--dataset_name={dataset_name}\",\n",
        "        f\"--output_dir={lora_output_dir}\",\n",
        "        f\"--merge_base_and_lora_output_dir={merged_model_output_dir}\",\n",
        "        \"--lora_rank=16\",\n",
        "        \"--lora_alpha=32\",\n",
        "        \"--lora_dropout=0.05\",\n",
        "        \"--warmup_steps=10\",\n",
        "        f\"--max_steps={max_steps}\",\n",
        "        \"--learning_rate=2e-4\",\n",
        "        f\"--precision_mode={finetuning_precision_mode}\",\n",
        "        f\"--template={template}\",\n",
        "    ],\n",
        "    replica_count=replica_count,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    boot_disk_size_gb=500,\n",
        ")\n",
        "\n",
        "print(\"The finetuned Lora adapter can be found at: \", lora_output_dir)\n",
        "print(\n",
        "    \"The finetuned Lora adapter merged with the base model can be found at: \",\n",
        "    merged_model_output_dir,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9vKI_xAqh7W"
      },
      "source": [
        "## Step 2 - Deploy the finetuned model with vLLM docker image\n",
        "\n",
        "This section uploads the model to Model Registry and deploys it on the Endpoint.\n",
        "The model deployment step will take ~15 minutes to complete.\n",
        "\n",
        "\n",
        "NOTE: vLLM requires a merged model with the base model and the finetuned LoRA adapter. Based on your business need, if you need the base model and the finetuned LoRA weight to be served separately, please consider using the regular Vertex AI serving instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GyDWPdV1NjMT"
      },
      "outputs": [],
      "source": [
        "# Finds Vertex AI prediction supported accelerators and regions in\n",
        "# https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
        "\n",
        "# Sets L4 to deploy Mixtral-8x7B.\n",
        "machine_type = \"g2-standard-96\"\n",
        "accelerator_type = \"NVIDIA_L4\"\n",
        "accelerator_count = 8\n",
        "\n",
        "model_with_peft, endpoint_with_peft = deploy_model_vllm(\n",
        "    model_name=create_name_with_datetime(prefix=\"mixtral-peft-serve-vllm\"),\n",
        "    model_id=merged_model_output_dir,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        ")\n",
        "\n",
        "print(\"endpoint_name:\", endpoint_with_peft.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki-m4hzHFqTu"
      },
      "source": [
        "## Step 3 Run inference to evaluate the finetuned model (vLLM Vertex endpoint)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4v2Mnui4tH1X"
      },
      "outputs": [],
      "source": [
        "instance = {\n",
        "    \"prompt\": \"What is Model Garden?\",\n",
        "    \"n\": 1,\n",
        "    \"max_tokens\": 250,\n",
        "    \"temperature\": 1.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"top_k\": 10,\n",
        "}\n",
        "response = endpoint_with_peft.predict(instances=[instance])\n",
        "print(response.predictions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsyIG4vl66o0"
      },
      "source": [
        "## Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9EMCOUJ6-ji"
      },
      "outputs": [],
      "source": [
        "# Undeploy models and delete endpoints.\n",
        "endpoint_with_peft.delete(force=True)\n",
        "\n",
        "# Delete models.\n",
        "model_with_peft.delete()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "model_garden_pytorch_mixtral_peft_tuning.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

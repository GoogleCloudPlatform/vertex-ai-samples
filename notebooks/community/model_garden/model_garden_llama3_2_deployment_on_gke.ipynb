{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7d9bbf86da5e"
      },
      "outputs": [],
      "source": [
        "# Copyright 2026 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Llama 3.2 deployment to GKE using GPU\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_llama3_2_deployment_on_gke.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_llama3_2_deployment_on_gke.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "This notebook demonstrates downloading, deploying, and serving prebuilt Llama 3.2 models on GPU Using GKE. The models uses Virtual Large Language Model [vLLM](https://developers.googleblog.com/en/inference-with-gemma-using-dataflow-and-vllm/#:~:text=model%20frameworks%20simple.-,What%20is%20vLLM%3F,-vLLM%20is%20an) inference server.\n",
        "\n",
        "\n",
        "\n",
        "### Objective\n",
        "\n",
        "Deploy and run inference for serving Llama 3.2 on GPUs.\n",
        "\n",
        "### GPUs\n",
        "\n",
        "GPUs let you accelerate specific workloads running on your nodes such as machine learning and data processing. GKE provides a range of machine type options for node configuration, including machine types with NVIDIA H100, L4, and A100 GPUs.\n",
        "\n",
        "Before you use GPUs in GKE, we recommend that you complete the following learning path:\n",
        "\n",
        "- Learn about [current GPU version availability](https://cloud.google.com/compute/docs/gpus)\n",
        "\n",
        "- Learn about [GPUs in GKE](https://cloud.google.com/kubernetes-engine/docs/concepts/gpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "264c07757582"
      },
      "source": [
        "## Run the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "855d6b96f291"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown 3. **[Optional]** Set `CLUSTER_NAME` if you want to use your own GKE cluster. If not set, this example will create a auto-pilot cluster in the specified project.\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "# Get the default cloud project id.\n",
        "default_project = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "PROJECT_ID = default_project  # @param {type:\"string\"}\n",
        "assert PROJECT_ID\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "\n",
        "REGION = \"us-central1\"  # @param [\"us-central1\", \"us-west1\", \"us-east4\"]\n",
        "\n",
        "# Set up gcloud.\n",
        "! gcloud config set project \"$PROJECT_ID\"\n",
        "! gcloud services enable container.googleapis.com\n",
        "\n",
        "# Add kubectl to the set of available tools.\n",
        "! mkdir -p /tools/google-cloud-sdk/.install\n",
        "! gcloud components install kubectl --quiet\n",
        "\n",
        "# The cluster name to create\n",
        "CLUSTER_NAME = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# Use existing GKE cluster or create a new cluster.\n",
        "if CLUSTER_NAME:\n",
        "    ! gcloud container clusters get-credentials {CLUSTER_NAME} --location {REGION}\n",
        "else:\n",
        "    now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "    CLUSTER_NAME = f\"gke-cluster-{now}\"\n",
        "    # create auto-pilot cluster\n",
        "    !gcloud container clusters create-auto {CLUSTER_NAME} --location={REGION} --project={PROJECT_ID}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6psJZY_zUDgj"
      },
      "outputs": [],
      "source": [
        "# @title Deploy\n",
        "\n",
        "# @markdown This section deploys llama 3.2 on GKE.\n",
        "\n",
        "# @markdown The model deployment takes about 5 to 15 minutes to complete. Larger models may take longer.\n",
        "\n",
        "# @markdown Select the model to deploy:\n",
        "MODEL_NAME = \"Llama-3-2-11B-Vision\"  # @param ['Llama-3-2-11B-Vision', 'Llama-3-2-11B-Vision-Instruct', 'Llama-3-2-3B', 'Llama-3-2-3B-Instruct', 'Llama-3-2-1B', 'Llama-3-2-1B-Instruct']\n",
        "ARGS_TEMPLATE = \"\"\"args:\n",
        "        - python\n",
        "        - -m\n",
        "        - vllm.entrypoints.api_server\n",
        "        - --host 0.0.0.0\n",
        "        - --port 7080\n",
        "        - --model=gs://vertex-model-garden-public-us/llama3.2/{}\n",
        "        - --tensor-parallel-size {}\n",
        "        - --swap-space 16\n",
        "        - --gpu-memory-utilization 0.95\n",
        "        {}\n",
        "        - --max-num-seqs {}\n",
        "        {}\n",
        "        - --enable-auto-tool-choice\n",
        "        {}\n",
        "        - --disable-log-stats\n",
        "        {}\"\"\"\n",
        "# Model_name, tensor_size, - --model-type=llama3.1, max_seqs, - --enforce-eager, - --limit_mm_per_prompt='image=1', - --max-model-len 8192\n",
        "\n",
        "\n",
        "def generate_args(missing_args):\n",
        "    args = ARGS_TEMPLATE.format(\n",
        "        missing_args[0],\n",
        "        missing_args[1],\n",
        "        missing_args[2],\n",
        "        missing_args[3],\n",
        "        missing_args[4],\n",
        "        missing_args[5],\n",
        "        missing_args[6],\n",
        "    )\n",
        "    lines = args.splitlines()\n",
        "    non_empty_lines = [line for line in lines if line.strip()]\n",
        "    return \"\\n\".join(non_empty_lines)\n",
        "\n",
        "\n",
        "attr = {\n",
        "    \"Llama-3-2-11B-Vision\": [\n",
        "        [\n",
        "            \"Llama-3.2-11B-Vision\",\n",
        "            \"2\",\n",
        "            \"- --tool-call-parser=vertex-llama-3\",\n",
        "            \"12\",\n",
        "            \"- --enforce-eager\",\n",
        "            \"- --limit_mm_per_prompt='image=1'\",\n",
        "            \"- --max-model-len 8192\",\n",
        "        ],\n",
        "        15,\n",
        "        \"58Gi\",\n",
        "        \"120Gi\",\n",
        "        2,\n",
        "    ],\n",
        "    \"Llama-3-2-11B-Vision-Instruct\": [\n",
        "        [\n",
        "            \"Llama-3.2-11B-Vision-Instruct\",\n",
        "            \"2\",\n",
        "            \"- --tool-call-parser=vertex-llama-3\",\n",
        "            \"12\",\n",
        "            \"- --enforce-eager\",\n",
        "            \"- --limit_mm_per_prompt='image=1'\",\n",
        "            \"- --max-model-len 8192\",\n",
        "        ],\n",
        "        15,\n",
        "        \"58Gi\",\n",
        "        \"120Gi\",\n",
        "        2,\n",
        "    ],\n",
        "    \"Llama-3-2-3B\": [\n",
        "        [\n",
        "            \"Llama-3.2-3B\",\n",
        "            \"1\",\n",
        "            \"\",\n",
        "            \"64\",\n",
        "            \"\",\n",
        "            \"\",\n",
        "            \"- --tool-call-parser=vertex-llama-3\",\n",
        "        ],\n",
        "        10,\n",
        "        \"39Gi\",\n",
        "        \"100Gi\",\n",
        "        1,\n",
        "    ],\n",
        "    \"Llama-3-2-3B-Instruct\": [\n",
        "        [\n",
        "            \"Llama-3.2-3B-Instruct\",\n",
        "            \"1\",\n",
        "            \"\",\n",
        "            \"64\",\n",
        "            \"\",\n",
        "            \"\",\n",
        "            \"- --tool-call-parser=vertex-llama-3\",\n",
        "        ],\n",
        "        10,\n",
        "        \"39Gi\",\n",
        "        \"100Gi\",\n",
        "        1,\n",
        "    ],\n",
        "    \"Llama-3-2-1B\": [\n",
        "        [\n",
        "            \"Llama-3.2-1B\",\n",
        "            \"1\",\n",
        "            \"\",\n",
        "            \"64\",\n",
        "            \"\",\n",
        "            \"\",\n",
        "            \"- --tool-call-parser=vertex-llama-3\",\n",
        "        ],\n",
        "        8,\n",
        "        \"29Gi\",\n",
        "        \"80Gi\",\n",
        "        1,\n",
        "    ],\n",
        "    \"Llama-3-2-1B-Instruct\": [\n",
        "        [\n",
        "            \"Llama-3.2-1B-Instruct\",\n",
        "            \"1\",\n",
        "            \"\",\n",
        "            \"64\",\n",
        "            \"\",\n",
        "            \"\",\n",
        "            \"- --tool-call-parser=vertex-llama-3\",\n",
        "        ],\n",
        "        8,\n",
        "        \"29Gi\",\n",
        "        \"80Gi\",\n",
        "        1,\n",
        "    ],\n",
        "}\n",
        "\n",
        "model_attr = attr[MODEL_NAME]\n",
        "ARGS = generate_args(model_attr[0])\n",
        "CPU_LIMITS = model_attr[1]\n",
        "MEMORY_SIZE = model_attr[2]\n",
        "EPHEMERAL_STORAGE_SIZE = model_attr[3]\n",
        "GPU_COUNT = model_attr[4]\n",
        "\n",
        "\n",
        "K8S_YAML = f\"\"\"\n",
        "apiVersion: apps/v1\n",
        "kind: Deployment\n",
        "metadata:\n",
        "  name: llama-deployment\n",
        "spec:\n",
        "  replicas: 1\n",
        "  selector:\n",
        "    matchLabels:\n",
        "      app: llama-server\n",
        "  template:\n",
        "    metadata:\n",
        "      labels:\n",
        "        app: llama-server\n",
        "        ai.gke.io/model: {MODEL_NAME}\n",
        "        ai.gke.io/inference-server: vllm\n",
        "        examples.ai.gke.io/source: model-garden\n",
        "    spec:\n",
        "      containers:\n",
        "      - name: inference-server\n",
        "        image: us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20241007_2233_RC00\n",
        "        resources:\n",
        "          requests:\n",
        "            cpu: {CPU_LIMITS}\n",
        "            memory: {MEMORY_SIZE}\n",
        "            ephemeral-storage: {EPHEMERAL_STORAGE_SIZE}\n",
        "            nvidia.com/gpu: {GPU_COUNT}\n",
        "          limits:\n",
        "            cpu: {CPU_LIMITS}\n",
        "            memory: {MEMORY_SIZE}\n",
        "            ephemeral-storage: {EPHEMERAL_STORAGE_SIZE}\n",
        "            nvidia.com/gpu: {GPU_COUNT}\n",
        "        {ARGS}\n",
        "        env:\n",
        "        - name: MODEL_ID\n",
        "          value: 'meta-llama/{MODEL_NAME}'\n",
        "        - name: DEPLOY_SOURCE\n",
        "          value: 'UI_NATIVE_MODEL'\n",
        "        volumeMounts:\n",
        "        - mountPath: /dev/shm\n",
        "          name: dshm\n",
        "      volumes:\n",
        "      - name: dshm\n",
        "        emptyDir:\n",
        "          medium: Memory\n",
        "      nodeSelector:\n",
        "        cloud.google.com/gke-accelerator: nvidia-l4\n",
        "---\n",
        "apiVersion: v1\n",
        "kind: Service\n",
        "metadata:\n",
        "  name: llama-service\n",
        "spec:\n",
        "  selector:\n",
        "    app: llama-server\n",
        "  type: ClusterIP\n",
        "  ports:\n",
        "  - protocol: TCP\n",
        "    port: 8000\n",
        "    targetPort: 7080\n",
        "\"\"\"\n",
        "\n",
        "with open(\"llama_32.yaml\", \"w\") as f:\n",
        "    f.write(K8S_YAML)\n",
        "\n",
        "! kubectl apply -f llama_32.yaml\n",
        "\n",
        "# Wait for container to be created.\n",
        "import time\n",
        "\n",
        "MAX_WAIT_TIME = 600  # 10 minutes in seconds\n",
        "start_time = time.time()\n",
        "end_time = time.time() + MAX_WAIT_TIME\n",
        "\n",
        "print(\"Waiting for container to be created...\\n\")\n",
        "while start_time < end_time:\n",
        "    shell_output = ! kubectl get pod -l app=llama-server\n",
        "    container_status = \"\\n\".join(shell_output)\n",
        "    if \"1/1\" in container_status:\n",
        "        break\n",
        "    time.sleep(15)\n",
        "    start_time += 15\n",
        "\n",
        "if start_time > end_time:\n",
        "    print(\"Deployment took longer than expected\")\n",
        "\n",
        "print(container_status)\n",
        "\n",
        "# Wait for downloading artifacts.\n",
        "start_time = time.time()\n",
        "end_time = time.time() + MAX_WAIT_TIME\n",
        "print(\"\\nDownloading artifacts...\")\n",
        "while start_time < end_time:\n",
        "    shell_output = ! kubectl logs -l app=llama-server\n",
        "    logs = \"\\n\".join(shell_output)\n",
        "    if \"Connected\" in logs or \"Uvicorn running\" in logs:\n",
        "        break\n",
        "    time.sleep(15)\n",
        "    start_time += 15\n",
        "\n",
        "if start_time > end_time:\n",
        "    print(\"Deployment took longer than expected\")\n",
        "\n",
        "\n",
        "print(\"\\nServer is up and running!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6rnlQ4jGmFzs"
      },
      "outputs": [],
      "source": [
        "# @title Chat completion for text-only models\n",
        "\n",
        "# @markdown Once the server is up and running, you may send prompts to local server for prediction.\n",
        "\n",
        "import json\n",
        "\n",
        "user_message = \"What is AI?\"  # @param {type: \"string\"}\n",
        "max_tokens = 50  # @param {type:\"integer\"}\n",
        "temperature = 0.9  # @param {type:\"number\"}\n",
        "\n",
        "# Overrides max_tokens and top_k parameters during inferences.\n",
        "request = {\n",
        "    \"prompt\": user_message,\n",
        "    \"max_tokens\": max_tokens,\n",
        "    \"temperature\": temperature,\n",
        "}\n",
        "get_pod = ! kubectl get pod -l app=llama-server -o jsonpath=\"{{.items[0].metadata.name}}\"\n",
        "pod_name = get_pod[0]\n",
        "\n",
        "exec_command = f\"\"\"kubectl exec -t {pod_name} -- curl -X POST http://localhost:7080/generate \\\n",
        "   -H \"Content-Type: application/json\" \\\n",
        "   -d '{json.dumps(request)}' \\\n",
        "   2> /dev/null\"\"\"\n",
        "\n",
        "response = !{exec_command}\n",
        "# print(response)\n",
        "# @markdown Response:\n",
        "print(json.loads(response[0])[\"predictions\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QDTasPgGW7EG"
      },
      "outputs": [],
      "source": [
        "# @title Chat completion for vision models\n",
        "\n",
        "# @markdown Next fill out some request parameters:\n",
        "\n",
        "user_image = \"https://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/The_Blue_Marble_%28remastered%29.jpg/580px-The_Blue_Marble_%28remastered%29.jpg\"  # @param {type: \"string\"}\n",
        "user_message = \"What is in the image?\"  # @param {type: \"string\"}\n",
        "max_tokens = 50  # @param {type: \"integer\"}\n",
        "temperature = 1.0  # @param {type: \"number\"}\n",
        "\n",
        "# @markdown Now we can send a request.\n",
        "\n",
        "prompt = f\"\"\"image_url: {user_image} user_prompt:{user_message}\"\"\"\n",
        "\n",
        "# Overrides max_tokens and top_k parameters during inferences.\n",
        "request = {\n",
        "    \"prompt\": prompt,\n",
        "    \"max_tokens\": max_tokens,\n",
        "    \"temperature\": temperature,\n",
        "}\n",
        "\n",
        "get_pod = ! kubectl get pod -l app=llama-server -o jsonpath=\"{{.items[0].metadata.name}}\"\n",
        "pod_name = get_pod[0]\n",
        "\n",
        "exec_command = f\"\"\"kubectl exec -t {pod_name} -- curl -X POST http://localhost:7080/generate \\\n",
        "   -H \"Content-Type: application/json\" \\\n",
        "   -d '{json.dumps(request)}' \\\n",
        "   2> /dev/null\"\"\"\n",
        "\n",
        "response = !{exec_command}\n",
        "print(json.loads(response[0])[\"predictions\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbRmgoOZF6es"
      },
      "source": [
        "## Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "911406c1561e"
      },
      "outputs": [],
      "source": [
        "# @markdown  Delete the experiment models and endpoints to recycle the resources\n",
        "# @markdown  and avoid unnecessary continouous charges that may incur.\n",
        "DELETE_DEPLOYMENT = False # @param {type: \"boolean\"}\n",
        "DELETE_CLUSTER = False # @param {type: \"boolean\"}\n",
        "\n",
        "if DELETE_CLUSTER or DELETE_DEPLOYMENT:\n",
        "  ! kubectl delete deployments llama-deployment\n",
        "  ! kubectl delete services llama-service\n",
        "\n",
        "if DELETE_CLUSTER:\n",
        "  ! gcloud container clusters delete {CLUSTER_NAME} \\\n",
        "    --region={REGION} \\\n",
        "    --quiet"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_llama3_2_deployment_on_gke.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

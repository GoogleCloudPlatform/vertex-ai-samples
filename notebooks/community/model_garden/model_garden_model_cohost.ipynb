{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "d8d8b3291675"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f19291c2dce"
      },
      "source": [
        "# Vertex AI Model Garden - Model Co-hosting Serving\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_model_cohost.ipynb\">\n",
        "      <img alt=\"Workbench logo\" src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" width=\"32px\"><br> Run in Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_model_cohost.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_model_cohost.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01286ee292d7"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook provides a step-by-step guide to (1) single-model multi-replica serving, and (2) multi-model serving. For single-model multi-replica serving, the notebook demonstrates a container-level solution using the Model Garden vLLM model co-hosting container and an infrastructure-level solution using pod co-scheduling and NVIDIA Multi-Instance GPU (MIG). For multi-model serving, the notebook demonstrates a container-level solution using the Model Garden vLLM model co-hosting container. The notebook additionally demonstrates finding the optimal serving recipe for the Model Garden vLLM model co-hosting container using a benchmark utility.\n",
        "\n",
        "### Objective\n",
        "\n",
        "The goal is to efficiently serve a single model with multiple replicas and serve multiple models on a full-shape VM, and to automate the process of testing various serving strategies (pipeline parallelism, tensor parallelism, and creating model replicas) to identify the recipe that provides the best performance (throughput and latency). We will then deploy the winning recipe to a Vertex AI Endpoint.\n",
        "\n",
        "### Steps\n",
        "\n",
        "#### Single-model Multi-replica Serving\n",
        "\n",
        "1.  **Setup**: Install libraries, authenticate with Google Cloud, and configure your environment.\n",
        "1.  **Prepare Benchmark Files**: Prepare benchmark files.\n",
        "1.  **Run Benchmark**: Execute the benchmark utility to test different serving configurations under various concurrencies.\n",
        "1.  **Review Reference Benchmark Results [Case Study]**: Review a reference set of benchmark results to learn how to interpret benchmark results and learn heuristics for optimal serving recipes.\n",
        "1.  **Analyze Benchmark Results**: Analyze the generated benchmark outputs, visualize the performance metrics, and select the optimal serving recipe.\n",
        "1.  **Deploy to Vertex AI and Test the Endpoint**: Upload the model to the Vertex AI Model Registry and deploy it to an Endpoint following the optimal serving recipe. Send a prediction request to the newly deployed endpoint.\n",
        "1.  **[Alternative Solution: Pod Co-scheduling + MIG] Review Reference Benchmark Results**: Review a reference set of benchmark results to understand the performance scaling of pod co-scheduling + MIG.\n",
        "1.  **[Alternative Solution: Pod Co-scheduling + MIG] Deploy to Vertex AI and Test the Endpoint**: Use the infrastructure-level solution, pod co-scheduling + MIG, to deploy the model with multiple replicas following the optimal serving recipe. Send a prediction request to the newly deployed endpoint.\n",
        "1.  **Clean Up**: Delete the created Vertex AI resources.\n",
        "\n",
        "#### Multi-model Serving\n",
        "\n",
        "1.  **Setup**: Install libraries, authenticate with Google Cloud, and configure your environment.\n",
        "1.  **Learn to Configure the Model Co-hosting Server**: Learn about the Model Garden model co-hosting server and how to use it to serve multiple models with the same container.\n",
        "1.  **Deploy to Vertex AI and Test the Endpoint**: Upload the model to the Vertex AI Model Registry and deploy it to an Endpoint following the optimal serving recipe. Send a prediction request to the newly deployed endpoint.\n",
        "1.  **Clean Up**: Delete the created Vertex AI resources.\n",
        "\n",
        "### File a bug\n",
        "\n",
        "File a bug on [GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/issues/new) if you encounter any issue with the notebook.\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea87745c7c4e"
      },
      "source": [
        "## Single-model Multi-replica Serving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e67705049064"
      },
      "source": [
        "## 1. Setup\n",
        "\n",
        "First, let's install the necessary packages and set up your Google Cloud project environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "c64b3b480432"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown 2. **[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment.\n",
        "\n",
        "REGION = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown 3. If you want to run predictions with H100 GPUs or H200 GPUs, we recommend using the regions listed below. **NOTE:** Make sure you have associated quota in selected regions. Click the links to see your current quota for H100s: [`CustomModelServingH100GPUsPerProjectPerRegion`](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_h100_gpus) and H200s: [`CustomModelServingH200GPUsPerProjectPerRegion`](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_h200_gpus). You can request for quota following the instructions at [\"Request a higher quota\"](https://cloud.google.com/docs/quota/view-manage#requesting_higher_quota).\n",
        "\n",
        "# @markdown | Machine Type | Accelerator Type | Recommended Regions |\n",
        "# @markdown | ----------- | ----------- | ----------- |\n",
        "# @markdown | a3-highgpu-8g | 8 NVIDIA_H100_80GB | asia-southeast1, europe-west4, us-central1, us-east5, us-west1 |\n",
        "# @markdown | a3-ultragpu-8g | 8 NVIDIA_H200_141GB | asia-south2, us-south1 |\n",
        "\n",
        "# Upgrade Vertex AI SDK.\n",
        "! pip3 install --upgrade --quiet 'google-cloud-aiplatform==1.103.0'\n",
        "! pip3 install --upgrade --quiet aiohttp matplotlib pandas seaborn\n",
        "\n",
        "# Import the necessary packages\n",
        "import importlib\n",
        "import os\n",
        "from typing import Tuple\n",
        "\n",
        "import requests\n",
        "from google import auth\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "# Upgrade Vertex AI SDK.\n",
        "if os.environ.get(\"VERTEX_PRODUCT\") != \"COLAB_ENTERPRISE\":\n",
        "    ! pip install --upgrade tensorflow\n",
        "! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
        "\n",
        "common_util = importlib.import_module(\n",
        "    \"vertex-ai-samples.notebooks.community.model_garden.docker_source_codes.notebook_util.common_util\"\n",
        ")\n",
        "\n",
        "models, endpoints = {}, {}\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "if not REGION:\n",
        "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "! gcloud config set project $PROJECT_ID\n",
        "\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(\n",
        "    project=PROJECT_ID,\n",
        "    location=REGION,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c3158e947b7"
      },
      "source": [
        "## 2. Prepare Benchmark Files\n",
        "\n",
        "The core of this workflow is a benchmark utility that automates the benchmark process for single-model multi-replica serving. The benchmark utility launches the vLLM model server through Docker and launches a benchmark client that sends prediction requests to the model server. This utility depends on a Python script that implements the benchmark client and a benchmark dataset. In this section, we prepare the necessary benchmark files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a6de84b199e"
      },
      "source": [
        "### The Main Benchmark Utility\n",
        "\n",
        "The utility takes as input (1) the vLLM container version and model for\n",
        "launching the vLLM server and (2) the benchmark setup (benchmark script,\n",
        "dataset, input length, output length, number of prompts, and concurrencies) for\n",
        "launching the benchmark client. The utility launches the vLLM server with\n",
        "docker, waits for the vLLM server to be ready, and then launches the benchmark\n",
        "client. When launching the benchmark client, the utility iterates over\n",
        "different possible combinations of tensor parallel size and number of model\n",
        "replicas settings, under different concurrencies. In addition, the utility\n",
        "allows the definition of maximum latency metrics. If the maximum latencies\n",
        "are set, the utility checks whether each benchmark run satisfies the latencies\n",
        "and marks it accordingly in the benchmark results. If set, the utility skips\n",
        "larger concurrencies if one or more maximum latencies are not met at some\n",
        "concurrency. The utility generates an analysis figure plotting metrics versus\n",
        "concurrencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9a7638f06f7e"
      },
      "outputs": [],
      "source": [
        "%%writefile benchmark_util.py\n",
        "\"\"\"Utility for benchmarking vLLM under different setups and concurrencies.\n",
        "\n",
        "The utility takes as input (1) the vLLM container version and model for\n",
        "launching the vLLM server and (2) the benchmark setup (benchmark script,\n",
        "dataset, input length, output length, number of prompts, and concurrencies) for\n",
        "launching the benchmark client. The utility launches the vLLM server with\n",
        "docker, waits for the vLLM server to be ready, and then launches the benchmark\n",
        "client. When launching the benchmark client, the utility iterates over\n",
        "different possible combinations of tensor parallel size and number of model\n",
        "replicas settings, under different concurrencies. In addition, the utility\n",
        "allows the definition of maximum latency metrics. If the maximum latencies\n",
        "are set, the utility checks whether each benchmark run satisfies the latencies\n",
        "and marks it accordingly in the benchmark results. If --no-skip-concurrencies-given-latency\n",
        "is set, the utility skips larger concurrencies if one or more maximum latencies\n",
        "are not met at some concurrency. The utility generates an analysis figure\n",
        "plotting metrics versus concurrencies.\n",
        "\n",
        "Sample command:\n",
        "\n",
        "python benchmark_util.py \\\n",
        "  --total-gpus 8 \\\n",
        "  --input-length 1200 \\\n",
        "  --output-length 250 \\\n",
        "  --num-prompts 2000 \\\n",
        "  --sonnet-prefix-len 49 \\\n",
        "  --concurrencies 1 8 16 \\\n",
        "  --max-median-ttft-ms 1000 \\\n",
        "  --max-p99-ttft-ms 10000 \\\n",
        "  --max-median-tpot-ms 100 \\\n",
        "  --max-p99-tpot-ms 1000 \\\n",
        "  --model /path/to/model \\\n",
        "  --docker-uri us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20250808_0916_RC01_maas \\\n",
        "  --server-init-timeout 600 \\\n",
        "  --benchmark-script-path /path/to/benchmark_serving.py \\\n",
        "  --dataset-path /path/to/sonnet.txt \\\n",
        "  --results-output-path /path/to/benchmark_results.csv \\\n",
        "  --figure-output-path /path/to/benchmark_figure.png \\\n",
        "  --no-skip-concurrencies-given-latency\n",
        "\"\"\"\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "from typing import List\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "BENCHMARK_BACKEND = \"chat_completions\"\n",
        "METRICS_TO_PLOT = [\n",
        "    \"request_throughput\",\n",
        "    \"input_throughput\",\n",
        "    \"output_throughput\",\n",
        "    \"median_latency_ms\",\n",
        "    \"median_ttft_ms\",\n",
        "    \"median_tpot_ms\",\n",
        "]\n",
        "\n",
        "parser = argparse.ArgumentParser(\n",
        "    description=\"vLLM Docker benchmark script.\"\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--total-gpus\",\n",
        "    type=int,\n",
        "    default=8,\n",
        "    help=\"Total number of GPUs available on the machine.\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--input-length\",\n",
        "    type=int,\n",
        "    default=1200,\n",
        "    help=\"Benchmark input length.\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--output-length\",\n",
        "    type=int,\n",
        "    default=250,\n",
        "    help=\"Benchmark output length.\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--num-prompts\",\n",
        "    type=int,\n",
        "    default=100,\n",
        "    help=\"Number of prompts to use in benchmark.\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--sonnet-prefix-len\",\n",
        "    type=int,\n",
        "    default=30,\n",
        "    help=\"Number of prefix tokens per request, used for sonnet dataset.\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--concurrencies\",\n",
        "    type=int,\n",
        "    nargs=\"+\",\n",
        "    default=[1, 8, 16, 32, 64, 128],\n",
        "    help=\"List of target concurrencies to test.\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--max-median-ttft-ms\",\n",
        "    type=float,\n",
        "    default=None,\n",
        "    help=\"Maximum allowed median Time to First Token (TTFT) in milliseconds.\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--max-p99-ttft-ms\",\n",
        "    type=float,\n",
        "    default=None,\n",
        "    help=\"Maximum allowed P99 Time to First Token (TTFT) in milliseconds.\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--max-median-tpot-ms\",\n",
        "    type=float,\n",
        "    default=None,\n",
        "    help=(\n",
        "        \"Maximum allowed median Time per Output Token (TPOT) in milliseconds.\"\n",
        "    ),\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--max-p99-tpot-ms\",\n",
        "    type=float,\n",
        "    default=None,\n",
        "    help=\"Maximum allowed P99 Time per Output Token (TPOT) in milliseconds.\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--model\",\n",
        "    type=str,\n",
        "    required=True,\n",
        "    help=\"Local path to the model or HuggingFace model ID.\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--docker-uri\",\n",
        "    type=str,\n",
        "    default=(\n",
        "        \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20250808_0916_RC01_maas\",\n",
        "    ),\n",
        "    help=\"Docker image URI for the vLLM server.\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--server-init-timeout\",\n",
        "    type=int,\n",
        "    default=600,\n",
        "    help=\"Timeout limit (in seconds) for server initialization.\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--benchmark-script-path\",\n",
        "    type=str,\n",
        "    required=True,\n",
        "    help=\"Path to the benchmark_serving.py script.\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--dataset-path\",\n",
        "    type=str,\n",
        "    required=True,\n",
        "    help=\"Path to the benchmark dataset.\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--results-output-path\",\n",
        "    type=str,\n",
        "    required=True,\n",
        "    help=\"Path to output benchmark results.\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--figure-output-path\",\n",
        "    type=str,\n",
        "    required=True,\n",
        "    help=\"Path to output the analysis figure.\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--skip-concurrencies-given-latency\",\n",
        "    action=argparse.BooleanOptionalAction,\n",
        "    default=False,\n",
        "    help=(\n",
        "        \"Skip larger concurrencies when one of more latency requirements are \"\n",
        "        \"not met at a concurrency.\"\n",
        "    ),\n",
        ")\n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "def wait_for_server(container_id: str, timeout: int = 1200) -> bool:\n",
        "    \"\"\"\n",
        "    Polls the Docker container's logs to wait for the server startup message.\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    while True:\n",
        "        if time.time() - start_time > timeout:\n",
        "            print(f\"Error: Server did not start within {timeout} seconds.\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            # Use 'docker logs --tail 1' to check the last log line\n",
        "            output = subprocess.check_output(\n",
        "                [\"docker\", \"logs\", \"--tail\", \"1\", container_id],\n",
        "                text=True,\n",
        "                stderr=subprocess.STDOUT\n",
        "            ).strip()\n",
        "\n",
        "            if \"Application startup complete\" in output:\n",
        "                time.sleep(5)  # Wait for 5 seconds for all model servers\n",
        "                print(\"vLLM server is ready! 🚀\")\n",
        "                return True\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(\n",
        "                f\"Error checking logs for container {container_id}: {e.output}\"\n",
        "            )\n",
        "            return False\n",
        "\n",
        "        time.sleep(5)  # Wait for 5 seconds before checking again\n",
        "\n",
        "\n",
        "def run_benchmark(\n",
        "    pp_size: int,\n",
        "    tp_size: int,\n",
        "    model_replicas: int,\n",
        "    concurrency_list: List[int],\n",
        "    input_length: int,\n",
        "    output_length: int,\n",
        "    num_prompts: int,\n",
        "    sonnet_prefix_len: int,\n",
        "    model: str,\n",
        "    server_init_timeout: int,\n",
        "    benchmark_script: str,\n",
        "    dataset: str,\n",
        "    vllm_host: str = \"0.0.0.0\",\n",
        "    vllm_port: int = 7080,\n",
        "    max_median_ttft_ms: float = None,\n",
        "    max_p99_ttft_ms: float = None,\n",
        "    max_median_tpot_ms: float = None,\n",
        "    max_p99_tpot_ms: float = None,\n",
        "    skip_concurrencies_given_latency: bool = False,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Launches the vLLM server and benchmark client.\"\"\"\n",
        "\n",
        "    print(\n",
        "        f\"Starting vLLM server with PP={pp_size}, TP={tp_size} and Replicas=\"\n",
        "        f\"{model_replicas}...\"\n",
        "    )\n",
        "\n",
        "    if model_replicas > 1:\n",
        "        api_server = \"vllm.entrypoints.nginx_server\"\n",
        "    else:\n",
        "        api_server = \"vllm.entrypoints.api_server\"\n",
        "\n",
        "    # Prepare vLLM server command\n",
        "    vllm_server_cmd = [\n",
        "        \"python\", \"-m\", api_server,\n",
        "        f\"--host={vllm_host}\",\n",
        "        f\"--port={vllm_port}\",\n",
        "        f\"--model={model}\",\n",
        "        f\"--pipeline-parallel-size={pp_size}\",\n",
        "        f\"--tensor-parallel-size={tp_size}\",\n",
        "        f\"--data-parallel-size=1\",\n",
        "        \"--swap-space=16\",\n",
        "        \"--gpu-memory-utilization=0.9\",\n",
        "        \"--no-enable-prefix-caching\",\n",
        "    ]\n",
        "\n",
        "    if model_replicas > 1:\n",
        "        vllm_server_cmd.extend([\n",
        "            f\"--num_instances={model_replicas}\",\n",
        "            f\"--total_gpus={int(tp_size * model_replicas)}\"\n",
        "        ])\n",
        "\n",
        "    # Prepare Docker command\n",
        "    docker_uri = args.docker_uri\n",
        "    gpu_devices = \",\".join([str(i) for i in range(args.total_gpus)])\n",
        "    docker_cmd = [\n",
        "        \"docker\", \"run\",\n",
        "        \"--entrypoint\", \"bash\",\n",
        "        \"-e\", f\"NVIDIA_VISIBLE_DEVICES={gpu_devices}\",\n",
        "        \"--gpus\", \"all\",\n",
        "        \"--network=host\",\n",
        "        \"-v\", f\"{os.path.expanduser('~')}:{os.path.expanduser('~')}\",\n",
        "        \"--shm-size\", \"19.2gb\",\n",
        "        \"-itd\",  # Run in detached mode\n",
        "        docker_uri,\n",
        "        \"-c\", \" \".join(vllm_server_cmd)\n",
        "    ]\n",
        "\n",
        "    # Start the vLLM server inside Docker\n",
        "    try:\n",
        "        print(\"Running Docker command:\", \" \".join(docker_cmd))\n",
        "        container_id = subprocess.check_output(docker_cmd, text=True).strip()\n",
        "        print(f\"vLLM server starting in container: {container_id}\")\n",
        "\n",
        "        # Wait for the server to be ready using the new function\n",
        "        if not wait_for_server(\n",
        "            container_id=container_id,\n",
        "            timeout=server_init_timeout,\n",
        "        ):\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        all_results_df = pd.DataFrame()\n",
        "\n",
        "        for concurrency in concurrency_list:\n",
        "            print(f\"Benchmarking with concurrency: {concurrency}\")\n",
        "\n",
        "            benchmark_cmd = [\n",
        "                \"python\", benchmark_script,\n",
        "                f\"--backend={BENCHMARK_BACKEND}\",\n",
        "                f\"--model={model}\",\n",
        "                f\"--tokenizer={model}\",\n",
        "                f\"--host={vllm_host}\",\n",
        "                f\"--port={vllm_port}\",\n",
        "                f\"--dataset={dataset}\",\n",
        "                f\"--max-input-length={input_length}\",\n",
        "                f\"--max-output-length={output_length}\",\n",
        "                f\"--num-prompts={num_prompts}\",\n",
        "                f\"--sonnet-prefix-len={sonnet_prefix_len}\",\n",
        "                f\"--c={concurrency}\",\n",
        "                f\"--output-dir={os.getcwd()}\",\n",
        "                f\"--name={pp_size}_{tp_size}_{model_replicas}_{concurrency}\",\n",
        "            ]\n",
        "\n",
        "            # Execute benchmark script\n",
        "            result = subprocess.run(\n",
        "                benchmark_cmd, capture_output=True, text=True, check=True\n",
        "            )\n",
        "            print(\"Benchmark command executed successfully.\")\n",
        "\n",
        "            full_results_filename = f\"{pp_size}_{tp_size}_{model_replicas}_{concurrency}_aggregated_results.json\"\n",
        "            full_results_path = os.path.join(os.getcwd(), full_results_filename)\n",
        "\n",
        "            if os.path.exists(full_results_path):\n",
        "                df = pd.read_json(full_results_path, lines=True)\n",
        "\n",
        "                # Add configuration columns\n",
        "                df[\"pp_size\"] = pp_size\n",
        "                df[\"tp_size\"] = tp_size\n",
        "                df[\"model_replicas\"] = model_replicas\n",
        "                df[\"docker_cmd\"] = \" \".join(docker_cmd)\n",
        "                df[\"benchmark_cmd\"] = \" \".join(benchmark_cmd)\n",
        "\n",
        "                # Compare latency metrics against optional max latency requirements\n",
        "                missed_latency_requirement = False\n",
        "                if max_median_ttft_ms is not None:\n",
        "                    median_ttft_ms = df['median_ttft_ms'].iloc[0] if not df['median_ttft_ms'].isnull().all() else float('inf')\n",
        "                    df[\"median_ttft_ok\"] = median_ttft_ms <= max_median_ttft_ms\n",
        "                    if median_ttft_ms > max_median_ttft_ms:\n",
        "                        missed_latency_requirement = True\n",
        "                if max_p99_ttft_ms is not None:\n",
        "                    p99_ttft_ms = df['p99_ttft_ms'].iloc[0] if not df['p99_ttft_ms'].isnull().all() else float('inf')\n",
        "                    df[\"p99_ttft_ok\"] = p99_ttft_ms <= max_p99_ttft_ms\n",
        "                    if p99_ttft_ms > max_p99_ttft_ms:\n",
        "                        missed_latency_requirement = True\n",
        "                if max_median_tpot_ms is not None:\n",
        "                    median_tpot_ms = df['median_tpot_ms'].iloc[0] if not df['median_tpot_ms'].isnull().all() else float('inf')\n",
        "                    df[\"median_tpot_ok\"] = median_tpot_ms <= max_median_tpot_ms\n",
        "                    if median_tpot_ms > max_median_tpot_ms:\n",
        "                        missed_latency_requirement = True\n",
        "                if max_p99_tpot_ms is not None:\n",
        "                    p99_tpot_ms = df['p99_tpot_ms'].iloc[0] if not df['p99_tpot_ms'].isnull().all() else float('inf')\n",
        "                    df[\"p99_tpot_ok\"] = p99_tpot_ms <= max_p99_tpot_ms\n",
        "                    if p99_tpot_ms > max_p99_tpot_ms:\n",
        "                        missed_latency_requirement = True\n",
        "\n",
        "                all_results_df = pd.concat([all_results_df, df], ignore_index=True)\n",
        "            else:\n",
        "                print(f\"Warning: Full results file not found at {full_results_path}\")\n",
        "\n",
        "            print(f\"Benchmark for PP={pp_size}, TP={tp_size}, Replicas={model_replicas}, Concurrency={concurrency} complete.\")\n",
        "\n",
        "            if skip_concurrencies_given_latency and missed_latency_requirement:\n",
        "                print(f\"Latency requirement(s) not met at concurrency={concurrency}. Skip larger concurrencies.\")\n",
        "                break\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error during benchmark: {e.stderr}\")\n",
        "        return pd.DataFrame()\n",
        "    finally:\n",
        "        # Stop and remove the Docker container\n",
        "        subprocess.run([\"docker\", \"stop\", container_id], check=False, text=True)\n",
        "        subprocess.run([\"docker\", \"rm\", container_id], check=False, text=True)\n",
        "        print(f\"Container {container_id} stopped and removed.\")\n",
        "\n",
        "    return all_results_df\n",
        "\n",
        "\n",
        "def plot_metric_by_concurrency(\n",
        "    results_path: str,\n",
        "    target_metrics: List[str] = [\"request_throughput\"],\n",
        "    figure_path: str = \"benchmark_figure.png\",\n",
        "):\n",
        "    \"\"\"Creates analysis figures based on benchmark results.\"\"\"\n",
        "    # Load benchmark results\n",
        "    all_results_df = pd.read_csv(results_path)\n",
        "\n",
        "    # Create a new column to represent each model server setup\n",
        "    all_results_df[\"Server Config\"] = all_results_df.apply(\n",
        "        lambda row: (\n",
        "            f\"PP={int(row['pp_size'])}, TP={int(row['tp_size'])}, Replicas=\"\n",
        "            f\"{int(row['model_replicas'])}\"\n",
        "        ),\n",
        "        axis=1,\n",
        "    )\n",
        "\n",
        "    # Melt the DataFrame to a long format for easier plotting\n",
        "    all_results_df_melted = all_results_df.melt(\n",
        "        id_vars=[\"concurrent_requests\", \"Server Config\"],\n",
        "        value_vars=target_metrics,\n",
        "        var_name=\"metric\",\n",
        "        value_name=\"value\",\n",
        "    )\n",
        "\n",
        "    sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "    # Create the multi-plot grid using relplot\n",
        "    g = sns.relplot(\n",
        "        data=all_results_df_melted,\n",
        "        x=\"concurrent_requests\",\n",
        "        y=\"value\",\n",
        "        hue=\"Server Config\",\n",
        "        col=\"metric\",\n",
        "        col_wrap=3,\n",
        "        kind=\"line\",\n",
        "        marker=\"o\",\n",
        "        height=4,\n",
        "        aspect=1.2,\n",
        "        facet_kws={'sharey': False},\n",
        "    )\n",
        "\n",
        "    def get_formatted_name(metric_name):\n",
        "        name_map = {\n",
        "            'request_throughput': 'Request Throughput (req/s)',\n",
        "            'output_throughput': 'Output Throughput (tok/s)',\n",
        "            'input_throughput': 'Input Throughput (tok/s)',\n",
        "            'median_latency_ms': 'Median E2E Latency (ms)',\n",
        "            'p99_latency_ms': 'P99 E2E Latency (ms)',\n",
        "            'median_ttft_ms': 'Median TTFT (ms)',\n",
        "            'p99_ttft_ms': 'P99 TTFT (ms)',\n",
        "            'median_tpot_ms': 'Median TPOT (ms)',\n",
        "            'p99_tpot_ms': 'P99 TPOT (ms)'\n",
        "        }\n",
        "        return name_map.get(metric_name, metric_name)\n",
        "\n",
        "    for ax in g.axes.flatten():\n",
        "        ax.set_title(get_formatted_name(ax.get_title().replace(\"metric = \", \"\")))\n",
        "        ax.set_xlabel(\"Concurrency\")\n",
        "        ax.set_ylabel(\"\")\n",
        "\n",
        "    g.figure.subplots_adjust(top=0.9)\n",
        "    plt.suptitle(\n",
        "        \"Performance Metrics vs. Concurrency Across Server Configs\",\n",
        "        fontsize=20,\n",
        "    )\n",
        "\n",
        "    plt.savefig(figure_path, dpi=300)\n",
        "    print(f\"Analysis figure saved to {figure_path}.\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    total_gpus = args.total_gpus\n",
        "    benchmark_script = args.benchmark_script_path\n",
        "\n",
        "    # Check if the benchmark script exists\n",
        "    if not os.path.exists(benchmark_script):\n",
        "        print(f\"Error: The benchmark script at {benchmark_script} does not exist.\")\n",
        "        return\n",
        "\n",
        "    # Pull the target Docker image\n",
        "    print(\"Pulling the Docker image...\")\n",
        "    try:\n",
        "        subprocess.run([\"docker\", \"pull\", args.docker_uri], check=True, text=True)\n",
        "        print(\"Docker image pulled successfully.\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error pulling Docker image: {e.stderr}\")\n",
        "        return\n",
        "\n",
        "    final_results = pd.DataFrame()\n",
        "\n",
        "    filtered_concurrency_list = list(set(\n",
        "        [c for c in args.concurrencies if c > 0]\n",
        "    ))\n",
        "    filtered_concurrency_list = sorted(filtered_concurrency_list)\n",
        "\n",
        "    # Iterate through different PP, TP and replica settings\n",
        "    for pp_size in range(1, total_gpus + 1):\n",
        "        for tp_size in range(1, total_gpus + 1):\n",
        "            if total_gpus % (pp_size * tp_size) == 0:\n",
        "                model_replicas = total_gpus // (pp_size * tp_size)\n",
        "\n",
        "                results_for_config = run_benchmark(\n",
        "                    pp_size=pp_size,\n",
        "                    tp_size=tp_size,\n",
        "                    model_replicas=model_replicas,\n",
        "                    concurrency_list=filtered_concurrency_list,\n",
        "                    input_length=args.input_length,\n",
        "                    output_length=args.output_length,\n",
        "                    num_prompts=args.num_prompts,\n",
        "                    sonnet_prefix_len=args.sonnet_prefix_len,\n",
        "                    model=args.model,\n",
        "                    server_init_timeout=args.server_init_timeout,\n",
        "                    benchmark_script=benchmark_script,\n",
        "                    dataset=args.dataset_path,\n",
        "                    max_median_ttft_ms=args.max_median_ttft_ms,\n",
        "                    max_p99_ttft_ms=args.max_p99_ttft_ms,\n",
        "                    max_median_tpot_ms=args.max_median_tpot_ms,\n",
        "                    max_p99_tpot_ms=args.max_p99_tpot_ms,\n",
        "                    skip_concurrencies_given_latency=args.skip_concurrencies_given_latency,\n",
        "                )\n",
        "                final_results = pd.concat([final_results, results_for_config], ignore_index=True)\n",
        "                final_results.to_csv(args.results_output_path)\n",
        "                print(f\"Intermediate benchmark results saved to {args.results_output_path}\")\n",
        "\n",
        "    # Print the final results table\n",
        "    if not final_results.empty:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"Final Benchmark Results Summary\")\n",
        "        print(\"=\"*80)\n",
        "        print(final_results.head().to_markdown(index=False)) # print first 5 rows\n",
        "        print(\"... and so on\")\n",
        "        print(\"Total rows collected:\", len(final_results))\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        final_results.to_csv(args.results_output_path)\n",
        "        print(f\"Benchmark results saved to {args.results_output_path}\")\n",
        "\n",
        "    # Create analysis figure\n",
        "    plot_metric_by_concurrency(\n",
        "        results_path=args.results_output_path,\n",
        "        target_metrics=METRICS_TO_PLOT,\n",
        "        figure_path=args.figure_output_path,\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e09c46442246"
      },
      "source": [
        "### Benchmark Client (`benchmark_serving.py`)\n",
        "\n",
        "This script, called by the main utility, is responsible for sending concurrent requests to the vLLM server and measuring performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ef4a2984e29e"
      },
      "outputs": [],
      "source": [
        "%%writefile benchmark_serving.py\n",
        "\"\"\"Benchmark client for LLM serving.\"\"\"\n",
        "\n",
        "# pylint: disable=g-multiple-import\n",
        "# pylint: disable=g-importing-member\n",
        "# pylint: disable=logging-fstring-interpolation\n",
        "# pylint: disable=f-string-without-interpolation\n",
        "\n",
        "from abc import ABC\n",
        "from abc import abstractmethod\n",
        "import argparse\n",
        "import asyncio\n",
        "from collections.abc import AsyncGenerator\n",
        "import dataclasses\n",
        "from dataclasses import dataclass\n",
        "from dataclasses import field\n",
        "from datetime import datetime\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "import traceback\n",
        "from typing import Any, Optional\n",
        "\n",
        "import aiohttp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tenacity import RetryCallState, retry, stop_after_attempt, wait_exponential\n",
        "from tqdm.asyncio import tqdm\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "CLIENT_TIMEOUT_SEC = 3 * 60 * 60\n",
        "AIOHTTP_TIMEOUT = aiohttp.ClientTimeout(total=CLIENT_TIMEOUT_SEC)\n",
        "\n",
        "\n",
        "class BaseTokenizer(ABC):\n",
        "    \"\"\"Abstract class for tokenizers.\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def encode(self, text: str, add_special_tokens: bool = True) -> list[int]:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def decode(self, token_ids: list[int]) -> str:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def apply_chat_template(\n",
        "        self,\n",
        "        message: list[dict[str, Any]],\n",
        "        add_generation_prompt: bool = True,\n",
        "        tokenize: bool = False,\n",
        "    ) -> str:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def all_special_ids(self) -> list[int]:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_vocab(self) -> dict[str, int]:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def bos_token(self) -> str:\n",
        "        pass\n",
        "\n",
        "\n",
        "class Llama3Tokenizer(BaseTokenizer):\n",
        "    \"\"\"Llama3 specific tokenizer, based on Tiktoken.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer_path: str):\n",
        "        from saxml.server.pax.lm import vocabularies  # pylint: disable=g-import-not-at-top\n",
        "\n",
        "        self._tokenizer = vocabularies.LLama3Vocabulary(tokenizer_path)\n",
        "\n",
        "    def encode(self, text: str, add_special_tokens: bool = True) -> list[int]:\n",
        "        del add_special_tokens\n",
        "        return list(self._tokenizer.encode(text))\n",
        "\n",
        "    def decode(self, token_ids: list[int]) -> str:\n",
        "        return self._tokenizer.decode(token_ids)\n",
        "\n",
        "    def apply_chat_template(\n",
        "        self,\n",
        "        message: list[dict[str, Any]],\n",
        "        add_generation_prompt: bool = True,\n",
        "        tokenize: bool = False,\n",
        "    ) -> str:\n",
        "        del add_generation_prompt, tokenize, message\n",
        "        # This is not required for the servomatic backend.\n",
        "        # The formatted prompt is ignored and regular prompt is used.\n",
        "        logging.debug(\"apply_chat_template is not supported for Llama3Tokenizer.\")\n",
        "        return \"\"\n",
        "\n",
        "    def all_special_ids(self) -> list[int]:\n",
        "        raise NotImplementedError(\"Not implemented for Llama3Tokenizer.\")\n",
        "\n",
        "    def get_vocab(self) -> dict[str, int]:\n",
        "        raise NotImplementedError(\"Not implemented for Llama3Tokenizer.\")\n",
        "\n",
        "    def bos_token(self) -> str:\n",
        "        raise NotImplementedError(\"Not implemented for Llama3Tokenizer.\")\n",
        "\n",
        "\n",
        "class GeneralTokenizer(BaseTokenizer):\n",
        "    \"\"\"General tokenizer, based on transformers.AutoTokenizer, used for OSS runs.\"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer_path: str, trust_remote_code: bool = False):\n",
        "        logging.info(\"GeneralTokenizer: tokenizer_path: %s\", tokenizer_path)\n",
        "        self._tokenizer = AutoTokenizer.from_pretrained(\n",
        "            tokenizer_path, trust_remote_code=trust_remote_code\n",
        "        )\n",
        "\n",
        "    def encode(self, text: str, add_special_tokens: bool = True) -> list[int]:\n",
        "        return list(\n",
        "            self._tokenizer.encode(text, add_special_tokens=add_special_tokens)\n",
        "        )\n",
        "\n",
        "    def decode(self, token_ids: list[int]) -> str:\n",
        "        return self._tokenizer.decode(token_ids)\n",
        "\n",
        "    def apply_chat_template(\n",
        "        self,\n",
        "        message: list[dict[str, Any]],\n",
        "        add_generation_prompt: bool = True,\n",
        "        tokenize: bool = False,\n",
        "    ) -> str:\n",
        "        return self._tokenizer.apply_chat_template(\n",
        "            message, add_generation_prompt=add_generation_prompt, tokenize=tokenize\n",
        "        )\n",
        "\n",
        "    def all_special_ids(self) -> list[int]:\n",
        "        return self._tokenizer.all_special_ids\n",
        "\n",
        "    def get_vocab(self) -> dict[str, int]:\n",
        "        return self._tokenizer.get_vocab()\n",
        "\n",
        "    def bos_token(self) -> str:\n",
        "        return self._tokenizer.bos_token\n",
        "\n",
        "\n",
        "def str2bool(v: str) -> Optional[bool]:\n",
        "    if v is None:\n",
        "        return None\n",
        "    if isinstance(v, bool):\n",
        "        return v\n",
        "    if v.lower() in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\n",
        "        return True\n",
        "    elif v.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n",
        "        return False\n",
        "    else:\n",
        "        raise argparse.ArgumentTypeError(\"Boolean value expected.\")\n",
        "\n",
        "\n",
        "def sample_sonnet_requests(\n",
        "    dataset_path: str,\n",
        "    num_requests: int,\n",
        "    min_input_len: int,\n",
        "    max_input_len: int,\n",
        "    min_output_len: int,\n",
        "    max_output_len: int,\n",
        "    prefix_len: int,\n",
        "    tokenizer: BaseTokenizer,\n",
        "    fixed_input_length: Optional[int] = None,\n",
        "    fixed_output_length: Optional[int] = None,\n",
        ") -> list[tuple[str, str, int, int, int]]:\n",
        "    \"\"\"Samples requests from the Sonnet dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset_path: Path to the Sonnet dataset.\n",
        "        num_requests: Number of requests to sample.\n",
        "        min_input_len: Minimum input length.\n",
        "        max_input_len: Maximum input length.\n",
        "        min_output_len: Minimum output length.\n",
        "        max_output_len: Maximum output length.\n",
        "        prefix_len: Number of prefix tokens per request.\n",
        "        tokenizer: Tokenizer to use.\n",
        "        fixed_input_length: If specified, forces input_len to be fixed_input_length.\n",
        "        fixed_output_length: If specified, forces output_len to be\n",
        "            fixed_output_length.\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples containing the prompt, formatted prompt, prompt length,\n",
        "        formatted prompt length, and output length.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the dataset.\n",
        "    with open(dataset_path) as f:\n",
        "        poem_lines = f.readlines()\n",
        "    poem_lines = poem_lines * 100\n",
        "\n",
        "    # Tokenize the poem lines.\n",
        "    poem_token_ids = [tokenizer.encode(poem_line) for poem_line in poem_lines]\n",
        "    average_poem_len = sum(len(token_ids) for token_ids in poem_token_ids) / len(\n",
        "        poem_token_ids\n",
        "    )\n",
        "\n",
        "    # Base prefix for all requests.\n",
        "    if dataset_path.endswith(\"code-sonnet.txt\"):\n",
        "        base_prompt = (\n",
        "            \"Repeated pick as many questions from each line and write the answer to\"\n",
        "            \" each question infinitly.\\n\"\n",
        "        )\n",
        "    else:\n",
        "        base_prompt = \"Pick as many lines as you can from these poem lines:\\n\"\n",
        "    base_message = [{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": base_prompt,\n",
        "    }]\n",
        "    base_prompt_formatted = tokenizer.apply_chat_template(\n",
        "        base_message, add_generation_prompt=True, tokenize=False\n",
        "    )\n",
        "    base_prompt_offset = len(tokenizer.encode(base_prompt_formatted))\n",
        "\n",
        "    logging.info(\"prefix_len: %s\", prefix_len)\n",
        "    logging.info(\"base_prompt_offset: %s\", base_prompt_offset)\n",
        "    logging.info(\"base_prompt_formatted: %s\", base_prompt_formatted)\n",
        "    logging.info(\n",
        "        \"base_prompt_formatted.input_ids: %s\",\n",
        "        tokenizer.encode(base_prompt_formatted),\n",
        "    )\n",
        "\n",
        "    # First approximately `prefix_len` number of tokens in the\n",
        "    # prompt are fixed poem lines.\n",
        "    assert (\n",
        "        prefix_len > base_prompt_offset\n",
        "    ), f\"Set 'args.sonnet-prefix-len' higher than {base_prompt_offset}.\"\n",
        "\n",
        "    num_prefix_lines = round((prefix_len - base_prompt_offset) / average_poem_len)\n",
        "    prefix_lines = poem_lines[:num_prefix_lines]\n",
        "\n",
        "    # Sample the rest of lines per request.\n",
        "    sampled_requests: list[tuple[str, str, int, int, int]] = []\n",
        "    for _ in range(num_requests):\n",
        "        if fixed_input_length:\n",
        "            input_len = fixed_input_length\n",
        "        else:\n",
        "            input_len = (\n",
        "                random.randrange(min_input_len, max_input_len)\n",
        "                if max_input_len > min_input_len\n",
        "                else min_input_len\n",
        "            )\n",
        "        assert (\n",
        "            input_len > prefix_len\n",
        "        ), \"'args.sonnet-input-len' must be greater than 'args.prefix-input-len'.\"\n",
        "        assert (\n",
        "            input_len > base_prompt_offset\n",
        "        ), f\"Set 'args.sonnet-input-len' higher than {base_prompt_offset}.\"\n",
        "        num_input_lines = round((input_len - base_prompt_offset) / average_poem_len)\n",
        "\n",
        "        if fixed_output_length:\n",
        "            output_len = fixed_output_length\n",
        "        else:\n",
        "            output_len = (\n",
        "                random.randrange(min_output_len, max_output_len)\n",
        "                if max_output_len > min_output_len\n",
        "                else min_output_len\n",
        "            )\n",
        "\n",
        "        sampled_lines = \"\".join(\n",
        "            prefix_lines\n",
        "            + random.sample(poem_lines, num_input_lines - num_prefix_lines)\n",
        "        )\n",
        "\n",
        "        prompt = f\"{base_prompt}{sampled_lines}\"\n",
        "        message = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            },\n",
        "        ]\n",
        "        prompt_formatted = tokenizer.apply_chat_template(\n",
        "            message, add_generation_prompt=True, tokenize=False\n",
        "        )\n",
        "\n",
        "        prompt_len = len(tokenizer.encode(prompt))\n",
        "        prompt_formatted_len = len(tokenizer.encode(prompt_formatted))\n",
        "        sampled_requests.append(\n",
        "            (prompt, prompt_formatted, prompt_len, prompt_formatted_len, output_len)\n",
        "        )\n",
        "\n",
        "    return sampled_requests\n",
        "\n",
        "\n",
        "async def get_request(\n",
        "    input_requests: list[tuple[str, int, int]],\n",
        ") -> AsyncGenerator[tuple[str, int, int], None]:\n",
        "    \"\"\"Gets request async.\"\"\"\n",
        "    input_requests = iter(input_requests)\n",
        "    for request in input_requests:\n",
        "        yield request\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class RequestFuncInput:\n",
        "    \"\"\"Input to the request function.\n",
        "\n",
        "    Attributes:\n",
        "        backend: Backend to benchmark.\n",
        "        api_url: The API URL to send the request to.\n",
        "        prompt: The prompt to send to the model.\n",
        "        prompt_len: The length of the prompt.\n",
        "        output_len: Expected output length.\n",
        "        enable_retry: Whether to enable retry on failure.\n",
        "        model: Model name.\n",
        "        extra_body: Extra body to send in the request.\n",
        "        max_context_length: Maximum context length.\n",
        "    \"\"\"\n",
        "\n",
        "    backend: str = \"\"\n",
        "    api_url: str = \"\"\n",
        "    prompt: str = \"\"\n",
        "    prompt_len: int = 0\n",
        "    output_len: int = 0\n",
        "    enable_retry: bool = False\n",
        "    model: str = \"\"\n",
        "    extra_body: str | dict[str, Any] | None = None\n",
        "    max_context_length: Optional[int] = None\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class RequestFuncOutput:\n",
        "    \"\"\"Output of the request function.\n",
        "\n",
        "    Attributes:\n",
        "        backend: Backend to benchmark.\n",
        "        model: Model name.\n",
        "        generated_text: Generated text in case of non-servomatic.\n",
        "        generated_token_ids: List of generated token ids in case of servomatic and\n",
        "            evergreen.\n",
        "        success: Whether the request was successful.\n",
        "        start_time: Timestamp when the request was sent.\n",
        "        latency: total request latency\n",
        "        prompt_len: input prompt length\n",
        "        error: Error message if any\n",
        "        ttft: Time to first token\n",
        "        itl: Inter-token latencies\n",
        "        requested_output_len:\n",
        "    \"\"\"\n",
        "\n",
        "    backend: str = \"\"\n",
        "    model: str = \"\"\n",
        "    generated_text: str = \"\"\n",
        "    generated_token_ids: Optional[list[int]] = None\n",
        "    success: bool = False\n",
        "    start_time: float = 0.0\n",
        "    latency: float = 0.0\n",
        "    prompt_len: int = 0\n",
        "    error: str = \"\"\n",
        "    ttft: Optional[float] = None  # Time to first token\n",
        "    itl: list[float] = field(\n",
        "        default_factory=list\n",
        "    )  # List of inter-token latencies\n",
        "    requested_output_len: Optional[int] = None\n",
        "\n",
        "\n",
        "def get_api_key() -> str:\n",
        "    \"\"\"Get the API key for the given request_input.\"\"\"\n",
        "    api_key = os.environ.get(\"OPENAI_API_KEY\", os.environ.get(\"API_KEY\", \"\"))\n",
        "    return api_key\n",
        "\n",
        "\n",
        "def create_retry_predicate(enable_retry: bool):\n",
        "    \"\"\"Create a retry gate.\"\"\"\n",
        "\n",
        "    def retry_if_status_is_429(retry_state: RetryCallState) -> bool:\n",
        "        \"\"\"Retry if the status is 429.\"\"\"\n",
        "        assert retry_state.outcome is not None\n",
        "\n",
        "        if not enable_retry:\n",
        "            return False\n",
        "        exception = retry_state.outcome.exception()\n",
        "        return (\n",
        "            isinstance(exception, aiohttp.ClientResponseError)\n",
        "            and exception.status == 429  # pytype: disable=attribute-error\n",
        "        )\n",
        "\n",
        "    return retry_if_status_is_429\n",
        "\n",
        "\n",
        "async def make_chat_completions_request(\n",
        "    session: aiohttp.ClientSession,\n",
        "    headers: dict[str, str],\n",
        "    request_input: RequestFuncInput,\n",
        "    payload: dict[str, Any],\n",
        "    stream: Optional[bool] = True,\n",
        "    ttft: float = 0.0,\n",
        "    most_recent_timestamp: float = 0.0,\n",
        "    generated_text: str = \"\",\n",
        "    output: RequestFuncOutput = RequestFuncOutput(),\n",
        ") -> RequestFuncOutput:\n",
        "    \"\"\"Make a chat completions request.\"\"\"\n",
        "    st = time.perf_counter()  # Reset st for each retry.\n",
        "    async with session.post(\n",
        "        url=request_input.api_url, json=payload, headers=headers\n",
        "    ) as response:\n",
        "        if response.status == 200:\n",
        "            output.success = True\n",
        "            async for chunk_bytes in response.content:\n",
        "                chunk_bytes = chunk_bytes.strip()\n",
        "                if not chunk_bytes:\n",
        "                    continue\n",
        "\n",
        "                chunk = chunk_bytes.decode(\"utf-8\").removeprefix(\"data:\").strip()\n",
        "                logging.debug(\"chunk: %s\", chunk)\n",
        "                if chunk != \"[DONE]\":\n",
        "                    try:\n",
        "                        data = json.loads(chunk)\n",
        "                    except json.decoder.JSONDecodeError:\n",
        "                        logging.error(f\"Failed to parse response chunk: {chunk}\")\n",
        "                        output.success = False\n",
        "                        continue\n",
        "                    timestamp = time.perf_counter()\n",
        "                    if \"choices\" not in data or not data[\"choices\"]:\n",
        "                        logging.info(\"empty chunk: %s\", chunk)\n",
        "                        continue\n",
        "                    if stream:\n",
        "                        if \"delta\" not in data[\"choices\"][0]:\n",
        "                            logging.info(\"empty delta in chunk: %s\", chunk)\n",
        "                            continue\n",
        "                        delta = data[\"choices\"][0][\"delta\"]\n",
        "                        tag = \"content\"\n",
        "                        if not delta.get(\"content\", None):\n",
        "                            tag = \"reasoning_content\"\n",
        "                        if delta.get(tag, None):\n",
        "                            # First token\n",
        "                            if ttft == 0.0:\n",
        "                                ttft = time.perf_counter() - st\n",
        "                                output.ttft = ttft\n",
        "\n",
        "                            # Decoding phase\n",
        "                            else:\n",
        "                                output.itl.append(timestamp - most_recent_timestamp)\n",
        "                            generated_text += delta[tag]\n",
        "                    else:\n",
        "                        assert not generated_text\n",
        "                        if \"message\" not in data[\"choices\"][0]:\n",
        "                            logging.info(\"empty message in chunk: %s\", chunk)\n",
        "                            continue\n",
        "                        if \"content\" not in data[\"choices\"][0][\"message\"]:\n",
        "                            logging.info(\"empty message.content in chunk: %s\", chunk)\n",
        "                            continue\n",
        "                        generated_text = data[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "                    most_recent_timestamp = timestamp\n",
        "\n",
        "            if not generated_text:\n",
        "                logging.error(\"Received empty response\")\n",
        "                output.success = False\n",
        "            output.generated_text = generated_text\n",
        "            output.latency = time.perf_counter() - st\n",
        "        else:\n",
        "            if response.content_type == \"application/json\":\n",
        "                try:\n",
        "                    response_json = await response.json()\n",
        "                    logging.error(\n",
        "                        \"Error from Server (JSON):\\n\"\n",
        "                        f\"{json.dumps(response_json, indent=2)}\"\n",
        "                    )\n",
        "                except aiohttp.ContentTypeError:\n",
        "                    logging.error(\"Response body expected JSON but failed to parse.\")\n",
        "                    logging.error(f\"Raw response text: {await response.text()}\")\n",
        "            else:\n",
        "                logging.error(\n",
        "                    f\"Response Content-Type is {response.content_type}. Reading\"\n",
        "                    \" as text.\"\n",
        "                )\n",
        "                logging.error(f\"Raw response text: {await response.text()}\")\n",
        "            response.raise_for_status()\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "async def send_chat_completions_request(\n",
        "    request_input: RequestFuncInput,\n",
        "    sem: asyncio.Semaphore,\n",
        "    pbar: Optional[tqdm] = None,\n",
        "    stream: Optional[bool] = True,\n",
        "    ignore_eos: bool = True,\n",
        ") -> RequestFuncOutput:\n",
        "    \"\"\"Sends a streaming request to OpenAI Chat Completions API.\"\"\"\n",
        "    assert request_input.api_url.endswith(\n",
        "        \"chat/completions\"\n",
        "    ), \"OpenAI Chat Completions API URL must end with 'chat/completions'.\"\n",
        "\n",
        "    if stream is None:\n",
        "        stream = True  # defaults to True\n",
        "\n",
        "    async with sem:\n",
        "        async with aiohttp.ClientSession(timeout=AIOHTTP_TIMEOUT) as session:\n",
        "            content = request_input.prompt\n",
        "\n",
        "            payload = {\n",
        "                \"model\": request_input.model,\n",
        "                \"messages\": [\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": content,\n",
        "                    },\n",
        "                ],\n",
        "                \"temperature\": 0.0,\n",
        "                \"max_tokens\": request_input.output_len,\n",
        "                \"stream\": stream,\n",
        "                \"ignore_eos\": ignore_eos,\n",
        "            }\n",
        "\n",
        "            output = RequestFuncOutput()\n",
        "            output.backend = request_input.backend\n",
        "            output.model = request_input.model\n",
        "            output.prompt_len = request_input.prompt_len\n",
        "            output.requested_output_len = request_input.output_len\n",
        "\n",
        "            if request_input.extra_body:\n",
        "                payload[\"extra_body\"] = request_input.extra_body\n",
        "            api_key = get_api_key()\n",
        "            headers = {\n",
        "                \"Content-Type\": \"application/json\",\n",
        "            }\n",
        "\n",
        "            if api_key:\n",
        "                headers[\"Authorization\"] = f\"Bearer {api_key}\"\n",
        "\n",
        "            generated_text = \"\"\n",
        "            ttft = 0.0\n",
        "            st = time.perf_counter()\n",
        "            most_recent_timestamp = st\n",
        "            output.start_time = time.time()\n",
        "            try:\n",
        "                logging.debug(\"request: %s\", json.dumps(payload, indent=2))\n",
        "                retry_decorator = retry(\n",
        "                    stop=stop_after_attempt(8),\n",
        "                    wait=wait_exponential(\n",
        "                        multiplier=1, min=2, max=1000\n",
        "                    ),  # Wait 2s, then 4s, 8s, ...\n",
        "                    retry=create_retry_predicate(request_input.enable_retry),\n",
        "                )\n",
        "                output = await retry_decorator(make_chat_completions_request)(\n",
        "                    session,\n",
        "                    headers,\n",
        "                    request_input,\n",
        "                    payload,\n",
        "                    stream,\n",
        "                    ttft,\n",
        "                    most_recent_timestamp,\n",
        "                    generated_text,\n",
        "                    output,\n",
        "                )\n",
        "            except Exception:  # pylint: disable=broad-except\n",
        "                output.success = False\n",
        "                exc_info = sys.exc_info()\n",
        "                output.error = \"\".join(traceback.format_exception(*exc_info))\n",
        "                logging.warning(output.error)\n",
        "\n",
        "            if pbar:\n",
        "                pbar.update(1)\n",
        "            return output\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class BenchmarkMetrics:\n",
        "    \"\"\"Aggregated metrics for a benchmark run.\"\"\"\n",
        "\n",
        "    requested: int\n",
        "    completed: int\n",
        "    total_input: int\n",
        "    total_output: int\n",
        "    request_throughput: float\n",
        "    input_throughput: float\n",
        "    output_throughput: float\n",
        "    mean_ttft_ms: Optional[float]\n",
        "    median_ttft_ms: Optional[float]\n",
        "    p99_ttft_ms: Optional[float]\n",
        "    mean_tpot_ms: Optional[float]\n",
        "    median_tpot_ms: Optional[float]\n",
        "    p99_tpot_ms: Optional[float]\n",
        "    mean_latency_ms: Optional[float]\n",
        "    median_latency_ms: Optional[float]\n",
        "    p99_latency_ms: Optional[float]\n",
        "    accept_length: Optional[float]\n",
        "\n",
        "\n",
        "def calculate_metrics(\n",
        "    outputs: list[RequestFuncOutput],\n",
        "    duration_sec: float,\n",
        "    tokenizer: BaseTokenizer,\n",
        ") -> tuple[BenchmarkMetrics, pd.DataFrame]:\n",
        "    \"\"\"Calculates the aggregated metrics for a benchmark run.\n",
        "\n",
        "    Args:\n",
        "        outputs: Benchmark outputs.\n",
        "        duration_sec: Duration of the benchmark run.\n",
        "        tokenizer: Tokenizer used for the benchmark.\n",
        "\n",
        "    Returns:\n",
        "        A BenchmarkMetrics.\n",
        "        A dataframe with the detailed per-request benchmark results.\n",
        "    \"\"\"\n",
        "    actual_output_lens = []\n",
        "    total_input = 0\n",
        "    completed = 0\n",
        "    results = []\n",
        "    tpots = []\n",
        "    ttfts = []\n",
        "    latencies = []\n",
        "    accept_lens = []\n",
        "\n",
        "    for i in range(len(outputs)):\n",
        "        dt = dataclasses.asdict(outputs[i])\n",
        "        if outputs[i].success:\n",
        "            dt.pop(\"generated_text\")\n",
        "            dt.pop(\"generated_token_ids\")\n",
        "\n",
        "            if outputs[i].generated_token_ids:\n",
        "                output_len = len(outputs[i].generated_token_ids)\n",
        "            else:\n",
        "                output_len = len(tokenizer.encode(outputs[i].generated_text))\n",
        "            if output_len != outputs[i].requested_output_len:\n",
        "                logging.debug(\n",
        "                    \"Output length mismatch: requested len: %d vs actual len:%d\",\n",
        "                    outputs[i].requested_output_len,\n",
        "                    output_len,\n",
        "                )\n",
        "            if \"itl\" in dt and len(dt[\"itl\"]) != output_len and dt[\"itl\"]:\n",
        "                accept_lens.append(output_len / len(dt[\"itl\"]))\n",
        "\n",
        "            if outputs[i].backend == \"vllm\" or outputs[i].backend == \"vllm_stream\":\n",
        "                output_len -= outputs[i].prompt_len\n",
        "            dt[\"output_len\"] = output_len\n",
        "            actual_output_lens.append(output_len)\n",
        "            total_input += outputs[i].prompt_len\n",
        "            completed += 1\n",
        "            latencies.append(outputs[i].latency)\n",
        "            if outputs[i].ttft:\n",
        "                if output_len > 1:\n",
        "                    tpots.append(\n",
        "                        (outputs[i].latency - outputs[i].ttft) / (output_len - 1)\n",
        "                    )\n",
        "                ttfts.append(outputs[i].ttft)\n",
        "        else:\n",
        "            dt[\"output_len\"] = 0\n",
        "            actual_output_lens.append(0)\n",
        "        results.append(dt)\n",
        "\n",
        "    metrics = BenchmarkMetrics(\n",
        "        # number of requested requests\n",
        "        requested=len(outputs),\n",
        "        # number of successful requests\n",
        "        completed=completed,\n",
        "        # sum of input prompts length\n",
        "        total_input=total_input,\n",
        "        # sum of output length\n",
        "        total_output=sum(actual_output_lens),\n",
        "        # throughput requests / sec\n",
        "        request_throughput=completed / duration_sec,\n",
        "        # input throughput input tokens / sec\n",
        "        input_throughput=total_input / duration_sec,\n",
        "        # output throughtput output tokens / sec\n",
        "        output_throughput=sum(actual_output_lens) / duration_sec,\n",
        "        mean_ttft_ms=np.mean(ttfts or 0) * 1000 if ttfts else None,\n",
        "        median_ttft_ms=np.median(ttfts or 0) * 1000 if ttfts else None,\n",
        "        p99_ttft_ms=np.percentile(ttfts or 0, 99) * 1000 if ttfts else None,\n",
        "        mean_tpot_ms=np.mean(tpots) * 1000 if tpots else None,\n",
        "        median_tpot_ms=np.median(tpots) * 1000 if tpots else None,\n",
        "        p99_tpot_ms=np.percentile(tpots, 99) * 1000 if tpots else None,\n",
        "        mean_latency_ms=np.mean(latencies or 0) * 1000 if latencies else None,\n",
        "        median_latency_ms=np.median(latencies or 0) * 1000 if latencies else None,\n",
        "        p99_latency_ms=np.percentile(latencies or 0, 99) * 1000\n",
        "        if latencies\n",
        "        else None,\n",
        "        accept_length=np.mean(accept_lens) if accept_lens else None,\n",
        "    )\n",
        "\n",
        "    return metrics, pd.DataFrame.from_dict(results)  # pytype: disable=wrong-arg-types\n",
        "\n",
        "\n",
        "async def benchmark(\n",
        "    args: argparse.Namespace,\n",
        "    api_urls: list[str],\n",
        "    input_requests: list[tuple[str, int, int]],\n",
        "    tokenizer: BaseTokenizer,\n",
        "    prefix: str,\n",
        "    max_input: int,\n",
        "    max_output: int,\n",
        "    concurrent_requests: Optional[int] = None,\n",
        "):\n",
        "    \"\"\"Runs benchmark with asynchronous requests.\"\"\"\n",
        "    print(\n",
        "        f\"Running benchmark for {args.backend}, max input: {max_input}, max\"\n",
        "        f\" output: {max_output}, concurrent requests: {concurrent_requests},\"\n",
        "        f\" request rate: {args.request_rate}, fixed qps: {args.fixed_qps}\"\n",
        "    )\n",
        "\n",
        "    tasks: list[asyncio.Task] = []\n",
        "    pbar = tqdm(total=len(input_requests))\n",
        "\n",
        "    benchmark_start_time = time.perf_counter()\n",
        "    start_time = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
        "    sem = (\n",
        "        asyncio.Semaphore(concurrent_requests)\n",
        "        if concurrent_requests\n",
        "        else asyncio.Semaphore(len(input_requests))\n",
        "    )\n",
        "    async for request in get_request(\n",
        "        input_requests,\n",
        "    ):\n",
        "        prompt, prompt_len, output_len = request\n",
        "        request_extra_body = None\n",
        "        if args.request_extra_body is not None:\n",
        "            try:\n",
        "                request_extra_body = json.loads(args.request_extra_body)\n",
        "            except json.decoder.JSONDecodeError:\n",
        "                request_extra_body = args.request_extra_body\n",
        "        api_url = random.choice(api_urls)\n",
        "        logging.debug(\"api url: %s\", api_url)\n",
        "        request_input = RequestFuncInput(\n",
        "            backend=args.backend,\n",
        "            api_url=api_url,\n",
        "            prompt=prompt,\n",
        "            prompt_len=prompt_len,\n",
        "            output_len=output_len,\n",
        "            enable_retry=args.enable_retry,\n",
        "            model=args.model,\n",
        "            extra_body=request_extra_body,\n",
        "            max_context_length=args.max_context_length,\n",
        "        )\n",
        "        if args.backend == \"chat_completions\":\n",
        "            request_func = send_chat_completions_request\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported backend: {args.backend}\")\n",
        "        task = asyncio.create_task(\n",
        "            request_func(\n",
        "                request_input,\n",
        "                sem,\n",
        "                pbar,\n",
        "                args.stream,\n",
        "                args.fixed_output_length,\n",
        "            )\n",
        "        )\n",
        "        if args.fixed_qps is not None:\n",
        "            # await here would force task to start when running in fixed_qps mode.\n",
        "            await asyncio.sleep(1.0 / args.fixed_qps)\n",
        "        tasks.append(task)\n",
        "    outputs: list[RequestFuncOutput] = await asyncio.gather(*tasks)\n",
        "    duration_sec = time.perf_counter() - benchmark_start_time\n",
        "    end_time = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
        "\n",
        "    if pbar is not None:\n",
        "        pbar.close()\n",
        "\n",
        "    metrics, full_results = calculate_metrics(outputs, duration_sec, tokenizer)\n",
        "    if concurrent_requests:\n",
        "        full_results = full_results.assign(\n",
        "            concurrent_requests=concurrent_requests,\n",
        "        )\n",
        "    elif args.fixed_qps:\n",
        "        full_results = full_results.assign(\n",
        "            fixed_qps=args.fixed_qps,\n",
        "        )\n",
        "    else:\n",
        "        full_results = full_results.assign(\n",
        "            request_rate=args.request_rate,\n",
        "        )\n",
        "\n",
        "    full_results = full_results.assign(\n",
        "        max_input=max_input,\n",
        "        max_output=max_output,\n",
        "    )\n",
        "\n",
        "    if args.save_full_results:\n",
        "        f = open(\n",
        "            os.path.join(\n",
        "                args.output_dir if args.output_dir else os.getcwd(),\n",
        "                f\"{prefix}_full_results.json\",\n",
        "            ),\n",
        "            mode=\"a\",\n",
        "        )\n",
        "        f.write(full_results.to_json(orient=\"records\", lines=True))\n",
        "        f.close()\n",
        "\n",
        "    print(\"{s:{c}^{n}}\".format(s=\" Serving Benchmark Result \", n=50, c=\"=\"))\n",
        "    print(\"{:<40} {:<10}\".format(\"Total requests:\", metrics.requested))\n",
        "    print(\"{:<40} {:<10}\".format(\"Successful requests:\", metrics.completed))\n",
        "    print(\"{:<40} {:<10.2f}\".format(\"Benchmark duration (s):\", duration_sec))\n",
        "    print(\"{:<40} {:<10}\".format(\"Total input tokens:\", metrics.total_input))\n",
        "    print(\"{:<40} {:<10}\".format(\"Total generated tokens:\", metrics.total_output))\n",
        "    print(\n",
        "        \"{:<40} {:<10}\".format(\n",
        "            \"Average input length:\", metrics.total_input / metrics.completed\n",
        "        )\n",
        "    )\n",
        "    print(\n",
        "        \"{:<40} {:<10}\".format(\n",
        "            \"Average output length:\", metrics.total_output / metrics.completed\n",
        "        )\n",
        "    )\n",
        "    print(\n",
        "        \"{:<40} {:<10.3f}\".format(\n",
        "            \"Request throughput (req/s):\", metrics.request_throughput\n",
        "        )\n",
        "    )\n",
        "    print(\n",
        "        \"{:<40} {:<10.2f}\".format(\n",
        "            \"Input token throughput (tok/s):\", metrics.input_throughput\n",
        "        )\n",
        "    )\n",
        "    print(\n",
        "        \"{:<40} {:<10.2f}\".format(\n",
        "            \"Output token throughput (tok/s):\", metrics.output_throughput\n",
        "        )\n",
        "    )\n",
        "    if metrics.mean_ttft_ms:\n",
        "        print(\"{s:{c}^{n}}\".format(s=\"Time to First Token\", n=50, c=\"-\"))\n",
        "        print(\"{:<40} {:<10.2f}\".format(\"Mean TTFT (ms):\", metrics.mean_ttft_ms))\n",
        "        print(\n",
        "            \"{:<40} {:<10.2f}\".format(\"Median TTFT (ms):\", metrics.median_ttft_ms)\n",
        "        )\n",
        "        print(\"{:<40} {:<10.2f}\".format(\"P99 TTFT (ms):\", metrics.p99_ttft_ms))\n",
        "        if metrics.mean_tpot_ms:\n",
        "            print(\n",
        "                \"{s:{c}^{n}}\".format(\n",
        "                    s=\"Time per Output Token (excl. 1st token)\", n=50, c=\"-\"\n",
        "                )\n",
        "            )\n",
        "            print(\"{:<40} {:<10.2f}\".format(\"Mean TPOT (ms):\", metrics.mean_tpot_ms))\n",
        "            print(\n",
        "                \"{:<40} {:<10.2f}\".format(\"Median TPOT (ms):\", metrics.median_tpot_ms)\n",
        "            )\n",
        "            print(\"{:<40} {:<10.2f}\".format(\"P99 TPOT (ms):\", metrics.p99_tpot_ms))\n",
        "    if metrics.mean_latency_ms:\n",
        "        print(\"{s:{c}^{n}}\".format(s=\"Latencies\", n=50, c=\"-\"))\n",
        "        print(\n",
        "            \"{:<40} {:<10.2f}\".format(\"Mean Latency (ms):\", metrics.mean_latency_ms)\n",
        "        )\n",
        "        print(\n",
        "            \"{:<40} {:<10.2f}\".format(\n",
        "                \"Median Latency (ms):\", metrics.median_latency_ms\n",
        "            )\n",
        "        )\n",
        "        print(\n",
        "            \"{:<40} {:<10.2f}\".format(\"P99 Latency (ms):\", metrics.p99_latency_ms)\n",
        "        )\n",
        "    if metrics.accept_length:\n",
        "        print(\"{s:{c}^{n}}\".format(s=\"Accept Length\", n=50, c=\"-\"))\n",
        "        print(\n",
        "            \"{:<40} {:<10.2f}\".format(\n",
        "                \"Mean Accept Length (tokens):\", metrics.accept_length\n",
        "            )\n",
        "        )\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    result = {\n",
        "        \"backend\": args.backend,\n",
        "        \"start\": start_time,\n",
        "        \"end\": end_time,\n",
        "        \"duration\": duration_sec,\n",
        "        \"completed\": metrics.completed,\n",
        "        \"total_input_tokens\": metrics.total_input,\n",
        "        \"total_output_tokens\": metrics.total_output,\n",
        "        \"request_throughput\": metrics.request_throughput,\n",
        "        \"input_throughput\": metrics.input_throughput,\n",
        "        \"output_throughput\": metrics.output_throughput,\n",
        "        \"mean_latency_ms\": metrics.mean_latency_ms,\n",
        "        \"median_latency_ms\": metrics.median_latency_ms,\n",
        "        \"p99_latency_ms\": metrics.p99_latency_ms,\n",
        "    }\n",
        "    if metrics.mean_ttft_ms:\n",
        "        result |= {\n",
        "            \"mean_ttft_ms\": metrics.mean_ttft_ms,\n",
        "            \"median_ttft_ms\": metrics.median_ttft_ms,\n",
        "            \"p99_ttft_ms\": metrics.p99_ttft_ms,\n",
        "            \"mean_tpot_ms\": metrics.mean_tpot_ms,\n",
        "            \"median_tpot_ms\": metrics.median_tpot_ms,\n",
        "            \"p99_tpot_ms\": metrics.p99_tpot_ms,\n",
        "        }\n",
        "    if metrics.accept_length:\n",
        "        result |= {\n",
        "            \"accept_length\": metrics.accept_length,\n",
        "        }\n",
        "    return result\n",
        "\n",
        "\n",
        "def main(args: argparse.Namespace):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "\n",
        "    log_levels = {\n",
        "        \"debug\": logging.DEBUG,\n",
        "        \"info\": logging.INFO,\n",
        "        \"warning\": logging.WARNING,\n",
        "        \"error\": logging.ERROR,\n",
        "        \"critical\": logging.CRITICAL,\n",
        "    }\n",
        "\n",
        "    # Configure the logging\n",
        "    logging.basicConfig(level=log_levels[args.verbosity])\n",
        "\n",
        "    endpoint = args.endpoint\n",
        "    if not args.endpoint:\n",
        "        if args.backend == \"chat_completions\":\n",
        "            endpoint = \"v1/chat/completions\"\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported backend: {args.backend}\")\n",
        "\n",
        "    port_str = \":\" + str(args.port) if args.port else \"\"\n",
        "    protocol = \"\" if args.host.startswith(\"http\") else \"http://\"\n",
        "    base_api_url = f\"{protocol}{args.host}{port_str}\"\n",
        "    api_url = f\"{base_api_url}/{endpoint}\"\n",
        "\n",
        "    api_urls = []\n",
        "    if args.endpoints:\n",
        "        with open(args.endpoints, \"r\") as f:\n",
        "            endpoints = f.readlines()\n",
        "            for endpoint in endpoints:\n",
        "                endpoint = endpoint.strip()\n",
        "                api_url = f\"{base_api_url}/{endpoint}\"\n",
        "                logging.debug(\"api url added to list: %s\", api_url)\n",
        "                api_urls.append(f\"{api_url}\")\n",
        "    else:\n",
        "        logging.debug(\"api url added to list: %s\", api_url)\n",
        "        api_urls.append(api_url)\n",
        "\n",
        "    if args.tokenizer_type == \"llama3\":\n",
        "        tokenizer = Llama3Tokenizer(args.tokenizer)\n",
        "    else:\n",
        "        tokenizer = GeneralTokenizer(args.tokenizer, args.trust_remote_code)\n",
        "\n",
        "    prefix = args.name if args.name else args.backend\n",
        "    fname = os.path.join(\n",
        "        args.output_dir if args.output_dir else os.getcwd(),\n",
        "        f\"{prefix}_aggregated_results.json\",\n",
        "    )\n",
        "\n",
        "    logging.info(\"preparing requests\")\n",
        "    for max_input in args.max_input_length:\n",
        "        for max_output in args.max_output_length:\n",
        "            if args.dataset.endswith(\"sonnet.txt\"):\n",
        "                min_input_len = int(max_input / 2)\n",
        "                max_input_len = max_input + min_input_len\n",
        "                min_output_len = int(max_output / 2)\n",
        "                max_output_len = max_output + min_output_len\n",
        "                input_requests = sample_sonnet_requests(\n",
        "                    dataset_path=args.dataset,\n",
        "                    num_requests=args.num_prompts,\n",
        "                    min_input_len=min_input_len,\n",
        "                    max_input_len=max_input_len,\n",
        "                    min_output_len=min_output_len,\n",
        "                    max_output_len=max_output_len,\n",
        "                    prefix_len=args.sonnet_prefix_len,\n",
        "                    tokenizer=tokenizer,\n",
        "                    fixed_input_length=(max_input if args.fixed_input_length else None),\n",
        "                    fixed_output_length=(\n",
        "                        max_output if args.fixed_output_length else None\n",
        "                    ),\n",
        "                )\n",
        "                if args.backend == \"chat_completions\":\n",
        "                    input_requests = [\n",
        "                        (prompt, prompt_len, output_len)\n",
        "                        for prompt, _, prompt_len, _, output_len in input_requests\n",
        "                    ]\n",
        "                else:\n",
        "                    raise ValueError(\"Unsupported backend: %s\" % args.backend)\n",
        "            else:\n",
        "                raise ValueError(\n",
        "                    f\"Unsupported dataset: {args.dataset}. Expected sonnet.txt.\"\n",
        "                )\n",
        "\n",
        "            logging.info(\"staring benchmark\")\n",
        "            c_list = args.c\n",
        "            if c_list is None:\n",
        "                c_list = [None]\n",
        "            for concurrent_requests in c_list:\n",
        "                results = asyncio.run(\n",
        "                    benchmark(\n",
        "                        args,\n",
        "                        api_urls,\n",
        "                        input_requests,\n",
        "                        tokenizer,\n",
        "                        prefix,\n",
        "                        max_input,\n",
        "                        max_output,\n",
        "                        concurrent_requests,\n",
        "                    )\n",
        "                )\n",
        "                print(f\"results: {results}\")\n",
        "\n",
        "                bm_configs = dict(vars(args).copy())\n",
        "                bm_configs.pop(\"save_full_results\")\n",
        "                bm_configs.pop(\"c\")\n",
        "                bm_configs.pop(\"max_input_length\")\n",
        "                bm_configs.pop(\"max_output_length\")\n",
        "                bm_configs[\"max_input_len\"] = max_input\n",
        "                bm_configs[\"max_output_len\"] = max_output\n",
        "                if concurrent_requests is not None:\n",
        "                    bm_configs[\"concurrent_requests\"] = concurrent_requests\n",
        "                    bm_configs.pop(\"request_rate\")\n",
        "                    bm_configs.pop(\"fixed_qps\")\n",
        "                results = results | bm_configs\n",
        "                df = pd.DataFrame([results])\n",
        "                f = open(fname, mode=\"a\")\n",
        "                f.write(df.to_json(orient=\"records\", lines=True))\n",
        "                f.close()\n",
        "    print(f\"Saved results to {fname}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Benchmark the online serving throughput.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--backend\",\n",
        "        type=str,\n",
        "        default=\"chat_completions\",\n",
        "        choices=[\"chat_completions\"],\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model\",\n",
        "        type=str,\n",
        "        default=\"\",\n",
        "        help=\"Model name to send request to at API server.\",\n",
        "    )\n",
        "    parser.add_argument(\"--endpoint\", type=str, default=None)\n",
        "    parser.add_argument(\"--host\", type=str, default=\"localhost\")\n",
        "    parser.add_argument(\"--port\", type=int, default=None)\n",
        "    parser.add_argument(\"--dataset\", type=str, help=\"Path to the dataset.\")\n",
        "    parser.add_argument(\n",
        "        \"--endpoints\",\n",
        "        type=str,\n",
        "        default=None,\n",
        "        help=\"Path to a file containing a list of endpoints.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--tokenizer\",\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"Name or path of the tokenizer.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--tokenizer-type\",\n",
        "        type=str,\n",
        "        required=False,\n",
        "        choices=[\n",
        "            \"general\",\n",
        "            \"llama3\",\n",
        "        ],\n",
        "        help=(\n",
        "            \"If provided, use the specified tokenizer type rather than relying on\"\n",
        "            \" implicit logic.\"\n",
        "        ),\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--stream\",\n",
        "        type=str2bool,\n",
        "        default=None,\n",
        "        help=\"Whether to uses streaming API.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--save-full-results\",\n",
        "        type=bool,\n",
        "        default=False,\n",
        "        help=\"Whether to save the full (per request) results.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output-dir\",\n",
        "        type=str,\n",
        "        default=None,\n",
        "        help=(\n",
        "            \"Directory to the output result file otherwise current directory is\"\n",
        "            \" used.\"\n",
        "        ),\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--num-prompts\",\n",
        "        type=int,\n",
        "        default=1000,\n",
        "        help=\"Number of prompts to process.\",\n",
        "    )\n",
        "\n",
        "    def _list_of_ints(arg: str) -> list[int]:\n",
        "        return list(map(int, arg.split(\",\")))\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--max-input-length\",\n",
        "        type=_list_of_ints,\n",
        "        default=[1024],\n",
        "        help=(\n",
        "            \"Maximum number of input tokens for filtering the benchmark dataset.\"\n",
        "            \" This argument can be a list of integers separated by ','.\"\n",
        "        ),\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--fixed-input-length\",\n",
        "        type=str2bool,\n",
        "        default=False,\n",
        "        help=\"If true, force the input length to be --max-input-length.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--max-output-length\",\n",
        "        type=_list_of_ints,\n",
        "        default=[1024],\n",
        "        help=(\n",
        "            \"Maximum number of input tokens for filtering the benchmark dataset.\"\n",
        "            \" This argument can be a list of integers separated by ','\"\n",
        "        ),\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--fixed-output-length\",\n",
        "        type=str2bool,\n",
        "        default=False,\n",
        "        help=\"If true, force the output length to be --max-output-length.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--max-context-length\",\n",
        "        type=int,\n",
        "        default=32768,\n",
        "        help=(\n",
        "            \"The maximum context length for the model. Some serving dockers\"\n",
        "            \" support overriding this value, such as Ollama.\"\n",
        "        ),\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--sonnet-prefix-len\",\n",
        "        type=int,\n",
        "        default=30,\n",
        "        help=\"Number of prefix tokens per request, used only for sonnet dataset.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--c\",\n",
        "        \"--concurrent-requests\",\n",
        "        type=_list_of_ints,\n",
        "        default=None,\n",
        "        help=(\n",
        "            \"The number of concurrent requests to send., This argument can be a\"\n",
        "            \" list of integers separated by ','\"\n",
        "        ),\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--enable-retry\",\n",
        "        action=\"store_true\",\n",
        "        default=False,\n",
        "        help=\"Whether to enable retry on retriable errors.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--request-rate\",\n",
        "        type=float,\n",
        "        default=float(\"inf\"),\n",
        "        help=(\n",
        "            \"If this is inf, all requests are sent at time 0. Otherwise, we take\"\n",
        "            \" 1 divided by this argument value to be the parameter of the Poisson\"\n",
        "            \" distribution for modeling the request arrival times. Ignored if\"\n",
        "            \" --concurrent-requests is set.\"\n",
        "        ),\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--fixed-qps\",\n",
        "        type=float,\n",
        "        help=(\n",
        "            \"Number of requests per second sent with equal intervals. If this\"\n",
        "            \" argument is set, we ignore request_rate and use a fixed QPS for\"\n",
        "            \" sending the requests. Ignored if --concurrent-requests is set.\"\n",
        "        ),\n",
        "    )\n",
        "    parser.add_argument(\"--seed\", type=int, default=0)\n",
        "    parser.add_argument(\n",
        "        \"--trust-remote-code\",\n",
        "        action=\"store_true\",\n",
        "        help=\"trust remote code from huggingface\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--name\",\n",
        "        type=str,\n",
        "        default=\"\",\n",
        "        help=(\n",
        "            \"The name of the benchmark. Will be used as the prefix of the saved\"\n",
        "            \" results files.\"\n",
        "        ),\n",
        "    )\n",
        "    # pylint: disable=line-too-long\n",
        "    parser.add_argument(\n",
        "        \"--request-extra-body\",\n",
        "        type=str,\n",
        "        default=\"\",\n",
        "        help=(\n",
        "            \"Extra body to send with request. To disable LLamaGuard, set it to:\"\n",
        "            ' \\'{\"google\": { \"model_safety_settings\": {\"enabled\": False,'\n",
        "            ' \"llama_guard_settings\": {}}}}\\''\n",
        "        ),\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"-v\",\n",
        "        \"--verbosity\",\n",
        "        help=\"Set the logging level (default: %(default)s)\",\n",
        "        default=\"warning\",\n",
        "        choices=[\"debug\", \"info\", \"warning\", \"error\", \"critical\"],\n",
        "    )\n",
        "\n",
        "    cmd_args = parser.parse_args()\n",
        "    main(cmd_args)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "231718602dbc"
      },
      "source": [
        "### Benchmark Dataset\n",
        "\n",
        "The benchmark client needs a dataset for constructing prompts. We will download the [sonnet.txt](https://github.com/vllm-project/vllm/blob/main/benchmarks/sonnet.txt) dataset directly from the official vLLM project repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "22bb9a2ad710"
      },
      "outputs": [],
      "source": [
        "# Download the dataset from the vLLM GitHub repository\n",
        "!wget https://raw.githubusercontent.com/vllm-project/vllm/main/benchmarks/sonnet.txt\n",
        "\n",
        "print(\"sonnet.txt downloaded successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0fe00ef3486"
      },
      "source": [
        "## 3. Run Benchmark\n",
        "\n",
        "Now we're ready to run the benchmark.\n",
        "\n",
        "⚠️ **Important**: This step requires a local Docker environment and access to NVIDIA GPUs. If you are running this notebook on a machine without GPUs (like a standard Colab instance), this command will fail. You should run this on a **GPU-enabled environment**, such as a Vertex AI Workbench instance or a GCE VM equipped with GPUs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6a89d25fee9"
      },
      "source": [
        "### Why Different Setups Need Different Recipes 🧠\n",
        "\n",
        "Before we run the command, it's crucial to understand *why* the optimal serving configuration isn't a one-size-fits-all solution. Finding the best \"recipe\" is a complex balancing act between a model's memory requirements, your hardware, and your expected user traffic. Let's break down the strategies you'll be testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1bdf0bdaf81"
      },
      "source": [
        "#### Key Serving Strategies Explained\n",
        "\n",
        "##### **Pipeline Parallelism (PP): The Assembly Line**\n",
        "**Pipeline Parallelism** splits a model's layers into sequential stages, placing each stage on a different GPU.\n",
        "\n",
        "* **Analogy 🚗:** Think of a car manufacturing assembly line. GPU 1 (Station 1) installs the engine (processes layers 1-16), then passes the car to GPU 2 (Station 2) to add the chassis (processes layers 17-32), and so on. A request \"moves\" from one GPU to the next until it's complete.\n",
        "* **Use Case:** Useful for serving a model that doesn't fit onto a single GPU and especially multi-host serving of large models. Its main drawback is potential GPU idle time, as later stages must wait for the first ones to finish (known as the \"pipeline bubble\"), and potential uneven allocation of layers across GPUs.\n",
        "\n",
        "##### **Tensor Parallelism (TP): The Chef Team**\n",
        "**Tensor Parallelism** splits individual layers—and the mathematical operations within them—across multiple GPUs.\n",
        "\n",
        "* **Analogy 🍕:** Imagine several chefs (GPUs) working on a single, enormous pizza (a single model layer) at the same time. Each chef is responsible for a slice, and they must communicate constantly to ensure the toppings are distributed perfectly.\n",
        "* **Use Case:** Useful for serving a model that doesn't fit onto a single GPU and can reduce inference latency by splitting large tensor computations into smaller components. It introduces communication overhead during per-layer computations.\n",
        "\n",
        "##### **Model Replicas (vLLM Instances): Multiple Kitchens**\n",
        "A **Model Replica** is a full, independent copy of the vLLM server instance. If a model can fit on a subset of your available GPUs, you can run multiple replicas to handle more users at once.\n",
        "\n",
        "* **Analogy 👨‍🍳:** Instead of building one giant, complex kitchen (a single large model instance), you open several smaller, independent kitchens (replicas). Each kitchen can take a customer's order and fulfill it from start to finish without waiting for the others.\n",
        "* **Use Case:** This is the primary strategy to maximize throughput.\n",
        "\n",
        "✨ **Special Feature**: The Vertex AI Model Garden vLLM container used in this section allows you to run **multiple independent vLLM server instances (replicas) as one container**. This feature is specifically designed to maximize throughput for high-concurrency applications.\n",
        "\n",
        "**Note**: The Model Garden vLLM container used in this tutorial builds upon vLLM's version: `vllm/vllm-openai@sha256:43892706699a4a390dab480e6a3b2f144203de11e0caebdbcb0c29ca1bce63c6`. It doesn't modify the kernel or engine implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60c4fa546877"
      },
      "source": [
        "#### How Your Use Case Affects the Optimal Recipe\n",
        "\n",
        "Now, let's understand how these strategies apply to different use cases:\n",
        "\n",
        "1.  **Model Size**: Larger models require a minimum setting of **TP** and/or **PP** to fit the weights into memory. In addition, duplication of their model weights in model replicas creates larger overhead in memory usage. Smaller models can fit onto a single GPU, allowing more possible combinations of **PP**, **TP** and **Model Replica** settings, and duplication of their model weights creates less overhead.\n",
        "\n",
        "2.  **Input and Output Length**: The input length and output length of requests affect the prefill and decode time it takes to process the requests, and the server's compute and memory usage patterns.\n",
        "\n",
        "3.  **Concurrency**: The concurrency of requests, the number of requests running or queued at the server, also affects the server's compute and memory usage patterns. It is a key factor to the latency and throughput tradeoff of the server.\n",
        "\n",
        "By running the benchmark utility, you are empirically testing these trade-offs to find the sweet spot for *your specific model, hardware, and expected traffic*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81cf18d6600b"
      },
      "source": [
        "First, let's define the parameters for our benchmark run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1d497dfbdc0b"
      },
      "outputs": [],
      "source": [
        "# The model we want to benchmark\n",
        "MODEL_PATH = \"/path/to/Llama-3.3-70B-Instruct\"  # @param {type:\"string\"}\n",
        "\n",
        "# The MG vLLM serving container supporting model replicas.\n",
        "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20250808_0916_RC01_maas\"  # @param {type:\"string\"}\n",
        "SERVER_INIT_TIMEOUT = 300  # @param {type:\"integer\"}\n",
        "\n",
        "# The total number of GPUs available on the machine.\n",
        "# This should match your hardware setup (e.g., 8 for an a3-highgpu-8g machine).\n",
        "TOTAL_GPUS = 8  # @param {type:\"integer\"}\n",
        "\n",
        "# Benchmark settings\n",
        "INPUT_LENGTH = 1200  # @param {type:\"integer\"}\n",
        "OUTPUT_LENGTH = 250  # @param {type:\"integer\"}\n",
        "NUM_PROMPTS = 10  # @param {type:\"integer\"}\n",
        "CONCURRENCIES = \"1 8 64\"  # @param {type:\"string\"}\n",
        "\n",
        "# Latency requirements (optional)\n",
        "# The utility will flag runs that don't meet the latency requirements in the\n",
        "# benchmark results. If SKIP_CONCURRENCIES_GIVEN_LATENCY is set to True, the\n",
        "# utility skips larger runs with larger concurrencies if the current run\n",
        "# doesn't satisfy any of the specified latency requirements.\n",
        "MAX_MEDIAN_TTFT_MS = 1000  # @param {type:\"number\"}\n",
        "MAX_MEDIAN_TPOT_MS = 200  # @param {type:\"number\"}\n",
        "SKIP_CONCURRENCIES_GIVEN_LATENCY = False  # @param {type:\"boolean\"}\n",
        "SKIP_CONCURRENCIES_GIVEN_LATENCY_ARG = (\n",
        "    \"--skip-concurrencies-given-latency\" if SKIP_CONCURRENCIES_GIVEN_LATENCY else \"\"\n",
        ")\n",
        "\n",
        "# Output file paths\n",
        "RESULTS_CSV_PATH = \"benchmark_results.csv\"\n",
        "FIGURE_PATH = \"benchmark_figure.png\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4ad0c03e951"
      },
      "source": [
        "Next, construct and execute the command. The utility will run benchmarks iterating through all valid combinations of pipeline parallelism (`PP`), tensor parallelism (`TP`), and model replicas that can be formed with the `TOTAL_GPUS`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "12fab0c0a742"
      },
      "outputs": [],
      "source": [
        "!python benchmark_util.py \\\n",
        "  --total-gpus $TOTAL_GPUS \\\n",
        "  --input-length $INPUT_LENGTH \\\n",
        "  --output-length $OUTPUT_LENGTH \\\n",
        "  --num-prompts $NUM_PROMPTS \\\n",
        "  --concurrencies $CONCURRENCIES \\\n",
        "  --max-median-ttft-ms $MAX_MEDIAN_TTFT_MS \\\n",
        "  --max-median-tpot-ms $MAX_MEDIAN_TPOT_MS \\\n",
        "  --model $MODEL_PATH \\\n",
        "  --docker-uri $VLLM_DOCKER_URI \\\n",
        "  --server-init-timeout $SERVER_INIT_TIMEOUT \\\n",
        "  --benchmark-script-path benchmark_serving.py \\\n",
        "  --dataset-path sonnet.txt \\\n",
        "  --results-output-path $RESULTS_CSV_PATH \\\n",
        "  --figure-output-path $FIGURE_PATH \\\n",
        "  $SKIP_CONCURRENCIES_GIVEN_LATENCY_ARG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e57fbd079cc"
      },
      "source": [
        "## 4. Review Reference Benchmark Results [Case Study] \n",
        "\n",
        "To provide general recommendations and an example of what the benchmark results look like and how to interpret them, this section is a case study on reference benchmark results.\n",
        "\n",
        "**Note**: The reference benchmark results and recommendations shared in this section are specific to Vertex AI's offering of 8 x H100 VMs and a certain vLLM configuration and benchmark methodology. They intend to only serve as general guidance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b6ecdf78e9f"
      },
      "source": [
        "### Part 1: General Recommendations for 8 x H100 Setups\n",
        "\n",
        "While the benchmark utility is the best way to find the precise optimal recipe for your specific needs, we would like to provide general recommendations on efficiently serving a model on 8 x H100 setups.\n",
        "\n",
        "| Model Size | Sample Model | Recommended Recipe |\n",
        "|------------|--------------|--------------------|\n",
        "| Small | [google/gemma-2-9b-it](https://huggingface.co/google/gemma-2-9b-it) | At smaller concurrecies, `TP=8` gives the best performance. At larger concurrencies, having multiple model replicas gives the best performance. As the concurrency increases, the optimal recipe shifts from `TP=8` to having gradually smaller `TP=4, TP=2, TP=1` and gradually more model replicas of 2, 4, and 8. |\n",
        "| Medium | [Qwen/Qwen2.5-32B-Instruct](https://huggingface.co/Qwen/Qwen2.5-32B-Instruct) | Similar to for a small model, as concurrency increases, the optimal recipe goes from `TP=8` to having more model replicas. Meanwhile, with a larger model, the concurrency thresholds at which we should have more model replicas increase. In other words, the optimal recipe has comparatively fewer model replicas. With a larger model, there is more memory overhead with having copies of the model weights. |\n",
        "| Large | [meta-llama/Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | The same trend holds that as concurrency increases, the optimal recipe goes from `TP=8` to having more model replicas. With a large model that doesn't fit onto one GPU, there are fewer possible combinations of serving strategies. In addition, empirically we find that using tensor parallelism and model replicas offer better performance than pipeline parallelism. |\n",
        "\n",
        "Note\n",
        "- The model size is denoted relative to the 8 x H100 VM.\n",
        "- The recommendations apply specifically to Vertex AI's 8 x H100 VMs. Different infrastructures and accelerators can require different recipes. For instance, for accelerators without efficient cross-GPU communication, pipeline parallelism (PP) can perform more favorably.\n",
        "\n",
        "Even though the hardware and models are specific, the underlying principles and trade-offs are broadly applicable. Reference these examples to build an intuition for how to approach your unique setup."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e5688faa6a3"
      },
      "source": [
        "### Part 2: Scalability of Model Replicas (Server Instances)\n",
        "\n",
        "The Vertex AI Model Garden vLLM container offers the feature of co-hosting multiple model replicas within a single container, by running mutliple vLLM server instances. To demonstrate the scalability of this feature, we run a benchmark with H100 GPUs using [google/gemma-2-9b-it](https://huggingface.co/google/gemma-2-9b-it), a certain server configuration, and an approximate input length of 1200 and output length of 250.\n",
        "\n",
        "| Number of Model Replicas | GPUs | Concurrency | Request Throughput (req/s) | Median Request Latency (ms) | Interpretation |\n",
        "|--------------------------|------|-------------|----------------------------|-----------------------------|----------------|\n",
        "| 1 | 1 x H100 | 2048 per GPU* | 11.245 | 180977 | A single model replica and vLLM server instance. Baseline. |\n",
        "| 8 | 8 x H100 | 2048 per GPU* | 88.006 (**7.8x**) | 182453 (**1.0x**) | With 8 model replicas, implemented with 8 vLLM server instances, we obtain linear improvement in request throughput, with no regression in request latency. |\n",
        "\n",
        "\\* A concurrency of 2048 per GPU is heavy traffic that saturates the server. This represents a maximum-throughput scenario which approaches offline inference.\n",
        "\n",
        "The results show that at saturating traffic, the model replicas implementation can achieve **linear throughput scaling with virtually no latency overhead**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffe7f107f41e"
      },
      "source": [
        "### Part 3: Concurrency Crossover: The Optimal Recipe Changes\n",
        "\n",
        "The optimal serving recipe depends on the traffic. A critical insight from benchmarking is understanding the \"crossover point\" where the optimal serving recipe changes.\n",
        "\n",
        "To illustrate this, let's compare two configurations on an 8 x H100 serving [meta-llama/Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct), benchmarked with a certain server configuration and an approximate input length of 1200 and output length of 250:\n",
        "\n",
        "| Setup | Concurrency | Request Throughput (req/s) | Median TTFT (ms) | Median TPOT (ms) |\n",
        "|-------|-------------|----------------------------|----------------------|----------------|\n",
        "| Setup A: `TP=8, Replicas=1` | 8 | 2.329 | 72.590 | 13.515 |\n",
        "| Setup B: `TP=4, Replicas=2` | 8 | 1.760 | 100.157 | 17.866 |\n",
        "\n",
        "The winner is setup A: `TP=8, Replicas=1`.\n",
        "\n",
        "| Setup | Concurrency | Request Throughput (req/s) | Median TTFT (ms) | Median TPOT (ms) |\n",
        "|-------|-------------|----------------------------|----------------------|----------------|\n",
        "| Setup A: `TP=8, Replicas=1` | 256 | 13.534 | 206.216 | 73.741 |\n",
        "| Setup B: `TP=4, Replicas=2` | 256 | 15.636 | 167.483 | 61.023 |\n",
        "\n",
        "The winner is setup B: `TP=4, Replicas=2`.\n",
        "\n",
        "**Key Takeaway**: There is no single best configuration--it depends on your traffic. Setup A, with a larger TP, is better for lower concurrency setups. As user traffic increases, the crossover happens. Setup B, with 2 server instances, is better for higher concurrency setups. This demonstrates the value of the benchmark utility and the importance of running benchmark experiments to **tailor the optimal serving recipe to your specific use case**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca96d5e687c7"
      },
      "source": [
        "## 5. Analyze Benchmark Results\n",
        "\n",
        "Now, we will analyze the results from the benchmark run. This process involves three steps:\n",
        "\n",
        "1. Visualize: Display the summary figure generated by the benchmark utility to get a high-level visual understanding of the performance trade-offs.\n",
        "2. Filter: Load the raw data and filter out configurations that failed to meet any specified latency requirements.\n",
        "3. Optimize: From the valid configurations, compile the optimal recipe for each concurrency in a table, and identify the optimal recipe for a target concurrency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "566105a5d90a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# --- Configuration ---\n",
        "RESULTS_CSV_PATH = \"benchmark_results.csv\"  # @param {type:\"string\"}\n",
        "FIGURE_PATH = \"benchmark_figure.png\"  # @param {type:\"string\"}\n",
        "TARGET_CONCURRENCY = 8  # @param {type:\"integer\"}\n",
        "\n",
        "# --- 1. Visualize Results ---\n",
        "print(\"--- 1. Visualizing Performance Chart ---\")\n",
        "if os.path.exists(FIGURE_PATH):\n",
        "    print(f\"Displaying benchmark summary from: {FIGURE_PATH}\")\n",
        "    display(Image(filename=FIGURE_PATH))\n",
        "else:\n",
        "    print(\n",
        "        f\"Warning: Figure file not found at '{FIGURE_PATH}'. Ensure the benchmark ran successfully.\"\n",
        "    )\n",
        "\n",
        "# --- 2. Load and Analyze Data ---\n",
        "print(\"\\n\\n--- 2. Analyzing Optimal Configuration per Concurrency ---\")\n",
        "try:\n",
        "    results_df = pd.read_csv(RESULTS_CSV_PATH)\n",
        "\n",
        "    # Filter for configurations that meet all latency requirements\n",
        "    valid_configs = results_df.copy()\n",
        "    latency_checks = [\"median_ttft_ok\", \"p99_ttft_ok\", \"median_tpot_ok\", \"p99_tpot_ok\"]\n",
        "    for check in latency_checks:\n",
        "        if check in valid_configs.columns:\n",
        "            valid_configs = valid_configs[valid_configs[check]]\n",
        "\n",
        "    if not valid_configs.empty:\n",
        "        # Find the best configuration for each concurrency level\n",
        "        # Group by concurrency and find the index of the max throughput in each group\n",
        "        optimal_indices = valid_configs.groupby(\"concurrent_requests\")[\n",
        "            \"request_throughput\"\n",
        "        ].idxmax()\n",
        "        optimal_per_concurrency = valid_configs.loc[optimal_indices]\n",
        "\n",
        "        print(\"🏆 Optimal Configuration per Concurrency (Meeting Latency Goals) 🏆\\n\")\n",
        "\n",
        "        display_cols = [\n",
        "            \"concurrent_requests\",\n",
        "            \"tp_size\",\n",
        "            \"model_replicas\",\n",
        "            \"request_throughput\",\n",
        "            \"median_ttft_ms\",\n",
        "            \"median_tpot_ms\",\n",
        "        ]\n",
        "        print(optimal_per_concurrency[display_cols].to_markdown(index=False))\n",
        "\n",
        "        # --- 3. Find Best Configuration for Target Concurrency ---\n",
        "        print(\n",
        "            f\"\\n\\n--- 3. Selecting Best Configuration for Target Concurrency: {TARGET_CONCURRENCY} ---\"\n",
        "        )\n",
        "\n",
        "        target_config_df = optimal_per_concurrency[\n",
        "            optimal_per_concurrency[\"concurrent_requests\"] == TARGET_CONCURRENCY\n",
        "        ]\n",
        "\n",
        "        if not target_config_df.empty:\n",
        "            best_overall_config = target_config_df.iloc[0]\n",
        "            print(\n",
        "                f\"✅ Found optimal configuration for target concurrency {TARGET_CONCURRENCY}.\"\n",
        "            )\n",
        "        else:\n",
        "            print(\n",
        "                f\"⚠️ Warning: No valid configuration found for target concurrency {TARGET_CONCURRENCY}.\"\n",
        "            )\n",
        "            best_overall_config = optimal_per_concurrency.loc[\n",
        "                optimal_per_concurrency[\"request_throughput\"].idxmax()\n",
        "            ]\n",
        "            print(\n",
        "                f\"Falling back to the configuration with the highest overall throughput at concurrency {int(best_overall_config['concurrent_requests'])}.\"\n",
        "            )\n",
        "\n",
        "        OPTIMAL_PP_SIZE = int(best_overall_config[\"pp_size\"])\n",
        "        OPTIMAL_TP_SIZE = int(best_overall_config[\"tp_size\"])\n",
        "        OPTIMAL_REPLICAS = int(best_overall_config[\"model_replicas\"])\n",
        "\n",
        "        print(\"=\" * 50)\n",
        "        print(\"🔧 Configuration for Deployment 🔧\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Pipeline Parallel Size (PP): {OPTIMAL_PP_SIZE}\")\n",
        "        print(f\"Tensor Parallel Size (TP): {OPTIMAL_TP_SIZE}\")\n",
        "        print(f\"Model Replicas: {OPTIMAL_REPLICAS}\")\n",
        "        print(\"=\" * 50)\n",
        "    else:\n",
        "        print(\"\\nCould not find any configuration that met all latency requirements.\")\n",
        "        OPTIMAL_PP_SIZE = 1\n",
        "        OPTIMAL_TP_SIZE = 8\n",
        "        OPTIMAL_REPLICAS = 1\n",
        "        print(\"Defaulting to PP=1, TP=8, Replicas=1 for deployment.\")\n",
        "except FileNotFoundError:\n",
        "    print(\n",
        "        f\"Results file '{RESULTS_CSV_PATH}' not found. Defaulting to PP=1, TP=8, Replicas=1 for deployment.\"\n",
        "    )\n",
        "    OPTIMAL_PP_SIZE = 1\n",
        "    OPTIMAL_TP_SIZE = 8\n",
        "    OPTIMAL_REPLICAS = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c13490552b71"
      },
      "source": [
        "## 6. Deploy to Vertex AI and Test the Endpoint\n",
        "\n",
        "With the optimal recipe identified, we now deploy the model to a Vertex AI Endpoint with 8 x H100 VM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "b179306a8f62"
      },
      "outputs": [],
      "source": [
        "# Set model to deploy\n",
        "base_model_name = \"Llama-3.3-70B-Instruct\"  # @param {type:\"string\"}\n",
        "model_id = \"meta-llama/Llama-3.3-70B-Instruct\"  # @param {type:\"string\"}\n",
        "HF_TOKEN = \"\"  # @param {type:\"string\"}\n",
        "hf_model_id = model_id\n",
        "publisher = \"meta\"\n",
        "publisher_model_id = \"llama3-3\"\n",
        "\n",
        "# Find Vertex AI prediction supported accelerators and regions at https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
        "accelerator_type = \"NVIDIA_H100_80GB\"\n",
        "accelerator_count = 8\n",
        "machine_type = \"a3-highgpu-8g\"\n",
        "multihost_gpu_node_count = 1\n",
        "gpu_memory_utilization = 0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0577acc064ff"
      },
      "outputs": [],
      "source": [
        "# @title Deploy with customized configs\n",
        "\n",
        "# @markdown Set use_dedicated_endpoint to False if you don't want to use [dedicated endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment#create-dedicated-endpoint). Note that [dedicated endpoint does not support VPC Service Controls](https://cloud.google.com/vertex-ai/docs/predictions/choose-endpoint-type), uncheck the box if you are using VPC-SC.\n",
        "use_dedicated_endpoint = True  # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown Choose whether to use a [Spot VM](https://cloud.google.com/compute/docs/instances/spot) for the deployment.\n",
        "is_spot = False  # @param {type:\"boolean\"}\n",
        "\n",
        "common_util.check_quota(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=int(accelerator_count * multihost_gpu_node_count),\n",
        "    is_for_training=False,\n",
        "    is_spot=is_spot,\n",
        ")\n",
        "\n",
        "# @markdown To enable the auto-scaling in deployment, you can set the following options:\n",
        "\n",
        "min_replica_count = 1  # @param {type:\"integer\"}\n",
        "max_replica_count = 1  # @param {type:\"integer\"}\n",
        "required_replica_count = 1  # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown Set the target of GPU duty cycle or CPU usage between 1 and 100 for auto-scaling.\n",
        "autoscale_by_gpu_duty_cycle_target = 0  # @param {type:\"integer\"}\n",
        "autoscale_by_cpu_usage_target = 0  # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown Note: GPU duty cycle is not the most accurate metric for scaling workloads. More advanced auto-scaling metrics are coming soon. See [the public doc](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/DedicatedResources#AutoscalingMetricSpec) for more details.\n",
        "\n",
        "\n",
        "def deploy_model_vllm_single_model_cohost(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    publisher: str,\n",
        "    publisher_model_id: str,\n",
        "    service_account: str = None,\n",
        "    base_model_id: str = None,\n",
        "    machine_type: str = \"a3-highgpu-8g\",\n",
        "    accelerator_type: str = \"NVIDIA_H100_80GB\",\n",
        "    accelerator_count: int = 8,\n",
        "    gpu_partition_size: str = \"\",\n",
        "    multihost_gpu_node_count: int = 1,\n",
        "    pipeline_parallel_size: int = 1,\n",
        "    tensor_parallel_size: int = 8,\n",
        "    model_replicas: int = 1,\n",
        "    gpu_memory_utilization: float = 0.95,\n",
        "    enable_trust_remote_code: bool = False,\n",
        "    use_dedicated_endpoint: bool = False,\n",
        "    min_replica_count: int = 1,\n",
        "    max_replica_count: int = 1,\n",
        "    required_replica_count: int = 1,\n",
        "    autoscale_by_gpu_duty_cycle_target: int = 0,\n",
        "    autoscale_by_cpu_usage_target: int = 0,\n",
        "    is_spot: bool = True,\n",
        "    model_cohost_feature: str = \"single-model-cohost\",\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys models with vLLM to Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(\n",
        "        display_name=f\"{model_name}-endpoint\",\n",
        "        dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
        "    )\n",
        "\n",
        "    if not base_model_id:\n",
        "        base_model_id = model_id\n",
        "\n",
        "    if model_replicas > 1:\n",
        "        api_server = \"vllm.entrypoints.nginx_server\"\n",
        "    else:\n",
        "        api_server = \"vllm.entrypoints.api_server\"\n",
        "\n",
        "    # See https://docs.vllm.ai/en/latest/models/engine_args.html for a list of possible arguments with descriptions.\n",
        "    vllm_args = [\n",
        "        \"python\",\n",
        "        \"-m\",\n",
        "        api_server,\n",
        "        \"--host=0.0.0.0\",\n",
        "        \"--port=8080\",\n",
        "        f\"--model={model_id}\",\n",
        "        f\"--pipeline-parallel-size={pipeline_parallel_size}\",\n",
        "        f\"--tensor-parallel-size={tensor_parallel_size}\",\n",
        "        \"--data-parallel-size=1\",\n",
        "        \"--swap-space=16\",\n",
        "    ]\n",
        "\n",
        "    if multihost_gpu_node_count > 1:\n",
        "        vllm_args = [\"/vllm-workspace/ray_launcher.sh\"] + vllm_args\n",
        "\n",
        "    if gpu_memory_utilization:\n",
        "        vllm_args.append(f\"--gpu-memory-utilization={gpu_memory_utilization}\")\n",
        "\n",
        "    if enable_trust_remote_code:\n",
        "        vllm_args.append(\"--trust-remote-code\")\n",
        "\n",
        "    if model_replicas > 1:\n",
        "        vllm_args.extend(\n",
        "            [\n",
        "                f\"--num_instances={model_replicas}\",\n",
        "                f\"--total_gpus={accelerator_count}\",\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    env_vars = {\n",
        "        \"MODEL_ID\": base_model_id,\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "    }\n",
        "\n",
        "    # HF_TOKEN is not a compulsory field and may not be defined.\n",
        "    try:\n",
        "        if HF_TOKEN:\n",
        "            env_vars[\"HF_TOKEN\"] = HF_TOKEN\n",
        "    except NameError:\n",
        "        pass\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
        "        serving_container_args=vllm_args,\n",
        "        serving_container_ports=[8080],\n",
        "        serving_container_predict_route=\"/generate\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "        serving_container_environment_variables=env_vars,\n",
        "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
        "        serving_container_deployment_timeout=7200,\n",
        "        model_garden_source_model_name=(\n",
        "            f\"publishers/{publisher}/models/{publisher_model_id}\"\n",
        "        ),\n",
        "    )\n",
        "    print(\n",
        "        f\"Deploying {model_name} on {multihost_gpu_node_count} host(s) of {machine_type} with {accelerator_type} GPU(s).\"\n",
        "    )\n",
        "\n",
        "    creds, _ = auth.default()\n",
        "    auth_req = auth.transport.requests.Request()\n",
        "    creds.refresh(auth_req)\n",
        "\n",
        "    url = f\"https://{REGION}-aiplatform.googleapis.com/ui/projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint.name}:deployModel\"\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {creds.token}\",\n",
        "    }\n",
        "    data = {\n",
        "        \"deployedModel\": {\n",
        "            \"model\": model.resource_name,\n",
        "            \"displayName\": model_name,\n",
        "            \"dedicatedResources\": {\n",
        "                \"machineSpec\": {\n",
        "                    \"machineType\": machine_type,\n",
        "                    \"multihostGpuNodeCount\": multihost_gpu_node_count,\n",
        "                    \"acceleratorType\": accelerator_type,\n",
        "                    \"acceleratorCount\": accelerator_count,\n",
        "                    \"gpuPartitionSize\": gpu_partition_size,\n",
        "                },\n",
        "                \"minReplicaCount\": min_replica_count,\n",
        "                \"requiredReplicaCount\": required_replica_count,\n",
        "                \"maxReplicaCount\": max_replica_count,\n",
        "            },\n",
        "            \"system_labels\": {\n",
        "                \"NOTEBOOK_NAME\": \"model_garden_model_cohost.ipynb\",\n",
        "                \"NOTEBOOK_ENVIRONMENT\": common_util.get_deploy_source(),\n",
        "                \"mg-serving-feature-model-cohost\": model_cohost_feature,\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "    if service_account:\n",
        "        data[\"deployedModel\"][\"serviceAccount\"] = service_account\n",
        "    if is_spot:\n",
        "        data[\"deployedModel\"][\"dedicatedResources\"][\"spot\"] = True\n",
        "    if autoscale_by_gpu_duty_cycle_target > 0 or autoscale_by_cpu_usage_target > 0:\n",
        "        data[\"deployedModel\"][\"dedicatedResources\"][\"autoscalingMetricSpecs\"] = []\n",
        "        if autoscale_by_gpu_duty_cycle_target > 0:\n",
        "            data[\"deployedModel\"][\"dedicatedResources\"][\n",
        "                \"autoscalingMetricSpecs\"\n",
        "            ].append(\n",
        "                {\n",
        "                    \"metricName\": \"aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle\",\n",
        "                    \"target\": autoscale_by_gpu_duty_cycle_target,\n",
        "                }\n",
        "            )\n",
        "        if autoscale_by_cpu_usage_target > 0:\n",
        "            data[\"deployedModel\"][\"dedicatedResources\"][\n",
        "                \"autoscalingMetricSpecs\"\n",
        "            ].append(\n",
        "                {\n",
        "                    \"metricName\": \"aiplatform.googleapis.com/prediction/online/cpu/utilization\",\n",
        "                    \"target\": autoscale_by_cpu_usage_target,\n",
        "                }\n",
        "            )\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "    print(f\"Deploy Model response: {response.json()}\")\n",
        "    if response.status_code != 200 or \"name\" not in response.json():\n",
        "        raise ValueError(f\"Failed to deploy model: {response.text}\")\n",
        "    common_util.poll_and_wait(response.json()[\"name\"], REGION, 7200)\n",
        "    print(\"endpoint_name:\", endpoint.name)\n",
        "\n",
        "    return model, endpoint\n",
        "\n",
        "\n",
        "(\n",
        "    models[\"vllm_gpu_single_model\"],\n",
        "    endpoints[\"vllm_gpu_single_model\"],\n",
        ") = deploy_model_vllm_single_model_cohost(\n",
        "    model_name=common_util.get_job_name_with_datetime(prefix=\"single-model-cohost\"),\n",
        "    model_id=model_id,\n",
        "    publisher=publisher,\n",
        "    publisher_model_id=publisher_model_id,\n",
        "    base_model_id=hf_model_id,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    multihost_gpu_node_count=multihost_gpu_node_count,\n",
        "    pipeline_parallel_size=OPTIMAL_PP_SIZE,\n",
        "    tensor_parallel_size=OPTIMAL_TP_SIZE,\n",
        "    model_replicas=OPTIMAL_REPLICAS,\n",
        "    gpu_memory_utilization=gpu_memory_utilization,\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        "    min_replica_count=min_replica_count,\n",
        "    max_replica_count=max_replica_count,\n",
        "    required_replica_count=required_replica_count,\n",
        "    autoscale_by_gpu_duty_cycle_target=autoscale_by_gpu_duty_cycle_target,\n",
        "    autoscale_by_cpu_usage_target=autoscale_by_cpu_usage_target,\n",
        "    is_spot=is_spot,\n",
        "    model_cohost_feature=\"single-model-cohost\",\n",
        ")\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "f8e5602b1fcc"
      },
      "outputs": [],
      "source": [
        "# @title Raw predict\n",
        "\n",
        "# @markdown Once deployment succeeds, you can send requests to the endpoint with text prompts. Sampling parameters supported by vLLM can be found [here](https://docs.vllm.ai/en/latest/dev/sampling_params.html).\n",
        "\n",
        "# @markdown Example:\n",
        "\n",
        "# @markdown ```\n",
        "# @markdown Human: What is a car?\n",
        "# @markdown Assistant:  A car, or a motor car, is a road-connected human-transportation system used to move people or goods from one place to another. The term also encompasses a wide range of vehicles, including motorboats, trains, and aircrafts. Cars typically have four wheels, a cabin for passengers, and an engine or motor. They have been around since the early 19th century and are now one of the most popular forms of transportation, used for daily commuting, shopping, and other purposes.\n",
        "# @markdown ```\n",
        "# @markdown Additionally, you can moderate the generated text with Vertex AI. See [Moderate text documentation](https://cloud.google.com/natural-language/docs/moderating-text) for more details.\n",
        "\n",
        "# Loads an existing endpoint instance using the endpoint name:\n",
        "# - Using `endpoint_name = endpoint.name` allows us to get the\n",
        "#   endpoint name of the endpoint `endpoint` created in the cell\n",
        "#   above.\n",
        "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
        "#   an existing endpoint with the ID 1234567890123456789.\n",
        "# You may uncomment the code below to load an existing endpoint.\n",
        "\n",
        "# endpoint_name = \"\"  # @param {type:\"string\"}\n",
        "# aip_endpoint_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
        "# )\n",
        "# endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
        "\n",
        "prompt = \"What is a car?\"  # @param {type: \"string\"}\n",
        "# @markdown If you encounter an issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, by lowering `max_tokens`.\n",
        "max_tokens = 50  # @param {type:\"integer\"}\n",
        "temperature = 1.0  # @param {type:\"number\"}\n",
        "top_p = 1.0  # @param {type:\"number\"}\n",
        "top_k = 1  # @param {type:\"integer\"}\n",
        "# @markdown Set `raw_response` to `True` to obtain the raw model output. Set `raw_response` to `False` to apply additional formatting in the structure of `\"Prompt:\\n{prompt.strip()}\\nOutput:\\n{output}\"`.\n",
        "raw_response = False  # @param {type:\"boolean\"}\n",
        "\n",
        "# Overrides parameters for inferences.\n",
        "instances = [\n",
        "    {\n",
        "        \"prompt\": prompt,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": top_p,\n",
        "        \"top_k\": top_k,\n",
        "        \"raw_response\": raw_response,\n",
        "    },\n",
        "]\n",
        "response = endpoints[\"vllm_gpu_single_model\"].predict(\n",
        "    instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
        ")\n",
        "\n",
        "for prediction in response.predictions:\n",
        "    print(prediction)\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3c3c715f66a5"
      },
      "outputs": [],
      "source": [
        "# @title Chat completion\n",
        "\n",
        "if use_dedicated_endpoint:\n",
        "    DEDICATED_ENDPOINT_DNS = endpoints[\n",
        "        \"vllm_gpu_single_model\"\n",
        "    ].gca_resource.dedicated_endpoint_dns\n",
        "ENDPOINT_RESOURCE_NAME = endpoints[\"vllm_gpu_single_model\"].resource_name\n",
        "\n",
        "# @title Chat Completions Inference\n",
        "\n",
        "# @markdown Once deployment succeeds, you can send requests to the endpoint using the OpenAI SDK.\n",
        "\n",
        "# @markdown First you will need to install the SDK and some auth-related dependencies.\n",
        "\n",
        "! pip install -qU openai google-auth requests\n",
        "\n",
        "# @markdown Next fill out some request parameters:\n",
        "\n",
        "user_message = \"How is your day going?\"  # @param {type: \"string\"}\n",
        "# @markdown If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, such as set `max_tokens` as 20.\n",
        "max_tokens = 50  # @param {type: \"integer\"}\n",
        "temperature = 1.0  # @param {type: \"number\"}\n",
        "stream = False  # @param {type: \"boolean\"}\n",
        "\n",
        "# @markdown Now we can send a request.\n",
        "\n",
        "import google.auth\n",
        "import openai\n",
        "\n",
        "creds, project = google.auth.default()\n",
        "auth_req = google.auth.transport.requests.Request()\n",
        "creds.refresh(auth_req)\n",
        "\n",
        "BASE_URL = (\n",
        "    f\"https://{REGION}-aiplatform.googleapis.com/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
        ")\n",
        "try:\n",
        "    if use_dedicated_endpoint:\n",
        "        BASE_URL = f\"https://{DEDICATED_ENDPOINT_DNS}/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "client = openai.OpenAI(base_url=BASE_URL, api_key=creds.token)\n",
        "\n",
        "model_response = client.chat.completions.create(\n",
        "    model=\"\",\n",
        "    messages=[{\"role\": \"user\", \"content\": user_message}],\n",
        "    temperature=temperature,\n",
        "    max_tokens=max_tokens,\n",
        "    stream=stream,\n",
        ")\n",
        "\n",
        "if stream:\n",
        "    usage = None\n",
        "    contents = []\n",
        "    for chunk in model_response:\n",
        "        if chunk.usage is not None:\n",
        "            usage = chunk.usage\n",
        "            continue\n",
        "        print(chunk.choices[0].delta.content, end=\"\")\n",
        "        contents.append(chunk.choices[0].delta.content)\n",
        "    print(f\"\\n\\n{usage}\")\n",
        "else:\n",
        "    print(model_response)\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e57fbd079cc"
      },
      "source": [
        "## 7. [Alternative Solution: Pod Co-scheduling + MIG] Review Reference Benchmark Results\n",
        "\n",
        "An alternative to the container-level solution is the infrastructure-level solution with pod co-scheduling + NVIDIA Multi-Instance GPU (MIG), which is available in [Preview](https://cloud.google.com/products?e=48754805&hl=en#product-launch-stages).\n",
        "\n",
        "- Co-scheduling with whole GPUs: We can assign one or more full hardware accelerators to each model replica. For example, we can deploy up to eight replicas on an 8 x H100 VM.\n",
        "\n",
        "- Partitioning with NVIDIA Multi-Instance GPU (MIG): For even greater efficiency with smaller workloads, we can partition a single physical GPU into multiple, smaller, fully isolated instances using NVIDIA MIG. This allows us to assign resources at a sub-GPU level, maximizing the utilization of each accelerator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e5688faa6a3"
      },
      "source": [
        "### Comparable Performance\n",
        "\n",
        "We run a benchmark with 8 x H100 Vertex Endpoints using [google/gemma-2-9b-it](https://huggingface.co/google/gemma-2-9b-it), a certain server configuration, and an approximate input length of 1200 and output length of 250.\n",
        "\n",
        "| Solution | Number of Model Replicas | Concurrency | Request Throughput (req/s) | Median Request Latency (ms) |\n",
        "|----------|--------------------------|-------------|----------------------------|-----------------------------|\n",
        "| Model Co-hosting Container | 8 | 128 | 34.367 | 3512 |\n",
        "| Pod Co-scheduling | 8 | 128 | 32.404 | 3656 |\n",
        "\n",
        "The results show that the two solutions offer **comparable performance with serving multiple replicas of a model**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c13490552b71"
      },
      "source": [
        "## 8. [Alternative Solution: Pod Co-scheduling + MIG] Deploy to Vertex AI and Test the Endpoint\n",
        "\n",
        "With the optimal recipe previously, we now deploy the model to a Vertex AI Endpoint with pod co-scheduling. Note that we set the accelerator count per replica at `accelerator_count`. We serve one replica per container (pod).\n",
        "\n",
        "When MIG is enabled, we can't use GPU sharing (each replica is limited to consuming MIG in a single GPU). Consequently, when a `gpu_partition_size` is specified, the `accelerator_count` must be set to 1. An exampe MIG setting of `gpu_partition_size` is `\"1g.10gb\"`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "b179306a8f62"
      },
      "outputs": [],
      "source": [
        "# Set model to deploy\n",
        "base_model_name = \"Llama-3.3-70B-Instruct\"  # @param {type:\"string\"}\n",
        "model_id = \"meta-llama/Llama-3.3-70B-Instruct\"  # @param {type:\"string\"}\n",
        "HF_TOKEN = \"\"  # @param {type:\"string\"}\n",
        "hf_model_id = model_id\n",
        "publisher = \"meta\"\n",
        "publisher_model_id = \"llama3-3\"\n",
        "\n",
        "# Find Vertex AI prediction supported accelerators and regions at https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
        "accelerator_type = \"NVIDIA_H100_80GB\"\n",
        "accelerator_count = 8\n",
        "accelerator_count_per_replica = 4\n",
        "gpu_partition_size = \"\"\n",
        "machine_type = \"a3-highgpu-8g\"\n",
        "multihost_gpu_node_count = 1\n",
        "pipeline_parallel_size = 1\n",
        "tensor_parallel_size = accelerator_count_per_replica\n",
        "model_replicas = 1\n",
        "gpu_memory_utilization = 0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0577acc064ff"
      },
      "outputs": [],
      "source": [
        "# @title Deploy with customized configs\n",
        "\n",
        "# @markdown Set use_dedicated_endpoint to False if you don't want to use [dedicated endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment#create-dedicated-endpoint). Note that [dedicated endpoint does not support VPC Service Controls](https://cloud.google.com/vertex-ai/docs/predictions/choose-endpoint-type), uncheck the box if you are using VPC-SC.\n",
        "use_dedicated_endpoint = True  # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown Choose whether to use a [Spot VM](https://cloud.google.com/compute/docs/instances/spot) for the deployment.\n",
        "is_spot = False  # @param {type:\"boolean\"}\n",
        "\n",
        "common_util.check_quota(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=int(accelerator_count * multihost_gpu_node_count),\n",
        "    is_for_training=False,\n",
        "    is_spot=is_spot,\n",
        ")\n",
        "\n",
        "# @markdown To enable the auto-scaling in deployment, you can set the following options:\n",
        "\n",
        "min_replica_count = 1  # @param {type:\"integer\"}\n",
        "max_replica_count = 1  # @param {type:\"integer\"}\n",
        "required_replica_count = 1  # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown Set the target of GPU duty cycle or CPU usage between 1 and 100 for auto-scaling.\n",
        "autoscale_by_gpu_duty_cycle_target = 0  # @param {type:\"integer\"}\n",
        "autoscale_by_cpu_usage_target = 0  # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown Note: GPU duty cycle is not the most accurate metric for scaling workloads. More advanced auto-scaling metrics are coming soon. See [the public doc](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/DedicatedResources#AutoscalingMetricSpec) for more details.\n",
        "\n",
        "\n",
        "def deploy_model_vllm_single_model_cohost(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    publisher: str,\n",
        "    publisher_model_id: str,\n",
        "    service_account: str = None,\n",
        "    base_model_id: str = None,\n",
        "    machine_type: str = \"a3-highgpu-8g\",\n",
        "    accelerator_type: str = \"NVIDIA_H100_80GB\",\n",
        "    accelerator_count: int = 8,\n",
        "    gpu_partition_size: str = \"\",\n",
        "    multihost_gpu_node_count: int = 1,\n",
        "    pipeline_parallel_size: int = 1,\n",
        "    tensor_parallel_size: int = 8,\n",
        "    model_replicas: int = 1,\n",
        "    gpu_memory_utilization: float = 0.95,\n",
        "    enable_trust_remote_code: bool = False,\n",
        "    use_dedicated_endpoint: bool = False,\n",
        "    min_replica_count: int = 1,\n",
        "    max_replica_count: int = 1,\n",
        "    required_replica_count: int = 1,\n",
        "    autoscale_by_gpu_duty_cycle_target: int = 0,\n",
        "    autoscale_by_cpu_usage_target: int = 0,\n",
        "    is_spot: bool = True,\n",
        "    model_cohost_feature: str = \"single-model-cohost\",\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys models with vLLM to Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(\n",
        "        display_name=f\"{model_name}-endpoint\",\n",
        "        dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
        "    )\n",
        "\n",
        "    if not base_model_id:\n",
        "        base_model_id = model_id\n",
        "\n",
        "    if model_replicas > 1:\n",
        "        api_server = \"vllm.entrypoints.nginx_server\"\n",
        "    else:\n",
        "        api_server = \"vllm.entrypoints.api_server\"\n",
        "\n",
        "    # See https://docs.vllm.ai/en/latest/models/engine_args.html for a list of possible arguments with descriptions.\n",
        "    vllm_args = [\n",
        "        \"python\",\n",
        "        \"-m\",\n",
        "        api_server,\n",
        "        \"--host=0.0.0.0\",\n",
        "        \"--port=8080\",\n",
        "        f\"--model={model_id}\",\n",
        "        f\"--pipeline-parallel-size={pipeline_parallel_size}\",\n",
        "        f\"--tensor-parallel-size={tensor_parallel_size}\",\n",
        "        \"--data-parallel-size=1\",\n",
        "        \"--swap-space=16\",\n",
        "    ]\n",
        "\n",
        "    if multihost_gpu_node_count > 1:\n",
        "        vllm_args = [\"/vllm-workspace/ray_launcher.sh\"] + vllm_args\n",
        "\n",
        "    if gpu_memory_utilization:\n",
        "        vllm_args.append(f\"--gpu-memory-utilization={gpu_memory_utilization}\")\n",
        "\n",
        "    if enable_trust_remote_code:\n",
        "        vllm_args.append(\"--trust-remote-code\")\n",
        "\n",
        "    if model_replicas > 1:\n",
        "        vllm_args.extend(\n",
        "            [\n",
        "                f\"--num_instances={model_replicas}\",\n",
        "                f\"--total_gpus={accelerator_count}\",\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    env_vars = {\n",
        "        \"MODEL_ID\": base_model_id,\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "    }\n",
        "\n",
        "    # HF_TOKEN is not a compulsory field and may not be defined.\n",
        "    try:\n",
        "        if HF_TOKEN:\n",
        "            env_vars[\"HF_TOKEN\"] = HF_TOKEN\n",
        "    except NameError:\n",
        "        pass\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
        "        serving_container_args=vllm_args,\n",
        "        serving_container_ports=[8080],\n",
        "        serving_container_predict_route=\"/generate\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "        serving_container_environment_variables=env_vars,\n",
        "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
        "        serving_container_deployment_timeout=7200,\n",
        "        model_garden_source_model_name=(\n",
        "            f\"publishers/{publisher}/models/{publisher_model_id}\"\n",
        "        ),\n",
        "    )\n",
        "    print(\n",
        "        f\"Deploying {model_name} on {multihost_gpu_node_count} host(s) of {machine_type} with {accelerator_type} GPU(s).\"\n",
        "    )\n",
        "\n",
        "    creds, _ = auth.default()\n",
        "    auth_req = auth.transport.requests.Request()\n",
        "    creds.refresh(auth_req)\n",
        "\n",
        "    url = f\"https://{REGION}-aiplatform.googleapis.com/ui/projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint.name}:deployModel\"\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {creds.token}\",\n",
        "    }\n",
        "    data = {\n",
        "        \"deployedModel\": {\n",
        "            \"model\": model.resource_name,\n",
        "            \"displayName\": model_name,\n",
        "            \"dedicatedResources\": {\n",
        "                \"machineSpec\": {\n",
        "                    \"machineType\": machine_type,\n",
        "                    \"multihostGpuNodeCount\": multihost_gpu_node_count,\n",
        "                    \"acceleratorType\": accelerator_type,\n",
        "                    \"acceleratorCount\": accelerator_count,\n",
        "                    \"gpuPartitionSize\": gpu_partition_size,\n",
        "                },\n",
        "                \"minReplicaCount\": min_replica_count,\n",
        "                \"requiredReplicaCount\": required_replica_count,\n",
        "                \"maxReplicaCount\": max_replica_count,\n",
        "            },\n",
        "            \"system_labels\": {\n",
        "                \"NOTEBOOK_NAME\": \"model_garden_model_cohost.ipynb\",\n",
        "                \"NOTEBOOK_ENVIRONMENT\": common_util.get_deploy_source(),\n",
        "                \"mg-serving-feature-model-cohost\": model_cohost_feature,\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "    if service_account:\n",
        "        data[\"deployedModel\"][\"serviceAccount\"] = service_account\n",
        "    if is_spot:\n",
        "        data[\"deployedModel\"][\"dedicatedResources\"][\"spot\"] = True\n",
        "    if autoscale_by_gpu_duty_cycle_target > 0 or autoscale_by_cpu_usage_target > 0:\n",
        "        data[\"deployedModel\"][\"dedicatedResources\"][\"autoscalingMetricSpecs\"] = []\n",
        "        if autoscale_by_gpu_duty_cycle_target > 0:\n",
        "            data[\"deployedModel\"][\"dedicatedResources\"][\n",
        "                \"autoscalingMetricSpecs\"\n",
        "            ].append(\n",
        "                {\n",
        "                    \"metricName\": \"aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle\",\n",
        "                    \"target\": autoscale_by_gpu_duty_cycle_target,\n",
        "                }\n",
        "            )\n",
        "        if autoscale_by_cpu_usage_target > 0:\n",
        "            data[\"deployedModel\"][\"dedicatedResources\"][\n",
        "                \"autoscalingMetricSpecs\"\n",
        "            ].append(\n",
        "                {\n",
        "                    \"metricName\": \"aiplatform.googleapis.com/prediction/online/cpu/utilization\",\n",
        "                    \"target\": autoscale_by_cpu_usage_target,\n",
        "                }\n",
        "            )\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "    print(f\"Deploy Model response: {response.json()}\")\n",
        "    if response.status_code != 200 or \"name\" not in response.json():\n",
        "        raise ValueError(f\"Failed to deploy model: {response.text}\")\n",
        "    common_util.poll_and_wait(response.json()[\"name\"], REGION, 7200)\n",
        "    print(\"endpoint_name:\", endpoint.name)\n",
        "\n",
        "    return model, endpoint\n",
        "\n",
        "\n",
        "(\n",
        "    models[\"vllm_gpu_pod_coschedule_mig\"],\n",
        "    endpoints[\"vllm_gpu_pod_coschedule_mig\"],\n",
        ") = deploy_model_vllm_single_model_cohost(\n",
        "    model_name=common_util.get_job_name_with_datetime(\n",
        "        prefix=\"single-model-cohost-pod-coschedule-mig\"\n",
        "    ),\n",
        "    model_id=model_id,\n",
        "    publisher=publisher,\n",
        "    publisher_model_id=publisher_model_id,\n",
        "    base_model_id=hf_model_id,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count_per_replica,\n",
        "    gpu_partition_size=gpu_partition_size,\n",
        "    multihost_gpu_node_count=multihost_gpu_node_count,\n",
        "    pipeline_parallel_size=pipeline_parallel_size,\n",
        "    tensor_parallel_size=tensor_parallel_size,\n",
        "    model_replicas=model_replicas,\n",
        "    gpu_memory_utilization=gpu_memory_utilization,\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        "    min_replica_count=min_replica_count,\n",
        "    max_replica_count=max_replica_count,\n",
        "    required_replica_count=required_replica_count,\n",
        "    autoscale_by_gpu_duty_cycle_target=autoscale_by_gpu_duty_cycle_target,\n",
        "    autoscale_by_cpu_usage_target=autoscale_by_cpu_usage_target,\n",
        "    is_spot=is_spot,\n",
        "    model_cohost_feature=\"single-model-cohost-pod-coschedule-mig\",\n",
        ")\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "f8e5602b1fcc"
      },
      "outputs": [],
      "source": [
        "# @title Raw predict\n",
        "\n",
        "# @markdown Once deployment succeeds, you can send requests to the endpoint with text prompts. Sampling parameters supported by vLLM can be found [here](https://docs.vllm.ai/en/latest/dev/sampling_params.html).\n",
        "\n",
        "# @markdown Example:\n",
        "\n",
        "# @markdown ```\n",
        "# @markdown Human: What is a car?\n",
        "# @markdown Assistant:  A car, or a motor car, is a road-connected human-transportation system used to move people or goods from one place to another. The term also encompasses a wide range of vehicles, including motorboats, trains, and aircrafts. Cars typically have four wheels, a cabin for passengers, and an engine or motor. They have been around since the early 19th century and are now one of the most popular forms of transportation, used for daily commuting, shopping, and other purposes.\n",
        "# @markdown ```\n",
        "# @markdown Additionally, you can moderate the generated text with Vertex AI. See [Moderate text documentation](https://cloud.google.com/natural-language/docs/moderating-text) for more details.\n",
        "\n",
        "# Loads an existing endpoint instance using the endpoint name:\n",
        "# - Using `endpoint_name = endpoint.name` allows us to get the\n",
        "#   endpoint name of the endpoint `endpoint` created in the cell\n",
        "#   above.\n",
        "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
        "#   an existing endpoint with the ID 1234567890123456789.\n",
        "# You may uncomment the code below to load an existing endpoint.\n",
        "\n",
        "# endpoint_name = \"\"  # @param {type:\"string\"}\n",
        "# aip_endpoint_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
        "# )\n",
        "# endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
        "\n",
        "prompt = \"What is a car?\"  # @param {type: \"string\"}\n",
        "# @markdown If you encounter an issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, by lowering `max_tokens`.\n",
        "max_tokens = 50  # @param {type:\"integer\"}\n",
        "temperature = 1.0  # @param {type:\"number\"}\n",
        "top_p = 1.0  # @param {type:\"number\"}\n",
        "top_k = 1  # @param {type:\"integer\"}\n",
        "# @markdown Set `raw_response` to `True` to obtain the raw model output. Set `raw_response` to `False` to apply additional formatting in the structure of `\"Prompt:\\n{prompt.strip()}\\nOutput:\\n{output}\"`.\n",
        "raw_response = False  # @param {type:\"boolean\"}\n",
        "\n",
        "# Overrides parameters for inferences.\n",
        "instances = [\n",
        "    {\n",
        "        \"prompt\": prompt,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": top_p,\n",
        "        \"top_k\": top_k,\n",
        "        \"raw_response\": raw_response,\n",
        "    },\n",
        "]\n",
        "response = endpoints[\"vllm_gpu_pod_coschedule_mig\"].predict(\n",
        "    instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
        ")\n",
        "\n",
        "for prediction in response.predictions:\n",
        "    print(prediction)\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3c3c715f66a5"
      },
      "outputs": [],
      "source": [
        "# @title Chat completion\n",
        "\n",
        "if use_dedicated_endpoint:\n",
        "    DEDICATED_ENDPOINT_DNS = endpoints[\n",
        "        \"vllm_gpu_pod_coschedule_mig\"\n",
        "    ].gca_resource.dedicated_endpoint_dns\n",
        "ENDPOINT_RESOURCE_NAME = endpoints[\"vllm_gpu_pod_coschedule_mig\"].resource_name\n",
        "\n",
        "# @title Chat Completions Inference\n",
        "\n",
        "# @markdown Once deployment succeeds, you can send requests to the endpoint using the OpenAI SDK.\n",
        "\n",
        "# @markdown First you will need to install the SDK and some auth-related dependencies.\n",
        "\n",
        "! pip install -qU openai google-auth requests\n",
        "\n",
        "# @markdown Next fill out some request parameters:\n",
        "\n",
        "user_message = \"How is your day going?\"  # @param {type: \"string\"}\n",
        "# @markdown If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, such as set `max_tokens` as 20.\n",
        "max_tokens = 50  # @param {type: \"integer\"}\n",
        "temperature = 1.0  # @param {type: \"number\"}\n",
        "stream = False  # @param {type: \"boolean\"}\n",
        "\n",
        "# @markdown Now we can send a request.\n",
        "\n",
        "import google.auth\n",
        "import openai\n",
        "\n",
        "creds, project = google.auth.default()\n",
        "auth_req = google.auth.transport.requests.Request()\n",
        "creds.refresh(auth_req)\n",
        "\n",
        "BASE_URL = (\n",
        "    f\"https://{REGION}-aiplatform.googleapis.com/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
        ")\n",
        "try:\n",
        "    if use_dedicated_endpoint:\n",
        "        BASE_URL = f\"https://{DEDICATED_ENDPOINT_DNS}/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "client = openai.OpenAI(base_url=BASE_URL, api_key=creds.token)\n",
        "\n",
        "model_response = client.chat.completions.create(\n",
        "    model=\"\",\n",
        "    messages=[{\"role\": \"user\", \"content\": user_message}],\n",
        "    temperature=temperature,\n",
        "    max_tokens=max_tokens,\n",
        "    stream=stream,\n",
        ")\n",
        "\n",
        "if stream:\n",
        "    usage = None\n",
        "    contents = []\n",
        "    for chunk in model_response:\n",
        "        if chunk.usage is not None:\n",
        "            usage = chunk.usage\n",
        "            continue\n",
        "        print(chunk.choices[0].delta.content, end=\"\")\n",
        "        contents.append(chunk.choices[0].delta.content)\n",
        "    print(f\"\\n\\n{usage}\")\n",
        "else:\n",
        "    print(model_response)\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e72c8e9cbb1"
      },
      "source": [
        "## 9. Clean Up\n",
        "\n",
        "To avoid incurring ongoing charges, it's important to clean up the resources you've created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4cbedace3cd7"
      },
      "outputs": [],
      "source": [
        "# @title Delete the models and endpoints\n",
        "\n",
        "# @markdown  Delete the experiment models and endpoints to recycle the resources\n",
        "# @markdown  and avoid unnecessary continuous charges that may incur.\n",
        "\n",
        "# Undeploy model and delete endpoint.\n",
        "for endpoint in endpoints.values():\n",
        "    endpoint.delete(force=True)\n",
        "\n",
        "# Delete models.\n",
        "for model in models.values():\n",
        "    model.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f01b5964e636"
      },
      "source": [
        "## Multi-model Serving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e67705049064"
      },
      "source": [
        "## 1. Setup (Same as Before)\n",
        "\n",
        "First, let's install the necessary packages and set up your Google Cloud project environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "c64b3b480432"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown 2. **[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment.\n",
        "\n",
        "REGION = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown 3. If you want to run predictions with H100 GPUs or H200 GPUs, we recommend using the regions listed below. **NOTE:** Make sure you have associated quota in selected regions. Click the links to see your current quota for H100s: [`CustomModelServingH100GPUsPerProjectPerRegion`](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_h100_gpus) and H200s: [`CustomModelServingH200GPUsPerProjectPerRegion`](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_h200_gpus). You can request for quota following the instructions at [\"Request a higher quota\"](https://cloud.google.com/docs/quota/view-manage#requesting_higher_quota).\n",
        "\n",
        "# @markdown | Machine Type | Accelerator Type | Recommended Regions |\n",
        "# @markdown | ----------- | ----------- | ----------- |\n",
        "# @markdown | a3-highgpu-8g | 8 NVIDIA_H100_80GB | asia-southeast1, europe-west4, us-central1, us-east5, us-west1 |\n",
        "# @markdown | a3-ultragpu-8g | 8 NVIDIA_H200_141GB | asia-south2, us-south1 |\n",
        "\n",
        "# Upgrade Vertex AI SDK.\n",
        "! pip3 install --upgrade --quiet 'google-cloud-aiplatform==1.103.0'\n",
        "! pip3 install --upgrade --quiet aiohttp matplotlib pandas seaborn\n",
        "\n",
        "# Import the necessary packages\n",
        "import importlib  # noqa: F811\n",
        "import os  # noqa: F811\n",
        "from typing import Tuple  # noqa: F811\n",
        "\n",
        "import requests  # noqa: F811\n",
        "from google import auth  # noqa: F811\n",
        "from google.cloud import aiplatform  # noqa: F811\n",
        "\n",
        "# Upgrade Vertex AI SDK.\n",
        "if os.environ.get(\"VERTEX_PRODUCT\") != \"COLAB_ENTERPRISE\":\n",
        "    ! pip install --upgrade tensorflow\n",
        "! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
        "\n",
        "common_util = importlib.import_module(\n",
        "    \"vertex-ai-samples.notebooks.community.model_garden.docker_source_codes.notebook_util.common_util\"\n",
        ")\n",
        "\n",
        "models, endpoints = {}, {}\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "if not REGION:\n",
        "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "! gcloud config set project $PROJECT_ID\n",
        "\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(\n",
        "    project=PROJECT_ID,\n",
        "    location=REGION,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6a89d25fee9"
      },
      "source": [
        "## 2. Learn to Configure the Model Co-hosting Server\n",
        "\n",
        "✨ **Special Feature**: The Vertex AI Model Garden vLLM container used in this section allows you to **co-host multiple models as one container, with each model having its dedicated pipeline parallelism (PP), tensor parallelism (TP) and model replicas strategies**.\n",
        "\n",
        "### Launch Arguments\n",
        "\n",
        "Below lists the key launch arguments.\n",
        "\n",
        "---\n",
        "\n",
        "#### 🛠️ Model Specification and Memory Allocation\n",
        "\n",
        "| Argument | Requirement | Example(s) | Description |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **`--model`** | Required | `model_a,model_b,model_c` | Comma-separated list of HuggingFace model IDs or GCS paths to load. |\n",
        "| **`--served-model-name`** | Optional | `model_x,model_y,model_z` | Comma-separated list of **model identifiers** to use in the API for each model. If not set, the value of `--model` is used. |\n",
        "| **`--gpu-memory-partition`** | Required | `0.5,0.25,0.25` <br> `0.25` | Comma-separated list of **GPU memory ratios** to reserve for each model out of the full VM (e.g., first model gets 50%, second gets 25%, etc.). A single value applies to all. *(New argument)* |\n",
        "| **`--model-replicas`** | Optional | `4,1,2` <br> `1` | Comma-separated list of the **number of model replicas** to create for each model, or a single number for all. If not set, all models have one replica. *(New argument)* |\n",
        "| **`--max-model-len`** | Optional | `1024,8192,8192` <br> `1024` | Comma-separated list of **maximum context lengths** for each model, or a single length for all. If unset, the length is derived from the model's configuration. |\n",
        "\n",
        "---\n",
        "\n",
        "#### ⚙️ Parallelism\n",
        "\n",
        "| Argument | Requirement | Example(s) | Description |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **`--tensor-parallel-size`** | Optional | `1,2,2` <br> `8` | Comma-separated list of **Tensor Parallelism (TP) sizes** for each model, or a single size for all. If unset, TP size defaults to the number of available GPUs. |\n",
        "| **`--pipeline-parallel-size`** | Optional | `1,2,2` <br> `1` | Comma-separated list of **Pipeline Parallelism (PP) sizes** for each model, or a single size for all. If unset, PP size defaults to 1. |\n",
        "\n",
        "---\n",
        "\n",
        "**Note**: The Model Garden vLLM container used in this section builds upon vLLM's version: https://github.com/vllm-project/vllm/commit/c8851a47235f5dfd3da3abf6c89453b3bdb41ad1. It doesn't modify the kernel or engine implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1d497dfbdc0b"
      },
      "outputs": [],
      "source": [
        "# The MG vLLM model co-hosting serving container.\n",
        "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-imageplatform/vertex-model-garden/vllm-inference-restricted-ubuntu22.04-py3.12:model-garden.vllm-restricted-x86-release_20251028.02_p0\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c13490552b71"
      },
      "source": [
        "## 3. Deploy to Vertex AI and Test the Endpoint\n",
        "\n",
        "With the optimal recipe identified, we now deploy the model to a Vertex AI Endpoint with 8 x H100 VM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "b179306a8f62"
      },
      "outputs": [],
      "source": [
        "# Set model to deploy\n",
        "base_model_name_a = \"gemma-3n-E2B-it\"  # @param {type:\"string\"}\n",
        "model_id_a = \"google/gemma-3n-E2B-it\"  # @param {type:\"string\"}\n",
        "hf_model_id_a = model_id_a\n",
        "publisher_a = \"google\"\n",
        "publisher_model_id_a = \"gemma3n\"\n",
        "\n",
        "base_model_name_b = \"Llama-3.1-8B-Instruct\"  # @param {type:\"string\"}\n",
        "model_id_b = \"meta-llama/Llama-3.1-8B-Instruct\"  # @param {type:\"string\"}\n",
        "hf_model_id_b = model_id_b\n",
        "publisher_b = \"meta\"\n",
        "publisher_model_id_b = \"llama3_1\"\n",
        "\n",
        "base_model_name = \",\".join([base_model_name_a, base_model_name_b])\n",
        "served_model_name = base_model_name\n",
        "model_id = \",\".join([model_id_a, model_id_b])\n",
        "\n",
        "gpu_memory_partition = \"0.4,0.4\"  # @param {type:\"string\"}\n",
        "pipeline_parallel_size = \"1,1\"  # @param {type:\"string\"}\n",
        "tensor_parallel_size = \"1,4\"  # @param {type:\"string\"}\n",
        "model_replicas = \"4,1\"  # @param {type:\"string\"}\n",
        "\n",
        "HF_TOKEN = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# Find Vertex AI prediction supported accelerators and regions at https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
        "accelerator_type = \"NVIDIA_H100_80GB\"\n",
        "accelerator_count = 8\n",
        "machine_type = \"a3-highgpu-8g\"\n",
        "multihost_gpu_node_count = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0577acc064ff"
      },
      "outputs": [],
      "source": [
        "# @title Deploy with customized configs\n",
        "\n",
        "# @markdown Set use_dedicated_endpoint to False if you don't want to use [dedicated endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment#create-dedicated-endpoint). Note that [dedicated endpoint does not support VPC Service Controls](https://cloud.google.com/vertex-ai/docs/predictions/choose-endpoint-type), uncheck the box if you are using VPC-SC.\n",
        "use_dedicated_endpoint = True  # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown Choose whether to use a [Spot VM](https://cloud.google.com/compute/docs/instances/spot) for the deployment.\n",
        "is_spot = False  # @param {type:\"boolean\"}\n",
        "\n",
        "common_util.check_quota(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=int(accelerator_count * multihost_gpu_node_count),\n",
        "    is_for_training=False,\n",
        "    is_spot=is_spot,\n",
        ")\n",
        "\n",
        "# @markdown To enable the auto-scaling in deployment, you can set the following options:\n",
        "\n",
        "min_replica_count = 1  # @param {type:\"integer\"}\n",
        "max_replica_count = 1  # @param {type:\"integer\"}\n",
        "required_replica_count = 1  # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown Set the target of GPU duty cycle or CPU usage between 1 and 100 for auto-scaling.\n",
        "autoscale_by_gpu_duty_cycle_target = 0  # @param {type:\"integer\"}\n",
        "autoscale_by_cpu_usage_target = 0  # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown Note: GPU duty cycle is not the most accurate metric for scaling workloads. More advanced auto-scaling metrics are coming soon. See [the public doc](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/DedicatedResources#AutoscalingMetricSpec) for more details.\n",
        "\n",
        "\n",
        "def deploy_model_vllm_multi_model_cohost(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    gpu_memory_partition: str,\n",
        "    publisher: str,\n",
        "    publisher_model_id: str,\n",
        "    service_account: str = None,\n",
        "    base_model_id: str = None,\n",
        "    served_model_name: str = \"\",\n",
        "    machine_type: str = \"a3-highgpu-8g\",\n",
        "    accelerator_type: str = \"NVIDIA_H100_80GB\",\n",
        "    accelerator_count: int = 8,\n",
        "    multihost_gpu_node_count: int = 1,\n",
        "    pipeline_parallel_size: int = 1,\n",
        "    tensor_parallel_size: int = 8,\n",
        "    model_replicas: int = 1,\n",
        "    enable_trust_remote_code: bool = False,\n",
        "    use_dedicated_endpoint: bool = False,\n",
        "    min_replica_count: int = 1,\n",
        "    max_replica_count: int = 1,\n",
        "    required_replica_count: int = 1,\n",
        "    autoscale_by_gpu_duty_cycle_target: int = 0,\n",
        "    autoscale_by_cpu_usage_target: int = 0,\n",
        "    is_spot: bool = True,\n",
        "    model_cohost_feature: str = \"multi-model-cohost\",\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys models with vLLM to Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(\n",
        "        display_name=f\"{model_name}-endpoint\",\n",
        "        dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
        "    )\n",
        "\n",
        "    if not base_model_id:\n",
        "        base_model_id = model_id\n",
        "\n",
        "    api_server = \"vllm.entrypoints.model_cohost_server\"\n",
        "\n",
        "    # See https://docs.vllm.ai/en/latest/models/engine_args.html for a list of possible arguments with descriptions.\n",
        "    vllm_args = [\n",
        "        \"python\",\n",
        "        \"-m\",\n",
        "        api_server,\n",
        "        \"--host=0.0.0.0\",\n",
        "        \"--port=8080\",\n",
        "        f\"--model={model_id}\",\n",
        "        f\"--gpu-memory-partition={gpu_memory_partition}\",\n",
        "        f\"--pipeline-parallel-size={pipeline_parallel_size}\",\n",
        "        f\"--tensor-parallel-size={tensor_parallel_size}\",\n",
        "        f\"--model-replicas={model_replicas}\",\n",
        "        \"--data-parallel-size=1\",\n",
        "        \"--swap-space=16\",\n",
        "    ]\n",
        "\n",
        "    if multihost_gpu_node_count > 1:\n",
        "        vllm_args = [\"/vllm-workspace/ray_launcher.sh\"] + vllm_args\n",
        "\n",
        "    if served_model_name:\n",
        "        vllm_args.append(f\"--served-model-name={served_model_name}\")\n",
        "\n",
        "    if enable_trust_remote_code:\n",
        "        vllm_args.append(\"--trust-remote-code\")\n",
        "\n",
        "    env_vars = {\n",
        "        \"MODEL_ID\": base_model_id,\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "    }\n",
        "\n",
        "    # HF_TOKEN is not a compulsory field and may not be defined.\n",
        "    try:\n",
        "        if HF_TOKEN:\n",
        "            env_vars[\"HF_TOKEN\"] = HF_TOKEN\n",
        "    except NameError:\n",
        "        pass\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
        "        serving_container_args=vllm_args,\n",
        "        serving_container_ports=[8080],\n",
        "        serving_container_predict_route=\"/generate\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "        serving_container_environment_variables=env_vars,\n",
        "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
        "        serving_container_deployment_timeout=7200,\n",
        "        model_garden_source_model_name=(\n",
        "            f\"publishers/{publisher}/models/{publisher_model_id}\"\n",
        "        ),\n",
        "    )\n",
        "    print(\n",
        "        f\"Deploying {model_name} on {multihost_gpu_node_count} host(s) of {machine_type} with {accelerator_type} GPU(s).\"\n",
        "    )\n",
        "\n",
        "    creds, _ = auth.default()\n",
        "    auth_req = auth.transport.requests.Request()\n",
        "    creds.refresh(auth_req)\n",
        "\n",
        "    url = f\"https://{REGION}-aiplatform.googleapis.com/ui/projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint.name}:deployModel\"\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {creds.token}\",\n",
        "    }\n",
        "    data = {\n",
        "        \"deployedModel\": {\n",
        "            \"model\": model.resource_name,\n",
        "            \"displayName\": model_name,\n",
        "            \"dedicatedResources\": {\n",
        "                \"machineSpec\": {\n",
        "                    \"machineType\": machine_type,\n",
        "                    \"multihostGpuNodeCount\": multihost_gpu_node_count,\n",
        "                    \"acceleratorType\": accelerator_type,\n",
        "                    \"acceleratorCount\": accelerator_count,\n",
        "                },\n",
        "                \"minReplicaCount\": min_replica_count,\n",
        "                \"requiredReplicaCount\": required_replica_count,\n",
        "                \"maxReplicaCount\": max_replica_count,\n",
        "            },\n",
        "            \"system_labels\": {\n",
        "                \"NOTEBOOK_NAME\": \"model_garden_model_cohost.ipynb\",\n",
        "                \"NOTEBOOK_ENVIRONMENT\": common_util.get_deploy_source(),\n",
        "                \"mg-serving-feature-model-cohost\": model_cohost_feature,\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "    if service_account:\n",
        "        data[\"deployedModel\"][\"serviceAccount\"] = service_account\n",
        "    if is_spot:\n",
        "        data[\"deployedModel\"][\"dedicatedResources\"][\"spot\"] = True\n",
        "    if autoscale_by_gpu_duty_cycle_target > 0 or autoscale_by_cpu_usage_target > 0:\n",
        "        data[\"deployedModel\"][\"dedicatedResources\"][\"autoscalingMetricSpecs\"] = []\n",
        "        if autoscale_by_gpu_duty_cycle_target > 0:\n",
        "            data[\"deployedModel\"][\"dedicatedResources\"][\n",
        "                \"autoscalingMetricSpecs\"\n",
        "            ].append(\n",
        "                {\n",
        "                    \"metricName\": \"aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle\",\n",
        "                    \"target\": autoscale_by_gpu_duty_cycle_target,\n",
        "                }\n",
        "            )\n",
        "        if autoscale_by_cpu_usage_target > 0:\n",
        "            data[\"deployedModel\"][\"dedicatedResources\"][\n",
        "                \"autoscalingMetricSpecs\"\n",
        "            ].append(\n",
        "                {\n",
        "                    \"metricName\": \"aiplatform.googleapis.com/prediction/online/cpu/utilization\",\n",
        "                    \"target\": autoscale_by_cpu_usage_target,\n",
        "                }\n",
        "            )\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "    print(f\"Deploy Model response: {response.json()}\")\n",
        "    if response.status_code != 200 or \"name\" not in response.json():\n",
        "        raise ValueError(f\"Failed to deploy model: {response.text}\")\n",
        "    common_util.poll_and_wait(response.json()[\"name\"], REGION, 7200)\n",
        "    print(\"endpoint_name:\", endpoint.name)\n",
        "\n",
        "    return model, endpoint\n",
        "\n",
        "\n",
        "(\n",
        "    models[\"vllm_gpu_multi_model\"],\n",
        "    endpoints[\"vllm_gpu_multi_model\"],\n",
        ") = deploy_model_vllm_multi_model_cohost(\n",
        "    model_name=common_util.get_job_name_with_datetime(prefix=\"multi-model-cohost\"),\n",
        "    model_id=model_id,\n",
        "    gpu_memory_partition=gpu_memory_partition,\n",
        "    served_model_name=served_model_name,\n",
        "    publisher=publisher_a,\n",
        "    publisher_model_id=publisher_model_id_a,\n",
        "    base_model_id=hf_model_id_a,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    multihost_gpu_node_count=multihost_gpu_node_count,\n",
        "    pipeline_parallel_size=pipeline_parallel_size,\n",
        "    tensor_parallel_size=tensor_parallel_size,\n",
        "    model_replicas=model_replicas,\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        "    min_replica_count=min_replica_count,\n",
        "    max_replica_count=max_replica_count,\n",
        "    required_replica_count=required_replica_count,\n",
        "    autoscale_by_gpu_duty_cycle_target=autoscale_by_gpu_duty_cycle_target,\n",
        "    autoscale_by_cpu_usage_target=autoscale_by_cpu_usage_target,\n",
        "    is_spot=is_spot,\n",
        "    model_cohost_feature=\"multi-model-cohost\",\n",
        ")\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3c3c715f66a5"
      },
      "outputs": [],
      "source": [
        "# @title Chat completion\n",
        "\n",
        "if use_dedicated_endpoint:\n",
        "    DEDICATED_ENDPOINT_DNS = endpoints[\n",
        "        \"vllm_gpu_multi_model\"\n",
        "    ].gca_resource.dedicated_endpoint_dns\n",
        "ENDPOINT_RESOURCE_NAME = endpoints[\"vllm_gpu_multi_model\"].resource_name\n",
        "\n",
        "# @title Chat Completions Inference\n",
        "\n",
        "# @markdown Once deployment succeeds, you can send requests to the endpoint using the OpenAI SDK.\n",
        "\n",
        "# @markdown First you will need to install the SDK and some auth-related dependencies.\n",
        "\n",
        "! pip install -qU openai google-auth requests\n",
        "\n",
        "# @markdown Next fill out some request parameters:\n",
        "\n",
        "model = \"\"  # @param {type: \"string\"}\n",
        "user_message = \"How is your day going?\"  # @param {type: \"string\"}\n",
        "# @markdown If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, such as set `max_tokens` as 20.\n",
        "max_tokens = 50  # @param {type: \"integer\"}\n",
        "temperature = 1.0  # @param {type: \"number\"}\n",
        "stream = False  # @param {type: \"boolean\"}\n",
        "\n",
        "# @markdown Now we can send a request.\n",
        "\n",
        "import google.auth\n",
        "import openai\n",
        "\n",
        "creds, project = google.auth.default()\n",
        "auth_req = google.auth.transport.requests.Request()\n",
        "creds.refresh(auth_req)\n",
        "\n",
        "BASE_URL = (\n",
        "    f\"https://{REGION}-aiplatform.googleapis.com/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
        ")\n",
        "try:\n",
        "    if use_dedicated_endpoint:\n",
        "        BASE_URL = f\"https://{DEDICATED_ENDPOINT_DNS}/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "client = openai.OpenAI(base_url=BASE_URL, api_key=creds.token)\n",
        "\n",
        "model_response = client.chat.completions.create(\n",
        "    model=model,\n",
        "    messages=[{\"role\": \"user\", \"content\": user_message}],\n",
        "    temperature=temperature,\n",
        "    max_tokens=max_tokens,\n",
        "    stream=stream,\n",
        ")\n",
        "\n",
        "if stream:\n",
        "    usage = None\n",
        "    contents = []\n",
        "    for chunk in model_response:\n",
        "        if chunk.usage is not None:\n",
        "            usage = chunk.usage\n",
        "            continue\n",
        "        print(chunk.choices[0].delta.content, end=\"\")\n",
        "        contents.append(chunk.choices[0].delta.content)\n",
        "    print(f\"\\n\\n{usage}\")\n",
        "else:\n",
        "    print(model_response)\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e72c8e9cbb1"
      },
      "source": [
        "## 4. Clean Up\n",
        "\n",
        "To avoid incurring ongoing charges, it's important to clean up the resources you've created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4cbedace3cd7"
      },
      "outputs": [],
      "source": [
        "# @title Delete the models and endpoints\n",
        "\n",
        "# @markdown  Delete the experiment models and endpoints to recycle the resources\n",
        "# @markdown  and avoid unnecessary continuous charges that may incur.\n",
        "\n",
        "# Undeploy model and delete endpoint.\n",
        "for endpoint in endpoints.values():\n",
        "    endpoint.delete(force=True)\n",
        "\n",
        "# Delete models.\n",
        "for model in models.values():\n",
        "    model.delete()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_model_cohost.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d9bbf86da5e"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Vertex AI Model Garden - Code LLaMA\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_codellama.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_codellama.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_pytorch_codellama.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "Open in Vertex AI Workbench\n",
        "    </a> (A Python-3 CPU notebook is recommended)\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook showcases deploying pretrained Code LLaMA models using a HuggingFace transformers based serving container and using [vLLM](https://github.com/vllm-project/vllm). This notebook also demonstrates how to evaluate the Code LLaMA models using EleutherAI's Language Model Evaluation Harness (lm-evaluation-harness) with Vertex CustomJob.\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Deploy pre-trained Code LLaMA models using a standard HuggingFace serving solution\n",
        "- Deploy pre-trained Code LLaMA models with [vLLM](https://github.com/vllm-project/vllm) with best serving throughput\n",
        "- Evaluate Code LLaMA models\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "264c07757582"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioensNKM8ned"
      },
      "source": [
        "### Colab only\n",
        "Run the following commands for Colab and skip this section if you are using Workbench."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2707b02ef5df"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    ! pip3 install --upgrade google-cloud-aiplatform\n",
        "    from google.colab import auth as google_auth\n",
        "\n",
        "    google_auth.authenticate_user()\n",
        "    # Install gdown for downloading example training images.\n",
        "    ! pip3 install gdown\n",
        "\n",
        "    # Restart the notebook kernel after installs.\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb7adab99e41"
      },
      "source": [
        "### Setup Google Cloud project\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "1. [Enable the Vertex AI API, Compute Engine API and Cloud Natural Language API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component,language.googleapis.com).\n",
        "\n",
        "1. [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs.\n",
        "\n",
        "1. [Create a service account](https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console) with `Vertex AI User` and `Storage Object Admin` roles for deploying fine tuned model to Vertex AI endpoint."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c460088b873"
      },
      "source": [
        "Set the following variables for the experiment environment. The specified Cloud Storage bucket (BUCKET_URI) should be located in the specified region (REGION). Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "855d6b96f291"
      },
      "outputs": [],
      "source": [
        "# Cloud project id.\n",
        "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# The region you want to launch jobs in.\n",
        "REGION = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# The Cloud Storage bucket for storing experiments output.\n",
        "# Start with gs:// prefix, e.g. gs://foo_bucket.\n",
        "GCS_BUCKET = \"gs://\"  # @param {type:\"string\"}\n",
        "\n",
        "! gcloud config set project $PROJECT_ID\n",
        "\n",
        "import os\n",
        "\n",
        "STAGING_BUCKET = os.path.join(GCS_BUCKET, \"staging\")\n",
        "MODEL_BUCKET = os.path.join(GCS_BUCKET, \"code-llama\")\n",
        "\n",
        "# The service account looks like:\n",
        "# '@.iam.gserviceaccount.com'\n",
        "# Please go to https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console\n",
        "# and create service account with `Vertex AI User` and `Storage Object Admin` roles.\n",
        "# The service account for deploying fine tuned model.\n",
        "SERVICE_ACCOUNT = \"\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e828eb320337"
      },
      "source": [
        "### Initialize Vertex AI API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12cd25839741"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cc825514deb"
      },
      "source": [
        "### Define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b42bd4fa2b2d"
      },
      "outputs": [],
      "source": [
        "# The pre-built serving docker images.\n",
        "PREDICTION_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-serve:20231026_1907_RC00\"\n",
        "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20231002_0916_RC00\"\n",
        "EVAL_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-lm-evaluation-harness:20231011_0934_RC00\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c250872074f"
      },
      "source": [
        "### Define common functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "354da31189dc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "\n",
        "def get_job_name_with_datetime(prefix: str) -> str:\n",
        "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
        "    jobs in Vertex AI.\n",
        "    \"\"\"\n",
        "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
        "\n",
        "\n",
        "def deploy_model(\n",
        "    model_name: str,\n",
        "    base_model_id: str,\n",
        "    finetuned_lora_model_path: str,\n",
        "    service_account: str,\n",
        "    task: str,\n",
        "    precision_loading_mode: str = \"float16\",\n",
        "    machine_type: str = \"n1-standard-8\",\n",
        "    accelerator_type: str = \"NVIDIA_TESLA_V100\",\n",
        "    accelerator_count: int = 1,\n",
        ") -> tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys trained models into Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
        "    serving_env = {\n",
        "        \"BASE_MODEL_ID\": base_model_id,\n",
        "        \"PRECISION_LOADING_MODE\": precision_loading_mode,\n",
        "        \"TASK\": task,\n",
        "    }\n",
        "    if finetuned_lora_model_path:\n",
        "        serving_env[\"FINETUNED_LORA_MODEL_PATH\"] = finetuned_lora_model_path\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=PREDICTION_DOCKER_URI,\n",
        "        serving_container_ports=[7080],\n",
        "        serving_container_predict_route=\"/predictions/peft_serving\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "        serving_container_environment_variables=serving_env,\n",
        "    )\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=service_account,\n",
        "    )\n",
        "    return model, endpoint\n",
        "\n",
        "\n",
        "def deploy_model_vllm(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    service_account: str,\n",
        "    precision: str = \"float16\",\n",
        "    swap_space: int = 16,\n",
        "    machine_type: str = \"n1-standard-8\",\n",
        "    accelerator_type: str = \"NVIDIA_TESLA_V100\",\n",
        "    accelerator_count: int = 1,\n",
        ") -> tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
        "\n",
        "    vllm_args = [\n",
        "        \"--host=0.0.0.0\",\n",
        "        \"--port=7080\",\n",
        "        f\"--model={model_id}\",\n",
        "        f\"--tensor-parallel-size={accelerator_count}\",\n",
        "        f\"--swap-space={swap_space}\",\n",
        "        \"--gpu-memory-utilization=0.9\",\n",
        "        \"--max-num-batched-tokens=4096\",\n",
        "        f\"--dtype={precision}\",\n",
        "        \"--disable-log-stats\",\n",
        "    ]\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
        "        serving_container_command=[\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
        "        serving_container_args=vllm_args,\n",
        "        serving_container_ports=[7080],\n",
        "        serving_container_predict_route=\"/generate\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "    )\n",
        "\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=service_account,\n",
        "    )\n",
        "    return model, endpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivs2RK093c8X"
      },
      "source": [
        "## Access pretrained Code LLaMA models\n",
        "The original models from Meta are converted into the HuggingFace format for serving in Vertex AI.\n",
        "\n",
        "Accept the model agreement to access the models:\n",
        "1. Navigate to the Vertex AI > Model Garden page in the Google Cloud console\n",
        "2. Find the Code LLaMA model card and click on \"VIEW DETAILS\"\n",
        "3. Review the agreement on the model card page\n",
        "4. After clicking the agreement of Code LLaMA, a Cloud Storage bucket containing Code LLaMA pretrained and finetuned models will be shared\n",
        "5. Paste the Cloud Storage bucket link below and assign it to `VERTEX_AI_MODEL_GARDEN_CODE_LLAMA`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jwn4PcTf4EMt"
      },
      "outputs": [],
      "source": [
        "VERTEX_AI_MODEL_GARDEN_CODE_LLAMA = (\n",
        "    \"\"  # This path will be shared once click the agreement in Vertex AI Model Garden.\n",
        ")\n",
        "assert (\n",
        "    VERTEX_AI_MODEL_GARDEN_CODE_LLAMA\n",
        "), \"Please click the agreement of Code LLaMA in Vertex AI Model Garden, and get the GCS path of Code LLaMA model artifacts.\"\n",
        "print(\n",
        "    \"Copy Code LLaMA model artifacts from\",\n",
        "    VERTEX_AI_MODEL_GARDEN_CODE_LLAMA,\n",
        "    \"to \",\n",
        "    MODEL_BUCKET,\n",
        ")\n",
        "! gsutil -m cp -R $VERTEX_AI_MODEL_GARDEN_CODE_LLAMA/* $MODEL_BUCKET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bf3fe80d1ef"
      },
      "source": [
        "Select the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8OiHHNNE_wj"
      },
      "outputs": [],
      "source": [
        "model_name = \"CodeLlama-7b-Instruct-hf\"  # @param [\"CodeLlama-7b-hf\", \"CodeLlama-7b-Python-hf\", \"CodeLlama-7b-Instruct-hf\", \"CodeLlama-13b-hf\", \"CodeLlama-13b-Python-hf\", \"CodeLlama-13b-Instruct-hf\", \"CodeLlama-34b-hf\", \"CodeLlama-34b-Python-hf\", \"CodeLlama-34b-Instruct-hf\"]\n",
        "model_id = os.path.join(MODEL_BUCKET, model_name)\n",
        "print(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc1e52452785"
      },
      "source": [
        "## Deploy pretrained Code LLaMA\n",
        "\n",
        "This section deploys prebuilt Code LLaMA models on Vertex AI. V100 GPUs are used for demonstration. Note that V100 serving generally offers better throughput and latency performance than L4 serving, while L4 serving is generally more cost efficient than V100 serving. The serving efficiency of V100 and L4 GPUs is inferior to that of A100 GPUs, but V100 and L4 GPUs are nevertheless good serving solutions if you do not have A100 quota.\n",
        "\n",
        "We use the PEFT serving image to deploy prebuilt Code LLaMA models, by setting finetuning LoRA model paths as empty. The model deployment step will take 15 minutes to 1 hour to complete, depending on the model sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa3053f0f1db"
      },
      "outputs": [],
      "source": [
        "# Finds Vertex AI prediction supported accelerators and regions in\n",
        "#  https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
        "\n",
        "# Sets 1 V100 (16G) to deploy 7B models.\n",
        "# V100 serving has better throughput and latency performance than L4 serving.\n",
        "machine_type = \"n1-standard-8\"\n",
        "accelerator_type = \"NVIDIA_TESLA_V100\"\n",
        "accelerator_count = 1\n",
        "\n",
        "# Sets 1 L4 (24G) to deploy 7B models.\n",
        "# L4 serving is more cost efficient than V100 serving.\n",
        "# machine_type = \"g2-standard-8\"\n",
        "# accelerator_type = \"NVIDIA_L4\"\n",
        "# accelerator_count = 1\n",
        "\n",
        "# Sets A100 (40G) to deploy 7B models.\n",
        "# machine_type = \"a2-highgpu-1g\"\n",
        "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
        "# accelerator_count = 1\n",
        "\n",
        "# Sets 2 V100 (16G) to deploy 13B models.\n",
        "# V100 serving has better throughput and latency performance than L4 serving.\n",
        "# machine_type = \"n1-standard-16\"\n",
        "# accelerator_type = \"NVIDIA_TESLA_V100\"\n",
        "# accelerator_count = 2\n",
        "\n",
        "# Sets 2 L4 (24G) to deploy 13B models.\n",
        "# L4 serving is more cost efficient than V100 serving.\n",
        "# machine_type = \"g2-standard-24\"\n",
        "# accelerator_type = \"NVIDIA_L4\"\n",
        "# accelerator_count = 2\n",
        "\n",
        "# Sets A100 (40G) to deploy 13B models.\n",
        "# machine_type = \"a2-highgpu-1g\"\n",
        "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
        "# accelerator_count = 1\n",
        "\n",
        "# Sets 8 V100 (16G) to deploy 34B models.\n",
        "# V100 serving has better throughput and latency performance than L4 serving.\n",
        "# machine_type = \"n1-standard-32\"\n",
        "# accelerator_type = \"NVIDIA_TESLA_V100\"\n",
        "# accelerator_count = 8\n",
        "\n",
        "# Sets 4 L4 (24G) to deploy 34B models.\n",
        "# L4 serving is more cost efficient than V100 serving.\n",
        "# machine_type = \"g2-standard-48\"\n",
        "# accelerator_type = \"NVIDIA_L4\"\n",
        "# accelerator_count = 4\n",
        "\n",
        "# Sets 2 A100 (40G) to deploy 34B models.\n",
        "# machine_type = \"a2-highgpu-2g\"\n",
        "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
        "# accelerator_count = 2\n",
        "\n",
        "precision_loading_mode = \"float16\"\n",
        "\n",
        "model, endpoint = deploy_model(\n",
        "    model_name=get_job_name_with_datetime(prefix=\"code-llama-serve-peft\"),\n",
        "    base_model_id=model_id,\n",
        "    finetuned_lora_model_path=\"\",  # This will avoid override finetuning models.\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    task=\"causal-language-modeling-lora\",\n",
        "    precision_loading_mode=precision_loading_mode,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        ")\n",
        "print(f\"Endpoint name: {endpoint.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGKIjgmDFRW2"
      },
      "source": [
        "NOTE: The prebuilt model weights will be downloaded on the fly from the $MODEL_BUCKET after the deployment succeeds. Thus additional 10 ~ 40 minutes of waiting time is needed **after** the above model deployment step succeeds and before you run the next step below. Otherwise you might see a `ServiceUnavailable: 503 502:Bad Gateway` error when you send requests to the endpoint.\n",
        "\n",
        "Once deployment succeeds, you can send requests to the endpoint with text prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3f5a1e1de60d"
      },
      "outputs": [],
      "source": [
        "# Loads an existing endpoint instance using the endpoint name:\n",
        "# - Using `endpoint_name = endpoint.name` allows us to get the endpoint name of\n",
        "#   the endpoint `endpoint` created in the cell above.\n",
        "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
        "#   an existing endpoint with the ID 1234567890123456789.\n",
        "# You may uncomment the code below to load an existing endpoint.\n",
        "\n",
        "# endpoint_name = endpoint.name\n",
        "# # endpoint_name = \"\"  # @param {type:\"string\"}\n",
        "# aip_endpoint_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
        "# )\n",
        "# endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
        "\n",
        "instances = [\n",
        "    {\n",
        "        \"prompt\": \"import argparse\",\n",
        "        \"max_tokens\": 200,\n",
        "        \"temperature\": 1.0,\n",
        "        \"top_p\": 1.0,\n",
        "        \"top_k\": 10,\n",
        "    },\n",
        "]\n",
        "response = endpoint.predict(instances=instances)\n",
        "print(response.predictions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8neJc8CnDDpu"
      },
      "source": [
        "## Deploy Pretrained Code LLaMA with vLLM\n",
        "\n",
        "This section deploys prebuilt Code LLaMA models with [vLLM](https://github.com/vllm-project/vllm) on the Endpoint. Code LLaMA model weights are stored in bfloat16 precision. L4 or A100 GPUs are needed for vLLM serving at bfloat16 precision. V100 GPUs can be used with vLLM serving at float16 precision. Changing the precision from bfloat16 to float16 can result in a change in performance, and this change can be an increase and a decrease. However, the performance change should be small (within 5%).\n",
        "\n",
        "V100 GPUs are used for demonstration. Note that V100 serving generally offers better throughput and latency performance than L4 serving, while L4 serving is generally more cost efficient than V100 serving. The serving efficiency of V100 and L4 GPUs is inferior to that of A100 GPUs, but V100 and L4 GPUs are nevertheless good serving solutions if you do not have A100 quota. The model deployment step will take 15 minutes to 1 hour to complete.\n",
        "\n",
        "The vLLM project is an highly optimized LLM serving framework which can increase serving throughput a lot. The higher QPS you have, the more benefits you get using vLLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03d504bcd60b"
      },
      "outputs": [],
      "source": [
        "# Finds Vertex AI prediction supported accelerators and regions in\n",
        "#  https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
        "\n",
        "# Sets 1 V100 (16G) to deploy 7B models.\n",
        "# V100 serving has better throughput and latency performance than L4 serving.\n",
        "machine_type = \"n1-standard-8\"\n",
        "accelerator_type = \"NVIDIA_TESLA_V100\"\n",
        "accelerator_count = 1\n",
        "vllm_precision = \"float16\"\n",
        "vllm_swap_space = 16\n",
        "\n",
        "# Sets 1 L4 (24G) to deploy 7B models.\n",
        "# L4 serving is more cost efficient than V100 serving.\n",
        "# machine_type = \"g2-standard-8\"\n",
        "# accelerator_type = \"NVIDIA_L4\"\n",
        "# accelerator_count = 1\n",
        "# vllm_precision = \"bfloat16\"\n",
        "# vllm_swap_space = 16\n",
        "\n",
        "# Sets A100 (40G) to deploy 7B models.\n",
        "# machine_type = \"a2-highgpu-1g\"\n",
        "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
        "# accelerator_count = 1\n",
        "# vllm_precision = \"bfloat16\"\n",
        "# vllm_swap_space = 16\n",
        "\n",
        "# Sets 2 V100 (16G) to deploy 13B models.\n",
        "# V100 serving has better throughput and latency performance than L4 serving.\n",
        "# machine_type = \"n1-standard-16\"\n",
        "# accelerator_type = \"NVIDIA_TESLA_V100\"\n",
        "# accelerator_count = 2\n",
        "# vllm_precision = \"float16\"\n",
        "# vllm_swap_space = 16\n",
        "\n",
        "# Sets 2 L4 (24G) to deploy 13B models.\n",
        "# L4 serving is more cost efficient than V100 serving.\n",
        "# machine_type = \"g2-standard-24\"\n",
        "# accelerator_type = \"NVIDIA_L4\"\n",
        "# accelerator_count = 2\n",
        "# vllm_precision = \"bfloat16\"\n",
        "# vllm_swap_space = 16\n",
        "\n",
        "# Sets A100 (40G) to deploy 13B models.\n",
        "# machine_type = \"a2-highgpu-1g\"\n",
        "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
        "# accelerator_count = 1\n",
        "# vllm_precision = \"bfloat16\"\n",
        "# vllm_swap_space = 16\n",
        "\n",
        "# Sets 8 V100 (16G) to deploy 34B models.\n",
        "# V100 serving has better throughput and latency performance than L4 serving.\n",
        "# machine_type = \"n1-highmem-32\"\n",
        "# accelerator_type = \"NVIDIA_TESLA_V100\"\n",
        "# accelerator_count = 8\n",
        "# vllm_precision = \"float16\"\n",
        "# vllm_swap_space = 12\n",
        "\n",
        "# Sets 4 L4 (24G) to deploy 34B models.\n",
        "# L4 serving is more cost efficient than V100 serving.\n",
        "# machine_type = \"g2-standard-48\"\n",
        "# accelerator_type = \"NVIDIA_L4\"\n",
        "# accelerator_count = 4\n",
        "# vllm_precision = \"bfloat16\"\n",
        "# vllm_swap_space = 16\n",
        "\n",
        "# Sets 2 A100 (40G) to deploy 34B models.\n",
        "# machine_type = \"a2-highgpu-2g\"\n",
        "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
        "# accelerator_count = 2\n",
        "# vllm_precision = \"bfloat16\"\n",
        "# vllm_swap_space = 16\n",
        "\n",
        "model_vllm, endpoint_vllm = deploy_model_vllm(\n",
        "    model_name=get_job_name_with_datetime(prefix=\"code-llama-serve-vllm\"),\n",
        "    model_id=model_id,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    precision=vllm_precision,\n",
        "    swap_space=vllm_swap_space,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        ")\n",
        "print(f\"Endpoint name: {endpoint_vllm.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THounKtQSE5r"
      },
      "source": [
        "NOTE: The prebuilt model weights will be downloaded on the fly from the $MODEL_BUCKET after the deployment succeeds. Thus additional 10 ~ 40 minutes of waiting time is needed **after** the above model deployment step succeeds and before you run the next step below. Otherwise you might see a `ServiceUnavailable: 503 502:Bad Gateway` error when you send requests to the endpoint.\n",
        "\n",
        "Once deployment succeeds, you can send requests to the endpoint with text prompts. Parameters supported by vLLM can be found [here](https://github.com/vllm-project/vllm/blob/2e8e49fce3775e7704d413b2f02da6d7c99525c9/vllm/sampling_params.py#L23-L64)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4Y1QjghSE5r"
      },
      "outputs": [],
      "source": [
        "# Loads an existing endpoint instance using the endpoint name:\n",
        "# - Using `endpoint_name = endpoint_vllm.name` allows us to get the endpoint\n",
        "#   name of the endpoint `endpoint_without_peft` created in the cell above.\n",
        "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
        "#   an existing endpoint with the ID 1234567890123456789.\n",
        "# You may uncomment the code below to load an existing endpoint.\n",
        "\n",
        "# endpoint_name = endpoint_vllm.name\n",
        "# # endpoint_name = \"\"  # @param {type:\"string\"}\n",
        "# aip_endpoint_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
        "# )\n",
        "# endpoint_vllm = aiplatform.Endpoint(aip_endpoint_name)\n",
        "\n",
        "instances = [\n",
        "    {\n",
        "        \"prompt\": \"import argparse\",\n",
        "        \"n\": 1,\n",
        "        \"max_tokens\": 200,\n",
        "        \"temperature\": 1.0,\n",
        "        \"top_p\": 1.0,\n",
        "        \"top_k\": 10,\n",
        "    },\n",
        "]\n",
        "response = endpoint_vllm.predict(instances=instances)\n",
        "print(response.predictions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "167e0808f0c2"
      },
      "source": [
        "## Evaluate PEFT-finetuned Code LLaMA models\n",
        "\n",
        "This section demonstrates how to evaluate the Code LLaMA models using EleutherAI's [Language Model Evaluation Harness (lm-evaluation-harness)](https://github.com/EleutherAI/lm-evaluation-harness) with Vertex CustomJob. Please reference the peak GPU memory usgaes for serving and adjust the machine type, accelerator type and accelerator count accordingly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a6cf4ff56e8"
      },
      "source": [
        "This example uses the dataset [GSM8K](https://arxiv.org/abs/2110.14168). All supported tasks are listed in [this task table](https://github.com/EleutherAI/lm-evaluation-harness/blob/master/docs/task_table.md).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87d47877c23c"
      },
      "outputs": [],
      "source": [
        "eval_dataset = \"gsm8k\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a61e2318d27"
      },
      "outputs": [],
      "source": [
        "# Worker pool spec.\n",
        "# Find Vertex AI supported accelerators and regions in:\n",
        "#  https://cloud.google.com/vertex-ai/docs/training/configure-compute\n",
        "\n",
        "# Sets 1 V100 (16G) to evaluate 7B models.\n",
        "machine_type = \"n1-standard-8\"\n",
        "accelerator_type = \"NVIDIA_TESLA_V100\"\n",
        "accelerator_count = 1\n",
        "\n",
        "# Sets 1 L4 (24G) to evaluate 7B models.\n",
        "# machine_type = \"g2-standard-8\"\n",
        "# accelerator_type = \"NVIDIA_L4\"\n",
        "# accelerator_count = 1\n",
        "\n",
        "# Sets A100 (40G) to evaluate 7B models.\n",
        "# machine_type = \"a2-highgpu-1g\"\n",
        "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
        "# accelerator_count = 1\n",
        "\n",
        "# Sets 2 V100 (16G) to evaluate 13B models.\n",
        "# machine_type = \"n1-standard-16\"\n",
        "# accelerator_type = \"NVIDIA_TESLA_V100\"\n",
        "# accelerator_count = 2\n",
        "\n",
        "# Sets 2 L4 (24G) to evaluate 13B models.\n",
        "# machine_type = \"g2-standard-24\"\n",
        "# accelerator_type = \"NVIDIA_L4\"\n",
        "# accelerator_count = 2\n",
        "\n",
        "# Sets A100 (40G) to evaluate 13B models.\n",
        "# machine_type = \"a2-highgpu-1g\"\n",
        "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
        "# accelerator_count = 1\n",
        "\n",
        "# Sets 8 V100 (16G) to evaluate 34B models.\n",
        "machine_type = \"n1-standard-32\"\n",
        "accelerator_type = \"NVIDIA_TESLA_V100\"\n",
        "accelerator_count = 8\n",
        "\n",
        "# Sets 4 L4 (24G) to evaluate 34B models.\n",
        "# machine_type = \"g2-standard-48\"\n",
        "# accelerator_type = \"NVIDIA_L4\"\n",
        "# accelerator_count = 4\n",
        "\n",
        "# Sets 2 A100 (40G) to evaluate 34B models.\n",
        "# machine_type = \"a2-highgpu-2g\"\n",
        "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
        "# accelerator_count = 2\n",
        "\n",
        "replica_count = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eb8a8dce447"
      },
      "outputs": [],
      "source": [
        "# Setup evaluation job.\n",
        "job_name = get_job_name_with_datetime(prefix=\"code-llama-eval\")\n",
        "eval_output_dir = os.path.join(MODEL_BUCKET, job_name)\n",
        "eval_output_dir_gcsfuse = eval_output_dir.replace(\"gs://\", \"/gcs/\")\n",
        "model_id_gcsfuse = model_id.replace(\"gs://\", \"/gcs/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2366f130ebe"
      },
      "outputs": [],
      "source": [
        "# Prepare evaluation script that runs the evaluation harness.\n",
        "# We set `trust_remote_code = True` because evaluating the model requires\n",
        "#  executing code from the model repository.\n",
        "# We set `use_accelerate = True` to enable evaluation across multiple GPUs.\n",
        "script_path = \"./eval_script.py\"  # @param {type:\"string\"}\n",
        "\n",
        "eval_command = f\"\"\"import subprocess\n",
        "\n",
        "subprocess.call([\n",
        "    'python',\n",
        "    'main.py',\n",
        "    '--model',\n",
        "    'hf-causal-experimental',\n",
        "    '--model_args',\n",
        "    'pretrained={model_id_gcsfuse},trust_remote_code=True,use_accelerate=True,device_map_option=auto',\n",
        "    '--tasks',\n",
        "    '{eval_dataset}',\n",
        "    '--output_path',\n",
        "    '{eval_output_dir_gcsfuse}',\n",
        "])\n",
        "\"\"\"\n",
        "\n",
        "with open(script_path, \"w\") as fp:\n",
        "    fp.write(eval_command)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81af0f073f67"
      },
      "source": [
        "### Submit evaluation CustomJob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99a7eb37dc9c"
      },
      "outputs": [],
      "source": [
        "# Pass evaluation arguments and launch job.\n",
        "eval_job = aiplatform.CustomJob.from_local_script(\n",
        "    display_name=job_name,\n",
        "    script_path=script_path,\n",
        "    container_uri=EVAL_DOCKER_URI,\n",
        "    replica_count=replica_count,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    base_output_dir=eval_output_dir,\n",
        "    boot_disk_size_gb=500,\n",
        ")\n",
        "\n",
        "eval_job.run()\n",
        "\n",
        "print(\"Evaluation results were saved in:\", eval_output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66ddd462fe00"
      },
      "source": [
        "### Fetch and print evaluation results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1eb3ccaee95"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "from google.cloud import storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c68afa05a8e1"
      },
      "outputs": [],
      "source": [
        "# Fetch evaluation results.\n",
        "storage_client = storage.Client()\n",
        "BUCKET_NAME = GCS_BUCKET.split(\"gs://\")[1]\n",
        "bucket = storage_client.get_bucket(BUCKET_NAME)\n",
        "RESULT_FILE_PATH = eval_output_dir[len(GCS_BUCKET) + 1 :]\n",
        "blob = bucket.blob(RESULT_FILE_PATH)\n",
        "raw_result = blob.download_as_string()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f007661069a3"
      },
      "outputs": [],
      "source": [
        "# Print evaluation results.\n",
        "result = json.loads(raw_result)\n",
        "result_formatted = json.dumps(result, indent=2)\n",
        "print(f\"Evaluation result:\\n{result_formatted}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af21a3cff1e0"
      },
      "source": [
        "## Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "911406c1561e"
      },
      "outputs": [],
      "source": [
        "# Undeploy models and delete endpoints.\n",
        "endpoint.delete(force=True)\n",
        "endpoint_vllm.delete(force=True)\n",
        "\n",
        "# Delete models.\n",
        "model.delete()\n",
        "model_vllm.delete()\n",
        "\n",
        "# Delete evaluation job.\n",
        "eval_job.delete()\n",
        "\n",
        "# Uncomment below to delete all artifacts\n",
        "# !gsutil -m rm -r $STAGING_BUCKET $MODEL_BUCKET"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_pytorch_codellama.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d9bbf86da5e"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Vertex AI Model Garden GenAI Workshop for Instant ID\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_pytorch_instant_id_gradio.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_instant_id_gradio.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates starting a playground for model [InstantX/InstantID](https://www.gradio.app/InstantX/InstantID) based on [Gradio UI](https://www.gradio.app/), which allows users to interact with the identity-preserving image generation model more easily and intuitively.\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Deploy model to a [Vertex AI Endpoint resource](https://cloud.google.com/vertex-ai/docs/predictions/using-private-endpoints).\n",
        "- Run online predictions for `instant-id` tasks, from the UI.\n",
        "- Adjust the parameters, such as prompt, negative_prompt, num_inference_steps, and check out the generated images for best image quality.\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "879fca33129c"
      },
      "source": [
        "## Run the playground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "855d6b96f291"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project and prepare the dependencies\n",
        "\n",
        "# @markdown [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "! pip3 install --upgrade gradio==4.29.0 opencv-python\n",
        "# Uninstall nest-asyncio and uvloop as a workaround to https://github.com/gradio-app/gradio/issues/8238#issuecomment-2101066984\n",
        "! pip3 uninstall --yes nest-asyncio uvloop\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
        "\n",
        "# Set up the default SERVICE_ACCOUNT.\n",
        "SERVICE_ACCOUNT = None\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "\n",
        "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
        "\n",
        "# The pre-built serving docker image. It contains serving scripts and models.\n",
        "SERVE_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-diffusers-serve-opt:20240605_1400_RC00\"\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user(project_id=PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1cc26e68d7b0"
      },
      "outputs": [],
      "source": [
        "# @title Start the playground\n",
        "\n",
        "# @markdown This is a playground for generating identify-preserving model like [InstantX/InstantID](https://www.gradio.app/InstantX/InstantID).\n",
        "# @markdown After the cell runs, this playground is avaible in a separate browser tab if you click the public URL.\n",
        "# @markdown Sometsomething similar to [\"https://####.gradio.live\"](#) in the output of the cell.\n",
        "\n",
        "# @markdown **How to use:**\n",
        "# @markdown 1. Important: Notebook cell reruns create new public URLs. Previous URLs will stop working.\n",
        "# @markdown 1. Before you start, you need to select a Vertex prediction endpoint, with a matching model deployed to the endpoint\n",
        "# @markdown from the endpoint dropdown list, that has been deployed in the project and region;\n",
        "# @markdown 1. If no models were deployed in the past, you can create a new Vertex prediction\n",
        "# @markdown endpoint by selecting your favorite model and click \"Deploy\".\n",
        "# @markdown 1. New model deployment takes ~20 minutes. You can check the progress at [Vertex Online Prediction](https://console.cloud.google.com/vertex-ai/online-prediction/endpoints).\n",
        "# @markdown 1. Adjust the prompt/negative-prompt, image-dimension, inference steps, guidance-scale to achieve the optimum image quality and inference latency.\n",
        "# @markdown 1. Don't forget to undeploy the models after all the experiment to avoid continuous charges to the project.\n",
        "\n",
        "# @markdown Note: this workshop/notebook is specially built for the [InstantX/InstantID] model.\n",
        "# @markdown Other models may work, but they are not tested please use with caution.\n",
        "\n",
        "import base64\n",
        "from datetime import datetime\n",
        "from io import BytesIO\n",
        "\n",
        "import gradio as gr\n",
        "from google.cloud import aiplatform\n",
        "from PIL import Image\n",
        "\n",
        "style_list = [\n",
        "    {\n",
        "        \"name\": \"(No style)\",\n",
        "        \"prompt\": \"{prompt}\",\n",
        "        \"negative_prompt\": \"\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Cinematic\",\n",
        "        \"prompt\": \"cinematic still {prompt} . emotional, harmonious, vignette, highly detailed, high budget, bokeh, cinemascope, moody, epic, gorgeous, film grain, grainy\",\n",
        "        \"negative_prompt\": \"anime, cartoon, graphic, text, painting, crayon, graphite, abstract, glitch, deformed, mutated, ugly, disfigured\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Photographic\",\n",
        "        \"prompt\": \"cinematic photo {prompt} . 35mm photograph, film, bokeh, professional, 4k, highly detailed\",\n",
        "        \"negative_prompt\": \"drawing, painting, crayon, sketch, graphite, impressionist, noisy, blurry, soft, deformed, ugly\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Anime\",\n",
        "        \"prompt\": \"anime artwork {prompt} . anime style, key visual, vibrant, studio anime,  highly detailed\",\n",
        "        \"negative_prompt\": \"photo, deformed, black and white, realism, disfigured, low contrast\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Manga\",\n",
        "        \"prompt\": \"manga style {prompt} . vibrant, high-energy, detailed, iconic, Japanese comic style\",\n",
        "        \"negative_prompt\": \"ugly, deformed, noisy, blurry, low contrast, realism, photorealistic, Western comic style\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Digital Art\",\n",
        "        \"prompt\": \"concept art {prompt} . digital artwork, illustrative, painterly, matte painting, highly detailed\",\n",
        "        \"negative_prompt\": \"photo, photorealistic, realism, ugly\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Pixel art\",\n",
        "        \"prompt\": \"pixel-art {prompt} . low-res, blocky, pixel art style, 8-bit graphics\",\n",
        "        \"negative_prompt\": \"sloppy, messy, blurry, noisy, highly detailed, ultra textured, photo, realistic\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Fantasy art\",\n",
        "        \"prompt\": \"ethereal fantasy concept art of  {prompt} . magnificent, celestial, ethereal, painterly, epic, majestic, magical, fantasy art, cover art, dreamy\",\n",
        "        \"negative_prompt\": \"photographic, realistic, realism, 35mm film, dslr, cropped, frame, text, deformed, glitch, noise, noisy, off-center, deformed, cross-eyed, closed eyes, bad anatomy, ugly, disfigured, sloppy, duplicate, mutated, black and white\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Neonpunk\",\n",
        "        \"prompt\": \"neonpunk style {prompt} . cyberpunk, vaporwave, neon, vibes, vibrant, stunningly beautiful, crisp, detailed, sleek, ultramodern, magenta highlights, dark purple shadows, high contrast, cinematic, ultra detailed, intricate, professional\",\n",
        "        \"negative_prompt\": \"painting, drawing, illustration, glitch, deformed, mutated, cross-eyed, ugly, disfigured\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"3D Model\",\n",
        "        \"prompt\": \"professional 3d model {prompt} . octane render, highly detailed, volumetric, dramatic lighting\",\n",
        "        \"negative_prompt\": \"ugly, deformed, noisy, low poly, blurry, painting\",\n",
        "    },\n",
        "]\n",
        "\n",
        "styles = {k[\"name\"]: (k[\"prompt\"], k[\"negative_prompt\"]) for k in style_list}\n",
        "STYLE_NAMES = list(styles.keys())\n",
        "DEFAULT_STYLE_NAME = \"(No style)\"\n",
        "\n",
        "\n",
        "def create_job_name(prefix):\n",
        "    now = datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
        "    job_name = f\"{prefix}-gradio-{now}\"\n",
        "    return job_name\n",
        "\n",
        "\n",
        "def base64_to_image(image_str: str) -> Image:\n",
        "    \"\"\"Convert base64 encoded string to an image.\"\"\"\n",
        "    image = Image.open(BytesIO(base64.b64decode(image_str)))\n",
        "    return image\n",
        "\n",
        "\n",
        "def image_to_base64(image: Image, format=\"JPEG\") -> str:\n",
        "    buffer = BytesIO()\n",
        "    image.save(buffer, format=format)\n",
        "    image_str = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
        "    return image_str\n",
        "\n",
        "\n",
        "def is_instantid_endpoint(endpoint: aiplatform.Endpoint) -> bool:\n",
        "    \"\"\"Returns True if the endpoint is an Instant ID endpoint.\"\"\"\n",
        "    return (\n",
        "        \"instant_id\" in endpoint.display_name.lower()\n",
        "        or \"instant-id\" in endpoint.display_name.lower()\n",
        "        or \"instant\" in endpoint.display_name.lower()\n",
        "    )\n",
        "\n",
        "\n",
        "def list_endpoints() -> list[str]:\n",
        "    \"\"\"Returns all valid prediction endpoints for in the project and region.\"\"\"\n",
        "    # Gets all the valid endpoints in the project and region.\n",
        "    endpoints = aiplatform.Endpoint.list(order_by=\"create_time desc\")\n",
        "    # Filters out the endpoints which do not have a deployed model, and the endpoint is for image generation\n",
        "    endpoints = list(\n",
        "        filter(\n",
        "            lambda endpoint: endpoint.traffic_split and is_instantid_endpoint(endpoint),\n",
        "            endpoints,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    endpoint_names = list(\n",
        "        map(\n",
        "            lambda endpoint: f\"{endpoint.name} - {endpoint.display_name[:40]}\",\n",
        "            endpoints,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return endpoint_names\n",
        "\n",
        "\n",
        "def get_endpoint(endpoint_name: str) -> aiplatform.Endpoint:\n",
        "    \"\"\"Returns a Vertex endpoint for the given endpoint_name.\"\"\"\n",
        "\n",
        "    endpoint_id = endpoint_name.split(\" - \")[0]\n",
        "    endpoint = aiplatform.Endpoint(\n",
        "        f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_id}\"\n",
        "    )\n",
        "\n",
        "    return endpoint\n",
        "\n",
        "\n",
        "def get_task_name(model_name: str) -> str:\n",
        "    \"\"\"Returns the corresponding task name for the given model_name.\"\"\"\n",
        "\n",
        "    model_to_task_dict = {\n",
        "        \"instantx/instantid\": \"instant-id\",\n",
        "    }\n",
        "\n",
        "    if model_name.lower() not in model_to_task_dict.keys():\n",
        "        print(model_name)\n",
        "        raise gr.Error(\"Please select a valid model name for Endpoint creation.\")\n",
        "\n",
        "    return model_to_task_dict[model_name.lower()]\n",
        "\n",
        "\n",
        "def deploy_model(model_name: str) -> aiplatform.Endpoint:\n",
        "    \"\"\"Creates a new Vertex prediction endpoint and deploys a model to it.\"\"\"\n",
        "\n",
        "    if not model_name:\n",
        "        raise gr.Error(\"Please select a valid model name for model list.\")\n",
        "        return\n",
        "\n",
        "    gr.Info(\"Model deployment started. Please wait...\")\n",
        "\n",
        "    model_id = model_name.split(\": \")[1]\n",
        "    task_name = get_task_name(model_id)\n",
        "\n",
        "    display_name = create_job_name(model_id)\n",
        "    endpoint = aiplatform.Endpoint.create(display_name=display_name)\n",
        "    serving_env = {\n",
        "        \"MODEL_ID\": model_id,\n",
        "        \"TASK\": task_name,\n",
        "        \"DEPLOY_SOURCE\": \"notebook_gradio\",\n",
        "    }\n",
        "\n",
        "    display_name = create_job_name(model_id)\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_id,\n",
        "        serving_container_image_uri=SERVE_DOCKER_URI,\n",
        "        serving_container_ports=[7080],\n",
        "        serving_container_predict_route=\"/predictions/diffusers_serving\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "        serving_container_environment_variables=serving_env,\n",
        "    )\n",
        "    machine_type = \"g2-standard-8\"\n",
        "    accelerator_type = \"NVIDIA_L4\"\n",
        "\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=1,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=SERVICE_ACCOUNT,\n",
        "        sync=False,\n",
        "    )\n",
        "\n",
        "    gr.Info(\n",
        "        f\"Model {display_name} is being deployed. It may take ~20 minutes to complete.\"\n",
        "    )\n",
        "\n",
        "    return endpoint\n",
        "\n",
        "\n",
        "def get_default_dimension(model_name: str) -> int:\n",
        "    \"\"\"Returns the default dimension for the given model_name.\"\"\"\n",
        "\n",
        "    return 1024\n",
        "\n",
        "\n",
        "def get_default_guidance_scale(model_name: str) -> int:\n",
        "    \"\"\"Returns the default guidance scale for the given model_name.\"\"\"\n",
        "\n",
        "    return 1.2\n",
        "\n",
        "\n",
        "def get_default_num_inference_steps(model_name: str) -> int:\n",
        "    \"\"\"Returns the default num_inference_steps for the given model_name.\"\"\"\n",
        "\n",
        "    return 5\n",
        "\n",
        "\n",
        "def apply_style(style_name: str, positive: str, negative: str = \"\") -> tuple[str, str]:\n",
        "    p, n = styles.get(style_name, styles[DEFAULT_STYLE_NAME])\n",
        "    return p.replace(\"{prompt}\", positive), n + negative\n",
        "\n",
        "\n",
        "def generate_images(\n",
        "    endpoint_name,\n",
        "    style_name=None,\n",
        "    prompt=\"\",\n",
        "    negative_prompt=\"\",\n",
        "    guidance_scale=1.2,\n",
        "    num_inference_steps=5,\n",
        "    image_dimension=1024,\n",
        "    face_image=None,\n",
        "    pose_image=None,\n",
        ") -> list[Image.Image]:\n",
        "    if not endpoint_name:\n",
        "        raise gr.Error(\"Please select (or deploy) a model first!\")\n",
        "\n",
        "    prompt, negative_prompt = apply_style(style_name, prompt, negative_prompt)\n",
        "    payload = {\n",
        "        \"prompt\": prompt,\n",
        "        \"negative_prompt\": negative_prompt,\n",
        "        \"height\": image_dimension,\n",
        "        \"width\": image_dimension,\n",
        "        \"guidance_scale\": guidance_scale,\n",
        "        \"num_inference_steps\": num_inference_steps,\n",
        "        \"face_image\": image_to_base64(face_image),\n",
        "    }\n",
        "\n",
        "    if pose_image:\n",
        "        payload = {\n",
        "            **payload,\n",
        "            \"pose_image\": image_to_base64(pose_image),\n",
        "        }\n",
        "\n",
        "    instances = [\n",
        "        payload,\n",
        "    ]\n",
        "\n",
        "    response = get_endpoint(endpoint_name).predict(instances=instances)\n",
        "    images = [base64_to_image(image) for image in response.predictions]\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "def update_default_parameters(model_name: str):\n",
        "    \"\"\"Updates the default inference parameters based on the selected model.\"\"\"\n",
        "    return {\n",
        "        guidance_scale: gr.update(value=get_default_guidance_scale(model_name)),\n",
        "        num_inference_steps: gr.update(\n",
        "            value=get_default_num_inference_steps(model_name)\n",
        "        ),\n",
        "        image_dimension: gr.update(value=get_default_dimension(model_name)),\n",
        "    }\n",
        "\n",
        "\n",
        "tip_text = r\"\"\"\n",
        "1. Select a Vertex prediction endpoint with a model deployed for your chosen task. Mismatched models can lead to unreliable outcomes.\n",
        "2. New model deployment takes ~20 minutes. You can check the progress at [Vertex Online Prediction](https://console.cloud.google.com/vertex-ai/online-prediction/endpoints).\n",
        "3. After the model deployment is complete, restart the playground in Colab to see the updated endpoint list.\n",
        "\"\"\"\n",
        "\n",
        "css = \"\"\"\n",
        ".gradio-container {\n",
        "  width: 90% !important\n",
        "}\n",
        "\"\"\"\n",
        "with gr.Blocks(\n",
        "    css=css, theme=gr.themes.Default(primary_hue=\"orange\", secondary_hue=\"blue\")\n",
        ") as demo:\n",
        "    gr.Markdown(\"# Model Garden Playground for InstantID\")\n",
        "\n",
        "    with gr.Accordion(\"How To Use\", open=False):\n",
        "        tip = gr.Markdown(tip_text)\n",
        "\n",
        "    with gr.Row(equal_height=True):\n",
        "        with gr.Column(scale=3):\n",
        "            prompt = gr.Textbox(label=\"Prompt\", lines=1)\n",
        "            negative_prompt = gr.Textbox(label=\"Negative Prompt\", lines=1)\n",
        "        with gr.Column(scale=1):\n",
        "            endpoint_name = gr.Dropdown(\n",
        "                label=\"Select a model previously deployed on Vertex\",\n",
        "                choices=list_endpoints(),\n",
        "                value=None,\n",
        "            )\n",
        "            with gr.Row():\n",
        "                selected_model = gr.Dropdown(\n",
        "                    scale=7,\n",
        "                    label=\"Deploy a new model to Vertex\",\n",
        "                    choices=[\n",
        "                        \"instant-id: instantx/instantid\",\n",
        "                    ],\n",
        "                    value=None,\n",
        "                )\n",
        "                deploy_model_button = gr.Button(\n",
        "                    \"Deploy\", scale=1, variant=\"primary\", min_width=10\n",
        "                )\n",
        "\n",
        "    with gr.Row(equal_height=True):\n",
        "        with gr.Column(scale=1):\n",
        "            generate_button = gr.Button(\"Generate\", variant=\"primary\")\n",
        "\n",
        "            image_dimension = gr.Slider(\n",
        "                label=\"Image dimension\", value=1024, step=128, minimum=512, maximum=1024\n",
        "            )\n",
        "            num_inference_steps = gr.Slider(\n",
        "                label=\"Sampling steps\", value=5, step=1, minimum=1, maximum=25\n",
        "            )\n",
        "            guidance_scale = gr.Slider(\n",
        "                label=\"Guidance scale\", value=1.2, step=0.1, minimum=0, maximum=10.0\n",
        "            )\n",
        "            with gr.Accordion(\"Styles\", open=False):\n",
        "                style_selection = gr.Radio(\n",
        "                    show_label=True,\n",
        "                    container=True,\n",
        "                    interactive=True,\n",
        "                    choices=STYLE_NAMES,\n",
        "                    value=DEFAULT_STYLE_NAME,\n",
        "                    label=\"Image Style\",\n",
        "                )\n",
        "\n",
        "        with gr.Column(scale=4):\n",
        "            with gr.Row(equal_height=True):\n",
        "                with gr.Column(scale=2):\n",
        "                    face_image_input = gr.Image(\n",
        "                        type=\"pil\",\n",
        "                        label=\"Upload a photo of your face\",\n",
        "                        sources=\"upload\",\n",
        "                        height=350,\n",
        "                        interactive=True,\n",
        "                    )\n",
        "                    pose_image_input = gr.Image(\n",
        "                        type=\"pil\",\n",
        "                        label=\"Upload a reference pose image (Optional)\",\n",
        "                        sources=\"upload\",\n",
        "                        height=350,\n",
        "                        interactive=True,\n",
        "                    )\n",
        "                with gr.Column(scale=3):\n",
        "                    image_output = gr.Gallery(\n",
        "                        label=\"Generated Images\",\n",
        "                        rows=1,\n",
        "                        height=715,\n",
        "                        preview=True,\n",
        "                    )\n",
        "\n",
        "    endpoint_name.change(\n",
        "        update_default_parameters,\n",
        "        endpoint_name,\n",
        "        [\n",
        "            guidance_scale,\n",
        "            num_inference_steps,\n",
        "            image_dimension,\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    deploy_model_button.click(\n",
        "        deploy_model,\n",
        "        inputs=[selected_model],\n",
        "        outputs=[],\n",
        "    )\n",
        "\n",
        "    generate_button.click(\n",
        "        generate_images,\n",
        "        inputs=[\n",
        "            endpoint_name,\n",
        "            style_selection,\n",
        "            prompt,\n",
        "            negative_prompt,\n",
        "            guidance_scale,\n",
        "            num_inference_steps,\n",
        "            image_dimension,\n",
        "            face_image_input,\n",
        "            pose_image_input,\n",
        "        ],\n",
        "        outputs=image_output,\n",
        "    )\n",
        "\n",
        "show_debug_logs = True  # @param {type: \"boolean\"}\n",
        "demo.queue()\n",
        "demo.launch(share=True, inline=False, debug=show_debug_logs, show_error=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_pytorch_instant_id_gradio.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

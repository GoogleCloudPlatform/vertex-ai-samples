{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8S-yo8qTIcO"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTRywGxLTZfU"
      },
      "source": [
        "# Vertex AI Model Garden - Gemma Evaluation\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_gemma_evaluation.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma_evaluation.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CXS0vZfT8_7"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates evaluating pre-trained and instruction-tuned Gemma models in Vertex AI.\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Evaluate pre-trained and instruction-tuned Gemma model on any of the benchmark datasets\n",
        "- Clean up the resources\n",
        "\n",
        "| Models |\n",
        "| :- |\n",
        "| [google/gemma-2b](https://huggingface.co/google/gemma-2b)\n",
        "| [google/gemma-2b-it](https://huggingface.co/google/gemma-2b-it)\n",
        "| [google/gemma-7b](https://huggingface.co/google/gemma-7b)\n",
        "| [google/gemma-7b-it](https://huggingface.co/google/gemma-7b-it)\n",
        "| [google/gemma-1.1-2b-it](https://huggingface.co/google/gemma-1.1-2b-it)\n",
        "| [google/gemma-1.1-7b-it](https://huggingface.co/google/gemma-1.1-7b-it)\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCY8PGrFUbT1"
      },
      "source": [
        "## Run the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "81CC3tL1T_TL"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown 2. [Optional] [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs. Set the BUCKET_URI for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`) should be located in the same region as where the notebook was launched. Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\"). If not set, a unique GCS bucket will be created instead.\n",
        "\n",
        "# Import the necessary packages\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
        "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
        "\n",
        "# Cloud Storage bucket for storing the experiment artifacts.\n",
        "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
        "# prefer using your own GCS bucket, please change the value yourself below.\n",
        "now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
        "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
        "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
        "    # Create a unique GCS bucket for this notebook, if not specified by the user\n",
        "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}\"\n",
        "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
        "else:\n",
        "    shell_output = ! gsutil ls -Lb {BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
        "    bucket_region = shell_output[0].strip().lower()\n",
        "    if bucket_region != REGION:\n",
        "        raise ValueError(\n",
        "            \"Bucket region %s is different from notebook region %s\"\n",
        "            % (bucket_region, REGION)\n",
        "        )\n",
        "\n",
        "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
        "\n",
        "# Gets the default BUCKET_URI and SERVICE_ACCOUNT if they were not specified by the user.\n",
        "SERVICE_ACCOUNT = None\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
        "\n",
        "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "STAGING_BUCKET = os.path.join(BUCKET_URI, \"staging\")\n",
        "MODEL_BUCKET = os.path.join(STAGING_BUCKET, \"model\")\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "! gcloud config set project $PROJECT_ID\n",
        "\n",
        "# The evaluation docker image.\n",
        "EVAL_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-lm-evaluation-harness:20240320_0655_RC00\"\n",
        "\n",
        "# Define common functions\n",
        "\n",
        "\n",
        "def get_job_name_with_datetime(prefix: str) -> str:\n",
        "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
        "    jobs in Vertex AI.\n",
        "    \"\"\"\n",
        "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
        "\n",
        "\n",
        "def get_quota(project_id: str, region: str, resource_id: str) -> int:\n",
        "  \"\"\"Returns the quota for a resource in a region. Returns -1 if can not figure out the quota.\"\"\"\n",
        "  service_endpoint = \"aiplatform.googleapis.com\"\n",
        "  quota_list_output = !gcloud alpha services quota list --service=$service_endpoint  --consumer=projects/$project_id --filter=\"$service_endpoint/$resource_id\" --format=json\n",
        "  # Use '.s' on the command output because it is an SList type.\n",
        "  quota_data = json.loads(quota_list_output.s)\n",
        "  if len(quota_data) == 0 or \"consumerQuotaLimits\" not in quota_data[0]:\n",
        "    return -1\n",
        "  if len(quota_data[0][\"consumerQuotaLimits\"]) == 0 or \"quotaBuckets\" not in quota_data[0][\"consumerQuotaLimits\"][0]:\n",
        "    return -1\n",
        "  all_regions_data = quota_data[0][\"consumerQuotaLimits\"][0][\"quotaBuckets\"]\n",
        "  for region_data in all_regions_data:\n",
        "    if region_data.get('dimensions') and region_data['dimensions']['region'] == region:\n",
        "      if 'effectiveLimit' in region_data:\n",
        "        return int(region_data['effectiveLimit'])\n",
        "      else:\n",
        "        return 0\n",
        "  return -1\n",
        "\n",
        "\n",
        "def get_resource_id(accelerator_type: str, is_for_training: bool) -> str:\n",
        "  \"\"\"Returns the resource id for a given accelerator type and the use case.\n",
        "  Args:\n",
        "    accelerator_type: The accelerator type.\n",
        "    is_for_training: Whether the resource is used for training. Set false\n",
        "    for serving use case.\n",
        "  Returns:\n",
        "    The resource id.\n",
        "  \"\"\"\n",
        "  training_accelerator_map = {\n",
        "      \"NVIDIA_TESLA_V100\": \"custom_model_training_nvidia_v100_gpus\",\n",
        "      \"NVIDIA_L4\": \"custom_model_training_nvidia_l4_gpus\",\n",
        "      \"NVIDIA_TESLA_A100\": \"custom_model_training_nvidia_a100_gpus\",\n",
        "      \"NVIDIA_TESLA_T4\": \"custom_model_training_nvidia_t4_gpus\",\n",
        "      \"TPU_V5e\": \"custom_model_training_tpu_v5e\",\n",
        "      \"TPU_V3\": \"custom_model_training_tpu_v3\",\n",
        "  }\n",
        "  serving_accelerator_map = {\n",
        "      \"NVIDIA_TESLA_V100\": \"custom_model_serving_nvidia_v100_gpus\",\n",
        "      \"NVIDIA_L4\": \"custom_model_serving_nvidia_l4_gpus\",\n",
        "      \"NVIDIA_TESLA_A100\": \"custom_model_serving_nvidia_a100_gpus\",\n",
        "      \"NVIDIA_TESLA_T4\": \"custom_model_serving_nvidia_t4_gpus\",\n",
        "      \"TPU_V5e\": \"custom_model_serving_tpu_v5e\",\n",
        "  }\n",
        "  if is_for_training:\n",
        "    if accelerator_type in training_accelerator_map:\n",
        "      return training_accelerator_map[accelerator_type]\n",
        "    else:\n",
        "      raise ValueError(\n",
        "          f\"Could not find accelerator type: {accelerator_type} for training.\"\n",
        "      )\n",
        "  else:\n",
        "    if accelerator_type in serving_accelerator_map:\n",
        "      return serving_accelerator_map[accelerator_type]\n",
        "    else:\n",
        "      raise ValueError(\n",
        "          f\"Could not find accelerator type: {accelerator_type} for serving.\"\n",
        "      )\n",
        "\n",
        "\n",
        "def check_quota(project_id:str, region: str, accelerator_type: str,\n",
        "                accelerator_count: int, is_for_training: bool):\n",
        "  \"\"\"Checks if the project and the region has the required quota.\"\"\"\n",
        "  resource_id = get_resource_id(accelerator_type, is_for_training)\n",
        "  quota = get_quota(project_id, region, resource_id)\n",
        "  quota_request_instruction = (\"Either use \"\n",
        "            \"a different region or request additional quota. Follow \"\n",
        "            \"instructions here \"\n",
        "            \"https://cloud.google.com/docs/quotas/view-manage#requesting_higher_quota\"\n",
        "            \" to check quota in a region or request additional quota for \"\n",
        "            \"your project.\")\n",
        "  if quota == -1:\n",
        "    raise ValueError(\n",
        "            f\"\"\"Quota not found for: {resource_id} in {region}.\n",
        "            {quota_request_instruction}\"\"\"\n",
        "        )\n",
        "  if quota < accelerator_count:\n",
        "    raise ValueError(\n",
        "            f\"\"\"Quota not enough for {resource_id} in {region}:\n",
        "            {quota} < {accelerator_count}.\n",
        "            {quota_request_instruction}\"\"\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pNHMbjr0UjrK"
      },
      "outputs": [],
      "source": [
        "# @title Evaluate Gemma models\n",
        "\n",
        "# @markdown This section demonstrates how to evaluate the Gemma models with and without finetuned LoRA adapters using EleutherAI's [Language Model Evaluation Harness (lm-evaluation-harness)](https://github.com/EleutherAI/lm-evaluation-harness) with Vertex CustomJob. Please reference the peak GPU memory usage for serving and adjust the machine type, accelerator type and accelerator count accordingly.\n",
        "\n",
        "# @markdown You must provide a Hugging Face User Access Token (read) to access the Gemma models. You can follow the [Hugging Face documentation](https://huggingface.co/docs/hub/en/security-tokens) to create a **read** access token and put it in the `HF_TOKEN` field below.\n",
        "HF_TOKEN = \"\"  # @param {type:\"string\", isTemplate:true}\n",
        "\n",
        "# @markdown This example uses the dataset [HellaSwag](https://arxiv.org/abs/1905.07830). All supported tasks are listed in [this task table](https://github.com/EleutherAI/lm-evaluation-harness/blob/master/docs/task_table.md).\n",
        "# @markdown Set evaluation dataset.\n",
        "eval_dataset = \"hellaswag\"  # @param {type:\"string\"}\n",
        "\n",
        "# Worker pool spec.\n",
        "# Find Vertex AI supported accelerators and regions in:\n",
        "#  https://cloud.google.com/vertex-ai/docs/training/configure-compute\n",
        "\n",
        "\n",
        "# Setup evaluation job.\n",
        "# @markdown Set the base model id.\n",
        "base_model_id = \"google/gemma-1.1-2b-it\"  # @param[\"google/gemma-2b\", \"google/gemma-2b-it\", \"google/gemma-7b\", \"google/gemma-7b-it\", \"google/gemma-1.1-2b-it\", \"google/gemma-1.1-7b-it\"] {isTemplate:true}\n",
        "job_name = get_job_name_with_datetime(prefix=\"gemma-eval\")\n",
        "eval_output_dir = os.path.join(MODEL_BUCKET, job_name)\n",
        "eval_output_dir_gcsfuse = eval_output_dir.replace(\"gs://\", \"/gcs/\")\n",
        "\n",
        "# @markdown Set the accelerator type.\n",
        "accelerator_type = \"NVIDIA_L4\"  # @param[\"NVIDIA_TESLA_V100\", \"NVIDIA_L4\", \"NVIDIA_TESLA_A100\"]\n",
        "\n",
        "# @markdown To evaluate a PEFT-finetuned model, enter the PEFT output directory to the LoRA adapter below.\n",
        "# @markdown Otherwise, leave it empty.\n",
        "# @markdown See the [finetuning notebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma_finetuning_on_vertex.ipynb) for more details.\n",
        "# @markdown Set the PEFT output directory.\n",
        "peft_output_dir = \"\"  # @param {type:\"string\"}\n",
        "peft_output_dir_gcsfuse = peft_output_dir.replace(\"gs://\", \"/gcs/\")\n",
        "\n",
        "if accelerator_type == \"NVIDIA_TESLA_A100\":\n",
        "    machine_type = \"a2-highgpu-1g\"\n",
        "    accelerator_count = 1\n",
        "elif accelerator_type == \"NVIDIA_TESLA_V100\":\n",
        "    machine_type = \"n1-standard-8\"\n",
        "    accelerator_count = 2\n",
        "elif accelerator_type == \"NVIDIA_L4\":\n",
        "    machine_type = \"g2-standard-8\"\n",
        "    accelerator_count = 1\n",
        "else:\n",
        "    print(f\"Unsupported accelerator type: {accelerator_type}\")\n",
        "\n",
        "replica_count = 1\n",
        "\n",
        "check_quota(project_id=PROJECT_ID,\n",
        "            region=REGION,\n",
        "            accelerator_type=accelerator_type,\n",
        "            accelerator_count=accelerator_count,\n",
        "            is_for_training=True)\n",
        "\n",
        "# Prepare evaluation command that runs the evaluation harness.\n",
        "# Set `trust_remote_code = True` because evaluating the model requires\n",
        "# executing code from the model repository.\n",
        "# Set `use_accelerate = True` to enable evaluation across multiple GPUs.\n",
        "eval_command = [\n",
        "    \"lm_eval\",\n",
        "    \"--model\",\n",
        "    \"hf\",\n",
        "    \"--tasks\",\n",
        "    f\"{eval_dataset}\",\n",
        "    \"--output_path\",\n",
        "    f\"{eval_output_dir_gcsfuse}\",\n",
        "]\n",
        "\n",
        "if peft_output_dir_gcsfuse:\n",
        "    eval_command += [\n",
        "        \"--model_args\",\n",
        "        f\"pretrained={base_model_id},peft={peft_output_dir_gcsfuse},trust_remote_code=True,parallelize=True,device_map_option=auto\",\n",
        "    ]\n",
        "else:\n",
        "    eval_command += [\n",
        "        \"--model_args\",\n",
        "        f\"pretrained={base_model_id},trust_remote_code=True,parallelize=True,device_map_option=auto\",\n",
        "    ]\n",
        "\n",
        "# Pass evaluation arguments and launch job.\n",
        "worker_pool_specs = [\n",
        "    {\n",
        "        \"machine_spec\": {\n",
        "            \"machine_type\": machine_type,\n",
        "            \"accelerator_type\": accelerator_type,\n",
        "            \"accelerator_count\": accelerator_count,\n",
        "        },\n",
        "        \"replica_count\": replica_count,\n",
        "        \"disk_spec\": {\n",
        "            \"boot_disk_size_gb\": 500,\n",
        "        },\n",
        "        \"container_spec\": {\n",
        "            \"image_uri\": EVAL_DOCKER_URI,\n",
        "            \"env\": [\n",
        "                {\n",
        "                    \"name\": \"HF_TOKEN\",\n",
        "                    \"value\": HF_TOKEN,\n",
        "                }\n",
        "            ],\n",
        "            \"command\": eval_command,\n",
        "            \"args\": [],\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "eval_job = aiplatform.CustomJob(\n",
        "    display_name=job_name,\n",
        "    worker_pool_specs=worker_pool_specs,\n",
        "    base_output_dir=eval_output_dir,\n",
        ")\n",
        "\n",
        "eval_job.run()\n",
        "\n",
        "print(\"Evaluation results were saved in:\", eval_output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CVBxGpwWU3kY"
      },
      "outputs": [],
      "source": [
        "# @title Fetch and print evaluation results\n",
        "import json\n",
        "\n",
        "from google.cloud import storage\n",
        "\n",
        "# Fetch evaluation results.\n",
        "storage_client = storage.Client()\n",
        "BUCKET_NAME = BUCKET_URI.split(\"gs://\")[1]\n",
        "bucket = storage_client.get_bucket(BUCKET_NAME)\n",
        "RESULT_FILE_PATH = eval_output_dir[len(BUCKET_URI) + 1 :] + \"/results.json\"\n",
        "blob = bucket.blob(RESULT_FILE_PATH)\n",
        "raw_result = blob.download_as_string()\n",
        "\n",
        "# Print evaluation results.\n",
        "result = json.loads(raw_result)\n",
        "result_formatted = json.dumps(result, indent=2)\n",
        "print(f\"Evaluation result:\\n{result_formatted}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qWN3cl_VU7pa"
      },
      "outputs": [],
      "source": [
        "# @title Clean up resources\n",
        "# Delete evaluation job.\n",
        "\n",
        "delete_bucket = False  # @param {type:\"boolean\"}\n",
        "if delete_bucket:\n",
        "    ! gsutil -m rm -r $BUCKET_URI\n",
        "    # Uncomment below to delete all artifacts\n",
        "    # !gsutil -m rm -r $STAGING_BUCKET $MODEL_BUCKET $EXPERIMENT_BUCKET\n",
        "\n",
        "eval_job.delete()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_gemma_evaluation.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

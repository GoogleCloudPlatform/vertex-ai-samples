{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d9bbf86da5e"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Vertex AI Model Garden - Stable Diffusion XL 1.0 (Dreambooth LoRA Finetuning)\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_pytorch_sd_xl_finetuning_dreambooth_lora.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_sd_xl_finetuning_dreambooth_lora.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates finetuning [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) with [Dreambooth LoRA](https://huggingface.co/docs/diffusers/en/training/sdxl) and deploying it on Vertex AI for online prediction.\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Finetune the [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) model with [Dreambooth LoRA](https://huggingface.co/docs/diffusers/en/training/sdxl).\n",
        "- Upload the model to [Vertex AI Model Registry](https://cloud.google.com/vertex-ai/docs/model-registry/introduction).\n",
        "- Deploy the model to a [Vertex AI Endpoint resource](https://cloud.google.com/vertex-ai/docs/predictions/using-private-endpoints).\n",
        "- Run online predictions for text-to-image.\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6546207faba5"
      },
      "source": [
        "## LoRA Finetune with Dreambooth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2707b02ef5df"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown 2. [Optional] [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing\n",
        "# @markdown experiment outputs. Set the BUCKET_URI for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`)\n",
        "# @markdown should be located in the same region as where the notebook was launched. Note that a multi-region bucket (eg. \"us\") is\n",
        "# @markdown not considered a match for a single region covered by the multi-region range (eg. \"us-central1\").\n",
        "# @markdown If not set, a unique GCS bucket will be created instead.\n",
        "\n",
        "import base64\n",
        "import glob\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "from io import BytesIO\n",
        "\n",
        "import requests\n",
        "from google.cloud import aiplatform, storage\n",
        "from PIL import Image\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
        "print(\"Enabling Vertex AI and Compute Engine API.\")\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
        "\n",
        "# Cloud Storage bucket for storing the experiment artifacts.\n",
        "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
        "# prefer using your own GCS bucket, change the value yourself below.\n",
        "now = datetime.now().strftime(\"%Y%m%d%-H%M%S\")\n",
        "BUCKET_URI = \"gs://\"  # @param {type: \"string\"}\n",
        "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "\n",
        "assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
        "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
        "    # Create a unique GCS bucket for this notebook, if not specified by the user\n",
        "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}-{str(uuid.uuid4())[:4]}\"\n",
        "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
        "else:\n",
        "    shell_output = ! gsutil ls -Lb {BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
        "    bucket_region = shell_output[0].strip().lower()\n",
        "    if bucket_region != REGION:\n",
        "        raise ValueError(\n",
        "            \"Bucket region %s is different from notebook region %s\"\n",
        "            % (bucket_region, REGION)\n",
        "        )\n",
        "\n",
        "# Cloud Storage bucket for storing the experiment artifacts.\n",
        "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
        "# prefer using your own GCS bucket, change the value yourself below.\n",
        "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
        "\n",
        "# Set up the default SERVICE_ACCOUNT.\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "SERVICE_ACCOUNT_CC = (\n",
        "    f\"service-{project_number}@gcp-sa-aiplatform-cc.iam.gserviceaccount.com\"\n",
        ")\n",
        "\n",
        "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
        "\n",
        "# Provision permissions to the two SERVICE_ACCOUNTs with the GCS bucket\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT_CC}:roles/storage.admin $BUCKET_NAME\n",
        "\n",
        "# The pre-built training docker images. They contain training scripts and models.\n",
        "TRAIN_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-train:20240318_0936_RC00\"\n",
        "# The pre-built serving docker images. They contains serving scripts and models.\n",
        "SERVE_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-diffusers-serve-opt:20240605_1400_RC00\"\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user(project_id=PROJECT_ID)\n",
        "\n",
        "\n",
        "def create_job_name(prefix):\n",
        "    now = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    job_name = f\"{prefix}-{now}\"\n",
        "    return job_name\n",
        "\n",
        "\n",
        "def base64_to_image(image_str):\n",
        "    \"\"\"Convert base64 encoded string to an image.\"\"\"\n",
        "    image = Image.open(BytesIO(base64.b64decode(image_str)))\n",
        "    return image\n",
        "\n",
        "\n",
        "def image_to_base64(image, format=\"JPEG\"):\n",
        "    buffer = BytesIO()\n",
        "    image.save(buffer, format=format)\n",
        "    image_str = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
        "    return image_str\n",
        "\n",
        "\n",
        "def download_image(url):\n",
        "    response = requests.get(url)\n",
        "    return Image.open(BytesIO(response.content))\n",
        "\n",
        "\n",
        "def image_grid(imgs, rows=2, cols=2):\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new(\n",
        "        mode=\"RGB\", size=(cols * w + 10 * cols, rows * h), color=(255, 255, 255)\n",
        "    )\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i % cols * w + 10 * i, i // cols * h))\n",
        "    return grid\n",
        "\n",
        "\n",
        "def get_bucket_and_blob_name(filepath):\n",
        "    # The gcs path is of the form gs://<bucket-name>/<blob-name>\n",
        "    gs_suffix = filepath.split(\"gs://\", 1)[1]\n",
        "    return tuple(gs_suffix.split(\"/\", 1))\n",
        "\n",
        "\n",
        "def download_gcs_dir_to_local(gcs_dir_path, local_dir_path):\n",
        "    \"\"\"Downloads files in a GCS directory to a local directory.\"\"\"\n",
        "    assert gcs_dir_path.startswith(\"gs://\"), \"gcs_dir_path must start with `gs://`.\"\n",
        "    bucket_name = gcs_dir_path.split(\"/\")[2]\n",
        "    prefix = gcs_dir_path[len(\"gs://\" + bucket_name) :].strip(\"/\") + \"/\"\n",
        "    client = storage.Client()\n",
        "    blobs = client.list_blobs(bucket_name, prefix=prefix)\n",
        "    for blob in blobs:\n",
        "        if blob.name[-1] == \"/\":\n",
        "            continue\n",
        "        file_path = blob.name[len(prefix) :].strip(\"/\")\n",
        "        local_file_path = os.path.join(local_dir_path, file_path)\n",
        "        os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
        "\n",
        "        print(f\"Downloading {file_path} to {local_file_path}\")\n",
        "        blob.download_to_filename(local_file_path)\n",
        "\n",
        "\n",
        "def upload_local_dir_to_gcs(local_dir_path, gcs_dir_path):\n",
        "    \"\"\"Uploads files in a local directory to a GCS directory.\"\"\"\n",
        "    client = storage.Client()\n",
        "    bucket_name = gcs_dir_path.split(\"/\")[2]\n",
        "    bucket = client.get_bucket(bucket_name)\n",
        "    for local_file in glob.glob(local_dir_path + \"/**\"):\n",
        "        if not os.path.isfile(local_file):\n",
        "            continue\n",
        "        filename = local_file[1 + len(local_dir_path) :]\n",
        "        gcs_file_path = os.path.join(gcs_dir_path, filename)\n",
        "        _, blob_name = get_bucket_and_blob_name(gcs_file_path)\n",
        "        blob = bucket.blob(blob_name)\n",
        "        blob.upload_from_filename(local_file)\n",
        "        print(\"Copied {} to {}.\".format(local_file, gcs_file_path))\n",
        "\n",
        "\n",
        "def deploy_model(model_id, lora_id, task):\n",
        "    \"\"\"Create a Vertex AI Endpoint and deploy the specified model to the endpoint.\"\"\"\n",
        "    model_name = model_id\n",
        "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
        "    serving_env = {\n",
        "        \"MODEL_ID\": model_id,\n",
        "        \"LORA_ID\": lora_id,\n",
        "        \"TASK\": task,\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "    }\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=SERVE_DOCKER_URI,\n",
        "        serving_container_ports=[7080],\n",
        "        serving_container_predict_route=\"/predictions/diffusers_serving\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "        serving_container_environment_variables=serving_env,\n",
        "    )\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=\"g2-standard-8\",\n",
        "        accelerator_type=\"NVIDIA_L4\",\n",
        "        accelerator_count=1,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=SERVICE_ACCOUNT,\n",
        "    )\n",
        "    print(\"To load this existing endpoint from a different session:\")\n",
        "    print(\n",
        "        f'endpoint = aiplatform.Endpoint(\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint.name}\")'\n",
        "    )\n",
        "    return model, endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "d133319b6a53"
      },
      "outputs": [],
      "source": [
        "# @title Start Dreambooth LoRA finetune\n",
        "\n",
        "# @markdown This section uses [Dreambooth LoRA](https://dreambooth.github.io/) to finetune\n",
        "# @markdown the [stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) model\n",
        "# @markdown with [5 dog images](https://drive.google.com/drive/folders/1BO_dyz-p65qhBRRMRA4TbZ8qW4rB99JZ) to\n",
        "# @markdown personalize the text-to-image model.\n",
        "\n",
        "# @markdown In this example, we will default to use the images in [diffusers/dog-example](https://huggingface.co/datasets/diffusers/dog-example)\n",
        "# @markdown as the training dataset. If you need to train based on your own images, choose the `IMAGE_SOURCE` option as\n",
        "# @markdown \"Use my own images in a GCS bucket\" and provide the GCS path to your images below.\n",
        "\n",
        "# @markdown It finetunes both text encoder and unet of the stable diffusion model up to 200 steps.\n",
        "# @markdown The whole finetuning job takes 20 minutes to finish using 1 L4 GPU.\n",
        "\n",
        "# @markdown The full model will be saved after the finetuning job finishes and it can be loaded\n",
        "# @markdown by the [StableDiffusionPipeline](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/text2img)\n",
        "# @markdown to run inference.\n",
        "\n",
        "# @markdown Click \"Show code\" to modify the code to change GPU type, count, and other training parameters.\n",
        "\n",
        "IMAGE_SOURCE = \"Default image examples from HuggingFace\"  # @param [\"Default image examples from HuggingFace\", \"Use my own images in a GCS bucket\"] {isTemplate:true}\n",
        "\n",
        "# Input and output path.\n",
        "instance_dir = os.path.join(BUCKET_URI, \"dreambooth-lora-sdxl/images\")\n",
        "class_dir = os.path.join(BUCKET_URI, \"dreambooth-lora-sdxl/image_class\")\n",
        "output_dir = os.path.join(BUCKET_URI, \"dreambooth-lora-sdxl/output\")\n",
        "\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "local_dir = \"./images\"\n",
        "if IMAGE_SOURCE == \"Default image examples from HuggingFace\":\n",
        "    snapshot_download(\n",
        "        \"diffusers/dog-example\",\n",
        "        local_dir=local_dir,\n",
        "        repo_type=\"dataset\",\n",
        "        ignore_patterns=\".gitattributes\",\n",
        "    )\n",
        "    print(\"Finished downloading training images from huggingface.\")\n",
        "else:\n",
        "    # @markdown **[Optional]** If using own images, provide the GCS path to the images.\n",
        "    # @markdown Make sure you have permission to access the bucket.\n",
        "    source_image_gcs_dir = \"gs://\"  # @param {type:\"string\"}\n",
        "    assert source_image_gcs_dir.startswith(\n",
        "        \"gs://\"\n",
        "    ), \"source_image_gcs_dir must start with `gs://`.\"\n",
        "    print(\n",
        "        f\"Now downloading the images from the source gcs directory: {source_image_gcs_dir}\"\n",
        "    )\n",
        "    download_gcs_dir_to_local(source_image_gcs_dir, local_dir)\n",
        "\n",
        "\n",
        "# Upload data to Cloud Storage bucket.\n",
        "print(f\"Now uploading the images to the bucket: {instance_dir}\")\n",
        "upload_local_dir_to_gcs(local_dir, instance_dir)\n",
        "upload_local_dir_to_gcs(local_dir, class_dir)\n",
        "\n",
        "# The pre-trained model to be loaded.\n",
        "model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "\n",
        "# Worker pool spec.\n",
        "machine_type = \"g2-standard-8\"\n",
        "num_nodes = 1\n",
        "gpu_type = \"NVIDIA_L4\"\n",
        "num_gpus = 1\n",
        "\n",
        "# @markdown Default to 200. Increase it to 400 or 800 if you want to achieve a higher model quality.\n",
        "train_steps = 200  # @param {type: \"number\"}\n",
        "\n",
        "# Setup training job.\n",
        "\n",
        "job_name = create_job_name(\"dreambooth-lora-sdxl\")\n",
        "job = aiplatform.CustomContainerTrainingJob(\n",
        "    display_name=job_name,\n",
        "    container_uri=TRAIN_DOCKER_URI,\n",
        ")\n",
        "\n",
        "BUCKET_GCS_FUSE = BUCKET_URI.replace(\"gs://\", \"/gcs/\")\n",
        "instance_dir_fuse = instance_dir.replace(\"gs://\", \"/gcs/\")\n",
        "class_dir_fuse = class_dir.replace(\"gs://\", \"/gcs/\")\n",
        "output_dir_fuse = output_dir.replace(\"gs://\", \"/gcs/\")\n",
        "\n",
        "\n",
        "concept_prompt = \"A picture of a sks dog\"  # @param {type:\"string\"}\n",
        "model = job.run(\n",
        "    args=[\n",
        "        \"--task=text-to-image-dreambooth-lora-sdxl\",\n",
        "        f\"--pretrained_model_name_or_path={model_id}\",\n",
        "        f\"--instance_data_dir={instance_dir_fuse}\",\n",
        "        f\"--class_data_dir={class_dir_fuse}\",\n",
        "        f\"--output_dir={output_dir_fuse}\",\n",
        "        f\"--instance_prompt={concept_prompt}\",\n",
        "        \"--pretrained_vae_model_name_or_path=madebyollin/sdxl-vae-fp16-fix\",\n",
        "        \"--mixed_precision=fp16\",\n",
        "        \"--resolution=1024\",\n",
        "        \"--train_batch_size=1\",\n",
        "        \"--gradient_accumulation_steps=4\",\n",
        "        \"--gradient_checkpointing\",\n",
        "        \"--learning_rate=1e-4\",\n",
        "        \"--lr_scheduler=linear\",\n",
        "        \"--lr_warmup_steps=0 \",\n",
        "        \"--use_8bit_adam\",\n",
        "        f\"--max_train_steps={int(train_steps)}\",\n",
        "        f\"--checkpointing_steps={int(train_steps // 2)}\",\n",
        "        \"--seed=0\",\n",
        "    ],\n",
        "    replica_count=num_nodes,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=gpu_type,\n",
        "    accelerator_count=num_gpus,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "356db37ed6ce"
      },
      "outputs": [],
      "source": [
        "# @title Deploy the SD model to Vertex for online predictions\n",
        "\n",
        "# @markdown This section uploads the model to Model Registry and deploys it on the Endpoint. It takes ~15 minutes to finish.\n",
        "# @markdown Click \"Show Code\" to see more details.\n",
        "\n",
        "# @markdown `text-to-image` lets you send text prompts to the endpoint to generate images.\n",
        "\n",
        "\n",
        "model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "lora_id = os.path.join(BUCKET_URI, \"dreambooth-lora-sdxl/output\")\n",
        "\n",
        "print(\"LoRA weights are saved in:\", lora_id)\n",
        "\n",
        "# Set the model_id to \"stabilityai/stable-diffusion-xl-base-1.0\" to load the OSS pre-trained model.\n",
        "model, endpoint = deploy_model(\n",
        "    model_id=model_id, lora_id=lora_id, task=\"text-to-image-sdxl\"\n",
        ")\n",
        "print(\"endpoint_name:\", endpoint.name)\n",
        "\n",
        "# Loads an existing endpoint instance using the endpoint name:\n",
        "# - Using `endpoint_name = endpoint.name` allows us to get the\n",
        "#   endpoint name of the endpoint `endpoint` created in the cell\n",
        "#   above.\n",
        "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
        "#   an existing endpoint with the ID 1234567890123456789.\n",
        "# You may uncomment the code below to load an existing endpoint.\n",
        "\n",
        "# endpoint_name = \"\"  # @param {type:\"string\"}\n",
        "# aip_endpoint_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
        "# )\n",
        "# endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
        "\n",
        "print(\"To load this existing endpoint from a different session:\")\n",
        "print(\n",
        "    f'endpoint = aiplatform.Endpoint(\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint.name}\")'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1ca644d9f49a"
      },
      "outputs": [],
      "source": [
        "# @title Predict (text-to-image)\n",
        "\n",
        "# @markdown This section is only for sending predictions to an endpoint with the task `text-to-image`.\n",
        "\n",
        "# @markdown Once deployment succeeds, you can generate images by sending text prompts to the endpoint.\n",
        "\n",
        "# @markdown You can also batch send prompts by separating them with a comma. Be sure to include\n",
        "# @markdown the `concept_prompt` of your instance in the prompt.\n",
        "# @markdown You may adjust the parameters below to achieve best image quality.\n",
        "\n",
        "comma_separated_prompt_list = \"A picture of a sks dog in a house, A picture of a sks dog catching a frisbee\"  # @param {type: \"string\"}\n",
        "prompt_list = [x.strip() for x in comma_separated_prompt_list.split(\",\")]\n",
        "height = 1024  # @param {type:\"number\"}\n",
        "width = 1024  # @param {type:\"number\"}\n",
        "num_inference_steps = 25  # @param {type:\"number\"}\n",
        "guidance_scale = 7.5  # @param {type:\"number\"}\n",
        "\n",
        "instances = [\n",
        "    {\n",
        "        \"prompt\": prompt,\n",
        "        \"negative_prompt\": \"\",\n",
        "        \"height\": height,\n",
        "        \"width\": width,\n",
        "        \"num_inference_steps\": num_inference_steps,\n",
        "        \"guidance_scale\": guidance_scale,\n",
        "    }\n",
        "    for prompt in prompt_list\n",
        "]\n",
        "\n",
        "response = endpoint.predict(instances=instances)\n",
        "images = [base64_to_image(image) for image in response.predictions]\n",
        "image_grid(images, rows=math.ceil(len(images) ** 0.5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "c38e9c7e2b69"
      },
      "outputs": [],
      "source": [
        "# @title Clean up resources\n",
        "# @markdown  Delete the experiment models and endpoints to recycle the resources\n",
        "# @markdown  and avoid unnecessary continouous charges that may incur.\n",
        "\n",
        "# Undeploy model and delete endpoint.\n",
        "endpoint.delete(force=True)\n",
        "\n",
        "# Delete models.\n",
        "model.delete()\n",
        "\n",
        "delete_bucket = False  # @param {type:\"boolean\"}\n",
        "if delete_bucket:\n",
        "    ! gsutil -m rm -r $BUCKET_NAME"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_pytorch_sd_xl_finetuning_dreambooth_lora.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

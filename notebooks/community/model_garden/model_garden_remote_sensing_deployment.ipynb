{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "w5feg0ieNxrp"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "13rZccLtXENK"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown 2. **[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment.\n",
        "\n",
        "REGION = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown 3. If you want to run predictions with A100 80GB or H100 GPUs, we recommend using the regions listed below. **NOTE:** Make sure you have associated quota in selected regions. Click the links to see your current quota for each GPU type: [Nvidia A100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_a100_80gb_gpus), [Nvidia H100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_h100_gpus). You can request for quota following the instructions at [\"Request a higher quota\"](https://cloud.google.com/docs/quota/view-manage#requesting_higher_quota).\n",
        "\n",
        "# @markdown | Machine Type | Accelerator Type | Recommended Regions |\n",
        "# @markdown | ----------- | ----------- | ----------- |\n",
        "# @markdown | a2-ultragpu-1g | 1 NVIDIA_A100_80GB | us-central1, us-east4, europe-west4, asia-southeast1, us-east4 |\n",
        "# @markdown | a3-highgpu-2g | 2 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "# @markdown | a3-highgpu-4g | 4 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "# @markdown | a3-highgpu-8g | 8 NVIDIA_H100_80GB | us-central1, europe-west4, us-west1, asia-southeast1 |\n",
        "\n",
        "import importlib\n",
        "import os\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "# Import common utils\n",
        "if os.environ.get(\"VERTEX_PRODUCT\") != \"COLAB_ENTERPRISE\":\n",
        "    ! pip install --upgrade tensorflow\n",
        "! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
        "\n",
        "common_util = importlib.import_module(\n",
        "    \"vertex-ai-samples.notebooks.community.model_garden.docker_source_codes.notebook_util.common_util\"\n",
        ")\n",
        "\n",
        "# Setup GCP & VertexAI\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "if not REGION:\n",
        "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
        "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "# Gets the default SERVICE_ACCOUNT.\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
        "\n",
        "! gcloud config set project $PROJECT_ID\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(\n",
        "    project=PROJECT_ID,\n",
        "    location=REGION,\n",
        ")\n",
        "\n",
        "# Model configuration & utils\n",
        "SERVE_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai-restricted/vertex-vision-model-garden-dockers/remote-sensing-serve-tf-gpu:latest\"\n",
        "MODEL_CONFIGS = {\n",
        "    \"OWLVIT\": (\n",
        "        \"earth-ai-imagery-owlvit-eap-10-2025\",\n",
        "        \"publishers/google/models/remote_sensing_owlvit\",\n",
        "        \"gs://vertex-model-garden-restricted-us/remote-sensing/OVD_OWL-ViT_So400M_RGB1008_V1\",\n",
        "    ),\n",
        "    \"MAMMUT\": (\n",
        "        \"earth-ai-imagery-mammut-eap-10-2025\",\n",
        "        \"publishers/google/models/remote_sensing_mammut\",\n",
        "        \"gs://vertex-model-garden-restricted-us/remote-sensing/MaMMUT_So400M_RGB224_V1\",\n",
        "    ),\n",
        "}\n",
        "\n",
        "\n",
        "def _get_platform_config(accelerator: str):\n",
        "    \"\"\"Returns the platform config for the given accelerator type.\"\"\"\n",
        "    if accelerator == \"CPU\":\n",
        "        return \"cpu\", \"e2-standard-8\", None, None\n",
        "    if accelerator == \"NVIDIA_L4\":\n",
        "        return \"gpu\", \"g2-standard-8\", \"NVIDIA_L4\", 1\n",
        "    if accelerator == \"NVIDIA_A100_80GB\":\n",
        "        return \"gpu\", \"a2-ultragpu-1g\", \"NVIDIA_A100_80GB\", 1\n",
        "    raise f\"Accelerator config is not supported {accelerator}\"\n",
        "\n",
        "\n",
        "def deploy(\n",
        "    name: str,\n",
        "    model_type: str,\n",
        "    model_mode: str,\n",
        "    platform: str,\n",
        "    machine_type: str,\n",
        "    accelerator_type: str,\n",
        "    accelerator_count,\n",
        "    service_account: str = None,\n",
        "    use_dedicated_endpoint: bool = False,\n",
        "    min_replica_count: int = 1,\n",
        "    max_replica_count: int = 1,\n",
        ") -> tuple[aiplatform.Endpoint, aiplatform.Model]:\n",
        "    \"\"\"Deploys the model to a GPU endpoint with accelerator support.\n",
        "\n",
        "    Args:\n",
        "      name: the endpoint name to use for deployment.\n",
        "      model_type: The model type to deploy, either MAMMUT or OWLVIT.\n",
        "      model_mode: The model mode to deploy, e.g. COMBINED, IMAGE_ONLY or\n",
        "        TEXT_ONLY.\n",
        "      platform: The deployment platform, CPU, NVIDIA_L4 or NVIDIA_A100_80GB.\n",
        "      machine_type: The instance machine type to use, see\n",
        "        https://cloud.google.com/compute/docs/machine-resource\n",
        "      accelerator_type: The GPU type to deploy, defaults to NVIDIA_L4, see\n",
        "        https://cloud.google.com/compute/docs/gpus\n",
        "      accelerator_count: The number of GPUs (Accelerators) to use.\n",
        "    \"\"\"\n",
        "    model_id, model_name, model_path = MODEL_CONFIGS[model_type]\n",
        "\n",
        "    common_util.check_quota(\n",
        "        project_id=PROJECT_ID,\n",
        "        region=REGION,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        is_for_training=False,\n",
        "    )\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=f\"{name}-model\",\n",
        "        serving_container_image_uri=SERVE_DOCKER_URI,\n",
        "        serving_container_ports=[8080],\n",
        "        serving_container_predict_route=\"/predict\",\n",
        "        serving_container_health_route=\"/health\",\n",
        "        serving_container_environment_variables={\n",
        "            \"DEPLOY_SOURCE\": \"notebook\",\n",
        "            \"MODEL_ID\": model_id,\n",
        "            \"MODEL_PATH\": model_path,\n",
        "            \"MODEL_TYPE\": model_type,\n",
        "            \"MODEL_MODE\": model_mode,\n",
        "            \"PLATFORM\": platform,\n",
        "        },\n",
        "        model_garden_source_model_name=model_name,\n",
        "    )\n",
        "    endpoint = aiplatform.Endpoint.create(\n",
        "        name, dedicated_endpoint_enabled=use_dedicated_endpoint\n",
        "    )\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        service_account=service_account,\n",
        "        deploy_request_timeout=1800,\n",
        "        enable_access_logging=True,\n",
        "        min_replica_count=min_replica_count,\n",
        "        max_replica_count=max_replica_count,\n",
        "        sync=True,\n",
        "        system_labels={\"NOTEBOOK_NAME\": \"model_garden_remote_sensing_deployment.ipynb\"},\n",
        "    )\n",
        "    return endpoint, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LmC9mUmDSSUF"
      },
      "outputs": [],
      "source": [
        "# @title Deploy model\n",
        "\n",
        "# @markdown **Choose an endpoint name (to be deployed)**\n",
        "ENDPOINT_NAME = \"mammut-combined-test-l4\"  # @param { 'type' : 'string' }\n",
        "# @markdown **Specify the model type, variant mode and accelerator (platform) config.**\n",
        "MODEL_TYPE = \"MAMMUT\"  # @param [\"MAMMUT\", \"OWLVIT\"]\n",
        "MODEL_MODE = \"COMBINED\"  # @param [\"IMAGE_ONLY\", \"TEXT_ONLY\", \"COMBINED\"]\n",
        "ACCELERATOR = \"NVIDIA_L4\"  # @param [\"CPU\", \"NVIDIA_L4\", \"NVIDIA_A100_80GB\"]\n",
        "# @markdown **Note:** For OWLVIT it is recommended to use a dedicated endpoint\n",
        "# @markdown as it increases the input size from 1.5 MB to 10MB.\n",
        "use_dedicated_endpoint = True  # @param { 'type' : 'boolean' }\n",
        "platform, machine_type, acc_type, num_gpus = _get_platform_config(ACCELERATOR)\n",
        "\n",
        "endpoint, model = deploy(\n",
        "    name=ENDPOINT_NAME,\n",
        "    model_type=MODEL_TYPE,\n",
        "    model_mode=MODEL_MODE,\n",
        "    platform=platform,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=acc_type,\n",
        "    accelerator_count=num_gpus,\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OXApz2t7YCfZ"
      },
      "outputs": [],
      "source": [
        "# @title Cleanup Resources\n",
        "# @markdown  Delete the experiment models and endpoints to recycle the resources\n",
        "# @markdown  and avoid unnecessary continuous charges that may incur.\n",
        "\n",
        "endpoint.delete(force=True)\n",
        "model.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYGKMPJ_kAXZ"
      },
      "source": [
        "## Inference examples\n",
        "\n",
        "* Below there are 2 sets of samples: Object Detection (OWL-ViT) and  Classification (MaMMUT), make sure that the deployed endpoint has the correct model type, otherwise you can override it below.\n",
        "\n",
        "* The samples are designed to work with the COMBINED mode, i.e. a variant\n",
        "of the model that can accept text, image or both as input.\n",
        "\n",
        "* Make sure you **cleanup unused resources** (endpoint) in the end. You can use\n",
        "the cleanup section above.\n",
        "\n",
        "* To get the best performance it is advised to use at least an **NVIDIA_L4 GPU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mNTUQVO0j5Li"
      },
      "outputs": [],
      "source": [
        "# @title Inference setup & utils.\n",
        "\n",
        "# @markdown If you've just deployed a new endpoint you can use it directly,\n",
        "# @markdown otherwise specify an endpoint id to override it.\n",
        "\n",
        "import base64\n",
        "import io\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def _b64_png(image: Image.Image) -> str:\n",
        "    arr_bytes = io.BytesIO()\n",
        "    image.save(arr_bytes, format=\"PNG\")\n",
        "    return base64.b64encode(arr_bytes.getvalue()).decode(\"utf-8\")\n",
        "\n",
        "\n",
        "# Override the endpoint, if kept empty uses the recently deployed endpoint.\n",
        "ENDPOINT_ID = \"\"  # @param { 'type': 'string' }\n",
        "use_dedicated_endpoint = True  # @param { 'type' : 'boolean' }\n",
        "\n",
        "if ENDPOINT_ID:\n",
        "    endpoint = aiplatform.Endpoint(ENDPOINT_ID)\n",
        "\n",
        "# Download sample images\n",
        "!wget -O harbor.jpg https://mrsg.aegean.gr/images/uploads/it2zi0eidej4ql33llj.jpg\n",
        "!wget -O palace.jpeg https://www.spaceintelreport.com/wp-content/uploads/2021/05/Pleiades-NEO-US-Capitol-30cm.jpeg\n",
        "harbor_img = Image.open(\"harbor.jpg\")\n",
        "palace_img = Image.open(\"palace.jpeg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "seGRCV5zHund"
      },
      "outputs": [],
      "source": [
        "# @title Classification (MaMMUT) Inference Examples\n",
        "# Make sure that the deployed endpoint above is a MaMMUT model.\n",
        "\n",
        "# Call the image encoder with multiple images, batch_size is 1 by default.\n",
        "result = endpoint.predict(\n",
        "    instances=[\n",
        "        {\"image\": _b64_png(harbor_img)},\n",
        "        {\"image\": _b64_png(palace_img)},\n",
        "    ],\n",
        "    parameters={\"batch_size\": 2},\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        ")\n",
        "print(result)\n",
        "\n",
        "# Call text encoder with multiple input instances\n",
        "result = endpoint.predict(\n",
        "    instances=[\n",
        "        {\"text\": \"text\"},\n",
        "        {\"text\": \"second text\"},\n",
        "        {\"text\": \"this is a longer sentence\"},\n",
        "        {\"text\": \"this is a another long sentence, longer than the previous\"},\n",
        "    ],\n",
        "    parameters={\"batch_size\": 2},\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        ")\n",
        "print(result)\n",
        "\n",
        "# Call the zero-shot classification on the harbor & palace image, returns\n",
        "# similarity scores for each image/text, used\n",
        "labels = [\"airport\", \"palace\", \"harbor\", \"shipyard\", \"park\"]\n",
        "result = endpoint.predict(\n",
        "    instances=[\n",
        "        {\"image\": _b64_png(harbor_img), \"texts\": labels},\n",
        "        {\"image\": _b64_png(palace_img), \"texts\": labels},\n",
        "    ],\n",
        "    parameters={\"batch_size\": 2},\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4ALPx-WdjSBD"
      },
      "outputs": [],
      "source": [
        "# @title Object Detection (OWL-ViT) Inference Examples\n",
        "\n",
        "# Make sure that the deployed endpoint above is OWL-ViT. It is advised to deploy\n",
        "# a dedicated endpoint for OWL-ViT as the input size is relatively large.\n",
        "\n",
        "# Call the image detection model, returns a list of object detections with\n",
        "# bounding boxes, scores & embeddings.\n",
        "result = endpoint.predict(\n",
        "    instances=[\n",
        "        {\"image\": _b64_png(harbor_img)},\n",
        "    ],\n",
        "    parameters={\"batch_size\": 1},\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        ")\n",
        "print(result)\n",
        "\n",
        "# Call text encoder with multiple texts, returns text embeddings for each input.\n",
        "result = endpoint.predict(\n",
        "    instances=[\n",
        "        {\"text\": \"text\"},\n",
        "        {\"text\": \"another text\"},\n",
        "        {\"text\": \"this is a longer sentence\"},\n",
        "        {\"text\": \"this is a very long sentence, even longer than above.\"},\n",
        "    ],\n",
        "    parameters={\"batch_size\": 4},\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        ")\n",
        "print(result)\n",
        "\n",
        "# Call the Open Vocabulary Detection mode with image/texts pairs, returns\n",
        "# object detections and labels, including bounding boxes, scores & embeddings.\n",
        "labels = [\"ship\", \"harbor\", \"dome\", \"building\", \"bridge\"]\n",
        "result = endpoint.predict(\n",
        "    instances=[\n",
        "        {\"image\": _b64_png(harbor_img), \"texts\": labels},\n",
        "        {\"image\": _b64_png(palace_img), \"texts\": labels},\n",
        "    ],\n",
        "    parameters={\n",
        "        \"batch_size\": 4,\n",
        "        # Return only the top 100 detections based on objectness_score.\n",
        "        \"top_k_objects\": 100,\n",
        "        # Discard the object/text embeddings, overall reduces the output size.\n",
        "        \"keep_embeddings\": False,\n",
        "    },\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        ")\n",
        "print(result)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_remote_sensing_deployment.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

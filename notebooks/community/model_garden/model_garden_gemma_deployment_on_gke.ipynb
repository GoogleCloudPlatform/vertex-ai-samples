{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d9bbf86da5e"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Gemma deployment to GKE using TGI on GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates downloading and deploying Gemma, open models from Google DeepMind using Text Generation Inference [TGI](https://github.com/), an efficient serving option to improve serving throughput. In this notebook we will deploy and serve TGI on GPUs. In this guide we specifically use L4 GPUs but this guide should also work for A100(40 GB), A100(80 GB), H100(80 GB) GPUs.\n",
        "\n",
        "\n",
        "### Objective\n",
        "\n",
        "Deploy and run inference for serving Gemma with TGI on GPUs.\n",
        "\n",
        "### GPUs\n",
        "\n",
        "GPUs let you accelerate specific workloads running on your nodes such as machine learning and data processing. GKE provides a range of machine type options for node configuration, including machine types with NVIDIA H100, L4, and A100 GPUs.\n",
        "\n",
        "Before you use GPUs in GKE, we recommend that you complete the following learning path:\n",
        "\n",
        "Learn about [current GPU version availability](https://cloud.google.com/compute/docs/gpus)\n",
        "\n",
        "Learn about [GPUs in GKE](https://cloud.google.com/kubernetes-engine/docs/concepts/gpus)\n",
        "\n",
        "\n",
        "### TGI\n",
        "\n",
        "TGI is a highly optimized open-source LLM serving framework that can increase serving throughput on GPUs. TGI includes features such as:\n",
        "\n",
        "Optimized transformer implementation with PagedAttention\n",
        "Continuous batching to improve the overall serving throughput\n",
        "Tensor parallelism and distributed serving on multiple GPUs\n",
        "\n",
        "To learn more, refer to the [TGI documentation](https://github.com/huggingface/text-generation-inference/blob/main/README.md)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "264c07757582"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35Dvbzb0hH3-"
      },
      "source": [
        "### Configure Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c460088b873"
      },
      "source": [
        "Set the following variables for the experiment environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "855d6b96f291"
      },
      "outputs": [],
      "source": [
        "# The HuggingFace token used to download models.\n",
        "HF_TOKEN = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# The size of the model to launch\n",
        "MODEL_SIZE = \"2b\"  # @param [\"2b\", \"7b\"]\n",
        "\n",
        "# Cloud project id.\n",
        "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# Region for launching clusters.\n",
        "REGION = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# The cluster name to create\n",
        "CLUSTER_NAME = \"gke-gemma-cluster\"  # @param {type:\"string\"}\n",
        "\n",
        "# The number of GPUs to run: 1 for 2b, 2 for 7b\n",
        "GPU_COUNT = 1\n",
        "if MODEL_SIZE == \"7b\":\n",
        "    GPU_COUNT = 2\n",
        "\n",
        "# Ephemeral storage\n",
        "EPHEMERAL_STORAGE_SIZE = \"20Gi\"\n",
        "if MODEL_SIZE == \"7b\":\n",
        "    EPHEMERAL_STORAGE_SIZE = \"40Gi\"\n",
        "\n",
        "# Memory size\n",
        "MEMORY_SIZE = \"7Gi\"\n",
        "if MODEL_SIZE == \"7b\":\n",
        "    MEMORY_SIZE = \"25Gi\"\n",
        "\n",
        "GPU_SHARD = 1\n",
        "if MODEL_SIZE == \"7b\":\n",
        "    GPU_SHARD = 2\n",
        "\n",
        "CPU_LIMITS = 2\n",
        "if MODEL_SIZE == \"7b\":\n",
        "    CPU_LIMITS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klPAnx16cVd7"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project \"$PROJECT_ID\"\n",
        "! gcloud services enable container.googleapis.com\n",
        "\n",
        "# Add kubectl to the set of available tools.\n",
        "! mkdir -p /tools/google-cloud-sdk/.install\n",
        "! gcloud components install kubectl --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e828eb320337"
      },
      "source": [
        "### Create a GKE cluster and a node pool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKhdKv1vK9Lg"
      },
      "source": [
        "GKE creates the following resources for the model based on the MODEL_SIZE environment variable set above.\n",
        "\n",
        "- Standard cluster\n",
        "- 1 or 2 NVIDIA L4 GPU accelerators depending on whether you are deploying Gemma 2b or Gemma 7b respectively.\n",
        "\n",
        "If you already have a cluster, you can skip to `Use an existing GKE cluster` instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12cd25839741"
      },
      "outputs": [],
      "source": [
        "! gcloud container clusters create {CLUSTER_NAME} \\\n",
        "  --project={PROJECT_ID} \\\n",
        "  --region={REGION} \\\n",
        "  --workload-pool={PROJECT_ID}.svc.id.goog \\\n",
        "  --release-channel=rapid \\\n",
        "  --num-nodes=4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwbkQwzMJ7hb"
      },
      "outputs": [],
      "source": [
        "! gcloud container node-pools create gpupool \\\n",
        "  --accelerator type=nvidia-l4,count=2,gpu-driver-version=latest \\\n",
        "  --project={PROJECT_ID} \\\n",
        "  --location={REGION} \\\n",
        "  --node-locations={REGION}-a \\\n",
        "  --cluster={CLUSTER_NAME} \\\n",
        "  --machine-type=g2-standard-24 \\\n",
        "  --num-nodes=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ydvYk7FLJz_"
      },
      "source": [
        "### Use an existing GKE cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmpNpYF-LRut"
      },
      "outputs": [],
      "source": [
        "! gcloud container clusters get-credentials {CLUSTER_NAME} --location {REGION}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cc825514deb"
      },
      "source": [
        "### Create Kubernetes secret for Hugging Face credentials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgZfNOSyOY7_"
      },
      "source": [
        "Create a Kubernetes Secret that contains the Hugging Face token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b42bd4fa2b2d"
      },
      "outputs": [],
      "source": [
        "! kubectl create secret generic hf-secret \\\n",
        "    --from-literal=hf_api_token={HF_TOKEN} \\\n",
        "    --dry-run=client -o yaml > hf-secret.yaml\n",
        "\n",
        "! kubectl apply -f hf-secret.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c250872074f"
      },
      "source": [
        "### Deploy TGI\n",
        "\n",
        "Use the YAML to deploy Gemma on TGI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6psJZY_zUDgj"
      },
      "outputs": [],
      "source": [
        "K8S_YAML = f\"\"\"\n",
        "apiVersion: apps/v1\n",
        "kind: Deployment\n",
        "metadata:\n",
        "  name: tgi-gemma-deployment\n",
        "spec:\n",
        "  replicas: 1\n",
        "  selector:\n",
        "    matchLabels:\n",
        "      app: gemma-server\n",
        "  template:\n",
        "    metadata:\n",
        "      labels:\n",
        "        app: gemma-server\n",
        "        ai.gke.io/model: gemma-{MODEL_SIZE}\n",
        "        ai.gke.io/inference-server: text-generation-inference\n",
        "        examples.ai.gke.io/source: user-guide\n",
        "    spec:\n",
        "      containers:\n",
        "      - name: inference-server\n",
        "        image: us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-hf-tgi-serve:20240220_0936_RC01\n",
        "        resources:\n",
        "          requests:\n",
        "            cpu: \"2\"\n",
        "            memory: {MEMORY_SIZE}\n",
        "            ephemeral-storage: {EPHEMERAL_STORAGE_SIZE}\n",
        "            nvidia.com/gpu: {GPU_COUNT}\n",
        "          limits:\n",
        "            cpu: {CPU_LIMITS}\n",
        "            memory: {MEMORY_SIZE}\n",
        "            ephemeral-storage: {EPHEMERAL_STORAGE_SIZE}\n",
        "            nvidia.com/gpu: {GPU_COUNT}\n",
        "        args:\n",
        "        - --model-id=$(MODEL_ID)\n",
        "        - --num-shard={GPU_SHARD}\n",
        "        env:\n",
        "        - name: MODEL_ID\n",
        "          value: google/gemma-{MODEL_SIZE}-it\n",
        "        - name: PORT\n",
        "          value: \"8000\"\n",
        "        - name: HUGGING_FACE_HUB_TOKEN\n",
        "          valueFrom:\n",
        "            secretKeyRef:\n",
        "              name: hf-secret\n",
        "              key: hf_api_token\n",
        "        volumeMounts:\n",
        "        - mountPath: /dev/shm\n",
        "          name: dshm\n",
        "      volumes:\n",
        "      - name: dshm\n",
        "        emptyDir:\n",
        "          medium: Memory\n",
        "      nodeSelector:\n",
        "        cloud.google.com/gke-accelerator: nvidia-l4\n",
        "---\n",
        "apiVersion: v1\n",
        "kind: Service\n",
        "metadata:\n",
        "  name: llm-service\n",
        "spec:\n",
        "  selector:\n",
        "    app: gemma-server\n",
        "  type: ClusterIP\n",
        "  ports:\n",
        "  - protocol: TCP\n",
        "    port: 8000\n",
        "    targetPort: 8000\n",
        "\"\"\"\n",
        "\n",
        "with open(\"tgi.yaml\", \"w\") as f:\n",
        "    f.write(K8S_YAML)\n",
        "\n",
        "! kubectl apply -f tgi.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYMesXi7WqCu"
      },
      "source": [
        "#### Waiting for the container to create"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKwbzKXuWvoL"
      },
      "source": [
        "Use the command below to check on the status of the container."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXbPCrWtWqbk"
      },
      "outputs": [],
      "source": [
        "! kubectl get pod"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzINwFr_WVAB"
      },
      "source": [
        "#### View the logs from the running deployment\n",
        "\n",
        "##### This will download the needed artifacts, this process will take close to 5 minutes depending on what runtime you are using to run your colab environment. The server is up and running and ready to take inference request once you see log messages like these :\n",
        "\n",
        "```\n",
        "INFO text_generation_router: router/src/main.rs:237: Using the Hugging Face API to retrieve tokenizer config\n",
        "INFO text_generation_router: router/src/main.rs:280: Warming up model\n",
        "INFO text_generation_router: router/src/main.rs:316: Setting max batch total tokens to 666672\n",
        "INFO text_generation_router: router/src/main.rs:317: Connected\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPKVtosbWYjg"
      },
      "outputs": [],
      "source": [
        "! kubectl logs -f -l app=gemma-server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7Q_oG4TW6sD"
      },
      "source": [
        "#### Set up port forwarding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDTasPgGW7EG"
      },
      "outputs": [],
      "source": [
        "! kubectl exec -t $( kubectl get pod -l app=gemma-server -o jsonpath=\"{.items[0].metadata.name}\" ) -c inference-server -- curl -X POST http://localhost:8000/generate \\\n",
        "   -H \"Content-Type: application/json\" \\\n",
        "   -d '{ \"inputs\": \"What are the top 5 most popular programming languages? Please be brief.\", \"temperature\": 0.40, \"top_p\": 0.1, \"max_tokens\": 250 }' \\\n",
        "   2> /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af21a3cff1e0"
      },
      "source": [
        "## Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "911406c1561e"
      },
      "outputs": [],
      "source": [
        "! kubectl delete deployments tgi-gemma-deployment\n",
        "! kubectl delete services llm-service\n",
        "! kubectl delete secrets hf-secret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2acSuqPeNjJJ"
      },
      "outputs": [],
      "source": [
        "! gcloud container clusters delete {CLUSTER_NAME} \\\n",
        "  --region={REGION} \\\n",
        "  --quiet"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_gemma_deployment_on_gke.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

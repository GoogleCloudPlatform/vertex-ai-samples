{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7d9bbf86da5e"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Mistral models deployment to GKE using GPU\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_tgi_mistral_deployment_on_gke.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_tgi_mistral_deployment_on_gke.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates downloading and deploying Mistral, open models from Google DeepMind using Text Generation Inference [TGI](https://github.com/), an efficient serving option to improve serving throughput. In this notebook we will deploy and serve TGI on GPUs. In this guide we specifically use L4 GPUs but this guide should also work for A100(40 GB), A100(80 GB), H100(80 GB) GPUs.\n",
        "\n",
        "\n",
        "### Objective\n",
        "\n",
        "Deploy and run inference for serving Mistral with TGI on GPUs.\n",
        "\n",
        "### GPUs\n",
        "\n",
        "GPUs let you accelerate specific workloads running on your nodes such as machine learning and data processing. GKE provides a range of machine type options for node configuration, including machine types with NVIDIA H100, L4, and A100 GPUs.\n",
        "\n",
        "Before you use GPUs in GKE, we recommend that you complete the following learning path:\n",
        "\n",
        "Learn about [current GPU version availability](https://cloud.google.com/compute/docs/gpus)\n",
        "\n",
        "Learn about [GPUs in GKE](https://cloud.google.com/kubernetes-engine/docs/concepts/gpus)\n",
        "\n",
        "\n",
        "### TGI\n",
        "\n",
        "TGI is a highly optimized open-source LLM serving framework that can increase serving throughput on GPUs. TGI includes features such as:\n",
        "\n",
        "Optimized transformer implementation with PagedAttention\n",
        "Continuous batching to improve the overall serving throughput\n",
        "Tensor parallelism and distributed serving on multiple GPUs\n",
        "\n",
        "To learn more, refer to the [TGI documentation](https://github.com/huggingface/text-generation-inference/blob/main/README.md)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "264c07757582"
      },
      "source": [
        "## Run the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "855d6b96f291"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project\n",
        "\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown 2. Set Hugging Face access token in `HF_TOKEN` field. If you don't already have a \"read\" access token, follow the [Hugging Face documentation](https://huggingface.co/docs/hub/en/security-tokens) to create an access token with \"read\" permission. You can find your existing access tokens in the Hugging Face [Access Token](https://huggingface.co/settings/tokens) page.\n",
        "\n",
        "# @markdown 3. **[Optional]** Set `CLUSTER_NAME` if you want to use your own GKE cluster. If not set, this example will create a standard cluster with 2 NVIDIA L4 GPU accelerators.\n",
        "\n",
        "# @markdown 3. **[Optional]** Set `PROJECT_ID` if you have a specific GCP project you want to use.\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "# The HuggingFace token used to download models.\n",
        "HF_TOKEN = \"\"  # @param {type:\"string\"}\n",
        "assert HF_TOKEN, \"Set Hugging Face access token in `HF_TOKEN`.\"\n",
        "\n",
        "# The cluster name to create\n",
        "CLUSTER_NAME = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
        "if not PROJECT_ID:\n",
        "    PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "REGION = \"us-central1\"  # @param [\"us-central1\", \"us-west1\", \"us-east4\"]\n",
        "\n",
        "# Set up gcloud.\n",
        "! gcloud config set project \"$PROJECT_ID\"\n",
        "! gcloud services enable container.googleapis.com\n",
        "\n",
        "# Add kubectl to the set of available tools.\n",
        "! mkdir -p /tools/google-cloud-sdk/.install\n",
        "! gcloud components install kubectl --quiet\n",
        "\n",
        "# Use existing GKE cluster or create a new cluster.\n",
        "if CLUSTER_NAME:\n",
        "  ! gcloud container clusters get-credentials {CLUSTER_NAME} --location {REGION}\n",
        "else:\n",
        "  now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "  CLUSTER_NAME = f\"gke-cluster-{now}\"\n",
        "  # create auto-pilot cluster\n",
        "  !gcloud container clusters create-auto {CLUSTER_NAME} --location={REGION} --project={PROJECT_ID}\n",
        "\n",
        "# Create Kubernetes secret for Hugging Face credentials\n",
        "! kubectl create secret generic hf-secret \\\n",
        "    --from-literal=hf_api_token={HF_TOKEN} \\\n",
        "    --dry-run=client -o yaml > hf-secret.yaml\n",
        "\n",
        "! kubectl apply -f hf-secret.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6psJZY_zUDgj"
      },
      "outputs": [],
      "source": [
        "# @title Deploy\n",
        "\n",
        "# @markdown This section deploys Mistral models on GKE.\n",
        "\n",
        "# @markdown The model deployment takes about 5 to 15 minutes to complete.\n",
        "\n",
        "# @markdown Select the model to deploy:\n",
        "MODEL_NAME = \"Mistral-7B-v0.1\"  # @param ['Mistral-7B-v0.1','Mistral-7B-Instruct-v0.2']\n",
        "\n",
        "\n",
        "MISTRAL_YAML = f\"\"\"\n",
        "apiVersion: apps/v1\n",
        "kind: Deployment\n",
        "metadata:\n",
        "  name: mistral-deployment\n",
        "spec:\n",
        "  replicas: 1\n",
        "  selector:\n",
        "    matchLabels:\n",
        "      app: mistral-server\n",
        "  template:\n",
        "    metadata:\n",
        "      labels:\n",
        "        app: mistral-server\n",
        "        ai.gke.io/model: {MODEL_NAME}\n",
        "        ai.gke.io/inference-server: text-generation-inference\n",
        "        examples.ai.gke.io/source: model-garden\n",
        "    spec:\n",
        "      containers:\n",
        "      - name: inference-server\n",
        "        image: us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu121.2-1.ubuntu2204.py310\n",
        "        resources:\n",
        "          requests:\n",
        "            cpu: 8\n",
        "            memory: 29Gi\n",
        "            ephemeral-storage: 80Gi\n",
        "            nvidia.com/gpu : 1\n",
        "          limits:\n",
        "            cpu: 8\n",
        "            memory: 29Gi\n",
        "            ephemeral-storage: 80Gi\n",
        "            nvidia.com/gpu : 1\n",
        "        command:\n",
        "        args:\n",
        "        - --model-id=mistralai/{MODEL_NAME}\n",
        "        - --cuda-memory-fraction=0.9\n",
        "        env:\n",
        "        - name: DEPLOY_SOURCE\n",
        "          value: UI_HF_VERIFIED_MODEL\n",
        "        - name: MAX_INPUT_LENGTH\n",
        "          value: \"512\"\n",
        "        - name: MAX_TOTAL_TOKENS\n",
        "          value: \"1024\"\n",
        "        - name: MAX_BATCH_PREFILL_TOKENS\n",
        "          value: \"2048\"\n",
        "        - name: TRUST_REMOTE_CODE\n",
        "          value: \"true\"\n",
        "        - name: MODEL_ID\n",
        "          value: \"mistralai/{MODEL_NAME}\"\n",
        "        - name: NUM_SHARD\n",
        "          value: \"1\"\n",
        "        - name: HUGGING_FACE_HUB_TOKEN\n",
        "          valueFrom:\n",
        "            secretKeyRef:\n",
        "              name: hf-secret\n",
        "              key: hf_api_token\n",
        "        - name: \"AIP_HTTP_PORT\"\n",
        "          value: \"80\"\n",
        "        volumeMounts:\n",
        "        - mountPath: /dev/shm\n",
        "          name: dshm\n",
        "      volumes:\n",
        "      - name: dshm\n",
        "        emptyDir:\n",
        "          medium: Memory\n",
        "      nodeSelector:\n",
        "        cloud.google.com/gke-accelerator: nvidia-l4\n",
        "\n",
        "---\n",
        "apiVersion: v1\n",
        "kind: Service\n",
        "metadata:\n",
        "  name: mistral-service\n",
        "spec:\n",
        "  selector:\n",
        "    app: mistral-server\n",
        "  type: ClusterIP\n",
        "  ports:\n",
        "  - protocol: TCP\n",
        "    port: 8000\n",
        "    targetPort: 80\n",
        "\"\"\"\n",
        "\n",
        "with open(\"mistral_tgi.yaml\", \"w\") as f:\n",
        "    f.write(MISTRAL_YAML)\n",
        "! kubectl apply -f mistral_tgi.yaml\n",
        "\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "\n",
        "def wait_for_status(\n",
        "    command, expected_output, timeout_seconds=600, poll_interval=15, message=\"\"\n",
        "):\n",
        "    \"\"\"Waits for a command's output to contain an expected string.\"\"\"\n",
        "\n",
        "    start_time = time.time()\n",
        "    end_time = start_time + timeout_seconds\n",
        "\n",
        "    print(f\"{message}...\")\n",
        "    while time.time() < end_time:\n",
        "        try:\n",
        "            output = subprocess.check_output(command, text=True)\n",
        "            if expected_output in output:\n",
        "                print(\"Done!\\n\")\n",
        "                return\n",
        "        except subprocess.CalledProcessError:\n",
        "            pass  # Ignore errors and continue polling\n",
        "        time.sleep(poll_interval)\n",
        "    print(\"Timeout!\\n\")\n",
        "\n",
        "\n",
        "# wait for container to be running\n",
        "wait_for_status(\n",
        "    [\"kubectl\", \"get\", \"pod\", \"-l\", \"app=mistral-server\"],\n",
        "    \"1/1\",\n",
        "    message=\"Waiting for container to be created\",\n",
        ")\n",
        "\n",
        "wait_for_status(\n",
        "    [\"kubectl\", \"logs\", \"-l\", \"app=mistral-server\"],\n",
        "    \"Connected\",\n",
        "    message=\"Downloading artifacts (checking for 'Connected')\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6rnlQ4jGmFzs"
      },
      "outputs": [],
      "source": [
        "# @title Chat completion for text-only models\n",
        "\n",
        "# @markdown Once the server is up and running, you may send prompts to local server for prediction.\n",
        "\n",
        "import json\n",
        "\n",
        "prompt = \"What is AI?\"  # @param {type: \"string\"}\n",
        "temperature = 0.40  # @param {type: \"number\"}\n",
        "top_p = 0.1  # @param {type: \"number\"}\n",
        "max_tokens = 250  # @param {type: \"number\"}\n",
        "\n",
        "request = {\n",
        "    \"inputs\": prompt,\n",
        "    \"temperature\": temperature,\n",
        "    \"top_p\": top_p,\n",
        "    \"max_tokens\": max_tokens,\n",
        "}\n",
        "\n",
        "get_pod = ! kubectl get pod -l app=mistral-server -o jsonpath=\"{{.items[0].metadata.name}}\"\n",
        "pod_name = get_pod[0]\n",
        "\n",
        "exec_command = f\"\"\"kubectl exec -t {pod_name} -- curl -X POST http://localhost:80/generate \\\n",
        "   -H \"Content-Type: application/json\" \\\n",
        "   -d '{json.dumps(request)}' \\\n",
        "   2> /dev/null\"\"\"\n",
        "\n",
        "response = !{exec_command}\n",
        "# @markdown Response:\n",
        "print(json.loads(response[0])[\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbRmgoOZF6es"
      },
      "source": [
        "## Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "911406c1561e"
      },
      "outputs": [],
      "source": [
        "# @markdown  Delete the experiment models and endpoints to recycle the resources\n",
        "# @markdown  and avoid unnecessary continouous charges that may incur.\n",
        "DELETE_DEPLOYMENT = False # @param {type: \"boolean\"}\n",
        "DELETE_CLUSTER = False # @param {type: \"boolean\"}\n",
        "\n",
        "if DELETE_CLUSTER or DELETE_DEPLOYMENT:\n",
        "  ! kubectl delete deployment mistral-deployment\n",
        "  ! kubectl delete service mistral-service\n",
        "\n",
        "if DELETE_CLUSTER:\n",
        "  ! gcloud container clusters delete {CLUSTER_NAME} \\\n",
        "    --region={REGION} \\\n",
        "    --quiet"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_tgi_mistral_deployment_on_gke.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7d9bbf86da5e"
      },
      "outputs": [],
      "source": [
        "# Copyright 2026 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dc4391f6be7"
      },
      "source": [
        "# Vertex AI Model Garden - Recursion MAE Image Feature Extraction local inference\n",
        "\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_recursion_mae_local_inference.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_recursion_mae_local_inference.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e8a0fdd6f44"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates how to install the necessary libraries and run local inference with the Recursion MAE model in a [Colab Enterprise Instance](https://cloud.google.com/colab/docs) for Image Feature Extraction.\n",
        "\n",
        "\n",
        "### OpenPhenom Model Licensing\n",
        "\n",
        "* OpenPhenom Model is available under a Non-Commercial End User License Agreement license. For full details, please refer to the [license documentation](https://huggingface.co/recursionpharma/OpenPhenom/blob/main/LICENSE) that governs the use of this model.\n",
        "\n",
        "\n",
        "### Objective\n",
        "\n",
        "* Run local inference with the Recursion MAE model for image feature extraction.\n",
        "\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69453bf7230e"
      },
      "source": [
        "## Install dependencies\n",
        "\n",
        "Before you begin, make sure you are connecting to a [Colab Enterprise runtime](https://cloud.google.com/colab/docs/connect-to-runtime) with CPU. GPU is not required for this model. If not, we recommend [creating a runtime template](https://cloud.google.com/colab/docs/create-runtime-template) with the `defalut` template (with a machine type of `e2-standard-4`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3d342b32fb08"
      },
      "outputs": [],
      "source": [
        "! pip3 install --upgrade pip\n",
        "! pip3 install huggingface-hub==0.25.2\n",
        "! pip3 install timm==1.0.11\n",
        "! pip3 install torch~=2.4.0\n",
        "! pip3 install torchmetrics~=1.5.1\n",
        "! pip3 install torchvision~=0.19.0\n",
        "! pip3 install tqdm~=4.66.6\n",
        "! pip3 install transformers~=4.46.1\n",
        "! pip3 install pandas~=2.2.3\n",
        "! pip3 install zarr~=2.18.3\n",
        "! pip3 install hydra-core~=1.3.2\n",
        "! pip3 install pytorch-lightning~=2.1\n",
        "! pip3 install isort~=5.13.2\n",
        "! pip3 install ruff~=0.7.2\n",
        "! apt-get update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8gDpTu00pdQi"
      },
      "outputs": [],
      "source": [
        "# @title Git clone the official Recursion MAE repo\n",
        "\n",
        "! git clone https://github.com/recursionpharma/maes_microscopy.git\n",
        "%cd /content/maes_microscopy\n",
        "# Pin the repo to the commit on 2024-11-04\n",
        "! git reset --hard 42cfc25290f0a09f6db2a27fdf238d064f5c0760"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ad1a690839d5"
      },
      "outputs": [],
      "source": [
        "# @title Utility functions\n",
        "\n",
        "from typing import Iterator, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.cloud import storage\n",
        "\n",
        "\n",
        "def download_gcs_dir_to_local(gcs_dir_path: str, local_dir_path: str):\n",
        "    \"\"\"Downloads files in a GCS directory to a local directory.\"\"\"\n",
        "    assert gcs_dir_path.startswith(\"gs://\"), \"gcs_dir_path must start with `gs://`.\"\n",
        "    bucket_name = gcs_dir_path.split(\"/\")[2]\n",
        "    prefix = gcs_dir_path[len(\"gs://\" + bucket_name) :].strip(\"/\") + \"/\"\n",
        "    client = storage.Client()\n",
        "    blobs = client.list_blobs(bucket_name, prefix=prefix)\n",
        "    for blob in blobs:\n",
        "        if blob.name[-1] == \"/\":\n",
        "            continue\n",
        "        file_path = blob.name[len(prefix) :].strip(\"/\")\n",
        "        local_file_path = os.path.join(local_dir_path, file_path)\n",
        "        os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
        "\n",
        "        print(f\"Downloading {file_path} to {local_file_path}\")\n",
        "        blob.download_to_filename(local_file_path)\n",
        "\n",
        "\n",
        "def iter_border_patches(\n",
        "    width: int, height: int, patch_size: int, border_trim_size: int = 0\n",
        ") -> Iterator[Tuple[int, int]]:\n",
        "    \"\"\"Generates (x, y) coordinates of patches along the borders of an image.\n",
        "\n",
        "    This function iterates over the patches along the outer edge of an image,\n",
        "    excluding the corners. It's useful for prioritizing inference on the borders\n",
        "    where objects are often partially occluded.\n",
        "\n",
        "    Args:\n",
        "      width: Width of the image in pixels.\n",
        "      height: Height of the image in pixels.\n",
        "      patch_size: Size of each patch in pixels (assumed to be square).\n",
        "      border_trim_size: Number of pixels to trim from each border. This is used to\n",
        "        skip patches at the very edge of the image, potentially improving\n",
        "        inference time. Must be divisible by half the patch size.\n",
        "\n",
        "    Yields:\n",
        "      Tuples of (x, y) coordinates representing the top-left corner of each patch.\n",
        "    \"\"\"\n",
        "    if border_trim_size % (patch_size / 2) != 0:\n",
        "        raise ValueError(\"Border trim size has to be divisible by half the patch size\")\n",
        "    x_start, x_end, y_start, y_end = (\n",
        "        border_trim_size,\n",
        "        width - border_trim_size,\n",
        "        border_trim_size,\n",
        "        height - border_trim_size,\n",
        "    )\n",
        "\n",
        "    for x in range(x_start, x_end - patch_size + 1, patch_size):\n",
        "        for y in range(y_start, y_end - patch_size + 1, patch_size):\n",
        "            yield x, y\n",
        "\n",
        "\n",
        "def patch_image(\n",
        "    image_array: np.ndarray, patch_size: int = 256, border_trim_size: int = 0\n",
        ") -> Tuple[np.ndarray, pd.DataFrame]:\n",
        "    \"\"\"Applied to each sample in the dataset where the image has been loaded.\"\"\"\n",
        "    width, height, _ = image_array.shape\n",
        "    output_rows = []\n",
        "    output_patches = []\n",
        "    patch_count = 0\n",
        "    for x, y in iter_border_patches(\n",
        "        width, height, patch_size, border_trim_size=border_trim_size\n",
        "    ):\n",
        "        r = dict()\n",
        "        patch = image_array[y : y + patch_size, x : x + patch_size, :].copy()\n",
        "        patch_count += 1\n",
        "        r[\"patch_top_left_y\"] = y\n",
        "        r[\"patch_top_left_x\"] = x\n",
        "        r[\"patch_width\"] = patch_size\n",
        "        r[\"patch_height\"] = patch_size\n",
        "        output_rows.append(r)\n",
        "        output_patches.append(patch)\n",
        "\n",
        "    output_patches = np.stack(output_patches, axis=0)\n",
        "    output_rows = pd.DataFrame(output_rows)\n",
        "\n",
        "    return output_patches, output_rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgeuC_FrJ0ZY"
      },
      "source": [
        "# Image Embedding Generation with MAE Model\n",
        "\n",
        "The code generates embeddings for multi-channel microscopy images. It uses a Masked Autoencoder (MAE) model developed by Recursion to process images, dividing them into 256x256 patches and generating an embedding vector for each patch.\n",
        "\n",
        "**Requirements:**\n",
        "\n",
        "* **Input:** Multiple image files (different channels) with dimensions divisible by 256.\n",
        "* **GCS Paths:**  The demostration uses images from the `SAMPLE_IMAGES_GCS_PATH` GCS bucket.\n",
        "    The model weights can also be pulled from a GCS bucket, or directly pull from Huggingface.\n",
        "\n",
        "**Output:**\n",
        "\n",
        "A Pandas DataFrame containing embeddings and metadata (e.g., patch coordinates) for each patch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pVkDyAHhJ0ZY"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "import time\n",
        "\n",
        "import torch\n",
        "from huggingface_hub import snapshot_download\n",
        "from huggingface_mae import MAEModel\n",
        "from PIL import Image\n",
        "\n",
        "MAX_IMAGE_CHANNELS = 11\n",
        "LOCAL_MODEL_DIR = \"/content/model\"\n",
        "LOCAL_IMAGES_DIR = \"/content/images\"\n",
        "\n",
        "# Download the model weights to a local dir\n",
        "MODEL_ID = \"recursionpharma/OpenPhenom\"\n",
        "print(f\"Downloading model from {MODEL_ID} to {LOCAL_MODEL_DIR}\")\n",
        "\n",
        "if MODEL_ID.startswith(\"gs://\"):\n",
        "    download_gcs_dir_to_local(MODEL_ID, LOCAL_MODEL_DIR)\n",
        "    MODEL_ID = LOCAL_MODEL_DIR\n",
        "else:\n",
        "    snapshot_download(\n",
        "        repo_id=MODEL_ID,\n",
        "        local_dir=LOCAL_MODEL_DIR,\n",
        "    )\n",
        "    MODEL_ID = LOCAL_MODEL_DIR\n",
        "\n",
        "model = MAEModel.from_pretrained(MODEL_ID)\n",
        "model = model.eval().cpu()\n",
        "\n",
        "SAMPLE_IMAGES_GCS_PATH = (\n",
        "    \"gs://cloud-samples-data/vertex-ai/model-garden/recursion-mae-images\"\n",
        ")\n",
        "print(f\"Downloading images from {SAMPLE_IMAGES_GCS_PATH} to {LOCAL_IMAGES_DIR}\")\n",
        "download_gcs_dir_to_local(SAMPLE_IMAGES_GCS_PATH, LOCAL_IMAGES_DIR)\n",
        "\n",
        "# Load and Validate Images\n",
        "image_paths = sorted(glob.glob(f\"{LOCAL_IMAGES_DIR}/*\"))\n",
        "cur_images = []\n",
        "for img_path in image_paths:\n",
        "    img_channel = Image.open(img_path)\n",
        "\n",
        "    # Check if image dimensions are divisible by 256 (MAE requirement)\n",
        "    width, height = img_channel.size\n",
        "    if width % 256 != 0 or height % 256 != 0:\n",
        "        print(f\"Image [{img_path}] dimensions are not divisible by 256.\")\n",
        "        continue\n",
        "    cur_images.append(np.array(img_channel))\n",
        "\n",
        "if not cur_images:\n",
        "    raise ValueError(\"No valid images found in the input GCS folder.\")\n",
        "if len(cur_images) > MAX_IMAGE_CHANNELS:\n",
        "    raise ValueError(\n",
        "        f\"Too many channel images found in the input GCS folder. Max allowed is {MAX_IMAGE_CHANNELS}.\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Run Model Inference\n",
        "embeddings = []\n",
        "metadata_df = []\n",
        "inference_start = time.time()\n",
        "\n",
        "# Divide the input images into 256x256 patches (MAE requirement)\n",
        "stacked_images = np.dstack(cur_images)\n",
        "stacked_patches, metadata = patch_image(stacked_images)\n",
        "\n",
        "# Convert to PyTorch tensor and permute dimensions\n",
        "torch_im = torch.from_numpy(stacked_patches).permute(0, 3, 1, 2)\n",
        "\n",
        "# Perform inference with autocast for mixed precision and no_grad to save memory\n",
        "with torch.amp.autocast(\"cuda\"), torch.no_grad():\n",
        "    embedding = model.predict(torch_im)\n",
        "\n",
        "embeddings.append(embedding)\n",
        "metadata_df.append(metadata)\n",
        "\n",
        "# Process Embeddings and Metadata\n",
        "embedding_df = pd.DataFrame(np.concatenate(embeddings, axis=0))\n",
        "embedding_df.columns = [f\"feature_{i}\" for i in range(embedding_df.shape[1])]\n",
        "metadata_df = pd.concat(metadata_df).reset_index(drop=True)\n",
        "\n",
        "# Sanity check to ensure embeddings and metadata have the same length\n",
        "assert len(metadata_df) == len(embedding_df), (\n",
        "    f\"Embedding and metadata don't match; got {len(embedding_df)} embs and\"\n",
        "    f\" {len(metadata_df)} metadata\"\n",
        ")\n",
        "\n",
        "print(\"Prediction was made in %.2f seconds.\" % (time.time() - inference_start))\n",
        "\n",
        "# Combine Metadata and Embeddings\n",
        "final_df = pd.concat([metadata_df, embedding_df], axis=1)\n",
        "print(final_df)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_recursion_mae_local_inference.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

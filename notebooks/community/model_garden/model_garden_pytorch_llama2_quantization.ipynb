{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d9bbf86da5e"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Vertex AI Model Garden - LLaMA2 (Quantization)\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_llama2_quantization.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_llama2_quantization.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_pytorch_llama2_quantization.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "Open in Vertex AI Workbench\n",
        "    </a> (A Python-3 CPU notebook is recommended)\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates downloading prebuilt [LLaMA2 models](https://huggingface.co/meta-llama), deploying prequantized LLaMA2 models with [vLLM](https://github.com/vllm-project/vllm), quantizating LLaMA2 models yourself using either AWQ or GPTQ to reduce the GPU memory requirements and then deploying these models to vLLM as well. This notebook uses [Text moderation APIs](https://cloud.google.com/natural-language/docs/moderating-text) to analyze predictions against a list of safety attributes.\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Download prebuilt LLaMA2 models\n",
        "- Deploy prequantized LLaMA2 models on vLLM\n",
        "- Quantize LLaMA2 models with AWQ or GPTQ and deploy on vLLM\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "* Cloud NL APIs\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), [Cloud NL API pricing](https://cloud.google.com/natural-language/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "264c07757582"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioensNKM8ned"
      },
      "source": [
        "### Colab only\n",
        "Run the following commands for Colab and skip this section if you are using Workbench."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2707b02ef5df"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    ! pip3 install --upgrade google-cloud-aiplatform\n",
        "    ! pip3 install ipython pandas[output_formatting] google-cloud-language==2.10.0\n",
        "    from google.colab import auth as google_auth\n",
        "\n",
        "    google_auth.authenticate_user()\n",
        "\n",
        "    # Restart the notebook kernel after installs.\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "038512c9338a"
      },
      "source": [
        "### Workbench only\n",
        "If you are using Workbench, you should find that the necessary dependencies are already pre-installed. If this is not the case or if you have previously modified the existing libraries, you may install the dependencies using the following commands:\n",
        "```\n",
        "! pip3 install --upgrade google-cloud-aiplatform\n",
        "! pip3 install ipython pandas[output_formatting] google-cloud-language==2.10.0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb7adab99e41"
      },
      "source": [
        "### Setup Google Cloud project\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "1. [Enable the Vertex AI API, Compute Engine API and Cloud Natural Language API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component,language.googleapis.com).\n",
        "\n",
        "1. [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs.\n",
        "\n",
        "1. [Create a service account](https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console) with `Vertex AI User` and `Storage Object Admin` roles for deploying fine tuned model to Vertex AI endpoint."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35Dvbzb0hH3-"
      },
      "source": [
        "### Import the necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLsuvskfhOv4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from datetime import datetime\n",
        "from typing import Tuple\n",
        "\n",
        "from google.cloud import aiplatform, language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c460088b873"
      },
      "source": [
        "Set the following variables for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`) should be located in the specified region (`REGION`). Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "855d6b96f291"
      },
      "outputs": [],
      "source": [
        "# Cloud project id.\n",
        "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# Region for launching jobs.\n",
        "REGION = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# Cloud Storage bucket for storing experiments output.\n",
        "# Start with gs:// prefix, e.g. gs://foo_bucket.\n",
        "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
        "\n",
        "! gcloud config set project $PROJECT_ID\n",
        "! gcloud services enable language.googleapis.com\n",
        "\n",
        "\n",
        "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
        "EXPERIMENT_BUCKET = os.path.join(BUCKET_URI, \"peft\")\n",
        "BASE_MODEL_BUCKET = os.path.join(EXPERIMENT_BUCKET, \"base_model\")\n",
        "\n",
        "# The service account looks like:\n",
        "# '@.iam.gserviceaccount.com'\n",
        "# Please go to https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console\n",
        "# and create service account with `Vertex AI User` and `Storage Object Admin` roles.\n",
        "# The service account for deploying fine tuned model.\n",
        "SERVICE_ACCOUNT = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user(project_id=PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e828eb320337"
      },
      "source": [
        "### Initialize Vertex AI API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12cd25839741"
      },
      "outputs": [],
      "source": [
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cc825514deb"
      },
      "source": [
        "### Define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b42bd4fa2b2d"
      },
      "outputs": [],
      "source": [
        "# The pre-built training, serving and evaluation docker images.\n",
        "TRAIN_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-train:20231211_0936_RC00\"\n",
        "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20231127_0916_RC00\"\n",
        "VLLM_GPTQ_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:gptq\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c250872074f"
      },
      "source": [
        "### Define common functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "354da31189dc"
      },
      "outputs": [],
      "source": [
        "def get_job_name_with_datetime(prefix: str) -> str:\n",
        "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
        "    jobs in Vertex AI.\n",
        "    \"\"\"\n",
        "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
        "\n",
        "\n",
        "def deploy_model_vllm(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    service_account: str,\n",
        "    machine_type: str = \"n1-standard-8\",\n",
        "    accelerator_type: str = \"NVIDIA_TESLA_V100\",\n",
        "    accelerator_count: int = 1,\n",
        "    quantization_method: str = \"\",\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
        "\n",
        "    vllm_args = [\n",
        "        \"--host=0.0.0.0\",\n",
        "        \"--port=7080\",\n",
        "        f\"--model={model_id}\",\n",
        "        f\"--tensor-parallel-size={accelerator_count}\",\n",
        "        \"--swap-space=16\",\n",
        "        \"--gpu-memory-utilization=0.9\",\n",
        "        \"--max-num-batched-tokens=4096\",\n",
        "        \"--disable-log-stats\",\n",
        "    ]\n",
        "    if quantization_method:\n",
        "        vllm_args.append(f\"--quantization={quantization_method}\")\n",
        "    if quantization_method == \"gptq\":\n",
        "        vllm_docker_uri = VLLM_GPTQ_DOCKER_URI\n",
        "    else:\n",
        "        vllm_docker_uri = VLLM_DOCKER_URI\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=vllm_docker_uri,\n",
        "        serving_container_command=[\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
        "        serving_container_args=vllm_args,\n",
        "        serving_container_ports=[7080],\n",
        "        serving_container_predict_route=\"/generate\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "    )\n",
        "\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=service_account,\n",
        "    )\n",
        "    return model, endpoint\n",
        "\n",
        "\n",
        "def moderate_text(text: str) -> language.ModerateTextResponse:\n",
        "    \"\"\"Calls Vertex AI APIs to analyze text moderations.\"\"\"\n",
        "    client = language.LanguageServiceClient()\n",
        "    document = language.Document(\n",
        "        content=text,\n",
        "        type_=language.Document.Type.PLAIN_TEXT,\n",
        "    )\n",
        "    return client.moderate_text(document=document)\n",
        "\n",
        "\n",
        "def show_text_moderation(text: str, response: language.ModerateTextResponse) -> None:\n",
        "    \"\"\"Shows text moderation results.\"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    def confidence(category: language.ClassificationCategory) -> float:\n",
        "        return category.confidence\n",
        "\n",
        "    columns = [\"category\", \"confidence\"]\n",
        "    categories = sorted(response.moderation_categories, key=confidence, reverse=True)\n",
        "    data = ((category.name, category.confidence) for category in categories)\n",
        "    df = pd.DataFrame(columns=columns, data=data)\n",
        "\n",
        "    print(f\"Text analyzed:\\n{text}\")\n",
        "    print(df.to_markdown(index=False, tablefmt=\"presto\", floatfmt=\".0%\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivs2RK093c8X"
      },
      "source": [
        "## Access LLaMA2 pretrained and finetuned models\n",
        "The original models from Meta are converted into the Hugging Face format for finetuning and serving in Vertex AI.\n",
        "\n",
        "Accept the model agreement to access the models:\n",
        "1. Navigate to the Vertex AI > Model Garden page in the Google Cloud console\n",
        "2. Find the LLaMA2 model card and click on \"VIEW DETAILS\"\n",
        "3. Review the agreement on the model card page\n",
        "4. After clicking the agreement of LLaMA2, a Cloud Storage bucket containing LLaMA2 pretrained and finetuned models will be shared\n",
        "5. Paste the Cloud Storage bucket link below and assign it to `VERTEX_AI_MODEL_GARDEN_LLAMA2`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jwn4PcTf4EMt"
      },
      "outputs": [],
      "source": [
        "VERTEX_AI_MODEL_GARDEN_LLAMA2 = \"\"  # This will be shared once click the agreement of LLaMA2 in Vertex AI Model Garden.\n",
        "assert (\n",
        "    VERTEX_AI_MODEL_GARDEN_LLAMA2\n",
        "), \"Please click the agreement of LLaMA2 in Vertex AI Model Garden, and get the GCS path of LLaMA2 model artifacts.\"\n",
        "print(\n",
        "    \"Copy LLaMA2 model artifacts from\",\n",
        "    VERTEX_AI_MODEL_GARDEN_LLAMA2,\n",
        "    \"to \",\n",
        "    BASE_MODEL_BUCKET,\n",
        ")\n",
        "! gsutil -m cp -R $VERTEX_AI_MODEL_GARDEN_LLAMA2/* $BASE_MODEL_BUCKET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MjaORIIFDVu"
      },
      "source": [
        "Set the base model id."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8OiHHNNE_wj"
      },
      "outputs": [],
      "source": [
        "base_model_name = \"llama2-7b-chat-hf\"  # @param [\"llama2-7b-hf\", \"llama2-7b-chat-hf\", \"llama2-13b-hf\", \"llama2-13b-chat-hf\", \"llama2-70b-hf\", \"llama2-70b-chat-hf\"]\n",
        "base_model_id = os.path.join(BASE_MODEL_BUCKET, base_model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXaszFtvOk8v"
      },
      "source": [
        "## Quantize and deploy LLaMA 2 models\n",
        "\n",
        "This section demonstrates post-training quantization of LLaMA2 models with Vertex Custom Job. Quantization reduces the memory required by a model while attempting to retain the same performance. Two such algorithms to do so are AWQ and GPTQ. Read more about AWQ in the following publication: [AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](https://arxiv.org/abs/2306.00978). Read more about GPTQ in the following publication: [GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers\n",
        "](https://arxiv.org/abs/2210.17323)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fZKui4v1gm3"
      },
      "source": [
        "### Deploy pre-quantized models with Google Cloud Text Moderation\n",
        "Many AWQ-quantized models are provided by TheBloke [here](https://huggingface.co/TheBloke?search_models=-awq), and GPTQ-quantized models are provided [here](https://huggingface.co/TheBloke?search_models=-gptq).\n",
        "\n",
        "This section uploads the model to Model Registry and deploys it on the Endpoint.\n",
        "\n",
        "The model deployment step will take 15 minutes to 1 hour to complete, depending on the model sizes.\n",
        "\n",
        "Notice that deploying a quantized model requires much less GPU.\n",
        "We can deploy a quantized 13B model with only one L4 instead of four, and\n",
        "we can deploy a quantized 70B model with only two L4s instead of eight."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fb99b15ee8f"
      },
      "outputs": [],
      "source": [
        "quantized_model_id = \"TheBloke/Llama-2-7B-chat-AWQ\"  # @param [\"TheBloke/Llama-2-7B-chat-AWQ\", \"TheBloke/Llama-2-13B-chat-AWQ\", \"TheBloke/Llama-2-70B-chat-AWQ\", \"TheBloke/Llama-2-7B-chat-GPTQ\", \"TheBloke/Llama-2-13B-chat-GPTQ\", \"TheBloke/Llama-2-70B-chat-GPTQ\"]\n",
        "\n",
        "quantization_method = quantized_model_id.split(\"-\")[-1].lower()\n",
        "\n",
        "# Finds Vertex AI prediction supported accelerators and regions in\n",
        "# https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
        "\n",
        "# Sets 1 L4 (24G) to deploy LLaMA2 7B and 13B models.\n",
        "machine_type = \"g2-standard-8\"\n",
        "accelerator_type = \"NVIDIA_L4\"\n",
        "accelerator_count = 1\n",
        "\n",
        "# Sets 2 L4's (24G) to deploy LLaMA2 70B models.\n",
        "# machine_type = \"g2-standard-24\"\n",
        "# accelerator_type = \"NVIDIA_L4\"\n",
        "# accelerator_count = 2\n",
        "\n",
        "model_prequantized_vllm, endpoint_prequantized_vllm = deploy_model_vllm(\n",
        "    model_name=get_job_name_with_datetime(prefix=\"llama2-serve-vllm-prequantized\"),\n",
        "    model_id=quantized_model_id,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    quantization_method=quantization_method,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMqARZni4LXq"
      },
      "source": [
        "NOTE: After the deployment succeeds, the model weights will be downloaded on the fly. Thus additional 10 ~ 40 minutes (depending on the model sizes) of waiting time is needed **after** the above model deployment step succeeds and before you run the next step below. Otherwise you might see a `ServiceUnavailable: 503 502:Bad Gateway` error when you send requests to the endpoint.\n",
        "\n",
        "Once deployment succeeds, you can send requests to the endpoint with text prompts.\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "Human: What is a car?\n",
        "Assistant:  A car, or a motor car, is a road-connected human-transportation system used to move people or goods from one place to another. The term also encompasses a wide range of vehicles, including motorboats, trains, and aircrafts. Cars typically have four wheels, a cabin for passengers, and an engine or motor. They have been around since the early 19th century and are now one of the most popular forms of transportation, used for daily commuting, shopping, and other purposes.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uLlv-uG4LXq"
      },
      "outputs": [],
      "source": [
        "# Loads an existing endpoint instance using the endpoint name:\n",
        "# - Using `endpoint_name = endpoint_prequantized_vllm.name` allows us to get the\n",
        "#   endpoint name of the endpoint `endpoint_prequantized_vllm` created in the cell\n",
        "#   above.\n",
        "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
        "#   an existing endpoint with the ID 1234567890123456789.\n",
        "# You may uncomment the code below to load an existing endpoint.\n",
        "\n",
        "# endpoint_name = endpoint_prequantized_vllm.name\n",
        "# # endpoint_name = \"\"  # @param {type:\"string\"}\n",
        "# aip_endpoint_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
        "# )\n",
        "# endpoint_prequantized_vllm = aiplatform.Endpoint(aip_endpoint_name)\n",
        "\n",
        "\n",
        "# Overides max_length and top_k parameters during inferences.\n",
        "# If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`,\n",
        "# you can reduce the max length, such as set max_length as 20.\n",
        "# If you are using L4 GPUs to serve LLaMA2 70B models, you should set\n",
        "# max_length to around 1,000 tokens or fewer. If you need longer generated\n",
        "# sequences, please file a request with Vertex to allowlist your project for a\n",
        "# longer timeout threshold with Vertex endpoints.\n",
        "instances = [\n",
        "    {\n",
        "        \"prompt\": \"What is a car?\",\n",
        "        \"max_tokens\": 50,\n",
        "        \"temperature\": 1.0,\n",
        "        \"top_p\": 1.0,\n",
        "        \"top_k\": 10,\n",
        "    },\n",
        "]\n",
        "response = endpoint_prequantized_vllm.predict(instances=instances)\n",
        "\n",
        "for prediction in response.predictions:\n",
        "    print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwcWPHCx4LXq"
      },
      "source": [
        "Text moderation analyzes a document against a list of safety attributes, which include \"harmful categories\" and topics that may be considered sensitive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEL89BkW4LXq"
      },
      "outputs": [],
      "source": [
        "for generated_text in response.predictions:\n",
        "    # Send a request to the API.\n",
        "    response = moderate_text(generated_text)\n",
        "    # Show the results.\n",
        "    show_text_moderation(generated_text, response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yHz2iiAOk8v"
      },
      "source": [
        "### Quantize LLaMA2 models\n",
        "\n",
        "Quantization reduces the amount of GPU required to serve a model by reducing the bit precision of the weights while minimizing drop in performance. Serving quantized models on VLLM requires models to be quantized to 4 bits. It is recommended to first search if a model has already been quantized and made publicly available: [AWQ](https://huggingface.co/TheBloke?search_models=-awq) and [GPTQ](https://huggingface.co/TheBloke?search_models=-gptq). Quantizing models with AWQ will take around 0.5 hours for LLaMA2 7B, 1.5 hours for LLaMA2 13B, and 4.5 hours for LLaMA2 70B, using 1 NVIDIA_L4 GPU for 7B and 13B models and 8 NVIDIA_L4 GPUs for 70B model. Quantizing models with GPTQ will take around 1.5 hours for LLaMA2 7B, 3 hours for LLaMA2 2.5 hours for LLaMA2 13B, and 6 hours for LLaMA 70B models, using 1 NVIDIA_L4 GPU for 7B and 13B models and 8 NVIDIA_L4 GPUs for 70B model. Finetuned LLaMA2 models can also be quantized, so long as the LoRA weights are merged with the base model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsJI-amIOk8v"
      },
      "outputs": [],
      "source": [
        "# Setup quantization job.\n",
        "\n",
        "# Set `finetuned_model_path` to a finetuned LLaMA2 model stored in GCS to\n",
        "# quantize it. If not, the base model will be quantized.\n",
        "finetuned_model_path = \"\"  # @param {type:\"string\"}\n",
        "if finetuned_model_path:\n",
        "    prequantized_model_path = finetuned_model_path\n",
        "else:\n",
        "    prequantized_model_path = base_model_id\n",
        "\n",
        "quantization_method = \"awq\"  # @param [\"awq\", \"gptq\"]\n",
        "quantization_job_name = get_job_name_with_datetime(\n",
        "    f\"llama2-{quantization_method}-quantize\"\n",
        ")\n",
        "\n",
        "quantization_output_dir = os.path.join(MODEL_BUCKET, quantization_job_name)\n",
        "quantization_output_dir_gcsfuse = quantization_output_dir.replace(\"gs://\", \"/gcs/\")\n",
        "\n",
        "# Worker pool spec.\n",
        "\n",
        "# Sets 1 L4 (24G) to quantize 7B and 13B models.\n",
        "machine_type = \"g2-standard-16\"\n",
        "accelerator_type = \"NVIDIA_L4\"\n",
        "accelerator_count = 1\n",
        "\n",
        "# Sets 8 L4 (24G) to quantize 70B models.\n",
        "# machine_type = \"g2-standard-96\"\n",
        "# accelerator_type = \"NVIDIA_L4\"\n",
        "# accelerator_count = 8\n",
        "\n",
        "\n",
        "# Quantization parameters.\n",
        "quantization_precision_mode = \"4bit\"\n",
        "if quantization_method == \"awq\":\n",
        "    awq_dataset_name = \"pileval\"\n",
        "    group_size = 128\n",
        "    quantization_args = [\n",
        "        \"--task=quantize-model\",\n",
        "        f\"--quantization_method={quantization_method}\",\n",
        "        f\"--pretrained_model_id={base_model_id}\",\n",
        "        f\"--quantization_precision_mode={quantization_precision_mode}\",\n",
        "        f\"--quantization_output_dir={quantization_output_dir_gcsfuse}\",\n",
        "        f\"--quantization_dataset_name={awq_dataset_name}\",\n",
        "        f\"--group_size={group_size}\",\n",
        "    ]\n",
        "else:\n",
        "    # The original datasets used in GPTQ paper [\"wikitext2\",\"c4\",\"c4-new\",\"ptb\",\"ptb-new\"].\n",
        "    gptq_dataset_name = \"c4\"  # @param {type:\"string\"}\n",
        "    gptq_precision_mode = \"4bit\"\n",
        "    group_size = -1\n",
        "    damp_percent = 0.1\n",
        "    desc_act = True\n",
        "    quantization_args = [\n",
        "        \"--task=quantize-model\",\n",
        "        f\"--quantization_method={quantization_method}\",\n",
        "        f\"--pretrained_model_id={base_model_id}\",\n",
        "        f\"--quantization_precision_mode={quantization_precision_mode}\",\n",
        "        f\"--quantization_output_dir={quantization_output_dir_gcsfuse}\",\n",
        "        f\"--quantization_dataset_name={gptq_dataset_name}\",\n",
        "        f\"--group_size={group_size}\",\n",
        "        f\"--damp_percent={damp_percent}\",\n",
        "        f\"--desc_act={desc_act}\",\n",
        "        \"--cache_examples_on_gpu=False\",\n",
        "    ]\n",
        "\n",
        "# Pass quantization arguments and launch job.\n",
        "worker_pool_specs = [\n",
        "    {\n",
        "        \"machine_spec\": {\n",
        "            \"machine_type\": machine_type,\n",
        "            \"accelerator_type\": accelerator_type,\n",
        "            \"accelerator_count\": accelerator_count,\n",
        "        },\n",
        "        \"replica_count\": 1,\n",
        "        \"disk_spec\": {\n",
        "            \"boot_disk_type\": \"pd-ssd\",\n",
        "            \"boot_disk_size_gb\": 500,\n",
        "        },\n",
        "        \"container_spec\": {\n",
        "            \"image_uri\": TRAIN_DOCKER_URI,\n",
        "            \"env\": [\n",
        "                {\n",
        "                    \"name\": \"PYTORCH_CUDA_ALLOC_CONF\",\n",
        "                    \"value\": \"max_split_size_mb:32\",\n",
        "                },\n",
        "            ],\n",
        "            \"command\": [],\n",
        "            \"args\": quantization_args,\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"Quantizing {prequantized_model_path}.\")\n",
        "quantize_job = aiplatform.CustomJob(\n",
        "    display_name=quantization_job_name,\n",
        "    project=PROJECT_ID,\n",
        "    worker_pool_specs=worker_pool_specs,\n",
        "    staging_bucket=STAGING_BUCKET,\n",
        ")\n",
        "quantize_job.run()\n",
        "\n",
        "print(\"Quantized models were saved in: \", quantization_output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB6P-InJOk8w"
      },
      "source": [
        "### Deploy quantized models with Google Cloud Text Moderation\n",
        "This section uploads the model to Model Registry and deploys it on the Endpoint.\n",
        "\n",
        "The model deployment step will take 15 minutes to 1 hour to complete, depending on the model sizes.\n",
        "\n",
        "Notice that deploying a quantized model requires much less GPU.\n",
        "We can deploy a quantized 13B model with only one L4 instead of four, and\n",
        "we can deploy a quantized 70B model with only two L4s instead of eight."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ExjQ5JAOk8w"
      },
      "outputs": [],
      "source": [
        "# Finds Vertex AI prediction supported accelerators and regions in\n",
        "# https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
        "\n",
        "# Sets 1 L4 (24G) to deploy LLaMA2 7B and 13B models.\n",
        "machine_type = \"g2-standard-8\"\n",
        "accelerator_type = \"NVIDIA_L4\"\n",
        "accelerator_count = 1\n",
        "\n",
        "# Sets 4 L4's (24G) to deploy LLaMA2 70B models.\n",
        "# machine_type = \"g2-standard-24\"\n",
        "# accelerator_type = \"NVIDIA_L4\"\n",
        "# accelerator_count = 2\n",
        "\n",
        "model_quantized_vllm, endpoint_quantized_vllm = deploy_model_vllm(\n",
        "    model_name=get_job_name_with_datetime(prefix=\"llama2-serve-vllm-quantized\"),\n",
        "    model_id=quantization_output_dir,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    quantization_method=quantization_method,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuywY2xnQxTK"
      },
      "source": [
        "NOTE: After the deployment succeeds, the model weights will be downloaded on the fly. Thus additional 10 ~ 40 minutes (depending on the model sizes) of waiting time is needed **after** the above model deployment step succeeds and before you run the next step below. Otherwise you might see a `ServiceUnavailable: 503 502:Bad Gateway` error when you send requests to the endpoint.\n",
        "\n",
        "Once deployment succeeds, you can send requests to the endpoint with text prompts.\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "Human: What is a car?\n",
        "Assistant:  A car, or a motor car, is a road-connected human-transportation system used to move people or goods from one place to another. The term also encompasses a wide range of vehicles, including motorboats, trains, and aircrafts. Cars typically have four wheels, a cabin for passengers, and an engine or motor. They have been around since the early 19th century and are now one of the most popular forms of transportation, used for daily commuting, shopping, and other purposes.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwGJnNXqQzIW"
      },
      "outputs": [],
      "source": [
        "# Loads an existing endpoint instance using the endpoint name:\n",
        "# - Using `endpoint_name = endpoint_quantized_vllm.name` allows us to get the\n",
        "#   endpoint name of the endpoint `endpoint_quantized_vllm` created in the cell\n",
        "#   above.\n",
        "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
        "#   an existing endpoint with the ID 1234567890123456789.\n",
        "# You may uncomment the code below to load an existing endpoint.\n",
        "\n",
        "# endpoint_name = endpoint_quantized_vllm.name\n",
        "# # endpoint_name = \"\"  # @param {type:\"string\"}\n",
        "# aip_endpoint_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
        "# )\n",
        "# endpoint_quantized_vllm = aiplatform.Endpoint(aip_endpoint_name)\n",
        "\n",
        "\n",
        "# Overides max_length and top_k parameters during inferences.\n",
        "# If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`,\n",
        "# you can reduce the max length, such as set max_length as 20.\n",
        "# If you are using L4 GPUs to serve LLaMA2 70B models, you should set\n",
        "# max_length to around 1,000 tokens or fewer. If you need longer generated\n",
        "# sequences, please file a request with Vertex to allowlist your project for a\n",
        "# longer timeout threshold with Vertex endpoints.\n",
        "instances = [\n",
        "    {\n",
        "        \"prompt\": \"What is a car?\",\n",
        "        \"max_tokens\": 50,\n",
        "        \"temperature\": 1.0,\n",
        "        \"top_p\": 1.0,\n",
        "        \"top_k\": 10,\n",
        "    },\n",
        "]\n",
        "response = endpoint_quantized_vllm.predict(instances=instances)\n",
        "\n",
        "for prediction in response.predictions:\n",
        "    print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a0_lqNzQ25q"
      },
      "source": [
        "Text moderation analyzes a document against a list of safety attributes, which include \"harmful categories\" and topics that may be considered sensitive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rC4HXayZQ41C"
      },
      "outputs": [],
      "source": [
        "for generated_text in response.predictions:\n",
        "    # Send a request to the API.\n",
        "    response = moderate_text(generated_text)\n",
        "    # Show the results.\n",
        "    show_text_moderation(generated_text, response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af21a3cff1e0"
      },
      "source": [
        "## Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "911406c1561e"
      },
      "outputs": [],
      "source": [
        "# Delete the quantization job.\n",
        "quantize_job.delete()\n",
        "\n",
        "# Undeploy model and delete endpoint.\n",
        "endpoint_prequantized_vllm.delete(force=True)\n",
        "endpoint_quantized_vllm.delete(force=True)\n",
        "\n",
        "# Delete models.\n",
        "model_prequantized_vllm.delete()\n",
        "model_quantized_vllm.delete()\n",
        "\n",
        "# Delete Cloud Storage objects that were created\n",
        "delete_bucket = False\n",
        "if delete_bucket:\n",
        "    ! gsutil -m rm -r $EXPERIMENT_BUCKET"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_pytorch_llama2_quantization.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

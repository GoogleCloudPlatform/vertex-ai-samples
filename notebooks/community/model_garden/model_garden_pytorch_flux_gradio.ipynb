{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FvusFZ1lHNIh"
      },
      "outputs": [],
      "source": [
        "# Copyright 2026 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEF8AdbMHNIh"
      },
      "source": [
        "# Vertex AI Model Garden GenAI Workshop for Flux\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_pytorch_flux_gradio.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_flux_gradio.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDVdfbWXJn8Y"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates starting a playground for [black-forest-labs/FLUX.1-schnell](https://huggingface.co/black-forest-labs/FLUX.1-schnell) model based on [Gradio UI](https://www.gradio.app/), which allows users to interact with the identity-preserving image generation model more easily and intuitively.\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Deploy model to a [Vertex AI Endpoint resource](https://cloud.google.com/vertex-ai/docs/predictions/using-private-endpoints).\n",
        "- Run online predictions for `text-to-image` tasks from the UI.\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3yxOr7vJn8Y"
      },
      "source": [
        "## Run the playground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hmHDYTk9Jn8Y"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project and prepare dependencies\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown 2. **[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment.\n",
        "\n",
        "REGION = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown 3. If you want to run predictions with A100 80GB or H100 GPUs, we recommend using the regions listed below. **NOTE:** Make sure you have associated quota in selected regions. Click the links to see your current quota for each GPU type: [Nvidia A100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_a100_80gb_gpus), [Nvidia H100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_h100_gpus). You can request for quota following the instructions at [\"Request a higher quota\"](https://cloud.google.com/docs/quota/view-manage#requesting_higher_quota).\n",
        "\n",
        "# @markdown | Machine Type | Accelerator Type | Recommended Regions |\n",
        "# @markdown | ----------- | ----------- | ----------- |\n",
        "# @markdown | a2-ultragpu-1g | 1 NVIDIA_A100_80GB | us-central1, us-east4, europe-west4, asia-southeast1, us-east4 |\n",
        "# @markdown | a3-highgpu-2g | 2 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "# @markdown | a3-highgpu-4g | 4 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "# @markdown | a3-highgpu-8g | 8 NVIDIA_H100_80GB | us-central1, europe-west4, us-west1, asia-southeast1 |\n",
        "\n",
        "! pip3 install --upgrade gradio==4.29.0 opencv-python\n",
        "# Uninstall nest-asyncio and uvloop as a workaround to https://github.com/gradio-app/gradio/issues/8238#issuecomment-2101066984\n",
        "! pip3 uninstall --yes nest-asyncio uvloop\n",
        "# A workaround for the compatibility between the fastapi and pydantic\n",
        "! pip3 install fastapi==0.112.3\n",
        "\n",
        "import importlib\n",
        "import os\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
        "\n",
        "common_util = importlib.import_module(\n",
        "    \"vertex-ai-samples.notebooks.community.model_garden.docker_source_codes.notebook_util.common_util\"\n",
        ")\n",
        "\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "if not REGION:\n",
        "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
        "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "# Gets the default SERVICE_ACCOUNT.\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
        "\n",
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AWnhhOHmJn8Y"
      },
      "outputs": [],
      "source": [
        "# @title Start the playground\n",
        "\n",
        "# @markdown This is a playground for running Flux models.\n",
        "\n",
        "# @markdown After running the cell, a public URL ([\"https://*.gradio.live\"](#)) will appear in the cell output. The playground is available in a separate browser tab when you click the URL.\n",
        "\n",
        "# @markdown **How to use:**\n",
        "# @markdown 1. Select or deploy model\n",
        "# @markdown   1. In the playground, select a previous deployed Flux model.\n",
        "# @markdown   2. If you don't have any deployed model, deploy a new model in the playground.\n",
        "# @markdown   3. New deployment takes ~20 minutes. You can check the progress at [Vertex Online Prediction](https://console.cloud.google.com/vertex-ai/online-prediction/endpoints).\n",
        "# @markdown   4. After the model deployment is complete, restart the playground in Colab to see the updated endpoint list.\n",
        "# @markdown 1. Inference\n",
        "# @markdown   1. In the \"Inference\" section, fill in prompt and parameters\n",
        "# @markdown   2. Click \"Generate\" to generate image from text prompt.\n",
        "\n",
        "# @markdown **Important notes**\n",
        "# @markdown 1. Reruning this notebook cell creates a new public URL. Previous URLs will stop working.\n",
        "# @markdown 2. After experiments, manually undeploy models to avoid continuous charges to the project.\n",
        "# @markdown 3. This playground is specially built for the Flux models.\n",
        "# @markdown Other models may work, but they are not tested. Use with caution.\n",
        "\n",
        "import gradio as gr\n",
        "from google.cloud import aiplatform\n",
        "from PIL import Image\n",
        "\n",
        "# The pre-built serving docker image. It contains serving scripts and models.\n",
        "SERVE_DOCKER_URI = \"us-docker.pkg.dev/deeplearning-platform-release/vertex-model-garden/pytorch-inference.cu125.0-4.ubuntu2204.py310\"\n",
        "\n",
        "\n",
        "def is_flux_endpoint(endpoint: aiplatform.Endpoint) -> bool:\n",
        "    \"\"\"Returns True if the endpoint is an Flux endpoint.\"\"\"\n",
        "    return \"flux\" in endpoint.display_name.lower()\n",
        "\n",
        "\n",
        "def list_endpoints() -> list[str]:\n",
        "    \"\"\"Returns all valid prediction endpoints for in the project and region.\"\"\"\n",
        "    # Gets all the valid endpoints in the project and region.\n",
        "    endpoints = aiplatform.Endpoint.list(order_by=\"create_time desc\")\n",
        "    # Filters out the endpoints which do not have a deployed model, and the endpoint is for image generation\n",
        "    endpoints = list(\n",
        "        filter(\n",
        "            lambda endpoint: endpoint.traffic_split and is_flux_endpoint(endpoint),\n",
        "            endpoints,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    endpoint_names = list(\n",
        "        map(\n",
        "            lambda endpoint: f\"{endpoint.name} - {endpoint.display_name[:40]}\",\n",
        "            endpoints,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return endpoint_names\n",
        "\n",
        "\n",
        "def get_endpoint(endpoint_name: str) -> aiplatform.Endpoint:\n",
        "    \"\"\"Returns a Vertex endpoint for the given endpoint_name.\"\"\"\n",
        "\n",
        "    endpoint_id = endpoint_name.split(\" - \")[0]\n",
        "    endpoint = aiplatform.Endpoint(\n",
        "        f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_id}\"\n",
        "    )\n",
        "\n",
        "    return endpoint\n",
        "\n",
        "\n",
        "def deploy_model(model_id: str, accelerator_type: str) -> aiplatform.Endpoint:\n",
        "    \"\"\"Creates a new Vertex prediction endpoint and deploys a model to it.\"\"\"\n",
        "\n",
        "    if not model_id:\n",
        "        raise gr.Error(\"Select a valid model name for model list.\")\n",
        "        return\n",
        "\n",
        "    gr.Info(\"Model deployment started.\")\n",
        "\n",
        "    task_id = \"text-to-image\"\n",
        "    display_name = common_util.create_job_name(model_id)\n",
        "    endpoint = aiplatform.Endpoint.create(display_name=display_name)\n",
        "    serving_env = {\n",
        "        \"MODEL_ID\": model_id,\n",
        "        \"TASK\": task_id,\n",
        "        \"DEPLOY_SOURCE\": \"notebook_gradio\",\n",
        "    }\n",
        "\n",
        "    machine_type_map = {\n",
        "        \"NVIDIA_TESLA_A100\": \"a2-highgpu-1g\",\n",
        "        \"NVIDIA_A100_80GB\": \"a2-ultragpu-1g\",\n",
        "    }\n",
        "    machine_type = machine_type_map.get(accelerator_type)\n",
        "    accelerator_count = 1\n",
        "    common_util.check_quota(\n",
        "        project_id=PROJECT_ID,\n",
        "        region=REGION,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        is_for_training=False,\n",
        "    )\n",
        "\n",
        "    display_name = common_util.create_job_name(model_id)\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_id,\n",
        "        serving_container_image_uri=SERVE_DOCKER_URI,\n",
        "        serving_container_ports=[7080],\n",
        "        serving_container_predict_route=\"/predict\",\n",
        "        serving_container_health_route=\"/health\",\n",
        "        serving_container_environment_variables=serving_env,\n",
        "        model_garden_source_model_name=\"publishers/black-forest-labs/models/flux1-schnell\",\n",
        "    )\n",
        "\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=SERVICE_ACCOUNT,\n",
        "        sync=False,\n",
        "        system_labels={\"NOTEBOOK_NAME\": \"model_garden_pytorch_flux_gradio.ipynb\"},\n",
        "    )\n",
        "\n",
        "    gr.Info(\n",
        "        f\"Model {display_name} is being deployed. It may take ~20 minutes to complete.\"\n",
        "    )\n",
        "\n",
        "    return endpoint\n",
        "\n",
        "\n",
        "def get_default_image_height(model_name: str) -> int:\n",
        "    \"\"\"Returns the default image height for the given model_name.\"\"\"\n",
        "\n",
        "    return 1024\n",
        "\n",
        "\n",
        "def get_default_image_width(model_name: str) -> int:\n",
        "    \"\"\"Returns the default image width for the given model_name.\"\"\"\n",
        "\n",
        "    return 1024\n",
        "\n",
        "\n",
        "def get_default_num_inference_steps(model_name: str) -> int:\n",
        "    \"\"\"Returns the default num_inference_steps for the given model_name.\"\"\"\n",
        "\n",
        "    return 4\n",
        "\n",
        "\n",
        "def generate_images(\n",
        "    endpoint_name,\n",
        "    prompt=\"\",\n",
        "    num_inference_steps=5,\n",
        "    image_width=1024,\n",
        "    image_height=1024,\n",
        ") -> list[Image.Image]:\n",
        "    if not endpoint_name:\n",
        "        raise gr.Error(\"Select (or deploy) a model first.\")\n",
        "\n",
        "    instances = [{\"text\": prompt}]\n",
        "    parameters = {\n",
        "        \"height\": image_height,\n",
        "        \"width\": image_width,\n",
        "        \"num_inference_steps\": num_inference_steps,\n",
        "    }\n",
        "\n",
        "    response = get_endpoint(endpoint_name).predict(\n",
        "        instances=instances, parameters=parameters\n",
        "    )\n",
        "    images = [\n",
        "        common_util.base64_to_image(prediction.get(\"output\"))\n",
        "        for prediction in response.predictions\n",
        "    ]\n",
        "    return images\n",
        "\n",
        "\n",
        "def update_default_parameters(model_name: str):\n",
        "    \"\"\"Updates the default inference parameters based on the selected model.\"\"\"\n",
        "    return {\n",
        "        num_inference_steps: gr.update(\n",
        "            value=get_default_num_inference_steps(model_name)\n",
        "        ),\n",
        "        image_width: gr.update(value=get_default_image_width(model_name)),\n",
        "        image_height: gr.update(value=get_default_image_height(model_name)),\n",
        "    }\n",
        "\n",
        "\n",
        "tip_text = r\"\"\"\n",
        "1. Select a previous deployed Flux model.\n",
        "2. If you don't have any deployed model, deploy a new model. The deployment takes ~20 minutes. You can check the progress at [Vertex Online Prediction](https://console.cloud.google.com/vertex-ai/online-prediction/endpoints). After the model deployment is complete, restart the playground in Colab to see the updated endpoint list.\n",
        "3. In the \"Inference\" section, fill the prompt and parameters, then click \"Generate\" to generate image from text prompt.\n",
        "\"\"\"\n",
        "\n",
        "css = \"\"\"\n",
        ".gradio-container {\n",
        "  width: 90% !important\n",
        "}\n",
        "\"\"\"\n",
        "with gr.Blocks(\n",
        "    css=css, theme=gr.themes.Default(primary_hue=\"orange\", secondary_hue=\"blue\")\n",
        ") as demo:\n",
        "    gr.Markdown(\"# Model Garden Playground for Flux\")\n",
        "\n",
        "    with gr.Accordion(\"How To Use\", open=True):\n",
        "        tip = gr.Markdown(tip_text)\n",
        "\n",
        "    gr.Markdown(\"## Select or deploy model\")\n",
        "    gr.Markdown(\"### Select a previously deployed model\")\n",
        "    endpoint_name = gr.Dropdown(\n",
        "        label=\"Select a model previously deployed on Vertex\",\n",
        "        choices=list_endpoints(),\n",
        "        value=None,\n",
        "    )\n",
        "    gr.Markdown(\"### Deploy a new model to Vertex\")\n",
        "    model_id = gr.Dropdown(\n",
        "        label=\"Select a model to deploy\",\n",
        "        choices=[\n",
        "            \"black-forest-labs/FLUX.1-schnell\",\n",
        "        ],\n",
        "        value=None,\n",
        "    )\n",
        "    with gr.Row(equal_height=True):\n",
        "        with gr.Column(scale=7):\n",
        "            accelerator_type = gr.Dropdown(\n",
        "                label=\"Select accelerator type for deployment\",\n",
        "                choices=[\n",
        "                    \"NVIDIA_TESLA_A100\",\n",
        "                    \"NVIDIA_A100_80GB\",\n",
        "                ],\n",
        "                value=None,\n",
        "            )\n",
        "        with gr.Column(scale=1):\n",
        "            deploy_model_button = gr.Button(\n",
        "                \"Deploy\", scale=1, variant=\"primary\", min_width=10\n",
        "            )\n",
        "\n",
        "    gr.Markdown(\"## Inference\")\n",
        "    with gr.Row(equal_height=True):\n",
        "        with gr.Column(scale=1):\n",
        "            prompt = gr.Textbox(label=\"Prompt\", lines=3)\n",
        "            gr.Markdown(\"### Parameters\")\n",
        "            image_width = gr.Slider(\n",
        "                label=\"Image width\", value=1024, step=128, minimum=512, maximum=1024\n",
        "            )\n",
        "            image_height = gr.Slider(\n",
        "                label=\"Image height\", value=1024, step=128, minimum=512, maximum=1024\n",
        "            )\n",
        "            num_inference_steps = gr.Slider(\n",
        "                label=\"Inference steps\", value=4, step=1, minimum=1, maximum=25\n",
        "            )\n",
        "            generate_button = gr.Button(\"Generate\", variant=\"primary\")\n",
        "\n",
        "        with gr.Column(scale=4):\n",
        "            with gr.Row(equal_height=True):\n",
        "                with gr.Column(scale=3):\n",
        "                    image_output = gr.Gallery(\n",
        "                        label=\"Generated Images\",\n",
        "                        rows=1,\n",
        "                        height=715,\n",
        "                        preview=True,\n",
        "                    )\n",
        "\n",
        "    endpoint_name.change(\n",
        "        update_default_parameters,\n",
        "        endpoint_name,\n",
        "        [\n",
        "            num_inference_steps,\n",
        "            image_width,\n",
        "            image_height,\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    deploy_model_button.click(\n",
        "        deploy_model,\n",
        "        inputs=[model_id, accelerator_type],\n",
        "        outputs=[],\n",
        "    )\n",
        "\n",
        "    generate_button.click(\n",
        "        generate_images,\n",
        "        inputs=[\n",
        "            endpoint_name,\n",
        "            prompt,\n",
        "            num_inference_steps,\n",
        "            image_width,\n",
        "            image_height,\n",
        "        ],\n",
        "        outputs=image_output,\n",
        "    )\n",
        "\n",
        "show_debug_logs = True  # @param {type: \"boolean\"}\n",
        "demo.queue()\n",
        "demo.launch(share=True, inline=False, debug=show_debug_logs, show_error=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_pytorch_flux_gradio.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

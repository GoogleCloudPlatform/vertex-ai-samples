{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SgQ6t5bqZVlH"
      },
      "outputs": [],
      "source": [
        "# Copyright 2026 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Vertex AI Model Garden + Agent Engine - Build, Deploy and Test Agents using a Self-deployed Endpoint\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fmodel_garden%2Fmodel_garden_pytorch_llama3_1_agent_engine.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_llama3_1_agent_engine.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates downloading, deploying, and serving prebuilt Llama 3.1 models with [vLLM](https://github.com/vllm-project/vllm) (standard and optimized) and integrating with Agent Engine.\n",
        "\n",
        "[Agent Engine](https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/overview) (LangChain on Vertex AI) is a managed service in Vertex AI that helps you build and deploy model-based agents. It gives you the flexibility to choose how much reasoning you want to delegate to the LLM and how much you want to handle with custom code.\n",
        "\n",
        "A previous [notebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_openai_api_llama3_1.ipynb) demonstrates how to use Llama 3.1 models as Model-as-a-service (MaaS) to build `chatbot` and `translator` agents.\n",
        "\n",
        "This notebook demonstrates how to build, deploy and test these agents using [Agent Engine](https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/overview) with self-deployed model in Vertex AI.\n",
        "\n",
        "\n",
        "### Objective\n",
        "\n",
        "- Select one of the following three ways to deploy Llama 3.1 to an endpoint\n",
        "    - Deploy Llama 3.1 8B Instruct with the Fast Deployment feature\n",
        "    - Deploy Llama 3.1 8B, 70B and 405B with standard vLLM on GPU, optionally with dynamic LoRA adapters\n",
        "    - Deploy Llama 3.1 8B and 70B with optimized vLLM on GPU\n",
        "    \n",
        "- Integrate with Agent Engine: Use the Vertex AI SDK to build three simple agents with the deployed endpoint:\n",
        "    - A Chatbot Agent\n",
        "    - A Translator Agent\n",
        "    - An Agent that uses [an Exchange Rate Tool](https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/develop#define-function)\n",
        "- Test your agent locally.\n",
        "- Deploy and test your agent on the Agent Engine.\n",
        "\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "264c07757582"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ax7zWynUDcjk"
      },
      "outputs": [],
      "source": [
        "# @title Request for quota\n",
        "\n",
        "# @markdown By default, the quota for TPU deployment `Custom model serving TPU v5e cores per region` is 4, which is sufficient for serving the Llama 3.1 8B model. The Llama 3.1 70B model requires 16 TPU v5e cores. TPU quota is only available in `us-west1`. You can request for higher TPU quota following the instructions at [\"Request a higher quota\"](https://cloud.google.com/docs/quota/view-manage#requesting_higher_quota).\n",
        "\n",
        "# @markdown By default, the quota for H100 deployment `Custom model serving per region` is 0. You need to request for H100 quota following the instructions at [\"Request a higher quota\"](https://cloud.google.com/docs/quota/view-manage#requesting_higher_quota)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YXFGIp1l-qtT"
      },
      "outputs": [],
      "source": [
        "# @title Setup Google Cloud project\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown 2. **[Optional]** [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs. Set the BUCKET_URI for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`) should be located in the same region as where the notebook was launched. Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\"). If not set, a unique GCS bucket will be created instead.\n",
        "\n",
        "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown 3. **[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment.\n",
        "\n",
        "REGION = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown 4. If you want to run predictions with A100 80GB or H100 GPUs, we recommend using the regions listed below. **NOTE:** Make sure you have associated quota in selected regions. Click the links to see your current quota for each GPU type: [Nvidia A100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_a100_80gb_gpus), [Nvidia H100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_h100_gpus). You can request for quota following the instructions at [\"Request a higher quota\"](https://cloud.google.com/docs/quota/view-manage#requesting_higher_quota).\n",
        "\n",
        "# @markdown | Machine Type | Accelerator Type | Recommended Regions |\n",
        "# @markdown | ----------- | ----------- | ----------- |\n",
        "# @markdown | a2-ultragpu-1g | 1 NVIDIA_A100_80GB | us-central1, us-east4, europe-west4, asia-southeast1, us-east4 |\n",
        "# @markdown | a3-highgpu-2g | 2 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "# @markdown | a3-highgpu-4g | 4 NVIDIA_H100_80GB | us-west1, asia-southeast1, europe-west4 |\n",
        "# @markdown | a3-highgpu-8g | 8 NVIDIA_H100_80GB | us-central1, europe-west4, us-west1, asia-southeast1 |\n",
        "\n",
        "# Import the necessary packages\n",
        "\n",
        "# Upgrade Vertex AI SDK.\n",
        "! pip3 install --upgrade --quiet \\\n",
        "    \"google-cloud-aiplatform>=1.64.0\" \\\n",
        "    cloudpickle==3.0.0 \\\n",
        "    pydantic==2.10.6 \\\n",
        "    requests \\\n",
        "    langchain-openai\n",
        "! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
        "\n",
        "import datetime\n",
        "import importlib\n",
        "import os\n",
        "import requests\n",
        "import uuid\n",
        "from typing import Tuple\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "common_util = importlib.import_module(\n",
        "    \"vertex-ai-samples.notebooks.community.model_garden.docker_source_codes.notebook_util.common_util\"\n",
        ")\n",
        "\n",
        "models, endpoints = {}, {}\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "if not REGION:\n",
        "    if not os.environ.get(\"GOOGLE_CLOUD_REGION\"):\n",
        "        raise ValueError(\n",
        "            \"REGION must be set. See\"\n",
        "            \" https://cloud.google.com/vertex-ai/docs/general/locations for\"\n",
        "            \" available cloud locations.\"\n",
        "        )\n",
        "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
        "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
        "\n",
        "# Cloud Storage bucket for storing the experiment artifacts.\n",
        "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
        "# prefer using your own GCS bucket, change the value yourself below.\n",
        "now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "\n",
        "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
        "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}-{str(uuid.uuid4())[:4]}\"\n",
        "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
        "else:\n",
        "    assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
        "    shell_output = ! gsutil ls -Lb {BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
        "    bucket_region = shell_output[0].strip().lower()\n",
        "    if bucket_region != REGION:\n",
        "        raise ValueError(\n",
        "            \"Bucket region %s is different from notebook region %s\"\n",
        "            % (bucket_region, REGION)\n",
        "        )\n",
        "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
        "\n",
        "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
        "MODEL_BUCKET = os.path.join(BUCKET_URI, \"llama3_1\")\n",
        "\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "# Gets the default SERVICE_ACCOUNT.\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
        "\n",
        "\n",
        "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
        "\n",
        "! gcloud config set project $PROJECT_ID\n",
        "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/storage.admin\"\n",
        "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/aiplatform.user\"\n",
        "\n",
        "# @markdown # Access Llama 3.1 models on Vertex AI for serving\n",
        "# @markdown The original models from Meta are converted into the Hugging Face format for serving in Vertex AI.\n",
        "# @markdown Accept the model agreement to access the models:\n",
        "# @markdown 1. Open the [Llama 3.1 model card](https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama3_1) from [Vertex AI Model Garden](https://cloud.google.com/model-garden).\n",
        "# @markdown 2. Review and accept the agreement in the pop-up window on the model card page. If you have previously accepted the model agreement, there will not be a pop-up window on the model card page and this step is not needed.\n",
        "# @markdown 3. After accepting the agreement of Llama 3.1, a `gs://` URI containing Llama 3.1 pretrained and instruction-tuned models will be shared.\n",
        "# @markdown 4. Paste the URI in the `VERTEX_AI_MODEL_GARDEN_LLAMA_3_1` field below.\n",
        "\n",
        "\n",
        "VERTEX_AI_MODEL_GARDEN_LLAMA_3_1 = \"\"  # @param {type:\"string\", isTemplate:true}\n",
        "assert (\n",
        "    VERTEX_AI_MODEL_GARDEN_LLAMA_3_1\n",
        "), \"Click the agreement of Llama 3.1 in Vertex AI Model Garden, and get the GCS path of Llama 3.1 model artifacts.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eE_eYCDGiIZ"
      },
      "source": [
        "### Select one of the following three ways to deploy the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HyYES4HB5EHm"
      },
      "outputs": [],
      "source": [
        "# @title Deploy prebuilt Llama 3.1 8B Instruct with standard vLLM and Fast Deployment\n",
        "\n",
        "# @markdown This section demonstrates how to use the Fast Deployment feature.\n",
        "\n",
        "# @markdown The Fast Deployment feature prioritizes speed for model exploration, making it ideal for initial testing and experimentation. For sensitive data or production workloads, use the Standard environment for enhanced security and stability.\n",
        "\n",
        "# @markdown Note that only a subset of the models support the Fast Deployment feature.\n",
        "\n",
        "FAST_DEPLOYMENT_REGION = \"us-central1\"  # @param [\"us-central1\"] {isTemplate:true}\n",
        "\n",
        "API_ENDPOINT = f\"{FAST_DEPLOYMENT_REGION}-aiplatform.googleapis.com\"\n",
        "\n",
        "\n",
        "def fast_deploy(\n",
        "    publisher: str, publisher_model_id: str, version_id: str\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    url = f\"https://{API_ENDPOINT}/v1/publishers/{publisher}/models/{publisher_model_id}@{version_id}\"\n",
        "    access_token = ! gcloud auth print-access-token\n",
        "    access_token = access_token[0]\n",
        "\n",
        "    response = requests.get(\n",
        "        url,\n",
        "        headers={\n",
        "            \"Authorization\": f\"Bearer {access_token}\",\n",
        "            \"Content-Type\": \"application/json\",\n",
        "        },\n",
        "    )\n",
        "    response.raise_for_status()\n",
        "    response = response.json()\n",
        "    if (\n",
        "        len(\n",
        "            response.get(\"supportedActions\", {})\n",
        "            .get(\"multiDeployVertex\", {})\n",
        "            .get(\"multiDeployVertex\", {})\n",
        "        )\n",
        "        == 0\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            f\"No supportedActions.multiDeployVertex found in {FAST_DEPLOYMENT_REGION}. You can skip\"\n",
        "            \" this section or try a different region.\"\n",
        "        )\n",
        "    deploy_configs = response[\"supportedActions\"][\"multiDeployVertex\"][\n",
        "        \"multiDeployVertex\"\n",
        "    ]\n",
        "    fast_deploy_config = [\n",
        "        config\n",
        "        for config in deploy_configs\n",
        "        if config[\"deployMetadata\"]\n",
        "        .get(\"labels\", {})\n",
        "        .get(\"show-faster-deployment-option\")\n",
        "        == \"true\"\n",
        "    ]\n",
        "    if fast_deploy_config:\n",
        "        fast_deploy_config = fast_deploy_config[0]\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            f\"No Fast Deployment config found in {FAST_DEPLOYMENT_REGION}. You can skip this\"\n",
        "            \" section or try a different region.\"\n",
        "        )\n",
        "\n",
        "    container_spec = fast_deploy_config[\"containerSpec\"]\n",
        "    machine_spec = fast_deploy_config[\"dedicatedResources\"][\"machineSpec\"]\n",
        "    machine_type = machine_spec[\"machineType\"]\n",
        "    accelerator_type = machine_spec[\"acceleratorType\"]\n",
        "    accelerator_count = machine_spec[\"acceleratorCount\"]\n",
        "    env = {item[\"name\"]: item[\"value\"] for item in container_spec.get(\"env\", [])}\n",
        "    if \"DEPLOY_SOURCE\" in env:\n",
        "        del env[\"DEPLOY_SOURCE\"]\n",
        "    port = container_spec.get(\"ports\", [{}])[0].get(\"containerPort\")\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=fast_deploy_config.get(\"modelDisplayName\"),\n",
        "        serving_container_image_uri=container_spec.get(\"imageUri\"),\n",
        "        serving_container_args=container_spec.get(\"args\"),\n",
        "        serving_container_environment_variables=env,\n",
        "        serving_container_ports=[port],\n",
        "        serving_container_predict_route=container_spec.get(\"predictRoute\"),\n",
        "        serving_container_health_route=container_spec.get(\"healthRoute\"),\n",
        "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
        "        serving_container_deployment_timeout=7200,\n",
        "        location=FAST_DEPLOYMENT_REGION,\n",
        "        model_garden_source_model_name=(\n",
        "            f\"publishers/{publisher}/models/{publisher_model_id}\"\n",
        "        ),\n",
        "    )\n",
        "    endpoint = aiplatform.Endpoint.create(\n",
        "        display_name=model.name + \"-endpoint\",\n",
        "        location=FAST_DEPLOYMENT_REGION,\n",
        "        dedicated_endpoint_enabled=True,\n",
        "    )\n",
        "    print(\n",
        "        f\"Deploying {model.name} on {machine_type} with\"\n",
        "        f\" {accelerator_count} {accelerator_type} GPU(s).\"\n",
        "    )\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        deploy_request_timeout=1800,\n",
        "        disable_container_logging=True,\n",
        "        fast_tryout_enabled=True,\n",
        "        system_labels={\n",
        "            \"DEPLOY_SOURCE\": \"notebook\",\n",
        "            \"NOTEBOOK_NAME\": \"model_garden_pytorch_llama3_1_agent_engine.ipynb\",\n",
        "        },\n",
        "    )\n",
        "    print(\"endpoint_name:\", endpoint.name)\n",
        "    return model, endpoint\n",
        "\n",
        "\n",
        "# @markdown The Llama 3.1 8B Instruct model will be deployed to a dedicated endpoint on an `a2-ultragpu-1g` machine with Fast Deployment.\n",
        "# @markdown **Currently, the Fast Deployment is only supported in the `us-central1` region.**\n",
        "\n",
        "use_dedicated_endpoint = True  # Fast Deployment only supports dedicated endpoints.\n",
        "models[\"vllm_fast\"], endpoints[\"vllm_fast\"] = fast_deploy(\n",
        "    \"meta\", \"llama3_1\", \"llama-3.1-8b-instruct\"\n",
        ")\n",
        "ENDPOINT_RESOURCE_NAME = endpoints[\"vllm_fast\"].resource_name\n",
        "BASE_URL = (\n",
        "    f\"https://{REGION}-aiplatform.googleapis.com/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
        ")\n",
        "try:\n",
        "    if use_dedicated_endpoint:\n",
        "        DEDICATED_ENDPOINT_DNS = endpoints[\n",
        "            \"vllm_fast\"\n",
        "        ].gca_resource.dedicated_endpoint_dns\n",
        "        BASE_URL = f\"https://{DEDICATED_ENDPOINT_DNS}/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
        "except NameError:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "E8OiHHNNE_wj"
      },
      "outputs": [],
      "source": [
        "# @title Deploy prebuilt Llama 3.1 8B, 70B and 405B Instruct with standard vLLM\n",
        "\n",
        "# @markdown This section uploads prebuilt Llama 3.1 models to Model Registry and deploys it to a Vertex AI Endpoint. It takes 15 minutes to 1 hour to finish depending on the size of the model.\n",
        "\n",
        "# @markdown NVIDIA_L4 GPUs are used for demonstration. The serving efficiency of L4 GPUs is inferior to that of H100 GPUs, but L4 GPUs are nevertheless good serving solutions if you do not have H100 quota.\n",
        "\n",
        "# @markdown H100 is hard to get for now. It's recommended to use the deployment button in the model card. You can still try to deploy H100 endpoint through the notebook, but there is a chance that resource is not available.\n",
        "\n",
        "# @markdown Set the model to deploy.\n",
        "\n",
        "base_model_name = \"Meta-Llama-3.1-8B-Instruct\"  # @param [\"Meta-Llama-3.1-8B-Instruct\", \"Meta-Llama-3.1-70B-Instruct\", \"Meta-Llama-3.1-405B-Instruct-FP8\"] {isTemplate:true}\n",
        "model_id = os.path.join(VERTEX_AI_MODEL_GARDEN_LLAMA_3_1, base_model_name)\n",
        "ENABLE_DYNAMIC_LORA = True  # @param {type:\"boolean\", isTemplate:true}\n",
        "hf_model_id = \"meta-llama/\" + base_model_name\n",
        "\n",
        "# The pre-built serving docker images.\n",
        "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20241210_0916_RC00\"\n",
        "\n",
        "# @markdown Set `use_dedicated_endpoint` to False if you don't want to use [dedicated endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment#create-dedicated-endpoint).\n",
        "use_dedicated_endpoint = True  # @param {type:\"boolean\"}\n",
        "# @markdown Find Vertex AI prediction supported accelerators and regions at https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
        "if \"8b\" in base_model_name.lower():\n",
        "    accelerator_type = \"NVIDIA_L4\"\n",
        "    machine_type = \"g2-standard-12\"\n",
        "    accelerator_count = 1\n",
        "    max_loras = 5\n",
        "elif \"70b\" in base_model_name.lower():\n",
        "    accelerator_type = \"NVIDIA_H100_80GB\"\n",
        "    machine_type = \"a3-highgpu-4g\"\n",
        "    accelerator_count = 4\n",
        "    max_loras = 1\n",
        "elif \"405b\" in base_model_name.lower():\n",
        "    accelerator_type = \"NVIDIA_H100_80GB\"\n",
        "    machine_type = \"a3-highgpu-8g\"\n",
        "    accelerator_count = 8\n",
        "    max_loras = 1\n",
        "else:\n",
        "    raise ValueError(\n",
        "        f\"Recommended GPU setting not found for: {accelerator_type} and {base_model_name}.\"\n",
        "    )\n",
        "\n",
        "common_util.check_quota(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    is_for_training=False,\n",
        ")\n",
        "\n",
        "gpu_memory_utilization = 0.95\n",
        "max_model_len = 8192  # Maximum context length.\n",
        "\n",
        "\n",
        "# Enable automatic prefix caching using GPU HBM\n",
        "enable_prefix_cache = True\n",
        "# Setting this value >0 will use the idle host memory for a second-tier prefix kv\n",
        "# cache beneath the HBM cache. It only has effect if enable_prefix_cache=True.\n",
        "# The range of this value: [0, 1)\n",
        "# Setting host_prefix_kv_cache_utilization_target to 0 will disable the host memory prefix kv cache.\n",
        "host_prefix_kv_cache_utilization_target = 0.7\n",
        "\n",
        "\n",
        "def deploy_model_vllm(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    publisher: str,\n",
        "    publisher_model_id: str,\n",
        "    service_account: str,\n",
        "    base_model_id: str = None,\n",
        "    machine_type: str = \"g2-standard-8\",\n",
        "    accelerator_type: str = \"NVIDIA_L4\",\n",
        "    accelerator_count: int = 1,\n",
        "    gpu_memory_utilization: float = 0.9,\n",
        "    max_model_len: int = 4096,\n",
        "    dtype: str = \"auto\",\n",
        "    enable_trust_remote_code: bool = False,\n",
        "    enforce_eager: bool = False,\n",
        "    enable_lora: bool = False,\n",
        "    enable_chunked_prefill: bool = False,\n",
        "    enable_prefix_cache: bool = False,\n",
        "    host_prefix_kv_cache_utilization_target: float = 0.0,\n",
        "    max_loras: int = 1,\n",
        "    max_cpu_loras: int = 8,\n",
        "    use_dedicated_endpoint: bool = False,\n",
        "    max_num_seqs: int = 256,\n",
        "    model_type: str = None,\n",
        "    enable_llama_tool_parser: bool = False,\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(\n",
        "        display_name=f\"{model_name}-endpoint\",\n",
        "        dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
        "    )\n",
        "\n",
        "    if not base_model_id:\n",
        "        base_model_id = model_id\n",
        "\n",
        "    # See https://docs.vllm.ai/en/latest/models/engine_args.html for a list of possible arguments with descriptions.\n",
        "    vllm_args = [\n",
        "        \"python\",\n",
        "        \"-m\",\n",
        "        \"vllm.entrypoints.api_server\",\n",
        "        \"--host=0.0.0.0\",\n",
        "        \"--port=8080\",\n",
        "        f\"--model={model_id}\",\n",
        "        f\"--tensor-parallel-size={accelerator_count}\",\n",
        "        \"--swap-space=16\",\n",
        "        f\"--max-model-len={max_model_len}\",\n",
        "        f\"--dtype={dtype}\",\n",
        "        f\"--max-loras={max_loras}\",\n",
        "        f\"--max-cpu-loras={max_cpu_loras}\",\n",
        "        f\"--max-num-seqs={max_num_seqs}\",\n",
        "        \"--disable-log-stats\",\n",
        "    ]\n",
        "\n",
        "    if gpu_memory_utilization:\n",
        "        vllm_args.append(f\"--gpu-memory-utilization={gpu_memory_utilization}\")\n",
        "\n",
        "    if enable_trust_remote_code:\n",
        "        vllm_args.append(\"--trust-remote-code\")\n",
        "\n",
        "    if enforce_eager:\n",
        "        vllm_args.append(\"--enforce-eager\")\n",
        "\n",
        "    if enable_lora:\n",
        "        vllm_args.append(\"--enable-lora\")\n",
        "\n",
        "    if enable_chunked_prefill:\n",
        "        vllm_args.append(\"--enable-chunked-prefill\")\n",
        "\n",
        "    if enable_prefix_cache:\n",
        "        vllm_args.append(\"--enable-prefix-caching\")\n",
        "\n",
        "    if 0 < host_prefix_kv_cache_utilization_target < 1:\n",
        "        vllm_args.append(\n",
        "            f\"--host-prefix-kv-cache-utilization-target={host_prefix_kv_cache_utilization_target}\"\n",
        "        )\n",
        "\n",
        "    if model_type:\n",
        "        vllm_args.append(f\"--model-type={model_type}\")\n",
        "\n",
        "    if enable_llama_tool_parser:\n",
        "        vllm_args.append(\"--enable-auto-tool-choice\")\n",
        "        vllm_args.append(\"--tool-call-parser=vertex-llama-3\")\n",
        "\n",
        "    env_vars = {\n",
        "        \"MODEL_ID\": base_model_id,\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "    }\n",
        "\n",
        "    # HF_TOKEN is not a compulsory field and may not be defined.\n",
        "    try:\n",
        "        if HF_TOKEN:\n",
        "            env_vars[\"HF_TOKEN\"] = HF_TOKEN\n",
        "    except NameError:\n",
        "        pass\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
        "        serving_container_args=vllm_args,\n",
        "        serving_container_ports=[8080],\n",
        "        serving_container_predict_route=\"/generate\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "        serving_container_environment_variables=env_vars,\n",
        "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
        "        serving_container_deployment_timeout=7200,\n",
        "        model_garden_source_model_name=(\n",
        "            f\"publishers/{publisher}/models/{publisher_model_id}\"\n",
        "        ),\n",
        "    )\n",
        "    print(\n",
        "        f\"Deploying {model_name} on {machine_type} with {accelerator_count} {accelerator_type} GPU(s).\"\n",
        "    )\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=service_account,\n",
        "        system_labels={\n",
        "            \"NOTEBOOK_NAME\": \"model_garden_pytorch_llama3_1_agent_engine.ipynb\",\n",
        "            \"NOTEBOOK_ENVIRONMENT\": common_util.get_deploy_source(),\n",
        "        },\n",
        "    )\n",
        "    print(\"endpoint_name:\", endpoint.name)\n",
        "\n",
        "    return model, endpoint\n",
        "\n",
        "\n",
        "models[\"vllm_gpu\"], endpoints[\"vllm_gpu\"] = deploy_model_vllm(\n",
        "    model_name=common_util.get_job_name_with_datetime(prefix=\"llama3_1-serve\"),\n",
        "    model_id=model_id,\n",
        "    publisher=\"meta\",\n",
        "    publisher_model_id=\"llama3_1\",\n",
        "    base_model_id=hf_model_id,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    gpu_memory_utilization=gpu_memory_utilization,\n",
        "    max_model_len=max_model_len,\n",
        "    max_loras=max_loras,\n",
        "    enforce_eager=True,\n",
        "    enable_lora=ENABLE_DYNAMIC_LORA,\n",
        "    enable_chunked_prefill=not ENABLE_DYNAMIC_LORA,\n",
        "    enable_prefix_cache=enable_prefix_cache,\n",
        "    host_prefix_kv_cache_utilization_target=host_prefix_kv_cache_utilization_target,\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        ")\n",
        "\n",
        "if use_dedicated_endpoint:\n",
        "    DEDICATED_ENDPOINT_DNS = endpoints[\"vllm_gpu\"].gca_resource.dedicated_endpoint_dns\n",
        "ENDPOINT_RESOURCE_NAME = \"projects/{}/locations/{}/endpoints/{}\".format(\n",
        "    PROJECT_ID, REGION, endpoints[\"vllm_gpu\"].name\n",
        ")\n",
        "\n",
        "BASE_URL = (\n",
        "    f\"https://{REGION}-aiplatform.googleapis.com/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
        ")\n",
        "try:\n",
        "    if use_dedicated_endpoint:\n",
        "        BASE_URL = f\"https://{DEDICATED_ENDPOINT_DNS}/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5DJwoySkz1ub"
      },
      "outputs": [],
      "source": [
        "# @title Deploy prebuilt Llama 3.1 8B instruct and 70B instruct with optimized vLLM\n",
        "\n",
        "# @markdown This section uploads prebuilt Llama 3.1 models to Model Registry and deploys it to a Vertex AI Endpoint. It takes 15 minutes to 1 hour to finish depending on the size of the model.\n",
        "\n",
        "# @markdown NVIDIA_L4 GPUs are used for demonstration. The serving efficiency of L4 GPUs is inferior to that of H100 GPUs, but L4 GPUs are nevertheless good serving solutions if you do not have H100 quota.\n",
        "\n",
        "# @markdown H100 is hard to get for now. It's recommended to use the deployment button in the model card. You can still try to deploy H100 endpoint through the notebook, but there is a chance that resource is not available.\n",
        "\n",
        "# @markdown Set the model to deploy.\n",
        "\n",
        "base_model_name = \"Meta-Llama-3.1-8B-Instruct\"  # @param [\"Meta-Llama-3.1-8B-Instruct\", \"Meta-Llama-3.1-70B-Instruct\"] {isTemplate:true}\n",
        "model_id = os.path.join(VERTEX_AI_MODEL_GARDEN_LLAMA_3_1, base_model_name)\n",
        "hf_model_id = \"meta-llama/\" + base_model_name\n",
        "\n",
        "# The pre-built serving docker images.\n",
        "OPTIMIZED_VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai-restricted/vertex-vision-model-garden-dockers/pytorch-vllm-optimized-serve:20241029_0835_RC00\"\n",
        "\n",
        "# @markdown Find Vertex AI prediction supported accelerators and regions at https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
        "if \"8b\" in base_model_name.lower():\n",
        "    accelerator_type = \"NVIDIA_L4\"\n",
        "    machine_type = \"g2-standard-12\"\n",
        "    accelerator_count = 1\n",
        "elif \"70b\" in base_model_name.lower():\n",
        "    accelerator_type = \"NVIDIA_H100_80GB\"\n",
        "    machine_type = \"a3-highgpu-8g\"\n",
        "    accelerator_count = 8\n",
        "else:\n",
        "    raise ValueError(\n",
        "        f\"Recommended GPU setting not found for: {accelerator_type} and {base_model_name}.\"\n",
        "    )\n",
        "\n",
        "common_util.check_quota(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    is_for_training=False,\n",
        ")\n",
        "\n",
        "max_model_len = 8192  # Maximum context length.\n",
        "\n",
        "\n",
        "def deploy_model_optimized_vllm(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    publisher: str,\n",
        "    publisher_model_id: str,\n",
        "    service_account: str,\n",
        "    base_model_id: str = None,\n",
        "    machine_type: str = \"g2-standard-12\",\n",
        "    accelerator_type: str = \"NVIDIA_L4\",\n",
        "    accelerator_count: int = 1,\n",
        "    max_model_len: int = 4096,\n",
        "    enable_trust_remote_code: bool = False,\n",
        "    use_dedicated_endpoint: bool = False,\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys models with optimized vLLM into Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(\n",
        "        display_name=f\"{model_name}-endpoint\",\n",
        "        dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
        "    )\n",
        "\n",
        "    if not base_model_id:\n",
        "        base_model_id = model_id\n",
        "\n",
        "    vllm_args = [\n",
        "        \"python\",\n",
        "        \"-m\",\n",
        "        \"vllm.entrypoints.api_server\",\n",
        "        \"--host=0.0.0.0\",\n",
        "        \"--port=8080\",\n",
        "        f\"--model={model_id}\",\n",
        "        f\"--tensor-parallel-size={accelerator_count}\",\n",
        "        \"--swap-space=16\",\n",
        "        f\"--max-model-len={max_model_len}\",\n",
        "        \"--disable-log-stats\",\n",
        "    ]\n",
        "\n",
        "    if enable_trust_remote_code:\n",
        "        vllm_args.append(\"--trust-remote-code\")\n",
        "\n",
        "    env_vars = {\n",
        "        \"MODEL_ID\": base_model_id,\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "    }\n",
        "\n",
        "    # HF_TOKEN is not a compulsory field and may not be defined.\n",
        "    try:\n",
        "        if HF_TOKEN:\n",
        "            env_vars[\"HF_TOKEN\"] = HF_TOKEN\n",
        "    except NameError:\n",
        "        pass\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=OPTIMIZED_VLLM_DOCKER_URI,\n",
        "        serving_container_args=vllm_args,\n",
        "        serving_container_ports=[8080],\n",
        "        serving_container_predict_route=\"/generate\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "        serving_container_environment_variables=env_vars,\n",
        "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
        "        serving_container_deployment_timeout=7200,\n",
        "        model_garden_source_model_name=(\n",
        "            f\"publishers/{publisher}/models/{publisher_model_id}\"\n",
        "        ),\n",
        "    )\n",
        "    print(\n",
        "        f\"Deploying {model_name} on {machine_type} with {accelerator_count} {accelerator_type} GPU(s).\"\n",
        "    )\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=service_account,\n",
        "        system_labels={\n",
        "            \"NOTEBOOK_NAME\": \"model_garden_pytorch_llama3_1_agent_engine.ipynb\",\n",
        "        },\n",
        "    )\n",
        "    print(\"endpoint_name:\", endpoint.name)\n",
        "\n",
        "    return model, endpoint\n",
        "\n",
        "\n",
        "(\n",
        "    models[\"optimized_vllm_gpu\"],\n",
        "    endpoints[\"optimized_vllm_gpu\"],\n",
        ") = deploy_model_optimized_vllm(\n",
        "    model_name=common_util.get_job_name_with_datetime(prefix=\"llama3_1-serve\"),\n",
        "    model_id=model_id,\n",
        "    publisher=\"meta\",\n",
        "    publisher_model_id=\"llama3_1\",\n",
        "    base_model_id=hf_model_id,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    max_model_len=max_model_len,\n",
        "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
        ")\n",
        "\n",
        "if use_dedicated_endpoint:\n",
        "    DEDICATED_ENDPOINT_DNS = endpoints[\"vllm_gpu\"].gca_resource.dedicated_endpoint_dns\n",
        "ENDPOINT_RESOURCE_NAME = \"projects/{}/locations/{}/endpoints/{}\".format(\n",
        "    PROJECT_ID, REGION, endpoints[\"vllm_gpu\"].name\n",
        ")\n",
        "\n",
        "BASE_URL = (\n",
        "    f\"https://{REGION}-aiplatform.googleapis.com/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
        ")\n",
        "try:\n",
        "    if use_dedicated_endpoint:\n",
        "        BASE_URL = f\"https://{DEDICATED_ENDPOINT_DNS}/v1beta1/{ENDPOINT_RESOURCE_NAME}\"\n",
        "except NameError:\n",
        "    pass\n",
        "# @markdown Click \"Show Code\" to see more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blbSoA-3gSyN"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KsF63jInoByL"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "B19rfdsboByL"
      },
      "outputs": [],
      "source": [
        "# @title Set cloud storage\n",
        "\n",
        "# @markdown To get started using Vertex AI, you must enable the Vertex AI API(https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com) and create a bucket created for agent engine.\n",
        "\n",
        "BUCKET_NAME = \"\"  # @param {type:\"string\", placeholder: \"[your-bucket-name]\"}\n",
        "STAGING_BUCKET = f\"gs://{BUCKET_NAME}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SLq7whlDoByL"
      },
      "outputs": [],
      "source": [
        "# @title Initialize Vertex AI SDK for Python\n",
        "\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wL1jYls9oByL"
      },
      "outputs": [],
      "source": [
        "# @title Import libraries\n",
        "\n",
        "# Import libraries to use in this tutorial.\n",
        "\n",
        "import google.auth\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from vertexai import agent_engines\n",
        "from vertexai.preview import reasoning_engines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71WcCJ57gJZM"
      },
      "source": [
        "### Chat with `Agent Engine`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sgG-NyAvoByL"
      },
      "outputs": [],
      "source": [
        "# @title `Agent Engine` use Llama 3.1 with different configuration\n",
        "\n",
        "# @markdown To use the self-deployed API endpoint with Agent Engine capabilities, you need to request the access token and configure the langchain ChatOpenAI to point to the API endpoint.\n",
        "\n",
        "# @markdown In previous [notebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_openai_api_llama3_1.ipynb), we demonstrated how to `Ask Llama 3.1 using different model configuration`.\n",
        "\n",
        "# @markdown In this colab, we will show you how to use the `Agent Engine` to send a request to the Llama 3.1 model with different model configuration.\n",
        "\n",
        "\n",
        "def model_builder(\n",
        "    *,\n",
        "    model_name: str,\n",
        "    model_kwargs=None,\n",
        "    project: str,  # Specified via vertexai.init\n",
        "    location: str,  # Specified via vertexai.init\n",
        "    **kwargs,\n",
        "):\n",
        "\n",
        "    # Note: the credential lives for 1 hour by default.\n",
        "    # After expiration, it must be refreshed.\n",
        "    creds, _ = google.auth.default(\n",
        "        scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n",
        "    )\n",
        "    auth_req = google.auth.transport.requests.Request()\n",
        "    creds.refresh(auth_req)\n",
        "\n",
        "    if model_kwargs is None:\n",
        "        model_kwargs = {}\n",
        "\n",
        "    return ChatOpenAI(\n",
        "        model=\"\",\n",
        "        base_url=BASE_URL,\n",
        "        api_key=creds.token,\n",
        "        **model_kwargs,\n",
        "    )\n",
        "\n",
        "\n",
        "# @markdown Use the following parameters to generate different answers:\n",
        "# @markdown *   `temperature` to control the randomness of the response\n",
        "# @markdown *   `top_p` to control the quality of the response\n",
        "\n",
        "temperature = 1.0  # @param {type:\"number\"}\n",
        "top_p = 1.0  # @param {type:\"number\"}\n",
        "\n",
        "agent = reasoning_engines.LangchainAgent(\n",
        "    model=\"\",  # Required.\n",
        "    model_builder=model_builder,  # Required.\n",
        "    model_kwargs={\n",
        "        \"temperature\": temperature,  # Optional.\n",
        "        \"top_p\": top_p,  # Optional.\n",
        "        \"extra_body\": {},\n",
        "    },\n",
        ")\n",
        "\n",
        "# @markdown Now we can test the model and agent behavior to ensure that it's working as expected before we deploy it:\n",
        "\n",
        "response = agent.query(input=\"Hello, Llama 3.1!\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rn-cHx7xoByL"
      },
      "outputs": [],
      "source": [
        "# @title Deploy your agent on Vertex AI\n",
        "\n",
        "# @markdown Now that you've specified a model, and reasoning for your agent and tested it out, you're ready to deploy your agent as a remote service in Vertex AI!\n",
        "\n",
        "remote_agent = agent_engines.create(\n",
        "    agent,\n",
        "    requirements=[\n",
        "        \"google-cloud-aiplatform[langchain,agent_engines]\",\n",
        "        \"cloudpickle==3.0.0\",\n",
        "        \"pydantic==2.10.6\",\n",
        "        \"requests\",\n",
        "        \"langchain-openai\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "response = remote_agent.query(input=\"Hello, Llama 3.1!\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MjwPLr0LoByL"
      },
      "outputs": [],
      "source": [
        "# @title Reusing your deployed agent from other applications or SDKs\n",
        "\n",
        "# @markdown The remotely deployed `Agent Engine` is now available for import and use. You can access it within your current notebook session, a different notebook, or a Python script.\n",
        "\n",
        "AGENT_ENGINE_RESOURCE_NAME = remote_agent.resource_name\n",
        "print(AGENT_ENGINE_RESOURCE_NAME)\n",
        "\n",
        "# Afterwards, you can use the below code:\n",
        "\n",
        "# from vertexai.preview import agent_engines`\n",
        "\n",
        "# remote_agent = agent_engines.get(AGENT_ENGINE_RESOURCE_NAME)`\n",
        "# response = remote_agent.query(input=query)`\n",
        "\n",
        "# @markdown Alternatively, you can query your agent from other programming languages using any of the [available client libraries in Vertex AI](https://cloud.google.com/vertex-ai/docs/start/client-libraries), including C#, Java, Node.js, Python, Go, or REST API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ato0F6XFoByL"
      },
      "source": [
        "### Simple Translator Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bI3JqaeMoByL"
      },
      "outputs": [],
      "source": [
        "# @title Use Agent Engine to build a simple translator agent\n",
        "\n",
        "# @markdown In previous [notebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_openai_api_llama3_1.ipynb), we demonstrates how to use `LangChain Expression Language` (LCEL) to build a simple chain which translates some `text_to_translate` to the specified `target_language`.\n",
        "\n",
        "# @markdown In this colab, we will show you how to use the `Agent Engine` to build and deploy the agent.\n",
        "\n",
        "\n",
        "def lcel_builder(*, model, **kwargs):\n",
        "\n",
        "    template = \"\"\"Translate the following {text} to {target_language}:\"\"\"\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"text\", \"target_language\"], template=template\n",
        "    )\n",
        "\n",
        "    return prompt | model | StrOutputParser()\n",
        "\n",
        "\n",
        "agent = reasoning_engines.LangchainAgent(\n",
        "    model=\"\",\n",
        "    model_builder=model_builder,\n",
        "    runnable_builder=lcel_builder,\n",
        ")\n",
        "\n",
        "text_to_translate = \"\"  # @param {type:\"string\", placeholder:\"Hello Llama 3.1!\"}\n",
        "target_language = \"\"  # @param {type:\"string\", placeholder:\"Italian\"}\n",
        "\n",
        "response = agent.query(\n",
        "    input={\"text\": text_to_translate, \"target_language\": target_language}\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XNF9slsSEHLz"
      },
      "outputs": [],
      "source": [
        "# @title Deploy your agent on Vertex AI\n",
        "\n",
        "# @markdown Now that you've specified a model, and reasoning for your agent and tested it out, you're ready to deploy your agent as a remote service in Vertex AI!\n",
        "\n",
        "remote_agent = agent_engines.create(\n",
        "    agent,\n",
        "    requirements=[\n",
        "        \"google-cloud-aiplatform[langchain,agent_engines]\",\n",
        "        \"cloudpickle==3.0.0\",\n",
        "        \"pydantic==2.10.6\",\n",
        "        \"requests\",\n",
        "        \"langchain-openai\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "response = remote_agent.query(\n",
        "    input={\"text\": text_to_translate, \"target_language\": target_language}\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tnbhCYwbERSx"
      },
      "outputs": [],
      "source": [
        "# @title Reusing your deployed agent from other applications or SDKs\n",
        "\n",
        "# @markdown The remotely deployed `Agent Engine` is now available for import and use. You can access it within your current notebook session, a different notebook, or a Python script.\n",
        "\n",
        "AGENT_ENGINE_RESOURCE_NAME = remote_agent.resource_name\n",
        "print(AGENT_ENGINE_RESOURCE_NAME)\n",
        "\n",
        "# Afterwards, you can use the below code:\n",
        "\n",
        "# from vertexai.preview import agent_engines`\n",
        "\n",
        "# remote_agent = agent_engines.get(AGENT_ENGINE_RESOURCE_NAME)`\n",
        "# response = remote_agent.query(input=query)`\n",
        "\n",
        "# @markdown Alternatively, you can query your agent from other programming languages using any of the [available client libraries in Vertex AI](https://cloud.google.com/vertex-ai/docs/start/client-libraries), including C#, Java, Node.js, Python, Go, or REST API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lI86Eu7oByL"
      },
      "source": [
        "### Exchange Rate Tool\n",
        "\n",
        "[Function calling](https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/function-calling) lets developers create a description of a function in their code, then pass that description to a language model in a request. The response from the model includes the name of a function that matches the description and the arguments to call it with.\n",
        "\n",
        "In this example, we will use an Exchange Rate tool in the Agent Engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Dmv42OOOoByL"
      },
      "outputs": [],
      "source": [
        "# @title Agent that uses an Exchange Rate Tool\n",
        "\n",
        "# @markdown Tools and functions enable the generative model to interact with external systems, databases, document stores, and other APIs so that the model can get the most up-to-date information or take action with those systems.\n",
        "\n",
        "# @markdown In this example, you'll define a function called get_exchange_rate that uses the requests library to retrieve real-time currency exchange information from an API:\n",
        "\n",
        "\n",
        "def get_exchange_rate(\n",
        "    currency_from: str = \"USD\",\n",
        "    currency_to: str = \"EUR\",\n",
        "    currency_date: str = \"latest\",\n",
        "):\n",
        "    \"\"\"Retrieves the exchange rate between two currencies on a specified date.\n",
        "    Args:\n",
        "        currency_from: The source currency code.\n",
        "        currency_to: The target currency code.\n",
        "        currency_date: The date to retrieve the exchange rate.\n",
        "    Returns:\n",
        "        Exchange rate between two currencies on a specified date.\n",
        "    \"\"\"\n",
        "    response = requests.get(\n",
        "        f\"https://api.frankfurter.app/{currency_date}\",\n",
        "        params={\"from\": currency_from, \"to\": currency_to},\n",
        "    )\n",
        "    return response.json()\n",
        "\n",
        "\n",
        "get_exchange_rate(currency_from=\"USD\", currency_to=\"SEK\")\n",
        "\n",
        "\n",
        "agent = reasoning_engines.LangchainAgent(\n",
        "    model=\"\",  # Required.\n",
        "    model_builder=model_builder,  # Required.\n",
        "    tools=[get_exchange_rate],  # Optional.\n",
        "    agent_executor_kwargs={\n",
        "        \"return_intermediate_steps\": True,\n",
        "        \"stream_runnable\": False,\n",
        "    },  # Optional.\n",
        ")\n",
        "\n",
        "# @markdown Test the function with sample inputs to ensure that it's working as expected:\n",
        "response = agent.query(\n",
        "    input=\"What's the exchange rate from US dollars to Swedish currency at 2024-07-26?\"\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6y2G_bjbDam_"
      },
      "outputs": [],
      "source": [
        "# @title Deploy your agent on Vertex AI\n",
        "\n",
        "# @markdown Now that you've specified a model, and agent for your agent and tested it out, you're ready to deploy your agent as a remote service in Vertex AI!\n",
        "\n",
        "remote_agent = agent_engines.create(\n",
        "    agent,\n",
        "    requirements=[\n",
        "        \"google-cloud-aiplatform[langchain,agent_engines]\",\n",
        "        \"cloudpickle==3.0.0\",\n",
        "        \"pydantic==2.10.6\",\n",
        "        \"requests\",\n",
        "        \"langchain-openai\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "response = remote_agent.query(\n",
        "    input=\"What's the exchange rate from US dollars to Swedish currency at 2024-07-26?\"\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oEbI1hm1KoQE"
      },
      "outputs": [],
      "source": [
        "# @title Reusing your deployed agent from other applications or SDKs\n",
        "\n",
        "# @markdown The remotely deployed `Agent Engine` is now available for import and use. You can access it within your current notebook session, a different notebook, or a Python script.\n",
        "\n",
        "AGENT_ENGINE_RESOURCE_NAME = remote_agent.resource_name\n",
        "print(AGENT_ENGINE_RESOURCE_NAME)\n",
        "\n",
        "# Afterwards, you can use the below code:\n",
        "\n",
        "# from vertexai.preview import agent_engines`\n",
        "\n",
        "# remote_agent = agent_engines.get(AGENT_ENGINE_RESOURCE_NAME)`\n",
        "# response = remote_agent.query(input=query)`\n",
        "\n",
        "# @markdown Alternatively, you can query your agent from other programming languages using any of the [available client libraries in Vertex AI](https://cloud.google.com/vertex-ai/docs/start/client-libraries), including C#, Java, Node.js, Python, Go, or REST API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JETd33jIDcjm"
      },
      "source": [
        "## Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "911406c1561e"
      },
      "outputs": [],
      "source": [
        "# @title Delete the models, endpoints, buckets and agent engines\n",
        "\n",
        "# @markdown  Delete the experiment models and endpoints to recycle the resources\n",
        "# @markdown  and avoid unnecessary continuous charges that may incur.\n",
        "\n",
        "# Undeploy model and delete endpoint.\n",
        "for endpoint in endpoints.values():\n",
        "    endpoint.delete(force=True)\n",
        "\n",
        "# Delete models.\n",
        "for model in models.values():\n",
        "    model.delete()\n",
        "\n",
        "delete_bucket = False  # @param {type:\"boolean\"}\n",
        "if delete_bucket:\n",
        "    ! gsutil -m rm -r $BUCKET_NAME\n",
        "\n",
        "delete_agent_engine = False  # @param {type:\"boolean\"}\n",
        "\n",
        "if delete_agent_engine:\n",
        "    remote_agent.delete()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_pytorch_llama3_1_agent_engine.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJc36RtD90jd"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9EezHSo90jf"
      },
      "source": [
        "# Vertex AI Model Garden - Mistral-7B (PEFT)\n",
        "\n",
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_mistral_peft_tuning.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_mistral_peft_tuning.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/model_garden/model_garden_pytorch_mistral_peft_tuning.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>                                                                                               \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybMCVFh0_5R8"
      },
      "source": [
        "## Overview\n",
        "In this notebook you will learn how to fine tune Mistral-7B with QLoRa.\n",
        "\n",
        "### Objective\n",
        "\n",
        "*   **Step 1** Load quantized Mistral-7B model with bnb and run Local inference\n",
        "*   **Step 2** Fine tune Mistral-7B model with PEFT\n",
        "    -   Option 1: Finetune and merge Mistral-7B model with peft train docker image (maintained by Vertex AI Model Garden). Optionally run Hyperparameter tuning to find the best parameters.\n",
        "    -   Option 2: Manually fine-tune Mistral-7B with bnb, peft and SFTTrainer. Merge the LoRA weights with the base Mistral-7B model with peft train.\n",
        "*   **Step 3** Deploy the finetuned model with vLLM docker image on a Vertex AI Endpoint\n",
        "*   **Step 4** Run inference to evaluate the finetuned model and compare with initial local inference with the based model\n",
        "    -   Option 1: Run inference with Merged model\n",
        "    -   Option 2: Run inference with Adapter model\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoTMt4AXgYhr"
      },
      "source": [
        "# Step 0 - Initiatialise the notebook\n",
        "## Define some helper functions and variables:\n",
        "\n",
        "0. Define some variables and APIs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-OjzhpyMHsu"
      },
      "outputs": [],
      "source": [
        "# Cloud project id.\n",
        "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# The region you want to launch jobs in.\n",
        "REGION = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# The Cloud Storage bucket for storing experiments output.\n",
        "# Start with gs:// prefix, e.g. gs://foo_bucket.\n",
        "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
        "\n",
        "! gcloud config set project $PROJECT_ID\n",
        "! gcloud services enable language.googleapis.com\n",
        "\n",
        "import os\n",
        "\n",
        "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
        "EXPERIMENT_BUCKET = os.path.join(BUCKET_URI, \"peft\")\n",
        "DATA_BUCKET = os.path.join(EXPERIMENT_BUCKET, \"data\")\n",
        "MODEL_BUCKET = os.path.join(EXPERIMENT_BUCKET, \"model\")\n",
        "\n",
        "# The service account looks like:\n",
        "# '@.iam.gserviceaccount.com'\n",
        "# Please go to https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console\n",
        "# and create service account with `Vertex AI User` and `Storage Object Admin` roles.\n",
        "# The service account for deploying fine tuned model.\n",
        "SERVICE_ACCOUNT = \"\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgNendbgjSJ6"
      },
      "source": [
        "## Installation : *Vertex AI API*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mj-SxEcNjQNN"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okocadG3huSB"
      },
      "source": [
        "## Define *constants*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsIcP7nqh1FL"
      },
      "outputs": [],
      "source": [
        "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240112_0916_RC00\"\n",
        "TRAIN_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-train:20240126_0936_RC00\"\n",
        "VLLM_GPTQ_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:gptq\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arrkM1d69rGK"
      },
      "source": [
        "## Define common functions\n",
        "1. Define a wrapper function which pass your query to the model for inference and return decoded model's completion(response)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVZbaVf69quq"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "\n",
        "def get_completion(query: str, model, tokenizer) -> str:\n",
        "    device = \"cuda:0\"\n",
        "\n",
        "    prompt_template = \"\"\"\n",
        "  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "  ### Question:\n",
        "  {query}\n",
        "\n",
        "  ### Answer:\n",
        "  \"\"\"\n",
        "    prompt = prompt_template.format(query=query)\n",
        "\n",
        "    encoded = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
        "\n",
        "    model_inputs = encoded.to(device)\n",
        "\n",
        "    generated_ids = model.generate(\n",
        "        **model_inputs,\n",
        "        max_new_tokens=250,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    decoded = tokenizer.batch_decode(generated_ids)\n",
        "    return decoded[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qoa8GXFBiJ6I"
      },
      "source": [
        "1. Define model deployment functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qxd5gsGhiKZ4"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "\n",
        "def create_name_with_datetime(prefix: str) -> str:\n",
        "    \"\"\"Creates a name with date time when triggering training or deployment\n",
        "    jobs in Vertex AI.\n",
        "    \"\"\"\n",
        "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
        "\n",
        "\n",
        "# Can add precision as parameter\n",
        "\n",
        "\n",
        "def deploy_model_vllm(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    service_account: str,\n",
        "    machine_type: str = \"n1-standard-8\",\n",
        "    accelerator_type: str = \"NVIDIA_TESLA_V100\",\n",
        "    accelerator_count: int = 1,\n",
        "    quantization_method: str = \"\",\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
        "\n",
        "    vllm_args = [\n",
        "        \"--host=0.0.0.0\",\n",
        "        \"--port=7080\",\n",
        "        f\"--model={model_id}\",\n",
        "        f\"--tensor-parallel-size={accelerator_count}\",\n",
        "        \"--swap-space=16\",\n",
        "        \"--gpu-memory-utilization=0.9\",\n",
        "        \"--max-num-batched-tokens=4096\",\n",
        "        \"--disable-log-stats\",\n",
        "    ]\n",
        "    if quantization_method:\n",
        "        vllm_args.append(f\"--quantization={quantization_method}\")\n",
        "    if quantization_method == \"gptq\":\n",
        "        vllm_docker_uri = VLLM_GPTQ_DOCKER_URI\n",
        "    else:\n",
        "        vllm_docker_uri = VLLM_DOCKER_URI\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=vllm_docker_uri,\n",
        "        serving_container_command=[\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
        "        serving_container_args=vllm_args,\n",
        "        serving_container_ports=[7080],\n",
        "        serving_container_predict_route=\"/generate\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "    )\n",
        "\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=service_account,\n",
        "    )\n",
        "    return model, endpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juj9DSmmh-Gw"
      },
      "source": [
        "## Install necessary packages\n",
        "First, install the dependencies below to get started. As these features are available on the main branches only, you need to install the libraries below from source."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nNWXXc7ol1n"
      },
      "outputs": [],
      "source": [
        "# Using BitsAndBytes Library for quantization\n",
        "!pip install -q -U bitsandbytes\n",
        "\n",
        "# Transformers provides all API for downloading and working with pre-trained models that are in the HF hub.\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "\n",
        "# This package provides all the APIs you will need to perform the LoRA technique.\n",
        "!pip install -q peft==0.6.2\n",
        "\n",
        "# Powerful Huggingface package, that hides the complexity of the developer trying to write/manage code needed to use multi-GPUs/TPU/fp16.\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "\n",
        "! pip3 install sentencepiece==0.1.99\n",
        "\n",
        "# This Huggingface package provides access to the various datasets in the Huggingface hub.\n",
        "!pip install -q datasets\n",
        "\n",
        "# This library provides access to the Weights and Biases library to capture various metrics, during the fine-tuning process.\n",
        "!pip install -q wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NVvNhohkvwF"
      },
      "source": [
        "# Step 1 - Load quantized Mistral-7B model with bnb and run local inference\n",
        "We'll load the model using QLoRA quantization to reduce the usage of memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvvLg99Opw5R"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import (AutoModelForCausalLM, AutoTokenizer,\n",
        "                          BitsAndBytesConfig)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,  # loading the base model in 4bit quantization. Also need to check model weights in config file.\n",
        "    bnb_4bit_use_double_quant=True,  # Double Quantization\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,  # Would use float16 with compute capabilities below 8 (T4, V100)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgqxSuxsBX3r"
      },
      "source": [
        "Now you specify the model ID and then you load it with your previously defined quantization configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7St-hFLNmS2v"
      },
      "outputs": [],
      "source": [
        "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "# Load Mistral-7B quantized with BitsAndBytesConfig defined above.\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id, quantization_config=bnb_config, device_map={\"\": 0}\n",
        ")\n",
        "\n",
        "# Define the tokenizer\n",
        "# Using AutoTokenizers for creating a tokenizer for Mistral-7B\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Omw10c2djdIw"
      },
      "source": [
        "Run a inference on the base model. The model does not seem to understand your instruction and gives us a list of questions related to your query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDkUkF2So2-7"
      },
      "outputs": [],
      "source": [
        "result = get_completion(query=\"What is Model Garden?\", model=model, tokenizer=tokenizer)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m06rH8cTrZof"
      },
      "source": [
        "# Step 2 - Fine tune Mistral-7B model with PEFT\n",
        "This section demonstrates how to finetune the Mistral-7b model, merge the finetuned LoRA adapter with the base model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-131f-kFncq6"
      },
      "source": [
        "Set the base model id."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89NX4-1jniGJ"
      },
      "outputs": [],
      "source": [
        "base_model_id = \"mistralai/Mistral-7B-v0.1\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBI4xnMEon8u"
      },
      "source": [
        "## **Option 1** Finetune and merge Mistral-7B model with peft train docker image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIdjRx6tnyZz"
      },
      "source": [
        "### Finetune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paBvlvREn014"
      },
      "source": [
        "Use the Vertex AI SDK to create and run the custom training jobs with Vertex AI Model Garden training images.\n",
        "\n",
        "This example uses the dataset fredmo/vertexai-qna-500 , a small dataset containing questions and answers about GCP Vertex AI Documentation. You can either use a dataset from huggingface or a custom JSONL dataset in Vertex AI text model dataset format stored in Cloud Storage. The template parameter is optional.\n",
        "\n",
        "In order to make the finetuning efficient, you enabled quantization for loading pretrained models for finetuning LoRA models. Precision options include \"4bit\", \"8bit\", \"float16\" (default) and \"float32\", and the precision can be set via \"--precision_mode\".\n",
        "\n",
        "In this section, the finetuned LoRA adapter will be saved to a GCS bucket specified by the variable lora_adapter_dir below; and you merge the LoRa adapter with the base model, and save it to a separate GCS bucket specified by merged_model_output_dir below.\n",
        "\n",
        "### Finetune with a custom dataset\n",
        "\n",
        "To use a custom dataset, you should supply a gs:// URI to a JSONL file in Vertex text model dataset format in the dataset_name below.\n",
        "\n",
        "For example, you can download the template file here https://github.com/thomaslemoullec/QLora_vLLM_Mistral7B/blob/main/vertexAI_q%26a_template.json and upload it to your bucket, then reference the gs:// URI.\n",
        "\n",
        "{\"input_text\":\"TRANSCRIPT: \\nREASON FOR EVALUATION:,\\n\\n LABEL:\",\"output_text\":\"Chiropractic\"}\n",
        "\n",
        "To use this sample dataset that contains input_text and output_text fields, set dataset_name to gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl and template to vertex_sample. For advanced usage with custom datatset fields, see the template example and supply your own JSON template as gs:// URIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7GUQgq5pkF0"
      },
      "outputs": [],
      "source": [
        "# Huggingface dataset name or gs:// URI to a custom JSONL dataset.\n",
        "dataset_name = \"fredmo/vertexai-qna-500\"  # @param {type:\"string\"}\n",
        "# Optional. Template name or gs:// URI to a custom template.\n",
        "template = \"vertex_sample\"  # @param {type:\"string\"}\n",
        "# Runs 10 training steps as a minimal example.\n",
        "max_steps = 10  # @param {type:\"integer\"}\n",
        "\n",
        "finetuning_precision_mode = \"float16\"  # @param {type:\"string\"}\n",
        "\n",
        "# Worker pool spec.\n",
        "# Finetunes mistral-7B with 1 V100 (16G).\n",
        "machine_type = \"n1-highmem-16\"\n",
        "accelerator_type = \"NVIDIA_TESLA_V100\"\n",
        "accelerator_count = 2\n",
        "\n",
        "# Finetunes mistral-7B with 1 L4 (24G).\n",
        "# machine_type = \"g2-standard-8\"\n",
        "# accelerator_type = \"NVIDIA_L4\"\n",
        "# accelerator_count = 1\n",
        "\n",
        "replica_count = 1\n",
        "\n",
        "\n",
        "# Setup training job.\n",
        "job_name = create_name_with_datetime(\"mistral-lora-train\")\n",
        "train_job = aiplatform.CustomContainerTrainingJob(\n",
        "    display_name=job_name,\n",
        "    container_uri=TRAIN_DOCKER_URI,\n",
        ")\n",
        "\n",
        "# Create a GCS folder to store the LORA adapter.\n",
        "lora_adapter_dir = create_name_with_datetime(\"mistral-lora-adapter\")\n",
        "lora_output_dir = os.path.join(MODEL_BUCKET, lora_adapter_dir)\n",
        "\n",
        "# Create a GCS folder to store the merged model with the base model and the\n",
        "# finetuned LORA adapter.\n",
        "merged_model_dir = create_name_with_datetime(\"mistral-merged-model\")\n",
        "merged_model_output_dir = os.path.join(MODEL_BUCKET, merged_model_dir)\n",
        "\n",
        "# Pass training arguments and launch job.\n",
        "train_job.run(\n",
        "    args=[\n",
        "        \"--task=causal-language-modeling-lora\",\n",
        "        f\"--pretrained_model_id={base_model_id}\",\n",
        "        f\"--dataset_name={dataset_name}\",\n",
        "        f\"--output_dir={lora_output_dir}\",\n",
        "        f\"--merge_base_and_lora_output_dir={merged_model_output_dir}\",\n",
        "        \"--lora_rank=16\",\n",
        "        \"--lora_alpha=32\",\n",
        "        \"--lora_dropout=0.05\",\n",
        "        \"--warmup_steps=10\",\n",
        "        f\"--max_steps={max_steps}\",\n",
        "        \"--learning_rate=2e-4\",\n",
        "        f\"--precision_mode={finetuning_precision_mode}\",\n",
        "        f\"--template={template}\",\n",
        "    ],\n",
        "    replica_count=replica_count,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    boot_disk_size_gb=500,\n",
        ")\n",
        "\n",
        "print(\"The finetuned Lora adapter can be found at: \", lora_output_dir)\n",
        "print(\n",
        "    \"The finetuned Lora adapter merged with the base model can be found at: \",\n",
        "    merged_model_output_dir,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvV1kjIYpFCV"
      },
      "source": [
        "## **Option 2** Finetune Mistral-7B locally and merge Lora weights afterwards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWIG1eL3nLK1"
      },
      "source": [
        "### Load dataset for finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy6QLtYfrxD5"
      },
      "source": [
        "Let's load a dataset on Vertex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAP-jYBjrwUc"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\"fredmo/vertexai-qna-500\", split=\"train\")  # Full train split\n",
        "\n",
        "# Explore the data\n",
        "df = data.to_pandas()\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2f5tr1-SJd6"
      },
      "source": [
        "Instruction Finetuning - Prepare the dataset under the format of \"prompt\" so the model can better understand :\n",
        "1. the function generate_prompt : take the instruction and output and generate a prompt\n",
        "2. shuffle the dataset\n",
        "3. tokenizer the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mjgn9ptNTrw8"
      },
      "outputs": [],
      "source": [
        "def generate_prompt(data_point):\n",
        "    \"\"\"Gen. input text based on a prompt, task instruction, (context info.), and answer\n",
        "\n",
        "    :param data_point: dict: Data point\n",
        "    :return: dict: tokenized prompt\n",
        "    \"\"\"\n",
        "    text = (\n",
        "        \"Below is an instruction that describes a question. Write a response that \"\n",
        "        \"appropriately answer the request.\\n\\n\"\n",
        "    )\n",
        "    text += f'### Instruction:\\n{data_point[\"input_text\"]}\\n\\n'\n",
        "    text += f'### Response:\\n{data_point[\"output_text\"]}'\n",
        "    return text\n",
        "\n",
        "\n",
        "# add the \"prompt\" column in the dataset\n",
        "text_column = [generate_prompt(data_point) for data_point in data]\n",
        "data = data.add_column(\"prompt\", text_column)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmwdXOBGoZF7"
      },
      "source": [
        "You need to tokenize your data so the model can understand.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "810o72N7SI9A"
      },
      "outputs": [],
      "source": [
        "data = data.shuffle(seed=1234)  # Shuffle dataset here\n",
        "data = data.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H_wXxkhC8Yv"
      },
      "source": [
        "Split dataset into 90% for training and 10% for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TKCLTOVDR1x"
      },
      "outputs": [],
      "source": [
        "data = data.train_test_split(test_size=0.1)\n",
        "train_data = data[\"train\"]\n",
        "test_data = data[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgAyy_xDamxg"
      },
      "outputs": [],
      "source": [
        "print(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzNQf6lkqo-T"
      },
      "source": [
        "### Apply Lora  \n",
        "Here comes the magic with peft! Let's load a PeftModel and specify that you are going to use low-rank adapters (LoRA) using get_peft_model utility function and  the prepare_model_for_kbit_training method from PEFT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMELsVV6q2my"
      },
      "outputs": [],
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model.gradient_checkpointing_enable()  # Discarding intermediate activation values during the forward pass, add computation in backward pass\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cm3nXV988zew"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ4oR_hH9nF5"
      },
      "source": [
        "Use the following function to find out the linear layers for fine tuning.\n",
        "QLoRA paper : \"We find that the most critical LoRA hyperparameter is how many LoRA adapters are used in total and that LoRA on all linear transformer block layers is required to match full finetuning performance.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acCr5AZ0831z"
      },
      "outputs": [],
      "source": [
        "import bitsandbytes as bnb\n",
        "\n",
        "\n",
        "def find_all_linear_names(model):\n",
        "    cls = bnb.nn.Linear4bit\n",
        "    lora_module_names = set()\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, cls):\n",
        "            names = name.split(\".\")\n",
        "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "        if \"lm_head\" in lora_module_names:  # needed for 16-bit\n",
        "            lora_module_names.remove(\"lm_head\")\n",
        "    return list(lora_module_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhtO5dMr9Gq3"
      },
      "outputs": [],
      "source": [
        "modules = find_all_linear_names(model)\n",
        "print(modules)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glEtbT3z_hme"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# PEFT library supports various other PEFT methods such as prefix tuning, P-tuning, and Prompt Tuning. etc.\n",
        "# Since you are using the LoRA method, you are using the LoraConfig class.\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # dimension of the low-rank matrix\n",
        "    lora_alpha=32,  # adjusts the magnitude of the combined result (base model output + low-rank adaptation)\n",
        "    target_modules=modules,\n",
        "    lora_dropout=0.05,  # 5% dropout neuron probability of the LoRA layers. To avoid overfitting.\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIWgYbz9C2ee"
      },
      "outputs": [],
      "source": [
        "trainable, total = model.get_nb_trainable_parameters()\n",
        "print(\n",
        "    f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jGWwA25r-x0"
      },
      "source": [
        "### Run the training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5G6w-TvuU5lN"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# Log in to HF Hub\n",
        "notebook_login()\n",
        "\n",
        "wandb.login()\n",
        "%env WANDB_PROJECT=python-fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlBiY1OzFZnN"
      },
      "source": [
        "Setting the training arguments:\n",
        "* for the reason of demo, you just ran it for few steps (100) just to showcase how to use this integration with existing tools on the HF ecosystem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZRzecfdjb1B"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "\n",
        "import transformers\n",
        "\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "# ‚ÄúTransformer Reinforcement Learning‚Äù is used for fine-tuning the transformer model using reinforcement learning.\n",
        "# You will use your instruction dataset to perform this reinforcement learning and fine-tune the model.\n",
        "# You will be using SFTrainer object to perform the fine-tuning.\n",
        "\n",
        "!pip install -q trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hd1CV0KUdqe8"
      },
      "outputs": [],
      "source": [
        "# Some parameters to consider:\n",
        "# gradient_checkpointing (already enabled on the model). Used to reduce mem by re-computing intermediate activations during backwards instead of storing them all.\n",
        "# weight decay : to prevent overfitting by adding penalty to loss function\n",
        "\n",
        "trainingArgs = transformers.TrainingArguments(\n",
        "    per_device_train_batch_size=3,  # Batch size per GPU\n",
        "    gradient_accumulation_steps=4,  # Number of update steps to accumulate the gradient for\n",
        "    warmup_steps=0.03,\n",
        "    max_steps=100,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=1,  # Frequency of logging\n",
        "    output_dir=\"outputs\",  # Model predictions and checkpoints storage\n",
        "    optim=\"paged_adamw_8bit\",  # optimizer is responsible for computing the gradient statistics for back propagation. Done in 8-bit to save memory.\n",
        "    report_to=\"wandb\",\n",
        "    save_strategy=\"epoch\",  # save after every epoch\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQyMqLg5izHF"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=test_data,\n",
        "    dataset_text_field=\"prompt\",\n",
        "    peft_config=lora_config,\n",
        "    args=trainingArgs,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "# MLM is a training method used in models like BERT, where some tokens in the input sequence are masked,\n",
        "# and the model learns to predict the masked tokens based on the surrounding context.\n",
        "# MLM has the advantage of bidirectional context, allowing the model to consider both past and future tokens when making predictions.\n",
        "# This approach is especially useful for tasks like text classification, sentiment analysis, and named entity recognition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnXtaw9qFcz6"
      },
      "source": [
        "Start the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4HNvrh5FYqM"
      },
      "outputs": [],
      "source": [
        "print(\"Start the supervised fine tuning of Mistral-7B\")\n",
        "\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()\n",
        "\n",
        "print(\"Done Training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rC5KoPcp2s3"
      },
      "source": [
        "### Push the Lora Adapter and tokenizer to Hugging face Hub (or GCS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTjpXInGgPf4"
      },
      "outputs": [],
      "source": [
        "# stop reporting to wandb\n",
        "wandb.finish()\n",
        "\n",
        "# save model\n",
        "trainer.save_model()\n",
        "print(\"Model saved\")\n",
        "\n",
        "# push to hub the LORA adapter\n",
        "model.push_to_hub(\"Thomas-lemoullec/mistral_7b_vertexQandA\")\n",
        "tokenizer.push_to_hub(\"Thomas-lemoullec/mistral_7b_vertexQandA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kChtvouuGFvT"
      },
      "source": [
        "### Merge the LORA adapter to the main model with peft train docker image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRwy6h0zGBOB"
      },
      "outputs": [],
      "source": [
        "merge_job_name = create_name_with_datetime(prefix=\"mistral-peft-merge\")\n",
        "\n",
        "# The base model to be merged upon. It can be a huggingface model id, or a GCS\n",
        "# path where the base model was stored.\n",
        "base_model_dir = \"mistralai/Mistral-7B-v0.1\"  # @param {type:\"string\"}\n",
        "# The previously trained LoRA adapter. It needs to be stored in a GCS path.\n",
        "finetuned_lora_adapter_dir = (\n",
        "    \"Thomas-lemoullec/mistral_7b_vertexQandA\"  # \"gs://mistral-lora-weights/outputs\"\n",
        ")\n",
        "\n",
        "\n",
        "print(finetuned_lora_adapter_dir)\n",
        "\n",
        "# The GCS path to save the merged model\n",
        "merged_model_output_dir = os.path.join(BUCKET_URI, merge_job_name)\n",
        "\n",
        "machine_type = \"n1-highmem-16\"\n",
        "accelerator_type = \"NVIDIA_TESLA_V100\"\n",
        "accelerator_count = 2\n",
        "\n",
        "# machine_type = \"g2-standard-8\"\n",
        "# accelerator_type = \"NVIDIA_L4\"\n",
        "\n",
        "worker_pool_specs = [\n",
        "    {\n",
        "        \"machine_spec\": {\n",
        "            \"machine_type\": machine_type,\n",
        "            \"accelerator_type\": accelerator_type,\n",
        "            \"accelerator_count\": accelerator_count,\n",
        "        },\n",
        "        \"replica_count\": 1,\n",
        "        \"container_spec\": {\n",
        "            \"image_uri\": TRAIN_DOCKER_URI,\n",
        "            \"command\": [],\n",
        "            \"args\": [\n",
        "                \"--task=merge-causal-language-model-lora\",\n",
        "                \"--merge_model_precision_mode=float16\",\n",
        "                \"--pretrained_model_id=%s\" % base_model_dir,\n",
        "                \"--finetuned_lora_model_dir=%s\" % finetuned_lora_adapter_dir,\n",
        "                \"--merge_base_and_lora_output_dir=%s\" % merged_model_output_dir,\n",
        "            ],\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "merge_custom_job = aiplatform.CustomJob(\n",
        "    display_name=merge_job_name,\n",
        "    project=PROJECT_ID,\n",
        "    worker_pool_specs=worker_pool_specs,\n",
        "    staging_bucket=STAGING_BUCKET,\n",
        ")\n",
        "\n",
        "merge_custom_job.run()\n",
        "\n",
        "print(\"The merged model is stored at: \", merged_model_output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9vKI_xAqh7W"
      },
      "source": [
        "## Step 3 - Deploy the finetuned model with vLLM docker image\n",
        "\n",
        "This section uploads the model to Model Registry and deploys it on the Endpoint.\n",
        "The model deployment step will take ~15 minutes to complete.\n",
        "\n",
        "\n",
        "NOTE: vLLM requires a merged model with the base model and the finetuned LoRA adapter. Based on your business need, if you need the base model and the finetuned LoRA weight to be served separately, please consider using the regular Vertex AI serving instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyDWPdV1NjMT"
      },
      "outputs": [],
      "source": [
        "# Finds Vertex AI prediction supported accelerators and regions in\n",
        "# https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
        "\n",
        "# Sets V100 to deploy Mistral-7B\n",
        "# V100 serving has better throughput and latency performance than L4 serving.\n",
        "machine_type = \"n1-highmem-8\"\n",
        "accelerator_type = \"NVIDIA_TESLA_V100\"\n",
        "accelerator_count = 2\n",
        "\n",
        "# Sets L4 to deploy Mistral-7B\n",
        "# L4 serving is more cost efficient than V100 serving.\n",
        "# machine_type = \"g2-standard-8\"\n",
        "# accelerator_type = \"NVIDIA_L4\"\n",
        "# accelerator_count = 1\n",
        "\n",
        "model_with_peft, endpoint_with_peft = deploy_model_vllm(\n",
        "    model_name=create_name_with_datetime(prefix=\"mistral-peft-serve-vllm\"),\n",
        "    model_id=merged_model_output_dir,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        ")\n",
        "\n",
        "print(\"endpoint_name:\", endpoint_with_peft.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykceTVLlqxFs"
      },
      "source": [
        "## Cleaning the memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-G6g2HhCOHJL"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "import torch\n",
        "\n",
        "# clear the VRAM\n",
        "\n",
        "\n",
        "def memory_stats():\n",
        "    print(\"allocated:\")\n",
        "    print(torch.cuda.memory_allocated() / 1024**2)\n",
        "    print(\"cached:\")\n",
        "    print(torch.cuda.memory_cached() / 1024**2)\n",
        "\n",
        "\n",
        "memory_stats()\n",
        "\n",
        "# del trained_model\n",
        "# del lora_merged_model\n",
        "# del trainer\n",
        "# del model\n",
        "# del tokenizer\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "memory_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki-m4hzHFqTu"
      },
      "source": [
        "## Step 4 Run inference to evaluate the finetuned model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5xuRAa0sd3m"
      },
      "source": [
        "### **Option 1** Run inference with the Merged finetuned Model (vLLM Vertex endpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4v2Mnui4tH1X"
      },
      "outputs": [],
      "source": [
        "instance = {\n",
        "    \"prompt\": \"What is Model Garden?\",\n",
        "    \"n\": 1,\n",
        "    \"max_tokens\": 250,\n",
        "    \"temperature\": 1.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"top_k\": 10,\n",
        "}\n",
        "response = endpoint_with_peft.predict(instances=[instance])\n",
        "print(response.predictions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmnvCXfIsyYf"
      },
      "source": [
        "### **Option 2** Run inference with the adapter model hosted in the Hugging Face hub (uploaded in finetuning option 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edIRBmRBaPBG"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BIqHF9Zlfrc"
      },
      "source": [
        "Load directly adapters from the Hub using the command below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UW3oeIAMlerc"
      },
      "outputs": [],
      "source": [
        "# Based on your business need, if you need the base model and the finetuned LoRA weight to be served separately\n",
        "\n",
        "import torch\n",
        "from peft import PeftConfig, PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "peft_model_id = \"Thomas-lemoullec/mistral_7b_vertexQandA\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.base_model_name_or_path,\n",
        "    return_dict=True,\n",
        "    load_in_4bit=True,\n",
        "    device_map=device_map,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "# Load the Lora model\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz0l3qR7oC3h"
      },
      "source": [
        "You can then directly use the trained model that you have loaded from the ü§ó Hub for inference as you would do it usually in transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdx9wI0ZdVZr"
      },
      "outputs": [],
      "source": [
        "result = get_completion(query=\"What is Model Garden?\", model=model, tokenizer=tokenizer)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsyIG4vl66o0"
      },
      "source": [
        "## Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9EMCOUJ6-ji"
      },
      "outputs": [],
      "source": [
        "# Undeploy models and delete endpoints.\n",
        "endpoint_with_peft.delete(force=True)\n",
        "\n",
        "# Delete models.\n",
        "model_with_peft.delete()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "model_garden_pytorch_mistral_peft_tuning.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybMCVFh0_5R8"
      },
      "source": [
        "# Fine-tuning Mistral-7b on Vertex AI\n",
        "\n",
        "Note that this could be used for any model that supports device_map (i.e. loading the model with accelerate).\n",
        "\n",
        "Through This Notebook you will:\n",
        "\n",
        "\n",
        "*   **Step 1** Load quantized Mistral-7B model with bnb and run Local inference\n",
        "*   **Step 2** Fine tune Mistral-7B model with PEFT\n",
        "  *   *Option 1:* Finetune and merge Mistral-7B model with peft train docker image (maintained by Vertex Model Garden). Optionally run Hyperparameter tuning to find the best parameters.\n",
        "  *   *Option 2:* Finetuned manually Mistral-7B with bnb, peft and SFTTrainer. Merge the LoRA weights with the base Mistral-7B model with peft train.\n",
        "*   **Step 3** Deploy the finetuned model with vLLM docker image on a Vertex Endpoint\n",
        "*   **Step 4** Run inference to evaluate the finetuned model and compare with initial local inference with the based model\n",
        "  *   *Option 1:* Run inference with Merged model\n",
        "  *   *Option 2:* Run inference with Adapter model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoTMt4AXgYhr"
      },
      "source": [
        "# Step 0 - Initiatialise the Notebook\n",
        "## Define some helper functions and variables:\n",
        "\n",
        "0. Define some variables and APIs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 2654,
          "status": "ok",
          "timestamp": 1701888542840,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "I-OjzhpyMHsu",
        "outputId": "0d470d04-86f6-4221-c14a-ae5f7af49061"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ],
      "source": [
        "# Cloud project id.\n",
        "PROJECT_ID = \"[PROJECT_ID]\"  # @param {type:\"string\"}\n",
        "\n",
        "# The region you want to launch jobs in.\n",
        "REGION = \"[REGION]\"  # @param {type:\"string\"}\n",
        "\n",
        "# The Cloud Storage bucket for storing experiments output.\n",
        "# Start with gs:// prefix, e.g. gs://foo_bucket.\n",
        "BUCKET_URI = \"[BUCKET_URI]\"  # @param {type:\"string\"}\n",
        "\n",
        "! gcloud config set project $PROJECT_ID\n",
        "! gcloud services enable language.googleapis.com\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
        "EXPERIMENT_BUCKET = os.path.join(BUCKET_URI, \"peft\")\n",
        "DATA_BUCKET = os.path.join(EXPERIMENT_BUCKET, \"data\")\n",
        "MODEL_BUCKET = os.path.join(EXPERIMENT_BUCKET, \"model\")\n",
        "\n",
        "# The service account looks like:\n",
        "# '@.iam.gserviceaccount.com'\n",
        "# Please go to https://cloud.google.com/iam/docs/service-accounts-create#iam-service-accounts-create-console\n",
        "# and create service account with `Vertex AI User` and `Storage Object Admin` roles.\n",
        "# The service account for deploying fine tuned model.\n",
        "SERVICE_ACCOUNT = \"[SERVICE_ACCOUNT]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgNendbgjSJ6"
      },
      "source": [
        "## Initialize *Vertex AI API*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "executionInfo": {
          "elapsed": 1332,
          "status": "ok",
          "timestamp": 1701888544169,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "mj-SxEcNjQNN"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform, language, storage\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okocadG3huSB"
      },
      "source": [
        "## Define *constants*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1701888544169,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "KsIcP7nqh1FL"
      },
      "outputs": [],
      "source": [
        "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20231127_0916_RC00\"\n",
        "TRAIN_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-train:20240126_0936_RC00\"\n",
        "VLLM_GPTQ_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:gptq\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arrkM1d69rGK"
      },
      "source": [
        "## Define Common Functions\n",
        "1. Define a wrapper function which pass our query to the model for inference and return decoded model's completion(response)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1701888544169,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "NVZbaVf69quq"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def get_completion(query: str, model, tokenizer) -> str:\n",
        "  device = \"cuda:0\"\n",
        "\n",
        "  prompt_template = \"\"\"\n",
        "  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "  ### Question:\n",
        "  {query}\n",
        "\n",
        "  ### Answer:\n",
        "  \"\"\"\n",
        "  prompt = prompt_template.format(query=query)\n",
        "\n",
        "  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
        "\n",
        "  model_inputs = encodeds.to(device)\n",
        "\n",
        "\n",
        "  generated_ids = model.generate(**model_inputs, max_new_tokens=250, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
        "  decoded = tokenizer.batch_decode(generated_ids)\n",
        "  return (decoded[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qoa8GXFBiJ6I"
      },
      "source": [
        "1. Define model deployment functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1701888544169,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "Qxd5gsGhiKZ4"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "from typing import Tuple\n",
        "\n",
        "def create_name_with_datetime(prefix: str) -> str:\n",
        "    \"\"\"Creates a name with date time when triggering training or deployment\n",
        "    jobs in Vertex AI.\n",
        "    \"\"\"\n",
        "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Can add precision as parameter\n",
        "def deploy_model_vllm(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    service_account: str,\n",
        "    machine_type: str = \"n1-standard-8\",\n",
        "    accelerator_type: str = \"NVIDIA_TESLA_V100\",\n",
        "    accelerator_count: int = 1,\n",
        "    quantization_method: str = \"\",\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
        "\n",
        "    vllm_args = [\n",
        "        \"--host=0.0.0.0\",\n",
        "        \"--port=7080\",\n",
        "        f\"--model={model_id}\",\n",
        "        f\"--tensor-parallel-size={accelerator_count}\",\n",
        "        \"--swap-space=16\",\n",
        "        \"--gpu-memory-utilization=0.9\",\n",
        "        \"--max-num-batched-tokens=4096\",\n",
        "        \"--disable-log-stats\",\n",
        "    ]\n",
        "    if quantization_method:\n",
        "        vllm_args.append(f\"--quantization={quantization_method}\")\n",
        "    if quantization_method == \"gptq\":\n",
        "        vllm_docker_uri = VLLM_GPTQ_DOCKER_URI\n",
        "    else:\n",
        "        vllm_docker_uri = VLLM_DOCKER_URI\n",
        "\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=vllm_docker_uri,\n",
        "        serving_container_command=[\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
        "        serving_container_args=vllm_args,\n",
        "        serving_container_ports=[7080],\n",
        "        serving_container_predict_route=\"/generate\",\n",
        "        serving_container_health_route=\"/ping\",\n",
        "    )\n",
        "\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=service_account,\n",
        "    )\n",
        "    return model, endpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juj9DSmmh-Gw"
      },
      "source": [
        "## Install necessary packages\n",
        "First, install the dependencies below to get started. As these features are available on the main branches only, we need to install the libraries below from source."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 79275,
          "status": "ok",
          "timestamp": 1701888623442,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "3nNWXXc7ol1n",
        "outputId": "e5d3f5de-7c84-48b4-958b-155e424c1aea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sentencepiece==0.1.99\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "# Using BitsAndBytes Library for quantization\n",
        "!pip install -q -U bitsandbytes\n",
        "\n",
        "# Tranformers provides all API for downloading and working with pre-trained models that are in the HF hub.\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "\n",
        "# This package provides all the APIs we will need to perform the LoRA technique.\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "\n",
        "# Powerful huggingface package, that hides the complexity of the developer trying to write/manage code needed to use multi-GPUs/TPU/fp16.\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "\n",
        "! pip3 install sentencepiece==0.1.99\n",
        "\n",
        "# This huggingface package provides access to the various datasets in the huggingface hub.\n",
        "!pip install -q datasets\n",
        "\n",
        "# This library provides access to the Weights and Biases library to capture various metrics, during the fine-tuning process.\n",
        "!pip install -q wandb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NVvNhohkvwF"
      },
      "source": [
        "# Step 1 - Load quantized Mistral-7B model with bnb and run Local inference\n",
        "We'll load the model using QLoRA quantization to reduce the usage of memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "executionInfo": {
          "elapsed": 2880,
          "status": "ok",
          "timestamp": 1701888626318,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "kvvLg99Opw5R"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, # loading the base model in 4bit quantization. Also need to check model weights in config file.\n",
        "    bnb_4bit_use_double_quant=True, # Double Quantization\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16 # Would use float16 with compute capabilities below 8 (T4, V100)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgqxSuxsBX3r"
      },
      "source": [
        "Now we specify the model ID and then we load it with our previously defined quantization configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "834468dbd17d4dcf8669cc50e0357f22",
            "d9218f7f0b30409b9797376470f87bea",
            "fe2e9398633c4888ae9d77dbeac9f490",
            "b9215245db3c4d159316a46a81b00f33",
            "cfc1214ad4f94ec8b71698d9b413fadf",
            "7688bb8679d0484dbe66eae4f8e643b3",
            "6983004e8f56465393eeeb2f2e77b33c",
            "8d2894895193444791b63ce0dd36d072",
            "bc0e5b90951c4aa5ad38dd59cf0354fa",
            "30288a72713a483e825c25a7063f8f36",
            "41c593ae31994ef4bb00c8dd3797af3e"
          ]
        },
        "executionInfo": {
          "elapsed": 18708,
          "status": "ok",
          "timestamp": 1701888645022,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "7St-hFLNmS2v",
        "outputId": "1767030e-d5d8-4b8e-8d9d-8d7df1b006e8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "834468dbd17d4dcf8669cc50e0357f22",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "# Load Mistral-7B quantized with BitsAndBytesConfig defined above.\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\": 0})\n",
        "\n",
        "# Define the tokenizer\n",
        "# Using AutoTokenizers for creating a tokenizer for Mistral-7B\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Omw10c2djdIw"
      },
      "source": [
        "Run a inference on the base model. The model does not seem to understand our instruction and gives us a list of questions related to our query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 24812,
          "status": "ok",
          "timestamp": 1701888669831,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "TDkUkF2So2-7",
        "outputId": "8ad89c4c-bb77-4cb6-b514-519187f54c57"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s> \n",
            "  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "  ### Question:\n",
            "  What is Model Garden?\n",
            "\n",
            "  ### Answer:\n",
            "  </s>  The Model Garden Project is an international, joint development between The Horticulture Innovation Lab at the University of California, Tucson (UA), and researchers, including myself, who is an agricultural scientist.\n",
            "    My interest in this project began in 2008,  when my supervisor suggested that I join a development team that had started work  on a community development project at a rural area of  the Indian state of Karnataka. To get a feel of the situation and to understand the problems  to be addressed in the project, we had to first study the situation  with the help of a few local farmers.\n",
            "  I was surprised to see how backward the entire farming community was, both in terms of their knowledge and farming practices. The farmers, most of whom had been born and  brought up in the area, had little or no access to education or exposure to  any outside world. As a young man, I had often taken for granted  the education and exposure I had in a small Indian village, where I could not only go to school but also access a library   as many as 30 km away, to learn more about the world around me.\n",
            "   In stark\n"
          ]
        }
      ],
      "source": [
        "result = get_completion(query=\"What is Model Garden?\", model=model, tokenizer=tokenizer)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m06rH8cTrZof"
      },
      "source": [
        "# Step 2 - Fine tune Mistral-7B model with PEFT\n",
        "This section demonstrates how to finetune the Mistral-7b model, merge the finetuned LoRA adapter with the base model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-131f-kFncq6"
      },
      "source": [
        "Set the base model id."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "executionInfo": {
          "elapsed": 5,
          "status": "ok",
          "timestamp": 1701888669831,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "89NX4-1jniGJ"
      },
      "outputs": [],
      "source": [
        "base_model_id = \"mistralai/Mistral-7B-v0.1\" # @param"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBI4xnMEon8u"
      },
      "source": [
        "## **Option 1** Finetune and Merge Mistral-7B model with peft train docker image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIdjRx6tnyZz"
      },
      "source": [
        "### Finetune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paBvlvREn014"
      },
      "source": [
        "Use the Vertex AI SDK to create and run the custom training jobs with Vertex AI Model Garden training images.\n",
        "\n",
        "This example uses the dataset fredmo/vertexai-qna-500 , a small dataset containing questions and answers about GCP Vertex AI Documentation. You can either use a dataset from huggingface or a custom JSONL dataset in Vertex text model dataset format stored in Cloud Storage. The template parameter is optional.\n",
        "\n",
        "In order to make the finetuning efficiently, we enabled quantization for loading pretrained models for finetuning LoRA models. Precision options include \"4bit\", \"8bit\", \"float16\" (default) and \"float32\", and the precision can be set via \"--precision_mode\".\n",
        "\n",
        "In this section, the finetuned LoRA adapter will be saved to a GCS bucket specified by the variable lora_adapter_dir below; and we merge the LoRa adapter with the base model, and save it to a separate GCS bucket specified by merged_model_output_dir below.\n",
        "\n",
        "### Finetune with a custom dataset\n",
        "**WIP : Currently waiting for instructions regarding the Dataset expected format. Currently using the Abirate dataset instead of the VertexQ&A**\n",
        "\n",
        "To use a custom dataset, you should supply a gs:// URI to a JSONL file in Vertex text model dataset format in the dataset_name below.\n",
        "\n",
        "For example, here is one data point from the sample dataset gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl:\n",
        "\n",
        "{\"input_text\":\"TRANSCRIPT: \\nREASON FOR EVALUATION:,\\n\\n LABEL:\",\"output_text\":\"Chiropractic\"}\n",
        "\n",
        "To use this sample dataset that contains input_text and output_text fields, set dataset_name to gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl and template to vertex_sample. For advanced usage with custom datatset fields, see the template example and supply your own JSON template as gs:// URIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7GUQgq5pkF0"
      },
      "outputs": [],
      "source": [
        "# Huggingface dataset name or gs:// URI to a custom JSONL dataset.\n",
        "dataset_name = \"fredmo/vertexai-qna-500\"  # @param {type:\"string\"}\n",
        "# Optional. Template name or gs:// URI to a custom template.\n",
        "template = \"gs://mistral-finetuning-public/vertexAI_q&a_template.json\"  # @param {type:\"string\"}\n",
        "\n",
        "finetuning_precision_mode = \"float16\"\n",
        "\n",
        "# Worker pool spec.\n",
        "# Finetunes mistral-7B with 1 V100 (16G).\n",
        "machine_type = \"n1-highmem-16\"\n",
        "accelerator_type = \"NVIDIA_TESLA_V100\"\n",
        "accelerator_count = 2\n",
        "\n",
        "# Finetunes mistral-7B with 1 L4 (24G).\n",
        "#machine_type = \"g2-standard-8\"\n",
        "#accelerator_type = \"NVIDIA_L4\"\n",
        "#accelerator_count = 1\n",
        "\n",
        "replica_count = 1\n",
        "\n",
        "\n",
        "# Setup training job.\n",
        "job_name = create_name_with_datetime(\"mistral-lora-train\")\n",
        "train_job = aiplatform.CustomContainerTrainingJob(\n",
        "    display_name=job_name,\n",
        "    container_uri=TRAIN_DOCKER_URI,\n",
        ")\n",
        "\n",
        "# Create a GCS folder to store the LORA adapter.\n",
        "lora_adapter_dir = create_name_with_datetime(\"mistral-lora-adapter\")\n",
        "lora_output_dir = os.path.join(MODEL_BUCKET, lora_adapter_dir)\n",
        "lora_output_dir_gcsfuse = lora_output_dir.replace(\"gs://\", \"/gcs/\")\n",
        "\n",
        "# Create a GCS folder to store the merged model with the base model and the\n",
        "# finetuned LORA adapter.\n",
        "merged_model_dir = create_name_with_datetime(\"mistral-merged-model\")\n",
        "merged_model_output_dir = os.path.join(MODEL_BUCKET, merged_model_dir)\n",
        "merged_model_output_dir_gcsfuse = merged_model_output_dir.replace(\"gs://\", \"/gcs/\")\n",
        "\n",
        "# Pass training arguments and launch job.\n",
        "train_job.run(\n",
        "    args=[\n",
        "        \"--task=causal-language-modeling-lora\",\n",
        "        f\"--pretrained_model_id={base_model_id}\",\n",
        "        f\"--dataset_name={dataset_name}\",\n",
        "        f\"--output_dir={lora_output_dir_gcsfuse}\",\n",
        "        f\"--merge_base_and_lora_output_dir={merged_model_output_dir_gcsfuse}\",\n",
        "        \"--lora_rank=16\",\n",
        "        \"--lora_alpha=32\",\n",
        "        \"--lora_dropout=0.05\",\n",
        "        \"--warmup_steps=10\",\n",
        "        \"--max_steps=10\",\n",
        "        \"--learning_rate=2e-4\",\n",
        "        f\"--precision_mode={finetuning_precision_mode}\",\n",
        "        f\"--template={template}\",\n",
        "    ],\n",
        "    replica_count=replica_count,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    boot_disk_size_gb=500,\n",
        ")\n",
        "\n",
        "print(\"The finetuned Lora adapter can be found at: \", lora_output_dir)\n",
        "print(\n",
        "    \"The finetuned Lora adapter merged with the base model can be found at: \",\n",
        "    merged_model_output_dir,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vkfwFYkeXGi"
      },
      "source": [
        "### [Optional] Hyperparameter tuning\n",
        "You can use the Vertex AI SDK to create and run the hyperparameter tuning job to obtain a better performance by experimenting with different hyperparameters such as learning rates.\n",
        "\n",
        "Define the following specifications:\n",
        "\n",
        "worker_pool_specs: Dictionary specifying the machine type and Docker image.\n",
        "\n",
        "parameter_spec: Dictionary specifying the parameters to optimize. The dictionary key is the string assigned to the command line argument for each hyperparameter in your training application code, and the dictionary value is the parameter specification. The parameter specification includes the type, min/max values, and scale for the hyperparameter.\n",
        "\n",
        "metric_spec: Dictionary specifying the metric to optimize. The dictionary key is the hyperparameter_metric_tag that you set in your training application code, and the value is the optimization goal.\n",
        "\n",
        "The following example runs 4 trials on different learning rates, and evaluates the model on 100 examples selected from the truthfulqa_mc2 dataset. You can customize the search space by extending the range of learning rates, adding other parameters such as LoRA rank, etc. Please refer to the hyperparameter tuning documentation for more information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 6,
          "status": "aborted",
          "timestamp": 1701889006892,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "ThWFsWvuevXt"
      },
      "outputs": [],
      "source": [
        "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
        "\n",
        "# Refer to https://github.com/EleutherAI/lm-evaluation-harness/tree/big-refactor/lm_eval/tasks\n",
        "# for supported tasks and eval metrics.\n",
        "eval_task = \"truthfulqa_mc2\"  # @param {type:\"string\"}\n",
        "eval_metric_name = \"acc\"  # @param {type:\"string\"}\n",
        "\n",
        "flags = {\n",
        "    \"learning_rate\": 2e-4,\n",
        "    \"precision_mode\": \"float16\",\n",
        "    \"task\": \"causal-language-modeling-lora\",\n",
        "    \"pretrained_model_id\": base_model_id,\n",
        "    \"output_dir\": output_dir,\n",
        "    \"warmup_steps\": 10,\n",
        "    \"max_steps\": 10,\n",
        "    \"lora_rank\": 16,\n",
        "    \"lora_alpha\": 32,\n",
        "    \"lora_dropout\": 0.05,\n",
        "    \"dataset_name\": dataset_name,\n",
        "    \"eval_steps\": 10,\n",
        "    \"eval_tasks\": eval_task,\n",
        "    \"eval_limit\": 100,\n",
        "    \"eval_metric_name\": eval_metric_name,\n",
        "}\n",
        "\n",
        "worker_pool_specs = [\n",
        "    {\n",
        "        \"machine_spec\": {\n",
        "            \"machine_type\": machine_type,\n",
        "            \"accelerator_type\": accelerator_type,\n",
        "            \"accelerator_count\": accelerator_count,\n",
        "        },\n",
        "        \"replica_count\": replica_count,\n",
        "        \"container_spec\": {\n",
        "            \"image_uri\": TRAIN_DOCKER_URI,\n",
        "            \"args\": [\"--{}={}\".format(k, v) for k, v in flags.items()],\n",
        "        },\n",
        "    }\n",
        "]\n",
        "metric_spec = {\"model_performance\": \"maximize\"}\n",
        "parameter_spec = {\n",
        "    \"learning_rate\": hpt.DiscreteParameterSpec(\n",
        "        values=[4e-5, 4.3e-5, 4.6e-5, 5e-5], scale=\"linear\"\n",
        "    ),\n",
        "}\n",
        "train_job = aiplatform.CustomJob(\n",
        "    display_name=job_name,\n",
        "    worker_pool_specs=worker_pool_specs,\n",
        "    staging_bucket=STAGING_BUCKET,\n",
        ")\n",
        "\n",
        "train_hpt_job = aiplatform.HyperparameterTuningJob(\n",
        "    display_name=f\"{job_name}_hpt\",\n",
        "    custom_job=train_job,\n",
        "    metric_spec=metric_spec,\n",
        "    parameter_spec=parameter_spec,\n",
        "    max_trial_count=4,\n",
        "    parallel_trial_count=2,\n",
        ")\n",
        "\n",
        "train_hpt_job.run()\n",
        "\n",
        "print(\"Trained models were saved in: \", output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14nHZc7Pe0kt"
      },
      "source": [
        "Then, find the best trial from the hyperparameter tuning job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 6,
          "status": "aborted",
          "timestamp": 1701889006892,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "C5EVR_f7e33Z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "best_trial_id = (\n",
        "    np.argmax(\n",
        "        [trial.final_measurement.metrics[0].value for trial in train_hpt_job.trials]\n",
        "    )\n",
        "    + 1\n",
        ")\n",
        "output_dir = os.path.join(output_dir, f\"trial_{best_trial_id}\")\n",
        "output_dir_gcsfuse = output_dir.replace(\"gs://\", \"/gcs/\")\n",
        "print(f\"Best trial {best_trial_id} saved model in:\", output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvV1kjIYpFCV"
      },
      "source": [
        "## **Option 2** Finetuned manually Mistral-7B and merge Lora weights afterwards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWIG1eL3nLK1"
      },
      "source": [
        "### Load dataset for finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy6QLtYfrxD5"
      },
      "source": [
        "Let's load a dataset on Vertex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 6,
          "status": "aborted",
          "timestamp": 1701889006892,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "NAP-jYBjrwUc"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\"fredmo/vertexai-qna-500\", split='train') #Full train split\n",
        "\n",
        "# Explore the data\n",
        "df = data.to_pandas()\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2f5tr1-SJd6"
      },
      "source": [
        "Instruction Finetuning - Prepare the dataset under the format of \"prompt\" so the model can better understand :\n",
        "1. the function generate_prompt : take the instruction and output and generate a prompt\n",
        "2. shuffle the dataset\n",
        "3. tokenizer the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 6,
          "status": "aborted",
          "timestamp": 1701889006892,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "Mjgn9ptNTrw8"
      },
      "outputs": [],
      "source": [
        "def generate_prompt(data_point):\n",
        "    \"\"\"Gen. input text based on a prompt, task instruction, (context info.), and answer\n",
        "\n",
        "    :param data_point: dict: Data point\n",
        "    :return: dict: tokenzed prompt\n",
        "    \"\"\"\n",
        "    text = 'Below is an instruction that describes a question. Write a response that ' \\\n",
        "            'appropriately answer the request.\\n\\n'\n",
        "    text += f'### Instruction:\\n{data_point[\"input_text\"]}\\n\\n'\n",
        "    text += f'### Response:\\n{data_point[\"output_text\"]}'\n",
        "    return text\n",
        "\n",
        "# add the \"prompt\" column in the dataset\n",
        "text_column = [generate_prompt(data_point) for data_point in data]\n",
        "data = data.add_column(\"prompt\", text_column)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmwdXOBGoZF7"
      },
      "source": [
        "We'll need to tokenize our data so the model can understand.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 5,
          "status": "aborted",
          "timestamp": 1701889006892,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "810o72N7SI9A"
      },
      "outputs": [],
      "source": [
        "data = data.shuffle(seed=1234)  # Shuffle dataset here\n",
        "data = data.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H_wXxkhC8Yv"
      },
      "source": [
        "Split dataset into 90% for training and 10% for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 5,
          "status": "aborted",
          "timestamp": 1701889006892,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "3TKCLTOVDR1x"
      },
      "outputs": [],
      "source": [
        "data = data.train_test_split(test_size=0.1)\n",
        "train_data = data[\"train\"]\n",
        "test_data = data[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 5,
          "status": "aborted",
          "timestamp": 1701889006892,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "TgAyy_xDamxg"
      },
      "outputs": [],
      "source": [
        "print(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzNQf6lkqo-T"
      },
      "source": [
        "### Apply Lora  \n",
        "Here comes the magic with peft! Let's load a PeftModel and specify that we are going to use low-rank adapters (LoRA) using get_peft_model utility function and  the prepare_model_for_kbit_training method from PEFT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 6,
          "status": "aborted",
          "timestamp": 1701889006893,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "NMELsVV6q2my"
      },
      "outputs": [],
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model.gradient_checkpointing_enable() # Discarding intermediate activation values during the forward pass, add computation in backward pass\n",
        "model = prepare_model_for_kbit_training(model) # TODO: Explain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 6,
          "status": "aborted",
          "timestamp": 1701889006893,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "cm3nXV988zew"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ4oR_hH9nF5"
      },
      "source": [
        "Use the following function to find out the linear layers for fine tuning.\n",
        "QLoRA paper : \"We find that the most critical LoRA hyperparameter is how many LoRA adapters are used in total and that LoRA on all linear transformer block layers is required to match full finetuning performance.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 6,
          "status": "aborted",
          "timestamp": 1701889006893,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "acCr5AZ0831z"
      },
      "outputs": [],
      "source": [
        "import bitsandbytes as bnb\n",
        "def find_all_linear_names(model):\n",
        "  cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
        "  lora_module_names = set()\n",
        "  for name, module in model.named_modules():\n",
        "    if isinstance(module, cls):\n",
        "      names = name.split('.')\n",
        "      lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
        "      lora_module_names.remove('lm_head')\n",
        "  return list(lora_module_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 6,
          "status": "aborted",
          "timestamp": 1701889006893,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "DhtO5dMr9Gq3"
      },
      "outputs": [],
      "source": [
        "modules = find_all_linear_names(model)\n",
        "print(modules)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 6,
          "status": "aborted",
          "timestamp": 1701889006893,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "glEtbT3z_hme"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# PEFT library supports various other PEFT methods such as prefix tuning, P-tuning, and Prompt Tuning. etc.\n",
        "# Since we are using the LoRA method, we are using the LoraConfig class.\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8, # dimension of the low-rank matrix\n",
        "    lora_alpha=32, # adjusts the magnitude of the combined result (base model output + low-rank adaptation)\n",
        "    target_modules=modules,\n",
        "    lora_dropout=0.05, # 5% dropout neuron probability of the LoRA layers. To avoid overfitting.\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 6,
          "status": "aborted",
          "timestamp": 1701889006893,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "LIWgYbz9C2ee"
      },
      "outputs": [],
      "source": [
        "trainable, total = model.get_nb_trainable_parameters()\n",
        "print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jGWwA25r-x0"
      },
      "source": [
        "### Run the training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 6,
          "status": "aborted",
          "timestamp": 1701889006893,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "5G6w-TvuU5lN"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "import wandb\n",
        "\n",
        "\n",
        "# Log in to HF Hub\n",
        "notebook_login()\n",
        "\n",
        "wandb.login()\n",
        "%env WANDB_PROJECT=python-fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlBiY1OzFZnN"
      },
      "source": [
        "Setting the training arguments:\n",
        "* for the reason of demo, we just ran it for few steps (100) just to showcase how to use this integration with existing tools on the HF ecosystem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 6,
          "status": "aborted",
          "timestamp": 1701889006893,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "xZRzecfdjb1B"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "# “Transformer Reinforcement Learning” is used for fine-tuning the transformer model using reinforcement learning.\n",
        "# We will use our instruction dataset to perform this reinforcement learning and fine-tune the model.\n",
        "# We will be using SFTrainer object to perform the fine-tuning.\n",
        "\n",
        "!pip install -q trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 6,
          "status": "aborted",
          "timestamp": 1701889006893,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "hd1CV0KUdqe8"
      },
      "outputs": [],
      "source": [
        "# Some parameters to consider:\n",
        "# gradient_checkpointing (already enabled on the model). Used to reduce mem by re-computing intermediate activations during backwards instead of storing them all.\n",
        "# weigth decay : to prevent overfitting by adding penalty to loss function\n",
        "\n",
        "trainingArgs = transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=3, # Batch size per GPU\n",
        "        gradient_accumulation_steps=4, # Number of update steps to accumulate the gradient for\n",
        "        warmup_steps=0.03,\n",
        "        max_steps=100,\n",
        "        learning_rate=2e-4,\n",
        "        logging_steps=1, # Frequency of logging\n",
        "        output_dir=\"outputs\", # Model predictions and checkpoints storage\n",
        "        optim=\"paged_adamw_8bit\", # optimizer is responsible for computing the gradient statistics for back propagation. Done in 8-bit to save memory.\n",
        "        report_to=\"wandb\",\n",
        "        save_strategy=\"epoch\", # save after every epoch\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 5,
          "status": "aborted",
          "timestamp": 1701889006893,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "pQyMqLg5izHF"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token #TODO: Explain\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=test_data, #TODO: Explain\n",
        "    dataset_text_field=\"prompt\", #TODO: Explain\n",
        "    peft_config=lora_config,\n",
        "    args=trainingArgs,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False), #TODO: CLM and not MLM\n",
        ")\n",
        "\n",
        "#MLM is a training method used in models like BERT, where some tokens in the input sequence are masked,\n",
        "# and the model learns to predict the masked tokens based on the surrounding context.\n",
        "# MLM has the advantage of bidirectional context, allowing the model to consider both past and future tokens when making predictions.\n",
        "# This approach is especially useful for tasks like text classification, sentiment analysis, and named entity recognition.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnXtaw9qFcz6"
      },
      "source": [
        "Start the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 5,
          "status": "aborted",
          "timestamp": 1701889006893,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "W4HNvrh5FYqM"
      },
      "outputs": [],
      "source": [
        "print('Start the supervised fine tuning of Mistral-7B')\n",
        "\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()\n",
        "\n",
        "print('Done Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rC5KoPcp2s3"
      },
      "source": [
        "### Push the Lora Adapater and tokenizer to Hugging face Hub (or GCS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 5,
          "status": "aborted",
          "timestamp": 1701889006893,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "uTjpXInGgPf4"
      },
      "outputs": [],
      "source": [
        "#stop reporting to wandb\n",
        "wandb.finish()\n",
        "\n",
        "# save model\n",
        "trainer.save_model()\n",
        "print(\"Model saved\")\n",
        "\n",
        "# push to hub the LORA adapter\n",
        "model.push_to_hub(\"Thomas-lemoullec/mistral_7b_vertexQandA\")\n",
        "tokenizer.push_to_hub(\"Thomas-lemoullec/mistral_7b_vertexQandA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kChtvouuGFvT"
      },
      "source": [
        "### Merge the LORA adapter to the main model with peft train docker image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 5,
          "status": "aborted",
          "timestamp": 1701889006893,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "TRwy6h0zGBOB"
      },
      "outputs": [],
      "source": [
        "merge_job_name = create_name_with_datetime(prefix=\"mistral-peft-merge\")\n",
        "\n",
        "# The base model to be merged upon. It can be a huggingface model id, or a GCS\n",
        "# path where the base model was stored.\n",
        "base_model_dir = \"mistralai/Mistral-7B-v0.1\"  # @param {type:\"string\"}\n",
        "# The previously trained LoRA adapter. It needs to be stored in a GCS path.\n",
        "finetuned_lora_adapter_dir = \"Thomas-lemoullec/mistral_7b_vertexQandA\"#\"gs://mistral-lora-weights/outputs\"\n",
        "\n",
        "\n",
        "print(finetuned_lora_adapter_dir)\n",
        "\n",
        "# The GCS path to save the merged model\n",
        "merged_model_output_dir = os.path.join(BUCKET_URI, merge_job_name)\n",
        "merged_model_output_dir_gcsfuse = merged_model_output_dir.replace(\"gs://\", \"/gcs/\")\n",
        "\n",
        "machine_type = \"n1-highmem-16\"\n",
        "accelerator_type = \"NVIDIA_TESLA_V100\"\n",
        "accelerator_count = 2\n",
        "\n",
        "#machine_type = \"g2-standard-8\"\n",
        "#accelerator_type = \"NVIDIA_L4\"\n",
        "\n",
        "worker_pool_specs = [\n",
        "    {\n",
        "        \"machine_spec\": {\n",
        "            \"machine_type\": machine_type,\n",
        "            \"accelerator_type\": accelerator_type,\n",
        "            \"accelerator_count\": accelerator_count,\n",
        "        },\n",
        "        \"replica_count\": 1,\n",
        "        \"container_spec\": {\n",
        "            \"image_uri\": TRAIN_DOCKER_URI,\n",
        "            \"command\": [],\n",
        "            \"args\": [\n",
        "                \"--task=merge-causal-language-model-lora\",\n",
        "                \"--merge_model_precision_mode=float16\",\n",
        "                \"--pretrained_model_id=%s\" % base_model_dir,\n",
        "                \"--finetuned_lora_model_dir=%s\" % finetuned_lora_adapter_dir,\n",
        "                \"--merge_base_and_lora_output_dir=%s\" % merged_model_output_dir_gcsfuse,\n",
        "            ],\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "merge_custom_job = aiplatform.CustomJob(\n",
        "    display_name=merge_job_name,\n",
        "    project=PROJECT_ID,\n",
        "    worker_pool_specs=worker_pool_specs,\n",
        "    staging_bucket=STAGING_BUCKET,\n",
        ")\n",
        "\n",
        "merge_custom_job.run()\n",
        "\n",
        "print(\"The merged model is stored at: \", merged_model_output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9vKI_xAqh7W"
      },
      "source": [
        "## Step 3 - Deploy the finetuned model with vLLM docker image\n",
        "\n",
        "This section uploads the model to Model Registry and deploys it on the Endpoint.\n",
        "The model deployment step will take ~15 minutes to complete.\n",
        "\n",
        "\n",
        "NOTE: vLLM requires a merged model with the base model and the finetuned LoRA adapter. Based on your business need, if you need the base model and the finetuned LoRA weight to be served separately, please consider using the regular Vertex serving instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 6,
          "status": "aborted",
          "timestamp": 1701889006894,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "GyDWPdV1NjMT"
      },
      "outputs": [],
      "source": [
        "# Finds Vertex AI prediction supported accelerators and regions in\n",
        "# https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
        "\n",
        "# Sets V100 to deploy Mistral-7B\n",
        "# V100 serving has better throughput and latency performance than L4 serving.\n",
        "machine_type = \"n1-highmem-8\"\n",
        "accelerator_type = \"NVIDIA_TESLA_V100\"\n",
        "accelerator_count = 2\n",
        "\n",
        "# Sets L4 to deploy Mistral-7B\n",
        "# L4 serving is more cost efficient than V100 serving.\n",
        "# machine_type = \"g2-standard-8\"\n",
        "# accelerator_type = \"NVIDIA_L4\"\n",
        "# accelerator_count = 1\n",
        "\n",
        "model_with_peft, endpoint_with_peft = deploy_model_vllm(\n",
        "    model_name=create_name_with_datetime(prefix=\"mistral-peft-serve-vllm\"),\n",
        "    model_id=merged_model_output_dir,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        ")\n",
        "\n",
        "print(\"endpoint_name:\", endpoint_with_peft.name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykceTVLlqxFs"
      },
      "source": [
        "## Cleaning the memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 6,
          "status": "aborted",
          "timestamp": 1701889006894,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "-G6g2HhCOHJL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import gc\n",
        "# clear the VRAM\n",
        "\n",
        "\n",
        "def memory_stats():\n",
        "    print('allocated:')\n",
        "    print(torch.cuda.memory_allocated()/1024**2)\n",
        "    print('cached:')\n",
        "    print(torch.cuda.memory_cached()/1024**2)\n",
        "\n",
        "memory_stats()\n",
        "\n",
        "#del trained_model\n",
        "#del lora_merged_model\n",
        "#del trainer\n",
        "#del model\n",
        "#del tokenizer\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "memory_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki-m4hzHFqTu"
      },
      "source": [
        "## Step 4 Run inference to evaluate the finetuned model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5xuRAa0sd3m"
      },
      "source": [
        "### **Option 1** Run inference with the Merged finetuned Model (vLLM Vertex endpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 6,
          "status": "aborted",
          "timestamp": 1701889006894,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "4v2Mnui4tH1X"
      },
      "outputs": [],
      "source": [
        "instance = {\n",
        "    \"prompt\": \"What is Model Garden?\",\n",
        "    \"n\": 1,\n",
        "    \"max_tokens\": 250,\n",
        "    \"temperature\": 1.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"top_k\": 10,\n",
        "}\n",
        "response = endpoint_with_peft.predict(instances=[instance])\n",
        "print(response.predictions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmnvCXfIsyYf"
      },
      "source": [
        "### **Option 2** Run inference with the adapter model hosted in the Hugging Face hub (uploaded in finetuning option 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 6,
          "status": "aborted",
          "timestamp": 1701889006894,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "edIRBmRBaPBG"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BIqHF9Zlfrc"
      },
      "source": [
        "Load directly adapters from the Hub using the command below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 6,
          "status": "aborted",
          "timestamp": 1701889006894,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "UW3oeIAMlerc"
      },
      "outputs": [],
      "source": [
        "# Based on your business need, if you need the base model and the finetuned LoRA weight to be served separately\n",
        "\n",
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "peft_model_id = \"Thomas-lemoullec/mistral_7b_vertexQandA\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_4bit=True, device_map=device_map)\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "# Load the Lora model\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz0l3qR7oC3h"
      },
      "source": [
        "You can then directly use the trained model that you have loaded from the 🤗 Hub for inference as you would do it usually in transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 6,
          "status": "aborted",
          "timestamp": 1701889006894,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -60
        },
        "id": "kdx9wI0ZdVZr"
      },
      "outputs": [],
      "source": [
        "result = get_completion(query=\"What is Model Garden?\", model=model, tokenizer=tokenizer)\n",
        "print(result)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "30288a72713a483e825c25a7063f8f36": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41c593ae31994ef4bb00c8dd3797af3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6983004e8f56465393eeeb2f2e77b33c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7688bb8679d0484dbe66eae4f8e643b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "834468dbd17d4dcf8669cc50e0357f22": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d9218f7f0b30409b9797376470f87bea",
              "IPY_MODEL_fe2e9398633c4888ae9d77dbeac9f490",
              "IPY_MODEL_b9215245db3c4d159316a46a81b00f33"
            ],
            "layout": "IPY_MODEL_cfc1214ad4f94ec8b71698d9b413fadf"
          }
        },
        "8d2894895193444791b63ce0dd36d072": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9215245db3c4d159316a46a81b00f33": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30288a72713a483e825c25a7063f8f36",
            "placeholder": "​",
            "style": "IPY_MODEL_41c593ae31994ef4bb00c8dd3797af3e",
            "value": " 2/2 [00:15&lt;00:00,  7.21s/it]"
          }
        },
        "bc0e5b90951c4aa5ad38dd59cf0354fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cfc1214ad4f94ec8b71698d9b413fadf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9218f7f0b30409b9797376470f87bea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7688bb8679d0484dbe66eae4f8e643b3",
            "placeholder": "​",
            "style": "IPY_MODEL_6983004e8f56465393eeeb2f2e77b33c",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "fe2e9398633c4888ae9d77dbeac9f490": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d2894895193444791b63ce0dd36d072",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bc0e5b90951c4aa5ad38dd59cf0354fa",
            "value": 2
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

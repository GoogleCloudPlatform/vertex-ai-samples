{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems on Retail/E-commerce data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Of Contents\n",
    "* [Overview](#section-1)\n",
    "* [Dataset](#section-2)\n",
    "* [Objective](#section-3)\n",
    "* [Cost](#section-4)\n",
    "* [Load Data](#section-5)\n",
    "* [Analyze the data](#section-6)\n",
    "* [Perform feature Engineering](#section-7)\n",
    "* [Select required amount of training data](#section-8)\n",
    "* [Define Training Parameters](#section-9)\n",
    "* [Perform Cross-Validation](#section-10)\n",
    "* [Evaluate the best model from Cross-Validation](#section-11)\n",
    "* [Generate recommendations](#section-12)\n",
    "* [Write the recommendations to Bigquery](#section-13)\n",
    "* [Save the trained model to GCS path](#section-14)\n",
    "* [Clean Up](#section-15)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "<a name=\"section-1\"></a>\n",
    "\n",
    "\n",
    "Recommender systems are powerful tools that model existing customer behavior to generate recommendations. These models generally build complex matrices and map out existing customer preferences in order to find intersecting interests and offer recommendations. These matrices can be very large and will benefit from distributed computing and large memory pools. This is a perfect application for Vertex-AI and Pyspark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "<a name=\"section-2\"></a>\n",
    "\n",
    "\n",
    "This notebook uses the  <a href=\"https://www.kaggle.com/retailrocket/ecommerce-dataset\">\"Retailrocket recommender system dataset - Ecommerce data: web events, item properties (with texts), category tree\"</a> dataset from Kaggle. The dataset consists of three files, a behaviour, items and categories set.\n",
    " \n",
    "The behaviour data, i.e. events like clicks, add to carts, transactions, represent interactions that were collected over a period of 4.5 months. A visitor can make three types of events, namely “view”, “addtocart” or “transaction”. In total there are 2,756,101 events including 2,664,312 views, 69,332 add to carts and 22,457 transactions produced by 1,407,580 unique visitors. \n",
    "\n",
    "Users and products have been obfuscated by replacing the text with numerical IDs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "<a name=\"section-3\"></a>\n",
    "\n",
    "\n",
    "In this notebook, we are going to build a recommendation system on <a href=\"https://www.kaggle.com/retailrocket/ecommerce-dataset\">Retail-Rocket dataset</a>. To do so, we shall use managed instances from Vertex-AI and interactive Pyspark services offered by Veretex-AI. The approach we are going to take is the <a href=\"https://en.wikipedia.org/wiki/Collaborative_filtering#:~:text=Collaborative%20filtering%20(CF)%20is%20a%20technique%20used%20by%20recommender%20systems.&text=In%20the%20newer%2C%20narrower%20sense,from%20many%20users%20(collaborating).\">collaborative filtering</a> approach with a learning algorithm as the <a href=\"http://dl.acm.org/citation.cfm?id=1608614\"><b>Alternating Least Squares(ALS)</b></a> method.\n",
    "\n",
    "Things to do before running this notebook : \n",
    "* Collect data from Kaggle and store them into a GCS bucket\n",
    "* Spawn a Dataproc cluster with JupyterLab extension and component gateway enabled.\n",
    "* Change the kernel of this notebook on the Vertex-AI's managed instance to the Pyspark on created dataproc cluster(remote).\n",
    "\n",
    "<img src=\"images/Cluster_setup.PNG\"></img>\n",
    "\n",
    "Once the cluster creation step has finished, wait ~3 minutes for it to become available to the Managed Notebooks. Once the kernel for the cluster is available, select it.\n",
    "\n",
    "<img src=\"images/cluster_kernel_selection.PNG\"></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, set your project ID here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost\n",
    "<a name=\"section-4\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "<a name=\"section-5\"></a>\n",
    "\n",
    "The dataset mainly consists of events data and items data. In the current notebook, we will consider only events data to perform collaborative-filtering approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data from the GCS buckets. File contents of GCS buckets can be browsed from the GCS file browser on the left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "events = pd.read_csv('path to events.csv file')\n",
    "print (events.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the data\n",
    "<a name=\"section-6\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the event distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events['event'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check null values in the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the unique ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (events['visitorid'].unique().shape, events['itemid'].unique().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three types of events in the data : \"view\", \"addtocart\" and \"transaction\" corresponding to ~1M visitorids and ~200K itemids. Among the given fields, transcationid has many null values which makes sense as the visitor may not always make a transaction. Most of the times, the visitor may just view or add an item to cart without any purchase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform feature Engineering\n",
    "<a name=\"section-7\"></a>\n",
    "\n",
    "\n",
    "Generally in Collaborative filtering technique, a user-item matrix is generated which provides a quantitative measure of the association between users and items. In order to build such an association between users and items, a new column <b>product_rating</b> is defined based on the events taken by the user in the current solution. This new column would serve as a score that is being given between each user and the items associated with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign a score associated with each item for a user based on the events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_rating(interactions):\n",
    "    addtocart = 0\n",
    "    view = 0\n",
    "    \n",
    "    for e in interactions:\n",
    "        if e == 'transaction':\n",
    "            return 3\n",
    "        elif e == 'addtocart':\n",
    "            addtocart += 1\n",
    "        elif e == 'view':\n",
    "            view += 1\n",
    "         \n",
    "    if addtocart > 0:\n",
    "        return 2\n",
    "    \n",
    "    if view > 0:\n",
    "        return 1\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate the data to collect the event data for each user and apply the defined product_rating function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convertedData = events.groupby(by=['visitorid', 'itemid'])['event'].agg([product_rating]).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (convertedData.shape)\n",
    "convertedData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the distribution of the product_rating column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(convertedData['product_rating'], kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check unique user ids and item ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (convertedData['visitorid'].unique().shape, convertedData['itemid'].unique().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the distribution of number of items associated with each user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_count = convertedData[['visitorid','itemid']].groupby(by=['visitorid']).count()\n",
    "sns.boxplot(x=item_count['itemid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select required amount of training data\n",
    "<a name=\"section-8\"></a>\n",
    "\n",
    "\n",
    "It can be seen that most of the users are in the low ranges i.e., who are associated with 1 or items. Also, there are 1.5 million users in this dataset which is way many for the current solution. Moving ahead, the training dataset will be limited to the top 1000 users based on their item association."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the top 1000 users based on the number of items they are associated with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_users = 1000\n",
    "item_count.sort_values(by='itemid', ascending=False, inplace=True)\n",
    "convertedData = convertedData[convertedData['visitorid'].isin(item_count.iloc[:total_users].index)]\n",
    "convertedData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the spark context to ensure it is connected to the remote Dataproc cluster.\n",
    "* **Note: To connect to the Dataproc cluster, change the kernel to remote pyspark instance for the created clsuter.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession.builder \\\n",
    ".appName('Recommendations') \\\n",
    ".config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar') \\ ## specify the jar files required to instantiate Bigquery Connector\n",
    ".getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Parameters\n",
    "<a name=\"section-9\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create ALS model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(\n",
    "         userCol=\"visitorid\", \n",
    "         itemCol=\"itemid\",\n",
    "         ratingCol=\"product_rating\", \n",
    "         nonnegative = True, \n",
    "         implicitPrefs = False,\n",
    "         coldStartStrategy=\"drop\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add hyperparameters and their respective values to param_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(als.rank, [10, 150]) \\\n",
    "            .addGrid(als.regParam, [.01, .1]) \\\n",
    "            .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define evaluator as RMSE and print length of evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "           metricName=\"rmse\", \n",
    "           labelCol=\"product_rating\", \n",
    "           predictionCol=\"prediction\") \n",
    "print (\"Num models to be tested: \", len(param_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a SparkDataframe to train the ALS model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = convertedData[['visitorid', 'itemid', 'product_rating']]\n",
    "ratings=spark.createDataFrame(ratings) \n",
    "ratings.printSchema()\n",
    "ratings.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the data count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into Train-Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create test and train set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test) = ratings.randomSplit([0.8, 0.2], seed = 36)\n",
    "train.count(), test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Cross-Validation\n",
    "<a name=\"section-10\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build cross validation using CrossValidator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit cross validator to the train dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cv.fit(train)\n",
    "#Extract best model from the cv model above\n",
    "best_model = model.bestModel\n",
    "print(\"##Parameters for the Best Model##\")\n",
    "print(\"Rank:\", best_model._java_obj.parent().getRank())\n",
    "print(\"MaxIter:\", best_model._java_obj.parent().getMaxIter())\n",
    "print(\"RegParam:\", best_model._java_obj.parent().getRegParam())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the best model from Cross-Validation\n",
    "<a name=\"section-11\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the rating predictions by the model on train and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = best_model.transform(train)\n",
    "train_RMSE = evaluator.evaluate(train_predictions)\n",
    "\n",
    "test_predictions = best_model.transform(test)\n",
    "test_RMSE = evaluator.evaluate(test_predictions)\n",
    "\n",
    "print(\"Train RMSE \", train_RMSE)\n",
    "print(\"Test RMSE \" , test_RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate recommendations\n",
    "<a name=\"section-12\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate n Recommendations for all users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrecommendations = best_model.recommendForAllUsers(10)\n",
    "nrecommendations.limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate for a specific user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the items already associated with a user \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.where(train.visitorid == 2326 ).select (\"itemid\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get recommendations for the items for the selected user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrecommendations.where(nrecommendations.visitorid == 2326).select(\"recommendations.itemid\", \"recommendations.rating\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the recommendations to Bigquery\n",
    "<a name=\"section-13\"></a>\n",
    "\n",
    "\n",
    "Spark's ALS model generates specified number of item-recommendations for all the users it was created on. Further from the generated recommendations, the required user's recommendations can be filtered. So, in order to serve the recommendations to the end-users or any applications it can be hosted to a Bigquery table using Spark's Bigquery connector.\n",
    "\n",
    "\n",
    "### Create a Dataset in Bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#@bigquery\n",
    "-- create a dataset in Bigquery\n",
    "CREATE SCHEMA recommender_sys\n",
    "OPTIONS(\n",
    "  location=\"us\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the Recommendations to Bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"recommender_sys\"\n",
    "TABLE = \"recommendations\"\n",
    "TEMPORARY_GCS_PATH = \"vertex_ai_managed_services_demo/recommender_systems/temporarySparkfolder\"\n",
    "\n",
    "nrecommendations.write \\\n",
    "  .format(\"bigquery\") \\\n",
    "  .option(\"table\",\"{}.{}\".format(DATASET, TABLE)) \\\n",
    "  .option(\"temporaryGcsBucket\", TEMPORARY_GCS_PATH) \\\n",
    "  .mode('overwrite') \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the trained model to GCS path\n",
    "<a name=\"section-14\"></a>\n",
    "\n",
    "\n",
    "Pyspark's ALS.save() method will create a folder at the specified path where it saves the trained model. With GCS file browser available, this method can directly save the model to a GCS bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_OUTPUT_PATH = \"gs://path-to-save-the-model\"\n",
    "best_model.save(GCS_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "<a name=\"section-15\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : Clean up \n",
    "\n",
    "## After successful training and saving of the model, it is suggested to turn off or delete the created DataProc\n",
    "## cluster to avoid unnecessary charges.\n",
    "\n",
    "\n",
    "# Construct a BigQuery client object.\n",
    "client = bigquery.Client()\n",
    "\n",
    "# TODO(developer): Set model_id to the ID of the model to fetch.\n",
    "# dataset_id = 'your-project.your_dataset'\n",
    "\n",
    "# Use the delete_contents parameter to delete a dataset and its contents.\n",
    "# Use the not_found_ok parameter to not receive an error if the dataset has already been deleted.\n",
    "client.delete_dataset(\n",
    "    dataset_id, delete_contents=True, not_found_ok=True\n",
    ")  # Make an API request.\n",
    "\n",
    "print(\"Deleted dataset '{}'.\".format(dataset_id))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "managed-notebooks.m78",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/managed-notebooks:m78"
  },
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "local-pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

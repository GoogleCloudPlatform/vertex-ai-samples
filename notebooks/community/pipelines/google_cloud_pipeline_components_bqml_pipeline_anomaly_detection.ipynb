{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Anomaly detection with BigQuery ML and Vertex AI\n",
        "\n",
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/tree/main/notebooks/community/pipelines/google_cloud_pipeline_components_bqml_pipeline_anomaly_detection.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/pipelines/google_cloud_pipeline_components_bqml_pipeline_anomaly_detection.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/pipelines/google_cloud_pipeline_components_bqml_pipeline_anomaly_detection.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24743cf4a1e1"
      },
      "source": [
        "**_NOTE_**: This notebook has been tested in the following environment:\n",
        "\n",
        "* Python version = 3.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Anomaly detection is the identification of rare observations which deviate significantly from the data using ML. Anomaly detection can be done in many ways. Supervised, unsupervised, graph-based. It is particularly important for certain industries like telecommunications, manufacturing, and financial services.\n",
        "\n",
        "For instance, in a manufacturing scenario, you may collect some sensor data to predict the number remaining cycles before engine failure (TTF). In this way, you can take actionable decisions about maintenance planning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d975e698c9a4"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In the absence of labelled data, you may wonder how to best create an anomaly detector.\n",
        "\n",
        "In this notebook, you learn how to use autoencoders to detect anomalies from turbo fan engine data, and from there build an anomaly detection pipeline.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services and resources:\n",
        "\n",
        "- `Vertex AI Pipelines`\n",
        "- `BigQuery ML pipeline components`\n",
        "\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Define a custom evaluation and metrics visualization components\n",
        "- Define a pipeline:\n",
        "  - Build training dataset in BigQuery\n",
        "  - Train a BigQuery AutoEncoder model\n",
        "  - Evaluate the BigQuery AutoEncoder model\n",
        "  - Check the model performance\n",
        "  - Build test dataset in BigQuery\n",
        "  - Detect anomalies\n",
        "  - Generate the MSE plot to evaluate predictions\n",
        "- Compile the pipeline.\n",
        "- Execute the pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08d289fa873f"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The [`NASA Turbofan Jet Engine Data Set`](https://www.kaggle.com/datasets/behrad3d/nasa-cmaps) is a multivariate time series where time series describes a different engine.\n",
        "\n",
        "The dataset contains 26 columns and it consists data taken during a single operational cycle.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aed92deeb4a0"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* BigQuery\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing),\n",
        "[BigQuery pricing](https://cloud.google.com/bigquery/pricing)\n",
        "and [Cloud Storage pricing](https://cloud.google.com/storage/pricing),\n",
        "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7EUnXsZhAGF"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Install the following packages required to execute this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b4ef9b72d43"
      },
      "outputs": [],
      "source": [
        "# Install the packages\n",
        "! pip3 install --user --upgrade jinja2 google-cloud-bigquery kfp google-cloud-aiplatform google_cloud_pipeline_components -q --no-warn-conflicts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58707a750154"
      },
      "source": [
        "### Colab only: Uncomment the following cell to restart the kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f200f10a1da3"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "# import IPython\n",
        "\n",
        "# app = IPython.Application.instance()\n",
        "# app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF1j6f9HApxa"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "3. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "4. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WReHDGG5g0XY"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, try the following:\n",
        "* Run `gcloud config list`.\n",
        "* Run `gcloud projects list`.\n",
        "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM1iC_MfAts1"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXtUY-LAEB7c"
      },
      "outputs": [],
      "source": [
        "REGION = \"us-central1\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBCra4QMA2wR"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74ccc9e52986"
      },
      "source": [
        "**1. Vertex AI Workbench**\n",
        "* Do nothing as you are already authenticated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de775a3773ba"
      },
      "source": [
        "**2. Local JupyterLab instance, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "254614fa0c46"
      },
      "outputs": [],
      "source": [
        "# ! gcloud auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef21552ccea8"
      },
      "source": [
        "**3. Colab, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "603adbbf0532"
      },
      "outputs": [],
      "source": [
        "# from google.colab import auth\n",
        "# auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6b2ccc891ed"
      },
      "source": [
        "**4. Service account or other**\n",
        "* See how to grant Cloud Storage permissions to your service account at https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgPO1eR3CYjk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "Create a storage bucket to store intermediate artifacts such as datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzGDU7TWdts_"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = \"gs://your-bucket-name-unique\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EcIXiGsCePi"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIq7R4HZCfIc"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gc1ubsMoF7wn"
      },
      "source": [
        "### Set project template\n",
        "\n",
        "You create a set of repositories to organize your project locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "420y8i4KF_z4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "KFP_COMPONENTS_PATH = \"components\"\n",
        "PIPELINES_PATH = \"pipelines\"\n",
        "TRAIN_PIPELINES_PATH = os.path.join(PIPELINES_PATH, \"train_pipelines\")\n",
        "TEST_PIPELINES_PATH = os.path.join(PIPELINES_PATH, \"test_pipelines\")\n",
        "\n",
        "! mkdir -m 777 -p {KFP_COMPONENTS_PATH} {TRAIN_PIPELINES_PATH} {TEST_PIPELINES_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRRE4t_bdzj_"
      },
      "source": [
        "### Prepare the training data\n",
        "\n",
        "Next, you make a copy of the CSV training data into your Cloud Storage bucket and then create a BigQuery dataset table for the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u7VsRuLaaVC"
      },
      "outputs": [],
      "source": [
        "PUBLIC_DATA_URI = (\n",
        "    \"gs://cloud-samples-data/vertex-ai/pipeline-deployment/datasets/turbofan_anomaly\"\n",
        ")\n",
        "GCS_TRAIN_URI = f\"{PUBLIC_DATA_URI}/train_FD001.csv\"\n",
        "GCS_TEST_URI = f\"{PUBLIC_DATA_URI}/test_FD001.csv\"\n",
        "GCS_LABELS_URI = f\"{PUBLIC_DATA_URI}/RUL_FD001.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf3vIGViHYo3"
      },
      "source": [
        "### Set the BigQuery datasets\n",
        "\n",
        "You create the following BigQuery datasets for the tutorial:\n",
        "\n",
        "- `sensors_train_raw_data_<timestamp>` contains training data collected from sensors\n",
        "- `sensors_test_raw_data_<timestamp>` contains testing data collected from sensors\n",
        "- `sensors_label_data_<timestamp>` contains testing label collected to validate results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-2dBWfq1FPq"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZfxQL8JHdHE"
      },
      "outputs": [],
      "source": [
        "LOCATION = REGION.split(\"-\")[0]\n",
        "BQ_DATASET = \"iot_dataset\"\n",
        "BQ_TRAIN_RAW_TABLE = f\"sensors_train_raw_data_{TIMESTAMP}\"\n",
        "BQ_TEST_RAW_TABLE = f\"sensors_test_raw_data_{TIMESTAMP}\"\n",
        "BQ_LABELS_TABLE = f\"sensors_label_data_{TIMESTAMP}\"\n",
        "\n",
        "! bq mk --location={LOCATION} --dataset {PROJECT_ID}:{BQ_DATASET}\n",
        "\n",
        "! bq load \\\n",
        "  --location={LOCATION} \\\n",
        "  --source_format=CSV \\\n",
        "  --skip_leading_rows=1 \\\n",
        "  {BQ_DATASET}.{BQ_TRAIN_RAW_TABLE} \\\n",
        "  {GCS_TRAIN_URI} \\\n",
        "  id:INT64,cycle:INT64,setting1:FLOAT64,setting2:FLOAT64,setting3:FLOAT64,sensor:STRING,value:FLOAT64\n",
        "\n",
        "! bq load \\\n",
        "  --location={LOCATION} \\\n",
        "  --source_format=CSV \\\n",
        "  --skip_leading_rows=1 \\\n",
        "  {BQ_DATASET}.{BQ_TEST_RAW_TABLE} \\\n",
        "  {GCS_TEST_URI} \\\n",
        "  id:INT64,cycle:INT64,setting1:FLOAT64,setting2:FLOAT64,setting3:FLOAT64,sensor:STRING,value:FLOAT64\n",
        "\n",
        "! bq load \\\n",
        "  --location={LOCATION} \\\n",
        "  --source_format=CSV \\\n",
        "  --skip_leading_rows=1 \\\n",
        "  {BQ_DATASET}.{BQ_LABELS_TABLE} \\\n",
        "  {GCS_LABELS_URI} \\\n",
        "  id:INT64,time_to_failure:FLOAT64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "960505627ddf"
      },
      "source": [
        "### Import libraries\n",
        "\n",
        "Next, import libraries and set up some variables used throughout the tutorial.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "outputs": [],
      "source": [
        "from typing import NamedTuple\n",
        "\n",
        "import tensorflow as tf\n",
        "from google.cloud import aiplatform as vertex_ai\n",
        "from google.cloud import bigquery\n",
        "from google_cloud_pipeline_components.v1.bigquery import (\n",
        "    BigqueryCreateModelJobOp, BigqueryEvaluateModelJobOp, BigqueryQueryJobOp)\n",
        "from jinja2 import Template\n",
        "from kfp.v2 import compiler, dsl\n",
        "from kfp.v2.dsl import (HTML, Artifact, Condition, Input, Metrics, Output,\n",
        "                        component)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOskzw0enAyi"
      },
      "source": [
        "###Â Set up variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Sf2DSzwnD8H"
      },
      "outputs": [],
      "source": [
        "# SQL templates\n",
        "SENSORS = (\n",
        "    \"s1\",\n",
        "    \"s2\",\n",
        "    \"s3\",\n",
        "    \"s4\",\n",
        "    \"s5\",\n",
        "    \"s6\",\n",
        "    \"s7\",\n",
        "    \"s8\",\n",
        "    \"s9\",\n",
        "    \"s10\",\n",
        "    \"s11\",\n",
        "    \"s12\",\n",
        "    \"s13\",\n",
        "    \"s14\",\n",
        "    \"s15\",\n",
        "    \"s16\",\n",
        "    \"s17\",\n",
        "    \"s18\",\n",
        "    \"s19\",\n",
        "    \"s20\",\n",
        "    \"s21\",\n",
        ")\n",
        "WINDOW = 5\n",
        "PERIOD = 30\n",
        "TARGET = \"is_anomalous_ttf\"\n",
        "EXCLUDED_VARIABLES = \"id, cycle, setting1, setting2, setting3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyDIqmRWsjL4"
      },
      "source": [
        "### Helper functions\n",
        "\n",
        "The `print_pipeline_output` helper function allows to validate the pipeline run checking for executed job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3m59SG8Vsk50"
      },
      "outputs": [],
      "source": [
        "def print_pipeline_output(pipeline_root, job, output_task_name):\n",
        "    JOB_ID = job.name\n",
        "    print(JOB_ID)\n",
        "    for _ in range(len(job.gca_resource.job_detail.task_details)):\n",
        "        TASK_ID = job.gca_resource.job_detail.task_details[_].task_id\n",
        "        EXECUTE_OUTPUT = (\n",
        "            pipeline_root\n",
        "            + \"/\"\n",
        "            + PROJECT_NUMBER\n",
        "            + \"/\"\n",
        "            + JOB_ID\n",
        "            + \"/\"\n",
        "            + output_task_name\n",
        "            + \"_\"\n",
        "            + str(TASK_ID)\n",
        "            + \"/executor_output.json\"\n",
        "        )\n",
        "        GCP_RESOURCES = (\n",
        "            pipeline_root\n",
        "            + \"/\"\n",
        "            + PROJECT_NUMBER\n",
        "            + \"/\"\n",
        "            + JOB_ID\n",
        "            + \"/\"\n",
        "            + output_task_name\n",
        "            + \"_\"\n",
        "            + str(TASK_ID)\n",
        "            + \"/gcp_resources\"\n",
        "        )\n",
        "        EVAL_METRICS = (\n",
        "            pipeline_root\n",
        "            + \"/\"\n",
        "            + PROJECT_NUMBER\n",
        "            + \"/\"\n",
        "            + JOB_ID\n",
        "            + \"/\"\n",
        "            + output_task_name\n",
        "            + \"_\"\n",
        "            + str(TASK_ID)\n",
        "            + \"/evaluation_metrics\"\n",
        "        )\n",
        "        if tf.io.gfile.exists(EXECUTE_OUTPUT):\n",
        "            ! gsutil cat $EXECUTE_OUTPUT\n",
        "            return EXECUTE_OUTPUT\n",
        "        elif tf.io.gfile.exists(GCP_RESOURCES):\n",
        "            ! gsutil cat $GCP_RESOURCES\n",
        "            return GCP_RESOURCES\n",
        "        elif tf.io.gfile.exists(EVAL_METRICS):\n",
        "            ! gsutil cat $EVAL_METRICS\n",
        "            return EVAL_METRICS\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTWzEnp4EB7e"
      },
      "outputs": [],
      "source": [
        "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9iMCmfFBn6e"
      },
      "source": [
        "### Initialize BigQuery SDK for Python\n",
        "\n",
        "Initialize the BigQuery SDK for Python for your project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Md0UdedBn6f"
      },
      "outputs": [],
      "source": [
        "bq_client = bigquery.Client(project=PROJECT_ID, location=REGION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpNpPUp0m80Y"
      },
      "source": [
        "## BigQuery ML pipeline formalization\n",
        "\n",
        "In the next cells, you build the components and pipeline to train and evaluate the anomaly detection model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLwG5IBnF_f0"
      },
      "source": [
        "### Set variables for running the pipeline\n",
        "\n",
        "Below you initialize a set of variables that are specific to the pipeline run you are going to run in this tutorial. For instance, you define the pipeline configuration passing training table name, model configuration and performance threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2mtPcwyGE1P"
      },
      "outputs": [],
      "source": [
        "# BQML pipeline job configuation\n",
        "TRAIN_PIPELINE_NAME = \"bqml-anomaly-detection-train-pipeline\"\n",
        "TRAIN_PIPELINE_ROOT = (\n",
        "    urlparse(BUCKET_URI)._replace(path=\"pipelines/train_pipelines\").geturl()\n",
        ")\n",
        "TRAIN_PIPELINE_PACKAGE = os.path.join(\n",
        "    TRAIN_PIPELINES_PATH, f\"{TRAIN_PIPELINE_NAME}.json\"\n",
        ")\n",
        "\n",
        "# BQML pipeline conponent configuration\n",
        "BQ_TRAIN_FEATURES_TABLE_PREFIX = \"train_features\"\n",
        "BQ_TEST_FEATURES_TABLE_PREFIX = \"test_features\"\n",
        "BQ_TRAIN_TABLE_PREFIX = \"train_dataset\"\n",
        "BQ_TEST_TABLE_PREFIX = \"test_dataset\"\n",
        "BQ_RECOSTRUCTION_MODEL_TABLE_PREFIX = \"reconstruction_model\"\n",
        "DETECT_ANOMALIES_TABLE_PREFIX = \"detect_anomalies\"\n",
        "BQ_TRAIN_FEATURES_TABLE = f\"{BQ_TRAIN_FEATURES_TABLE_PREFIX}_{TIMESTAMP}\"\n",
        "BQ_TEST_FEATURES_TABLE = f\"{BQ_TEST_FEATURES_TABLE_PREFIX}_{TIMESTAMP}\"\n",
        "BQ_TRAIN_TABLE = f\"{BQ_TRAIN_TABLE_PREFIX}_{TIMESTAMP}\"\n",
        "BQ_TEST_TABLE = f\"{BQ_TEST_TABLE_PREFIX}_{TIMESTAMP}\"\n",
        "BQ_RECOSTRUCTION_MODEL_TABLE = f\"{BQ_RECOSTRUCTION_MODEL_TABLE_PREFIX}_{TIMESTAMP}\"\n",
        "DETECT_ANOMALIES_TABLE = f\"{DETECT_ANOMALIES_TABLE_PREFIX}_{TIMESTAMP}\"\n",
        "CONTAMINATION_THRESHOLD = 0.1\n",
        "PERF_THRESHOLD = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m59LNgggyEYG"
      },
      "source": [
        "### Set SQL queries using templates\n",
        "\n",
        "One way to run BigQuery and BigQuery ML pipelines on Vertex AI is defining sql queries as Jinja templates and pass them as parameters of `pipeline components`.\n",
        "\n",
        "In this tutorial, you define the following templates:\n",
        "\n",
        "  - `CREATE_FEATURES_SQL_TEMPLATE` to run feature engineering\n",
        "  - `CREATE_TRAIN_SQL_TEMPLATE` to create the training dataset\n",
        "  - `TRAIN_RECONSTRUCTION_MODEL_TEMPLATE` to build a reconstruction model using BigQuery ML AutoEncoder model\n",
        "  - `CREATE_TEST_SQL_TEMPLATE` to create the testing dataset\n",
        "  - `DETECT_ANOMALIES_TEMPLATE` to detect anomalies\n",
        "  - `VISUALIZE_MSE_TEMPLATE` to visualize MSE plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4c0wqn5GE8w"
      },
      "source": [
        "#### Define SQL query templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gss6JR0YyaS3"
      },
      "outputs": [],
      "source": [
        "# Training ---------------------------------------------------------------------\n",
        "CREATE_FEATURES_SQL_TEMPLATE = \"\"\"\n",
        "CREATE OR REPLACE TABLE\n",
        "  `{{project_id}}.{{bq_dataset}}.{{features_table}}` AS\n",
        "WITH\n",
        "  get_long_from_wide_table AS (\n",
        "    SELECT *\n",
        "    FROM `{{project_id}}.{{bq_dataset}}.{{data_table}}`\n",
        "    PIVOT(MAX(value) FOR sensor IN {{sensors}})\n",
        "  ),\n",
        "\n",
        "  get_features_table AS (\n",
        "    SELECT\n",
        "    *,\n",
        "    {%- for sensor in sensors %}\n",
        "    -- calculate rolling average sensor value\n",
        "    AVG({{sensor}}) OVER(PARTITION BY id ORDER BY cycle RANGE BETWEEN {{window}} PRECEDING AND CURRENT ROW) AS {{\"rolling_avg_\" ~ sensor}},\n",
        "    -- calculate rolling stdev sensor value\n",
        "    IFNULL(STDDEV({{sensor}}) OVER(PARTITION BY id ORDER BY cycle RANGE BETWEEN {{window}} PRECEDING AND CURRENT ROW), 0) AS {{\"rolling_sd_\" ~ sensor}}\n",
        "    {%- if not loop.last -%}\n",
        "        ,\n",
        "    {%- endif -%}\n",
        "    {%- endfor %}\n",
        "    FROM get_long_from_wide_table\n",
        "  )\n",
        "\n",
        "  SELECT * FROM get_features_table ORDER BY id, cycle\n",
        "\"\"\"\n",
        "\n",
        "CREATE_TRAIN_SQL_TEMPLATE = \"\"\"\n",
        "DECLARE period INT64 DEFAULT {{period}};\n",
        "\n",
        "CREATE OR REPLACE TABLE\n",
        "  `{{project_id}}.{{bq_dataset}}.{{train_table}}` AS\n",
        "WITH\n",
        "  get_last_cycle AS (\n",
        "    SELECT id, max(cycle) as last_cycle\n",
        "    FROM `{{project_id}}.{{bq_dataset}}.{{features_table}}`\n",
        "    GROUP BY id\n",
        "  ),\n",
        "\n",
        "  get_target_train AS (\n",
        "    SELECT\n",
        "    a.*,\n",
        "    CASE WHEN (b.last_cycle - a.cycle) < period THEN 1 ELSE 0 END AS {{target}},\n",
        "    FROM `{{project_id}}.{{bq_dataset}}.{{features_table}}` as a\n",
        "    LEFT JOIN get_last_cycle as b on a.id = b.id\n",
        "  )\n",
        "\n",
        "  SELECT * EXCEPT({{excluded_variables}}) FROM get_target_train\n",
        "\"\"\"\n",
        "\n",
        "TRAIN_RECONSTRUCTION_MODEL_TEMPLATE = \"\"\"\n",
        "CREATE OR REPLACE MODEL `{{project_id}}.{{bq_dataset}}.{{recostruction_model_name}}`\n",
        "OPTIONS(MODEL_TYPE='AUTOENCODER',\n",
        "        ACTIVATION_FN='RELU',\n",
        "        HIDDEN_UNITS=[32, 16, 4, 16, 32],\n",
        "        BATCH_SIZE=8,\n",
        "        DROPOUT=0.2,\n",
        "        EARLY_STOP=TRUE,\n",
        "        LEARN_RATE=0.001,\n",
        "        L1_REG_ACTIVATION=0.0001,\n",
        "        OPTIMIZER='ADAM',\n",
        "        MODEL_REGISTRY = 'vertex_ai',\n",
        "        VERTEX_AI_MODEL_ID = 'reconstruction_model',\n",
        "        VERTEX_AI_MODEL_VERSION_ALIASES = ['staging']\n",
        "        )\n",
        "AS SELECT * FROM `{{project_id}}.{{bq_dataset}}.{{train_table}}`\n",
        "\"\"\"\n",
        "\n",
        "# Test -------------------------------------------------------------------------\n",
        "CREATE_TEST_SQL_TEMPLATE = \"\"\"\n",
        "DECLARE period INT64 DEFAULT {{period}};\n",
        "\n",
        "CREATE OR REPLACE TABLE\n",
        " `{{project_id}}.{{bq_dataset}}.{{test_table}}` AS\n",
        "WITH\n",
        " get_last_cycle AS (\n",
        "   SELECT id, max(cycle) as last_cycle\n",
        "   FROM `{{project_id}}.{{bq_dataset}}.{{features_table}}`\n",
        "   GROUP BY id\n",
        " ),\n",
        "\n",
        " get_target_test AS (\n",
        "   SELECT\n",
        "   a.*\n",
        "   FROM `{{project_id}}.{{bq_dataset}}.{{features_table}}` as a\n",
        "   LEFT JOIN get_last_cycle as b ON a.id = b.id\n",
        "   WHERE a.cycle = b.last_cycle\n",
        " )\n",
        "\n",
        " SELECT\n",
        " a.*,\n",
        " CASE WHEN b.time_to_failure < period THEN 1 ELSE 0 END AS {{target}}\n",
        " FROM get_target_test as a\n",
        " LEFT JOIN `{{project_id}}.{{bq_dataset}}.{{labels_table}}` as b ON a.id = b.id\n",
        "\"\"\"\n",
        "\n",
        "DETECT_ANOMALIES_TEMPLATE = \"\"\"\n",
        "CREATE OR REPLACE TABLE\n",
        "  `{{project_id}}.{{bq_dataset}}.{{anomalies_table}}` AS\n",
        "SELECT\n",
        "  is_anomaly, mean_squared_error, {{target}}\n",
        "FROM\n",
        "  ML.DETECT_ANOMALIES(MODEL `{{project_id}}.{{bq_dataset}}.{{recostruction_model_name}}`,\n",
        "                      STRUCT({{contamination_thr}} AS contamination),\n",
        "                      TABLE `{{project_id}}.{{bq_dataset}}.{{test_table}}`)\n",
        "\"\"\"\n",
        "\n",
        "VISUALIZE_MSE_TEMPLATE = \"\"\"\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  `{{project_id}}.{{bq_dataset}}.{{anomalies_table}}`\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIUYHHbiGPWe"
      },
      "source": [
        "#### Compile SQL query templates\n",
        "\n",
        "After defining the SQL query templates, you compile them passing training and testing parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lD-Q1Qek6Xrc"
      },
      "outputs": [],
      "source": [
        "# Training parameters specification\n",
        "TRAIN_SQL_PARAMS = dict(\n",
        "    project_id=PROJECT_ID,\n",
        "    bq_dataset=BQ_DATASET,\n",
        "    sensors=SENSORS,\n",
        "    period=PERIOD,\n",
        "    window=WINDOW,\n",
        "    target=TARGET,\n",
        "    excluded_variables=EXCLUDED_VARIABLES,\n",
        "    contamination_threshold=CONTAMINATION_THRESHOLD,\n",
        "    data_table=BQ_TRAIN_RAW_TABLE,\n",
        "    features_table=BQ_TRAIN_FEATURES_TABLE,\n",
        "    train_table=BQ_TRAIN_TABLE,\n",
        "    recostruction_model_name=BQ_RECOSTRUCTION_MODEL_TABLE,\n",
        "    anomalies_table=DETECT_ANOMALIES_TABLE,\n",
        "    contamination_thr=CONTAMINATION_THRESHOLD,\n",
        ")\n",
        "\n",
        "CREATE_TRAIN_FEATURES_QUERY = Template(CREATE_FEATURES_SQL_TEMPLATE).render(\n",
        "    TRAIN_SQL_PARAMS\n",
        ")\n",
        "CREATE_TRAIN_TABLE_QUERY = Template(CREATE_TRAIN_SQL_TEMPLATE).render(TRAIN_SQL_PARAMS)\n",
        "TRAIN_RECOSTRUCTION_MODEL_QUERY = Template(TRAIN_RECONSTRUCTION_MODEL_TEMPLATE).render(\n",
        "    TRAIN_SQL_PARAMS\n",
        ")\n",
        "\n",
        "# Testing parameters specification\n",
        "TEST_SQL_PARAMS = dict(\n",
        "    project_id=PROJECT_ID,\n",
        "    bq_dataset=BQ_DATASET,\n",
        "    sensors=SENSORS,\n",
        "    period=PERIOD,\n",
        "    window=WINDOW,\n",
        "    data_table=BQ_TEST_RAW_TABLE,\n",
        "    labels_table=BQ_LABELS_TABLE,\n",
        "    target=TARGET,\n",
        "    features_table=BQ_TEST_FEATURES_TABLE,\n",
        "    test_table=BQ_TEST_TABLE,\n",
        "    recostruction_model_name=BQ_RECOSTRUCTION_MODEL_TABLE,\n",
        "    anomalies_table=DETECT_ANOMALIES_TABLE,\n",
        "    contamination_thr=CONTAMINATION_THRESHOLD,\n",
        ")\n",
        "\n",
        "CREATE_TEST_FEATURES_QUERY = Template(CREATE_FEATURES_SQL_TEMPLATE).render(\n",
        "    TEST_SQL_PARAMS\n",
        ")\n",
        "CREATE_TEST_TABLE_QUERY = Template(CREATE_TEST_SQL_TEMPLATE).render(TEST_SQL_PARAMS)\n",
        "DETECT_ANOMALIES_QUERY = Template(DETECT_ANOMALIES_TEMPLATE).render(TEST_SQL_PARAMS)\n",
        "VISUALIZE_MSE_QUERY = Template(VISUALIZE_MSE_TEMPLATE).render(TRAIN_SQL_PARAMS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZHOP2TzBn6k"
      },
      "source": [
        "### Create a custom component to read model evaluation metrics\n",
        "\n",
        "Build a custom component to consume model evaluation metrics for visualizations in the Vertex AI Pipelines UI using Kubeflow SDK visualization APIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iybwx_Z4Bn6k"
      },
      "outputs": [],
      "source": [
        "@component(\n",
        "    base_image=\"python:3.8-slim\",\n",
        "    output_component_file=f\"{KFP_COMPONENTS_PATH}/build_bq_evaluate_metrics.yaml\",\n",
        ")\n",
        "def get_model_evaluation_metrics(\n",
        "    metrics_in: Input[Artifact],\n",
        "    metrics_out: Output[Metrics],\n",
        "    model_out: Output[Artifact],\n",
        ") -> NamedTuple(\"Outputs\", [(\"mean_squared_error\", float)]):\n",
        "    \"\"\"\n",
        "    Get the average mean absolute error from the metrics\n",
        "    Args:\n",
        "        metrics_in: metrics artifact\n",
        "        metrics_out: resulting metrics artifact\n",
        "        model_out: resulting model artifact\n",
        "    Returns:\n",
        "        avg_mean_absolute_error: average mean absolute error\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract rows and schema from metrics artifact\n",
        "    rows = metrics_in.metadata[\"rows\"]\n",
        "    schema = metrics_in.metadata[\"schema\"]\n",
        "\n",
        "    # Convert into a dictionary format\n",
        "    columns = [metrics[\"name\"] for metrics in schema[\"fields\"] if \"name\" in metrics]\n",
        "    records = [dl[\"v\"] for dl in rows[0][\"f\"]]\n",
        "    metrics = {key: round(float(value), 3) for key, value in zip(columns, records)}\n",
        "\n",
        "    # Log metrics\n",
        "    for key in metrics.keys():\n",
        "        metrics_out.log_metric(key, metrics[key])\n",
        "\n",
        "    # Return the target metrics\n",
        "    mean_absolute_error = metrics[\"mean_squared_error\"]\n",
        "    component_outputs = NamedTuple(\"Outputs\", [(\"mean_squared_error\", float)])\n",
        "\n",
        "    # model metadata\n",
        "    model_framework = \"BQML\"\n",
        "    model_type = \"AutoEncoder\"\n",
        "    model_user = \"Author\"\n",
        "    model_function = \"Reconstruction model\"\n",
        "    model_out.metadata[\"framework\"] = model_framework\n",
        "    model_out.metadata[\"type\"] = model_type\n",
        "    model_out.metadata[\"model function\"] = model_function\n",
        "    model_out.metadata[\"modified by\"] = model_user\n",
        "\n",
        "    return component_outputs(mean_absolute_error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEVHpbmiUmzB"
      },
      "source": [
        "### Create a custom component to visualize MSE per label\n",
        "\n",
        "Build a custom component to visualize MSE per label in the Vertex AI Pipelines UI using Kubeflow SDK visualization APIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjJWfui7U9ux"
      },
      "outputs": [],
      "source": [
        "@component(\n",
        "    base_image=\"python:3.8-slim\",\n",
        "    packages_to_install=[\"pandas\", \"google-cloud-bigquery[bqstorage,pandas]\", \"plotly\"],\n",
        "    output_component_file=f\"{KFP_COMPONENTS_PATH}/build_evaluation_plot.yaml\",\n",
        ")\n",
        "def get_mse_plots(\n",
        "    query: str,\n",
        "    project: str,\n",
        "    location: str,\n",
        "    metrics_out: Output[HTML],\n",
        "    model_out: Output[Artifact],\n",
        "):\n",
        "    \"\"\"\n",
        "    Get the mean squared error per labels\n",
        "    Args:\n",
        "        query: the query to generate the metrics\n",
        "        project: the project id to iniziate the BQ client\n",
        "        location: the region to iniziate the BQ client\n",
        "        metrics_out: resulting metrics artifact\n",
        "        model_out: resulting model artifact\n",
        "    Returns:\n",
        "        avg_mean_absolute_error: average mean absolute error\n",
        "    \"\"\"\n",
        "\n",
        "    import plotly.graph_objects as go\n",
        "    from google.cloud import bigquery\n",
        "    from plotly.subplots import make_subplots\n",
        "\n",
        "    # Initiate client\n",
        "    client = bigquery.Client(project=project, location=location)\n",
        "\n",
        "    # Run a Standard SQL query using the environment's default project\n",
        "    table_df = client.query(query).to_dataframe()\n",
        "\n",
        "    # Create anomalies/no anomalies datasets\n",
        "    anomalies_df = table_df.query(\"is_anomalous_ttf == 1\")\n",
        "    no_anomalies_df = table_df.query(\"is_anomalous_ttf == 0\")\n",
        "\n",
        "    # Create a figure with subplots\n",
        "    fig = make_subplots(\n",
        "        rows=2,\n",
        "        cols=2,\n",
        "        specs=[[{\"colspan\": 2}, None], [{}, {}]],\n",
        "        subplot_titles=(\n",
        "            \"Distribution of mean squared error (MSE) for anomaly and not anomaly sensor data\",\n",
        "            \"Distribution of mean squared error (MSE) for anomaly sensor data\",\n",
        "            \"Distribution of mean squared error (MSE) for not anomaly sensor data\",\n",
        "        ),\n",
        "        x_title=\"Mean squared error (MSE)\",\n",
        "        y_title=\"Density\",\n",
        "    )\n",
        "\n",
        "    # Add subplots to figure\n",
        "    fig.add_trace(\n",
        "        go.Histogram(\n",
        "            x=anomalies_df[\"mean_squared_error\"],\n",
        "            name=\"Anomaly\",\n",
        "            marker_color=\"blue\",\n",
        "            showlegend=True,\n",
        "        ),\n",
        "        row=1,\n",
        "        col=1,\n",
        "    )\n",
        "    fig.add_trace(\n",
        "        go.Histogram(\n",
        "            x=no_anomalies_df[\"mean_squared_error\"],\n",
        "            name=\"No Anomaly\",\n",
        "            marker_color=\"orange\",\n",
        "            showlegend=True,\n",
        "        ),\n",
        "        row=1,\n",
        "        col=1,\n",
        "    )\n",
        "    fig.add_trace(\n",
        "        go.Histogram(\n",
        "            x=anomalies_df[\"mean_squared_error\"],\n",
        "            name=\"MSE_1\",\n",
        "            marker_color=\"red\",\n",
        "            showlegend=False,\n",
        "        ),\n",
        "        row=2,\n",
        "        col=1,\n",
        "    )\n",
        "    fig.add_trace(\n",
        "        go.Histogram(\n",
        "            x=no_anomalies_df[\"mean_squared_error\"],\n",
        "            name=\"MSE_2\",\n",
        "            marker_color=\"green\",\n",
        "            showlegend=False,\n",
        "        ),\n",
        "        row=2,\n",
        "        col=2,\n",
        "    )\n",
        "\n",
        "    # Update figure properties\n",
        "    fig.update_layout(\n",
        "        title=\"Anomaly detection report\",\n",
        "        title_x=0.5,\n",
        "        bargap=0.2,\n",
        "        bargroupgap=0.1,\n",
        "        showlegend=True,\n",
        "    )\n",
        "\n",
        "    # Save output to static HTML file\n",
        "    fig.write_html(metrics_out.path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcSL1FHk69KT"
      },
      "source": [
        "### Build the BQML training pipeline\n",
        "\n",
        "Define your workflow using Kubeflow Pipelines DSL package.\n",
        "\n",
        "Below you have the steps of the pipeline workflow:\n",
        "\n",
        "1. Build training dataset in BigQuery\n",
        "2. Train a BigQuery AutoEncoder model\n",
        "3. Evaluate the BigQuery AutoEncoder model\n",
        "4. Check the model performance\n",
        "5. Build test dataset in BigQuery\n",
        "6. Detect anomalies\n",
        "7. Generate the MSE plot to evaluate predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlFXqsPIAk0l"
      },
      "outputs": [],
      "source": [
        "@dsl.pipeline(\n",
        "    name=TRAIN_PIPELINE_NAME,\n",
        "    description=\"A batch pipeline to train recostruction model using BQML\",\n",
        ")\n",
        "def pipeline(\n",
        "    create_train_features_query: str,\n",
        "    create_train_table_query: str,\n",
        "    train_recostruction_model_query: str,\n",
        "    create_test_features_query: str,\n",
        "    create_test_table_query: str,\n",
        "    generate_anomalies_query: str,\n",
        "    performance_thr: float,\n",
        "    visualize_mse_query: str,\n",
        "    project: str,\n",
        "    location: str,\n",
        "):\n",
        "\n",
        "    # Create training features\n",
        "    create_train_features_op = BigqueryQueryJobOp(\n",
        "        query=create_train_features_query,\n",
        "        project=project,\n",
        "        location=location,\n",
        "    ).set_display_name(\"build train features\")\n",
        "\n",
        "    # Create train dataset\n",
        "    create_train_dataset_op = (\n",
        "        BigqueryQueryJobOp(\n",
        "            query=create_train_table_query, project=project, location=location\n",
        "        )\n",
        "        .set_display_name(\"build train table\")\n",
        "        .after(create_train_features_op)\n",
        "    )\n",
        "\n",
        "    # Train the recostruction model\n",
        "    bq_recostruction_model_op = (\n",
        "        BigqueryCreateModelJobOp(\n",
        "            query=train_recostruction_model_query,\n",
        "            project=project,\n",
        "            location=location,\n",
        "        )\n",
        "        .set_display_name(\"train reconstruction model\")\n",
        "        .after(create_train_dataset_op)\n",
        "    )\n",
        "\n",
        "    # Evaluate recostruction model\n",
        "    bq_arima_evaluate_model_op = (\n",
        "        BigqueryEvaluateModelJobOp(\n",
        "            model=bq_recostruction_model_op.outputs[\"model\"],\n",
        "            project=project,\n",
        "            location=location,\n",
        "        )\n",
        "        .set_display_name(\"evaluate reconstruction model\")\n",
        "        .after(bq_recostruction_model_op)\n",
        "    )\n",
        "\n",
        "    # Plot model metrics\n",
        "    get_evaluation_model_metrics_op = (\n",
        "        get_model_evaluation_metrics(\n",
        "            bq_arima_evaluate_model_op.outputs[\"evaluation_metrics\"]\n",
        "        )\n",
        "        .after(bq_arima_evaluate_model_op)\n",
        "        .set_display_name(\"generate evaluation metrics\")\n",
        "    )\n",
        "\n",
        "    # Check the model performance. If AUTOENCODER MSE metric is below to a minimal threshold\n",
        "    with Condition(\n",
        "        get_evaluation_model_metrics_op.outputs[\"mean_squared_error\"] < performance_thr,\n",
        "        name=\"MSE good\",\n",
        "    ):\n",
        "\n",
        "        # Create test features dataset\n",
        "        create_test_features_op = BigqueryQueryJobOp(\n",
        "            query=create_test_features_query,\n",
        "            project=project,\n",
        "            location=location,\n",
        "        ).set_display_name(\"build test features\")\n",
        "\n",
        "        # Create test dataset\n",
        "        create_test_dataset_op = (\n",
        "            BigqueryQueryJobOp(\n",
        "                query=create_test_table_query, project=project, location=location\n",
        "            )\n",
        "            .set_display_name(\"build test table\")\n",
        "            .after(create_test_features_op)\n",
        "        )\n",
        "\n",
        "        # Generate anomalies\n",
        "        generate_anomalies_op = (\n",
        "            BigqueryQueryJobOp(\n",
        "                query=generate_anomalies_query,\n",
        "                project=project,\n",
        "                location=location,\n",
        "            )\n",
        "            .after(create_test_dataset_op)\n",
        "            .set_display_name(\"generate anomalies\")\n",
        "        )\n",
        "\n",
        "        # Plot mse graph of anomalies\n",
        "        _ = (\n",
        "            get_mse_plots(query=visualize_mse_query, project=project, location=location)\n",
        "            .after(generate_anomalies_op)\n",
        "            .set_display_name(\"plot mse report\")\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nghLONQX7JNg"
      },
      "source": [
        "### Compile the pipeline into a JSON file\n",
        "\n",
        "Next, you compile the pipeline, which produces a JSON specification for your pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8l6IR7OoADJV"
      },
      "outputs": [],
      "source": [
        "compiler.Compiler().compile(pipeline_func=pipeline, package_path=TRAIN_PIPELINE_PACKAGE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gtlwu0Xo1WcT"
      },
      "source": [
        "### Execute your pipeline\n",
        "\n",
        "Next, you execute the pipeline. It takes the following parameters which you set as default:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzg2JDlsG2cd"
      },
      "source": [
        "#### Submit pipeline job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8meEMA6NO3aO"
      },
      "outputs": [],
      "source": [
        "TRAIN_PIPELINE_RUN_PARAMS = dict(\n",
        "    create_train_features_query=CREATE_TRAIN_FEATURES_QUERY,\n",
        "    create_train_table_query=CREATE_TRAIN_TABLE_QUERY,\n",
        "    train_recostruction_model_query=TRAIN_RECOSTRUCTION_MODEL_QUERY,\n",
        "    create_test_features_query=CREATE_TEST_FEATURES_QUERY,\n",
        "    create_test_table_query=CREATE_TEST_TABLE_QUERY,\n",
        "    generate_anomalies_query=DETECT_ANOMALIES_QUERY,\n",
        "    performance_thr=PERF_THRESHOLD,\n",
        "    visualize_mse_query=VISUALIZE_MSE_QUERY,\n",
        "    project=PROJECT_ID,\n",
        "    location=LOCATION,\n",
        ")\n",
        "\n",
        "bqml_train_pipeline = vertex_ai.PipelineJob(\n",
        "    display_name=f\"{TRAIN_PIPELINE_PACKAGE}-job\",\n",
        "    template_path=TRAIN_PIPELINE_PACKAGE,\n",
        "    parameter_values=TRAIN_PIPELINE_RUN_PARAMS,\n",
        "    pipeline_root=TRAIN_PIPELINE_ROOT,\n",
        "    enable_caching=True,\n",
        ")\n",
        "\n",
        "bqml_train_pipeline.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKgPkm4cgbHd"
      },
      "source": [
        "#### View BigQuery ML training pipeline results\n",
        "\n",
        "Finally, you will view the artifact outputs of each task in the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apGt59bCgbHd"
      },
      "outputs": [],
      "source": [
        "PROJECT_NUMBER = bqml_train_pipeline.gca_resource.name.split(\"/\")[1]\n",
        "print(\"PROJECT NUMBER: \", PROJECT_NUMBER)\n",
        "print(\"\\n\\n\")\n",
        "print(\"bigquery-create-model-job\")\n",
        "artifacts = print_pipeline_output(\n",
        "    TRAIN_PIPELINE_ROOT, bqml_train_pipeline, \"bigquery-create-model-job\"\n",
        ")\n",
        "print(\"\\n\\n\")\n",
        "print(\"bigquery-ml-evaluate-job\")\n",
        "artifacts = print_pipeline_output(\n",
        "    TRAIN_PIPELINE_ROOT, bqml_train_pipeline, \"bigquery-evaluate-model-job\"\n",
        ")\n",
        "print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcGuzM7nEqmj"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, you built a ML pipeline to train an autoencoder for detecting anomalies using Vertex AI Pipelines and BigQuery ML.\n",
        "\n",
        "Now you know how to leverage prebuilt `google_cloud_components` for training BigQuery ML model and how to build custom components to evaluate and visualize performance metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0Ks1UZpoRXS"
      },
      "outputs": [],
      "source": [
        "# delete pipeline\n",
        "delete_pipeline = False\n",
        "if delete_pipeline:\n",
        "    vertex_ai_pipeline_jobs = vertex_ai.PipelineJob.list(\n",
        "        filter=f'pipeline_name=\"{TRAIN_PIPELINE_NAME}\"'\n",
        "    )\n",
        "    for pipeline_job in vertex_ai_pipeline_jobs:\n",
        "        pipeline_job.delete()\n",
        "\n",
        "# delete model\n",
        "delete_model = False\n",
        "if delete_model:\n",
        "    DELETE_MODEL_SQL = f\"DROP MODEL {BQ_DATASET}.{BQ_RECOSTRUCTION_MODEL_TABLE}\"\n",
        "    try:\n",
        "        delete_model_query_job = bq_client.query(DELETE_MODEL_SQL)\n",
        "        delete_model_query_result = delete_model_query_job.result()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "# delete bucket\n",
        "delete_bucket = False\n",
        "if os.getenv(\"IS_TESTING\") or delete_bucket:\n",
        "    ! gsutil -m rm -r $BUCKET_URI\n",
        "\n",
        "# Remove local resorces\n",
        "delete_local_resources = False\n",
        "if delete_local_resources:\n",
        "    ! rm -rf {KFP_COMPONENTS_PATH}\n",
        "    ! rm -rf {TRAIN_PIPELINES_PATH}\n",
        "    ! rm -rf {TEST_PIPELINES_PATH}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "google_cloud_pipeline_components_bqml_pipeline_anomaly_detection.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Narwhalprime/vertex-ai-samples/blob/main/notebooks/community/pipelines/google_cloud_pipeline_components_cloud_natural_language_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1142fd18"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwO30Ag12YcB"
      },
      "source": [
        "# Vertex Pipelines: Cloud Natural Language model training pipeline\n",
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/pipelines/google_cloud_pipeline_components_cloud_natural_language_pipeline.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/pipelines/google_cloud_pipeline_components_cloud_natural_language_pipeline.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/ai/platform/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/natural_language/cloud_natural_language_pipeline.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "This notebook shows how to use [Google Cloud Pipeline Components SDK](https://cloud.google.com/vertex-ai/docs/pipelines/components-introduction) and additional components in this directory to run a machine learning pipeline in [Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction) to train a TensorFlow text classification model.\n",
        "\n",
        "In this pipeline, the model training Docker image utilizes [TFHub](https://tfhub.dev/) models to perform state-of-the-art text classification training. The image is pre-built and ready to use, so no additional Docker setup is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d975e698c9a4"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to construct an end-to-end training pipeine within Vertex AI pipelines that ingests a dataset, trains a text classification model on it, and outputs evaluation metrics.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services and resources:\n",
        "\n",
        "- Vertex AI Pipelines\n",
        "- Vertex AI Datasets\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Define Kubeflow pipeline components\n",
        "- Setup Kubeflow pipeline\n",
        "- Run pipeline on Vertex AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08d289fa873f"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "This notebook requires that the user has two datasets exported from Vertex AI [managed datasets](https://cloud.google.com/vertex-ai/docs/training/using-managed-datasets): one with train and validation data splits, and the other with test data used for evaluation. Please ensure no data is shared between the two datasets (in particular, no evaluation data should be part of the train or validation splits). To export a Vertex AI dataset, please follow the following public docs:\n",
        "* [Preparing data](https://cloud.google.com/vertex-ai/docs/text-data/classification/prepare-data)\n",
        "* [Creating a Vertex AI dataset](https://cloud.google.com/vertex-ai/docs/text-data/classification/create-dataset) from the above data\n",
        "* [Exporting dataset and its annotations](https://cloud.google.com/vertex-ai/docs/datasets/export-metadata-annotations); ensure the resulting export is located in a Google Cloud Storage (GCS) bucket you own. You may need to manually separate the test split data into its own file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aed92deeb4a0"
      },
      "source": [
        "## Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_local"
      },
      "source": [
        "## Setup\n",
        "\n",
        "If you are using Colab or Google Vertex AI Workbench Notebooks, your environment already meets all the requirements to run this notebook. You can skip this step.\n",
        "\n",
        "***NOTE***: This notebook has been tested in the following environment:\n",
        "\n",
        "* Python version = 3.8\n",
        "\n",
        "Otherwise, make sure your environment meets this notebook's requirements. You need the following:\n",
        "\n",
        "- The Cloud Storage SDK\n",
        "- Python 3\n",
        "- virtualenv\n",
        "- Jupyter notebook running in a virtual environment with Python 3\n",
        "\n",
        "The Cloud Storage guide to [Setting up a Python development environment](https://cloud.google.com/python/setup) and the [Jupyter installation guide](https://jupyter.org/install) provide detailed instructions for meeting these requirements. The following steps provide a condensed set of instructions:\n",
        "\n",
        "1. [Install and initialize the SDK](https://cloud.google.com/sdk/docs/).\n",
        "\n",
        "2. [Install Python 3](https://cloud.google.com/python/setup#installing_python).\n",
        "\n",
        "3. [Install virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv) and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
        "\n",
        "4. Activate that environment and run `pip3 install Jupyter` in a terminal shell to install Jupyter.\n",
        "\n",
        "5. Run `jupyter notebook` on the command line in a terminal shell to launch Jupyter.\n",
        "\n",
        "6. Open this notebook in the Jupyter Notebook Dashboard.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "568d5c16"
      },
      "source": [
        "### Install additional packages\n",
        "\n",
        "Run the following commands to setup the packages for this notebook. Note that the last code snippet in this section restarts your kernel in order to load the installs properly, so when initalizing this notebook from scratch, it is recommended to run up to that cell, then afterwards you may start running the cell after that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dac98aac"
      },
      "outputs": [],
      "source": [
        "# Install using pip3\n",
        "!pip3 install -U tensorflow google-cloud-pipeline-components google-cloud-aiplatform kfp==1.8.16 \"shapely<2\" -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alRWYgYTdz7P"
      },
      "outputs": [],
      "source": [
        "# Version check\n",
        "# This has been tested with KFP 1.8.16\n",
        "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
        "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0a15440"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9IYalYObAbY"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "3. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,storage.googleapis.com).\n",
        "\n",
        "4. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA_kzAIIj2G_"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using Vertex AI Workbench Notebooks**, your environment is already\n",
        "authenticated. Skip this step.\n",
        "\n",
        "**If you are using Colab**, run the cell below and follow the instructions\n",
        "when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "1. In the Cloud Console, go to the [**Create service account key**\n",
        "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
        "\n",
        "2. Click **Create service account**.\n",
        "\n",
        "3. In the **Service account name** field, enter a name, and\n",
        "   click **Create**.\n",
        "\n",
        "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
        "into the filter box, and select\n",
        "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "\n",
        "5. Click *Create*. A JSON file that contains your key downloads to your\n",
        "local environment.\n",
        "\n",
        "6. Enter the path to your service account key as the\n",
        "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "outputs": [],
      "source": [
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# If on Vertex AI Workbench, then don't execute this code\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
        "    \"DL_ANACONDA_HOME\"\n",
        "):\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "set_service_account"
      },
      "source": [
        "### Set project ID\n",
        "\n",
        "Set your project ID here. If you don't know this, the following snippet attempts to deterine this from your gcloud config. Please continue only if the notebook can see your desired project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkqEd5Gin9mn"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"your-project-id\"  # @param {type:\"string\"}\n",
        "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "print(\"Project ID:\", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVO_gUqpFEP2"
      },
      "outputs": [],
      "source": [
        "!gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a27d4cee"
      },
      "source": [
        "### Setup project information\n",
        "\n",
        "Enter information about your project and datasets here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e9477a2"
      },
      "outputs": [],
      "source": [
        "REGION = \"us\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "TRAINING_DATA_LOCATION = \"gs://your-training-data-location\"  # @param {type:\"string\"}\n",
        "TASK_TYPE = \"CLASSIFICATION\"  # @param [\"CLASSIFICATION\", \"MULTILABEL_CLASSIFICATION\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-MZnHsimbOH"
      },
      "outputs": [],
      "source": [
        "# Since we are training a custom model, we need to specify the list of possible\n",
        "# classes/labels.\n",
        "# e.g, [\"FirstClass\", \"SecondClass\"]\n",
        "# An additional class \"[UNK]\" will be added to the list indicating that none of\n",
        "# the specified labels are a match.\n",
        "CLASS_NAMES = [\"\"]\n",
        "\n",
        "# This is a list of GCS URIs; e.g., [\"gs://your-bucket-name-here/your-input-file.jsonl\"].\n",
        "TEST_DATA_URIS = [\"gs://your-bucket-name-here/your-input-file.jsonl\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "timestamp"
      },
      "source": [
        "#### UUID\n",
        "\n",
        "To avoid name collisions with other resources in your project, you can create a UUID with the code below and append it onto the name of the bucket(s) created in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wh9sgzemwLXE"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "\n",
        "# Generate a uuid of a specifed length(default=8)\n",
        "def generate_uuid(length: int = 8) -> str:\n",
        "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
        "\n",
        "\n",
        "UUID = generate_uuid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bucket:mbsdk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "When you initialize the Vertex AI SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bucket"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_bucket"
      },
      "outputs": [],
      "source": [
        "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
        "    BUCKET_NAME = PROJECT_ID + \"aip-\" + UUID\n",
        "    BUCKET_URI = \"gs://\" + BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_bucket"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dO0NV93IwLXF"
      },
      "outputs": [],
      "source": [
        "!gsutil mb -l $REGION $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "validate_bucket"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hg5f2oKBwLXG"
      },
      "outputs": [],
      "source": [
        "!gsutil ls -al $BUCKET_URI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuFETRptyKXc"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3a09765"
      },
      "source": [
        "## Create training pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89bb4a50"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0f361e65"
      },
      "outputs": [],
      "source": [
        "from google_cloud_pipeline_components.aiplatform import ModelBatchPredictOp\n",
        "from google_cloud_pipeline_components.experimental import natural_language\n",
        "from google_cloud_pipeline_components.experimental.evaluation import (\n",
        "    GetVertexModelOp, ModelEvaluationClassificationOp,\n",
        "    TargetFieldDataRemoverOp)\n",
        "from kfp import components\n",
        "from kfp.v2 import compiler, dsl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d33c87e4-2ada-4b87-bf75-064247f3162d"
      },
      "source": [
        "### Define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36ceb9f8"
      },
      "outputs": [],
      "source": [
        "# Worker pool specs\n",
        "TRAINING_MACHINE_TYPE = \"n1-highmem-8\"\n",
        "ACCELERATOR_TYPE = \"NVIDIA_TESLA_T4\"\n",
        "ACCELERATOR_COUNT = 1\n",
        "EVAL_MACHINE_TYPE = \"n1-highmem-8\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAaMJKrhAe5L"
      },
      "source": [
        "## Define components\n",
        "\n",
        "This pipeline is composed from the following components:\n",
        "\n",
        "- **train-tfhub-model** - Trains a new Tensorflow model using TFHub layers from pre-built Docker image\n",
        "- **upload-tensorflow-model-to-google-cloud-vertex-ai** - Uploads resulting model to Vertex AI model registry\n",
        "- **get-vertex-model** - Gets model that has just been uploaded as an artifact in pipeline\n",
        "- **convert-dataset-export-for-batch-predict** - Preprocessing component that takes the test dataset exported from Vertex datasets and converts it to a simpler compatible one that is readable from the batch predict component\n",
        "- **target-field-data-remover** - Removes the target field (i.e., label) in the test dataset for the downstream batch predict component\n",
        "- **model-batch-predict** - Performs a batch prediction job\n",
        "- **model-evaluation-classification** - Calculates the evaluation metrics from the above batch predict job and exports the metrics artifact\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKe2iQNKgpKG"
      },
      "outputs": [],
      "source": [
        "# Load upload TF model component\n",
        "upload_tensorflow_model_to_vertex_op = components.load_component_from_url(\n",
        "    \"https://raw.githubusercontent.com/Ark-kun/pipeline_components/c6a8b67d1ada2cc17665c99ff6b410df588bee28/components/google-cloud/Vertex_AI/Models/Upload_Tensorflow_model/workaround_for_buggy_KFPv2_compiler/component.yaml\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEnh9Pcx6Xfi"
      },
      "source": [
        "### Define the pipeline\n",
        "\n",
        "The pipeline performs the following steps:\n",
        "- Trains new text classification model\n",
        "- Uploads model to Vertex AI Model Registry\n",
        "- Performs preprocessing steps on test dataset export: formats data for batch predcition, removes target field\n",
        "- Performs batch prediction on preprocessed test data\n",
        "- Evaluates performance of model based on batch prediction output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2a67cde8"
      },
      "outputs": [],
      "source": [
        "@dsl.pipeline(name=\"text-classification-model\")\n",
        "def pipeline():\n",
        "    train_task = natural_language.TrainTextClassificationOp()(\n",
        "        project=PROJECT_ID,\n",
        "        location=LOCATION,\n",
        "        machine_type=TRAINING_MACHINE_TYPE,\n",
        "        accelerator_type=ACCELERATOR_TYPE,\n",
        "        accelerator_count=ACCELERATOR_COUNT,\n",
        "        input_data_path=TRAINING_DATA_LOCATION,\n",
        "        input_format=\"jsonl\",\n",
        "        natural_language_task_type=TASK_TYPE,\n",
        "    )\n",
        "\n",
        "    upload_task = upload_tensorflow_model_to_vertex_op(\n",
        "        model=train_task.outputs[\"model_output\"]\n",
        "    )\n",
        "\n",
        "    get_model_task = GetVertexModelOp(\n",
        "        model_resource_name=upload_task.outputs[\"model_name\"]\n",
        "    )\n",
        "\n",
        "    classification_type = (\n",
        "        \"multilabel\" if TASK_TYPE == \"MULTILABEL_CLASSIFICATION\" else \"multiclass\"\n",
        "    )\n",
        "\n",
        "    convert_dataset_task = natural_language.ConvertDatasetExportForBatchPredictOp(\n",
        "        file_paths=TEST_DATA_URIS, classification_type=classification_type\n",
        "    )\n",
        "\n",
        "    target_field_remover_task = TargetFieldDataRemoverOp(\n",
        "        project=PROJECT_ID,\n",
        "        location=LOCATION,\n",
        "        root_dir=BUCKET_URI,\n",
        "        gcs_source_uris=convert_dataset_task.outputs[\"output_files\"],\n",
        "        target_field_name=\"labels\",\n",
        "        instances_format=\"jsonl\",\n",
        "    )\n",
        "\n",
        "    # Note: ModelBatchPredictOp doesn't support accelerators currently.\n",
        "    batch_predict_task = ModelBatchPredictOp(\n",
        "        project=PROJECT_ID,\n",
        "        location=LOCATION,\n",
        "        model=get_model_task.outputs[\"model\"],\n",
        "        job_display_name=\"nl-batch-predict-evaluation\",\n",
        "        gcs_source_uris=target_field_remover_task.outputs[\"gcs_output_directory\"],\n",
        "        instances_format=\"jsonl\",\n",
        "        predictions_format=\"jsonl\",\n",
        "        gcs_destination_output_uri_prefix=BUCKET_URI,\n",
        "        machine_type=EVAL_MACHINE_TYPE,\n",
        "    )\n",
        "\n",
        "    # Note: Because we're running a custom training pipeline, the model source\n",
        "    # is detected as Custom and thus it doesn't use AutoML NL's default settings\n",
        "    # and fails if class_labels is excluded.\n",
        "    ModelEvaluationClassificationOp(\n",
        "        project=PROJECT_ID,\n",
        "        location=LOCATION,\n",
        "        root_dir=BUCKET_URI,\n",
        "        class_labels=CLASS_NAMES + [\"[UNK]\"],\n",
        "        predictions_gcs_source=batch_predict_task.outputs[\"gcs_output_directory\"],\n",
        "        predictions_format=\"jsonl\",\n",
        "        prediction_label_column=\"prediction.displayNames\",\n",
        "        prediction_score_column=\"prediction.confidences\",\n",
        "        ground_truth_gcs_source=convert_dataset_task.outputs[\"output_files\"],\n",
        "        ground_truth_format=\"jsonl\",\n",
        "        target_field_name=\"labels\",\n",
        "        classification_type=TASK_TYPE,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3211ba19"
      },
      "source": [
        "### Compile the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c368c73f"
      },
      "outputs": [],
      "source": [
        "compiler.Compiler().compile(pipeline, \"nl_pipeline.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_Vxwz5cdF5f"
      },
      "source": [
        "Running the above line will generate a file locally or in Colab's directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax0jOxIaholy"
      },
      "source": [
        "### Run the pipeline\n",
        "\n",
        "This sends a create pipeline job request to Vertex Pipelines. Note that this  task run synchronously and may take a while to complete.\n",
        "\n",
        "You may view the progress of the job at any time by clicking on the generated links (after \"View Pipeline Job\" in the console output of the cell below). Once the pipeline finishes, you may examine the artifacts produced from this pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wfs7QOSxhp_n"
      },
      "outputs": [],
      "source": [
        "job = aiplatform.PipelineJob(\n",
        "    display_name=\"nl_pipeline\",\n",
        "    template_path=\"nl_pipeline.json\",\n",
        "    location=LOCATION,\n",
        "    enable_caching=True,\n",
        "    parameter_values={},\n",
        ")\n",
        "\n",
        "job.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIyGPaihWJWn"
      },
      "source": [
        "Once the pipeline successfully finishes, go to the pipeline and examine the resulting metrics artifacts for the results. Otherwise, refer to the failing step(s) in the pipeline to determine the cause of any errors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoexTJTy9jnH"
      },
      "source": [
        "## View model evaluation results\n",
        "\n",
        "To check the results of evaluation after pipeline execution, find the \"model-evaluation-classification\" subdirectory in the Cloud Storage bucket created by this pipeline. You may also run the following to directly output the contents of the metrics file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9EqPCQF9lN9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "EVAL_TASK_NAME = \"model-evaluation-classification\"\n",
        "PROJECT_NUMBER = job.gca_resource.name.split(\"/\")[1]\n",
        "for _ in range(len(job.gca_resource.job_detail.task_details)):\n",
        "    TASK_ID = job.gca_resource.job_detail.task_details[_].task_id\n",
        "    EVAL_METRICS = (\n",
        "        BUCKET_URI\n",
        "        + \"/\"\n",
        "        + PROJECT_NUMBER\n",
        "        + \"/\"\n",
        "        + job.name\n",
        "        + \"/\"\n",
        "        + EVAL_TASK_NAME\n",
        "        + \"_\"\n",
        "        + str(TASK_ID)\n",
        "        + \"/executor_output.json\"\n",
        "    )\n",
        "    if tf.io.gfile.exists(EVAL_METRICS):\n",
        "        ! gsutil cat $EVAL_METRICS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up the resources used by this pipeline, run the command below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx_vKniMq9ZX"
      },
      "outputs": [],
      "source": [
        "# Delete GCS bucket.\n",
        "!gsutil -m rm -r {BUCKET_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMuyzrnZLoUa"
      },
      "source": [
        "# Next steps\n",
        "\n",
        "For an alternate approach, please check out the [\"ready-to-go\" text classification pipeline](https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/pipelines/google_cloud_pipeline_components_ready_to_go_text_classification_pipeline.ipynb). This pipeline exposes the model logic for further customization if needed, and adds an additional pipeline step to deploy the model to enable online predictions."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "d975e698c9a4",
        "08d289fa873f",
        "d33c87e4-2ada-4b87-bf75-064247f3162d",
        "3211ba19",
        "TpV-iwP9qw9c",
        "UMuyzrnZLoUa"
      ],
      "name": "google_cloud_pipeline_components_cloud_natural_language_pipeline.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

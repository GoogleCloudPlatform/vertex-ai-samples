{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Narwhalprime/vertex-ai-samples/blob/main/notebooks/community/pipelines/google_cloud_pipeline_components_ready_to_go_text_classification_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tb01JWKr4ima"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Vertex Pipelines: Ready-to-go text classification model training pipeline\n",
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/pipelines/google_cloud_pipeline_components_ready_to_go_text_classification_pipeline.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/pipelines/google_cloud_pipeline_components_ready_to_go_text_classification_pipeline.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/ai/platform/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/community-content/ready_to_go_text_classification_pipeline/ready_to_go_text_classification_pipeline.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This demo showcases the use of [Google Cloud Pipeline Components (GCPC)](https://pypi.org/project/google-cloud-pipeline-components/), [Kubeflow Pipelines (KFP)](https://pypi.org/project/kfp/), and various Vertex AI services such as [Vertex Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction), [Vertex Tensorboard](https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-overview), [Vertex Training for distributed training](https://cloud.google.com/vertex-ai/docs/training/distributed-training) with accelerators, [Vertex Online Prediction](https://cloud.google.com/vertex-ai/docs/predictions/getting-predictions), and [Vertex Model Evaluation components](https://cloud.google.com/vertex-ai/docs/pipelines/model-evaluation-component) in building an end-to-end text classification pipeline that identifies the category of a news article based on its headline and a short description. The demo is intended to show developers how to build end-to-end pipelines using KFP and Vertex Pipelines to classify their own text data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLMHSSKNGuB-"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you will build an end-to-end text classification pipeline to classify news headlines and descriptions. You will use KFP, Vertex AI, and Vertex Pipelines to generate a managed and highly scalable solution.\n",
        "\n",
        "The Text Classification Pipeline includes the following steps:\n",
        "\n",
        "- Split the data into training and validation datasets\n",
        "- Fine-tune a pre-trained [BERT](https://www.tensorflow.org/text/tutorials/classify_text_with_bert) model\n",
        "- Upload your model to Vertex\n",
        "- Deploy your model to a Vertex endpoint\n",
        "- Perform model evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMxTrJCKGqsD"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "This demo uses a Kaggle [News Category Dataset](https://www.kaggle.com/datasets/rmisra/news-category-dataset), which contains around 200k news headlines from the years 2012-2018 obtained from HuffPost. It is located in the public samples Cloud Storage bucket as `gs://cloud-samples-data/vertex-ai/community-content/datasets/news/news_category_data.json`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPPEG6lwGxPv"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze4-nDLfK4pw"
      },
      "source": [
        "### Set up your local development environment\n",
        "\n",
        "**If you are using Colab or Vertex AI Workbench Notebooks**, your environment already meets\n",
        "all the requirements to run this notebook. Skip to the 'Install additional packages' section below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCuSR8GkAgzl"
      },
      "source": [
        "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
        "You need the following:\n",
        "\n",
        "* The Google Cloud SDK\n",
        "* Git\n",
        "* Python 3\n",
        "* virtualenv\n",
        "* Jupyter notebook running in a virtual environment with Python 3\n",
        "\n",
        "The Google Cloud guide to [Setting up a Python development\n",
        "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
        "installation guide](https://jupyter.org/install) provide detailed instructions\n",
        "for meeting these requirements. The following steps provide a condensed set of\n",
        "instructions:\n",
        "\n",
        "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
        "\n",
        "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
        "\n",
        "1. [Install\n",
        "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
        "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
        "\n",
        "1. To install Jupyter, run `pip3 install jupyter` on the\n",
        "command-line in a terminal shell.\n",
        "\n",
        "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
        "\n",
        "1. Open this notebook in the Jupyter Notebook Dashboard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7EUnXsZhAGF"
      },
      "source": [
        "### Install additional packages\n",
        "\n",
        "Install the following packages required to execute this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b4ef9b72d43"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Vertex AI Workbench Notebook product has specific requirements\n",
        "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
        "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
        "    \"/opt/deeplearning/metadata/env_version\"\n",
        ")\n",
        "\n",
        "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_WORKBENCH_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoSXmPuYg5zA"
      },
      "outputs": [],
      "source": [
        "! pip3 install --upgrade pip google-cloud-aiplatform google-cloud-pipeline-components kfp tensorflow tensorboard numpy {USER_FLAG} -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhq5zEbGg0XX"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "Restart your notebook kernel to ensure all newly installed packages can be found."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzrelQZ22IZj"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWEdiXsJg0XY"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF1j6f9HApxa"
      },
      "source": [
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "1. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,storage.googleapis.com).\n",
        "\n",
        "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
        "\n",
        "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WReHDGG5g0XY"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM1iC_MfAts1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"\"\n",
        "\n",
        "# Get your Google Cloud project ID from gcloud\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID: \", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJYoRfYng0XZ"
      },
      "source": [
        "Otherwise, set your project ID here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riG_qUokg0XZ"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
        "    PROJECT_ID = \"\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_gcloud_project_id"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LYAz8zhg5zC"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
        "\n",
        "- Americas: `us-central1`\n",
        "- Europe: `europe-west4`\n",
        "- Asia Pacific: `asia-east1`\n",
        "\n",
        "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
        "\n",
        "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "zW8Byl_jg5zC"
      },
      "outputs": [],
      "source": [
        "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
        "\n",
        "if REGION == \"[your-region]\":\n",
        "    REGION = \"us-central1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "A75Sm4Zpg5zC"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "\n",
        "# Generate a uuid of a specifed length(default=8)\n",
        "def generate_uuid(length: int = 8) -> str:\n",
        "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
        "\n",
        "\n",
        "UUID = generate_uuid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr--iN2kAylZ"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using Vertex AI Workbench Notebooks**, your environment is already\n",
        "authenticated. Skip this step.\n",
        "\n",
        "**If you are using Colab**, run the cell below and follow the instructions\n",
        "when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "1. In the Cloud Console, go to the [**Create service account key**\n",
        "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
        "\n",
        "2. Click **Create service account**.\n",
        "\n",
        "3. In the **Service account name** field, enter a name, and\n",
        "   click **Create**.\n",
        "\n",
        "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
        "into the filter box, and select\n",
        "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "\n",
        "5. Click *Create*. A JSON file that contains your key downloads to your\n",
        "local environment.\n",
        "\n",
        "6. Enter the path to your service account key as the\n",
        "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "outputs": [],
      "source": [
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# If on Vertex AI Workbench, then don't execute this code\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
        "    \"DL_ANACONDA_HOME\"\n",
        "):\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgPO1eR3CYjk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "In this tutorial, a Cloud Storage bucket holds the News Category dataset file that is used to train the model. Vertex AI also saves artifacts, such as the split training and validation datasets generated by the preprocessing component, in the same bucket. Using this model artifact, you can then create a Vertex AI model and endpoint in order to serve online predictions.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
        "Cloud Storage buckets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzGDU7TWdts_"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf221059d072"
      },
      "outputs": [],
      "source": [
        "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
        "    BUCKET_NAME = PROJECT_ID + \"aip-\" + UUID\n",
        "    BUCKET_URI = f\"gs://{BUCKET_NAME}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EcIXiGsCePi"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIq7R4HZCfIc"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucvCsknMCims"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhOb7YnwClBb"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "set_service_account"
      },
      "source": [
        "### Setup service account and permissions\n",
        "A service account will be used to create a custom training job. If you do not want to use your project's Compute Engine service account, set SERVICE_ACCOUNT to another service account ID. You can create a service account by following the [instructions](https://cloud.google.com/iam/docs/creating-managing-service-accounts#creating)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGFb5_BNg5zD"
      },
      "outputs": [],
      "source": [
        "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autoset_service_account"
      },
      "outputs": [],
      "source": [
        "if (\n",
        "    SERVICE_ACCOUNT == \"\"\n",
        "    or SERVICE_ACCOUNT is None\n",
        "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
        "):\n",
        "    # Get your service account from gcloud\n",
        "    if not IS_COLAB:\n",
        "        shell_output = !gcloud auth list 2>/dev/null\n",
        "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
        "\n",
        "    else:  # IS_COLAB:\n",
        "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
        "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "\n",
        "    print(\"Service Account:\", SERVICE_ACCOUNT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "set_service_account:pipelines"
      },
      "source": [
        "#### Set service account access\n",
        "\n",
        "Run the following commands to grant your service account access to read and write pipeline artifacts in the bucket that you created in the previous step. You only need to run this step once per service account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxXITjj2g5zE"
      },
      "outputs": [],
      "source": [
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
        "\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRUOFELefqf1"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "from kfp import components\n",
        "from kfp.v2 import compiler, dsl\n",
        "from kfp.v2.dsl import InputPath, component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCGX-LsRg5zE"
      },
      "outputs": [],
      "source": [
        "# Model evaluation components\n",
        "from google_cloud_pipeline_components.experimental.evaluation import \\\n",
        "    GetVertexModelOp as get_vertex_model_op\n",
        "from google_cloud_pipeline_components.experimental.evaluation import \\\n",
        "    ModelEvaluationClassificationOp as evaluation_classification_op\n",
        "from google_cloud_pipeline_components.experimental.evaluation import \\\n",
        "    ModelImportEvaluationOp as model_import_evaluation_op\n",
        "from google_cloud_pipeline_components.experimental.evaluation import \\\n",
        "    TargetFieldDataRemoverOp as target_field_data_remover_op\n",
        "# Text Classification components\n",
        "from google_cloud_pipeline_components.experimental.sklearn import \\\n",
        "    SklearnTrainTestSplitJsonlOp as train_test_split_op\n",
        "from google_cloud_pipeline_components.experimental.text_classification import \\\n",
        "    TextClassificationTrainingOp as text_classification_training_op\n",
        "from google_cloud_pipeline_components.v1.batch_predict_job import \\\n",
        "    ModelBatchPredictOp as batch_prediction_op"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IFdqoBqg5zE"
      },
      "source": [
        "### Fill out the following required configurations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoWhYTV3g5zE"
      },
      "source": [
        "This pipeline accepts a JSONL dataset where each JSON object sample has two required keys: `text` and `label`. The `text` key should map to the sample's text data, while the `label` key should map to its classified category.\n",
        "\n",
        "Here is an example of a JSON object in the dataset used in this demo.\n",
        "\n",
        "{\n",
        "**\"label\"**:\"CRIME\",\n",
        "**\"text\"**:\"There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T17qYMhxg5zE"
      },
      "outputs": [],
      "source": [
        "BASE_OUTPUT_DIR = f\"gs://{BUCKET_NAME}\"\n",
        "\n",
        "SAMPLE_DATA_URI = \"gs://cloud-samples-data/vertex-ai/community-content/datasets/news/news_category_data.json\"\n",
        "TRAINING_DATA_URI = (\n",
        "    f\"{BASE_OUTPUT_DIR}/data/news_category_data.json\"  # @param {type:\"string\"}\n",
        ")\n",
        "\n",
        "# The GCS directory for keeping staging files for model evaluation.\n",
        "ROOT_DIR = \"'f\\\"{BASE_OUTPUT_DIR}/root\\\"'\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcimKmcMg5zE"
      },
      "source": [
        "Copy the sample data to TRAINING_DATA_URI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nr3KLUEyg5zE"
      },
      "outputs": [],
      "source": [
        "! gsutil cp {SAMPLE_DATA_URI} {TRAINING_DATA_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pIcOb86g5zE"
      },
      "source": [
        "`CLASS_NAMES` should be a list of all the categories to which a text sample can be classified as.\n",
        "\n",
        "In this demo, there are 41 genre categories (listed below) that a headline can be classfied as."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZTJ1NwIg5zE"
      },
      "outputs": [],
      "source": [
        "CLASS_NAMES = [\n",
        "    \"GOOD NEWS\",\n",
        "    \"STYLE\",\n",
        "    \"STYLE & BEAUTY\",\n",
        "    \"ARTS\",\n",
        "    \"IMPACT\",\n",
        "    \"WEIRD NEWS\",\n",
        "    \"FIFTY\",\n",
        "    \"ENTERTAINMENT\",\n",
        "    \"ARTS & CULTURE\",\n",
        "    \"HEALTHY LIVING\",\n",
        "    \"WEDDINGS\",\n",
        "    \"PARENTING\",\n",
        "    \"BLACK VOICES\",\n",
        "    \"GREEN\",\n",
        "    \"RELIGION\",\n",
        "    \"POLITICS\",\n",
        "    \"PARENTS\",\n",
        "    \"BUSINESS\",\n",
        "    \"DIVORCE\",\n",
        "    \"WELLNESS\",\n",
        "    \"FOOD & DRINK\",\n",
        "    \"THE WORLDPOST\",\n",
        "    \"MEDIA\",\n",
        "    \"COLLEGE\",\n",
        "    \"WOMEN\",\n",
        "    \"TASTE\",\n",
        "    \"WORLDPOST\",\n",
        "    \"TRAVEL\",\n",
        "    \"CULTURE & ARTS\",\n",
        "    \"SPORTS\",\n",
        "    \"CRIME\",\n",
        "    \"QUEER VOICES\",\n",
        "    \"TECH\",\n",
        "    \"COMEDY\",\n",
        "    \"MONEY\",\n",
        "    \"WORLD NEWS\",\n",
        "    \"LATINO VOICES\",\n",
        "    \"SCIENCE\",\n",
        "    \"EDUCATION\",\n",
        "    \"HOME & LIVING\",\n",
        "    \"ENVIRONMENT\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqSq5J7pg5zF"
      },
      "source": [
        "### Load Components\n",
        "The KFP SDK provides various methods to [load a component](https://www.kubeflow.org/docs/components/pipelines/sdk/component-development/#using-your-component-in-a-pipeline) for use in a pipeline. In this demo, we will be loading five components.\n",
        "\n",
        "This pipeline is composed of the following components:\n",
        "\n",
        "- **train_test_split_jsonl_with_sklearn** - splits data into training and validation datasets.\n",
        "- **train_tensorflow_text_classification_model** - creates a trained text classification TensorFlow Model.\n",
        "- **upload_Tensorflow_model_to_Google_Cloud_Vertex_AI** - converts a TensorFlow model to a Vertex model and uploads it to Vertex.\n",
        "- **deploy_model_to_endpoint** - deploys a Vertex model to an endpoint for online predictions.\n",
        "- **get_gcs_uris_from_jsonl_artifact** - A python function based op to convert data artifact to a format acceptable by model evaluation components.\n",
        "- **target_field_data_remover** - removes the target (label) field in the validation data for downstream Vertex Batch Predictions.\n",
        "- **model_batch_predict** - submits a batch prediction job.\n",
        "- **model_evaluation_classification** - calculates and exports evaluation metrics.\n",
        "- **model_evaluation_import** - imports model evaluation metrics results.\n",
        "\n",
        "Use the `load_component_from_url` for published components that have been made available by GCPC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQtIfCuqg5zF"
      },
      "outputs": [],
      "source": [
        "upload_tensorflow_model_to_vertex_op = components.load_component_from_url(\n",
        "    \"https://raw.githubusercontent.com/Ark-kun/pipeline_components/c6a8b67d1ada2cc17665c99ff6b410df588bee28/components/google-cloud/Vertex_AI/Models/Upload_Tensorflow_model/workaround_for_buggy_KFPv2_compiler/component.yaml\"\n",
        ")\n",
        "deploy_model_to_endpoint_op = components.load_component_from_url(\n",
        "    \"https://raw.githubusercontent.com/Ark-kun/pipeline_components/27a5ea25e849c9e8c0cb6ed65518bc3ece259aaf/components/google-cloud/Vertex_AI/Models/Deploy_to_endpoint/workaround_for_buggy_KFPv2_compiler/component.yaml\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdi60Egyg5zM"
      },
      "source": [
        "Convert the validation data (an Artifact with the annotation of \"JSONLines\") to the output parameter gcs_source_uris (type: Sequence[str]) that can be ingested by the downstream \"target_field_data_remover_op\" and \"model_evaluation_op\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lw5YefFeg5zM"
      },
      "outputs": [],
      "source": [
        "@component(\n",
        "    base_image=\"python:3.9\",\n",
        ")\n",
        "def get_gcs_uris_from_jsonl_artifact(input_jsonl: InputPath(\"JSONLines\")) -> list:\n",
        "    return [\"gs://\" + input_jsonl[5:]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xX9pQ0T_g5zM"
      },
      "source": [
        "### Build a pipeline\n",
        "The following pipeline code links the inputs and outputs of the loaded components. The resulting pipeline performs the following steps:\n",
        "- Partitions data into train and test splits.\n",
        "- Trains new text classification model.\n",
        "- Uploads model to Vertex AI Model Registry.\n",
        "- Performs batch prediction with test data.\n",
        "- Evaluates performance of model using above batch prediction.\n",
        "- Imports evaluation metrics into model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_Ev_TOug5zM"
      },
      "outputs": [],
      "source": [
        "@dsl.pipeline(name=\"text-classification-pipeline\")\n",
        "def text_pipeline(\n",
        "    project: str,\n",
        "    training_data_uri: str,\n",
        "    class_names: list,\n",
        "    root_dir: str,\n",
        "    target_field_name: str,\n",
        "    batch_predict_display_name: str,\n",
        "    batch_predict_instances_format: str = \"jsonl\",\n",
        "    batch_predictions_format: str = \"jsonl\",\n",
        "    model_name: str = \"small_bert/bert_en_uncased_L-2_H-128_A-2\",\n",
        "    validation_split: float = 0.2,\n",
        "    batch_size: int = 256,\n",
        "    learning_rate: float = 3e-4,\n",
        "    num_epochs: int = 5,\n",
        "    random_seed: int = 0,\n",
        ") -> None:\n",
        "\n",
        "    \"\"\"End-to-end text classification pipeline.\n",
        "\n",
        "    Args:\n",
        "    project: Required. GCP project ID.\n",
        "    training_data_uri: Required. Data in JSON lines format.\n",
        "    class_names: Required. List of categories (string) for classification.\n",
        "    root_dir: Required. The GCS directory for keeping staging files for model evaluation.\n",
        "    target_field_name: Required. The name of the features target field in the predictions file (e.g. 'label').\n",
        "    batch_predict_instances_format: The file format for the ground truth files.\n",
        "    batch_predictions_format: The file format for the batch prediction results.\n",
        "    batch_predict_display_name: Required. The user-defined name of this BatchPredictionJob.\n",
        "    model_name: Optional. Name of pre-trained BERT model to be used.\n",
        "                Default: \"small_bert/bert_en_uncased_L-2_H-128_A-2\"\n",
        "    validation_split: Optional. Fraction of data that will make up validation dataset.\n",
        "                      Default: 0.2\n",
        "    batch_size: Optional. Batch size\n",
        "                Default: 256\n",
        "    learning_rate: Optional. Learning rate\n",
        "                   Default: 3e-4\n",
        "    num_epochs: Optional. Number of epochs\n",
        "                Default: 10\n",
        "    random_seed: Optional. Random seed\n",
        "                 Default: 0\n",
        "    \"\"\"\n",
        "\n",
        "    text_data_preprocess_task = train_test_split_op(\n",
        "        input_data_path=training_data_uri,\n",
        "    )\n",
        "\n",
        "    training_data = text_data_preprocess_task.outputs[\"training_data_path\"]\n",
        "\n",
        "    validation_data = text_data_preprocess_task.outputs[\"validation_data_path\"]\n",
        "\n",
        "    # Set CPU, memory, and GPU configuration settings for this step (https://cloud.google.com/vertex-ai/docs/pipelines/machine-types)\n",
        "    model = (\n",
        "        text_classification_training_op(\n",
        "            preprocessed_training_data_path=training_data,\n",
        "            preprocessed_validation_data_path=validation_data,\n",
        "            model_name=model_name,\n",
        "            class_names=class_names,\n",
        "            batch_size=batch_size,\n",
        "            learning_rate=learning_rate,\n",
        "            num_epochs=num_epochs,\n",
        "            random_seed=random_seed,\n",
        "        )\n",
        "    ).add_node_selector_constraint(\n",
        "        \"cloud.google.com/gke-accelerator\", \"NVIDIA_TESLA_A100\"\n",
        "    )  # Note that A100 is available on us-central1\n",
        "\n",
        "    vertex_model_name = upload_tensorflow_model_to_vertex_op(\n",
        "        model=model.outputs[\"trained_model_path\"],\n",
        "    ).outputs[\"model_name\"]\n",
        "\n",
        "    # Model evaluation\n",
        "    # Need a component to convert Artifact('JsonLinesDataset') to JsonArray\n",
        "    validation_data_uris = get_gcs_uris_from_jsonl_artifact(validation_data).output\n",
        "\n",
        "    evaluation_data_for_batch_predict = target_field_data_remover_op(\n",
        "        project=project,\n",
        "        root_dir=root_dir,\n",
        "        target_field_name=target_field_name,\n",
        "        gcs_source_uris=validation_data_uris,\n",
        "    ).outputs[\"gcs_output_directory\"]\n",
        "\n",
        "    vertex_model = get_vertex_model_op(\n",
        "        model_resource_name=vertex_model_name,\n",
        "    ).outputs[\"model\"]\n",
        "\n",
        "    batch_prediction_task = batch_prediction_op(\n",
        "        project=project,\n",
        "        model=vertex_model,\n",
        "        job_display_name=batch_predict_display_name,\n",
        "        gcs_source_uris=evaluation_data_for_batch_predict,\n",
        "        gcs_destination_output_uri_prefix=root_dir,\n",
        "        instances_format=batch_predict_instances_format,\n",
        "        predictions_format=batch_predictions_format,\n",
        "        machine_type=\"n1-standard-32\",\n",
        "        starting_replica_count=5,\n",
        "        max_replica_count=10,\n",
        "    )\n",
        "\n",
        "    # Run the evaluation based on prediction type\n",
        "    eval_task = evaluation_classification_op(\n",
        "        project=project,\n",
        "        root_dir=root_dir,\n",
        "        ground_truth_gcs_source=validation_data_uris,\n",
        "        target_field_name=target_field_name,\n",
        "        prediction_score_column=\"prediction\",\n",
        "        prediction_label_column=\"\",\n",
        "        class_labels=class_names,\n",
        "        ground_truth_format=batch_predict_instances_format,\n",
        "        predictions_format=batch_predictions_format,\n",
        "        predictions_gcs_source=batch_prediction_task.outputs[\"gcs_output_directory\"],\n",
        "    )\n",
        "    # Import the model evaluations to the Vertex AI model\n",
        "    model_import_evaluation_op(\n",
        "        classification_metrics=eval_task.outputs[\"evaluation_metrics\"],\n",
        "        model=vertex_model,\n",
        "        dataset_type=\"jsonl\",\n",
        "    )\n",
        "\n",
        "    # For online predictions\n",
        "    _ = deploy_model_to_endpoint_op(\n",
        "        model_name=vertex_model_name,\n",
        "    ).outputs[\"endpoint_name\"]\n",
        "\n",
        "\n",
        "pipeline_func = text_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiuoDAI8g5zM"
      },
      "source": [
        "### Run the Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tjbfj0Dwg5zM"
      },
      "source": [
        "The following block creates a pipline run from the pipeline function above and submits to the Vertex AI platform. You can view the pipeline's artifacts in [Vertex ML Metadata (MLMD)](https://cloud.google.com/vertex-ai/docs/ml-metadata/introduction) at the link that is ouputted when the next block is run.\n",
        "\n",
        "Only the parameters of this pipeline need to be changed to adapt to your specific usecase. Specify the required pipeline parameters and any optional ones in the `parameter_values` dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMZJdsVCg5zM"
      },
      "outputs": [],
      "source": [
        "compiler.Compiler().compile(\n",
        "    pipeline_func=pipeline_func,\n",
        "    package_path=\"text_classification_pipeline.json\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnpTVTkZg5zM"
      },
      "outputs": [],
      "source": [
        "PIPELINE_DISPLAY_NAME = f\"text-classification-train-evaluate-{UUID}\"  # \"[your-pipeline-display-name]\"  # @param {type:\"string\"}\n",
        "\n",
        "BATCH_PREDICTION_DISPLAY_NAME = f\"batch-prediction-on-pipelines-model-{UUID}\"\n",
        "\n",
        "parameters = {\n",
        "    \"project\": PROJECT_ID,\n",
        "    \"training_data_uri\": TRAINING_DATA_URI,\n",
        "    \"class_names\": CLASS_NAMES,\n",
        "    \"num_epochs\": 5,\n",
        "    \"root_dir\": ROOT_DIR,\n",
        "    \"target_field_name\": \"label\",\n",
        "    \"batch_predict_display_name\": BATCH_PREDICTION_DISPLAY_NAME,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tjK6MUYg5zM"
      },
      "outputs": [],
      "source": [
        "job = aiplatform.PipelineJob(\n",
        "    display_name=PIPELINE_DISPLAY_NAME,\n",
        "    template_path=\"text_classification_pipeline.json\",\n",
        "    location=REGION,\n",
        "    enable_caching=True,\n",
        "    parameter_values=parameters,\n",
        ")\n",
        "\n",
        "job.submit(service_account=SERVICE_ACCOUNT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNkT5JDHg5zM"
      },
      "source": [
        "## Make predictions\n",
        "\n",
        "Once your model is deployed to an endpoint, it can be used to make predictions using the UI or KFP SDK.\n",
        "\n",
        "To use the UI, watch this short [tutorial](https://screencast.googleplex.com/cast/NDY5Nzk3NzUzNzk1Mzc5MnxiNDMxZTFkNi1lNg) or follow the steps highlighted [here](https://cloud.google.com/vertex-ai/docs/predictions/online-predictions-custom-models).\n",
        "\n",
        "To make predictions using the KFP SDK, the endpoint to which the model was deployed is needed. The code below extracts the `ENDPOINT_ID` from the PipelineJob."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9y_riwHg5zM"
      },
      "outputs": [],
      "source": [
        "task_id = \"deploy-model-to-endpoint-for-google-cloud-vertex-ai-model\"\n",
        "deploy_task_detail = [\n",
        "    task_details\n",
        "    for task_details in job.task_details\n",
        "    if task_details.task_name == task_id\n",
        "][0]\n",
        "ENDPOINT_ID = deploy_task_detail.execution.metadata[\"output:endpoint_name\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skY2oAy8g5zN"
      },
      "source": [
        "Each request must be its own JSON object with a `text` key. `instances` should be a list that holds all the requests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-GNIT9Ng5zN"
      },
      "outputs": [],
      "source": [
        "instances = [\n",
        "    {\n",
        "        \"text\": \"Irish Voters Set To Liberalize Abortion Laws In Landslide, Exit Poll Signals Vote counting will begin Saturday.\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DXeesSog5zN"
      },
      "source": [
        "A list of predictions in order of how the requests were structured above. Each prediction is a vector of with probabilities of each category in `class_names` being the proper label. The probabilites match up to each category in `class_names` in order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRMUx8tng5zN"
      },
      "outputs": [],
      "source": [
        "endpoint = aiplatform.Endpoint(ENDPOINT_ID)\n",
        "prediction = endpoint.predict(instances=instances)\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx_vKniMq9ZX"
      },
      "outputs": [],
      "source": [
        "# Delete GCS bucket.\n",
        "! gsutil -m rm -r {BUCKET_URI}\n",
        "\n",
        "# Delete endpoint resource.\n",
        "! gcloud ai endpoints delete $ENDPOINT_ID --quiet --region $REGION"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "google_cloud_pipeline_components_ready_to_go_text_classification_pipeline.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18ebbd838e32"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64f7165bd1ac"
      },
      "source": [
        "# Telecom subscriber churn prediction on Vertex AI\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/workbench/subscriber_churn_prediction/telecom-subscriber-churn-prediction.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "    <td>\n",
        "        <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/workbench/subscriber_churn_prediction/telecom-subscriber-churn-prediction.ipynb\">\n",
        "        <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\\\" alt=\"Colab logo\"> Run in Colab\n",
        "        </a>\n",
        "  </td>\n",
        "  <td>\n",
        "<a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/workbench/subscriber_churn_prediction/telecom-subscriber-churn-prediction.ipynb\" target='_blank'>\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "<br/><br/><br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ccb933a1de0"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This example demonstrates building a subscriber churn prediction model on a [telecom customer churn dataset](https://www.kaggle.com/c/customer-churn-prediction-2020/overview). The generated churn model is further deployed to Vertex AI Endpoints and explanations are generated using the Explainable AI feature of Vertex AI. \n",
        "\n",
        "Learn more about [Vertex AI Workbench](https://cloud.google.com/vertex-ai/docs/workbench/introduction) and [Vertex Explainable AI](https://cloud.google.com/vertex-ai/docs/explainable-ai/overview)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c7726146192"
      },
      "source": [
        "### Objective\n",
        "\n",
        "This tutorial shows you how to do exploratory data analysis, preprocess data, train, deploy and get predictions from a churn prediction model on a tabular churn dataset. The objectives of this tutorial are as follows:\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services and resources:\n",
        "\n",
        "- `Vertex AI Model` resource\n",
        "- `Vertex AI Endpoint` resource\n",
        "- `Vertex Explainable AI`\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Load data from a Cloud Storage path\n",
        "- Perform exploratory data analysis (EDA)\n",
        "- Preprocess the data\n",
        "- Train a scikit-learn model\n",
        "- Evaluate the scikit-learn model\n",
        "- Save the model to a Cloud Storage path\n",
        "- Create a model and an endpoint in Vertex AI\n",
        "- Deploy the trained model to an endpoint\n",
        "- Generate predictions and explanations on test data from the hosted model\n",
        "- Undeploy the model resource"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bae972f3229"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset used in this tutorial is Telecom-Customer Churn dataset publicly available on Kaggle. See [Customer Churn Prediction 2020](https://www.kaggle.com/c/customer-churn-prediction-2020/data). This dataset is used to build and deploy a churn prediction model using Vertex AI in this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "556bf343c423"
      },
      "source": [
        "### Costs \n",
        "\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0b0e0803638"
      },
      "source": [
        "### Set up your local development environment\n",
        "\n",
        "**If you are using Colab or Vertex AI Workbench Notebooks**, your environment already meets\n",
        "all the requirements to run this notebook. You can skip this step.\n",
        "\n",
        "**_NOTE_**: This notebook has been tested in the following environment:\n",
        "\n",
        "* Python version = 3.9\n",
        "\n",
        "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
        "You need the following:\n",
        "\n",
        "* The Google Cloud SDK\n",
        "\n",
        "The Google Cloud guide to [Setting up a Python development\n",
        "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
        "installation guide](https://jupyter.org/install) provide detailed instructions\n",
        "for meeting these requirements. The following steps provide a condensed set of\n",
        "instructions:\n",
        "\n",
        "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
        "\n",
        "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
        "\n",
        "1. [Install\n",
        "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
        "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
        "\n",
        "1. To install Jupyter, run `pip3 install jupyter` on the\n",
        "command-line in a terminal shell.\n",
        "\n",
        "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
        "\n",
        "1. Open this notebook in the Jupyter Notebook Dashboard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44b8ae8e2d19"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Install the following packages required to execute this notebook. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab450121b368"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Vertex AI Workbench Notebook product has specific requirements\n",
        "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
        "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
        "    \"/opt/deeplearning/metadata/env_version\"\n",
        ")\n",
        "\n",
        "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_WORKBENCH_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\"\n",
        "    \n",
        "! pip3 install {USER_FLAG} --upgrade google-cloud-aiplatform \\\n",
        "                                    google-cloud-storage \\\n",
        "                                    category_encoders \\\n",
        "                                    seaborn \\\n",
        "                                    scikit-learn \\\n",
        "                                    pandas \\\n",
        "                                    fsspec \\\n",
        "                                    gcsfs -q "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b24902cde81b"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c61d171395d7"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b012ef94ce80"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "1. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). \n",
        "\n",
        "1. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
        "\n",
        "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96ff17f75e21"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2e80fda2ea2"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6855b42885bf"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
        "    # Get your GCP project id from gcloud\n",
        "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID:\", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2e3c0f2cbfb"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60d535f443ac"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook.  Below are regions supported for Vertex AI. It is recommended that you choose the region closest to you.\n",
        "\n",
        "- Americas: `us-central1`\n",
        "- Europe: `europe-west4`\n",
        "- Asia Pacific: `asia-east1`\n",
        "\n",
        "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
        "\n",
        "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aaadaaf9b30"
      },
      "outputs": [],
      "source": [
        "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
        "\n",
        "if REGION == \"[your-region]\":\n",
        "    REGION = \"us-central1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e663bd062c6f"
      },
      "source": [
        "#### UUID\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a uuid for each instance session, and append it onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "953fa6e5ddda"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "\n",
        "# Generate a uuid of a specifed length(default=8)\n",
        "def generate_uuid(length: int = 8) -> str:\n",
        "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
        "\n",
        "\n",
        "UUID = generate_uuid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ffa6b6c7cdb"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using Vertex AI Workbench Notebooks**, your environment is already\n",
        "authenticated. \n",
        "\n",
        "**If you are using Colab**, run the cell below and follow the instructions\n",
        "when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "1. In the Cloud Console, go to the [**Create service account key**\n",
        "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
        "\n",
        "2. Click **Create service account**.\n",
        "\n",
        "3. In the **Service account name** field, enter a name, and\n",
        "   click **Create**.\n",
        "\n",
        "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
        "into the filter box, and select\n",
        "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "\n",
        "5. Click *Create*. A JSON file that contains your key downloads to your\n",
        "local environment.\n",
        "\n",
        "6. Enter the path to your service account key as the\n",
        "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b72272258fc"
      },
      "outputs": [],
      "source": [
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# If on Vertex AI Workbench, then don't execute this code\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
        "    \"DL_ANACONDA_HOME\"\n",
        "):\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS '[your-service-account-key-path]'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "637c90fe1062"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "\n",
        "When you create a model in Vertex AI using the Cloud SDK, you give a Cloud Storage path where the trained model is saved. \n",
        "In this tutorial, Vertex AI saves the trained model to a Cloud Storage bucket. Using this model artifact, you can then\n",
        "create Vertex AI model and endpoint resources in order to serve\n",
        "online predictions.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
        "Cloud Storage buckets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65e1e634c920"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68d1f4908641"
      },
      "outputs": [],
      "source": [
        "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
        "    BUCKET_NAME = PROJECT_ID + \"aip-\" + UUID\n",
        "    BUCKET_URI = f\"gs://{BUCKET_NAME}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cb016da6de3"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e66f36b71bc3"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51a8afe42b5e"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "835bf2f84d86"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91a6f77f3d94"
      },
      "source": [
        "## Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b02bcf983b82"
      },
      "source": [
        "### Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1c9e504395c"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "%matplotlib inline\n",
        "import pickle\n",
        "# configure to don't display the warnings\n",
        "import warnings\n",
        "\n",
        "import category_encoders as ce\n",
        "import seaborn as sns\n",
        "from google.cloud import aiplatform, storage\n",
        "from google.cloud.aiplatform_v1.types import SampledShapleyAttribution\n",
        "from google.cloud.aiplatform_v1.types.explanation import ExplanationParameters\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, plot_roc_curve\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e37354341588"
      },
      "source": [
        "### Load data from Cloud Storage using Pandas\n",
        "\n",
        "The Telecom-Customer Churn dataset from [Kaggle](https://www.kaggle.com/c/customer-churn-prediction-2020/overview) is made available on a public Cloud Storage bucket at: \n",
        "\n",
        "```gs://cloud-samples-data/vertex-ai/managed_notebooks/telecom_churn_prediction/train.csv```\n",
        "\n",
        "Use Pandas to read data directly from the URI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d949de05d3e3"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\n",
        "    \"gs://cloud-samples-data/vertex-ai/managed_notebooks/telecom_churn_prediction/train.csv\"\n",
        ")\n",
        "print(df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca1a5b9da481"
      },
      "source": [
        "## Perform EDA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fc64740bd98"
      },
      "source": [
        "Check the data types and null counts of the fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1305e25e8c85"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cabae9da850"
      },
      "source": [
        "The current dataset doesn't have any null or empty fields in it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c95d9008b0e"
      },
      "source": [
        "Check the class imbalance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a12839626b40"
      },
      "outputs": [],
      "source": [
        "df[\"churn\"].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2473365db828"
      },
      "source": [
        "There are 14% churners in the data which is not bad for training a churn prediction model. If the class imbalance seems high, oversampling or undersampling techniques can be considered to balance the class distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa0f8adef6f2"
      },
      "source": [
        "Separate the caetgorical and numerical columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1c94426e3b2"
      },
      "outputs": [],
      "source": [
        "categ_cols = [\"state\", \"area_code\", \"international_plan\", \"voice_mail_plan\"]\n",
        "target = \"churn\"\n",
        "num_cols = [i for i in df.columns if i not in categ_cols and i != target]\n",
        "print(len(categ_cols), len(num_cols))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c1f4744269d"
      },
      "source": [
        "Plot the level distribution for the categorical columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb016790dcdc"
      },
      "outputs": [],
      "source": [
        "for i in categ_cols:\n",
        "    df[i].value_counts().plot(kind=\"bar\")\n",
        "    plt.title(i)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7c1eaf4363f"
      },
      "outputs": [],
      "source": [
        "print(num_cols)\n",
        "df[\"number_vmail_messages\"].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7024efb18a27"
      },
      "source": [
        "Check the distributions for the numerical columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11165f493bbc"
      },
      "outputs": [],
      "source": [
        "for i in num_cols:\n",
        "    # check the Price field's distribution\n",
        "    _, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "    df[i].plot(kind=\"box\", ax=ax[0])\n",
        "    df[i].plot(kind=\"hist\", ax=ax[1])\n",
        "    plt.title(i)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eec92dc756a9"
      },
      "outputs": [],
      "source": [
        "# check pairplots for selected features\n",
        "selected_features = [\n",
        "    \"total_day_calls\",\n",
        "    \"total_eve_calls\",\n",
        "    \"number_customer_service_calls\",\n",
        "    \"number_vmail_messages\",\n",
        "    \"account_length\",\n",
        "    \"total_day_charge\",\n",
        "    \"total_eve_charge\",\n",
        "]\n",
        "sns.pairplot(df[selected_features])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56c8911c7588"
      },
      "source": [
        "Plot a heat map of the correlation matrix for the numerical features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79fdafb9e5a6"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(df[num_cols].corr(), annot=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19ef89810012"
      },
      "source": [
        "### Observations from EDA\n",
        "\n",
        "- There are many levels/categories in the categorical field <b>state</b>. In further steps, creating one-hot encoding vectors for this field would increase the columns drastically and so a binary encoding technique will be considered for encoding this field.\n",
        "- There are only 9% of customers in the data who have had international plans.\n",
        "- There are only a few customers who make frequent calls to customer service.\n",
        "- Only 25% of the customers had at least 16 voicemail messages and thus there was skewness in the distribution of the `number_vmail_messages` field.\n",
        "- Most of the feature combinations in the pair plot show a circular pattern that suggests that there is almost no correlation between the corresponding two features.\n",
        "- There seems to be a high correlation between minutes and charge. Either one of them can be dropped to avoid multi-collinearity or redundant features in the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28f0b40ea577"
      },
      "source": [
        "### Preprocess the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7dd6ade12cd"
      },
      "source": [
        "Drop the fields corresponding to the highly-correlated features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bb46ec159a3"
      },
      "outputs": [],
      "source": [
        "drop_cols = [\n",
        "    \"total_day_charge\",\n",
        "    \"total_eve_charge\",\n",
        "    \"total_night_charge\",\n",
        "    \"total_intl_charge\",\n",
        "]\n",
        "df.drop(columns=drop_cols, inplace=True)\n",
        "num_cols = list(set(num_cols).difference(set(drop_cols)))\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcc77c18541c"
      },
      "source": [
        "Binary encode the state feature (as there are many levels/categories)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9f8e1fdcbb8"
      },
      "outputs": [],
      "source": [
        "encoder = ce.BinaryEncoder(cols=[\"state\"], return_df=True)\n",
        "data_encoded = encoder.fit_transform(df)\n",
        "data_encoded.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c59a90d8961c"
      },
      "source": [
        "One-hot encode (drop the first level-column to avoid dummy-variable trap scenarios) the remaining categorical variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21c8c6262dbc"
      },
      "outputs": [],
      "source": [
        "def encode_cols(data, col):\n",
        "    # Creating a dummy variable for the variable 'CategoryID' and dropping the first one.\n",
        "    categ = pd.get_dummies(data[col], prefix=col, drop_first=True)\n",
        "    # Adding the results to the master dataframe\n",
        "    data = pd.concat([data, categ], axis=1)\n",
        "    return data\n",
        "\n",
        "\n",
        "for i in categ_cols + [target]:\n",
        "    if i != \"state\":\n",
        "        data_encoded = encode_cols(data_encoded, i)\n",
        "        data_encoded.drop(columns=[i], inplace=True)\n",
        "\n",
        "data_encoded.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7ee5d51d900"
      },
      "source": [
        "Check the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "980c42c98f3a"
      },
      "outputs": [],
      "source": [
        "data_encoded.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b402c7db5d1a"
      },
      "source": [
        "Check the columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e67c06b9938"
      },
      "outputs": [],
      "source": [
        "data_encoded.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a809e0c43ff"
      },
      "source": [
        "Split the data into train and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17aaffef959d"
      },
      "outputs": [],
      "source": [
        "X = data_encoded[[i for i in data_encoded.columns if i not in [\"churn_yes\"]]].copy()\n",
        "y = data_encoded[\"churn_yes\"].copy()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, train_size=0.7, test_size=0.3, random_state=100\n",
        ")\n",
        "print(X_train.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1875f9ec661f"
      },
      "source": [
        "Scale the numerical data using `MinMaxScaler`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed095d60fa47"
      },
      "outputs": [],
      "source": [
        "sc = MinMaxScaler()\n",
        "X_train.loc[:, num_cols] = sc.fit_transform(X_train[num_cols])\n",
        "X_test.loc[:, num_cols] = sc.transform(X_test[num_cols])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "229cfba1fd32"
      },
      "source": [
        "## Train a logistic regression model using scikit-learn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37a536623470"
      },
      "source": [
        "The argument `class_weight` adjusts the class weights to the target feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7acc267ad4c1"
      },
      "outputs": [],
      "source": [
        "model = LogisticRegression(class_weight=\"balanced\")\n",
        "model = model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18e23a047402"
      },
      "source": [
        "## Evaluate the trained model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e59e96403fdf"
      },
      "source": [
        "#### Plot the ROC and show AUC on train and test sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "038852060275"
      },
      "source": [
        "Plot the ROC for the model on train data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "482d7eb78295"
      },
      "outputs": [],
      "source": [
        "plot_roc_curve(model, X_train, y_train, drop_intermediate=False)\n",
        "plt.show()\n",
        "\n",
        "# plot the ROC for the model on test data\n",
        "plot_roc_curve(model, X_test, y_test, drop_intermediate=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb393fe2f3c4"
      },
      "source": [
        "#### Determine the optimal threshold for the binary classification \n",
        "\n",
        "In general, the logistic regression model outputs probability scores between 0 and 1 and a threshold needs to be determined to assign a class label. Depending on the sensitivity (true-positive rate) and specificity (true-negative rate) of the model, an optimal threshold can be determined."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9482b0a4d3a"
      },
      "source": [
        "Create columns with 10 different probability cutoffs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f19ea3f3a2ce"
      },
      "outputs": [],
      "source": [
        "y_train_pred = model.predict_proba(X_train)[:, 1]\n",
        "numbers = [float(x) / 10 for x in range(10)]\n",
        "y_train_pred_df = pd.DataFrame({\"true\": y_train, \"pred\": y_train_pred})\n",
        "for i in numbers:\n",
        "    y_train_pred_df[i] = y_train_pred_df.pred.map(lambda x: 1 if x > i else 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cff355e58790"
      },
      "source": [
        "Now calculate accuracy, sensitivity, and specificity for various probability cutoffs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "572a9dc237f6"
      },
      "outputs": [],
      "source": [
        "cutoff_df = pd.DataFrame(columns=[\"prob\", \"accuracy\", \"sensitivity\", \"specificity\"])\n",
        "\n",
        "# compute the parameters for each threshold considered\n",
        "for i in numbers:\n",
        "    cm1 = confusion_matrix(y_train_pred_df.true, y_train_pred_df[i])\n",
        "    total1 = sum(sum(cm1))\n",
        "    accuracy = (cm1[0, 0] + cm1[1, 1]) / total1\n",
        "\n",
        "    speci = cm1[0, 0] / (cm1[0, 0] + cm1[0, 1])\n",
        "    sensi = cm1[1, 1] / (cm1[1, 0] + cm1[1, 1])\n",
        "    cutoff_df.loc[i] = [i, accuracy, sensi, speci]\n",
        "\n",
        "# Let's plot accuracy sensitivity and specificity for various probabilities.\n",
        "cutoff_df.plot.line(x=\"prob\", y=[\"accuracy\", \"sensitivity\", \"specificity\"])\n",
        "plt.title(\"Comparison of performance across various thresholds\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ae7362a31f0"
      },
      "source": [
        "In general, a model with balanced sensitivity and specificity is preferred. In the current case, the threshold where the sensitivity and specifity curves intersect can be considered an optimal threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95d07b5422a6"
      },
      "outputs": [],
      "source": [
        "threshold = 0.5\n",
        "\n",
        "# Evaluate train and test sets\n",
        "y_test_pred = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# to get the performance stats, lets define a handy function\n",
        "\n",
        "\n",
        "def print_stats(y_true, y_pred):\n",
        "    # Confusion matrix\n",
        "\n",
        "    confusion = confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
        "    print(\"Confusion Matrix: \")\n",
        "    print(confusion)\n",
        "\n",
        "    TP = confusion[1, 1]  # true positive\n",
        "    TN = confusion[0, 0]  # true negatives\n",
        "    FP = confusion[0, 1]  # false positives\n",
        "    FN = confusion[1, 0]  # false negatives\n",
        "\n",
        "    # Let's see the sensitivity or recall of our logistic regression model\n",
        "    sensitivity = TP / float(TP + FN)\n",
        "    print(\"sensitivity = \", sensitivity)\n",
        "    # Let us calculate specificity\n",
        "    specificity = TN / float(TN + FP)\n",
        "    print(\"specificity = \", specificity)\n",
        "    # Calculate false postive rate - predicting conversion when customer didn't convert\n",
        "    fpr = FP / float(TN + FP)\n",
        "    print(\"False positive rate = \", fpr)\n",
        "    # positive predictive value\n",
        "    precision = TP / float(TP + FP)\n",
        "    print(\"precision = \", precision)\n",
        "    # accuracy\n",
        "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "    print(\"accuracy = \", accuracy)\n",
        "    return\n",
        "\n",
        "\n",
        "y_train_pred_sm = [1 if i > threshold else 0 for i in y_train_pred]\n",
        "y_test_pred_sm = [1 if i > threshold else 0 for i in y_test_pred]\n",
        "# Print the metrics for the model\n",
        "# on train data\n",
        "print(\"Train Data : \")\n",
        "print_stats(y_train, y_train_pred_sm)\n",
        "print(\"\\n\", \"*\" * 30, \"\\n\")\n",
        "# on test data\n",
        "print(\"Test Data : \")\n",
        "print_stats(y_test, y_test_pred_sm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b64f7190aad6"
      },
      "source": [
        "While the model's sensitivity and specificity are looking decent, the precision can be considered low. This type of situation may be acceptable to some extent because from a business standpoint in the telecom industry, it still makes sense to identify churners even though it means there'd be some mis-classifications of non-churners as churners."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7fc7b467b6f"
      },
      "source": [
        "## Save the model to a Cloud Storage path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae43e214775f"
      },
      "source": [
        "Save the trained model to a local file `model.pkl`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94ad4c932974"
      },
      "outputs": [],
      "source": [
        "FILE_NAME = \"model.pkl\"\n",
        "with open(FILE_NAME, \"wb\") as file:\n",
        "    pickle.dump(model, file)\n",
        "\n",
        "# Upload the saved model file to Cloud Storage\n",
        "BLOB_PATH = (\n",
        "    \"[your-blob-path]\"  # leave blank if no folders inside the bucket are needed.\n",
        ")\n",
        "\n",
        "if BLOB_PATH == (\"[your-blob-path]\"):\n",
        "    BLOB_PATH = \"\"\n",
        "\n",
        "BLOB_NAME = BLOB_PATH + FILE_NAME\n",
        "\n",
        "bucket = storage.Client().bucket(BUCKET_NAME)\n",
        "blob = bucket.blob(BLOB_NAME)\n",
        "blob.upload_from_filename(FILE_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa045b28545b"
      },
      "source": [
        "## Create a model with Explainable AI support in Vertex AI\n",
        "\n",
        "Before creating a model, configure the explanations for the model. For further details, see [Configuring explanations in Vertex AI](https://cloud.google.com/vertex-ai/docs/explainable-ai/configuring-explanations#scikit-learn-and-xgboost-pre-built-containers).\n",
        "\n",
        "Set a display name for the model resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a90b693b0a17"
      },
      "outputs": [],
      "source": [
        "# Set the model display name\n",
        "MODEL_DISPLAY_NAME = \"[your-model-display-name]\"  # @param {type:\"string\"}\n",
        "\n",
        "if MODEL_DISPLAY_NAME == \"[your-model-display-name]\":\n",
        "    MODEL_DISPLAY_NAME = \"subscriber_churn_model\"\n",
        "\n",
        "ARTIFACT_GCS_PATH = f\"gs://{BUCKET_NAME}/{BLOB_PATH}\"\n",
        "\n",
        "# Feature-name(Inp_feature) and Output-name(Model_output) can be arbitrary\n",
        "exp_metadata = {\"inputs\": {\"Inp_feature\": {}}, \"outputs\": {\"Model_output\": {}}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d28c0b203200"
      },
      "outputs": [],
      "source": [
        "# Create a Vertex AI model resource with support for explanations\n",
        "\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "model = aiplatform.Model.upload(\n",
        "    display_name=MODEL_DISPLAY_NAME,\n",
        "    artifact_uri=ARTIFACT_GCS_PATH,\n",
        "    serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\",\n",
        "    explanation_metadata=exp_metadata,\n",
        "    explanation_parameters=ExplanationParameters(\n",
        "        sampled_shapley_attribution=SampledShapleyAttribution(path_count=25)\n",
        "    ),\n",
        ")\n",
        "\n",
        "model.wait()\n",
        "\n",
        "print(model.display_name)\n",
        "print(model.resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3db48702657f"
      },
      "source": [
        "Alternatively, the following `gcloud` command can be used to create the model resource. The `explanation-metadata.json` file consists of the metadata that is used to configure explanations for the model resource.\n",
        "\n",
        "```\n",
        "gcloud beta ai models upload \\\n",
        "  --region=$REGION \\\n",
        "  --display-name=$MODEL_DISPLAY_NAME \\\n",
        "  --container-image-uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\" \\\n",
        "  --artifact-uri=$ARTIFACT_GCS_PATH \\\n",
        "  --explanation-method=sampled-shapley \\\n",
        "  --explanation-path-count=25 \\\n",
        "  --explanation-metadata-file=explanation-metadata.json\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab34c4b7183c"
      },
      "source": [
        "### Create an endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2330cab1093"
      },
      "outputs": [],
      "source": [
        "ENDPOINT_DISPLAY_NAME = \"[your-endpoint-display-name]\"  # @param {type:\"string\"}\n",
        "if ENDPOINT_DISPLAY_NAME == \"[your-endpoint-display-name]\":\n",
        "    ENDPOINT_DISPLAY_NAME = \"subsc_churn_endpoint\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42306a0791e6"
      },
      "outputs": [],
      "source": [
        "endpoint = aiplatform.Endpoint.create(\n",
        "    display_name=ENDPOINT_DISPLAY_NAME, project=PROJECT_ID, location=REGION\n",
        ")\n",
        "\n",
        "print(endpoint.display_name)\n",
        "print(endpoint.resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84a64b2a7399"
      },
      "source": [
        "### Deploy the model to the created endpoint\n",
        "\n",
        "Configure the depoyment name, machine-type, and other parameters for the deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb1c5e1a2fdf"
      },
      "outputs": [],
      "source": [
        "DEPLOYED_MODEL_NAME = \"[deployment-model-name]\"  # @param {type:\"string\"}\n",
        "MACHINE_TYPE = \"n1-standard-4\"\n",
        "\n",
        "if DEPLOYED_MODEL_NAME == \"[deployment-model-name]\":\n",
        "    DEPLOYED_MODEL_NAME = \"subsc_churn_deployment\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed6f6aefea51"
      },
      "outputs": [],
      "source": [
        "# deploy the model to the endpoint\n",
        "model.deploy(\n",
        "    endpoint=endpoint,\n",
        "    deployed_model_display_name=DEPLOYED_MODEL_NAME,\n",
        "    machine_type=MACHINE_TYPE,\n",
        ")\n",
        "\n",
        "model.wait()\n",
        "\n",
        "print(model.display_name)\n",
        "print(model.resource_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "359c43e630cb"
      },
      "source": [
        "To ensure the model is deployed, the ID of the deployed model can be checked using the `endpoint.list_models()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bb578a29d01"
      },
      "outputs": [],
      "source": [
        "endpoint.list_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21a50d4e9946"
      },
      "source": [
        "## Get explanations from the deployed model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b50c31e0552"
      },
      "source": [
        "Get explanations for a test instance from the hosted model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52cb8915b41c"
      },
      "outputs": [],
      "source": [
        "# format a test instance as the request's payload\n",
        "test_json = [X_test.iloc[0].tolist()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e476d770667"
      },
      "source": [
        "### Get explanations and plot the feature attributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81804daa141e"
      },
      "outputs": [],
      "source": [
        "features = X_train.columns.to_list()\n",
        "\n",
        "\n",
        "def plot_attributions(attrs):\n",
        "    \"\"\"\n",
        "    Function to plot the features and their attributions for an instance\n",
        "    \"\"\"\n",
        "    rows = {\"feature_name\": [], \"attribution\": []}\n",
        "    for i, val in enumerate(features):\n",
        "        rows[\"feature_name\"].append(val)\n",
        "        rows[\"attribution\"].append(attrs[\"Inp_feature\"][i])\n",
        "    attr_df = pd.DataFrame(rows).set_index(\"feature_name\")\n",
        "    attr_df.plot(kind=\"bar\")\n",
        "    plt.show()\n",
        "    return\n",
        "\n",
        "\n",
        "def explain_tabular_sample(project: str, location: str, endpoint, instances: list):\n",
        "    \"\"\"\n",
        "    Function to make an explanation request for the specified payload and generate feature attribution plots\n",
        "    \"\"\"\n",
        "    aiplatform.init(project=project, location=location)\n",
        "\n",
        "    # endpoint = aiplatform.Endpoint(endpoint_id)\n",
        "\n",
        "    response = endpoint.explain(instances=instances)\n",
        "    print(\"#\" * 10 + \"Explanations\" + \"#\" * 10)\n",
        "    for explanation in response.explanations:\n",
        "        print(\" explanation\")\n",
        "        # Feature attributions.\n",
        "        attributions = explanation.attributions\n",
        "\n",
        "        for attribution in attributions:\n",
        "            print(\"  attribution\")\n",
        "            print(\"   baseline_output_value:\", attribution.baseline_output_value)\n",
        "            print(\"   instance_output_value:\", attribution.instance_output_value)\n",
        "            print(\"   output_display_name:\", attribution.output_display_name)\n",
        "            print(\"   approximation_error:\", attribution.approximation_error)\n",
        "            print(\"   output_name:\", attribution.output_name)\n",
        "            output_index = attribution.output_index\n",
        "            for output_index in output_index:\n",
        "                print(\"   output_index:\", output_index)\n",
        "\n",
        "            plot_attributions(attribution.feature_attributions)\n",
        "\n",
        "    print(\"#\" * 10 + \"Predictions\" + \"#\" * 10)\n",
        "    for prediction in response.predictions:\n",
        "        print(prediction)\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "# Get explanations for the test instance\n",
        "prediction = explain_tabular_sample(PROJECT_ID, REGION, endpoint, test_json)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20dd765c9e30"
      },
      "source": [
        "## Clean up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:\n",
        "* Vertex AI Model\n",
        "* Vertex AI Endpoint\n",
        "* Cloud Storage bucket\n",
        "\n",
        "Set `delete_bucket` to *True* to delete the Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98f3a1d74d02"
      },
      "outputs": [],
      "source": [
        "# Undeploy model\n",
        "endpoint.undeploy_all()\n",
        "\n",
        "# Delete the endpoint\n",
        "endpoint.delete()\n",
        "\n",
        "# Delete the model\n",
        "model.delete()\n",
        "\n",
        "# Delete the Cloud Storage bucket\n",
        "delete_bucket = True\n",
        "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
        "    ! gsutil -m rm -r $BUCKET_URI"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "telecom-subscriber-churn-prediction.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "sKMNwpzpj0FzuiaWag9njBox",
      "metadata": {
        "id": "483138c1a042"
      },
      "outputs": [],
      "source": [
        "# Copyright 2026 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qOl_7KfS31to",
      "metadata": {
        "id": "dce4246d1205"
      },
      "source": [
        "# Cloud AI Alphagenome Fine tuning Notebook\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Falphagenome%2Fcloudai_alphagenome_finetune.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d00b5tkvO0Sx",
      "metadata": {
        "id": "8adf7af31973"
      },
      "source": [
        "# AlphaGenome Finetuning Tutorial using Google Cloud Platform\n",
        "\n",
        "This notebook demonstrates how to finetune an AlphaGenome model on custom genomic tracks. This notebook adds the Google Cloud Platform utilities to the Google Deep Mind's \n",
        "<a href=\"https://github.com/google-deepmind/alphagenome_research/blob/main/colabs/finetune.ipynb\">AlphaGenome Finetuning Tutorial</a>.\n",
        "\n",
        "**What you'll learn:**\n",
        "\n",
        "-   How to define custom track metadata for finetuning\n",
        "-   How to set up the data pipeline for training\n",
        "-   How to initialize and configure the model with new output heads\n",
        "-   How to run the training loop with JAX/Haiku\n",
        "-   How to use the finetuned model for inference\n",
        "-   How to save 'finetuned trained checkpoints' and 'prediction vs ground truth plot' in Google Cloud Storage"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hpA9Dc1JPSJW",
      "metadata": {
        "id": "846392dc5423"
      },
      "source": [
        "## Get started\n",
        "Goal: To fine tune Google DeepMind's AlphaGenome model.\n",
        "\n",
        "Process: The notebook uses four inputs and produces two outputs. The outputs are all stored in your Google Cloud Project.\n",
        "\n",
        "Terms and conditions (T&Cs): Agree to T&Cs of the respective organization/websites to download the inputs detailed in the below section.\n",
        "\n",
        "Disclaimer:\n",
        "- This is an experimental release.\n",
        "- Check frequently for updated content.\n",
        "\n",
        "Inputs and outputs:\n",
        "\n",
        "* Inputs:\n",
        "    * bigwig files - Can be downloaded from ENCODE portal.\n",
        "    * fasta files - Can be downloaded from https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_46/GRCh38.p14.genome.fa.gz\n",
        "    * sequences_human.bed - Can be downloaded from https://github.com/calico/borzoi/raw/5c9358222b5026abb733ed5fb84f3f6c77239b37/data/sequences_human.bed.gz.\n",
        "    * initial model weights - Can be downloaded from Huggingface\n",
        "* Outputs:\n",
        "    * The fine tuned weights .\n",
        "    * The predicted vs ground truth plot (.png).\n",
        "\n",
        "Steps:\n",
        "* The ***extremely*** important step: Carefully set/modify the variables.\n",
        "* Checkout the prerequisites.\n",
        "* Prepare the inputs.\n",
        "* Follow the notebook and run the cells.\n",
        "* Utility functions - download the weights after the pipeline is compled\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "x-_tQtXQP42a",
      "metadata": {
        "id": "8c8f59a6c426"
      },
      "source": [
        "## 0: Prerequisites\n",
        "\n",
        "- Install AlphaGenome Research and Google Cloud Platform packages.\n",
        "- (*) Choose either H100 or A100 specific vm notebook runtime.\n",
        "- (**) Save Huggingface credentials in Google Cloud Secret manager\n",
        "- (***) Install 0.9.0 jax libraries\n",
        "\n",
        "Notebook launch:\n",
        "- Launch the Notebook in Google Cloud Enterprise Colab.\n",
        "- Use a custom runtime(*) to run the notebook\n",
        "\n",
        "PS:\n",
        "- If the cell magic bash pip install errors in the notebook. Run them from command line.\n",
        "- Restart the session after installing the packages (pay attention to the instruction after you run the pip install cell)\n",
        "\n",
        "(*):\n",
        "- Request quota for h100 or 100\n",
        "- Create a reservation\n",
        "- Create a template using the above reservation\n",
        "- Creat runtime using the above template\n",
        "\n",
        "(**):\n",
        "You will be downloading weights from Huggingface.\n",
        "Ensure that:\n",
        "- You create a token that has 'Read access to contents of all public gated repos you can access' (under Finegrained control)\n",
        "- You accept the T&C of the [model](https://huggingface.co/google/alphagenome-fold-0).\n",
        "\n",
        "\n",
        "(***):\n",
        "Upgrading to 0.9.0 will require multiple runtime restarts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xq8DrJ1yOs1b",
      "metadata": {
        "id": "5dc874d97d52"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "! PIP_NO_BINARY=pyBigWig pip install git+https://github.com/google-deepmind/alphagenome_research.git\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "834dcef76adf"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "# We need >0.9.0 jax libs.\n",
        "# Check the jax version.\n",
        "import jaxlib\n",
        "\n",
        "print(f\"{jax.__version__=}\")\n",
        "print(f\"{jaxlib.__version__=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9a3295c80eb9"
      },
      "outputs": [],
      "source": [
        "# Uninstall the previous version.\n",
        "# Run only if version < 0.9.0.\n",
        "# Restart runtime/kernel after uninstalling.\n",
        "# Run from next cell after the kernel restart\n",
        "if jax.__version__ != \"0.9.0\":\n",
        "    print(f\"Unistalling {jax.__version__}\")\n",
        "    ! pip uninstall -y jax jaxlib jax_cuda12_plugin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e0625b2f13d"
      },
      "outputs": [],
      "source": [
        "# Install specific vesion.\n",
        "# Run only once after unistalling the jax packages.\n",
        "# Restart the runtime/kernel.\n",
        "# Run from next cell after the kernel restart.\n",
        "!pip install --upgrade jax[cuda12_pip]==0.9.0 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b666617f1088"
      },
      "outputs": [],
      "source": [
        "# Install specific vesion.\n",
        "# Run only once after the upgrade.\n",
        "# Restart the runtime/kernel.\n",
        "# Run from next cell after the kernel restart.\n",
        "!pip install jax_cuda12_plugin==0.9.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38HT5qEmS_qH",
      "metadata": {
        "id": "f3b548ac38fa"
      },
      "source": [
        "## 1: Imports\n",
        "\n",
        "Import the necessary libraries:\n",
        "\n",
        "-   `alphagenome_research.finetuning` contains the finetuning utilities\n",
        "-   `alphagenome_research.model` provides the model architecture and metadata\n",
        "    handling\n",
        "-   `alphagenome.data` provides genomic data utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A-H20Ot0TM6n",
      "metadata": {
        "id": "87b00db7e54c"
      },
      "outputs": [],
      "source": [
        "import dataclasses\n",
        "import os\n",
        "import pprint\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "import haiku as hk\n",
        "import huggingface_hub\n",
        "import numpy as np\n",
        "import optax\n",
        "import orbax.checkpoint as ocp\n",
        "import pandas as pd\n",
        "from alphagenome.data import fold_intervals, genome\n",
        "from alphagenome.visualization import plot_components\n",
        "from alphagenome_research.finetuning import dataset as dataset_lib\n",
        "from alphagenome_research.finetuning import finetune\n",
        "from alphagenome_research.model import dna_model\n",
        "from alphagenome_research.model.metadata import metadata as metadata_lib\n",
        "from etils import epath\n",
        "from google.cloud import secretmanager\n",
        "from huggingface_hub import login\n",
        "from jax import errors\n",
        "from jax.experimental import mesh_utils\n",
        "from jax.sharding import Mesh\n",
        "from jax.sharding import PartitionSpec as P"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FqPNAxo6SmC4",
      "metadata": {
        "id": "0b0689eb8d87"
      },
      "source": [
        "## 2: Environment Setup\n",
        "\n",
        "First, we configure TensorFlow to avoid GPU conflicts since we only use it for\n",
        "data loading (JAX handles the actual training)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ytJqlkJYS2g-",
      "metadata": {
        "id": "daf83280e500"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Hide local GPUs/TPUs. TensorFlow only used for data loading.\n",
        "tf.config.set_visible_devices([], \"GPU\")\n",
        "tf.config.set_visible_devices([], \"TPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f34c4142dd0"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "# we need >0.9.0 jax libs\n",
        "# check the jax versions\n",
        "import jaxlib\n",
        "\n",
        "print(f\"{jax.__version__=}\")\n",
        "print(f\"{jaxlib.__version__=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ezPyIXQAWyzn",
      "metadata": {
        "id": "ee98275ce2d2"
      },
      "outputs": [],
      "source": [
        "# Ensure you are running an Enterprise Colab with a runtime that\n",
        "# have GPUs\n",
        "\n",
        "\n",
        "def gpu_info() -> None:\n",
        "    \"\"\"Prints the GPU information.\"\"\"\n",
        "    try:\n",
        "        backend = jax.default_backend()\n",
        "        if backend == \"gpu\":\n",
        "            num_gpus = jax.local_device_count()\n",
        "            print(f\"JAX is using GPU backend with {num_gpus} GPU(s).\")\n",
        "        else:\n",
        "            print(f\"JAX default backend is {backend}, not GPU.\")\n",
        "    except errors.JaxRuntimeError as e:\n",
        "        print(f\"JAX runtime error occurred while detecting devices: {e}\")\n",
        "\n",
        "\n",
        "gpu_info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MDjQITzeR6XN",
      "metadata": {
        "id": "606ab6be6235"
      },
      "source": [
        "## 3: Notebook variables setup\n",
        "\n",
        "### Initialize Google Cloud Platform variables\n",
        "\n",
        "- `PROJECT_ID`: Google Cloud Project - a string variable.\n",
        "-  `BUCKET_NAME`: Google Cloud Storage bucket - 'gs://<your bucket name>'\n",
        "\n",
        "### Intitialize local file directories (No need to change)\n",
        "\n",
        "- `LOCAL_TAR_DIR`: Where the finetune weights are prepared before pushed to Google Cloud Storage for later reference\n",
        "- `LOCAL_BIGWIG_DIR`: Where the bigwig files are downloaded to.\n",
        "- `LOCAL_FASTA_DIR`: Where the fasta files are downloaded to.\n",
        "- `LOCAL_HUMAN_SEQ_DIR`: Where the human sequence files are download to.\n",
        "- `SAVE_CHECKPOINT_DIR`: Where the finetune checkpoint is stored\n",
        "\n",
        "### Model Configuration\n",
        "Define the key hyperparameters for finetuning:\n",
        "\n",
        "-   `LEARNING_RATE`: Controls the step size during optimization\n",
        "-   `MODEL_VERSION`: Which pretrained fold to use (FOLD_0 through FOLD_3)\n",
        "-   `NUM_TRAIN_STEPS`: Number of training steps for which we optimize the model.\n",
        "-   `SEQUENCE_LENGTH`: Length of input DNA sequences (1M bp = 2^20). Training\n",
        "    requires at least 2**17.\n",
        "-   `BATCH_SIZE`: Number of samples per device.\n",
        "-   `ORGANISM`: Target organism for predictions. Harded-coded to human for now.\n",
        "\n",
        "### Google Cloud Project Setup (outside this notebook's scope)\n",
        "To get started using Google Cloud Platform, you must have an existing Google Cloud project. Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mYgstS5SRCEv",
      "metadata": {
        "id": "7c4d2bb79a69"
      },
      "outputs": [],
      "source": [
        "# --- Google Cloud Platform variables ---\n",
        "PROJECT_ID = \"<your project>\"  # @param {type:'string'}\n",
        "BUCKET_NAME = \"<your bucket>\"  # @param {type: 'string'}\n",
        "\n",
        "# ---- Local file directories ----\n",
        "LOCAL_TAR_DIR = \"/tmp/tar_outputs\"\n",
        "LOCAL_BIGWIG_DIR = \"/tmp/bigwig\"\n",
        "LOCAL_FASTA_DIR = \"/tmp/fasta\"\n",
        "LOCAL_HUMAN_SEQ_DIR = \"/tmp/example_regions_path\"\n",
        "SAVE_CHECKPOINT_DIR = \"/tmp/checkpoint\"\n",
        "\n",
        "# --- AlphaGenome finetuning Model params---\n",
        "LEARNING_RATE = 5e-4\n",
        "NUM_TRAIN_STEPS = 1000\n",
        "MODEL_VERSION = dna_model.ModelVersion.FOLD_0\n",
        "SEQUENCE_LENGTH = int(2**20)\n",
        "BATCH_SIZE = 1  # Per device\n",
        "ORGANISM = dna_model.Organism.HOMO_SAPIENS\n",
        "\n",
        "# --- derived variables ---\n",
        "from datetime import datetime\n",
        "\n",
        "GCS_AGFT_NAME = f\"ag-ft-{BATCH_SIZE}-{NUM_TRAIN_STEPS}\"\n",
        "GCS_AGFT_NAME = f\"{GCS_AGFT_NAME}-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
        "GCS_AGFT_NAME = f\"{GCS_AGFT_NAME}.tar.gz\"\n",
        "GCS_AGFT_NAME = GCS_AGFT_NAME.replace(\"_\", \"-\")\n",
        "GCS_AGFT_PATH = f\"{BUCKET_NAME}/finetune/{GCS_AGFT_NAME}\"\n",
        "GCS_AGFT_PLOT_NAME = GCS_AGFT_NAME.replace(\".tar.gz\", \".png\")\n",
        "GCS_AGFT_PLOT_PATH = f\"{BUCKET_NAME}/finetune/{GCS_AGFT_PLOT_NAME}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ydvA5FV2sYT5",
      "metadata": {
        "id": "8f2738000438"
      },
      "source": [
        "## 4: Inputs preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aQxOns0W7l8-",
      "metadata": {
        "id": "db5e9c3c95d9"
      },
      "source": [
        "### Input - Fasta files\n",
        "* Download from [ENCODE](https://www.gencodegenes.org/human/release_46.html) portal directly.\n",
        "* unzip it\n",
        "* Create an index file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r0AcyUso7vF4",
      "metadata": {
        "id": "f75416f0bf63"
      },
      "outputs": [],
      "source": [
        "# Check if the file does NOT exist\n",
        "if not os.path.exists(LOCAL_FASTA_DIR):\n",
        "    ! mkdir -p $LOCAL_FASTA_DIR\n",
        "    ! echo wget -P \"$LOCAL_FASTA_DIR\" https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_46/GRCh38.p14.genome.fa.gz\n",
        "    ! wget -P \"$LOCAL_FASTA_DIR\" https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_46/GRCh38.p14.genome.fa.gz\n",
        "\n",
        "    # unzip, create an index file, .tar.gz, and upload to gcs\n",
        "    ! echo gunzip $LOCAL_FASTA_DIR/GRCh38.p14.genome.fa.gz\n",
        "    ! gunzip $LOCAL_FASTA_DIR/GRCh38.p14.genome.fa.gz\n",
        "\n",
        "    # install samtools to create the inded file\n",
        "    ! echo apt install samtools\n",
        "    ! apt install samtools\n",
        "\n",
        "    # create the index file\n",
        "    # the tool creates GRCh38.p14.genome.fa.fai and stores in the same dir.\n",
        "    ! echo samtools faidx $LOCAL_FASTA_DIR/GRCh38.p14.genome.fa\n",
        "    ! samtools faidx $LOCAL_FASTA_DIR/GRCh38.p14.genome.fa\n",
        "    print(\"All set to use fasta files in the training.\")\n",
        "else:\n",
        "    print(\"Going to use already prepared fasta files.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "k2rZepFVAdht",
      "metadata": {
        "id": "f782c845a98e"
      },
      "source": [
        "### Input - sequences_human.bed\n",
        "* Download from [GitHub Calico](https://github.com/calico/borzoi/raw/5c9358222b5026abb733ed5fb84f3f6c77239b37/data/sequences_human.bed.gz) portal directly.\n",
        "* unzip it\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qbaW4ijWAsaQ",
      "metadata": {
        "id": "df99fdb1d352"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(LOCAL_HUMAN_SEQ_DIR):\n",
        "    # create dir\n",
        "    ! echo mkdir $LOCAL_HUMAN_SEQ_DIR\n",
        "\n",
        "    # download the file\n",
        "    ! echo wget -P \"$LOCAL_HUMAN_SEQ_DIR\" https://github.com/calico/borzoi/raw/5c9358222b5026abb733ed5fb84f3f6c77239b37/data/sequences_human.bed.gz\n",
        "    ! wget -P \"$LOCAL_HUMAN_SEQ_DIR\" https://github.com/calico/borzoi/raw/5c9358222b5026abb733ed5fb84f3f6c77239b37/data/sequences_human.bed.gz\n",
        "\n",
        "    # unzip the file\n",
        "    ! echo gunzip $LOCAL_HUMAN_SEQ_DIR/sequences_human.bed.gz\n",
        "    ! gunzip $LOCAL_HUMAN_SEQ_DIR/sequences_human.bed.gz\n",
        "    print(\"All set to use human sequence file in the training.\")\n",
        "else:\n",
        "    print(\"Going to use already prepared human sequence files.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lMsnji1B5SMJ",
      "metadata": {
        "id": "e821d48b9727"
      },
      "source": [
        "### Input - BigWig files\n",
        "* Download from ENCODE portal directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R8kmPg71V4Ia",
      "metadata": {
        "id": "476426c06b62"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(LOCAL_BIGWIG_DIR):\n",
        "    # create the temp dire\n",
        "    ! mkdir -p $LOCAL_BIGWIG_DIR\n",
        "\n",
        "    # downloads the big wig files\n",
        "    ! pushd $LOCAL_BIGWIG_DIR && curl \\\n",
        "        -C - \\\n",
        "        -Z -O https://storage.googleapis.com/alphagenome/reference/encode/hg38/ENCFF018EZY.bigWig \\\n",
        "        -O https://storage.googleapis.com/alphagenome/reference/encode/hg38/ENCFF904TSK.bigWig \\\n",
        "        -O https://storage.googleapis.com/alphagenome/reference/encode/hg38/ENCFF218CLQ.bigWig && popd\n",
        "    print(\"All set to use the bigwig files in the training.\")\n",
        "else:\n",
        "    print(\"Going to use already prepared bigwig files.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zeyDbyuOshjP",
      "metadata": {
        "id": "5571a9eb3aff"
      },
      "source": [
        "## 5: Track Metadata\n",
        "\n",
        "Define which genomic tracks to finetune on. Each track requires:\n",
        "\n",
        "-   `name`: Human-readable name.\n",
        "-   `output_type`: Output type of assay (e.g., `RNA_SEQ`, `DNASE`, `CHIP_TF`,\n",
        "    `ATAC`). One of `dna_model.OutputType`.\n",
        "-   `strand`: Strand orientation (`+`, `-`, or `.` for unstranded)\n",
        "-   `nonzero_mean`: Optional mean of non-zero values (used for normalization).\n",
        "-   `file_path`: Path to the BigWig file containing the track data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Azgwuguospqp",
      "metadata": {
        "id": "20be846646fc"
      },
      "outputs": [],
      "source": [
        "TRACK_METADATA = pd.DataFrame(\n",
        "    data=[\n",
        "        [\n",
        "            \"RNA_SEQ\",\n",
        "            \"UBERON:0000948 total RNA-seq\",\n",
        "            \"+\",\n",
        "            f\"{LOCAL_BIGWIG_DIR}/ENCFF018EZY.bigWig\",\n",
        "        ],\n",
        "        [\n",
        "            \"RNA_SEQ\",\n",
        "            \"UBERON:0000948 total RNA-seq\",\n",
        "            \"-\",\n",
        "            f\"{LOCAL_BIGWIG_DIR}/ENCFF904TSK.bigWig\",\n",
        "        ],\n",
        "        [\n",
        "            \"DNASE\",\n",
        "            \"EFO:0005337 DNase-seq\",\n",
        "            \".\",\n",
        "            f\"{LOCAL_BIGWIG_DIR}/ENCFF218CLQ.bigWig\",\n",
        "        ],\n",
        "    ],\n",
        "    columns=[\"output_type\", \"name\", \"strand\", \"file_path\"],\n",
        ")\n",
        "TRACK_METADATA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wzjjJ7YuWdBZ",
      "metadata": {
        "id": "bbca14557fde"
      },
      "source": [
        "### Build Output Metadata\n",
        "\n",
        "Convert the track DataFrame into an `AlphaGenomeOutputMetadata` object that\n",
        "configures the model's output heads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "z_05I4wZWeWS",
      "metadata": {
        "id": "9415b4ee3717"
      },
      "outputs": [],
      "source": [
        "def build_output_metadata(\n",
        "    track_metadata: pd.DataFrame,\n",
        ") -> metadata_lib.AlphaGenomeOutputMetadata:\n",
        "    \"\"\"Builds AlphaGenomeOutputMetadata from the track metadata DataFrame.\n",
        "\n",
        "    Args:\n",
        "      track_metadata: A pandas DataFrame containing metadata for the tracks,\n",
        "        including 'output_type', 'name', 'strand', and 'file_path'.\n",
        "\n",
        "    Returns:\n",
        "      A dict mapping organism to AlphaGenomeOutputMetadata.\n",
        "    \"\"\"\n",
        "    required_cols = {\"file_path\", \"name\", \"output_type\", \"strand\"}\n",
        "    if not required_cols.issubset(track_metadata.columns):\n",
        "        raise ValueError(\n",
        "            f\"track_metadata must have columns {required_cols}. Missing: {required_cols - set(track_metadata.columns)}.\"\n",
        "        )\n",
        "    metadata = {}\n",
        "    for output_type, df_group in track_metadata.groupby(\"output_type\"):\n",
        "        try:\n",
        "            output_type = dna_model.OutputType[str(output_type)]\n",
        "        except KeyError as e:\n",
        "            raise ValueError(f\"Unknown output_type: {output_type}\") from e\n",
        "        metadata[output_type.name.lower()] = df_group\n",
        "    return metadata_lib.AlphaGenomeOutputMetadata(**metadata)\n",
        "\n",
        "\n",
        "output_metadata = {\n",
        "    dna_model.Organism.HOMO_SAPIENS: build_output_metadata(TRACK_METADATA)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8WBrYAnCXGTH",
      "metadata": {
        "id": "ef74f2221e1f"
      },
      "source": [
        "## 6: Data Pipeline\n",
        "\n",
        "Set up the training data iterator. This loads genomic intervals and\n",
        "corresponding track values from the specified BigWig files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5DE07nk5XJqN",
      "metadata": {
        "id": "5a16318e025c"
      },
      "outputs": [],
      "source": [
        "ds_iter = finetune.get_dataset_iterator(\n",
        "    batch_size=BATCH_SIZE * jax.local_device_count(),\n",
        "    sequence_length=SEQUENCE_LENGTH,\n",
        "    output_metadata=output_metadata[ORGANISM],\n",
        "    organism=ORGANISM,\n",
        "    model_version=MODEL_VERSION,\n",
        "    subset=fold_intervals.Subset.TRAIN,\n",
        "    fasta_path=f\"{LOCAL_FASTA_DIR}/GRCh38.p14.genome.fa\",\n",
        "    example_regions_path=f\"{LOCAL_HUMAN_SEQ_DIR}/sequences_human.bed\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dldd1UUXZ8l",
      "metadata": {
        "id": "7594457009e2"
      },
      "outputs": [],
      "source": [
        "# validate the shape\n",
        "batch = next(ds_iter)\n",
        "pprint.pprint(jax.tree.map(np.shape, batch))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xe4nDIOmXdIQ",
      "metadata": {
        "id": "c88915bf56fc"
      },
      "source": [
        "## 7: Model Initialization\n",
        "\n",
        "Load the pretrained AlphaGenome checkpoint and initialize new output heads for\n",
        "the finetuning tracks.\n",
        "\n",
        "You will be downloading weights from Huggingface.\n",
        "Ensure that:\n",
        "- You create a token that has 'Read access to contents of all public gated repos you can access' (under Finegrained control)\n",
        "- You accept the T&C of the [model](https://huggingface.co/google/alphagenome-fold-0).\n",
        "\n",
        "PS: A \"401 Unauthorized\" error in the cell below may occur if the Terms and Conditions (T&C) have not been accepted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vQvhdLR1XueG",
      "metadata": {
        "id": "e9d7ae634ca1"
      },
      "outputs": [],
      "source": [
        "# setup Huggingface credential and download the base model weights\n",
        "\n",
        "\n",
        "def setup_huggingface_auth(secret_id=\"HUGGINGFACE_API_TOKEN\", version_id=\"latest\"):\n",
        "    \"\"\"Fetches HF token from Secret Manager and configures auth.\"\"\"\n",
        "    try:\n",
        "        project_id = os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
        "        print(f\"{project_id=}\")\n",
        "        if not project_id:\n",
        "            try:\n",
        "                project_id = (\n",
        "                    subprocess.check_output(\n",
        "                        [\"gcloud\", \"config\", \"get-value\", \"project\"]\n",
        "                    )\n",
        "                    .decode(\"utf-8\")\n",
        "                    .strip()\n",
        "                )\n",
        "            except subprocess.CalledProcessError:\n",
        "                print(\"Could not automatically determine GCP Project ID.\")\n",
        "                return None\n",
        "\n",
        "        client = secretmanager.SecretManagerServiceClient()\n",
        "        name = f\"projects/{project_id}/secrets/{secret_id}/versions/{version_id}\"\n",
        "        response = client.access_secret_version(name=name)\n",
        "        hf_token = response.payload.data.decode(\"UTF-8\").strip()\n",
        "\n",
        "        if not hf_token:\n",
        "            print(f\"Secret {secret_id} is empty.\")\n",
        "            return None\n",
        "\n",
        "        print(\"Hugging Face token retrieved from Secret Manager.\")\n",
        "        return hf_token\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error setting up Hugging Face auth: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "hf_token = setup_huggingface_auth()\n",
        "\n",
        "if hf_token:\n",
        "    # Option 1: Log in using huggingface-cli\n",
        "    # This makes the token available for CLI commands and many libraries.\n",
        "    try:\n",
        "        # Use subprocess to handle the interactive nature of login\n",
        "        process = subprocess.Popen(\n",
        "            [\"huggingface-cli\", \"login\", \"--token\", hf_token],\n",
        "            stdin=subprocess.PIPE,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "        )\n",
        "        stdout, stderr = process.communicate()\n",
        "        if process.returncode == 0:\n",
        "            print(\"Hugging Face CLI login successful.\")\n",
        "        else:\n",
        "            print(f\"Hugging Face CLI login failed: {stderr.decode()}\")\n",
        "        print(stdout.decode())\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"huggingface-cli not found. Make sure huggingface_hub is installed.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during huggingface-cli login: {e}\")\n",
        "\n",
        "    # Option 2: Set as environment variable (useful for some tools)\n",
        "    os.environ[\"HF_TOKEN\"] = hf_token\n",
        "    print(\"HF_TOKEN environment variable set.\")\n",
        "\n",
        "    # Option 3: Programmatic login with huggingface_hub\n",
        "\n",
        "    try:\n",
        "        login(token=hf_token)\n",
        "        print(\"huggingface_hub programmatic login successful.\")\n",
        "    except Exception as e:\n",
        "        print(f\"huggingface_hub login error: {e}\")\n",
        "\n",
        "# Now try accessing the gated model again, for example:\n",
        "# from huggingface_hub import hf_hub_download\n",
        "repo = f\"google/alphagenome-{MODEL_VERSION.name.lower().replace('_', '-')}\"\n",
        "checkpoint_path = huggingface_hub.snapshot_download(repo_id=repo)\n",
        "checkpointer = ocp.StandardCheckpointer()\n",
        "params_base, state_base = checkpointer.restore(checkpoint_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2M5rGN5jZxW1",
      "metadata": {
        "id": "f11bfff85c19"
      },
      "source": [
        "### Set Up Device Mesh for Data Parallelism"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qUI7K6ipZzJ3",
      "metadata": {
        "id": "8dce769c3ea4"
      },
      "outputs": [],
      "source": [
        "num_devices = jax.local_device_count()\n",
        "devices = mesh_utils.create_device_mesh((num_devices,))\n",
        "mesh = Mesh(devices, axis_names=(\"data\",))\n",
        "data_sharding = P(\"data\")\n",
        "replicated_sharding = P()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B1kvhnAuZ6uF",
      "metadata": {
        "id": "f49f176f64d9"
      },
      "source": [
        "### Initialize New Output Heads\n",
        "\n",
        "Create the forward function configured for our finetuning tracks and initialize\n",
        "the new head parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nJGvL3VFZ9VZ",
      "metadata": {
        "id": "48d220e2a8dd"
      },
      "outputs": [],
      "source": [
        "forward_fn = finetune.get_forward_fn(output_metadata)\n",
        "with jax.set_mesh(mesh):\n",
        "    batch = jax.device_put(batch, data_sharding)\n",
        "    params_ft, state_ft = jax.jit(\n",
        "        forward_fn.init,\n",
        "        in_shardings=(replicated_sharding, data_sharding),\n",
        "        out_shardings=replicated_sharding,\n",
        "    )(jax.random.PRNGKey(0), batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oJRHuPmxaBTl",
      "metadata": {
        "id": "053d2f1ce485"
      },
      "source": [
        "### Merge Pretrained Trunk with New Heads\n",
        "\n",
        "Perform weight surgery: keep the pretrained trunk parameters and replace the\n",
        "head parameters with the newly initialized ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ctPEHZEaAbW",
      "metadata": {
        "id": "03854fafe49f"
      },
      "outputs": [],
      "source": [
        "params_ft_head = hk.data_structures.filter(\n",
        "    lambda module_name, *_: \"head\" in module_name, params_ft\n",
        ")\n",
        "params_base_no_head = hk.data_structures.filter(\n",
        "    lambda module_name, *_: \"head\" not in module_name, params_base\n",
        ")\n",
        "params = hk.data_structures.merge(params_base_no_head, params_ft_head)\n",
        "state = state_base\n",
        "optimizer = optax.chain(\n",
        "    optax.clip_by_global_norm(0.5),\n",
        "    optax.adam(LEARNING_RATE),\n",
        ")\n",
        "opt_state = optimizer.init(params)\n",
        "train_step = jax.jit(\n",
        "    finetune.get_train_step(forward_fn.apply, optimizer),\n",
        "    in_shardings=(\n",
        "        replicated_sharding,\n",
        "        replicated_sharding,\n",
        "        replicated_sharding,\n",
        "        data_sharding,\n",
        "    ),\n",
        "    out_shardings=(\n",
        "        replicated_sharding,\n",
        "        replicated_sharding,\n",
        "        replicated_sharding,\n",
        "        replicated_sharding,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gG6lx-v7aIuC",
      "metadata": {
        "id": "b4ce037a93df"
      },
      "source": [
        "## 8: Training Loop\n",
        "\n",
        "Set up checkpointing and run the finetuning training loop."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eJNFkuIEaVXH",
      "metadata": {
        "id": "d269175d30c7"
      },
      "source": [
        "### Configure Checkpoint Directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LA33-TLhaReH",
      "metadata": {
        "id": "48bba50831ae"
      },
      "outputs": [],
      "source": [
        "path_suffix = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "checkpoint_dir = epath.Path(SAVE_CHECKPOINT_DIR) / path_suffix\n",
        "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "checkpoint_dir = str(checkpoint_dir)\n",
        "print(f\"We will be saving the trained checkpoint at {checkpoint_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q2nKwlXrajSg",
      "metadata": {
        "id": "1f5406d18887"
      },
      "outputs": [],
      "source": [
        "checkpointer = ocp.StandardCheckpointer()\n",
        "\n",
        "\n",
        "def save(weights, idx):\n",
        "    ckpt_path = os.path.join(checkpoint_dir, \"checkpoint_{:05d}\".format(idx))\n",
        "    print(f\"Saving checkpoint to {ckpt_path}\")\n",
        "    checkpointer.save(ckpt_path, weights)\n",
        "    checkpointer.wait_until_finished()\n",
        "    print(f\"Saved checkpoint to {ckpt_path}\")\n",
        "    return ckpt_path"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4qGJBNCsamP3",
      "metadata": {
        "id": "257a65ab162b"
      },
      "source": [
        "### Run Training and Save chkpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w7Hlb2l4anWX",
      "metadata": {
        "id": "b44c978832e4"
      },
      "outputs": [],
      "source": [
        "loss, times = [], []\n",
        "for step in range(NUM_TRAIN_STEPS):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        batch = next(ds_iter)\n",
        "    except StopIteration:\n",
        "        print(\"Dataset exhausted\")\n",
        "        break\n",
        "    with jax.set_mesh(mesh):\n",
        "        batch = jax.device_put(batch, data_sharding)\n",
        "        params, state, opt_state, scalars = train_step(params, state, opt_state, batch)\n",
        "    loss.append(scalars[\"loss\"])\n",
        "    times.append(time.time() - start_time)\n",
        "    if step % 10 == 1:\n",
        "        print(\"loss\", step, loss[-1], f\"SPS: {1./np.mean(times[1:]):.4f}\")\n",
        "\n",
        "print(f\"Total Training time: {np.sum(times[1:]):.4f} seconds\")\n",
        "print(f\"Average Training time per step: {np.mean(times[1:]):.4f} seconds\")\n",
        "ckpt_path = save((params, state), step + 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7tLU9ttXbDI_",
      "metadata": {
        "id": "e32a2eebbf0b"
      },
      "source": [
        "### Upload the chkpt to Google Cloud Storage\n",
        "\n",
        "For future reference and usage, save a copy in the Google Cloud Storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XTgyXH4ya8n-",
      "metadata": {
        "id": "46d9870ac647"
      },
      "outputs": [],
      "source": [
        "# Check if the file or directory does NOT exist\n",
        "# gcs path to where the chkpt will be uploaded\n",
        "os.makedirs(LOCAL_TAR_DIR, exist_ok=True)\n",
        "\n",
        "# tar the files\n",
        "!echo tar -czvf $LOCAL_TAR_DIR/$GCS_AGFT_NAME $ckpt_path\n",
        "!tar -czvf $LOCAL_TAR_DIR/$GCS_AGFT_NAME $ckpt_path\n",
        "\n",
        "# list the tar file\n",
        "!echo ls -l $LOCAL_TAR_DIR/$GCS_AGFT_NAME\n",
        "!ls -l $LOCAL_TAR_DIR/$GCS_AGFT_NAME\n",
        "\n",
        "# upload the tar file to gcs bucket\n",
        "!echo gsutil cp $LOCAL_TAR_DIR/$GCS_AGFT_NAME $GCS_AGFT_PATH\n",
        "!gsutil cp $LOCAL_TAR_DIR/$GCS_AGFT_NAME $GCS_AGFT_PATH\n",
        "\n",
        "# list the tar file gcs bucket\n",
        "!echo gsutil ls $GCS_AGFT_PATH\n",
        "!gsutil ls $GCS_AGFT_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Of38if4E4v1r",
      "metadata": {
        "id": "4d6c1a3849e2"
      },
      "source": [
        "## 9: Inference with Finetuned Model\n",
        "\n",
        "Load the finetuned checkpoint into a `DnaModel` for inference and compare\n",
        "predictions against ground truth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bDXgEo2n3Dg5",
      "metadata": {
        "id": "5cc8c5b3c172"
      },
      "outputs": [],
      "source": [
        "# Load default organism settings but overwrite with fine-tuned output metadata.\n",
        "default_settings_human = dna_model.default_organism_settings()[\n",
        "    dna_model.Organism.HOMO_SAPIENS\n",
        "]\n",
        "settings_human_finetune = dataclasses.replace(\n",
        "    default_settings_human,\n",
        "    metadata=output_metadata[dna_model.Organism.HOMO_SAPIENS],\n",
        ")\n",
        "model = dna_model.create(\n",
        "    ckpt_path,\n",
        "    organism_settings={dna_model.Organism.HOMO_SAPIENS: settings_human_finetune},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efeiuHYc5NBT",
      "metadata": {
        "id": "63b082e0c2a1"
      },
      "source": [
        "### Select Test Interval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W8Jjke2h5BI8",
      "metadata": {
        "id": "81fd40b30aba"
      },
      "outputs": [],
      "source": [
        "interval = genome.Interval(chromosome=\"chr21\", start=46125238, end=46126738).resize(\n",
        "    SEQUENCE_LENGTH\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BLd5Nuqp5QzH",
      "metadata": {
        "id": "550d3ad04901"
      },
      "outputs": [],
      "source": [
        "preds = model.predict_interval(\n",
        "    interval,\n",
        "    requested_outputs=[dna_model.OutputType.RNA_SEQ],\n",
        "    ontology_terms=None,\n",
        ")\n",
        "preds"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zIjmLyzj5eoo",
      "metadata": {
        "id": "0e2359e0e48c"
      },
      "source": [
        "### Load Ground Truth Tracks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fIcdJj_95a9S",
      "metadata": {
        "id": "acbec3306ed1"
      },
      "outputs": [],
      "source": [
        "true_tracks = dataset_lib.MultiTrackExtractor(\n",
        "    output_metadata[ORGANISM], sequence_length=SEQUENCE_LENGTH\n",
        ").extract(interval)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hX1yDnaq50Ge",
      "metadata": {
        "id": "58ffc26a6998"
      },
      "source": [
        "### Visualize Predictions vs Ground Truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4Gms30nk5owl",
      "metadata": {
        "id": "9734af8d50c4"
      },
      "outputs": [],
      "source": [
        "def compact_dict(**kwargs):\n",
        "    return {k: v for k, v in kwargs.items() if v is not None}\n",
        "\n",
        "\n",
        "def plot(*, interval, predictions, targets=None):\n",
        "    if targets is None:\n",
        "        colors = {\"pred\": \"black\"}\n",
        "    else:\n",
        "        colors = {\"pred\": \"black\", \"true\": \"red\"}\n",
        "    fig = plot_components.plot(\n",
        "        [\n",
        "            plot_components.OverlaidTracks(\n",
        "                tdata=compact_dict(\n",
        "                    pred=predictions.rna_seq,\n",
        "                    true=(\n",
        "                        dataclasses.replace(\n",
        "                            predictions.rna_seq,\n",
        "                            values=targets[\"rna_seq\"].astype(np.float32),\n",
        "                        )\n",
        "                        if targets is not None\n",
        "                        else None\n",
        "                    ),\n",
        "                ),\n",
        "                colors=colors,\n",
        "                shared_y_scale=True,\n",
        "            ),\n",
        "        ],\n",
        "        interval=interval.resize(int(2**11)),\n",
        "    )\n",
        "    return fig\n",
        "\n",
        "\n",
        "pred_vs_groundtruth_fig = plot(\n",
        "    predictions=preds, interval=interval, targets=true_tracks\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gd2kDQpQMQBl",
      "metadata": {
        "id": "aeb9b1187f8d"
      },
      "source": [
        "### Save the plot and upload to Google Cloud Storage for latter reference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WTOsLsRnEOAe",
      "metadata": {
        "id": "0361f1ab658c"
      },
      "outputs": [],
      "source": [
        "# Save the figure to a file\n",
        "file_path = (\n",
        "    f\"{LOCAL_TAR_DIR}/{GCS_AGFT_PLOT_NAME}\"  # Choose your desired filename and format\n",
        ")\n",
        "pred_vs_groundtruth_fig.savefig(file_path)\n",
        "print(f\"Plot saved to {file_path}\")\n",
        "\n",
        "# upload the image file to gcs bucket\n",
        "!echo gsutil cp $file_path $GCS_AGFT_PLOT_PATH\n",
        "!gsutil cp $file_path $GCS_AGFT_PLOT_PATH\n",
        "\n",
        "# list the tar file gcs bucket\n",
        "!echo gsutil ls $GCS_AGFT_PLOT_PATH\n",
        "!gsutil ls $GCS_AGFT_PLOT_PATH"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "cloudai_alphagenome_finetune.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

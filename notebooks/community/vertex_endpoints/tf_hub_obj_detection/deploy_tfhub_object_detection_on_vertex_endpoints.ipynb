{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zB_PYUGd7-ko"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bcfa29cc2be"
      },
      "source": [
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/vertex_endpoints/tf_hub_obj_detection/deploy_tfhub_object_detection_on_vertex_endpoints.ipynb\"\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/vertex_endpoints/tf_hub_obj_detection/deploy_tfhub_object_detection_on_vertex_endpoints.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/vertex_endpoints/tf_hub_obj_detection/deploy_tfhub_object_detection_on_vertex_endpoints.ipynb\"\n",
        "      <img src=\"https://cloud.google.com/images/products/ai/ai-solutions-icon.svg\" alt=\"Vertex AI Workbench notebook\"> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td> \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmDKdOFh8Ko8"
      },
      "source": [
        "# Deploying a TensorFlow Hub object detection model using Vertex AI Endpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTe0sT1p8SHy"
      },
      "source": [
        "## Overview\n",
        "This tutorial demonstrates how to take a TensorFlow Hub object detection model, add a preprocessing layer and deploy it to a Vertex AI endpoint for online prediction.\n",
        "\n",
        "Because the object detection model accepts tensors as an input, we will add a preprocessing layer that accepts jpeg strings and decodes them. This makes it easier for clients to call the endpoint without having to implement their own TensorFlow logic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8OldBdp8VeZ"
      },
      "source": [
        "## Model\n",
        "The model used for this tutorial is the `CenterNet HourGlass104 Keypoints 512x512` from [TensorFlow Hub open source model repository](https://tfhub.dev/tensorflow/centernet/hourglass_512x512_kpts/1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DU7F8BCg8WgZ"
      },
      "source": [
        "## Objective\n",
        "\n",
        "The steps performed include:\n",
        "- Download a object detection model from TensorFlow Hub.\n",
        "- Create a preprocessing layer using @tf.function.\n",
        "- Upload the model to Vertex AI `Models`.\n",
        "- Create a Vertex AI `Endpoint`.\n",
        "- Call the endpoint with both the `Python Vertex AI SDK` and through command line using `CURL`.\n",
        "- Undeploy the endpoint and delete the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT8PQtz68aRa"
      },
      "source": [
        "## Costs\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "- Vertex AI\n",
        "- Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHC5aja38dPU"
      },
      "source": [
        "## Installation\n",
        "Install the latest version of Vertex SDK for Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NdnlHDb8gk7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0E-rLjd8kPJ"
      },
      "outputs": [],
      "source": [
        "! pip install {USER_FLAG} --upgrade google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCNy-5A19Iu8"
      },
      "source": [
        "Install TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZUC9iJa9TPd"
      },
      "outputs": [],
      "source": [
        "!pip install -U \"tensorflow>=2.7\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyv85kWHvkQ2"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "Once you've installed everything, you need to restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "av0sLWCDvmL6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Peg2Bwy_v2fe"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "3. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "4. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
        "\n",
        "5. Enter your project ID in the cell below. Then run the cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NRbJL7iwTwm"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you might be able to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVrtnNtuwOgv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"\"\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Get your Google Cloud project ID from gcloud\n",
        "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID: \", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHaNeGniwQwf"
      },
      "source": [
        "Otherwise, set your project ID here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wU5w8-WwakX"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
        "    PROJECT_ID = \"\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3YQn1FNv7I9"
      },
      "source": [
        "#### Timestamp\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, create a timestamp for each instance session, and append it onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TU_3QS-Rwk0v"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlLFGqlov_Ht"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using Google Cloud Notebooks**, your environment is already\n",
        "authenticated. Skip this step.\n",
        "\n",
        "**If you are using Colab**, run the cell below and follow the instructions\n",
        "when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "1. In the Cloud Console, go to the [**Create service account key**\n",
        "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
        "\n",
        "2. Click **Create service account**.\n",
        "\n",
        "3. In the **Service account name** field, enter a name, and\n",
        "   click **Create**.\n",
        "\n",
        "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
        "into the filter box, and select\n",
        "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "\n",
        "5. Click *Create*. A JSON file that contains your key downloads to your\n",
        "local environment.\n",
        "\n",
        "6. Enter the path to your service account key as the\n",
        "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBTUnqS1wqHo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# If on Google Cloud Notebooks, then don't execute this code\n",
        "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPrx2LnU8vFU"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "You first upload the model files to a Cloud Storage bucket. Using this model artifact, you can then\n",
        "create Vertex AI model and endpoint resources in order to serve\n",
        "online predictions.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
        "Cloud Storage buckets.\n",
        "\n",
        "You may also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook. Make sure to [choose a region where Vertex AI services are\n",
        "available](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions). You may\n",
        "not use a Multi-Regional Storage bucket for training with Vertex AI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef0nYMRKxvDV"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"\"  # @param {type:\"string\"}\n",
        "REGION = \"us-central1\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ti_79ErvxxeF"
      },
      "outputs": [],
      "source": [
        "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
        "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ig18nZMtmuGH"
      },
      "outputs": [],
      "source": [
        "print(BUCKET_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCZRfgA9x0Mz"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDX_aWtjxzSN"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -p $PROJECT_ID -l $REGION $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7IDVRdwx4hd"
      },
      "source": [
        "Finally, validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iA6Uqvqxx8Ls"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me1llTRsyImc"
      },
      "source": [
        "## Download and extract the model\n",
        "There are various object detection models in TensorFlow Hub. We will be using the `CenterNet HourGlass104 Keypoints 512x512`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6dmPQqxzGZp"
      },
      "outputs": [],
      "source": [
        "# Download and extract model\n",
        "!wget https://tfhub.dev/tensorflow/centernet/hourglass_512x512_kpts/1?tf-hub-format=compressed\n",
        "!tar xvzf 1?tf-hub-format=compressed\n",
        "!mkdir obj_detect_model\n",
        "!mv ./saved_model.pb obj_detect_model/\n",
        "!mv ./variables obj_detect_model/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAF8aH4HzXEY"
      },
      "source": [
        "## Visualization tools\n",
        "To visualize the images with the proper detected boxes, keypoints and segmentation, we will use the TensorFlow Object Detection API. To install it we will clone the repo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fV_q5QTakk4H"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from six import BytesIO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sztJl5lE0Lxd"
      },
      "outputs": [],
      "source": [
        "# Clone the tensorflow models repository\n",
        "!git clone --depth 1 https://github.com/tensorflow/models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYF0OD280SF-"
      },
      "source": [
        "Installing the object detection API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3m_twvfY0U9k"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "sudo apt install -y protobuf-compiler\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "pip install ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awZo_FXg1dkn"
      },
      "source": [
        "Now we can import the dependencies we will need later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aDl_EEH1lHh"
      },
      "outputs": [],
      "source": [
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXIEr4W52QY"
      },
      "source": [
        "## Load label map data (for plotting).\n",
        "Label maps correspond index numbers to category names, so that when our convolution network predicts 5, we know that this corresponds to airplane. Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine.\n",
        "\n",
        "We are going, for simplicity, to load from the repository that we loaded the Object Detection API code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e52z_Wi5Ar9"
      },
      "outputs": [],
      "source": [
        "PATH_TO_LABELS = \"./models/research/object_detection/data/mscoco_label_map.pbtxt\"\n",
        "category_index = label_map_util.create_category_index_from_labelmap(\n",
        "    PATH_TO_LABELS, use_display_name=True\n",
        ")\n",
        "print(category_index[5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swaDX8uE6Ksn"
      },
      "source": [
        "## Load the model\n",
        "Here we will load the downloaded model into memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0eejzM26QXb"
      },
      "outputs": [],
      "source": [
        "model = tf.saved_model.load(\"obj_detect_model/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckNI8ZRW63od"
      },
      "source": [
        "Load an image and use the model for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-J2YDW6r6_wO"
      },
      "outputs": [],
      "source": [
        "image_path = \"models/research/object_detection/test_images/image2.jpg\"\n",
        "\n",
        "\n",
        "def load_image_into_numpy_array(path):\n",
        "    image_data = tf.io.gfile.GFile(path, \"rb\").read()\n",
        "    image = Image.open(BytesIO(image_data))\n",
        "\n",
        "    (width, height) = image.size\n",
        "    return np.array(image.getdata()).reshape((1, height, width, 3)).astype(np.uint8)\n",
        "\n",
        "\n",
        "image_np = load_image_into_numpy_array(image_path)\n",
        "plt.figure(figsize=(24, 32))\n",
        "plt.imshow(image_np[0])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "results = model(image_np)\n",
        "result = {key: value.numpy() for key, value in results.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zMDFKfM7vuT"
      },
      "source": [
        "Visualize results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRV_-Q8x7uye"
      },
      "outputs": [],
      "source": [
        "COCO17_HUMAN_POSE_KEYPOINTS = [\n",
        "    (0, 1),\n",
        "    (0, 2),\n",
        "    (1, 3),\n",
        "    (2, 4),\n",
        "    (0, 5),\n",
        "    (0, 6),\n",
        "    (5, 7),\n",
        "    (7, 9),\n",
        "    (6, 8),\n",
        "    (8, 10),\n",
        "    (5, 6),\n",
        "    (5, 11),\n",
        "    (6, 12),\n",
        "    (11, 12),\n",
        "    (11, 13),\n",
        "    (13, 15),\n",
        "    (12, 14),\n",
        "    (14, 16),\n",
        "]\n",
        "\n",
        "label_id_offset = 0\n",
        "image_np_with_detections = image_np.copy()\n",
        "\n",
        "# Use keypoints if available in detections\n",
        "keypoints, keypoint_scores = None, None\n",
        "if \"detection_keypoints\" in result:\n",
        "    keypoints = result[\"detection_keypoints\"][0]\n",
        "    keypoint_scores = result[\"detection_keypoint_scores\"][0]\n",
        "\n",
        "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "    image_np_with_detections[0],\n",
        "    result[\"detection_boxes\"][0],\n",
        "    (result[\"detection_classes\"][0] + label_id_offset).astype(int),\n",
        "    result[\"detection_scores\"][0],\n",
        "    category_index,\n",
        "    use_normalized_coordinates=True,\n",
        "    max_boxes_to_draw=200,\n",
        "    min_score_thresh=0.30,\n",
        "    agnostic_mode=False,\n",
        "    keypoints=keypoints,\n",
        "    keypoint_scores=keypoint_scores,\n",
        "    keypoint_edges=COCO17_HUMAN_POSE_KEYPOINTS,\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(24, 32))\n",
        "plt.imshow(image_np_with_detections[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb4xge9UE-ka"
      },
      "source": [
        "## Create a preprocessing function for Vertex AI serving.\n",
        "The model expects a numpy array as an input. This creates two problems for our endpoint:\n",
        "* Vertex AI public endpoints have a maximum request size of 1.5 MB. Images are much larger than this.\n",
        "* It would make it more difficult for clients based in languages other than Python to build a request.\n",
        "\n",
        "These two limitations can be solved by building a preprocessing function and attaching it to our model.\n",
        "\n",
        "We will create a preprocessing function that takes a jpeg encoded image, resizes it to the model's minimum required input and passes this preprocessed input to the model. We will then save the model with the preprocessing function which will be ready to be uploaded to our Vertex AI endpoint.\n",
        "\n",
        "The image will be passed to our endpoint as a base64 encoded jpeg string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5U3eLrbK3oH"
      },
      "outputs": [],
      "source": [
        "VERTEX_MODEL_PATH = \"obj_detect_model_vertex/\"\n",
        "\n",
        "\n",
        "def _preprocess(bytes_inputs):\n",
        "    decoded = tf.io.decode_jpeg(bytes_inputs, channels=3)\n",
        "    resized = tf.image.resize(decoded, size=(512, 512))\n",
        "    return tf.cast(resized, dtype=tf.uint8)\n",
        "\n",
        "\n",
        "def _get_serve_image_fn(model):\n",
        "    @tf.function(input_signature=[tf.TensorSpec([None], tf.string)])\n",
        "    def serve_image_fn(bytes_inputs):\n",
        "        decoded_images = tf.map_fn(_preprocess, bytes_inputs, dtype=tf.uint8)\n",
        "        return model(decoded_images)\n",
        "\n",
        "    return serve_image_fn\n",
        "\n",
        "\n",
        "signatures = {\n",
        "    \"serving_default\": _get_serve_image_fn(model).get_concrete_function(\n",
        "        tf.TensorSpec(shape=[None], dtype=tf.string)\n",
        "    )\n",
        "}\n",
        "\n",
        "tf.saved_model.save(model, VERTEX_MODEL_PATH, signatures=signatures)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPVVu5xcMTtm"
      },
      "source": [
        "We will verify that the input was modified correctly by using the `saved_model_cli` command on both the original and vertexai prepared model.\n",
        "\n",
        "The results for the `serving_default` signature should be as follows.\n",
        "\n",
        "\n",
        "Original model:\n",
        "\n",
        "```\n",
        "signature_def['serving_default']:\n",
        "  The given SavedModel SignatureDef contains the following input(s):\n",
        "    inputs['input_tensor'] tensor_info:\n",
        "        dtype: DT_UINT8\n",
        "        shape: (1, -1, -1, 3)\n",
        "        name: serving_default_input_tensor:0\n",
        "```\n",
        "\n",
        "Vertex AI model:\n",
        "\n",
        "```\n",
        "signature_def['serving_default']:\n",
        "  The given SavedModel SignatureDef contains the following input(s):\n",
        "    inputs['bytes_inputs'] tensor_info:\n",
        "        dtype: DT_STRING\n",
        "        shape: (-1)\n",
        "        name: serving_default_bytes_inputs:0\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJ1x3YISNWuH"
      },
      "outputs": [],
      "source": [
        "!saved_model_cli show --dir obj_detect_model --all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trfpB2A8NS9k"
      },
      "outputs": [],
      "source": [
        "!saved_model_cli show --dir obj_detect_model_vertex --all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2vgZvn2NA2h"
      },
      "source": [
        "Lets test the preprocessing function by passing it a base 64 encoded jpeg image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GT55Kj_NGfC"
      },
      "outputs": [],
      "source": [
        "vertex_model = tf.saved_model.load(VERTEX_MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f8z6G7yNPaJ"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "\n",
        "\n",
        "def encode_image(image):\n",
        "    with open(image, \"rb\") as image_file:\n",
        "        encoded_string = base64.urlsafe_b64encode(image_file.read()).decode(\"utf-8\")\n",
        "    return encoded_string\n",
        "\n",
        "\n",
        "results = vertex_model([_preprocess(tf.io.decode_base64(encode_image(image_path)))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwntQ3m1O8s_"
      },
      "source": [
        "View the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hjSVSWiO-Qh"
      },
      "outputs": [],
      "source": [
        "# different object detection models have additional results\n",
        "# all of them are explained in the documentation\n",
        "result = {key: value.numpy() for key, value in results.items()}\n",
        "\n",
        "label_id_offset = 0\n",
        "image_np_with_detections = image_np.copy()\n",
        "\n",
        "# Use keypoints if available in detections\n",
        "keypoints, keypoint_scores = None, None\n",
        "if \"detection_keypoints\" in result:\n",
        "    keypoints = result[\"detection_keypoints\"][0]\n",
        "    keypoint_scores = result[\"detection_keypoint_scores\"][0]\n",
        "\n",
        "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "    image_np_with_detections[0],\n",
        "    result[\"detection_boxes\"][0],\n",
        "    (result[\"detection_classes\"][0] + label_id_offset).astype(int),\n",
        "    result[\"detection_scores\"][0],\n",
        "    category_index,\n",
        "    use_normalized_coordinates=True,\n",
        "    max_boxes_to_draw=200,\n",
        "    min_score_thresh=0.30,\n",
        "    agnostic_mode=False,\n",
        "    keypoints=keypoints,\n",
        "    keypoint_scores=keypoint_scores,\n",
        "    keypoint_edges=COCO17_HUMAN_POSE_KEYPOINTS,\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(24, 32))\n",
        "plt.imshow(image_np_with_detections[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43uTMJaQScuf"
      },
      "source": [
        "## Create a Vertex AI endpoint\n",
        "In this section we will upload the model to Google Cloud Storage and reference it inside Vertex AI for endpoints deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJg_8vLblmzQ"
      },
      "outputs": [],
      "source": [
        "!gsutil cp -r $VERTEX_MODEL_PATH $BUCKET_NAME/obj_detection_model_vertex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTKUaAyznRow"
      },
      "outputs": [],
      "source": [
        "!gsutil ls $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKdYthMAnok9"
      },
      "source": [
        "Create a model in Vertex AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKfDMn_Enqpn"
      },
      "outputs": [],
      "source": [
        "!gcloud ai models upload \\\n",
        "--region=us-central1 \\\n",
        "--project=$PROJECT_ID \\\n",
        "--display-name=object-detection \\\n",
        "--container-image-uri=us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-5:latest \\\n",
        "--artifact-uri=$BUCKET_NAME/obj_detection_model_vertex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxIgOjsltxdE"
      },
      "source": [
        "Create endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvGJgk8TulMl"
      },
      "outputs": [],
      "source": [
        "!gcloud ai endpoints create \\\n",
        "--project=$PROJECT_ID \\\n",
        "--region=$REGION \\\n",
        "--display-name=object-detection-endpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2cda10ebd5e"
      },
      "source": [
        "Retrieve MODEL_ID and ENDPPOINT_ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bff0694f6581"
      },
      "outputs": [],
      "source": [
        "%%bash -s \"$REGION\" \"$PROJECT_ID\" --out MODEL_ID\n",
        "MODEL_ID=`gcloud ai models list --region=$1 --project=$2 | grep object-detection`\n",
        "echo $MODEL_ID | cut -d' ' -f1 | tr -d '\\n'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e1ea39293ec"
      },
      "outputs": [],
      "source": [
        "%%bash -s \"$REGION\" \"$PROJECT_ID\" --out ENDPOINT_ID\n",
        "ENDPOINT_ID=`gcloud ai endpoints list --region=$1 --project=$2 | sed -n 2p`\n",
        "echo $ENDPOINT_ID | cut -d' ' -f1 | tr -d '\\n'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KB3pJ-nDt0gj"
      },
      "outputs": [],
      "source": [
        "!gcloud ai endpoints deploy-model $ENDPOINT_ID \\\n",
        "--project=$PROJECT_ID \\\n",
        "--region=$REGION \\\n",
        "--model=$MODEL_ID \\\n",
        "--display-name=object-detection-endpoint \\\n",
        "--traffic-split=0=100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcFSSWAmIevj"
      },
      "source": [
        "Write the request to a json file and call the endpoint using Curl.\n",
        "\n",
        "First we need to reduce our image memory footprint. As of Feb. 2022, Vertex AI endpoints has a maximum request size of 1.5mb. This is done to keep the containers behind endpoints from crashing during heavy load times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hK2sGh95KtpA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "print(os.stat(image_path).st_size)\n",
        "\n",
        "im = Image.open(image_path)\n",
        "im.save(\"image2.jpg\", quality=95)\n",
        "print(os.stat(\"image2.jpg\").st_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOPq1R2y6AvP"
      },
      "outputs": [],
      "source": [
        "!echo {\"\\\"\"instances\"\\\"\" : [{\"\\\"\"bytes_inputs\"\\\"\" : {\"\\\"\"b64\"\\\"\" : \"\\\"\"$(base64 \"image2.jpg\")\"\\\"\"}}]} > instances.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHbKrb7uw9Qn"
      },
      "outputs": [],
      "source": [
        "!curl POST  \\\n",
        "-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
        "-H \"Content-Type: application/json\" \\\n",
        "https://us-central1-aiplatform.googleapis.com/v1/projects/$PROJECT_ID/locations/us-central1/endpoints/$ENDPOINT_ID:predict \\\n",
        "-d @instances.json > results.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrG91gkiFd8E"
      },
      "source": [
        "## Make Predictions using the Vertex SDK\n",
        "The Vertex SDK has convenient methods to call endpoints to make predictions.\n",
        "First, we get the serving input from the model. This is what the endpoint expects as a key for the base64 encoded image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPDEyXplFiOr"
      },
      "outputs": [],
      "source": [
        "# Get the input key\n",
        "serving_input = list(\n",
        "    vertex_model.signatures[\"serving_default\"].structured_input_signature[1].keys()\n",
        ")[0]\n",
        "print(\"Serving input :\", serving_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjAKDKKuJWJA"
      },
      "source": [
        "Load an endpoint object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZAbDHD4FlJd"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "aip_endpoint_name = (\n",
        "    f\"projects/{PROJECT_ID}/locations/us-central1/endpoints/{ENDPOINT_ID}\"\n",
        ")\n",
        "endpoint = aiplatform.Endpoint(aip_endpoint_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBC_WUG7HDFe"
      },
      "outputs": [],
      "source": [
        "from google.protobuf import json_format\n",
        "from google.protobuf.struct_pb2 import Value\n",
        "\n",
        "\n",
        "# Endpoints will do the base64 decoding, so we change the function to encode the image a bit.\n",
        "def encode_image_bytes(image_path):\n",
        "    bytes = tf.io.read_file(image_path)\n",
        "    return base64.b64encode(bytes.numpy()).decode(\"utf-8\")\n",
        "\n",
        "\n",
        "instances_list = [{serving_input: {\"b64\": encode_image_bytes(\"image2.jpg\")}}]\n",
        "instances = [json_format.ParseDict(s, Value()) for s in instances_list]\n",
        "results = endpoint.predict(instances=instances)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRMUoG9zJg0_"
      },
      "source": [
        "View results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARTQn8dRHV-P"
      },
      "outputs": [],
      "source": [
        "# different object detection models have additional results\n",
        "# all of them are explained in the documentation\n",
        "prediction_results = results.predictions[0]\n",
        "result = {key: np.array([value]) for key, value in prediction_results.items()}\n",
        "\n",
        "label_id_offset = 0\n",
        "image_np_with_detections = image_np.copy()\n",
        "\n",
        "# Use keypoints if available in detections\n",
        "keypoints, keypoint_scores = None, None\n",
        "if \"detection_keypoints\" in result:\n",
        "    keypoints = result[\"detection_keypoints\"][0]\n",
        "    keypoint_scores = result[\"detection_keypoint_scores\"][0]\n",
        "\n",
        "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "    image_np_with_detections[0],\n",
        "    result[\"detection_boxes\"][0],\n",
        "    (result[\"detection_classes\"][0] + label_id_offset).astype(int),\n",
        "    result[\"detection_scores\"][0],\n",
        "    category_index,\n",
        "    use_normalized_coordinates=True,\n",
        "    max_boxes_to_draw=200,\n",
        "    min_score_thresh=0.30,\n",
        "    agnostic_mode=False,\n",
        "    keypoints=keypoints,\n",
        "    keypoint_scores=keypoint_scores,\n",
        "    keypoint_edges=COCO17_HUMAN_POSE_KEYPOINTS,\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(24, 32))\n",
        "plt.imshow(image_np_with_detections[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c83b5cf6819"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aokup0x_ZiJK"
      },
      "outputs": [],
      "source": [
        "%%bash -s \"$ENDPOINT_ID\" \"$REGION\" \"$PROJECT_ID\" --out ENDPOINT_MODEL_ID\n",
        "ENDPOINT_MODEL_ID=$(gcloud ai endpoints describe $1 --region=$2 --project=$3 | grep \"id:\")\n",
        "ENDPOINT_MODEL_ID=`echo $ENDPOINT_MODEL_ID | cut -d' ' -f2`\n",
        "echo $ENDPOINT_MODEL_ID | tr -d \"'\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35348dd21acd"
      },
      "outputs": [],
      "source": [
        "# Undeploy endpoint\n",
        "! gcloud ai endpoints undeploy-model $ENDPOINT_ID \\\n",
        "--project=$PROJECT_ID \\\n",
        "--region=$REGION \\\n",
        "--deployed-model-id=$ENDPOINT_MODEL_ID \\\n",
        "\n",
        "# Delete endpoint resource\n",
        "! gcloud ai endpoints delete $ENDPOINT_ID \\\n",
        "--project=$PROJECT_ID \\\n",
        "--region=$REGION \\\n",
        "--quiet\n",
        "\n",
        "# Delete model resource\n",
        "! gcloud ai models delete $MODEL_ID \\\n",
        "--project=$PROJECT_ID \\\n",
        "--region=$REGION \\\n",
        "--quiet\n",
        "\n",
        "# Delete Cloud Storage objects that were created\n",
        "#! gsutil -m rm -r $BUCKET_NAME"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "deploy_tfhub_object_detection_on_vertex_endpoints.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

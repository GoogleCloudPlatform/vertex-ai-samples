{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/vertex_endpoints/optimized_tensorflow_runtime/bert_optimized_online_prediction.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/vertex_endpoints/optimized_tensorflow_runtime/bert_optimized_online_prediction.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/vertex_endpoints/optimized_tensorflow_runtime/bert_optimized_online_prediction.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c214896cc5a2"
      },
      "source": [
        "# Deploying T5x base on Vertex AI Predictions using the optimized TensorFlow runtime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "In this sample you learn how to deploy a T5x base model to Vertex AI Prediction using optimized TensorFlow runtime containers.\n",
        "\n",
        "We will evaluate model performance with different optimizations available on optimized TensorFlow runtime containers using MLPerf inference Vertex Prediction benchmark tool.\n",
        "\n",
        "For additional information about Vertex AI Prediction optimized TensorFlow runtime containers, see https://cloud.google.com/vertex-ai/docs/predictions/optimized-tensorflow-runtime.\n",
        "\n",
        "\n",
        "### Objective\n",
        "\n",
        "In this notebook, you learn how to deploy a fine-tuned T5x base model to Vertex AI Prediction service using the optimized TensorFlow runtime. For the best performance we are going to use NVIDIA A100 GPUs.\n",
        "\n",
        "The steps you perform include:\n",
        "* Learn how to fine-tune T5x base model on Vertex\n",
        "* Deploy a T5x base model to Vertex AI Prediction using an optimized TensorFlow runtime container using different optimization options\n",
        "* Benchmark deployed modes and validate their predictions\n",
        "\n",
        "You can deploy fine-tuned model to Vertex AI Prediction using Colab. But in order to get reliable benchmark results, this walkthrough must be run on Jupyter VM running in the same region as your model.\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses the following billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "* Cloud TPU (if you choose to fine-tune model on your own)\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze4-nDLfK4pw"
      },
      "source": [
        "### Set up your local development environment\n",
        "\n",
        "**If you are using Colab or Vertex AI Workbench Notebooks**, your environment already meets\n",
        "all the requirements to run this notebook. You can skip this step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCuSR8GkAgzl"
      },
      "source": [
        "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
        "You need the following:\n",
        "\n",
        "* The Google Cloud SDK\n",
        "* Git\n",
        "* Python 3\n",
        "* virtualenv\n",
        "* Jupyter notebook running in a virtual environment with Python 3\n",
        "\n",
        "The Google Cloud guide to [Setting up a Python development\n",
        "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
        "installation guide](https://jupyter.org/install) provide detailed instructions\n",
        "for meeting these requirements. The following steps provide a condensed set of\n",
        "instructions:\n",
        "\n",
        "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
        "\n",
        "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
        "\n",
        "1. [Install\n",
        "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
        "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
        "\n",
        "1. To install Jupyter, run `pip3 install jupyter` on the\n",
        "command-line in a terminal shell.\n",
        "\n",
        "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
        "\n",
        "1. Open this notebook in the Jupyter Notebook Dashboard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7EUnXsZhAGF"
      },
      "source": [
        "### Install additional packages\n",
        "\n",
        "Install additional package dependencies not installed in your notebook environment, such as tensorflow, tensorflow-text, tensorflow serving APIs, and Vertex AI SDK. Use the latest major GA version of each package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b4ef9b72d43"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Vertex AI Workbench Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# Vertex AI Workbench Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyy5Lbnzg5fi"
      },
      "outputs": [],
      "source": [
        "! pip3 install {USER_FLAG} --upgrade google-cloud-aiplatform -q\n",
        "! pip3 install {USER_FLAG} --upgrade google-cloud-storage -q\n",
        "! pip3 install {USER_FLAG} --upgrade tensorflow-serving-api -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fd8df302e11"
      },
      "source": [
        "If you also plan to run MLPerf infereence benchmark, you'd also need to download and install additional dependencies (see https://github.com/tensorflow/tpu/tree/master/models/experimental/inference/load_test#run-the-benchmark-locally for details)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68c85f558bd4"
      },
      "outputs": [],
      "source": [
        "! pip3 install {USER_FLAG} transformers -q\n",
        "! pip3 install {USER_FLAG} tf-models-official -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dba7d2da016"
      },
      "outputs": [],
      "source": [
        "!git clone --recurse-submodules -b r1.0 https://github.com/mlcommons/inference.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf1b9005a7f5"
      },
      "outputs": [],
      "source": [
        "!cd inference/loadgen && CFLAGS=\"-std=c++14 -O3\" python3 setup.py bdist_wheel && pip3 install {USER_FLAG} --force-reinstall dist/mlperf_loadgen-*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a57f89a5675f"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/tensorflow/tpu.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhq5zEbGg0XX"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "After you install the additional packages, you must restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzrelQZ22IZj"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWEdiXsJg0XY"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF1j6f9HApxa"
      },
      "source": [
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required for all notebook environments.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 credit towards your compute and storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "1. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "1. If you run this notebook locally, you must install the [Cloud SDK](https://cloud.google.com/sdk).\n",
        "\n",
        "1. Enter your project ID in the following cell. Then run the cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WReHDGG5g0XY"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you can try to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM1iC_MfAts1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"\"\n",
        "\n",
        "# Get your Google Cloud project ID from gcloud\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID: \", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJYoRfYng0XZ"
      },
      "source": [
        "Otherwise, set your project ID here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riG_qUokg0XZ"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
        "    PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9a78e10b72b"
      },
      "source": [
        "#### Set your region\n",
        "\n",
        "Select region where you are going to deploy your model to. Note that if you plan to deploy model on NVIDIA A100, it is only available in select regions: https://cloud.google.com/vertex-ai/docs/general/locations#region_considerations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3f22a26ddc83"
      },
      "outputs": [],
      "source": [
        "REGION = \"[your-region]\"  # @param {type:\"string\"}\n",
        "\n",
        "if REGION == \"[your-region]\":\n",
        "    REGION = \"us-central1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr--iN2kAylZ"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using  Vertex AI Workbench Notebooks**, your environment is already\n",
        "authenticated. Skip this step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBCra4QMA2wR"
      },
      "source": [
        "**If you are using Colab**, run the cell below and follow the instructions\n",
        "when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "1. In the Cloud Console, go to the [**Create service account key**\n",
        "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
        "\n",
        "2. Click **Create service account**.\n",
        "\n",
        "3. In the **Service account name** field, enter a name, and\n",
        "   click **Create**.\n",
        "\n",
        "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
        "into the filter box, and select\n",
        "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "\n",
        "5. Click **Create**. A JSON file that contains your key downloads to your local environment.\n",
        "\n",
        "6. Enter the path to your service account key as the\n",
        "`GOOGLE_APPLICATION_CREDENTIALS` variable in the following cell, then and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "# The Vertex AI Workbench Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# If on  Vertex AI Workbench Notebooks, then don't execute this code\n",
        "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fea9a57a59b"
      },
      "source": [
        "# Fine-tune T5x base model\n",
        "\n",
        "In this sample you will use T5x base model fine-tuned to do English to German language translation.\n",
        "\n",
        "T5x is a JAX based model that can be trained and fine-tuned on Google Cloud TPUs, and then exported as a TensorFlow Saved model.\n",
        "\n",
        "To fine-tune model, please follow steps at https://github.com/google-research/t5x to fine-tune model on Cloud TPU VM. Alternatively you can fine-tune model using Vertex Training service, refer https://github.com/GoogleCloudPlatform/t5x-on-vertex-ai for the steps describing how to do that.\n",
        "\n",
        "For exporting fine-tuned model refer to [Exporting as TensorFlow Saved Model](https://github.com/google-research/t5x#exporting-as-tensorflow-saved-model) section.\n",
        "\n",
        "For the purpose of this guide, we are going to use already fine-tuned models available at gs://alekseyv-tfe-ie-demo/t5x/base/ (TODO(alekseyv): figure out where to publish these models). \n",
        "\n",
        "## Model Arithmetic and Weights Type\n",
        "\n",
        "Note that we have 2 types of model, one is exported with `float32` weights, another one with `bfloat16` weights.\n",
        "\n",
        "`bfloat16` is a native format for Google Cloud TPUs, and T5x model also using it by default.\n",
        "NVIDIA A100 GPU has support for `bfloat16` arithmetic, and optimized TensorFlow runtime allows to take advantage of this.\n",
        "If you plan to deploy model on GPU that doesn't have `bfloat16` support, such as NVIDIA T4 or NVIDIA V100, you'll need to use model with `float32` weights. Luckily optimized TensorFlow runtime has an optimization that allows running models on lower precision by specifying `--allow_compression` option."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d793476c569"
      },
      "outputs": [],
      "source": [
        "# TODO(alekseyv): see if we can move it under gs://cloud-samples-data/ai-platform-unified/models/\n",
        "\n",
        "T5X_BASE_FLOAT32_MODEL_URI = \"gs://alekseyv-tfe-ie-demo/t5x/base/saved_model.bs1.bm1.no_jit.float32_cpu/20221208215700\"\n",
        "T5X_BASE_BFLOAT16_MODEL_URI = \"gs://alekseyv-tfe-ie-demo/t5x/base/saved_model.bs1.bm1.no_jit.bfloat16_cpu/20221208220052\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d6602e97750"
      },
      "source": [
        "You can observe model definition using `saved_model_cli` tool that is part of TensorFlow. Feel free to ignore error related to `SentencepieceOp`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7b3cc6c3b0f3"
      },
      "outputs": [],
      "source": [
        "!saved_model_cli show --dir=$T5X_BASE_FLOAT32_MODEL_URI --all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df9dcbe4727a"
      },
      "source": [
        "# Deploy model to Vertex AI Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b80237db652"
      },
      "source": [
        "You deploy model using [Vertex Prediction SDK](https://cloud.google.com/python/docs/reference/aiplatform/latest), import it into your notebook environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c369155c05c2"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b095feae48f0"
      },
      "source": [
        "Define the node type to use for deployments. To learn about Vertex AI Prediction options, see [configure compute resources](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3f4d8bde9b5"
      },
      "source": [
        "You are going to deploy models using optimized TensorFlow runtime container, see the full list available containers in official documentation: https://cloud.google.com/vertex-ai/docs/predictions/optimized-tensorflow-runtime#available_container_images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa7bfb2b61ee"
      },
      "outputs": [],
      "source": [
        "OPTIMIZED_TF_RUNTIME_IMAGE_URI = (\n",
        "    \"us-docker.pkg.dev/vertex-ai-restricted/prediction/tf_opt-gpu.nightly:latest\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a6525cfd380"
      },
      "outputs": [],
      "source": [
        "# TODO(alekseyv):remove\n",
        "!gsutil ls gs://alekseyv-tfe-ie-demo/t5x/base/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6427d6a2e99"
      },
      "outputs": [],
      "source": [
        "# TODO(alekseyv):remove\n",
        "!gsutil ls gs://alekseyv-tfe-ie-demo/t5x/base/saved_model.bs1.bm1.no_jit.float32_cpu/20221208215700"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3b7e5804f06"
      },
      "source": [
        "You are going to deploy T5x model on NVIDIA T4 GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bd6d54513910"
      },
      "outputs": [],
      "source": [
        "DEPLOY_COMPUTE_T4 = \"n1-standard-8\"\n",
        "DEPLOY_GPU_T4 = \"NVIDIA_TESLA_T4\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdbdd5a570a5"
      },
      "source": [
        "Deploy T5x base model with float32 weights and no optimizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6832a146bfaa"
      },
      "outputs": [],
      "source": [
        "t5x_base_float32 = aiplatform.Model.upload(\n",
        "    display_name=\"t5x_base_float32\",\n",
        "    artifact_uri=T5X_BASE_FLOAT32_MODEL_URI,\n",
        "    serving_container_image_uri=OPTIMIZED_TF_RUNTIME_IMAGE_URI,\n",
        "    serving_container_args=[],\n",
        "    location=REGION,\n",
        ")\n",
        "\n",
        "t5x_base_float32_t4_endpoint = t5x_base_float32.deploy(\n",
        "    deployed_model_display_name=\"t5x_base_float32_deployed\",\n",
        "    traffic_split={\"0\": 100},\n",
        "    machine_type=DEPLOY_COMPUTE_T4,\n",
        "    accelerator_type=DEPLOY_GPU_T4,\n",
        "    accelerator_count=1,\n",
        "    min_replica_count=1,\n",
        "    max_replica_count=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bf5f14b146e"
      },
      "outputs": [],
      "source": [
        "t5x_base_float32_t4_endpoint = t5x_base_float32.deploy(\n",
        "    deployed_model_display_name=\"t5x_base_float32_deployed\",\n",
        "    traffic_split={\"0\": 100},\n",
        "    machine_type=DEPLOY_COMPUTE_T4,\n",
        "    accelerator_type=DEPLOY_GPU_T4,\n",
        "    accelerator_count=1,\n",
        "    min_replica_count=1,\n",
        "    max_replica_count=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca3fc5656fb5"
      },
      "source": [
        "Deploy T5x base model with float32 weights and precompilation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce09e2c6bc6a"
      },
      "outputs": [],
      "source": [
        "t5x_base_float32_precompiled = aiplatform.Model.upload(\n",
        "    display_name=\"t5x_base_float32_precompiled\",\n",
        "    artifact_uri=T5X_BASE_FLOAT32_MODEL_URI,\n",
        "    serving_container_image_uri=OPTIMIZED_TF_RUNTIME_IMAGE_URI,\n",
        "    serving_container_args=[\"--allow_precompilation\"],\n",
        "    location=REGION,\n",
        ")\n",
        "\n",
        "t5x_base_float32_precompiled_t4_endpoint = t5x_base_float32_precompiled.deploy(\n",
        "    deployed_model_display_name=\"t5x_base_float32_precompiled_deployed\",\n",
        "    traffic_split={\"0\": 100},\n",
        "    machine_type=DEPLOY_COMPUTE_T4,\n",
        "    accelerator_type=DEPLOY_GPU_T4,\n",
        "    accelerator_count=1,\n",
        "    min_replica_count=1,\n",
        "    max_replica_count=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f077f774408"
      },
      "source": [
        "Deploy T5x base model with float32 weights, precompilation and compression. Model compression optimizes model to make compute intensive parts of the model to run at lower float16 precision and utilize NVIDIA GPU TensorCores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dac28ddef3ce"
      },
      "outputs": [],
      "source": [
        "t5x_base_float32_precompiled_mixedprecision = aiplatform.Model.upload(\n",
        "    display_name=\"t5x_base_float32_precompiled_mixedprecision\",\n",
        "    artifact_uri=T5X_BASE_FLOAT32_MODEL_URI,\n",
        "    serving_container_image_uri=OPTIMIZED_TF_RUNTIME_IMAGE_URI,\n",
        "    serving_container_args=[\"--allow_precompilation\", \"--allow_compression\"],\n",
        "    location=REGION,\n",
        ")\n",
        "\n",
        "t5x_base_float32_precompiled_mixedprecision_t4_endpoint = t5x_base_float32_precompiled_mixedprecision.deploy(\n",
        "    deployed_model_display_name=\"t5x_base_float32_precompiled_mixedprecision_deployed\",\n",
        "    traffic_split={\"0\": 100},\n",
        "    machine_type=DEPLOY_COMPUTE_T4,\n",
        "    accelerator_type=DEPLOY_GPU_T4,\n",
        "    accelerator_count=1,\n",
        "    min_replica_count=1,\n",
        "    max_replica_count=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "888aa8507e0c"
      },
      "source": [
        "For the best performance you can deploy T5x base model with bfloat16 weights on NVIDIA A100 that has support for bfloat16 arithmetic. Note that in order for effectively utilize bfloat16 logic model has to be deployed with precompilation. Since model is already running at half precision, model compression is not needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29bd5ce90f1a"
      },
      "outputs": [],
      "source": [
        "DEPLOY_COMPUTE_A100 = \"a2-highgpu-1g\"\n",
        "DEPLOY_GPU_A100 = \"NVIDIA_TESLA_A100\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbaedf0b4f1e"
      },
      "outputs": [],
      "source": [
        "t5x_base_float32_a100_endpoint = t5x_base_float32.deploy(\n",
        "    deployed_model_display_name=\"t5x_base_float32_deployed\",\n",
        "    traffic_split={\"0\": 100},\n",
        "    machine_type=DEPLOY_COMPUTE_A100,\n",
        "    accelerator_type=DEPLOY_GPU_A100,\n",
        "    accelerator_count=1,\n",
        "    min_replica_count=1,\n",
        "    max_replica_count=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80f45a86d18a"
      },
      "outputs": [],
      "source": [
        "t5x_base_bfloat16_precompiled = aiplatform.Model.upload(\n",
        "    display_name=\"t5x_base_bfloat16_precompiled\",\n",
        "    artifact_uri=T5X_BASE_BFLOAT16_MODEL_URI,\n",
        "    serving_container_image_uri=OPTIMIZED_TF_RUNTIME_IMAGE_URI,\n",
        "    serving_container_args=[\"--allow_precompilation\"],\n",
        "    location=REGION,\n",
        ")\n",
        "\n",
        "t5x_base_bfloat16_precompiled_a100_endpoint = t5x_base_bfloat16_precompiled.deploy(\n",
        "    deployed_model_display_name=\"t5x_base_bfloat16_precompiled_deployed\",\n",
        "    traffic_split={\"0\": 100},\n",
        "    machine_type=DEPLOY_COMPUTE_A100,\n",
        "    accelerator_type=DEPLOY_GPU_A100,\n",
        "    accelerator_count=1,\n",
        "    min_replica_count=1,\n",
        "    max_replica_count=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0016bab3f15d"
      },
      "source": [
        "## Sending prediction requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca1dfbcdf81e"
      },
      "source": [
        "You can send requests directly from each endpoint. T5x models expects data to be in a dictionary with \"text_batch\" key (see response from `saved_model_cli` call above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e596e14f2f0"
      },
      "outputs": [],
      "source": [
        "instances = [{\"text_batch\": \"translate English to German: this is good\"}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ae0e1eef2cb"
      },
      "outputs": [],
      "source": [
        "t5x_base_float32_t4_endpoint.predict(instances=instances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1b562190c2e"
      },
      "outputs": [],
      "source": [
        "t5x_base_float32_precompiled_t4_endpoint.predict(instances=instances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ff281732942"
      },
      "outputs": [],
      "source": [
        "t5x_base_float32_precompiled_mixedprecision_t4_endpoint.predict(instances=instances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b04cb34b872c"
      },
      "outputs": [],
      "source": [
        "t5x_base_float32_a100_endpoint.predict(instances=instances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f58ddf970a4"
      },
      "outputs": [],
      "source": [
        "t5x_base_bfloat16_precompiled_a100_endpoint.predict(instances=instances)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddac29c44fab"
      },
      "source": [
        "Alternatively you can send POST REST requests without using the SDK. Learn more about https://cloud.google.com/vertex-ai/docs/predictions/online-predictions-custom-models#online_predict_custom_trained-drest.\n",
        "This method is slightly faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c36d488325dd"
      },
      "source": [
        "## Compare predictions\n",
        "\n",
        "To make sure that all models returns same results, we can send same requests to all endpoints and compare predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3746a32d3e05"
      },
      "outputs": [],
      "source": [
        "# Add your samples here\n",
        "samples = [\n",
        "    \"hello world\",\n",
        "    \"this is a prediction from T5x model\",\n",
        "    \"this is good\",\n",
        "    \"my name is T5x\",\n",
        "]\n",
        "\n",
        "endpoints = {\n",
        "    \"t5x_base_float32_t4\": t5x_base_float32_t4_endpoint,\n",
        "    \"t5x_base_float32_precompiled_t4\": t5x_base_float32_precompiled_t4_endpoint,\n",
        "    \"t5x_base_float32_precompiled_mixedprecision_t4\": t5x_base_float32_precompiled_mixedprecision_t4_endpoint,\n",
        "    \"t5x_base_float32_a100\": t5x_base_float32_a100_endpoint,\n",
        "    \"t5x_base_bfloat16_precompiled_a100\": t5x_base_bfloat16_precompiled_a100_endpoint,\n",
        "}\n",
        "\n",
        "prefix = \"translate English to German: \"\n",
        "\n",
        "for sample in samples:\n",
        "    print(f\"Prediction for: {prefix}{sample}\")\n",
        "    for model_name, endpoint in endpoints.items():\n",
        "        response = endpoint.predict(instances=[{\"text_batch\": f\"{prefix}{sample}\"}])\n",
        "        prediction = response.predictions[0][\"output_0\"][0]\n",
        "        print(f\"Model: {model_name} Prediction: {prediction}\")\n",
        "    print(\"-----------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5ce470e856f"
      },
      "source": [
        "## (optional) Compare performance of deployed models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f02ce0cc8d3c"
      },
      "source": [
        "You can run benchmarks from Colab environment, also in order to get reliable results you should use VM is in the same region as your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c047d35d3ef6"
      },
      "source": [
        "Import helper functions for benchmarking models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2a52069756ba"
      },
      "outputs": [],
      "source": [
        "!curl https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/vertex_endpoints/optimized_tensorflow_runtime/benchmark.py -o benchmark.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92e7ff416f7c"
      },
      "outputs": [],
      "source": [
        "from benchmark import benchmark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "287028072667"
      },
      "source": [
        "This code sends a specified number of requests asynchronously and uniformly at a given QPS, then records the observed latency. Next, the latency results are aggregated and percentiles are calculated. The actual_qps that the model can handle is calculated as the time it takes for a model to process the sent requests divided by the number of requests. By providing different implementations for send_request and build_request functions, the same code can be used for benchmarking models running locally or on Vertex AI Prediction using gRPC and REST protocols.\n",
        "\n",
        "The main goal of this benchmark is to measure model latency on different loads, and maximum throughput the model can handle. In order to find maximum throughput, gradually increase QPS until actual_qps stops increasing and latency increases dramatically.\n",
        "\n",
        "On the production deployment, the workload is not uniform, and therefore the maximum model throughput is likely to be lower. We are not trying to simulate production workload here. This benchmark is meant to compare latency and throughput for same model running on different environments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "382954a4d93b"
      },
      "outputs": [],
      "source": [
        "def build_rest_request(row_dict, model_name):\n",
        "    return row_dict\n",
        "\n",
        "\n",
        "def validate_response(response):\n",
        "    assert response\n",
        "    assert len(response.predictions) == 1\n",
        "    assert \"output_0\" in response.predictions[0]\n",
        "    assert response.predictions[0][\"output_0\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c4df717c84b"
      },
      "outputs": [],
      "source": [
        "def send_rest_request(request):\n",
        "    response = t5x_base_float32_t4_endpoint.predict(instances=[request])\n",
        "    validate_response(response)\n",
        "\n",
        "\n",
        "t5x_base_float32_t4_results = benchmark(\n",
        "    send_rest_request,\n",
        "    build_rest_request,\n",
        "    \"gs://alekseyv-tfe-ie-demo/t5x/requests/requests_100.jsonl\",\n",
        "    [0.5, 0.75, 1, 1.25, 1.5, 1.75, 2.0],\n",
        "    10,\n",
        ")\n",
        "\n",
        "t5x_base_float32_t4_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01c88cf59367"
      },
      "outputs": [],
      "source": [
        "def send_rest_request(request):\n",
        "    response = t5x_base_float32_precompiled_t4_endpoint.predict(instances=[request])\n",
        "    validate_response(response)\n",
        "\n",
        "\n",
        "t5x_base_float32_precompiled_t4_results = benchmark(\n",
        "    send_rest_request,\n",
        "    build_rest_request,\n",
        "    \"gs://alekseyv-tfe-ie-demo/t5x/requests/requests_100.jsonl\",\n",
        "    [0.5, 1, 2, 3, 4, 5],\n",
        "    10,\n",
        ")\n",
        "\n",
        "t5x_base_float32_precompiled_t4_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4311bebc18d"
      },
      "outputs": [],
      "source": [
        "def send_rest_request(request):\n",
        "    response = t5x_base_float32_precompiled_mixedprecision_t4_endpoint.predict(\n",
        "        instances=[request]\n",
        "    )\n",
        "    validate_response(response)\n",
        "\n",
        "\n",
        "t5x_base_float32_precompiled_mixedprecision_t4_results = benchmark(\n",
        "    send_rest_request,\n",
        "    build_rest_request,\n",
        "    \"gs://alekseyv-tfe-ie-demo/t5x/requests/requests_100.jsonl\",\n",
        "    [0.5, 1, 2, 3, 4, 5],\n",
        "    10,\n",
        ")\n",
        "\n",
        "t5x_base_float32_precompiled_mixedprecision_t4_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeff92ba97ba"
      },
      "outputs": [],
      "source": [
        "def send_rest_request(request):\n",
        "    response = t5x_base_float32_a100_endpoint.predict(instances=[request])\n",
        "    validate_response(response)\n",
        "\n",
        "\n",
        "t5x_base_float32_a100_results = benchmark(\n",
        "    send_rest_request,\n",
        "    build_rest_request,\n",
        "    \"gs://alekseyv-tfe-ie-demo/t5x/requests/requests_100.jsonl\",\n",
        "    [0.5, 1, 1.25, 1.5, 1.75, 2.0, 2.25],\n",
        "    10,\n",
        ")\n",
        "\n",
        "t5x_base_float32_a100_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4a1ba7d2a58"
      },
      "outputs": [],
      "source": [
        "def send_rest_request(request):\n",
        "    response = t5x_base_bfloat16_precompiled_a100_endpoint.predict(instances=[request])\n",
        "    validate_response(response)\n",
        "\n",
        "\n",
        "t5x_base_bfloat16_precompiled_a100_results = benchmark(\n",
        "    send_rest_request,\n",
        "    build_rest_request,\n",
        "    \"gs://alekseyv-tfe-ie-demo/t5x/requests/requests_100.jsonl\",\n",
        "    [0.5, 5, 7.5, 10, 12.5, 15],\n",
        "    10,\n",
        ")\n",
        "\n",
        "t5x_base_bfloat16_precompiled_a100_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f855885e50f"
      },
      "source": [
        "Combine and visualize results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1afa1a329b0e"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def build_graph(x_key, y_key, results_dict, axis, title=\"T5x base model latency\"):\n",
        "    matplotlib.rcParams[\"figure.figsize\"] = [10.0, 7.0]\n",
        "\n",
        "    fig, ax = plt.subplots(facecolor=(1, 1, 1))\n",
        "    ax.set_xlabel(\"QPS\")\n",
        "    ax.set_ylabel(\"Latency(ms)\")\n",
        "    for label, results in results_dict.items():\n",
        "        x = np.array(results[x_key])\n",
        "        y = np.array(results[y_key])\n",
        "        ax.plot(x, y, label=label, marker=\"s\")\n",
        "    ax.grid()\n",
        "    ax.legend()\n",
        "    ax.axis(axis)\n",
        "    ax.set_title(title)\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "204d66f54e46"
      },
      "outputs": [],
      "source": [
        "fig = build_graph(\n",
        "    \"actual_qps\",\n",
        "    \"p50\",\n",
        "    {\n",
        "        \"T5x base float32 on T4\": t5x_base_float32_t4_results,\n",
        "        \"T5x base float32 on T4 with precompilation\": t5x_base_float32_precompiled_t4_results,\n",
        "        \"T5x base float32 on T4 with precompilation and compression\": t5x_base_float32_precompiled_mixedprecision_t4_results,\n",
        "        \"T5x base float32 on A100\": t5x_base_float32_a100_results,\n",
        "        \"T5x base bfloat16 on A100 with precompilation\": t5x_base_bfloat16_precompiled_a100_results,\n",
        "    },\n",
        "    (0, 10, 0, 2500),\n",
        "    title=\"T5x base model p50 latency, batch size 1\",\n",
        ")\n",
        "fig.savefig(\"t5x_base_p50_latency.png\", bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7251aa3b7721"
      },
      "outputs": [],
      "source": [
        "fig = build_graph(\n",
        "    \"actual_qps\",\n",
        "    \"p99\",\n",
        "    {\n",
        "        \"T5x base float32 on T4\": t5x_base_float32_t4_results,\n",
        "        \"T5x base float32 on T4 with precompilation\": t5x_base_float32_precompiled_t4_results,\n",
        "        \"T5x base float32 on T4 with precompilation and compression\": t5x_base_float32_precompiled_mixedprecision_t4_results,\n",
        "        \"T5x base float32 on A100\": t5x_base_float32_a100_results,\n",
        "        \"T5x base bfloat16 on A100 with precompilation\": t5x_base_bfloat16_precompiled_a100_results,\n",
        "    },\n",
        "    (0, 5, 0, 5500),\n",
        "    title=\"T5x base model p99 latency, batch size 1\",\n",
        ")\n",
        "fig.savefig(\"t5x_base_p99_latency.png\", bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8e97703a929"
      },
      "source": [
        "As you can see Vertex AI Prediction optimized TensorFlow runtime optimizations offer signficantly higher throughput and lower latency for T5x base model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06f2711e6221"
      },
      "source": [
        "## (Optional) Compare performance of deployed models using MLPerf Inference loadgen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adcd7c46de26"
      },
      "source": [
        "MLPerf Inference is a benchmark suite for measuring how fast systems can run models in a variety of deployment scenarios. MLPerf is now an industry standard way of measuring model performance. You can follow instructions at https://github.com/tensorflow/tpu/tree/master/models/experimental/inference/load_test to run MLPerf Inferenence benchmark for deployed models.\n",
        "\n",
        "Unlike naive benchmark we used before, MLPerf loadgen is sending requests using [Poisson distribution](https://github.com/mlcommons/inference_policies/blob/master/inference_rules.adoc#3-scenarios)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d73bf182f27a"
      },
      "outputs": [],
      "source": [
        "project_id = t5x_base_float32_t4_endpoint.resource_name.split(\"/\")[1]\n",
        "project_id\n",
        "\n",
        "endpoint_id = t5x_base_float32_t4_endpoint.resource_name.split(\"/\")[-1]\n",
        "endpoint_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51a52c6698de"
      },
      "outputs": [],
      "source": [
        "%cd tpu/models/experimental/inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4f70d73f164"
      },
      "outputs": [],
      "source": [
        "!python3 -m load_test.examples.loadgen_vertex_main \\\n",
        "  --project_id={project_id} \\\n",
        "  --region={REGION} \\\n",
        "  --endpoint_id={t5x_base_float32_t4_endpoint.resource_name.split(\"/\")[-1]} \\\n",
        "  --dataset=generic_jsonl \\\n",
        "  --data_file=gs://alekseyv-tfe-ie-demo/t5x/requests/requests_100.jsonl \\\n",
        "  --api_type=rest \\\n",
        "  --min_query_count=10 \\    \n",
        "  --min_duration_ms=10000 \\\n",
        "  --qps=0.5 --qps=1.0 --qps=1.25 --qps=1.5 --qps=1.75 --qps=2.0 \\\n",
        "  --csv_report_filename=\"t5x_base_float32_t4_results.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "456e293c8419"
      },
      "outputs": [],
      "source": [
        "!python3 -m load_test.examples.loadgen_vertex_main \\\n",
        "  --project_id={project_id} \\\n",
        "  --region={REGION} \\\n",
        "  --endpoint_id={t5x_base_float32_precompiled_t4_endpoint.resource_name.split(\"/\")[-1]} \\\n",
        "  --dataset=generic_jsonl \\\n",
        "  --data_file=gs://alekseyv-tfe-ie-demo/t5x/requests/requests_100.jsonl \\\n",
        "  --api_type=rest \\\n",
        "  --min_query_count=10 \\    \n",
        "  --min_duration_ms=10000 \\\n",
        "  --qps=0.5 --qps=1 --qps=2 --qps=3 --qps=4 --qps=5 \\\n",
        "  --csv_report_filename=\"t5x_base_float32_precompiled_t4_results.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11b28328eb97"
      },
      "outputs": [],
      "source": [
        "!python3 -m load_test.examples.loadgen_vertex_main \\\n",
        "  --project_id={project_id} \\\n",
        "  --region={REGION} \\\n",
        "  --endpoint_id={t5x_base_float32_precompiled_mixedprecision_t4_endpoint.resource_name.split(\"/\")[-1]} \\\n",
        "  --dataset=generic_jsonl \\\n",
        "  --data_file=gs://alekseyv-tfe-ie-demo/t5x/requests/requests_100.jsonl \\\n",
        "  --api_type=rest \\\n",
        "  --min_query_count=10 \\    \n",
        "  --min_duration_ms=10000 \\\n",
        "  --qps=0.5 --qps=1 --qps=2 --qps=3 --qps=4 --qps=5 --qps=6 \\\n",
        "  --csv_report_filename=\"t5x_base_float32_precompiled_mixedprecision_t4_results.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23342227c786"
      },
      "outputs": [],
      "source": [
        "!python3 -m load_test.examples.loadgen_vertex_main \\\n",
        "  --project_id={project_id} \\\n",
        "  --region={REGION} \\\n",
        "  --endpoint_id={t5x_base_float32_a100_endpoint.resource_name.split(\"/\")[-1]} \\\n",
        "  --dataset=generic_jsonl \\\n",
        "  --data_file=gs://alekseyv-tfe-ie-demo/t5x/requests/requests_100.jsonl \\\n",
        "  --api_type=rest \\\n",
        "  --min_query_count=10 \\    \n",
        "  --min_duration_ms=10000 \\\n",
        "  --qps=0.5 --qps=1.0 --qps=1.25 --qps=1.5 --qps=1.75 --qps=2.0 --qps=2.25 \\\n",
        "  --csv_report_filename=\"t5x_base_float32_a100_results.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6ad17ddc3d7"
      },
      "outputs": [],
      "source": [
        "!python3 -m load_test.examples.loadgen_vertex_main \\\n",
        "  --project_id={project_id} \\\n",
        "  --region={REGION} \\\n",
        "  --endpoint_id={t5x_base_bfloat16_precompiled_a100_endpoint.resource_name.split(\"/\")[-1]} \\\n",
        "  --dataset=generic_jsonl \\\n",
        "  --data_file=gs://alekseyv-tfe-ie-demo/t5x/requests/requests_100.jsonl \\\n",
        "  --api_type=rest \\\n",
        "  --min_query_count=10 \\    \n",
        "  --min_duration_ms=10000 \\\n",
        "  --qps=0.5 --qps=5 --qps=7.5 --qps=10 --qps=12.5 --qps=15 \\\n",
        "  --csv_report_filename=\"t5x_base_bfloat16_precompiled_a100_results.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "310d3f7291c4"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "\n",
        "def parse_report_csv(file_name):\n",
        "    with open(file_name, newline=\"\") as f:\n",
        "        reader = csv.reader(f)\n",
        "\n",
        "        d = {}\n",
        "        index_to_key = {}\n",
        "        for row in reader:\n",
        "            if not d:\n",
        "                for index in range(len(row)):\n",
        "                    key = row[index]\n",
        "                    index_to_key[index] = key\n",
        "                    d[key] = []\n",
        "            else:\n",
        "                for index in range(len(row)):\n",
        "                    if index_to_key[index] != \"scenario\":\n",
        "                        d[index_to_key[index]].append(float(row[index]))\n",
        "                    else:\n",
        "                        d[index_to_key[index]].append(row[index])\n",
        "        return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09b0dd57beb1"
      },
      "outputs": [],
      "source": [
        "fig = build_graph(\n",
        "    \"actual_qps\",\n",
        "    \"p50\",\n",
        "    {\n",
        "        \"T5x base float32 on T4\": parse_report_csv(\"t5x_base_float32_t4_results.csv\"),\n",
        "        \"T5x base float32 on T4 with precompilation\": parse_report_csv(\n",
        "            \"t5x_base_float32_precompiled_t4_results.csv\"\n",
        "        ),\n",
        "        \"T5x base float32 on T4 with precompilation and compression\": parse_report_csv(\n",
        "            \"t5x_base_float32_precompiled_mixedprecision_t4_results.csv\"\n",
        "        ),\n",
        "        \"T5x base float32 on A100\": parse_report_csv(\n",
        "            \"t5x_base_float32_a100_results.csv\"\n",
        "        ),\n",
        "        \"T5x base bfloat16 on A100 with precompilation\": parse_report_csv(\n",
        "            \"t5x_base_bfloat16_precompiled_a100_results.csv\"\n",
        "        ),\n",
        "    },\n",
        "    (0, 10, 0, 2500),\n",
        "    title=\"T5x base model p50 latency measured by MLPerf loadgen, batch size 1\",\n",
        ")\n",
        "fig.savefig(\"t5x_base_p50_mlperf_latency.png\", bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ffb220d9cd6"
      },
      "outputs": [],
      "source": [
        "fig = build_graph(\n",
        "    \"actual_qps\",\n",
        "    \"p99\",\n",
        "    {\n",
        "        \"T5x base float32 on T4\": parse_report_csv(\"t5x_base_float32_t4_results.csv\"),\n",
        "        \"T5x base float32 on T4 with precompilation\": parse_report_csv(\n",
        "            \"t5x_base_float32_precompiled_t4_results.csv\"\n",
        "        ),\n",
        "        \"T5x base float32 on T4 with precompilation and compression\": parse_report_csv(\n",
        "            \"t5x_base_float32_precompiled_mixedprecision_t4_results.csv\"\n",
        "        ),\n",
        "        \"T5x base float32 on A100\": parse_report_csv(\n",
        "            \"t5x_base_float32_a100_results.csv\"\n",
        "        ),\n",
        "        \"T5x base bfloat16 on A100 with precompilation\": parse_report_csv(\n",
        "            \"t5x_base_bfloat16_precompiled_a100_results.csv\"\n",
        "        ),\n",
        "    },\n",
        "    (0, 10, 0, 3500),\n",
        "    title=\"T5x base model p99 latency measured by MLPerf loadgen, batch size 1\",\n",
        ")\n",
        "fig.savefig(\"t5x_base_p99_mlperf_latency.png\", bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f35eff5aea2e"
      },
      "source": [
        "These results are mostly consistent with results we observed using naive benchmarking code, except on for unoptimized models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c69570acca3d"
      },
      "source": [
        "## Cleanup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df0bd8f8c5f2"
      },
      "source": [
        "After you are done, it's safe to remove the endpoints you created and the model you deployed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab6cb1891003"
      },
      "outputs": [],
      "source": [
        "# Undeploy models\n",
        "t5x_base_float32_t4_endpoint.undeploy_all()\n",
        "t5x_base_float32_precompiled_t4_endpoint.undeploy_all()\n",
        "t5x_base_float32_precompiled_mixedprecision_t4_endpoint.undeploy_all()\n",
        "t5x_base_float32_a100_endpoint.undeploy_all()\n",
        "t5x_base_bfloat16_precompiled_a100_endpoint.undeploy_all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1389efff614"
      },
      "outputs": [],
      "source": [
        "# Delete models\n",
        "t5x_base_float32.delete()\n",
        "t5x_base_float32_precompiled.delete()\n",
        "t5x_base_float32_precompiled_mixedprecision.delete()\n",
        "t5x_base_bfloat16_precompiled.delete()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33f5b617c39b"
      },
      "outputs": [],
      "source": [
        "# Delete endpoints\n",
        "t5x_base_float32_t4_endpoint.delete()\n",
        "t5x_base_float32_precompiled_t4_endpoint.delete()\n",
        "t5x_base_float32_precompiled_mixedprecision_t4_endpoint.delete()\n",
        "t5x_base_float32_a100_endpoint.delete()\n",
        "t5x_base_bfloat16_precompiled_a100_endpoint.delete()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "t5x_base_optimized_online_prediction.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

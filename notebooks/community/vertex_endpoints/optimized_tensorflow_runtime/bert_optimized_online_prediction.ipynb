{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/vertex_endpoints/optimized_tensorflow_runtime/bert_optimized_online_prediction.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/vertex_endpoints/optimized_tensorflow_runtime/bert_optimized_online_prediction.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/vertex_endpoints/optimized_tensorflow_runtime/bert_optimized_online_prediction.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c214896cc5a2"
      },
      "source": [
        "# Fine-tuning a BERT base classification model and deploying it to Vertex AI Predictions using the optimized TensorFlow runtime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "In this sample you learn how to fine-tune a BERT base classification model for sentiment analysis.\n",
        "\n",
        "Then you export a trained model to Vertex AI Prediction service using an open source based TensorFlow 2.7 container and the optimized TensorFlow runtime container, run performance evaluation for those models and compare their predictions.\n",
        "\n",
        "For additional information about Vertex AI Prediction optimized TensorFlow runtime containers, see https://cloud.google.com/vertex-ai/docs/predictions/optimized-tensorflow-runtime.\n",
        "\n",
        "### Dataset\n",
        "\n",
        "This notebook trains a sentiment analysis model to classify movie reviews as positive or negative based on the text of the review.\n",
        "\n",
        "You use the [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/) that contains the 50,000 movie reviews from the [Internet Movie Database](https://www.imdb.com/).\n",
        "\n",
        "\n",
        "### Objective\n",
        "\n",
        "In this notebook, you learn how to deploy a fine-tuned BERT classification model to Vertex AI Prediction using the optimized TensorFlow runtime. Next, you compare its performance to an open source based TensorFlow container.\n",
        "\n",
        "The steps you perform include:\n",
        "* Download and preprocess the [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/)\n",
        "* Download the BERT base model from TF hub\n",
        "* Fine-tune the BERT classification model\n",
        "* Deploy a model to Vertex AI Prediction using a TensorFlow 2.7 container\n",
        "* Deploy a model to Vertex AI Prediction using an optimized TensorFlow runtime container\n",
        "* Benchmark the models and validate their predictions\n",
        "\n",
        "You can to fine-tune the BERT model and upload it to Vertex AI Prediction using Colab. To get reliable benchmark results, this walkthrough must be run on Jupyter VM running in the same region as your model.\n",
        "\n",
        "### Costs\n",
        "\n",
        "This tutorial uses the following billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze4-nDLfK4pw"
      },
      "source": [
        "### Set up your local development environment\n",
        "\n",
        "**If you are using Colab or Vertex AI Workbench Notebooks**, your environment already meets\n",
        "all the requirements to run this notebook. You can skip this step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCuSR8GkAgzl"
      },
      "source": [
        "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
        "You need the following:\n",
        "\n",
        "* The Google Cloud SDK\n",
        "* Git\n",
        "* Python 3\n",
        "* virtualenv\n",
        "* Jupyter notebook running in a virtual environment with Python 3\n",
        "* GPU drivers and CUDA 11.2 installed\n",
        "\n",
        "The Google Cloud guide to [Setting up a Python development\n",
        "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
        "installation guide](https://jupyter.org/install) provide detailed instructions\n",
        "for meeting these requirements. The following steps provide a condensed set of\n",
        "instructions:\n",
        "\n",
        "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
        "\n",
        "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
        "\n",
        "1. [Install\n",
        "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
        "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
        "\n",
        "1. To install Jupyter, run `pip3 install jupyter` on the\n",
        "command-line in a terminal shell.\n",
        "\n",
        "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
        "\n",
        "1. Open this notebook in the Jupyter Notebook Dashboard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7EUnXsZhAGF"
      },
      "source": [
        "### Install additional packages\n",
        "\n",
        "Install additional package dependencies not installed in your notebook environment, such as tensorflow, tensorflow-text, tensorflow serving APIs, and Vertex AI SDK. Use the latest major GA version of each package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b4ef9b72d43"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Vertex AI Workbench Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# Vertex AI Workbench Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyy5Lbnzg5fi"
      },
      "outputs": [],
      "source": [
        "! pip3 install {USER_FLAG} --upgrade tensorflow==2.7.0 -q\n",
        "! pip3 install {USER_FLAG} --upgrade tensorflow-text==2.7.0 -q\n",
        "! pip3 install {USER_FLAG} --upgrade tensorflow-serving-api==2.7.0 -q\n",
        "! pip3 install {USER_FLAG} --upgrade tf-models-official==2.7.0 -q\n",
        "! pip3 install {USER_FLAG} --upgrade google-cloud-aiplatform -q\n",
        "! pip3 install {USER_FLAG} --upgrade google-cloud-storage -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhq5zEbGg0XX"
      },
      "source": [
        "### Restart the kernel\n",
        "\n",
        "After you install the additional packages, you must restart the notebook kernel so it can find the packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzrelQZ22IZj"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWEdiXsJg0XY"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b37054d4d064"
      },
      "source": [
        "### Select a GPU runtime\n",
        "\n",
        "**Make sure you're running this notebook in a GPU runtime if you have that option. In Colab, select \"Runtime --> Change runtime type > GPU\"**.\n",
        "\n",
        "Please note that in order for you to be able to fine-tune model using GPU, your VM needs to have GPU drivers and CUDA 11.2 installed. You can use [TensorFlow Enterprise 2.7](https://cloud.google.com/tensorflow-enterprise/docs/use-with-notebooks) user-managed notebook instance or Colab with GPU runtime for this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF1j6f9HApxa"
      },
      "source": [
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required for all notebook environments.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 credit towards your compute and storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "1. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "1. If you run this notebook locally, you must install the [Cloud SDK](https://cloud.google.com/sdk).\n",
        "\n",
        "1. Enter your project ID in the following cell. Then run the cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WReHDGG5g0XY"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, you can try to get your project ID using `gcloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM1iC_MfAts1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"\"\n",
        "\n",
        "# Get your Google Cloud project ID from gcloud\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "    PROJECT_ID = shell_output[0]\n",
        "    print(\"Project ID: \", PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJYoRfYng0XZ"
      },
      "source": [
        "Otherwise, set your project ID here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riG_qUokg0XZ"
      },
      "outputs": [],
      "source": [
        "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
        "    PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06571eb4063b"
      },
      "source": [
        "#### Timestamp\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, create a timestamp for each instance session, then append it to the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "697568e92bd6"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr--iN2kAylZ"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "**If you are using  Vertex AI Workbench Notebooks**, your environment is already\n",
        "authenticated. Skip this step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBCra4QMA2wR"
      },
      "source": [
        "**If you are using Colab**, run the cell below and follow the instructions\n",
        "when prompted to authenticate your account via oAuth.\n",
        "\n",
        "**Otherwise**, follow these steps:\n",
        "\n",
        "1. In the Cloud Console, go to the [**Create service account key**\n",
        "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
        "\n",
        "2. Click **Create service account**.\n",
        "\n",
        "3. In the **Service account name** field, enter a name, and\n",
        "   click **Create**.\n",
        "\n",
        "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
        "into the filter box, and select\n",
        "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
        "\n",
        "5. Click **Create**. A JSON file that contains your key downloads to your local environment.\n",
        "\n",
        "6. Enter the path to your service account key as the\n",
        "`GOOGLE_APPLICATION_CREDENTIALS` variable in the following cell, then and run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "# The Vertex AI Workbench Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# If on  Vertex AI Workbench Notebooks, then don't execute this code\n",
        "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgPO1eR3CYjk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "**The following steps are required for all notebook environments.**\n",
        "\n",
        "In order for Vertex AI Prediction to be able to serve your model it has to be uploaded to Cloud Storage bucket first.\n",
        "\n",
        "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
        "Cloud Storage buckets.\n",
        "\n",
        "You may also change the `REGION` variable, which is used for operations\n",
        "throughout the rest of this notebook. We suggest that you [choose a region where Vertex AI services are\n",
        "available](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzGDU7TWdts_"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}\n",
        "REGION = \"[your-region]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf221059d072"
      },
      "outputs": [],
      "source": [
        "if BUCKET_URI == \"\" or BUCKET_URI is None or BUCKET_URI == \"gs://[your-bucket-name]\":\n",
        "    BUCKET_URI = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP\n",
        "\n",
        "if REGION == \"[your-region]\":\n",
        "    REGION = \"us-central1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EcIXiGsCePi"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIq7R4HZCfIc"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucvCsknMCims"
      },
      "source": [
        "The final step for your Cloud Storage bucket is to validate access to your Cloud Storage bucket by examining its contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhOb7YnwClBb"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRUOFELefqf1"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import requests as r\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text  # noqa: F401\n",
        "from official.nlp import optimization  # to create AdamW optimizer\n",
        "\n",
        "r.packages.urllib3.disable_warnings()\n",
        "\n",
        "logging = tf.get_logger()\n",
        "logging.propagate = False\n",
        "logging.setLevel(\"INFO\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08881d110f47"
      },
      "outputs": [],
      "source": [
        "LOCAL_DIRECTORY = \"~/bert_classification\"  # @param {type:\"string\"}\n",
        "LOCAL_DIRECTORY_FULL = os.path.expanduser(LOCAL_DIRECTORY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cbb5302a3c4"
      },
      "source": [
        "## Download the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a762c93fb848"
      },
      "source": [
        "Download the [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/) from the [Internet Movie Database](https://www.imdb.com/). This dataset contains the text of 50,000 movie reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbc5c536e55e"
      },
      "outputs": [],
      "source": [
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\n",
        "    \"aclImdb_v1.tar.gz\", url, untar=True, cache_dir=\".\", cache_subdir=\"\"\n",
        ")\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), \"aclImdb\")\n",
        "\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "\n",
        "# remove unused folders to make it easier to load the data\n",
        "remove_dir = os.path.join(train_dir, \"unsup\")\n",
        "shutil.rmtree(remove_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b7051896d19"
      },
      "source": [
        "## Preprocess the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e912d5db172"
      },
      "source": [
        "The IMDB dataset has already been divided into train and test, but it lacks a validation set. To create a validation set, in the following cell use an 80:20 split of the training data by using the `validation_split` argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b45dfad0d580"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=seed,\n",
        ")\n",
        "\n",
        "class_names = raw_train_ds.class_names\n",
        "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=seed,\n",
        ")\n",
        "\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")\n",
        "\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f104e8e76fe"
      },
      "source": [
        "Take a look at a few reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec8a171fa4f7"
      },
      "outputs": [],
      "source": [
        "for text_batch, label_batch in train_ds.take(1):\n",
        "    for i in range(3):\n",
        "        print(f\"Review: {text_batch.numpy()[i]}\")\n",
        "        label = label_batch.numpy()[i]\n",
        "        print(f\"Label : {label} ({class_names[label]})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "706c5e78b238"
      },
      "source": [
        "## Define BERT base classification model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d56746554dd6"
      },
      "source": [
        "As a base for our model you take uncased BERT-Base model from TensorFlow Hub:\n",
        "https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4324e29bd66"
      },
      "outputs": [],
      "source": [
        "bert_model_name = \"bert_en_uncased_L-12_H-768_A-12\"\n",
        "tfhub_handle_encoder = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48bc324d317a"
      },
      "source": [
        "To feed text input into this model, data is preprocessed using the corresponding BERT pre-processor from TensorFlow hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e7fd14957ed"
      },
      "outputs": [],
      "source": [
        "tfhub_handle_preprocess = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03bbfbecf2f9"
      },
      "outputs": [],
      "source": [
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c4a617d9b23"
      },
      "source": [
        "Define a classification model by feeding the output BERT encoder model into dropout and dense layers.\n",
        "Learn more about [Classify text with BERT]( https://www.tensorflow.org/text/tutorials/classify_text_with_bert)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64dc097e0fd5"
      },
      "outputs": [],
      "source": [
        "def build_classifier_model():\n",
        "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"text\")\n",
        "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name=\"preprocessing\")\n",
        "    encoder_inputs = preprocessing_layer(text_input)\n",
        "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name=\"BERT_encoder\")\n",
        "    outputs = encoder(encoder_inputs)\n",
        "    net = outputs[\"pooled_output\"]\n",
        "    net = tf.keras.layers.Dropout(0.1)(net)\n",
        "    net = tf.keras.layers.Dense(1, activation=tf.sigmoid, name=\"classifier\")(net)\n",
        "    return tf.keras.Model(text_input, net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2296da5bc1fa"
      },
      "outputs": [],
      "source": [
        "classifier_model = build_classifier_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5baf26b59486"
      },
      "outputs": [],
      "source": [
        "epochs = 3\n",
        "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1 * num_train_steps)\n",
        "\n",
        "init_lr = 3e-5\n",
        "optimizer = optimization.create_optimizer(\n",
        "    init_lr=init_lr,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    optimizer_type=\"adamw\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33cac949c431"
      },
      "outputs": [],
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "metrics = tf.metrics.BinaryAccuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "061751446860"
      },
      "outputs": [],
      "source": [
        "classifier_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0786a8eb8566"
      },
      "source": [
        "Fine-tuning the model takes some time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6abaf7811bc3"
      },
      "outputs": [],
      "source": [
        "print(f\"Training model with {tfhub_handle_encoder}\")\n",
        "history = classifier_model.fit(x=train_ds, validation_data=val_ds, epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b4cf5762fa1"
      },
      "source": [
        "Evaluate the model. Its validation loss is expected to be ~0.43."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d780483346fd"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = classifier_model.evaluate(test_ds)\n",
        "\n",
        "print(f\"Loss: {loss}\")\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93b391f42581"
      },
      "source": [
        "Export the model for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07fa7ee3b676"
      },
      "outputs": [],
      "source": [
        "!mkdir -p $LOCAL_DIRECTORY_FULL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9298e98110f9"
      },
      "outputs": [],
      "source": [
        "classifier_model.save(LOCAL_DIRECTORY_FULL, include_optimizer=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11cc837b06f9"
      },
      "source": [
        "Check the model signature to see which fields prediction request should have.\n",
        "\n",
        "Note that you might see a stacktrace message about a missing 'CaseFoldUTF8' op. This is a known issue with `saved_model_cli` that you can ignore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af1ad7b97608"
      },
      "outputs": [],
      "source": [
        "!saved_model_cli show --dir $LOCAL_DIRECTORY_FULL --all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd4c3d9bf3aa"
      },
      "source": [
        "## Generate prediction requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "953eb5e1e86e"
      },
      "source": [
        "Now you can generate requests to send to our model for inference. Requests are generated in the JSON Lines format, one request per line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e07ab9e6213b"
      },
      "outputs": [],
      "source": [
        "!mkdir -p $LOCAL_DIRECTORY_FULL/requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "755b073712eb"
      },
      "outputs": [],
      "source": [
        "def encode(text):\n",
        "    rows = []\n",
        "    for row in text.numpy().tolist():\n",
        "        rows.append(row.decode(\"utf-8\"))\n",
        "\n",
        "    return {\"text\": rows}\n",
        "\n",
        "\n",
        "def export_requests_jsonl(file_name, rows=2, batch_size=32):\n",
        "    with tf.io.gfile.GFile(file_name, mode=\"w\") as f:\n",
        "        for text in test_ds.unbatch().batch(batch_size).take(rows):\n",
        "            d = encode(text[0])\n",
        "            f.write(json.dumps(d))\n",
        "            f.write(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac85dcc6a578"
      },
      "outputs": [],
      "source": [
        "export_requests_jsonl(\n",
        "    os.path.join(LOCAL_DIRECTORY_FULL, \"requests\", \"requests_1_1.jsonl\"),\n",
        "    rows=1,\n",
        "    batch_size=1,\n",
        ")\n",
        "export_requests_jsonl(\n",
        "    os.path.join(LOCAL_DIRECTORY_FULL, \"requests\", \"requests_1_32.jsonl\"),\n",
        "    rows=1,\n",
        "    batch_size=32,\n",
        ")\n",
        "export_requests_jsonl(\n",
        "    os.path.join(LOCAL_DIRECTORY_FULL, \"requests\", \"requests_10_32.jsonl\"),\n",
        "    rows=10,\n",
        "    batch_size=32,\n",
        ")\n",
        "export_requests_jsonl(\n",
        "    os.path.join(LOCAL_DIRECTORY_FULL, \"requests\", \"requests_100_32.jsonl\"),\n",
        "    rows=100,\n",
        "    batch_size=32,\n",
        ")\n",
        "export_requests_jsonl(\n",
        "    os.path.join(LOCAL_DIRECTORY_FULL, \"requests\", \"requests_1000_32.jsonl\"),\n",
        "    rows=1000,\n",
        "    batch_size=32,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "291f18b4f085"
      },
      "outputs": [],
      "source": [
        "!cat $LOCAL_DIRECTORY_FULL/requests/requests_1_1.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95a4aacc6246"
      },
      "source": [
        "## (Optional) Generate warmup requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bbdded5d2ab"
      },
      "source": [
        "The TensorFlow runtime has components that are lazily initialized. Lazy initialization might result in high latency for the first requests that are sent to a model after it's loaded. This latency can be several orders of magnitude higher than that of a single inference request.\n",
        "\n",
        "For more information about SavedModel warmup, see https://www.tensorflow.org/tfx/serving/saved_model_warmup.\n",
        "\n",
        "For Vertex AI Prediction using the optimized TensorFlow runtime, when the model is precompiled the first request for each new batch size has higher latency. Precompilation is enabled when the `allow_precompilation` flag is set to true.\n",
        "\n",
        "To mitigate high latency, provide a warmup request for the runtime to load when it starts.\n",
        "The warmup file should include the various batch sizes you expect your model to receive in production.\n",
        "\n",
        "Note that providing a warmup request with multiple batch sizes increase the time for each node to start.\n",
        "\n",
        "If you expect the model to receive multiple batch sizes, you can use automatic server-side request batching with a set of `allowed_batch_sizes`. For more information, see https://www.tensorflow.org/tfx/serving/serving_config#batching_configurationß.\n",
        "\n",
        "To enable auto-batching for a model running on Vertex AI Prediction, put your batching configuration into the [config/batching_parameters_config](https://cloud.google.com/vertex-ai/docs/training/exporting-model-artifacts#enable_server-side_request_batching_for_tensorflow) file in the same GCS directory as saved_model.pb."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5d7a5991818"
      },
      "outputs": [],
      "source": [
        "!mkdir -p $LOCAL_DIRECTORY_FULL/assets.extra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "136ef90d0320"
      },
      "outputs": [],
      "source": [
        "from tensorflow_serving.apis import predict_pb2, prediction_log_pb2\n",
        "\n",
        "\n",
        "def build_grpc_request(\n",
        "    row_dict, model_name=\"default\", signature_name=\"serving_default\"\n",
        "):\n",
        "    \"\"\"Generate gRPC inference request with payload.\"\"\"\n",
        "\n",
        "    request = predict_pb2.PredictRequest()\n",
        "    request.model_spec.name = model_name\n",
        "    request.model_spec.signature_name = signature_name\n",
        "    for key, value in row_dict.items():\n",
        "        proto = tf.make_tensor_proto(value)\n",
        "        request.inputs[key].CopyFrom(proto)\n",
        "    return request\n",
        "\n",
        "\n",
        "def export_warmup_file(\n",
        "    request_files, export_path, model_name=\"default\", signature_name=\"serving_default\"\n",
        "):\n",
        "    with tf.io.TFRecordWriter(export_path) as writer:\n",
        "        for request_file_path in request_files:\n",
        "            with open(request_file_path) as f:\n",
        "                row_dict = json.loads(f.readline())\n",
        "                request = build_grpc_request(row_dict, model_name, signature_name)\n",
        "            log = prediction_log_pb2.PredictionLog(\n",
        "                predict_log=prediction_log_pb2.PredictLog(request=request)\n",
        "            )\n",
        "            writer.write(log.SerializeToString())\n",
        "\n",
        "\n",
        "export_warmup_file(\n",
        "    [\n",
        "        os.path.join(LOCAL_DIRECTORY_FULL, \"requests\", \"requests_1_1.jsonl\"),\n",
        "        os.path.join(LOCAL_DIRECTORY_FULL, \"requests\", \"requests_1_32.jsonl\"),\n",
        "    ],\n",
        "    os.path.join(LOCAL_DIRECTORY_FULL, \"assets.extra\", \"tf_serving_warmup_requests\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df9dcbe4727a"
      },
      "source": [
        "## Deploy model to Vertex AI Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b80237db652"
      },
      "source": [
        "To deploy a model to Vertex AI Prediction service, you must put it in a GCS bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7K_ebo3ZjeIO"
      },
      "outputs": [],
      "source": [
        "!gsutil rm -r $BUCKET_URI/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17d915525377"
      },
      "outputs": [],
      "source": [
        "!gsutil cp -r $LOCAL_DIRECTORY_FULL/* $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b89bf1e0ec3b"
      },
      "source": [
        "Import the Vertex AI Python client library into your notebook environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c369155c05c2"
      },
      "outputs": [],
      "source": [
        "from google.cloud.aiplatform import gapic as aip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b095feae48f0"
      },
      "source": [
        "Define the node type to use for deployments. To learn about Vertex AI Prediction options, see [configure compute resources](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bd6d54513910"
      },
      "outputs": [],
      "source": [
        "DEPLOY_COMPUTE = \"n1-standard-16\"\n",
        "DEPLOY_GPU = aip.AcceleratorType.NVIDIA_TESLA_T4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83f62a939359"
      },
      "source": [
        "The AI Platform Python client library works as a client/server model.\n",
        "\n",
        "You are going to use following clients in this sample:\n",
        "- Model Service for managing models.\n",
        "- Endpoint Service for deployment.\n",
        "- Prediction Service for serving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8851dabb0473"
      },
      "outputs": [],
      "source": [
        "API_ENDPOINT = f\"{REGION}-aiplatform.googleapis.com\"\n",
        "PARENT = f\"projects/{PROJECT_ID}/locations/{REGION}\"\n",
        "\n",
        "client_options = {\"api_endpoint\": API_ENDPOINT}\n",
        "model_service_client = aip.ModelServiceClient(client_options=client_options)\n",
        "endpoint_service_client = aip.EndpointServiceClient(client_options=client_options)\n",
        "prediction_service_client = aip.PredictionServiceClient(client_options=client_options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85076eb8d14e"
      },
      "source": [
        "### Upload models to Vertex AI Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e9070f4be1a"
      },
      "source": [
        "See [model_service.upload_model](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1.services.model_service.ModelServiceClient#google_cloud_aiplatform_v1_services_model_service_ModelServiceClient_upload_model) documentation for details.\n",
        "\n",
        "\n",
        "`artifact_uri` argument should point to a GCS path where `saved_model.pb` file is located for your model.\n",
        "\n",
        "`image_uri` specifies which docker image to use. Here you upload the same model using TF2.7 GPU and Vertex AI Prediction optimized TensorFlow runtime images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc626262a937"
      },
      "outputs": [],
      "source": [
        "tf27_gpu_model_dict = {\n",
        "    \"display_name\": \"BERT Base TF2.7 GPU model\",\n",
        "    \"artifact_uri\": BUCKET_URI,\n",
        "    \"container_spec\": {\n",
        "        \"image_uri\": \"us-docker.pkg.dev/vertex-ai/prediction/tf2-gpu.2-7:latest\",\n",
        "    },\n",
        "}\n",
        "tf27_gpu_model = (\n",
        "    model_service_client.upload_model(parent=PARENT, model=tf27_gpu_model_dict)\n",
        "    .result(timeout=180)\n",
        "    .model\n",
        ")\n",
        "tf27_gpu_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9eb58df8087"
      },
      "source": [
        "For deploying model using Vertex AI Prediction optimized TensorFlow runtime, use the `us-docker.pkg.dev/vertex-ai-restricted/prediction/tf_opt-gpu.nightly:latest` container.\n",
        "\n",
        "Two optimization options are applied to the model.\n",
        "- *allow_precompilation* - turns on model pre-compilation for better performance. Note that model precompilation happens when the first request with the new batch size arrives, and the response for that request is sent after precompilation is complete. To mitigate this, specify a warmup file (see the section earlier in this colab). Model precompilation works for different kinds of models, and in most cases has a positive effect on performance. However, we recommend that you try it out for your model before you enable it in production.\n",
        "- *allow_precision_affecting_optimizations* - enables precision affecting optimizations. In some cases this makes the model run significantly faster at the cost of very minimal loss to model prediction power. You should assess the precision impact to your model when using this optimization.\n",
        "\n",
        "For the list of available optimized TensorFlow runtimer containers and options, see https://cloud.google.com/vertex-ai/docs/predictions/optimized-tensorflow-runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1152b4321e91"
      },
      "outputs": [],
      "source": [
        "tf_opt_gpu_model_dict = {\n",
        "    \"display_name\": \"BERT Base optimized TensorFlow runtime GPU model\",\n",
        "    \"artifact_uri\": BUCKET_URI,\n",
        "    \"container_spec\": {\n",
        "        \"image_uri\": \"us-docker.pkg.dev/vertex-ai-restricted/prediction/tf_opt-gpu.nightly:latest\",\n",
        "        \"args\": [\n",
        "            \"--allow_precompilation=true\",\n",
        "            \"--allow_precision_affecting_optimizations=false\",\n",
        "        ],\n",
        "    },\n",
        "}\n",
        "\n",
        "tf_opt_gpu_model = (\n",
        "    model_service_client.upload_model(parent=PARENT, model=tf_opt_gpu_model_dict)\n",
        "    .result(timeout=180)\n",
        "    .model\n",
        ")\n",
        "tf_opt_gpu_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "610c6d77f5d9"
      },
      "outputs": [],
      "source": [
        "tf_opt_lossy_gpu_model_dict = {\n",
        "    \"display_name\": \"BERT Base optimized TensorFlow runtime GPU model with lossy optimizations\",\n",
        "    \"artifact_uri\": BUCKET_URI,\n",
        "    \"container_spec\": {\n",
        "        \"image_uri\": \"us-docker.pkg.dev/vertex-ai-restricted/prediction/tf_opt-gpu.nightly:latest\",\n",
        "        \"args\": [\n",
        "            \"--allow_precompilation=true\",\n",
        "            \"--allow_precision_affecting_optimizations=true\",\n",
        "        ],\n",
        "    },\n",
        "}\n",
        "\n",
        "tf_opt_lossy_gpu_model = (\n",
        "    model_service_client.upload_model(parent=PARENT, model=tf_opt_lossy_gpu_model_dict)\n",
        "    .result(timeout=180)\n",
        "    .model\n",
        ")\n",
        "tf_opt_lossy_gpu_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9fb2a4e6c63"
      },
      "source": [
        "List all models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "396cc71ebd9e"
      },
      "outputs": [],
      "source": [
        "model_service_client.list_models(parent=PARENT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4605d8befbfd"
      },
      "source": [
        "### Create endpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ff288387d03"
      },
      "source": [
        "Learn more about [endpoint_service.create_endpoint](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1.services.endpoint_service.EndpointServiceClient#google_cloud_aiplatform_v1_services_endpoint_service_EndpointServiceClient_create_endpoint)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e9cf3f15310"
      },
      "outputs": [],
      "source": [
        "tf27_gpu_endpoint_dict = {\n",
        "    \"display_name\": \"BERT Base TF2.7 GPU endpoint\",\n",
        "}\n",
        "tf27_gpu_endpoint = (\n",
        "    endpoint_service_client.create_endpoint(\n",
        "        parent=PARENT, endpoint=tf27_gpu_endpoint_dict\n",
        "    )\n",
        "    .result(timeout=300)\n",
        "    .name\n",
        ")\n",
        "tf27_gpu_endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eb899497ecf"
      },
      "outputs": [],
      "source": [
        "tf_opt_gpu_endpoint_dict = {\n",
        "    \"display_name\": \"BERT Base optimized TensorFlow runtime GPU endpoint\",\n",
        "}\n",
        "tf_opt_gpu_endpoint = (\n",
        "    endpoint_service_client.create_endpoint(\n",
        "        parent=PARENT, endpoint=tf_opt_gpu_endpoint_dict\n",
        "    )\n",
        "    .result(timeout=300)\n",
        "    .name\n",
        ")\n",
        "tf_opt_gpu_endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bf458b8ede48"
      },
      "outputs": [],
      "source": [
        "tf_opt_lossy_gpu_endpoint_dict = {\n",
        "    \"display_name\": \"BERT Base optimized TensorFlow runtime GPU with lossy optimizations endpoint\",\n",
        "}\n",
        "tf_opt_lossy_gpu_endpoint = (\n",
        "    endpoint_service_client.create_endpoint(\n",
        "        parent=PARENT, endpoint=tf_opt_lossy_gpu_endpoint_dict\n",
        "    )\n",
        "    .result(timeout=300)\n",
        "    .name\n",
        ")\n",
        "tf_opt_lossy_gpu_endpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6873939a0a1"
      },
      "source": [
        "### Deploy models to endpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebb2d55b45e0"
      },
      "source": [
        "Learn more about [enpoint_service.deploy_model](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1.services.endpoint_service.EndpointServiceClient#google_cloud_aiplatform_v1_services_endpoint_service_EndpointServiceClient_deploy_model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20f414f9e8be"
      },
      "outputs": [],
      "source": [
        "tf27_gpu_deployed_model_dict = {\n",
        "    \"model\": tf27_gpu_model,\n",
        "    \"display_name\": \"BERT Base TF2.7 GPU deployed model\",\n",
        "    \"dedicated_resources\": {\n",
        "        \"min_replica_count\": 1,\n",
        "        \"max_replica_count\": 1,\n",
        "        \"machine_spec\": {\n",
        "            \"machine_type\": DEPLOY_COMPUTE,\n",
        "            \"accelerator_type\": DEPLOY_GPU,\n",
        "            \"accelerator_count\": 1,\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "tf27_gpu_deployed_model = endpoint_service_client.deploy_model(\n",
        "    endpoint=tf27_gpu_endpoint,\n",
        "    deployed_model=tf27_gpu_deployed_model_dict,\n",
        "    traffic_split={\"0\": 100},\n",
        ").result()\n",
        "tf27_gpu_deployed_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bd1358689794"
      },
      "outputs": [],
      "source": [
        "tf_opt_gpu_deployed_model_dict = {\n",
        "    \"model\": tf_opt_gpu_model,\n",
        "    \"display_name\": \"BERT Base optimized TensorFlow runtime GPU model\",\n",
        "    \"dedicated_resources\": {\n",
        "        \"min_replica_count\": 1,\n",
        "        \"max_replica_count\": 1,\n",
        "        \"machine_spec\": {\n",
        "            \"machine_type\": DEPLOY_COMPUTE,\n",
        "            \"accelerator_type\": DEPLOY_GPU,\n",
        "            \"accelerator_count\": 1,\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "tf_opt_gpu_deployed_model = endpoint_service_client.deploy_model(\n",
        "    endpoint=tf_opt_gpu_endpoint,\n",
        "    deployed_model=tf_opt_gpu_deployed_model_dict,\n",
        "    traffic_split={\"0\": 100},\n",
        ").result()\n",
        "tf_opt_gpu_deployed_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25b5541a0e2f"
      },
      "outputs": [],
      "source": [
        "tf_opt_lossy_gpu_deployed_model_dict = {\n",
        "    \"model\": tf_opt_lossy_gpu_model,\n",
        "    \"display_name\": \"BERT Base optimized TensorFlow runtime GPU model with lossy optimizations\",\n",
        "    \"dedicated_resources\": {\n",
        "        \"min_replica_count\": 1,\n",
        "        \"max_replica_count\": 1,\n",
        "        \"machine_spec\": {\n",
        "            \"machine_type\": DEPLOY_COMPUTE,\n",
        "            \"accelerator_type\": DEPLOY_GPU,\n",
        "            \"accelerator_count\": 1,\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "tf_opt_lossy_gpu_deployed_model = endpoint_service_client.deploy_model(\n",
        "    endpoint=tf_opt_lossy_gpu_endpoint,\n",
        "    deployed_model=tf_opt_lossy_gpu_deployed_model_dict,\n",
        "    traffic_split={\"0\": 100},\n",
        ").result()\n",
        "tf_opt_lossy_gpu_deployed_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0016bab3f15d"
      },
      "source": [
        "### Sending prediction request"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edae193ab9ef"
      },
      "source": [
        "Now you can use the `prediction_service_client.predict` API to send prediction requests to your models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13a9f2499fd9"
      },
      "outputs": [],
      "source": [
        "prediction_service_client.predict(\n",
        "    endpoint=tf27_gpu_endpoint,\n",
        "    instances=[\"This was the best movie ever\", \"Movie was boring\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca1dfbcdf81e"
      },
      "source": [
        "Alternatively you can send POST REST requests without using the SDK. Learn more about https://cloud.google.com/vertex-ai/docs/predictions/online-predictions-custom-models#online_predict_custom_trained-drest.\n",
        "This method is slightly faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ff281732942"
      },
      "outputs": [],
      "source": [
        "def get_headers():\n",
        "    gcloud_access_token = (\n",
        "        subprocess.check_output(\"gcloud auth print-access-token\".split(\" \"))\n",
        "        .decode()\n",
        "        .rstrip(\"\\n\")\n",
        "    )\n",
        "    return {\"authorization\": \"Bearer \" + gcloud_access_token}\n",
        "\n",
        "\n",
        "def send_post_request(uri, request_dict):\n",
        "    return r.post(\n",
        "        uri, data=json.dumps(request_dict), headers=get_headers(), verify=False\n",
        "    )\n",
        "\n",
        "\n",
        "uri = f\"https://{REGION}-aiplatform.googleapis.com/v1/{tf27_gpu_endpoint}:predict\"\n",
        "print(uri)\n",
        "\n",
        "request = {\n",
        "    \"instances\": [\n",
        "        {\"text\": \"This was the best movie ever\"},\n",
        "        {\"text\": \"Movie was boring\"},\n",
        "    ]\n",
        "}\n",
        "response = send_post_request(uri, request)\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5ce470e856f"
      },
      "source": [
        "## (optional) Benchmark deployed models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f02ce0cc8d3c"
      },
      "source": [
        "You can run benchmarks from Colab environment, also in order to get reliable results you should use VM is in the same region as your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d59163882bc"
      },
      "source": [
        "Import helper functions for benchmarking models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f213c470f4f"
      },
      "outputs": [],
      "source": [
        "!curl https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/vertex_endpoints/optimized_tensorflow_runtime/benchmark.py -o benchmark.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a54efa306ae8"
      },
      "outputs": [],
      "source": [
        "from benchmark import benchmark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15e169255bbe"
      },
      "source": [
        "This code sends a specified number of requests asynchronously and uniformly at a given QPS, then records the observed latency. Next, the latency results are aggregated and percentiles are calculated.\n",
        "The `actual_qps` that the model can handle is calculated as the time it takes for a model to process the sent requests divided by the number of requests.\n",
        "By providing different implementations for `send_request` and `build_request` functions, the same code can be used for benchmarking models running locally or on Vertex AI Prediction using gRPC and REST protocols.\n",
        "\n",
        "The main goal of this benchmark is to measure model latency on different loads, and maximum throughput the model can handle. In order to find maximum throughput, gradually increase QPS until `actual_qps` stops increasing and latency increases dramatically.\n",
        "\n",
        "On the production deployment, the workload is not uniform, and therefore the maximum model throughput is likely to be lower.\n",
        "We are not trying to simulate production workload here. This benchmark is meant to compare latency and throughput for same model running on different environments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b1efbe01d49"
      },
      "outputs": [],
      "source": [
        "def build_rest_request(row_dict, model_name):\n",
        "    payload = json.dumps({\"instances\": row_dict})\n",
        "    return payload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47bf78949b57"
      },
      "outputs": [],
      "source": [
        "headers = get_headers()\n",
        "\n",
        "\n",
        "def send_rest_request(request):\n",
        "    res = r.post(\n",
        "        f\"https://{REGION}-aiplatform.googleapis.com/v1/{tf27_gpu_endpoint}:predict\",\n",
        "        data=request,\n",
        "        headers=headers,\n",
        "        verify=False,\n",
        "    )\n",
        "    assert res.status_code == 200\n",
        "    return res\n",
        "\n",
        "\n",
        "tf27_gpu_results = benchmark(\n",
        "    send_rest_request,\n",
        "    build_rest_request,\n",
        "    f\"{LOCAL_DIRECTORY_FULL}/requests/requests_10_32.jsonl\",\n",
        "    [1, 2, 3, 4, 5],\n",
        "    5,\n",
        ")\n",
        "\n",
        "tf27_gpu_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41de13a31620"
      },
      "outputs": [],
      "source": [
        "headers = get_headers()\n",
        "\n",
        "\n",
        "def send_rest_request(request):\n",
        "    res = r.post(\n",
        "        f\"https://{REGION}-aiplatform.googleapis.com/v1/{tf_opt_gpu_endpoint}:predict\",\n",
        "        data=request,\n",
        "        headers=headers,\n",
        "        verify=False,\n",
        "    )\n",
        "    assert res.status_code == 200\n",
        "    return res\n",
        "\n",
        "\n",
        "tf_opt_gpu_results = benchmark(\n",
        "    send_rest_request,\n",
        "    build_rest_request,\n",
        "    f\"{LOCAL_DIRECTORY_FULL}/requests/requests_10_32.jsonl\",\n",
        "    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    5,\n",
        ")\n",
        "\n",
        "tf_opt_gpu_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83378b26638a"
      },
      "outputs": [],
      "source": [
        "headers = get_headers()\n",
        "\n",
        "\n",
        "def send_rest_request(request):\n",
        "    res = r.post(\n",
        "        f\"https://{REGION}-aiplatform.googleapis.com/v1/{tf_opt_lossy_gpu_endpoint}:predict\",\n",
        "        data=request,\n",
        "        headers=headers,\n",
        "        verify=False,\n",
        "    )\n",
        "    assert res.status_code == 200\n",
        "    return res\n",
        "\n",
        "\n",
        "tf_opt_lossy_gpu_results = benchmark(\n",
        "    send_rest_request,\n",
        "    build_rest_request,\n",
        "    f\"{LOCAL_DIRECTORY_FULL}/requests/requests_10_32.jsonl\",\n",
        "    [1, 5, 10, 15, 20, 21, 22, 23, 24, 25],\n",
        "    5,\n",
        ")\n",
        "\n",
        "tf_opt_lossy_gpu_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f855885e50f"
      },
      "source": [
        "Combine and visualize results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1afa1a329b0e"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def build_graph(x_key, y_key, results_dict, axis):\n",
        "    matplotlib.rcParams[\"figure.figsize\"] = [10.0, 7.0]\n",
        "\n",
        "    fig, ax = plt.subplots(facecolor=(1, 1, 1))\n",
        "    ax.set_xlabel(\"QPS\")\n",
        "    ax.set_ylabel(\"Latency(ms)\")\n",
        "    for title, results in results_dict.items():\n",
        "        x = np.array(results[x_key])\n",
        "        y = np.array(results[y_key])\n",
        "        ax.plot(x, y, label=title)\n",
        "    ax.legend()\n",
        "    ax.axis(axis)\n",
        "    ax.set_title(f\"BERT base model {y_key} latency, batch size 32\")\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "204d66f54e46"
      },
      "outputs": [],
      "source": [
        "fig = build_graph(\n",
        "    \"actual_qps\",\n",
        "    \"p50\",\n",
        "    {\n",
        "        \"TF2.7 GPU\": tf27_gpu_results,\n",
        "        \"TF opt GPU\": tf_opt_gpu_results,\n",
        "        \"TF opt GPU lossy\": tf_opt_lossy_gpu_results,\n",
        "    },\n",
        "    (0, 14, 0, 1000),\n",
        ")\n",
        "fig.savefig(\"bert_p50_latency_32.png\", bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7251aa3b7721"
      },
      "outputs": [],
      "source": [
        "fig = build_graph(\n",
        "    \"actual_qps\",\n",
        "    \"p99\",\n",
        "    {\n",
        "        \"TF2.7 GPU\": tf27_gpu_results,\n",
        "        \"TF opt GPU\": tf_opt_gpu_results,\n",
        "        \"TF opt GPU lossy\": tf_opt_lossy_gpu_results,\n",
        "    },\n",
        "    (0, 14, 0, 1000),\n",
        ")\n",
        "fig.savefig(\"bert_p99_latency_32.png\", bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8e97703a929"
      },
      "source": [
        "You can see that the Vertex AI Prediction optimized TensorFlow runtime has signficantly higher throughput and lower latency compared to TensorFlow 2.7."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06f2711e6221"
      },
      "source": [
        "## (Optional) Compare performance of deployed models using MLPerf Inference loadgen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adcd7c46de26"
      },
      "source": [
        "MLPerf Inference is a benchmark suite for measuring how fast systems can run models in a variety of deployment scenarios. MLPerf is now an industry standard way of measuring model performance. You can follow instructions at https://github.com/tensorflow/tpu/tree/master/models/experimental/inference/load_test to run MLPerf Inferenence benchmark for deployed models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45bb8944198c"
      },
      "source": [
        "## (Optional) Compare prediction results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f45ccc0dd077"
      },
      "source": [
        "In this sample the Vertex Prediction optimized TensorFlow runtime is used with the `allow_precision_affecting_optimizations` flag set to `true` to gain additional speedup. Now let's check how those optimizations effect prediction results.\n",
        "\n",
        "Compare the results of predictions for 32,000 requests for a model running on the optimized TensorFlow runtime with lossy optimizations and on TF2.7."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47c82c4797dc"
      },
      "outputs": [],
      "source": [
        "def get_predictions(endpoint, requests_file_path):\n",
        "    responses = []\n",
        "\n",
        "    with tf.io.gfile.GFile(requests_file_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            row_dict = json.loads(line)\n",
        "            response = prediction_service_client.predict(\n",
        "                endpoint=endpoint,\n",
        "                instances=row_dict[\"text\"],\n",
        "            )\n",
        "            for prediction in response.predictions:\n",
        "                responses.append(prediction[0])\n",
        "\n",
        "    return np.array(responses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "130c95fb82fa"
      },
      "outputs": [],
      "source": [
        "tf27_gpu_predictions = get_predictions(\n",
        "    tf27_gpu_endpoint, f\"{LOCAL_DIRECTORY_FULL}/requests/requests_1000_32.jsonl\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bd0c12a7ed0"
      },
      "outputs": [],
      "source": [
        "tf_opt_lossy_gpu_predictions = get_predictions(\n",
        "    tf_opt_lossy_gpu_endpoint, f\"{LOCAL_DIRECTORY_FULL}/requests/requests_1000_32.jsonl\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f60d1e7035e"
      },
      "outputs": [],
      "source": [
        "np.average(tf_opt_lossy_gpu_predictions - tf27_gpu_predictions) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61a550dc2270"
      },
      "outputs": [],
      "source": [
        "np.max(np.abs(tf_opt_lossy_gpu_predictions - tf27_gpu_predictions)) * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd7a55926226"
      },
      "source": [
        "You can see the average results are different for less than 0.01%. In the worst case the difference is less than 1%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c69570acca3d"
      },
      "source": [
        "## Cleanup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df0bd8f8c5f2"
      },
      "source": [
        "After you are done, it's safe to remove the endpoints you created and the model you deployed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18074be6b41c"
      },
      "outputs": [],
      "source": [
        "def cleanup(endpoint, model_name, deployed_model_id):\n",
        "    response = endpoint_service_client.undeploy_model(\n",
        "        endpoint=endpoint, deployed_model_id=deployed_model_id\n",
        "    )\n",
        "    print(\"running undeploy_model operation:\", response.operation.name)\n",
        "    print(response.result())\n",
        "\n",
        "    response = endpoint_service_client.delete_endpoint(name=endpoint)\n",
        "    print(\"running delete_endpoint operation:\", response.operation.name)\n",
        "    print(response.result())\n",
        "\n",
        "    response = model_service_client.delete_model(name=model_name)\n",
        "    print(\"running delete_model operation:\", response.operation.name)\n",
        "    print(response.result())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab6cb1891003"
      },
      "outputs": [],
      "source": [
        "cleanup(tf27_gpu_endpoint, tf27_gpu_model, tf27_gpu_deployed_model.deployed_model.id)\n",
        "cleanup(\n",
        "    tf_opt_gpu_endpoint, tf_opt_gpu_model, tf_opt_gpu_deployed_model.deployed_model.id\n",
        ")\n",
        "cleanup(\n",
        "    tf_opt_lossy_gpu_endpoint,\n",
        "    tf_opt_lossy_gpu_model,\n",
        "    tf_opt_lossy_gpu_deployed_model.deployed_model.id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2e0c5c6f2e9"
      },
      "source": [
        "You can now also remove model from GCS bucket as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45026d023cae"
      },
      "outputs": [],
      "source": [
        "# Set this to true only if you'd like to delete your bucket\n",
        "delete_bucket = False\n",
        "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
        "    !gsutil rm -r $BUCKET_URI"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "bert_optimized_online_prediction.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

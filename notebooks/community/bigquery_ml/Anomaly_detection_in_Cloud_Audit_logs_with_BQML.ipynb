{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Anomaly detection in security logs with BQML\n",
        "\n",
        "\n",
        "\n",
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/bigquery_ml/Anomaly_detection_in_Cloud_Audit_logs_with_BQML.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/bigquery_ml/Anomaly_detection_in_Cloud_Audit_logs_with_BQML.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/bigquery_ml/Anomaly_detection_in_Cloud_Audit_logs_with_BQML.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>                                                                                               \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24743cf4a1e1"
      },
      "source": [
        "**_NOTE_**: This notebook has been tested in the following environment:\n",
        "\n",
        "* Python version = 3.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This Colab notebook demonstrates how to use BigQuery ML to detect anomalies in Cloud Audit logs.  We'll use two different pre-built ML models for unsupervised anomaly detection, K-means clustering and Autoencoders, to help us identify outliers such as an uncommon API usage by any user identity. Identifying anomalies in audit logs is critical for cloud administrators and operators to identify potential threats from priviledge escalation to API abuse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d975e698c9a4"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to:\n",
        "\n",
        "* Apply feature enginering by preprocessing Cloud Audit logs\n",
        "* Use BigQuery ML for unsupervised anomaly detection in Cloud Audit logs\n",
        "* Train and evaluate ML models such as K-means clustering and Autoencoders\n",
        "* Extract and analyze outliers\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services and resources:\n",
        "\n",
        "- BigQuery\n",
        "- Cloud Storage\n",
        "- Log Analytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po3_GwJTcJAb"
      },
      "source": [
        "### Prerequisite\n",
        " If you haven't already done so, the only requirement is to [upgrade your existing log bucket](https://cloud.google.com/logging/docs/buckets#upgrade-bucket) to use Log Analytics which provides you with a linked BigQuery dataset with your own queryable logs data. This is a **one-click step without incurring additional costs**. By default, Cloud Audit Admin Activity logs are enabled, ingested and stored in every project's `_Required` bucket without any charges.\n",
        "\n",
        "![one click prerequisite](https://services.google.com/fh/files/misc/upgrade_log_bucket.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08d289fa873f"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "For this notebook, you will analyze your own Cloud Audit logs such as Admin Activity logs which are enabled and stored by default in every Google Cloud project. Unlike synthetic data, analyzing your own real data will provide you with actual insights but results will vary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aed92deeb4a0"
      },
      "source": [
        "### Costs\n",
        "\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* BigQuery\n",
        "\n",
        "Learn about [BigQuery pricing](https://cloud.google.com/bigquery/pricing)\n",
        "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF1j6f9HApxa"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "3. [Enable the BigQuery API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "4. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WReHDGG5g0XY"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, try the following:\n",
        "* Run `gcloud config list`.\n",
        "* Run `gcloud projects list`.\n",
        "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM1iC_MfAts1"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}\n",
        "%env GOOGLE_CLOUD_PROJECT=$PROJECT_ID\n",
        "!echo project_id = $PROJECT_ID > ~/.bigqueryrc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERhr_dYSaOYp"
      },
      "outputs": [],
      "source": [
        "REGION = \"[your-region]\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvNw41proyjI"
      },
      "source": [
        "Provide the Project, BigQuery dataset & BigQuery table where the audit logs are stored. You can find the linked BigQuery dataset ID for your log bucket from the [Logs Storage page](https://console.cloud.google.com/logs/storage)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJmMKlpgolG_"
      },
      "outputs": [],
      "source": [
        "logSourceProject = \"[your-log-source-project-id]\"  # @param {type:\"string\"} custom\n",
        "logSourceBqDataset = \"[your-log-source-dataset]\"  # @param {type:\"string\"} custom\n",
        "logSourceBqTable = \"[your-log-source-table]\"  # @param {type:\"string\"} custom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgGogNfOpC2k"
      },
      "source": [
        "This is the BigQuery dataset & BigQuery table where the preprocessed training dataset will be stored"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2o5lto2fxc9-"
      },
      "outputs": [],
      "source": [
        "BQ_DATASET_NAME = \"bqml_approach\"  # @param {type:\"string\"} custom\n",
        "BQ_TABLE_NAME = \"training_data\"  # @param {type:\"string\"} custom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHFuwMb4pVbM"
      },
      "source": [
        " Provide the BQML model names; These models will be saved under the above mentioned BQ dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aclo9Ll9pg1n"
      },
      "outputs": [],
      "source": [
        "KMEANS_MODEL = \"KMEANS_HTUNED\"  # @param {type:\"string\"} custom\n",
        "AUTO_ENCODER_MODEL = \"AUTOENCODER_HTUNED\"  # @param {type:\"string\"} custom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBCra4QMA2wR"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74ccc9e52986"
      },
      "source": [
        "**1. Vertex AI Workbench**\n",
        "* Do nothing as you are already authenticated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de775a3773ba"
      },
      "source": [
        "**2. Local JupyterLab instance, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "254614fa0c46"
      },
      "outputs": [],
      "source": [
        "# ! gcloud auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef21552ccea8"
      },
      "source": [
        "**3. Colab, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "603adbbf0532"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "960505627ddf"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "from google.cloud import bigquery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbV46JXUqb3E"
      },
      "source": [
        "## Training Data Preparation and Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RA2QEcAt-945"
      },
      "source": [
        "Cloud Audit logs contain a wealth of important information but their volume, velocity and variety makes it challenging to analyze at scale. Each log entry has a relatively [complex schema](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) which makes it further challenging to analyze in their raw format.\n",
        "\n",
        "Before running the ML models, you extract the relevant fields from these logs and aggregate (count) the **actions** by **day**, **actor**, **action**, and **source IP**. As we're primarily interested in determining user anomalous behavior, each of those features are relevant and collectively sufficient for our analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gzrfp-_jqlxH"
      },
      "outputs": [],
      "source": [
        "# This helper function executes the sql query, wait for query execution completion and returns the results as dataframe\n",
        "def execute_sql(sql_query: str):\n",
        "    \"\"\"The executes the sql.\n",
        "    Args:\n",
        "        sql_query:(:obj:`str`): SQL query to execute\n",
        "    \"\"\"\n",
        "    from google.cloud import bigquery\n",
        "\n",
        "    client = bigquery.Client()\n",
        "    import traceback\n",
        "\n",
        "    try:\n",
        "        client = bigquery.Client()\n",
        "        start = time.time()\n",
        "        query_job = client.query(sql_query)  # Make an API request.\n",
        "        print(\"Query Executed.Waiting for completion\")\n",
        "        results = query_job.result()  # Waits for job to complete.\n",
        "        end = time.time()\n",
        "        print(\"Query Execution completed\")\n",
        "        print(\"Time taken to execute:\", end - start)\n",
        "        if results.total_rows > 0:\n",
        "            df = results.to_dataframe()\n",
        "            df.head()\n",
        "            return df\n",
        "    except Exception as e:\n",
        "        error = traceback.format_exc()\n",
        "        print(error)\n",
        "        print(e)\n",
        "        raise RuntimeError(f\"Can't execute the query {sql_query}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AlJyrIh42G7"
      },
      "source": [
        "The following UDF extracts the resourced ID that was acted on per the audit log entry. In the audit log entry, The resource ID is specified in a different resource label field depending on the resource type. That's why this UDF is needed to normalize that resource ID field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpU-wlxq4wv3"
      },
      "outputs": [],
      "source": [
        "# Deduce resource ID from a log entry resource field\n",
        "UDF_NAME = \"getResourceId\"\n",
        "\n",
        "sql = \"\"\"\n",
        "CREATE OR REPLACE FUNCTION `{}.{}.{}`(\n",
        "  type STRING,\n",
        "  labels JSON\n",
        ")\n",
        "RETURNS STRING\n",
        "AS (\n",
        " COALESCE(\n",
        "  JSON_VALUE(labels.email_id),     # service_account\n",
        "  JSON_VALUE(labels.pod_id),       # container\n",
        "  JSON_VALUE(labels.instance_id),  # gce_instance, spanner_instance, redis_instance, ...\n",
        "  JSON_VALUE(labels.subnetwork_id),# gce_subnetwork,\n",
        "  JSON_VALUE(labels.network_id),   # gce_network, gce_network_region, ...\n",
        "  JSON_VALUE(labels.topic_id),     # pubsub_topic\n",
        "  JSON_VALUE(labels.subscription_id), # pubsub_subscription\n",
        "  JSON_VALUE(labels.endpoint_id),  # aiplatform.googleapis.com/Endpoint\n",
        "  JSON_VALUE(labels.job_id),       # dataflow_step\n",
        "  JSON_VALUE(labels.dataset_id),   # bigquery_dataset\n",
        "  JSON_VALUE(labels.project_id),\n",
        "  JSON_VALUE(labels.organization_id),\n",
        "  JSON_VALUE(labels.id),\n",
        "  \"other\")\n",
        ");\"\"\".format(\n",
        "    PROJECT_ID, BQ_DATASET_NAME, UDF_NAME\n",
        ")\n",
        "\n",
        "execute_sql(sql)\n",
        "print(f\"Created UDF {PROJECT_ID}.{BQ_DATASET_NAME}.{UDF_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqVkSSEM6dXa"
      },
      "source": [
        "The following UDF deduces where a user or system action occured from per the audit log entry. For example, an action may have occured through the Cloud Console, or using gcloud CLI, or via Terraform script or another unknown client or channel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqNnOlF96a9a"
      },
      "outputs": [],
      "source": [
        "# Deduce channel from a log entry request user agent\n",
        "UDF_NAME = \"getChannelType\"\n",
        "\n",
        "sql = \"\"\"CREATE OR REPLACE FUNCTION `{}.{}.{}`(\n",
        "  caller_supplied_user_agent STRING\n",
        ")\n",
        "RETURNS STRING\n",
        "AS (\n",
        "  CASE\n",
        "    WHEN caller_supplied_user_agent LIKE \"Mozilla/%\" THEN 'Cloud Console'\n",
        "    WHEN caller_supplied_user_agent LIKE \"google-cloud-sdk gcloud/%\" THEN 'gcloud CLI'\n",
        "    WHEN caller_supplied_user_agent LIKE \"google-api-go-client/% Terraform/%\" THEN 'Terraform'\n",
        "    ELSE 'other'\n",
        "  END\n",
        ");\"\"\".format(\n",
        "    PROJECT_ID, BQ_DATASET_NAME, UDF_NAME\n",
        ")\n",
        "\n",
        "execute_sql(sql)\n",
        "print(f\"Created UDF {PROJECT_ID}.{BQ_DATASET_NAME}.{UDF_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BciIe3NEGn7c"
      },
      "source": [
        "Query the log source to extract the training data with fields of interest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pO3BxDg8rKWx"
      },
      "outputs": [],
      "source": [
        "# Query to extract training data with fields of interest\n",
        "query_str = \"\"\" SELECT\n",
        "    EXTRACT(DATE FROM timestamp) AS day,\n",
        "    IFNULL(proto_payload.audit_log.authentication_info.principal_email, \"unknown\") as principal_email,\n",
        "    IFNULL(proto_payload.audit_log.method_name, \"unknown\") as action,\n",
        "    IFNULL(resource.type, \"unknown\") as resource_type,\n",
        "    {3}.getResourceId(resource.type, resource.labels) AS resource_id,\n",
        "    -- proto_payload.audit_log.resource_name as resource_name,\n",
        "    SPLIT(log_name, '/')[SAFE_OFFSET(0)] as container_type,\n",
        "    SPLIT(log_name, '/')[SAFE_OFFSET(1)] as container_id,\n",
        "    {3}.getChannelType(proto_payload.audit_log.request_metadata.caller_supplied_user_agent) AS channel,\n",
        "    IFNULL(proto_payload.audit_log.request_metadata.caller_ip, \"unknown\") as ip,\n",
        "    COUNT(*) counter,\n",
        "    -- ANY_VALUE(resource) as resource,           -- for debugging\n",
        "    -- ANY_VALUE(proto_payload) as proto_payload  -- for debugging\n",
        "  FROM  `{0}.{1}.{2}`\n",
        "  WHERE\n",
        "    -- log_id = \"cloudaudit.googleapis.com/activity\" AND\n",
        "    timestamp > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 360 DAY)\n",
        "  GROUP BY\n",
        "    day, principal_email, action, resource_type, resource_id, container_type, container_id, channel, ip, log_name\n",
        "  ORDER BY\n",
        "    day DESC, principal_email, action\"\"\".format(\n",
        "    logSourceProject, logSourceBqDataset, logSourceBqTable, BQ_DATASET_NAME\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAOZwnhOG2Jf"
      },
      "source": [
        "View the training data dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljnUdPUsq5YU"
      },
      "outputs": [],
      "source": [
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "df = client.query(query_str).to_dataframe()\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQN-5qq5RyeP"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elBU2zTJHEIL"
      },
      "source": [
        "Create a table in BQ with the extracted data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PC1WGinBR305"
      },
      "outputs": [],
      "source": [
        "create_training_data_table = (\n",
        "    \"\"\" CREATE OR REPLACE TABLE `{}.{}.{}` AS\"\"\".format(\n",
        "        PROJECT_ID, BQ_DATASET_NAME, BQ_TABLE_NAME\n",
        "    )\n",
        "    + query_str\n",
        ")\n",
        "client.query(create_training_data_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQiEbspFSFjz"
      },
      "source": [
        "## K-Means Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNhRilT_HOtJ"
      },
      "source": [
        "Create K-Means clusters with the training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-mhz2iSSLGF"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HS0GSJ2USOmC"
      },
      "outputs": [],
      "source": [
        "train_kmeans = \"\"\"CREATE MODEL IF NOT EXISTS `{0}.{1}`\n",
        "OPTIONS(MODEL_TYPE = 'KMEANS',\n",
        "NUM_CLUSTERS = HPARAM_RANGE(2, 10),\n",
        "KMEANS_INIT_METHOD = 'KMEANS++',\n",
        "DISTANCE_TYPE = 'COSINE',\n",
        "STANDARDIZE_FEATURES = TRUE,\n",
        "MAX_ITERATIONS = 10,\n",
        "EARLY_STOP = TRUE,\n",
        "NUM_TRIALS = 10\n",
        ") AS\n",
        "SELECT * FROM `{0}.{2}`;\"\"\".format(\n",
        "    BQ_DATASET_NAME, KMEANS_MODEL, BQ_TABLE_NAME\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4IvGcCdSTE1"
      },
      "outputs": [],
      "source": [
        "execute_sql(train_kmeans)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGFbQGIFTR8G"
      },
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHj8TRavTNeB"
      },
      "outputs": [],
      "source": [
        "eval_kmeans = \"\"\"SELECT * FROM ML.EVALUATE(MODEL `{}.{}`);\"\"\".format(\n",
        "    BQ_DATASET_NAME, KMEANS_MODEL\n",
        ")\n",
        "model_evalution = execute_sql(eval_kmeans)\n",
        "model_evalution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2_c3shTTcOP"
      },
      "source": [
        "### Outlier Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUOn36mzS0iB"
      },
      "outputs": [],
      "source": [
        "# --- DETECT ANOMALIES --- #\n",
        "detect_anomaly = \"\"\"SELECT * FROM ML.DETECT_ANOMALIES(MODEL `{0}.{1}.{2}`,\n",
        "STRUCT(0.001 AS contamination),\n",
        "TABLE `{0}.{1}.{3}`)\n",
        "WHERE is_anomaly=true\n",
        "ORDER BY normalized_distance DESC;\"\"\".format(\n",
        "    PROJECT_ID, BQ_DATASET_NAME, KMEANS_MODEL, BQ_TABLE_NAME\n",
        ")\n",
        "\n",
        "kmeans_outliers = execute_sql(detect_anomaly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpsyWaLoP7ae"
      },
      "outputs": [],
      "source": [
        "kmeans_outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmy2mzsQUiQK"
      },
      "source": [
        "## Auto Encoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISBDtEM7UkSs"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcPB70AuUoYL"
      },
      "outputs": [],
      "source": [
        "train_auto_encoder = \"\"\"\n",
        "CREATE MODEL IF NOT EXISTS `{0}.{1}`\n",
        "OPTIONS(\n",
        "MODEL_TYPE='autoencoder',\n",
        "L1_REG_ACTIVATION = HPARAM_CANDIDATES([0.001, 0.01, 0.1]),\n",
        "LEARN_RATE = HPARAM_CANDIDATES([0.001, 0.01, 0.1]),\n",
        "OPTIMIZER = HPARAM_CANDIDATES(['ADAGRAD', 'ADAM', 'FTRL', ''RMSPROP', 'SGD']),\n",
        "ACTIVATION_FN='relu',\n",
        "BATCH_SIZE = HPARAM_CANDIDATES([16, 32, 64]),\n",
        "DROPOUT = HPARAM_CANDIDATES([0.1, 0.2]),\n",
        "HIDDEN_UNITS=HPARAM_CANDIDATES([struct([[16, 8, 4, 8, 16]]), struct([[32, 16, 4, 16, 32]])]),\n",
        "TF_VERSION = '2.8.0',\n",
        "EARLY_STOP = TRUE,\n",
        "MIN_REL_PROGRESS = 0.01,\n",
        "MAX_ITERATIONS=20,\n",
        "WARM_START = TRUE,\n",
        "NUM_TRIALS = 60,\n",
        "MAX_PARALLEL_TRIALS = 1,\n",
        "HPARAM_TUNING_ALGORITHM =  'VIZIER_DEFAULT',\n",
        "HPARAM_TUNING_OBJECTIVES = MEAN_SQUARED_ERROR\n",
        ") AS\n",
        "SELECT\n",
        "*\n",
        "FROM `{0}.{2}`;\"\"\".format(\n",
        "    BQ_DATASET_NAME, AUTO_ENCODER_MODEL, BQ_TABLE_NAME\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nds2Sm-QUrgv"
      },
      "outputs": [],
      "source": [
        "execute_sql(train_auto_encoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d596XDbHU9dM"
      },
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0x_U039VA3c"
      },
      "outputs": [],
      "source": [
        "eval_auto_encoder = \"\"\"SELECT * FROM ML.EVALUATE(MODEL `{}.{}`);\"\"\".format(\n",
        "    BQ_DATASET_NAME, AUTO_ENCODER_MODEL\n",
        ")\n",
        "model_evalution = execute_sql(eval_auto_encoder)\n",
        "model_evalution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clGMuJcLVIFO"
      },
      "source": [
        "### Outlier Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fy9v8TQCVKZz"
      },
      "outputs": [],
      "source": [
        "# --- DETECT ANOMALIES --- #\n",
        "detect_anomaly_auto_encoder = \"\"\"SELECT * FROM ML.DETECT_ANOMALIES(MODEL `{0}.{1}.{2}`,\n",
        "STRUCT(0.001 AS contamination),\n",
        "TABLE `{0}.{1}.{3}`)\n",
        "WHERE is_anomaly=true order by mean_squared_error desc;\"\"\".format(\n",
        "    PROJECT_ID, BQ_DATASET_NAME, AUTO_ENCODER_MODEL, BQ_TABLE_NAME\n",
        ")\n",
        "# print(detect_anomaly_auto_encoder)\n",
        "autoencoder_outliers = execute_sql(detect_anomaly_auto_encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwmJ9unfXiT8"
      },
      "outputs": [],
      "source": [
        "autoencoder_outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM6bvT03YM8o"
      },
      "source": [
        "## Common Outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNiCl-VHWpUJ"
      },
      "source": [
        "Find out the outliers reported by both models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ6uKD4BvMAi"
      },
      "outputs": [],
      "source": [
        "df1 = kmeans_outliers[\n",
        "    [\n",
        "        \"day\",\n",
        "        \"principal_email\",\n",
        "        \"action\",\n",
        "        \"resource_type\",\n",
        "        \"resource_id\",\n",
        "        \"container_type\",\n",
        "        \"container_id\",\n",
        "        \"channel\",\n",
        "        \"ip\",\n",
        "        \"counter\",\n",
        "    ]\n",
        "]\n",
        "df2 = autoencoder_outliers[\n",
        "    [\n",
        "        \"day\",\n",
        "        \"principal_email\",\n",
        "        \"action\",\n",
        "        \"resource_type\",\n",
        "        \"resource_id\",\n",
        "        \"container_type\",\n",
        "        \"container_id\",\n",
        "        \"channel\",\n",
        "        \"ip\",\n",
        "        \"counter\",\n",
        "    ]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qR9WAT32XeH3"
      },
      "outputs": [],
      "source": [
        "common_outliers = df1.merge(\n",
        "    df2,\n",
        "    how=\"inner\",\n",
        "    on=[\n",
        "        \"day\",\n",
        "        \"principal_email\",\n",
        "        \"action\",\n",
        "        \"resource_type\",\n",
        "        \"resource_id\",\n",
        "        \"container_type\",\n",
        "        \"container_id\",\n",
        "        \"channel\",\n",
        "        \"ip\",\n",
        "        \"counter\",\n",
        "    ],\n",
        ")  # Replace 'column_name' if necessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kRTiFEyXtFg"
      },
      "outputs": [],
      "source": [
        "common_outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qgBA3HYVViz"
      },
      "outputs": [],
      "source": [
        "common_outliers.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bscC9pxUBU8z"
      },
      "source": [
        "## Uploading detected outliers to BQ table for further analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YP8ptrh2Nj0E"
      },
      "source": [
        "Create a table 'common_outliers'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qT5Ck3XNryf"
      },
      "outputs": [],
      "source": [
        "OUTLIERS_TABLE = \"[your-common-outliers-table]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-BfJ8Ew0tex"
      },
      "outputs": [],
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "\n",
        "def create_table(client, table_id, schema):\n",
        "    table = bigquery.Table(table_id, schema=schema)\n",
        "    table = client.create_table(table, exists_ok=True)  # Make an API request\n",
        "    print(\n",
        "        \"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id)\n",
        "    )\n",
        "\n",
        "\n",
        "def upload_df_into_bq(client, table_id, df):\n",
        "    job_config = bigquery.LoadJobConfig(schema=schema)\n",
        "    job_config.write_disposition = (\n",
        "        bigquery.WriteDisposition.WRITE_TRUNCATE\n",
        "    )  # If the table already exists, BigQuery overwrites the data, removes the constraints and uses the schema from the load job.\n",
        "    job_config.autodetect = False\n",
        "    job = client.load_table_from_dataframe(df, table_id, job_config=job_config)\n",
        "    job.result()\n",
        "    print(\"Uploaded dataframe into table {}.{}\".format(PROJECT_ID, table_id))\n",
        "\n",
        "\n",
        "schema = [\n",
        "    bigquery.SchemaField(\"day\", \"DATE\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"principal_email\", \"STRING\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"action\", \"STRING\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"resource_type\", \"STRING\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"resource_id\", \"STRING\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"container_type\", \"STRING\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"container_id\", \"STRING\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"channel\", \"STRING\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"ip\", \"STRING\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"counter\", \"INTEGER\", mode=\"REQUIRED\"),\n",
        "]\n",
        "client = bigquery.Client(PROJECT_ID)\n",
        "\n",
        "table_id = \"{}.{}.{}\".format(PROJECT_ID, BQ_DATASET_NAME, OUTLIERS_TABLE)\n",
        "\n",
        "create_table(client, table_id, schema)\n",
        "\n",
        "upload_df_into_bq(client, table_id, common_outliers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx_vKniMq9ZX"
      },
      "outputs": [],
      "source": [
        "# Delete the BigQuery dataset (including the models created & the tables)\n",
        "dataset_to_be_deleted = \"test\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tk9D2KRRCXrz"
      },
      "outputs": [],
      "source": [
        "!bq rm -r -f {PROJECT_ID}:{dataset_to_be_deleted}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Anomaly_detection_in_Cloud_Audit_logs_with_BQML.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

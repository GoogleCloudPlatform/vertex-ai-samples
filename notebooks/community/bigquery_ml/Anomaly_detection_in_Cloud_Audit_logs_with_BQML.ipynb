{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Anomaly detection in security logs with BQML\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/bigquery_ml/Anomaly_detection_in_Cloud_Audit_logs_with_BQML.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fbigquery_ml%2FAnomaly_detection_in_Cloud_Audit_logs_with_BQML.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>    \n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/bigquery_ml/Anomaly_detection_in_Cloud_Audit_logs_with_BQML.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/bigquery_ml/Anomaly_detection_in_Cloud_Audit_logs_with_BQML.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24743cf4a1e1"
      },
      "source": [
        "**_NOTE_**: This notebook has been tested in the following environment:\n",
        "\n",
        "* Python version = 3.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This Colab notebook demonstrates how you use BigQuery ML to detect anomalies in Cloud Audit logs. You use two different pre-built ML models for unsupervised anomaly detection: K-means clustering and Autoencoders. These models help you identify outliers, such as uncommon API usage by any user identity. Identifying anomalies in audit logs is critical for cloud administrators and operators to spot potential threats, from privilege escalation to API abuse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d975e698c9a4"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to:\n",
        "\n",
        "* Apply feature enginering by preprocessing Cloud Audit logs\n",
        "* Use BigQuery ML for unsupervised anomaly detection in Cloud Audit logs\n",
        "* Train and evaluate ML models such as K-means clustering and Autoencoders\n",
        "* Extract and analyze outliers\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services and resources:\n",
        "\n",
        "- BigQuery\n",
        "- Cloud Storage\n",
        "- Log Analytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po3_GwJTcJAb"
      },
      "source": [
        "### Prerequisite\n",
        " If you haven't already done so, the only requirement is to [upgrade your existing log bucket](https://cloud.google.com/logging/docs/buckets#upgrade-bucket) to use Log Analytics which provides you with a linked BigQuery dataset with your own queryable logs data. This is a **one-click step without incurring additional costs**. By default, Cloud Audit Admin Activity logs are enabled, ingested and stored in every project's `_Required` bucket without any charges.\n",
        "\n",
        "![one click prerequisite](https://services.google.com/fh/files/misc/upgrade_log_bucket.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08d289fa873f"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "In this notebook, you analyze your own Cloud Audit logs, such as Admin Activity logs, which are enabled and stored by default in every Google Cloud project. Unlike synthetic data, analyzing your real data provides you with actual insights, but the results can vary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aed92deeb4a0"
      },
      "source": [
        "### Costs\n",
        "\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* BigQuery\n",
        "\n",
        "Learn about [BigQuery pricing](https://cloud.google.com/bigquery/pricing)\n",
        "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f443f2611bbc"
      },
      "source": [
        "## Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15703bffa7fb"
      },
      "source": [
        "### Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf8edf110ba8"
      },
      "outputs": [],
      "source": [
        "! pip install --upgrade google-cloud-bigquery\n",
        "! pip install pandas\n",
        "! pip install db-dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea812d229d23"
      },
      "source": [
        "### Restart runtime (Colab only)\n",
        "\n",
        "To use the newly installed packages, you must restart the runtime on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a61e2fd1dc68"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb6faf50d3cd"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b055958409c"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "Authenticate your environment on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c74b83ece6c"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF1j6f9HApxa"
      },
      "source": [
        "### Set Google Cloud project information\n",
        "\n",
        "To get started using Bigquery, you must have an existing Google Cloud project and [Enable the BigQuery API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM1iC_MfAts1"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "LOCATION = \" [your-location]\"  # @param {type: \"string\"}\n",
        "DATA_LOCATION = \"US\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73330adca425"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78428c0c124f"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "from google.cloud import bigquery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d7f38ecd2f9"
      },
      "source": [
        "### Create a linked BigQuery dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c230c34c23e8"
      },
      "source": [
        "When you want to use the capabilities of BigQuery to analyze your log data, upgrade a log bucket to use Log Analytics, and then create a [linked dataset](https://cloud.google.com/logging/docs/buckets#link-bq-dataset). With this configuration, Cloud Logging stores your log data but BigQuery can read the log data.\n",
        "\n",
        "Learn more about [creating a linked BigQuery dataset](https://cloud.google.com/logging/docs/buckets#link-bq-dataset)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvNw41proyjI"
      },
      "source": [
        "Provide the Project ID, BigQuery dataset and BigQuery table where the audit logs are stored. You can find the linked BigQuery dataset ID for your log bucket from the [Logs Storage page](https://console.cloud.google.com/logs/storage)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJmMKlpgolG_"
      },
      "outputs": [],
      "source": [
        "# Set log source project id\n",
        "logSourceProject = \"[your-log-source-project-id]\"  # @param {type:\"string\"} custom\n",
        "# Set log source BigQuery dataset name\n",
        "logSourceBqDataset = \"[your-log-source-dataset]\"  # @param {type:\"string\"} custom\n",
        "# Set log source BigQuery table name\n",
        "logSourceBqTable = \"[your-log-source-table]\"  # @param {type:\"string\"} custom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae7709e2fc7f"
      },
      "source": [
        "If you don't have a linked dataset, run the below cells to create one for a log bucket that is upgraded to use Log Analytics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5beb93079e9"
      },
      "outputs": [],
      "source": [
        "# Set log source Cloud Storage bucket id\n",
        "bucket_id = \"[your-log-source-bucket-id]\"\n",
        "# Set log source location\n",
        "bucket_location = \"[your-log-source-bucket-location]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7741b1140116"
      },
      "outputs": [],
      "source": [
        "# Set the defaults if no values are provided\n",
        "if logSourceProject == \"[your-log-source-project-id]\":\n",
        "    logSourceProject = PROJECT_ID\n",
        "    \n",
        "if logSourceBqDataset == \"[your-log-source-dataset]\":\n",
        "    logSourceBqDataset = \"logs_linked_dataset\"\n",
        "\n",
        "if logSourceBqTable == \"[your-log-source-table]\":\n",
        "    logSourceBqTable = \"_AllLogs\"\n",
        "    \n",
        "if bucket_id == \"[your-log-source-bucket-id]\":\n",
        "    bucket_id = \"_Required\" \n",
        "\n",
        "if bucket_location = \"[your-log-source-bucket-location]\":\n",
        "    bucket_location = \"global\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0476f41dd8c"
      },
      "outputs": [],
      "source": [
        "# Create the linked BigQuery dataset\n",
        "! gcloud logging links create {logSourceBqDataset} --bucket={bucket_id} --location={bucket_location}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgGogNfOpC2k"
      },
      "source": [
        "Set the BigQuery dataset and BigQuery table where the preprocessed training dataset is stored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2o5lto2fxc9-"
      },
      "outputs": [],
      "source": [
        "BQ_DATASET_NAME = \"bqml_approach\"  # @param {type:\"string\"} custom\n",
        "BQ_TABLE_NAME = \"training_data\"  # @param {type:\"string\"} custom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e74657a52f8a"
      },
      "source": [
        "### Create a BigQuery dataset\n",
        "\n",
        "Create a dataset or load an existing dataset in BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ea802989324a"
      },
      "outputs": [],
      "source": [
        "BQ_DATASET_PATH = \".\".join([PROJECT_ID, BQ_DATASET_NAME])\n",
        "\n",
        "# Must be same location as TRAINING_DATASET_BQ_PATH.\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "bq_dataset_pre = bigquery.Dataset(BQ_DATASET_PATH)\n",
        "bq_dataset_pre.location = DATA_LOCATION\n",
        "try:\n",
        "    bq_dataset = client.create_dataset(bq_dataset_pre)\n",
        "except:\n",
        "    bq_dataset = client.get_dataset(bq_dataset_pre)\n",
        "print(f\"Created bigquery dataset {BQ_DATASET_PATH} in {DATA_LOCATION}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHFuwMb4pVbM"
      },
      "source": [
        " Provide the BQML model names. These models are saved under the above mentioned BQ dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aclo9Ll9pg1n"
      },
      "outputs": [],
      "source": [
        "KMEANS_MODEL = \"KMEANS_HTUNED\"  # @param {type:\"string\"} custom\n",
        "AUTO_ENCODER_MODEL = \"AUTOENCODER_HTUNED\"  # @param {type:\"string\"} custom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RA2QEcAt-945"
      },
      "source": [
        "## Training data preparation and analysis\n",
        "\n",
        "Cloud Audit logs contain a wealth of important information but their volume, velocity and variety makes it challenging to analyze at scale. Each log entry has a relatively [complex schema](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) which makes it further challenging to analyze in their raw format.\n",
        "\n",
        "Before running the ML models, you extract the relevant fields from these logs and aggregate (count) the **actions** by **day**, **actor**, **action**, and **source IP**. The objective behind training a machine learning model is to identify anomalous user behaviors for which the provided features are relevant and collectively sufficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gzrfp-_jqlxH"
      },
      "outputs": [],
      "source": [
        "# This helper function executes the sql query, wait for query execution completion and returns the results as dataframe\n",
        "def execute_sql(sql_query: str):\n",
        "    \"\"\"The executes the sql.\n",
        "    Args:\n",
        "        sql_query:(:obj:`str`): SQL query to execute\n",
        "    \"\"\"\n",
        "    from google.cloud import bigquery\n",
        "\n",
        "    client = bigquery.Client()\n",
        "    import traceback\n",
        "\n",
        "    try:\n",
        "        client = bigquery.Client()\n",
        "        start = time.time()\n",
        "        query_job = client.query(sql_query)  # Make an API request.\n",
        "        print(\"Query Executed.Waiting for completion\")\n",
        "        results = query_job.result()  # Waits for job to complete.\n",
        "        end = time.time()\n",
        "        print(\"Query Execution completed\")\n",
        "        print(\"Time taken to execute:\", end - start)\n",
        "        if results.total_rows > 0:\n",
        "            df = results.to_dataframe()\n",
        "            df.head()\n",
        "            return df\n",
        "    except Exception as e:\n",
        "        error = traceback.format_exc()\n",
        "        print(error)\n",
        "        print(e)\n",
        "        raise RuntimeError(f\"Can't execute the query {sql_query}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AlJyrIh42G7"
      },
      "source": [
        "The following user defined function(UDF) extracts the resourced ID that was acted on as per the audit log entry. In the audit log entry, the resource ID is specified within a resource label field based on the resource type. Hence, this UDF is needed to normalize that resource ID field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpU-wlxq4wv3"
      },
      "outputs": [],
      "source": [
        "# Deduce resource ID from a log entry resource field\n",
        "UDF_NAME = \"getResourceId\"\n",
        "\n",
        "sql = \"\"\"\n",
        "CREATE OR REPLACE FUNCTION `{}.{}.{}`(\n",
        "  type STRING,\n",
        "  labels JSON\n",
        ")\n",
        "RETURNS STRING\n",
        "AS (\n",
        " COALESCE(\n",
        "  JSON_VALUE(labels.email_id),     # service_account\n",
        "  JSON_VALUE(labels.pod_id),       # container\n",
        "  JSON_VALUE(labels.instance_id),  # gce_instance, spanner_instance, redis_instance, ...\n",
        "  JSON_VALUE(labels.subnetwork_id),# gce_subnetwork,\n",
        "  JSON_VALUE(labels.network_id),   # gce_network, gce_network_region, ...\n",
        "  JSON_VALUE(labels.topic_id),     # pubsub_topic\n",
        "  JSON_VALUE(labels.subscription_id), # pubsub_subscription\n",
        "  JSON_VALUE(labels.endpoint_id),  # aiplatform.googleapis.com/Endpoint\n",
        "  JSON_VALUE(labels.job_id),       # dataflow_step\n",
        "  JSON_VALUE(labels.dataset_id),   # bigquery_dataset\n",
        "  JSON_VALUE(labels.project_id),\n",
        "  JSON_VALUE(labels.organization_id),\n",
        "  JSON_VALUE(labels.id),\n",
        "  \"other\")\n",
        ");\"\"\".format(\n",
        "    PROJECT_ID, BQ_DATASET_NAME, UDF_NAME\n",
        ")\n",
        "\n",
        "execute_sql(sql)\n",
        "print(f\"Created UDF {PROJECT_ID}.{BQ_DATASET_NAME}.{UDF_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqVkSSEM6dXa"
      },
      "source": [
        "The following UDF deduces where a user or system action occured from as per the audit log entry. For example, an action might have occured through the Cloud Console, or using gcloud CLI, or via Terraform script or another unknown client or channel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqNnOlF96a9a"
      },
      "outputs": [],
      "source": [
        "# Deduce channel from a log entry request user agent\n",
        "UDF_NAME = \"getChannelType\"\n",
        "\n",
        "sql = \"\"\"CREATE OR REPLACE FUNCTION `{}.{}.{}`(\n",
        "  caller_supplied_user_agent STRING\n",
        ")\n",
        "RETURNS STRING\n",
        "AS (\n",
        "  CASE\n",
        "    WHEN caller_supplied_user_agent LIKE \"Mozilla/%\" THEN 'Cloud Console'\n",
        "    WHEN caller_supplied_user_agent LIKE \"google-cloud-sdk gcloud/%\" THEN 'gcloud CLI'\n",
        "    WHEN caller_supplied_user_agent LIKE \"google-api-go-client/% Terraform/%\" THEN 'Terraform'\n",
        "    ELSE 'other'\n",
        "  END\n",
        ");\"\"\".format(\n",
        "    PROJECT_ID, BQ_DATASET_NAME, UDF_NAME\n",
        ")\n",
        "\n",
        "execute_sql(sql)\n",
        "print(f\"Created UDF {PROJECT_ID}.{BQ_DATASET_NAME}.{UDF_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BciIe3NEGn7c"
      },
      "source": [
        "Query the log source to extract the training data with fields of interest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pO3BxDg8rKWx"
      },
      "outputs": [],
      "source": [
        "# Query to extract training data with fields of interest\n",
        "query_str = \"\"\" SELECT\n",
        "    EXTRACT(DATE FROM timestamp) AS day,\n",
        "    IFNULL(proto_payload.audit_log.authentication_info.principal_email, \"unknown\") as principal_email,\n",
        "    IFNULL(proto_payload.audit_log.method_name, \"unknown\") as action,\n",
        "    IFNULL(resource.type, \"unknown\") as resource_type,\n",
        "    {3}.getResourceId(resource.type, resource.labels) AS resource_id,\n",
        "    -- proto_payload.audit_log.resource_name as resource_name,\n",
        "    SPLIT(log_name, '/')[SAFE_OFFSET(0)] as container_type,\n",
        "    SPLIT(log_name, '/')[SAFE_OFFSET(1)] as container_id,\n",
        "    {3}.getChannelType(proto_payload.audit_log.request_metadata.caller_supplied_user_agent) AS channel,\n",
        "    IFNULL(proto_payload.audit_log.request_metadata.caller_ip, \"unknown\") as ip,\n",
        "    COUNT(*) counter,\n",
        "    -- ANY_VALUE(resource) as resource,           -- for debugging\n",
        "    -- ANY_VALUE(proto_payload) as proto_payload  -- for debugging\n",
        "  FROM  `{0}.{1}.{2}`\n",
        "  WHERE\n",
        "    -- log_id = \"cloudaudit.googleapis.com/activity\" AND\n",
        "    timestamp > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 360 DAY)\n",
        "  GROUP BY\n",
        "    day, principal_email, action, resource_type, resource_id, container_type, container_id, channel, ip, log_name\n",
        "  ORDER BY\n",
        "    day DESC, principal_email, action\"\"\".format(\n",
        "    logSourceProject, logSourceBqDataset, logSourceBqTable, BQ_DATASET_NAME\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAOZwnhOG2Jf"
      },
      "source": [
        "View the training data dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljnUdPUsq5YU"
      },
      "outputs": [],
      "source": [
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "df = client.query(query_str).to_dataframe()\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQN-5qq5RyeP"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elBU2zTJHEIL"
      },
      "source": [
        "Create a table in BQ with the extracted data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PC1WGinBR305"
      },
      "outputs": [],
      "source": [
        "create_training_data_table = (\n",
        "    \"\"\" CREATE OR REPLACE TABLE `{}.{}.{}` AS\"\"\".format(\n",
        "        PROJECT_ID, BQ_DATASET_NAME, BQ_TABLE_NAME\n",
        "    )\n",
        "    + query_str\n",
        ")\n",
        "client.query(create_training_data_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQiEbspFSFjz"
      },
      "source": [
        "## K-Means Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNhRilT_HOtJ"
      },
      "source": [
        "Here's a brief description of each parameter for training a `k-means model`:\n",
        "\n",
        "* `MODEL_TYPE`: Specify 'KMEANS' to use k-means clustering for data segmentation.\n",
        "* `NUM_CLUSTERS`: Define the number of clusters. You can set a specific number, use a range (e.g., HPARAM_RANGE(2, 25)), or provide discrete values (e.g., HPARAM_CANDIDATES([5, 10, 50])).\n",
        "* `KMEANS_INIT_METHOD`: Choose the initialization method for clusters. Options include 'RANDOM', 'KMEANS++', or 'CUSTOM'. If 'CUSTOM', specify a BOOL column with TRUE values as initial centroids using KMEANS_INIT_COL.\n",
        "* `DISTANCE_TYPE`: Select the metric for distance computation. Options are 'EUCLIDEAN' (default) or 'COSINE'.\n",
        "* `STANDARDIZE_FEATURES`: Decide whether to standardize numerical features. The default is TRUE.\n",
        "* `MAX_ITERATIONS`: Set the maximum number of training iterations. The default is 20.\n",
        "* `EARLY_STOP`: Choose whether to stop training early if the improvement in loss is below a threshold. The default is TRUE.\n",
        "* `NUM_TRIALS`: Set the maximum number of trials for hyperparameter tuning. The default is 10, and you should use at least (number_of_hyperparameters * 10) trials.\n",
        "\n",
        "For a full list of parameters for training the k-means model, see the [bigqueryml documentation](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-kmeans)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-mhz2iSSLGF"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HS0GSJ2USOmC"
      },
      "outputs": [],
      "source": [
        "# Create the query for model training\n",
        "train_kmeans = \"\"\"CREATE MODEL IF NOT EXISTS `{0}.{1}`\n",
        "OPTIONS(MODEL_TYPE = 'KMEANS',\n",
        "NUM_CLUSTERS = HPARAM_RANGE(2, 10),\n",
        "KMEANS_INIT_METHOD = 'KMEANS++',\n",
        "DISTANCE_TYPE = 'COSINE',\n",
        "STANDARDIZE_FEATURES = TRUE,\n",
        "MAX_ITERATIONS = 10,\n",
        "EARLY_STOP = TRUE,\n",
        "NUM_TRIALS = 10\n",
        ") AS\n",
        "SELECT * FROM `{0}.{2}`;\"\"\".format(\n",
        "    BQ_DATASET_NAME, KMEANS_MODEL, BQ_TABLE_NAME\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4IvGcCdSTE1"
      },
      "outputs": [],
      "source": [
        "# Execute the SQL query for training\n",
        "execute_sql(train_kmeans)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGFbQGIFTR8G"
      },
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHj8TRavTNeB"
      },
      "outputs": [],
      "source": [
        "# Prepare the query to run model evaluation and fetch the results\n",
        "eval_kmeans = \"\"\"SELECT * FROM ML.EVALUATE(MODEL `{}.{}`);\"\"\".format(\n",
        "    BQ_DATASET_NAME, KMEANS_MODEL\n",
        ")\n",
        "# Execute the query\n",
        "model_evalution = execute_sql(eval_kmeans)\n",
        "# Show the results\n",
        "model_evalution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2_c3shTTcOP"
      },
      "source": [
        "### Outlier Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "231207f0446b"
      },
      "source": [
        "Here’s what you get from the `K-means model` output:\n",
        "\n",
        "* `is_anomaly`: A BOOL value that shows whether the value is flagged as anomalous.\n",
        "* `normalized_distance`: A FLOAT64 value representing the shortest distance among the normalized distances from your input data to each cluster centroid. You compute normalized distances by taking the absolute distance from the input data to a cluster centroid and dividing it by the cluster's radius. The cluster radius is the root mean square of all distances from the cluster's assigned data points to its centroid. You use normalized distance instead of absolute distance to detect anomalies more effectively, as normalized distances account for the cluster radius. The distance type depends on the DISTANCE_TYPE value set during model training.\n",
        "* `centroid_id`: An INT64 value that indicates the ID of the centroid.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c731043fae6"
      },
      "source": [
        "Here’s how to use `ML.DETECT_ANOMALIES` with the following arguments:\n",
        "\n",
        "* `project_id`: Your project ID.\n",
        "* `dataset`: The BigQuery dataset that contains the model.\n",
        "* `model`: The name of the model.\n",
        "* `table`: The name of the table to use to perform anomaly detection.\n",
        "* `query_statement`: The GoogleSQL query that generates the data to use to perform anomaly detection.\n",
        "* `contamination`: a FLOAT64 value that identifies the proportion of anomalies in the training dataset that are used to create the autoencoder, k-means, or PCA input models. The value must be in the range [0, 0.5]. For example, contamination value of 0.1 means that 10% of the training data that was used to create the input model is anomalous. The contamination value determines the cutoff threshold of the target metric to become anomalous, and any input data with a target metric greater than the cutoff threshold is identified as anomalous. The target metric is mean squared error for autoencoder and PCA models, and the target metric is normalized distance for k-means models.\n",
        "\n",
        "Learn more about [outlier analysis](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-detect-anomalies)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUOn36mzS0iB"
      },
      "outputs": [],
      "source": [
        "# Create query to detect anomalies\n",
        "detect_anomaly = \"\"\"SELECT * FROM ML.DETECT_ANOMALIES(MODEL `{0}.{1}.{2}`,\n",
        "STRUCT(0.001 AS contamination),\n",
        "TABLE `{0}.{1}.{3}`)\n",
        "WHERE is_anomaly=true\n",
        "ORDER BY normalized_distance DESC;\"\"\".format(\n",
        "    PROJECT_ID, BQ_DATASET_NAME, KMEANS_MODEL, BQ_TABLE_NAME\n",
        ")\n",
        "# Run the query and fetch the outliers\n",
        "kmeans_outliers = execute_sql(detect_anomaly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpsyWaLoP7ae"
      },
      "outputs": [],
      "source": [
        "# Show the outliers\n",
        "kmeans_outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmy2mzsQUiQK"
      },
      "source": [
        "## Auto Encoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4f9d47f4fb9"
      },
      "source": [
        "Here's a brief description of each parameter for training an `autoencoder model`:\n",
        "\n",
        "* `MODEL_TYPE`: Specify 'AUTOENCODER' to indicate you're creating an autoencoder model.\n",
        "* `L1_REG_ACTIVATION`: Apply L1 regularization to the activations in the latent space to promote sparsity. You can set a fixed value or tune it with a range or candidate values.\n",
        "* `LEARN_RATE`: Determine the initial learning rate for training. You can use a specific value or explore a range of values for tuning.\n",
        "* `OPTIMIZER`: Choose the optimizer for training, such as 'ADAM', 'ADAGRAD', 'FTRL', 'RMSPROP', or 'SGD'. For hyperparameter tuning, you can select multiple options from candidates.\n",
        "* `ACTIVATION_FN`: Set the activation function for the neural network. Options include 'RELU', 'RELU6', 'ELU', 'SELU', 'SIGMOID', or 'TANH'. You can also use hyperparameter tuning to test different functions.\n",
        "* `BATCH_SIZE`: Specify the mini-batch size for training. Use a fixed value or tune it with a range or list of candidates.\n",
        "* `DROPOUT`: Define the dropout rate for the neural network units. You can set a specific value or explore a range or candidates for tuning.\n",
        "* `HIDDEN_UNITS`: List the number of units in each hidden layer of the neural network. This can be an array of integers, or you can use hyperparameter tuning to select from candidate architectures.\n",
        "* `TF_VERSION`: Choose the TensorFlow version for model training, either '1.15' or '2.8.0'.\n",
        "* `EARLY_STOP`: Decide if training should stop when the relative loss improvement is below a threshold. Set to TRUE or FALSE.\n",
        "* `MIN_REL_PROGRESS`: Set the minimum relative loss improvement required to continue training if EARLY_STOP is TRUE.\n",
        "* `MAX_ITERATIONS`: Define the maximum number of training iterations.\n",
        "* `WARM_START`: Choose whether to retrain the model with new data or options, while retaining previous model settings. Set to TRUE or FALSE.\n",
        "* `NUM_TRIALS`: Specify the maximum number of submodels to train for hyperparameter tuning.\n",
        "* `MAX_PARALLEL_TRIALS`: Set the maximum number of trials to run concurrently during hyperparameter tuning.\n",
        "* `HPARAM_TUNING_ALGORITHM`: Choose the algorithm for hyperparameter tuning, such as 'VIZIER_DEFAULT', 'RANDOM_SEARCH', or 'GRID_SEARCH'.\n",
        "* `HPARAM_TUNING_OBJECTIVES`: Define the objective for hyperparameter tuning, like 'MEAN_SQUARED_ERROR'.\n",
        "\n",
        "For a full list of parameters for training the autoencoder model, see the [bigqueryml documentation](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-autoencoder)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISBDtEM7UkSs"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcPB70AuUoYL"
      },
      "outputs": [],
      "source": [
        "# Create the query for training the model\n",
        "train_auto_encoder = \"\"\"\n",
        "CREATE MODEL IF NOT EXISTS `{0}.{1}`\n",
        "OPTIONS(\n",
        "MODEL_TYPE='autoencoder',\n",
        "L1_REG_ACTIVATION = HPARAM_CANDIDATES([0.001, 0.01, 0.1]),\n",
        "LEARN_RATE = HPARAM_CANDIDATES([0.001, 0.01, 0.1]),\n",
        "OPTIMIZER = HPARAM_CANDIDATES(['ADAGRAD', 'ADAM', 'FTRL', 'RMSPROP', 'SGD']),\n",
        "ACTIVATION_FN='relu',\n",
        "BATCH_SIZE = HPARAM_CANDIDATES([16, 32, 64]),\n",
        "DROPOUT = HPARAM_CANDIDATES([0.1, 0.2]),\n",
        "HIDDEN_UNITS = HPARAM_CANDIDATES([\n",
        "    STRUCT([16, 8, 4, 8, 16]),\n",
        "    STRUCT([32, 16, 4, 16, 32])\n",
        "  ]),\n",
        "TF_VERSION = '2.8.0',\n",
        "EARLY_STOP = TRUE,\n",
        "MIN_REL_PROGRESS = 0.01,\n",
        "MAX_ITERATIONS=20,\n",
        "WARM_START = TRUE,\n",
        "NUM_TRIALS = 60,\n",
        "MAX_PARALLEL_TRIALS = 1,\n",
        "HPARAM_TUNING_ALGORITHM =  'VIZIER_DEFAULT',\n",
        "HPARAM_TUNING_OBJECTIVES = ['MEAN_SQUARED_ERROR']\n",
        ") AS\n",
        "SELECT\n",
        "*\n",
        "FROM `{0}.{2}`;\"\"\".format(\n",
        "    BQ_DATASET_NAME, AUTO_ENCODER_MODEL, BQ_TABLE_NAME\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nds2Sm-QUrgv"
      },
      "outputs": [],
      "source": [
        "# Execute the query\n",
        "execute_sql(train_auto_encoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d596XDbHU9dM"
      },
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0x_U039VA3c"
      },
      "outputs": [],
      "source": [
        "# Create query to detect anomalies\n",
        "eval_auto_encoder = \"\"\"SELECT * FROM ML.EVALUATE(MODEL `{}.{}`);\"\"\".format(\n",
        "    BQ_DATASET_NAME, AUTO_ENCODER_MODEL\n",
        ")\n",
        "# Run the query and fetch the outliers\n",
        "model_evalution = execute_sql(eval_auto_encoder)\n",
        "\n",
        "model_evalution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clGMuJcLVIFO"
      },
      "source": [
        "### Outlier Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eb306cef63c"
      },
      "source": [
        "Here’s what you get from the `Autoencoder model` output:\n",
        "\n",
        "* `is_anomaly`: a BOOL value that indicates whether the value is anomalous.\n",
        "* `mean_squared_error`: a FLOAT64 value that contains the mean squared error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "058373249b1f"
      },
      "source": [
        "Here’s how to use `ML.DETECT_ANOMALIES` with the following arguments:\n",
        "\n",
        "* `project_id`: Your project ID.\n",
        "* `dataset`: The BigQuery dataset that contains the model.\n",
        "* `model`: The name of the model.\n",
        "* `table`: The name of the table to use to perform anomaly detection.\n",
        "* `query_statement`: The GoogleSQL query that generates the data to use to perform anomaly detection.\n",
        "* `contamination`: a FLOAT64 value that identifies the proportion of anomalies in the training dataset that are used to create the autoencoder, k-means, or PCA input models. The value must be in the range [0, 0.5]. For example, contamination value of 0.1 means that 10% of the training data that was used to create the input model is anomalous. The contamination value determines the cutoff threshold of the target metric to become anomalous, and any input data with a target metric greater than the cutoff threshold is identified as anomalous. The target metric is mean squared error for autoencoder and PCA models, and the target metric is normalized distance for k-means models.\n",
        "\n",
        "Learn more about [outlier analysis](https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-detect-anomalies)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fy9v8TQCVKZz"
      },
      "outputs": [],
      "source": [
        "# --- DETECT ANOMALIES --- #\n",
        "detect_anomaly_auto_encoder = \"\"\"SELECT * FROM ML.DETECT_ANOMALIES(MODEL `{0}.{1}.{2}`,\n",
        "STRUCT(0.001 AS contamination),\n",
        "TABLE `{0}.{1}.{3}`)\n",
        "WHERE is_anomaly=true order by mean_squared_error desc;\"\"\".format(\n",
        "    PROJECT_ID, BQ_DATASET_NAME, AUTO_ENCODER_MODEL, BQ_TABLE_NAME\n",
        ")\n",
        "# print(detect_anomaly_auto_encoder)\n",
        "autoencoder_outliers = execute_sql(detect_anomaly_auto_encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwmJ9unfXiT8"
      },
      "outputs": [],
      "source": [
        "# Show the outliers\n",
        "autoencoder_outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM6bvT03YM8o"
      },
      "source": [
        "## Common Outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNiCl-VHWpUJ"
      },
      "source": [
        "You compare the anomalies detected by the autoencoder model with those detected by the k-means model to find common outliers. First, you extract the relevant columns from both the k-means and autoencoder results. Then, you perform an inner join to find the rows that are present in both dataframes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ6uKD4BvMAi"
      },
      "outputs": [],
      "source": [
        "df1 = kmeans_outliers[\n",
        "    [\n",
        "        \"day\",\n",
        "        \"principal_email\",\n",
        "        \"action\",\n",
        "        \"resource_type\",\n",
        "        \"resource_id\",\n",
        "        \"container_type\",\n",
        "        \"container_id\",\n",
        "        \"channel\",\n",
        "        \"ip\",\n",
        "        \"counter\",\n",
        "    ]\n",
        "]\n",
        "df2 = autoencoder_outliers[\n",
        "    [\n",
        "        \"day\",\n",
        "        \"principal_email\",\n",
        "        \"action\",\n",
        "        \"resource_type\",\n",
        "        \"resource_id\",\n",
        "        \"container_type\",\n",
        "        \"container_id\",\n",
        "        \"channel\",\n",
        "        \"ip\",\n",
        "        \"counter\",\n",
        "    ]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qR9WAT32XeH3"
      },
      "outputs": [],
      "source": [
        "common_outliers = df1.merge(\n",
        "    df2,\n",
        "    how=\"inner\",\n",
        "    on=[\n",
        "        \"day\",\n",
        "        \"principal_email\",\n",
        "        \"action\",\n",
        "        \"resource_type\",\n",
        "        \"resource_id\",\n",
        "        \"container_type\",\n",
        "        \"container_id\",\n",
        "        \"channel\",\n",
        "        \"ip\",\n",
        "        \"counter\",\n",
        "    ],\n",
        ")  # Replace 'column_name' if necessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kRTiFEyXtFg"
      },
      "outputs": [],
      "source": [
        "common_outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qgBA3HYVViz"
      },
      "outputs": [],
      "source": [
        "common_outliers.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bscC9pxUBU8z"
      },
      "source": [
        "## Uploading detected outliers to BQ table for further analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YP8ptrh2Nj0E"
      },
      "source": [
        "Create a new table in BigQuery to store the common outliers and then upload the dataframe to this table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qT5Ck3XNryf"
      },
      "outputs": [],
      "source": [
        "OUTLIERS_TABLE = \"[your-common-outliers-table]\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-BfJ8Ew0tex"
      },
      "outputs": [],
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "\n",
        "def create_table(client, table_id, schema):\n",
        "    table = bigquery.Table(table_id, schema=schema)\n",
        "    table = client.create_table(table, exists_ok=True)  # Make an API request\n",
        "    print(\n",
        "        \"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id)\n",
        "    )\n",
        "\n",
        "\n",
        "def upload_df_into_bq(client, table_id, df):\n",
        "    job_config = bigquery.LoadJobConfig(schema=schema)\n",
        "    job_config.write_disposition = (\n",
        "        bigquery.WriteDisposition.WRITE_TRUNCATE\n",
        "    )  # If the table already exists, BigQuery overwrites the data, removes the constraints and uses the schema from the load job.\n",
        "    job_config.autodetect = False\n",
        "    job = client.load_table_from_dataframe(df, table_id, job_config=job_config)\n",
        "    job.result()\n",
        "    print(\"Uploaded dataframe into table {}.{}\".format(PROJECT_ID, table_id))\n",
        "\n",
        "\n",
        "schema = [\n",
        "    bigquery.SchemaField(\"day\", \"DATE\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"principal_email\", \"STRING\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"action\", \"STRING\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"resource_type\", \"STRING\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"resource_id\", \"STRING\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"container_type\", \"STRING\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"container_id\", \"STRING\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"channel\", \"STRING\", mode=\"NULLABLE\"),\n",
        "    bigquery.SchemaField(\"ip\", \"STRING\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"counter\", \"INTEGER\", mode=\"REQUIRED\"),\n",
        "]\n",
        "client = bigquery.Client(PROJECT_ID)\n",
        "\n",
        "table_id = \"{}.{}.{}\".format(PROJECT_ID, BQ_DATASET_NAME, OUTLIERS_TABLE)\n",
        "\n",
        "create_table(client, table_id, schema)\n",
        "\n",
        "upload_df_into_bq(client, table_id, common_outliers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx_vKniMq9ZX"
      },
      "outputs": [],
      "source": [
        "# Delete the BigQuery dataset (including the models created & the tables)\n",
        "!bq rm -r -f {PROJECT_ID}:{BQ_DATASET_NAME}\n",
        "\n",
        "# Delete the bigquery dataset linked to logs\n",
        "! gcloud logging links delete {logSourceBqDataset} --bucket={bucket_id} --location={bucket_location}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Anomaly_detection_in_Cloud_Audit_logs_with_BQML.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

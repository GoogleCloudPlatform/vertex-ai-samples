{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Using Vertex AI Vector Search for Text-to-Image Embeddings\n",
        "![ ](https://www.google-analytics.com/collect?v=2&tid=G-L6X3ECH596&cid=1&en=page_view&sid=1&dt=sdk_matching_engine_create_text_to_image_embeddings.ipynb&dl=notebooks%2Fofficial%2Fmatching_engine%2Fsdk_matching_engine_create_text_to_image_embeddings.ipynb)\n",
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/matching_engine/sdk_matching_engine_create_text_to_image_embeddings.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/matching_engine/sdk_matching_engine_create_text_to_image_embeddings.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "      <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/matching_engine/sdk_matching_engine_create_text_to_image_embeddings.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0a74aaf1481"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This example demonstrates how to create text-to-image embeddings using the DiffusionDB dataset and the CLIP model. These are uploaded to the Vertex AI Vector Search service. It is a high scale, low latency solution to find similar vectors for a large corpus. Moreover, it is a fully managed offering, further reducing operational overhead. It is built upon [Approximate Nearest Neighbor (ANN) technology](https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html) developed by Google Research.\n",
        "\n",
        "**Pre-requisite**: This notebook requires you to already have a VPC network set up. See the \"Prepare a VPC network\" section in [Create Vertex AI Vector Search index notebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/matching_engine/sdk_matching_engine_for_indexing.ipynb).\n",
        "\n",
        "Learn more about [Vertex AI Vector Search](https://cloud.google.com/vertex-ai/docs/matching-engine/overview)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34a4b245e795"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this notebook, you learn how to encode custom text embeddings, create an  Approximate Nearest Neighbor (ANN) index, and query against indexes.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services:\n",
        "\n",
        "- `Vertex AI Vector Search`\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "* Create ANN index\n",
        "* Create an index endpoint with VPC Network\n",
        "* Deploy ANN index\n",
        "* Perform online query\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset used for this tutorial is the [DiffusionDB dataset](https://github.com/poloclub/diffusiondb).\n",
        "\n",
        "> DiffusionDB is the first large-scale text-to-image prompt dataset. It contains 14 million images generated by Stable Diffusion using prompts and hyperparameters specified by real users. The unprecedented scale and diversity of this human-actuated dataset provide exciting research opportunities in understanding the interplay between prompts and generative models, detecting deepfakes, and designing human-AI interaction tools to help users more easily use these models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0f1bea346db"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Install the latest version of Cloud Storage, BigQuery and the Vertex AI SDK for Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfbccc635a17"
      },
      "outputs": [],
      "source": [
        "# Install the packages\n",
        "! pip3 install --upgrade google-cloud-aiplatform \\\n",
        "                         google-cloud-storage --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4967f0d712e"
      },
      "source": [
        "Install the latest version of transformers and torch libraries for encoding text and image embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f455fbba6a33"
      },
      "outputs": [],
      "source": [
        "# Install the packages\n",
        "! pip3 install --upgrade transformers torch --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97473593f37f"
      },
      "source": [
        "Install the latest version of google-cloud-vision for filtering for safe images and pyrate_limiter for limiting Google Cloud Vision API calls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5709d1d57927"
      },
      "outputs": [],
      "source": [
        "# Install the packages\n",
        "! pip install google-cloud-vision pyrate_limiter==2.10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b08ba354c6e"
      },
      "source": [
        "### Colab only: Uncomment the following cell to restart the kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bea801acf6b5"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "# import IPython\n",
        "\n",
        "# app = IPython.Application.instance()\n",
        "# app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd28c9e4f067"
      },
      "source": [
        "## Before you begin\n",
        "#### Set your project ID\n",
        "\n",
        "If you don't know your project ID, try the following:\n",
        "* Run `gcloud config list`.\n",
        "* Run `gcloud projects list`.\n",
        "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80c0215f05a0"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f4512bf63b3"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "474be5183c27"
      },
      "outputs": [],
      "source": [
        "REGION = \"us-central1\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "949271bfebe3"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b65b4ce80d9a"
      },
      "source": [
        "**1. Vertex AI Workbench**\n",
        "* Do nothing as you are already authenticated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "985cdbfe7372"
      },
      "source": [
        "**2. Local JupyterLab instance, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbc9cd30cc4b"
      },
      "outputs": [],
      "source": [
        "# ! gcloud auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79efab26ad02"
      },
      "source": [
        "**3. Colab, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a336a05c6149"
      },
      "outputs": [],
      "source": [
        "# from google.colab import auth\n",
        "# auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c0a44fa330f"
      },
      "source": [
        "**4. Service account or other**\n",
        "* See how to grant Cloud Storage permissions to your service account at https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3uj8x73nDX_"
      },
      "source": [
        "* Authentication: Rerun the `gcloud auth login` command in the Vertex AI Workbench notebook terminal when you are logged out and need the credential again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhq5zEbGg0XX"
      },
      "source": [
        "### Colab only: Uncomment the following cell to restart the kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzrelQZ22IZj"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "# import IPython\n",
        "\n",
        "# app = IPython.Application.instance()\n",
        "# app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgPO1eR3CYjk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "Create a storage bucket to store intermediate artifacts such as datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzGDU7TWdts_"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EcIXiGsCePi"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIq7R4HZCfIc"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR6Wwv-hCCN-"
      },
      "source": [
        "## Prepare the data\n",
        "\n",
        "You will use [DiffusionDB dataset](https://github.com/poloclub/diffusiondb) of image prompt and image pairs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e2a05095082"
      },
      "source": [
        "### Clone the DiffusionDB repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wzS85TeB9dG"
      },
      "outputs": [],
      "source": [
        "! git clone https://github.com/poloclub/diffusiondb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49ee8d2c2df7"
      },
      "source": [
        "### Install the dependencies for downloading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed67efa3c0f3"
      },
      "outputs": [],
      "source": [
        "! pip install -r diffusiondb/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0bc039db70e"
      },
      "source": [
        "### Download image files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d27d986c61f"
      },
      "outputs": [],
      "source": [
        "# Download image files from 1 to 5. Each file is 1000 images.\n",
        "! python diffusiondb/scripts/download.py -i 1 -r 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a514786370e5"
      },
      "source": [
        "### Extract image archives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b43937b6065d"
      },
      "outputs": [],
      "source": [
        "# Unzip all image files\n",
        "image_directory = \"extracted\"\n",
        "\n",
        "! unzip -n 'images/*.zip' -d '{image_directory}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09a6602bb745"
      },
      "source": [
        "### Load image metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cacd9869ee5"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "metadatas = {}\n",
        "for file_name in os.listdir(image_directory):\n",
        "    if file_name.endswith(\".json\"):\n",
        "        with open(os.path.join(image_directory, file_name)) as f:\n",
        "            metadata = json.load(f)\n",
        "            metadatas.update(metadata)\n",
        "\n",
        "image_names = list(metadatas.keys())\n",
        "image_paths = [os.path.join(image_directory, image_name) for image_name in image_names]\n",
        "\n",
        "len(metadatas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a593ec69048"
      },
      "source": [
        "### Define function to detect explicit images\n",
        "\n",
        "Define a function to query the Cloud Vision API to detect potential explicit images.\n",
        "\n",
        "Learn more about [Detect explicit content](https://cloud.google.com/vision/docs/detecting-safe-search)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce31cc893826"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "\n",
        "from google.cloud import vision\n",
        "\n",
        "client = vision.ImageAnnotatorClient()\n",
        "\n",
        "\n",
        "def detect_safe_search(path: str) -> Optional[bool]:\n",
        "    \"\"\"Detects unsafe features in the file.\"\"\"\n",
        "    import io\n",
        "\n",
        "    image_file = io.open(path, \"rb\")\n",
        "    content = image_file.read()\n",
        "    image_file.close()\n",
        "\n",
        "    image = vision.Image(content=content)\n",
        "\n",
        "    response = client.safe_search_detection(image=image)\n",
        "\n",
        "    if response.error.message:\n",
        "        print(response.error.message)\n",
        "        return None\n",
        "\n",
        "    return response.safe_search_annotation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e330a70627ed"
      },
      "source": [
        "### Define safe search annotation conversion to boolean values\n",
        "Define a function to convert safe search annotation results to a boolean value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc3c32afb383"
      },
      "outputs": [],
      "source": [
        "from google.cloud.vision_v1.types.image_annotator import (Likelihood,\n",
        "                                                          SafeSearchAnnotation)\n",
        "\n",
        "\n",
        "# Returns true if some annotations have a potential safety issues\n",
        "def convert_annotation_to_safety(safe_search_annotation: SafeSearchAnnotation) -> bool:\n",
        "    return all(\n",
        "        [\n",
        "            (safe_level == Likelihood.VERY_UNLIKELY)\n",
        "            or (safe_level == Likelihood.UNLIKELY)\n",
        "            for safe_level in [\n",
        "                safe_search_annotation.adult,\n",
        "                safe_search_annotation.medical,\n",
        "                safe_search_annotation.violence,\n",
        "                safe_search_annotation.racy,\n",
        "            ]\n",
        "        ]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aded0a519b0c"
      },
      "source": [
        "### Perform rate-limited explicit image detection\n",
        "\n",
        "Google Cloud Vision has a rate limit for API requests.\n",
        "\n",
        "Use a rate limiter to ensure the requests go under this limit.\n",
        "For better performance, use a ThreadPool to make parallel requests. This is out-of-scope for this notebook.\n",
        "\n",
        "Learn more about [Quotas and Limits](https://cloud.google.com/vision/quotas?hl=en)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc753c9264a9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from pyrate_limiter import Duration, Limiter, RequestRate\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Create a rate limiter with a limit of 1800 requests per second\n",
        "limiter = Limiter(RequestRate(1800, Duration.MINUTE))\n",
        "\n",
        "safe_search_annotations = []\n",
        "for image_path in tqdm(image_paths, total=len(image_paths)):\n",
        "    limiter.try_acquire()\n",
        "    safe_search_annotations.append(detect_safe_search(image_path))\n",
        "\n",
        "# Convert annotations to boolean values\n",
        "is_safe_values_cloud_vision = list(\n",
        "    map(convert_annotation_to_safety, safe_search_annotations)\n",
        ")\n",
        "\n",
        "# Print number of safe images found\n",
        "print(\n",
        "    f\"Safe images = {np.array(is_safe_values_cloud_vision).sum()} out of {len(is_safe_values_cloud_vision)} images\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00c8234f9828"
      },
      "outputs": [],
      "source": [
        "# Filter images by safety\n",
        "metadatas = [\n",
        "    metadata\n",
        "    for metadata, is_safe in zip(metadatas, is_safe_values_cloud_vision)\n",
        "    if is_safe\n",
        "]\n",
        "image_names = [\n",
        "    image_name\n",
        "    for image_name, is_safe in zip(image_names, is_safe_values_cloud_vision)\n",
        "    if is_safe\n",
        "]\n",
        "image_paths = [\n",
        "    image_path\n",
        "    for image_path, is_safe in zip(image_paths, is_safe_values_cloud_vision)\n",
        "    if is_safe\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1124422cc200"
      },
      "source": [
        "#### Instantiate the text-to-image encoding model\n",
        "\n",
        "Use the [clip-vit-base-patch32 encoder](https://huggingface.co/openai/clip-vit-base-patch32) developed by OpenAI for converting text and images to embeddings.\n",
        "\n",
        "> The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed41c7712930"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import CLIPModel, CLIPProcessor, CLIPTokenizerFast\n",
        "\n",
        "MODEL_ID = \"openai/clip-vit-base-patch32\"\n",
        "\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        ")\n",
        "\n",
        "tokenizer = CLIPTokenizerFast.from_pretrained(MODEL_ID)\n",
        "processor = CLIPProcessor.from_pretrained(MODEL_ID)\n",
        "model = CLIPModel.from_pretrained(MODEL_ID).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43088937e820"
      },
      "source": [
        "#### Defining encoding functions\n",
        "\n",
        "Define functions to be used later that will take text and images and convert them to embeddings.\n",
        "\n",
        "See more info here: https://huggingface.co/openai/clip-vit-base-patch32#use-with-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0370bd840d2"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "def encode_text_to_embedding(model, tokenizer, text: List[str]) -> np.ndarray:\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    embeddings = model.get_text_features(**inputs)\n",
        "    return embeddings.cpu().detach().numpy()\n",
        "\n",
        "\n",
        "def encode_images_to_embedding(model, device, image_paths: List[str]) -> np.ndarray:\n",
        "    images = [copy.deepcopy(Image.open(path)) for path in image_paths]\n",
        "    image_pixel_values = processor(\n",
        "        text=None, images=images, return_tensors=\"pt\", padding=True\n",
        "    )[\"pixel_values\"].to(device)\n",
        "    embeddings = model.get_image_features(pixel_values=image_pixel_values)\n",
        "    return embeddings.squeeze(0).cpu().detach().numpy()\n",
        "\n",
        "\n",
        "def encode_images_to_embedding_chunked(\n",
        "    model, device, image_paths: List[str], batch_size: int = 16\n",
        ") -> np.ndarray:\n",
        "    embeddings_list = []\n",
        "\n",
        "    # Process images in batches to prevent out-of-memory errors.\n",
        "    for i in tqdm(range(0, len(image_paths), batch_size)):\n",
        "        embeddings_list.append(\n",
        "            encode_images_to_embedding(\n",
        "                model=model, device=device, image_paths=image_paths[i : i + batch_size]\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return np.vstack(embeddings_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba45d58bf96e"
      },
      "source": [
        "#### Test the encoding function\n",
        "\n",
        "Encode a subset of data and see if the embeddings and distance metrics make sense.\n",
        "\n",
        "According to the [CLIP research paper](https://arxiv.org/pdf/2103.00020.pdf), the similarities of embeddings are calculated using the cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9b01baa906b5"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# Encode 1000 images\n",
        "image_paths_filtered = random.sample(image_paths, 1000)\n",
        "image_embeddings = encode_images_to_embedding_chunked(\n",
        "    model=model, device=device, image_paths=image_paths_filtered\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9de878127530"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def cosine_similarity(\n",
        "    text_embedding: np.ndarray, image_embeddings: np.ndarray\n",
        ") -> np.ndarray:\n",
        "    # compute cosine similarity between text and image embeddings by taking the dot product normalized by the product of the L2 norms\n",
        "    return np.divide(\n",
        "        np.dot(text_embedding, image_embeddings.T),\n",
        "        (\n",
        "            np.linalg.norm(text_embedding)\n",
        "            * np.linalg.norm(image_embeddings, axis=1, keepdims=True)\n",
        "        ).squeeze(),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95e408daf219"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text_query = \"Birds in flight\"\n",
        "\n",
        "# Calculate text embedding of query\n",
        "text_embedding = encode_text_to_embedding(\n",
        "    model=model, tokenizer=tokenizer, text=[text_query]\n",
        ")[0]\n",
        "\n",
        "# Calculate cosine similarity\n",
        "scores = cosine_similarity(\n",
        "    text_embedding=text_embedding, image_embeddings=image_embeddings\n",
        ")\n",
        "\n",
        "# Set the maximum number of images to display\n",
        "MAX_IMAGES = 20\n",
        "\n",
        "# Sort images and scores by descending order of scores and select the top max_images\n",
        "sorted_data = sorted(\n",
        "    zip(image_paths_filtered, scores), key=lambda x: x[1], reverse=True\n",
        ")[:MAX_IMAGES]\n",
        "\n",
        "# Calculate the number of rows and columns needed to display the images\n",
        "num_cols = 4\n",
        "num_rows = math.ceil(len(sorted_data) / num_cols)\n",
        "\n",
        "\n",
        "# Create a grid of subplots to display the images\n",
        "fig, axs = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(10, 12))\n",
        "\n",
        "# Loop through the top max_images images and display them in the subplots\n",
        "for i, (image_path, score) in enumerate(sorted_data):\n",
        "    # Calculate the row and column index for the current image\n",
        "    row_idx = i // num_cols\n",
        "    col_idx = i % num_cols\n",
        "\n",
        "    # Display the image in the current subplot\n",
        "    image = copy.deepcopy(Image.open(image_path))\n",
        "    axs[row_idx, col_idx].imshow(image, cmap=\"gray\")\n",
        "\n",
        "    # Set the title of the subplot to the image index and score\n",
        "    axs[row_idx, col_idx].set_title(f\"Rank {i+1}, Score = {score:.2f}\")\n",
        "\n",
        "    # Remove ticks from the subplot\n",
        "    axs[row_idx, col_idx].set_xticks([])\n",
        "    axs[row_idx, col_idx].set_yticks([])\n",
        "\n",
        "# Adjust the spacing between subplots and display the plot\n",
        "plt.subplots_adjust(hspace=0.3, wspace=0.1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17f24794c9f0"
      },
      "source": [
        "Save the dimension size for later usage when creating the index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55a128883028"
      },
      "outputs": [],
      "source": [
        "DIMENSIONS = len(text_embedding)\n",
        "\n",
        "DIMENSIONS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQIQSyF9GtSv"
      },
      "source": [
        "#### Save the train split in JSONL format.\n",
        "\n",
        "The data must be formatted in JSONL format, which means each embedding dictionary is written as a JSON string on its own line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43aaff23416e"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "\n",
        "# Create temporary file to write embeddings to\n",
        "embeddings_file = tempfile.NamedTemporaryFile(suffix=\".json\", delete=False)\n",
        "\n",
        "embeddings_file.name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "307f468a3ecd"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "BATCH_SIZE = 1000\n",
        "\n",
        "with open(embeddings_file.name, \"a\") as f:\n",
        "    for i in tqdm(range(0, len(image_names), BATCH_SIZE)):\n",
        "        image_names_chunk = image_names[i : i + BATCH_SIZE]\n",
        "        image_paths_chunk = image_paths[i : i + BATCH_SIZE]\n",
        "\n",
        "        embeddings = encode_images_to_embedding_chunked(\n",
        "            model=model, device=device, image_paths=image_paths_chunk\n",
        "        )\n",
        "\n",
        "        # Append to file\n",
        "        embeddings_formatted = [\n",
        "            json.dumps(\n",
        "                {\n",
        "                    \"id\": str(id),\n",
        "                    \"embedding\": [str(value) for value in embedding],\n",
        "                }\n",
        "            )\n",
        "            + \"\\n\"\n",
        "            for id, embedding in zip(image_names_chunk, embeddings)\n",
        "        ]\n",
        "        f.writelines(embeddings_formatted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuVl8DrWG8NS"
      },
      "source": [
        "Upload the training data to GCS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PgsA_vbI8Vg"
      },
      "outputs": [],
      "source": [
        "UNIQUE_FOLDER_NAME = \"embeddings_folder_unique\"\n",
        "EMBEDDINGS_INITIAL_URI = f\"{BUCKET_URI}/{UNIQUE_FOLDER_NAME}/\"\n",
        "! gsutil cp {embeddings_file.name} {EMBEDDINGS_INITIAL_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mglUPwHpJH98"
      },
      "source": [
        "## Create Indexes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhIBCQ7dDSbW"
      },
      "source": [
        "### Create ANN Index (for Production Usage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiIg9b5zJLi1"
      },
      "outputs": [],
      "source": [
        "DISPLAY_NAME = \"text_to_image\"\n",
        "DESCRIPTION = \"CLIP text_to_image embeddings\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svLYiDf0OD2G"
      },
      "source": [
        "Create the ANN index configuration:\n",
        "\n",
        "To learn more about configuring the index, see [Input data format and structure](https://cloud.google.com/vertex-ai/docs/matching-engine/match-eng-setup#input-data-format).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4zooldkGoM4"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzY7TpUSJcTV"
      },
      "outputs": [],
      "source": [
        "tree_ah_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
        "    display_name=DISPLAY_NAME,\n",
        "    contents_delta_uri=EMBEDDINGS_INITIAL_URI,\n",
        "    dimensions=DIMENSIONS,\n",
        "    approximate_neighbors_count=150,\n",
        "    distance_measure_type=\"COSINE_DISTANCE\",\n",
        "    leaf_node_embedding_count=500,\n",
        "    leaf_nodes_to_search_percent=7,\n",
        "    description=DESCRIPTION,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17jrQi501QyX"
      },
      "outputs": [],
      "source": [
        "INDEX_RESOURCE_NAME = tree_ah_index.resource_name\n",
        "INDEX_RESOURCE_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f1a9fbecabb"
      },
      "source": [
        "Using the resource name, you can retrieve an existing MatchingEngineIndex."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ddb70647d98"
      },
      "outputs": [],
      "source": [
        "tree_ah_index = aiplatform.MatchingEngineIndex(index_name=INDEX_RESOURCE_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV2xjAnDDObD"
      },
      "source": [
        "## Create an IndexEndpoint with VPC Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpZQoJyxDlbO"
      },
      "outputs": [],
      "source": [
        "# Retrieve the project number\n",
        "PROJECT_NUMBER = !gcloud projects list --filter=\"PROJECT_ID:'{PROJECT_ID}'\" --format='value(PROJECT_NUMBER)'\n",
        "PROJECT_NUMBER = PROJECT_NUMBER[0]\n",
        "\n",
        "VPC_NETWORK = \"[YOUR-VPC-NETWORK]\"\n",
        "VPC_NETWORK_FULL = \"projects/{}/global/networks/{}\".format(PROJECT_NUMBER, VPC_NETWORK)\n",
        "VPC_NETWORK_FULL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuARXzJVGyQX"
      },
      "outputs": [],
      "source": [
        "my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
        "    display_name=DISPLAY_NAME,\n",
        "    description=DISPLAY_NAME,\n",
        "    network=VPC_NETWORK_FULL,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np2cgVuuIe9k"
      },
      "source": [
        "## Deploy Indexes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ew1UgcIIiJG"
      },
      "source": [
        "### Deploy ANN Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLOYTGygIlMK"
      },
      "outputs": [],
      "source": [
        "DEPLOYED_INDEX_ID = \"deployed_index_id_unique\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uK4WOgqN1NG"
      },
      "outputs": [],
      "source": [
        "my_index_endpoint = my_index_endpoint.deploy_index(\n",
        "    index=tree_ah_index, deployed_index_id=DEPLOYED_INDEX_ID\n",
        ")\n",
        "\n",
        "my_index_endpoint.deployed_indexes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LCGvBNvBd8D"
      },
      "source": [
        "## Create Online Queries\n",
        "\n",
        "After you built your indexes, you may query against the deployed index to find nearest neighbors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48e4e417a496"
      },
      "outputs": [],
      "source": [
        "# Encode query\n",
        "text_embeddings = encode_text_to_embedding(\n",
        "    model=model, tokenizer=tokenizer, text=[\"New York skyline\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3KYVw5HB-4v"
      },
      "outputs": [],
      "source": [
        "# Define number of neighbors to return\n",
        "NUM_NEIGHBORS = 20\n",
        "\n",
        "response = my_index_endpoint.match(\n",
        "    deployed_index_id=DEPLOYED_INDEX_ID,\n",
        "    queries=text_embeddings,\n",
        "    num_neighbors=NUM_NEIGHBORS,\n",
        ")\n",
        "\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1116c7c0d7d"
      },
      "source": [
        "Plot the response and verify that images match the text query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6f4ddb7c8d54"
      },
      "outputs": [],
      "source": [
        "# Sort images and scores by descending order of scores and select the top max_images\n",
        "sorted_data = sorted(response[0], key=lambda x: x.distance, reverse=True)\n",
        "\n",
        "# Create a grid of subplots to display the images\n",
        "fig, axs = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(10, 12))\n",
        "\n",
        "# Loop through the top max_images images and display them in the subplots\n",
        "for i, response in enumerate(sorted_data):\n",
        "    image_path = f\"{image_directory}/{response.id}\"\n",
        "    score = response.distance\n",
        "\n",
        "    # Calculate the row and column index for the current image\n",
        "    row_idx = i // num_cols\n",
        "    col_idx = i % num_cols\n",
        "\n",
        "    # Display the image in the current subplot\n",
        "    if os.path.exists(image_path):\n",
        "        image = copy.deepcopy(Image.open(image_path))\n",
        "        axs[row_idx, col_idx].imshow(image, cmap=\"gray\")\n",
        "\n",
        "        # Set the title of the subplot to the image index and score\n",
        "        axs[row_idx, col_idx].set_title(f\"Rank {i+1}, Score = {score:.2f}\")\n",
        "\n",
        "        # Remove ticks from the subplot\n",
        "        axs[row_idx, col_idx].set_xticks([])\n",
        "        axs[row_idx, col_idx].set_yticks([])\n",
        "\n",
        "# Adjust the spacing between subplots and display the plot\n",
        "plt.subplots_adjust(hspace=0.3, wspace=0.1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "You can also manually delete resources that you created by running the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx_vKniMq9ZX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "delete_bucket = False\n",
        "\n",
        "# Force undeployment of indexes and delete endpoint\n",
        "my_index_endpoint.delete(force=True)\n",
        "\n",
        "# Delete indexes\n",
        "tree_ah_index.delete()\n",
        "\n",
        "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
        "    ! gsutil -m rm -r $BUCKET_URI"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "sdk_matching_engine_create_text_to_image_embeddings.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

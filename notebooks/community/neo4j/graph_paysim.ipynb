{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpj6twWatL1e"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/neo4j/graph_paysim.ipynb\" target=\"_blank\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/tree/master/notebooks/community/neo4j/graph_paysim.ipynb\" target=\"_blank\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/laeg/vertex-ai-samples/main/notebooks/community/neo4j/graph_paysim.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">Open in Vertex AI Workbench\n",
        "    </a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YasYy1BqRHY8"
      },
      "source": [
        "# Overview\n",
        "In this notebook, you will learn how to use Neo4j AuraDS to create graph features.  You'll then use those new features to solve a classification problem with Vertex AI.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze4-nDLfK4pw"
      },
      "source": [
        "## Dataset\n",
        "This notebook uses a version of the PaySim dataset that has been modified to work with Neo4j's graph database.  PaySim is a synthetic fraud dataset.  The goal is to identify whether or not a given transaction constitutes fraud.  The [original version of the dataset](https://github.com/EdgarLopezPhD/PaySim) has tabular data.\n",
        "\n",
        "Neo4j has worked on a modified version that generates a graph dataset [here](https://github.com/voutilad/PaySim).  We've pregenerated a copy of that dataset that you can grab [here](https://storage.googleapis.com/neo4j-datasets/paysim.dump).  You'll want to download that dataset and then upload it to Neo4j AuraDS.  AuraDS is a graph data science tool that is offered as a service on GCP.  Instructions on signing up and uploading the dataset are available [here](https://github.com/neo4j-partners/aurads-paysim)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pD0fZLPdsAYf"
      },
      "source": [
        "##Costs\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Cloud Storage\n",
        "* Vertex AI\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m51HUN1aHNid"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbIYWyMksbpC"
      },
      "source": [
        "## Set up your development environment\n",
        "We suggest you use Colab for this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLocKiyCwtR7"
      },
      "source": [
        "## Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "1. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
        "\n",
        "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
        "Cloud SDK uses the right project for all the commands in this notebook.\n",
        "\n",
        "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKipBL0kWY7w"
      },
      "source": [
        "## Install additional Packages\n",
        "First off, you'll also need to install a few packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwKogqD_He_e"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet --upgrade graphdatascience==1.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDipS8p-27qg"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet google-cloud-storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ix0KpBl-hnxF"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet google.cloud.aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBXAh7fVt9Ou"
      },
      "source": [
        "## (Colab only) Restart the kernel\n",
        "After you install the additional packages, you need to restart the notebook kernel so it can find the packages.  When you run this, you may get a notification that the kernel crashed.  You can disregard that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySSyV4T_3dQB"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ldFrUMIHVHP"
      },
      "source": [
        "# Working with Neo4j"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMwl92_1HoIl"
      },
      "source": [
        "## Define Neo4J related variables\n",
        "\n",
        "You'll need to enter the credentials from your AuraDS instance below.  You can get your credentials by following this [walkthrough](https://github.com/neo4j-partners/aurads-paysim).\n",
        "\n",
        "The \"DB_NAME\" is always neo4j for AuraDS.  It is different from the name you gave your database tenant in the AuraDS console."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96IVMcJeH3N4"
      },
      "outputs": [],
      "source": [
        "DB_URL = \"neo4j+s://XXXXX.databases.neo4j.io\"\n",
        "DB_USER = \"neo4j\"\n",
        "DB_PASS = \"YOUR PASSWORD\"\n",
        "DB_NAME = \"neo4j\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpNk1MvcWY7x"
      },
      "source": [
        "In this section we're going to connect to Neo4j and look around the database.  We're going to generate some new features in the dataset using Neo4j's Graph Data Science library.  Finally, we'll load the data into a Pandas dataframe so that it's all ready to put into GCP Feature Store."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJptLsHUHgCV"
      },
      "source": [
        "## Exploring the database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiFDi4uLWY7x"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from graphdatascience import GraphDataScience"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgEy4q7iWY7y"
      },
      "outputs": [],
      "source": [
        "# If you are connecting the client to an AuraDS instance, you can get the recommended non-default configuration settings of the Python Driver applied automatically. To achieve this, set the constructor argument aura_ds=True\n",
        "gds = GraphDataScience(DB_URL, auth=(DB_USER, DB_PASS), aura_ds=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f14915ddd1fb"
      },
      "outputs": [],
      "source": [
        "gds.set_database(DB_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBpL6dY3HEMD"
      },
      "source": [
        "Now, let's explore the data in the database a bit to understand what we have to work with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4a0_CqVWY7y"
      },
      "outputs": [],
      "source": [
        "# node labels\n",
        "result = gds.run_cypher(\n",
        "    \"\"\"\n",
        "CALL db.labels() YIELD label\n",
        "CALL apoc.cypher.run('MATCH (:`'+label+'`) RETURN count(*) as freq', {})\n",
        "YIELD value\n",
        "RETURN label, value.freq AS freq\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "display(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrETUiWdFDoy"
      },
      "outputs": [],
      "source": [
        "# relationship types\n",
        "result = gds.run_cypher(\n",
        "    \"\"\"\n",
        "CALL db.relationshipTypes() YIELD relationshipType as type\n",
        "CALL apoc.cypher.run('MATCH ()-[:`'+type+'`]->() RETURN count(*) as freq', {})\n",
        "YIELD value\n",
        "RETURN type AS relationshipType, value.freq AS freq\n",
        "ORDER by freq DESC\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "display(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lsfbg8rpJcXo"
      },
      "outputs": [],
      "source": [
        "# transaction types\n",
        "result = gds.run_cypher(\n",
        "    \"\"\"\n",
        "      MATCH (t:Transaction)\n",
        "      WITH sum(t.amount) AS globalSum, count(t) AS globalCnt\n",
        "      WITH *, 10^3 AS scaleFactor\n",
        "      UNWIND ['CashIn', 'CashOut', 'Payment', 'Debit', 'Transfer'] AS txType\n",
        "        CALL apoc.cypher.run('MATCH (t:' + txType + ')\n",
        "          RETURN sum(t.amount) as txAmount, count(t) AS txCnt', {})\n",
        "        YIELD value\n",
        "      RETURN txType,value.txAmount AS TotalMarketValue\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "display(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKo4m-A4J9F8"
      },
      "source": [
        "## Create a New Feature with a Graph Embedding using Neo4j\n",
        "First we're going to create an in memory graph represtation of the data in Neo4j Graph Data Science (GDS).\n",
        "\n",
        "Note, if you get an error saying the graph already exists, that's probably because you ran this code before.  You can destroy it using the command in the cleanup section of this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdBkeDV7J8Ke"
      },
      "outputs": [],
      "source": [
        "# We get a tuple back with an object that represents the graph projection and the results of the GDS call\n",
        "G, results = gds.graph.project.cypher(\n",
        "    \"client_graph\",\n",
        "    \"MATCH (c:Client) RETURN id(c) as id, c.num_transactions as num_transactions, c.total_transaction_amnt as total_transaction_amnt, c.is_fraudster as is_fraudster\",\n",
        "    'MATCH (c:Client)-[:PERFORMED]->(t:Transaction)-[:TO]->(c2:Client) return id(c) as source, id(c2) as target, sum(t.amount) as amount, \"TRANSACTED_WITH\" as type ',\n",
        ")\n",
        "\n",
        "display(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WewKw5g4NKVo"
      },
      "source": [
        "Now we can generate an embedding from that graph.  This is a new feature we can use in our predictions.  We're using FastRP, which is a more full featured and higher performance of Node2Vec.  You can learn more about that [here](https://neo4j.com/docs/graph-data-science/current/algorithms/fastrp/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBa8ofijEtHn"
      },
      "outputs": [],
      "source": [
        "results = gds.fastRP.mutate(\n",
        "    G,\n",
        "    relationshipWeightProperty=\"amount\",\n",
        "    iterationWeights=[0.0, 1.00, 1.00, 0.80, 0.60],\n",
        "    featureProperties=[\"num_transactions\", \"total_transaction_amnt\"],\n",
        "    propertyRatio=0.25,\n",
        "    nodeSelfInfluence=0.15,\n",
        "    embeddingDimension=16,\n",
        "    randomSeed=1,\n",
        "    mutateProperty=\"embedding\",\n",
        ")\n",
        "\n",
        "display(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PCI0yiUNpLZ"
      },
      "source": [
        "Finally we dump that out to a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkALAMl5NtDg"
      },
      "outputs": [],
      "source": [
        "node_properties = gds.graph.streamNodeProperties(\n",
        "    G, [\"embedding\", \"num_transactions\", \"total_transaction_amnt\", \"is_fraudster\"]\n",
        ")\n",
        "\n",
        "node_properties.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzDiV7Efv40X"
      },
      "source": [
        "Now we need to take that dataframe and shape it into something that better represents our classification problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkzFxCgdqeTt"
      },
      "outputs": [],
      "source": [
        "x = node_properties.pivot(\n",
        "    index=\"nodeId\", columns=\"nodeProperty\", values=\"propertyValue\"\n",
        ")\n",
        "x = x.reset_index()\n",
        "x.columns.name = None\n",
        "x.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPim4AGB8w3Q"
      },
      "source": [
        "is_fraudster will have a value of 0 or 1 if populated.  If the value is -9223372036854775808 then it's unlabeled, so we're going to drop it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jZZg6ln8wW_"
      },
      "outputs": [],
      "source": [
        "x = x.loc[x[\"is_fraudster\"] != -9223372036854775808]\n",
        "x.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOt_DjoPVirz"
      },
      "source": [
        "Note that the embedding row is an array.  To make this dataset more consumable, we should flatten that out into multiple individual features: embedding_0, embedding_1, ... embedding_n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9j0PPn9H4RD"
      },
      "outputs": [],
      "source": [
        "FEATURES_FILENAME = \"features.csv\"\n",
        "\n",
        "embeddings = pd.DataFrame(x[\"embedding\"].values.tolist()).add_prefix(\"embedding_\")\n",
        "merged = x.drop(columns=[\"embedding\"]).merge(\n",
        "    embeddings, left_index=True, right_index=True\n",
        ")\n",
        "features_df = merged.drop(\n",
        "    columns=[\"is_fraudster\", \"num_transactions\", \"total_transaction_amnt\"]\n",
        ")\n",
        "train_df = merged.drop(columns=[\"nodeId\"])\n",
        "\n",
        "features_df.to_csv(FEATURES_FILENAME, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWBOxHrusEXk"
      },
      "source": [
        "This dataset is too small to use with Vertex AI for AutoML tabular data. For sake of demonstration, we're going to repeat it a few times. Don't do this in the real world."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMTvbcsvdVfb"
      },
      "outputs": [],
      "source": [
        "TRAINING_FILENAME = \"train.csv\"\n",
        "\n",
        "pd.concat([train_df for i in range(10)]).to_csv(TRAINING_FILENAME, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpNFaHfKK6jK"
      },
      "source": [
        "And that's it!  The dataframe now has a nice dataset that we can use with GCP Vertex AI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTIKVdXJIOaF"
      },
      "source": [
        "# Using Vertex AI with Neo4j data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrJyXGJAHr_M"
      },
      "source": [
        "## Define Google Cloud variables\n",
        "You'll need to set a few variables for your GCP environment.  PROJECT_ID and STORAGE_BUCKET are most critical.  The others will probably work with the defaults given."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-FC4GI1H3jx"
      },
      "outputs": [],
      "source": [
        "# Edit these variables!\n",
        "PROJECT_ID = \"YOUR-PROJECT-ID\"\n",
        "STORAGE_BUCKET = \"YOUR-BUCKET-NAME\"\n",
        "\n",
        "# You can leave these defaults\n",
        "REGION = \"us-central1\"\n",
        "STORAGE_PATH = \"paysim\"\n",
        "EMBEDDING_DIMENSION = 16\n",
        "FEATURESTORE_ID = \"paysim\"\n",
        "ENTITY_NAME = \"payer\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XoT1nT_JlYx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"GCLOUD_PROJECT\"] = PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id6tjQDbgf2S"
      },
      "source": [
        "## Authenticate your Google Cloud account\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HucMnpmVgfmX"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from google.colab import auth as google_auth\n",
        "\n",
        "    google_auth.authenticate_user()\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUU7z4FjJS90"
      },
      "source": [
        "##Upload to a GCP Cloud Storage Bucket\n",
        "\n",
        "To get the data into Vertex AI, we must first put it in a bucket as a CSV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3nbLg1cKJpJ"
      },
      "outputs": [],
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "client = storage.Client()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dAkAU5ALnUo"
      },
      "outputs": [],
      "source": [
        "bucket = client.bucket(STORAGE_BUCKET)\n",
        "client.create_bucket(bucket)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTo7-_oJL_dZ"
      },
      "outputs": [],
      "source": [
        "# Upload our files to that bucket\n",
        "for filename in [FEATURES_FILENAME, TRAINING_FILENAME]:\n",
        "    upload_path = os.path.join(STORAGE_PATH, filename)\n",
        "    blob = bucket.blob(upload_path)\n",
        "    blob.upload_from_filename(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArK3cfKsdT1x"
      },
      "source": [
        "## Train and deploy a model with Vertex AI\n",
        "We'll use the engineered features to train an AutoML Tabular Data, then deploy it to an endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGjrD-k3dsCN"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "dataset = aiplatform.TabularDataset.create(\n",
        "    display_name=\"paysim\",\n",
        "    gcs_source=os.path.join(\"gs://\", STORAGE_BUCKET, STORAGE_PATH, TRAINING_FILENAME),\n",
        ")\n",
        "dataset.wait()\n",
        "\n",
        "print(f'\\tDataset: \"{dataset.display_name}\"')\n",
        "print(f'\\tname: \"{dataset.resource_name}\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaSPuk31N2xS"
      },
      "outputs": [],
      "source": [
        "embedding_column_names = [\"embedding_{}\".format(i) for i in range(EMBEDDING_DIMENSION)]\n",
        "other_column_names = [\"num_transactions\", \"total_transaction_amnt\"]\n",
        "all_columns = other_column_names + embedding_column_names\n",
        "column_specs = {column: \"numeric\" for column in all_columns}\n",
        "\n",
        "job = aiplatform.AutoMLTabularTrainingJob(\n",
        "    display_name=\"train-paysim-automl-1\",\n",
        "    optimization_prediction_type=\"classification\",\n",
        "    column_specs=column_specs,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fqf44y_G8vi1"
      },
      "outputs": [],
      "source": [
        "model = job.run(\n",
        "    dataset=dataset,\n",
        "    target_column=\"is_fraudster\",\n",
        "    training_fraction_split=0.8,\n",
        "    validation_fraction_split=0.1,\n",
        "    test_fraction_split=0.1,\n",
        "    model_display_name=\"paysim-prediction-model\",\n",
        "    disable_early_stopping=False,\n",
        "    budget_milli_node_hours=1000,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoVThi28VO_R"
      },
      "outputs": [],
      "source": [
        "endpoint = model.deploy(machine_type=\"n1-standard-4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NnDaATyWY7z"
      },
      "source": [
        "## Loading Data into Vertex AI Feature Store\n",
        "In this section, we'll take our dataframe with newly engineered features and load that into Vertex AI Feature Store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0DcYzPkRrzj"
      },
      "outputs": [],
      "source": [
        "from google.cloud.aiplatform_v1 import FeaturestoreServiceClient\n",
        "\n",
        "api_endpoint = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
        "fs_client = FeaturestoreServiceClient(client_options={\"api_endpoint\": api_endpoint})\n",
        "\n",
        "resource_path = fs_client.common_location_path(PROJECT_ID, REGION)\n",
        "fs_path = fs_client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID)\n",
        "entity_path = fs_client.entity_type_path(\n",
        "    PROJECT_ID, REGION, FEATURESTORE_ID, ENTITY_NAME\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMN4Ue2hjdL3"
      },
      "source": [
        "First, let's check if the Feature Store already exists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYQknjQFsVNC"
      },
      "outputs": [],
      "source": [
        "from grpc import StatusCode\n",
        "\n",
        "\n",
        "def check_has_resource(callable):\n",
        "    has_resource = False\n",
        "    try:\n",
        "        callable()\n",
        "        has_resource = True\n",
        "    except Exception as e:\n",
        "        if (\n",
        "            not hasattr(e, \"grpc_status_code\")\n",
        "            or e.grpc_status_code != StatusCode.NOT_FOUND\n",
        "        ):\n",
        "            raise e\n",
        "    return has_resource"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTVIsom6eejQ"
      },
      "outputs": [],
      "source": [
        "feature_store_exists = check_has_resource(\n",
        "    lambda: fs_client.get_featurestore(name=fs_path)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caTWbgeChd_x"
      },
      "outputs": [],
      "source": [
        "from google.cloud.aiplatform_v1.types import entity_type as entity_type_pb2\n",
        "from google.cloud.aiplatform_v1.types import feature as feature_pb2\n",
        "from google.cloud.aiplatform_v1.types import featurestore as featurestore_pb2\n",
        "from google.cloud.aiplatform_v1.types import \\\n",
        "    featurestore_service as featurestore_service_pb2\n",
        "from google.cloud.aiplatform_v1.types import io as io_pb2\n",
        "\n",
        "if not feature_store_exists:\n",
        "    create_lro = fs_client.create_featurestore(\n",
        "        featurestore_service_pb2.CreateFeaturestoreRequest(\n",
        "            parent=resource_path,\n",
        "            featurestore_id=FEATURESTORE_ID,\n",
        "            featurestore=featurestore_pb2.Featurestore(\n",
        "                online_serving_config=featurestore_pb2.Featurestore.OnlineServingConfig(\n",
        "                    fixed_node_count=1\n",
        "                ),\n",
        "            ),\n",
        "        )\n",
        "    )\n",
        "\n",
        "    print(create_lro.result())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1JRwvvYJMBy"
      },
      "outputs": [],
      "source": [
        "entity_type_exists = check_has_resource(\n",
        "    lambda: fs_client.get_entity_type(name=entity_path)\n",
        ")\n",
        "\n",
        "if not entity_type_exists:\n",
        "    users_entity_type_lro = fs_client.create_entity_type(\n",
        "        featurestore_service_pb2.CreateEntityTypeRequest(\n",
        "            parent=fs_path,\n",
        "            entity_type_id=ENTITY_NAME,\n",
        "            entity_type=entity_type_pb2.EntityType(\n",
        "                description=\"Main entity type\",\n",
        "            ),\n",
        "        )\n",
        "    )\n",
        "    print(users_entity_type_lro.result())\n",
        "\n",
        "    feature_requests = [\n",
        "        featurestore_service_pb2.CreateFeatureRequest(\n",
        "            feature=feature_pb2.Feature(\n",
        "                value_type=feature_pb2.Feature.ValueType.DOUBLE,\n",
        "                description=\"Embedding {} from Neo4j\".format(i),\n",
        "            ),\n",
        "            feature_id=\"embedding_{}\".format(i),\n",
        "        )\n",
        "        for i in range(EMBEDDING_DIMENSION)\n",
        "    ]\n",
        "    create_features_lro = fs_client.batch_create_features(\n",
        "        parent=entity_path,\n",
        "        requests=feature_requests,\n",
        "    )\n",
        "    print(create_features_lro.result())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uz78rmNrwK0V"
      },
      "outputs": [],
      "source": [
        "feature_specs = [\n",
        "    featurestore_service_pb2.ImportFeatureValuesRequest.FeatureSpec(\n",
        "        id=\"embedding_{}\".format(i)\n",
        "    )\n",
        "    for i in range(EMBEDDING_DIMENSION)\n",
        "]\n",
        "\n",
        "from google.protobuf.timestamp_pb2 import Timestamp\n",
        "\n",
        "feature_time = Timestamp()\n",
        "feature_time.GetCurrentTime()\n",
        "feature_time.nanos = 0\n",
        "\n",
        "import_request = fs_client.import_feature_values(\n",
        "    featurestore_service_pb2.ImportFeatureValuesRequest(\n",
        "        entity_type=entity_path,\n",
        "        csv_source=io_pb2.CsvSource(\n",
        "            gcs_source=io_pb2.GcsSource(\n",
        "                uris=[\n",
        "                    os.path.join(\n",
        "                        \"gs://\", STORAGE_BUCKET, STORAGE_PATH, FEATURES_FILENAME\n",
        "                    )\n",
        "                ]\n",
        "            )\n",
        "        ),\n",
        "        entity_id_field=\"nodeId\",\n",
        "        feature_specs=feature_specs,\n",
        "        worker_count=1,\n",
        "        feature_time=feature_time,\n",
        "    )\n",
        ")\n",
        "\n",
        "print(import_request.result())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOeih_WxWhSx"
      },
      "source": [
        "## Sending a prediction using features from the feature store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFr8zWyiWxOa"
      },
      "outputs": [],
      "source": [
        "from google.cloud.aiplatform_v1 import FeaturestoreOnlineServingServiceClient\n",
        "\n",
        "data_client = FeaturestoreOnlineServingServiceClient(\n",
        "    client_options={\"api_endpoint\": api_endpoint}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnVC3BHmWylQ"
      },
      "outputs": [],
      "source": [
        "# Retrieve Neo4j embeddings from feature store\n",
        "from google.cloud.aiplatform_v1.types import FeatureSelector, IdMatcher\n",
        "from google.cloud.aiplatform_v1.types import \\\n",
        "    featurestore_online_service as featurestore_online_service_pb2\n",
        "\n",
        "feature_selector = FeatureSelector(\n",
        "    id_matcher=IdMatcher(\n",
        "        ids=[\"embedding_{}\".format(i) for i in range(EMBEDDING_DIMENSION)]\n",
        "    )\n",
        ")\n",
        "\n",
        "fs_features = data_client.read_feature_values(\n",
        "    featurestore_online_service_pb2.ReadFeatureValuesRequest(\n",
        "        entity_type=entity_path,\n",
        "        entity_id=\"5\",\n",
        "        feature_selector=feature_selector,\n",
        "    )\n",
        ")\n",
        "\n",
        "saved_embeddings = dict(\n",
        "    zip(\n",
        "        (fd.id for fd in fs_features.header.feature_descriptors),\n",
        "        (str(d.value.double_value) for d in fs_features.entity_view.data),\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgW3Ks0SihdN"
      },
      "outputs": [],
      "source": [
        "# Combine with other features. These might be sourced per transaction\n",
        "all_features = {\"num_transactions\": \"80\", \"total_dollar_amnt\": \"7484459.618641878\"}\n",
        "\n",
        "all_features.update(saved_embeddings)\n",
        "\n",
        "instances = [{key: str(value) for key, value in all_features.items()}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnK_FJeIi--4"
      },
      "outputs": [],
      "source": [
        "# Send a prediction\n",
        "endpoint.predict(instances=instances)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DU79nGz2gv_M"
      },
      "source": [
        "# Cleanup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBqqQEW_Kggf"
      },
      "source": [
        "## Neo4j cleanup\n",
        "\n",
        "To delete the Graph Data Science representation of the graph, run this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICSNRLM5YQ5N"
      },
      "outputs": [],
      "source": [
        "gds.graph.drop(G)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAM7PXfmKikz"
      },
      "source": [
        "## Google Cloud cleanup\n",
        "\n",
        "Delete the feature store and turn down the endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Es9wPH3UVbP-"
      },
      "outputs": [],
      "source": [
        "fs_client.delete_featurestore(\n",
        "    request=featurestore_service_pb2.DeleteFeaturestoreRequest(\n",
        "        name=fs_client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID),\n",
        "        force=True,\n",
        "    )\n",
        ").result()\n",
        "\n",
        "endpoint.delete()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "graph_paysim.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63c7b05c4717"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/xqr-g/vertex-ai-samples/blob/main/notebooks/community/generative_ai/backoff_and_retry_for_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "670bbc2007a2"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "890ac0f4e121"
   },
   "source": [
    "# Backoff and retry for LLM\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/generative_ai/backoff_and_retry_for_LLMs.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fgenerative-ai%2Fbackoff_and_retry_for_LLMs.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>    \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/generative_ai/backoff_and_retry_for_LLMs.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/generative_ai/backoff_and_retry_for_LLMs.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81ce710a5836"
   },
   "source": [
    "NOTE: This notebook has been tested in the following environment:\n",
    "\n",
    "Python version = 3.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ff76ac47eb9b"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how sending large amounts of traffic to Gemini-1.5-Pro can cause \"429 Quota Exceeded Errors\" and how implementing a backoff-and-retry strategy can help complete jobs without interrupting operations.\n",
    "\n",
    "This notebook provides examples for the blog post: Don't let 429 errors leave your users hanging: A guide to handling resource exhaustion\n",
    "\n",
    "This tutorial uses the following Google Cloud ML service:\n",
    "\n",
    "- Vertex LLM SDK\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Installation and imports\n",
    "- Asynchronously calling the Gemini model\n",
    "- Using the Tenacity retry decorator to implement backoff and retry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4e3e949c0bdd"
   },
   "source": [
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing),\n",
    "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage.\n",
    "\n",
    "**This notebook sends large amount of tokens to Gemini for inference, reduce the number of attempts or use smaller video to reduce costs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0316df526f8"
   },
   "source": [
    "## Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FyyMdUeAJIVv"
   },
   "source": [
    "## Install Vertex AI SDK for Python and other required packages\n",
    "\n",
    "Install the following packages required to execute this notebook.\n",
    "\n",
    "**Remember to restart the runtime after installation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "snBUuUamoJPz"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet google-cloud-aiplatform tenacity google-cloud-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WX3CHZitmSJM"
   },
   "source": [
    "### Restart runtime (Colab only)\n",
    "To use the newly installed packages, you must restart the runtime on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f09b4dff629a"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbmM4z7FOBpM"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74ccc9e52986"
   },
   "source": [
    "**1. Vertex AI Workbench**\n",
    "* Do nothing as you are already authenticated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de775a3773ba"
   },
   "source": [
    "**2. Local JupyterLab instance, uncomment and run:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "254614fa0c46"
   },
   "outputs": [],
   "source": [
    "# ! gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef21552ccea8"
   },
   "source": [
    "**3. Colab, uncomment and run:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "603adbbf0532"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# if \"google.colab\" in sys.modules:\n",
    "\n",
    "#     from google.colab import auth\n",
    "\n",
    "#     auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dae340cb-0583-4e7e-a562-6817ee4d7f6d"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "412d00f1-08db-4880-8ced-52a9583757b8"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "\n",
    "import nest_asyncio\n",
    "import vertexai\n",
    "\n",
    "nest_asyncio.apply()\n",
    "from google.cloud import storage\n",
    "from tenacity import retry, wait_random_exponential\n",
    "from vertexai.generative_models import GenerationConfig, GenerativeModel, Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "### Set Google Cloud project information and initialize Vertex AI SDK for Python\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3EdtdqnoldX4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "DEFAUL_MODEL_NAME = \"gemini-1.5-pro-001\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "\n",
    "# Initiate Vertex AI\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "config = GenerationConfig(temperature=0.5, max_output_tokens=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f50f22f3-ec85-463e-b6fe-5c8e6b80b07b"
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "b18a366df00b"
   },
   "outputs": [],
   "source": [
    "def get_images_uri_from_bucket(bucket_name, prefix, delimiter=None):\n",
    "    \"\"\"Lists all the images with extension '.jpg', 'jpeg' or 'png' in the bucket that begin with the prefix (folder).\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    blobs = storage_client.list_blobs(bucket_name, prefix=prefix, delimiter=delimiter)\n",
    "    images = [\n",
    "        f\"gs://{bucket_name}/{blob.name}\"\n",
    "        for blob in blobs\n",
    "        if blob.name.endswith(tuple([\".jpg\", \"jpeg\", \"png\"]))\n",
    "    ]\n",
    "    return images\n",
    "\n",
    "\n",
    "async def async_ask_gemini(contents, model_name=DEFAUL_MODEL_NAME):\n",
    "    # This basic function calls Gemini asynchronously without a retry logic\n",
    "    multimodal_model = GenerativeModel(model_name)\n",
    "    response = await multimodal_model.generate_content_async(\n",
    "        contents=contents, generation_config=config\n",
    "    )\n",
    "    return response.text\n",
    "\n",
    "\n",
    "@retry(wait=wait_random_exponential(multiplier=1, max=60))\n",
    "async def retry_async_ask_gemini(contents, model_name=DEFAUL_MODEL_NAME):\n",
    "    \"\"\"This is the same code as the async_ask_gemini function but implements a retry logic using tenacity decorator.\n",
    "    wait_random_exponential(multiplier=1, max=60) means that it will\n",
    "    Retry “Randomly wait up to 2^x * 1 seconds between each retry until the range reaches 60 seconds, then randomly up to 60 seconds afterwards.\n",
    "    \"\"\"\n",
    "\n",
    "    multimodal_model = GenerativeModel(model_name)\n",
    "    response = await multimodal_model.generate_content_async(\n",
    "        contents=contents, generation_config=config\n",
    "    )\n",
    "    return response.text\n",
    "\n",
    "\n",
    "async def load_test_gemini(function, model_name, attempts=5):\n",
    "    failed_attempts = 0\n",
    "    print(f\"Testing with model: {model_name} and function: {function.__name__}\")\n",
    "    for i in range(attempts):\n",
    "        try:\n",
    "            time_start = time.time()\n",
    "            get_gemini_responses = [\n",
    "                function(\n",
    "                    [\n",
    "                        prompt,\n",
    "                        video_part,\n",
    "                        Part.from_uri(image_uri, mime_type=\"image/jpeg\"),\n",
    "                    ],\n",
    "                    model_name=MODEL_NAME,\n",
    "                )\n",
    "                for image_uri in images_list\n",
    "            ]\n",
    "            async_poems = await asyncio.gather(*get_gemini_responses)\n",
    "            time_taken = time.time() - time_start\n",
    "            print(f\"{len(async_poems)} Poems written in {time_taken:.0f} seconds\")\n",
    "        except Exception as error:\n",
    "            failed_attempts += 1\n",
    "            print(\"An error occurred:\", error)\n",
    "\n",
    "    print(\n",
    "        f\"{failed_attempts} out of {attempts} failed\"\n",
    "    ) if failed_attempts > 0 else print(f\"All {attempts} attempts succeded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c01755042c4b"
   },
   "source": [
    "### Getting images and videos used for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "204228ea941e"
   },
   "outputs": [],
   "source": [
    "# The images and video used for this test are stored in a public GCS bucket: \"cloud-samples-data\"\n",
    "bucket_name = \"cloud-samples-data\"\n",
    "image_prefix = \"generative-ai/image/\"\n",
    "images_list = get_images_uri_from_bucket(bucket_name, image_prefix, delimiter=\"/\")\n",
    "\n",
    "prompt = \"Get the elements from the image, get all the animals from the video, print all the animals and elements found on a numbered list, and then write a poem about them\\n\"\n",
    "small_video_uri = \"gs://cloud-samples-data/generative-ai/video/animals.mp4\"\n",
    "large_video_uri = (\n",
    "    \"gs://cloud-samples-data/generative-ai/video/behind_the_scenes_pixel.mp4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4f08049c98e5"
   },
   "source": [
    "## Load testing Gemini "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91ffee55f2d7"
   },
   "source": [
    "### Test without retry and default quota for Gemini-1.5-pro-001 of 60 QPM\n",
    "\n",
    "4 out of 5 tests fail due to 429 Quota exceeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7add4399f0a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with model: gemini-1.5-pro-001 and function: async_ask_gemini\n",
      "72 Poems written in 23 seconds\n",
      "An error occurred: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-pro. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.\n",
      "An error occurred: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-pro. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.\n",
      "An error occurred: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-pro. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.\n",
      "An error occurred: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_input_tokens_per_minute_per_base_model with base model: gemini-1.5-pro. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.\n",
      "4 out of 5 failed\n"
     ]
    }
   ],
   "source": [
    "video_part = Part.from_uri(small_video_uri, mime_type=\"video/mp4\")\n",
    "MODEL_NAME = \"gemini-1.5-pro-001\"\n",
    "await (load_test_gemini(async_ask_gemini, MODEL_NAME, attempts=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4121c249d591"
   },
   "source": [
    "### Re-testing with backoff and retry mechanism enabled \n",
    "\n",
    "All tests finallize correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2338b49fd72d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with model: gemini-1.5-pro-001 and function: retry_async_ask_gemini\n",
      "72 Poems written in 21 seconds\n",
      "72 Poems written in 167 seconds\n",
      "72 Poems written in 18 seconds\n",
      "72 Poems written in 149 seconds\n",
      "72 Poems written in 22 seconds\n",
      "All 5 attempts succeded\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"gemini-1.5-pro-001\"\n",
    "await (load_test_gemini(retry_async_ask_gemini, MODEL_NAME, attempts=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d26a20615065"
   },
   "source": [
    "### Testing without retry but with Dynamic Shared Quota using Gemini-1.5-pro-002 \n",
    "\n",
    "All 5 attempts succeded with a small video as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "37cd3facf381"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with model: gemini-1.5-pro-002 and function: async_ask_gemini\n",
      "72 Poems written in 23 seconds\n",
      "72 Poems written in 21 seconds\n",
      "72 Poems written in 19 seconds\n",
      "72 Poems written in 17 seconds\n",
      "72 Poems written in 22 seconds\n",
      "All 5 attempts succeded\n"
     ]
    }
   ],
   "source": [
    "video_part = Part.from_uri(small_video_uri, mime_type=\"video/mp4\")\n",
    "MODEL_NAME = \"gemini-1.5-pro-002\"\n",
    "await (load_test_gemini(async_ask_gemini, MODEL_NAME, attempts=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5fa686b7c52"
   },
   "source": [
    "### Re-testing Dynamic Shared quota with larger video\n",
    "\n",
    "Without backoff and retry, testing Gemini-1.5-pro-002 with larger context window caused all tests to fail with 429 reason code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "d241524f5071"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with model: gemini-1.5-pro-002 and function: async_ask_gemini\n",
      "An error occurred: 429 Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/quotas#error-code-429 for more details.\n",
      "An error occurred: 429 Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/quotas#error-code-429 for more details.\n",
      "An error occurred: 429 Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/quotas#error-code-429 for more details.\n",
      "An error occurred: 429 Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/quotas#error-code-429 for more details.\n",
      "An error occurred: 429 Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/quotas#error-code-429 for more details.\n",
      "5 out of 5 failed\n"
     ]
    }
   ],
   "source": [
    "# Larger video  used to increase token input size\n",
    "video_part = Part.from_uri(large_video_uri, mime_type=\"video/mp4\")\n",
    "MODEL_NAME = \"gemini-1.5-pro-002\"\n",
    "await (load_test_gemini(async_ask_gemini, MODEL_NAME, attempts=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "999a6cd90e5d"
   },
   "source": [
    "### Adding Backoff and Retry to Dynamic Shared Quota Testing\n",
    "\n",
    "Adding backoff and retry mechanisms significantly increased inference time, but all tests completed successfully even with much larger context window.\n",
    "\n",
    "Provisioned Throughput should be used to guarantee the capacity and therefore reduce latency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "91c4d04ab9b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with model: gemini-1.5-pro-002 and function: retry_async_ask_gemini\n",
      "72 Poems written in 188 seconds\n",
      "72 Poems written in 205 seconds\n",
      "72 Poems written in 216 seconds\n",
      "All 3 attempts succeded\n"
     ]
    }
   ],
   "source": [
    "video_part = Part.from_uri(large_video_uri, mime_type=\"video/mp4\")\n",
    "MODEL_NAME = \"gemini-1.5-pro-002\"\n",
    "await (load_test_gemini(retry_async_ask_gemini, MODEL_NAME, attempts=3))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "backoff_and_retry_for_LLMs.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

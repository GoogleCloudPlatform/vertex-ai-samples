{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abbc85ff-e124-438b-9483-7cd5e35621d9",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/xqr-g/vertex-ai-samples/blob/main/notebooks/community/generative_ai/backoff_and_retry_for_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8862a559-a5d3-43a9-9296-68e0a4961f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef21adb8-8f15-43ed-8cbb-66f05bc9350e",
   "metadata": {},
   "source": [
    "# Backoff and retry for LLM\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/generative_ai/backoff_and_retry_for_LLMs.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/generative_ai/backoff_and_retry_for_LLMs.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/generative_ai/backoff_and_retry_for_LLMs.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>                                                                                               \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d55ae7-4556-4a41-9fa0-36b1e07344ad",
   "metadata": {},
   "source": [
    "NOTE: This notebook has been tested in the following environment:\n",
    "\n",
    "Python version = 3.10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678119f3-bb14-4b51-9152-dfca51276722",
   "metadata": {},
   "source": [
    "## TODO update link to blogpost\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how sending large amounts of traffic to Gemini-1.5-pro can cause \"429 Quota exceeded errors\" and how implementing a backoff and retry strategy can help complete jobs without interrupting operations.\n",
    "\n",
    "This notebook provides examples for the blog post: [Don't let 429 errors leave your users hanging: A guide to handling resource exhaustion](https://blog link)\n",
    "\n",
    "This tutorial uses the following Google Cloud ML service:\n",
    "\n",
    "- Vertex LLM SDK\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Installation and imports\n",
    "- Asynchronously calling the Gemini model\n",
    "- Using the Tenacity retry decorator to implement backoff and retry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e02b32-09f8-4b2a-82a4-8c34c0b67a8a",
   "metadata": {},
   "source": [
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing),\n",
    "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage.\n",
    "\n",
    "This notebook sends large amount of tokens to Gemini for inference, reduce the number of attempts or use smaller video to reduce costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37c6458-d674-447b-9b90-6bee1a5c1a8d",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "## Before you begin\n",
    "\n",
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "3. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
    "\n",
    "4. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622e3337-d42b-4aaa-ac5a-33f40c055758",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9110fa2c-a886-4ce7-9f99-b07c4e7873b8",
   "metadata": {
    "id": "74ccc9e52986"
   },
   "source": [
    "**1. Vertex AI Workbench**\n",
    "* Do nothing as you are already authenticated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e776c4a9-ba5e-4436-b456-7a255fb794a3",
   "metadata": {
    "id": "de775a3773ba"
   },
   "source": [
    "**2. Local JupyterLab instance, uncomment and run:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ebb1a8-29be-40e1-b72b-1c28d0b1d3c4",
   "metadata": {
    "id": "254614fa0c46"
   },
   "outputs": [],
   "source": [
    "# ! gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88dbebf-898b-423f-8928-148167615915",
   "metadata": {
    "id": "ef21552ccea8"
   },
   "source": [
    "**3. Colab, uncomment and run:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7fe0fa-addf-46ab-befe-c9ec3e030baf",
   "metadata": {
    "id": "603adbbf0532"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e2aa1e-80dd-4c4b-8ef8-52e822a95a5e",
   "metadata": {
    "id": "FyyMdUeAJIVv"
   },
   "source": [
    "## Installation\n",
    "\n",
    "Install the following packages required to execute this notebook.\n",
    "\n",
    "**Remember to restart the runtime after installation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00b96f5a-d230-49a9-9a95-358ca0d0e7a3",
   "metadata": {
    "id": "snBUuUamoJPz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade google-cloud-aiplatform tenacity google-cloud-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25ed45f-b207-4eba-9f5c-120e3274b85b",
   "metadata": {
    "id": "WX3CHZitmSJM",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Please restart the runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b836462-497f-4064-b180-b223a2a973c4",
   "metadata": {
    "id": "dae340cb-0583-4e7e-a562-6817ee4d7f6d"
   },
   "source": [
    "### Imports libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e602a47c-dae2-4a8a-bd6d-aaa0ec2024a3",
   "metadata": {
    "id": "412d00f1-08db-4880-8ced-52a9583757b8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import vertexai, asyncio, time, nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from vertexai.generative_models import GenerativeModel, GenerationConfig, Part\n",
    "from google.cloud import storage\n",
    "from tenacity import retry, wait_random_exponential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facabf11-db95-45f9-96da-8664dad11c25",
   "metadata": {
    "id": "MyMXIZoRlUcR"
   },
   "source": [
    "#### Set your project ID and initiate Vertex AI\n",
    "\n",
    "**If you don't know your project ID**, try the following:\n",
    "* Run `gcloud config list`.\n",
    "* Run `gcloud projects list`.\n",
    "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c4e5d2-2225-4f84-8da3-8b9570aabf26",
   "metadata": {
    "id": "3EdtdqnoldX4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"multi-tenancy-dataproc\"  # @param {type:\"string\"}\n",
    "DEFAUL_MODEL_NAME = \"gemini-1.5-pro-001\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}\n",
    "\n",
    "# Initiate Vertex AI\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "config = GenerationConfig(temperature=0.5, max_output_tokens=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de9c6ae-2ac3-4384-b8ee-28c32d1a6ca3",
   "metadata": {
    "id": "f50f22f3-ec85-463e-b6fe-5c8e6b80b07b"
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb08b997-0d97-4224-b1df-2a0d9b6ec3d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_images_uri_from_bucket(bucket_name, prefix, delimiter=None):\n",
    "    \"\"\"Lists all the images with extension '.jpg', 'jpeg' or 'png' in the bucket that begin with the prefix (folder).\n",
    "\n",
    "    \"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    blobs = storage_client.list_blobs(bucket_name, prefix=prefix, delimiter=delimiter)\n",
    "    images = [f'gs://{bucket_name}/{blob.name}' for blob in blobs if blob.name.endswith(tuple(['.jpg', 'jpeg', 'png']))]\n",
    "    return images\n",
    "\n",
    "\n",
    "async def async_ask_gemini(contents, model_name=DEFAUL_MODEL_NAME):\n",
    "    # This basic function calls Gemini asynchronously without a retry logic\n",
    "    multimodal_model = GenerativeModel(model_name)\n",
    "    response = await multimodal_model.generate_content_async(contents=contents, generation_config=config)\n",
    "    return response.text\n",
    "\n",
    "\n",
    "@retry(wait=wait_random_exponential(multiplier=1, max=60))\n",
    "async def retry_async_ask_gemini(contents, model_name=DEFAUL_MODEL_NAME):\n",
    "       \"\"\" This is the same code as the async_ask_gemini function but implements a retry logic using tenacity decorator.\n",
    "           wait_random_exponential(multiplier=1, max=60) means that it will \n",
    "           Retry â€œRandomly wait up to 2^x * 1 seconds between each retry until the range reaches 60 seconds, then randomly up to 60 seconds afterwards\"\n",
    "   \"\"\"\n",
    "\n",
    "    multimodal_model = GenerativeModel(model_name)\n",
    "    response = await multimodal_model.generate_content_async(contents=contents, generation_config=config)\n",
    "    return response.text\n",
    "\n",
    "\n",
    "async def load_test_gemini(function, model_name, attempts=5):\n",
    "    failed_attempts = 0\n",
    "    print(f\"Testing with model: {model_name} and function: {function.__name__}\")\n",
    "    for i in range(attempts):\n",
    "        try:\n",
    "            time_start =  time.time()\n",
    "            get_gemini_responses = [function([prompt, video_part, Part.from_uri(image_uri, mime_type=\"image/jpeg\"), ], model_name = MODEL_NAME) for image_uri in images_list]\n",
    "            async_poems = await asyncio.gather(*get_gemini_responses)\n",
    "            time_taken = time.time()-time_start\n",
    "            print(f\"{len(async_poems)} Poems written in {time_taken:.0f} seconds\")\n",
    "        except Exception as error:\n",
    "            failed_attempts +=1\n",
    "            print(\"An error occurred:\", error)\n",
    "\n",
    "    print(f\"{failed_attempts} out of {attempts} failed\") if failed_attempts > 0 else print(f\"All {attempts} attempts succeded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22e2086-9a94-4743-93d3-a65f54c3579b",
   "metadata": {},
   "source": [
    "### Getting images and videos used for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e34b4a5-1141-4473-bd63-97fae49f0705",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The images and video used for this test are stored in a public GCS bucket: \"cloud-samples-data\"\n",
    "bucket_name = \"cloud-samples-data\"\n",
    "image_prefix = \"generative-ai/image/\"\n",
    "images_list = get_images_uri_from_bucket(bucket_name, image_prefix, delimiter='/')\n",
    "\n",
    "prompt = \"Get the elements from the image, get all the animals from the video, print all the animals and elements found on a numbered list, and then write a poem about them\\n\"\n",
    "small_video_uri = \"gs://cloud-samples-data/generative-ai/video/animals.mp4\"\n",
    "large_video_uri = \"gs://cloud-samples-data/generative-ai/video/behind_the_scenes_pixel.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96a2c6b-2137-49a1-972a-9ee82c98d745",
   "metadata": {},
   "source": [
    "## Load testing Gemini "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320223c0-ab4f-43e2-99bb-c9457d43761d",
   "metadata": {},
   "source": [
    "### Test without retry and default quota for Gemini-1.5-pro-001 of 60 QPM\n",
    "\n",
    "4 out of 5 tests fail due to 429 Quota exceeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02e0c13b-c772-4c50-93e3-5c58db46e611",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with model: gemini-1.5-pro-001 and function: async_ask_gemini\n",
      "72 Poems written in 23 seconds\n",
      "An error occurred: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-pro. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.\n",
      "An error occurred: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-pro. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.\n",
      "An error occurred: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-pro. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.\n",
      "An error occurred: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_input_tokens_per_minute_per_base_model with base model: gemini-1.5-pro. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.\n",
      "4 out of 5 failed\n"
     ]
    }
   ],
   "source": [
    "video_part = Part.from_uri(small_video_uri, mime_type=\"video/mp4\")\n",
    "MODEL_NAME = \"gemini-1.5-pro-001\"\n",
    "await(load_test_gemini(async_ask_gemini, MODEL_NAME, attempts=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2edfc1f-41aa-4649-ba52-0e10a36a4a3d",
   "metadata": {},
   "source": [
    "### Re-testing with backoff and retry mechanism enabled \n",
    "\n",
    "All tests finallize correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "858a705b-9874-442b-8c73-4ff6a4af2481",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with model: gemini-1.5-pro-001 and function: retry_async_ask_gemini\n",
      "72 Poems written in 21 seconds\n",
      "72 Poems written in 167 seconds\n",
      "72 Poems written in 18 seconds\n",
      "72 Poems written in 149 seconds\n",
      "72 Poems written in 22 seconds\n",
      "All 5 attempts succeded\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"gemini-1.5-pro-001\"\n",
    "await(load_test_gemini(retry_async_ask_gemini, MODEL_NAME, attempts=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c05ef54-2e5b-4502-9be8-7d1de7dfd671",
   "metadata": {},
   "source": [
    "### Testing without retry but with Dynamic Shared Quota using Gemini-1.5-pro-002 \n",
    "\n",
    "All 5 attempts succeded with a small video as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb8ac684-f312-4592-8e55-59d6d55a8788",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with model: gemini-1.5-pro-002 and function: async_ask_gemini\n",
      "72 Poems written in 23 seconds\n",
      "72 Poems written in 21 seconds\n",
      "72 Poems written in 19 seconds\n",
      "72 Poems written in 17 seconds\n",
      "72 Poems written in 22 seconds\n",
      "All 5 attempts succeded\n"
     ]
    }
   ],
   "source": [
    "video_part = Part.from_uri(small_video_uri, mime_type=\"video/mp4\")\n",
    "MODEL_NAME = \"gemini-1.5-pro-002\"\n",
    "await(load_test_gemini(async_ask_gemini, MODEL_NAME, attempts=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c2fe3f-3130-4135-99cb-eedb000ae585",
   "metadata": {},
   "source": [
    "### Re-testing Dynamic Shared quota with larger video\n",
    "\n",
    "Without backoff and retry, testing Gemini-1.5-pro-002 with larger context window caused all tests to fail with 429 reason code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e5a5800-8e5a-425f-bf77-834a944f1a0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with model: gemini-1.5-pro-002 and function: async_ask_gemini\n",
      "An error occurred: 429 Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/quotas#error-code-429 for more details.\n",
      "An error occurred: 429 Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/quotas#error-code-429 for more details.\n",
      "An error occurred: 429 Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/quotas#error-code-429 for more details.\n",
      "An error occurred: 429 Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/quotas#error-code-429 for more details.\n",
      "An error occurred: 429 Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/quotas#error-code-429 for more details.\n",
      "5 out of 5 failed\n"
     ]
    }
   ],
   "source": [
    "# Larger video  used to increase token input size\n",
    "video_part = Part.from_uri(large_video_uri, mime_type=\"video/mp4\")\n",
    "MODEL_NAME = \"gemini-1.5-pro-002\"\n",
    "await(load_test_gemini(async_ask_gemini, MODEL_NAME, attempts=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbbe549-5176-455e-b2f9-49910978c360",
   "metadata": {},
   "source": [
    "### Adding Backoff and Retry to Dynamic Shared Quota Testing\n",
    "\n",
    "Adding backoff and retry mechanisms significantly increased inference time, but all tests completed successfully even with much larger context window.\n",
    "\n",
    "Provisioned Throughput should be used to guarantee the capacity and therefore reduce latency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ed8de93-52a5-4822-9dfb-da9acdbe53aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with model: gemini-1.5-pro-002 and function: retry_async_ask_gemini\n",
      "72 Poems written in 188 seconds\n",
      "72 Poems written in 205 seconds\n",
      "72 Poems written in 216 seconds\n",
      "All 3 attempts succeded\n"
     ]
    }
   ],
   "source": [
    "video_part = Part.from_uri(large_video_uri, mime_type=\"video/mp4\")\n",
    "MODEL_NAME = \"gemini-1.5-pro-002\"\n",
    "await(load_test_gemini(retry_async_ask_gemini, MODEL_NAME, attempts=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf4448b-9425-4da0-942e-e04799fe7c3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

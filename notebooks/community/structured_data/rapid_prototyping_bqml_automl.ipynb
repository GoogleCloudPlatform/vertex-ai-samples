{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40399883"
      },
      "outputs": [],
      "source": [
        "# Copyright 2021 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/structured_data/rapid_prototyping_bqml_automl.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/structured_data/rapid_prototyping_bqml_automl.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okkqa_U8AcsN"
      },
      "source": [
        "# BQML and AutoML - Experimenting with Vertex AI\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/rafacarv-public-bucket-do-not-delete/abalone/rapid_pipeline.png\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9681af0"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98513c63"
      },
      "source": [
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
        "\n",
        "1. You'll use the *gcloud* command throughout this notebook. In the following cell, enter your project name and run the cell to authenticate yourself with the Google Cloud and initialize your *gcloud* configuration settings.\n",
        "\n",
        "**For this lab, we're going to use region us-central1 for all our resources (BigQuery training data, Cloud Storage bucket, model and endpoint locations, etc.). Those resources can be deployed in other regions, as long as they're consistently co-located, but we're going to use one fixed region to keep things as simple and error free as possible.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "739011eb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90FInWYfG0A9"
      },
      "source": [
        "### Authenticate with GCP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c2da21e"
      },
      "outputs": [],
      "source": [
        "# Install Python package dependencies.\n",
        "print(\"Installing libraries\")\n",
        "! pip3 install {USER_FLAG} --quiet --upgrade google-cloud-pipeline-components==0.1.7 kfp==1.8.2\n",
        "! pip3 install {USER_FLAG} --quiet --upgrade google-cloud-aiplatform==1.4.3 google-cloud-bigquery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9a2bb523c478"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63_AplznG3J7"
      },
      "source": [
        "### Determine some project and pipeline variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7aBmVRZGr1d"
      },
      "source": [
        "### Required imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3444fe2c"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from datetime import datetime\n",
        "from typing import NamedTuple\n",
        "\n",
        "from google.cloud import aiplatform as vertex\n",
        "from google_cloud_pipeline_components import \\\n",
        "    aiplatform as vertex_pipeline_components\n",
        "from kfp import dsl\n",
        "from kfp.v2 import compiler\n",
        "from kfp.v2.dsl import (Artifact, Dataset, Input, Metrics, Model, Output,\n",
        "                        component)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL_jUN3avzBD"
      },
      "source": [
        "### Required Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy-vIuq2yWjw"
      },
      "source": [
        "Make sure the GCS bucket and the BigQuery Dataset do not exist. This script may **delete** any existing content.\n",
        "\n",
        "Your bucket must be on the same region as your Vertex AI resources.\n",
        "\n",
        "BQ region can be US or EU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef138d54"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"your-project-id\"  # @param {type:\"string\"}\n",
        "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
        "BUCKET_NAME = \"j90wipxexhrgq3cquanc5\"  # @param {type:\"string\"}\n",
        "PIPELINE_JSON_PKG_PATH = \"rapid_prototyping.json\"\n",
        "PIPELINE_ROOT = f\"gs://{BUCKET_NAME}/pipeline_root\"\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "DATA_FOLDER = f\"{BUCKET_NAME}/data\"\n",
        "\n",
        "TEST_DATASET_CSV_LOCATION = f\"gs://{DATA_FOLDER}/test_dataset\"\n",
        "RAW_INPUT_DATA = f\"gs://{DATA_FOLDER}/abalone.csv\"\n",
        "BQ_DATASET = \"j90wipxexhrgq3cquanc5\"  # @param {type:\"string\"}\n",
        "BQ_LOCATION = \"EU\"  # @param {type:\"string\"}\n",
        "BQ_LOCATION = BQ_LOCATION.upper()\n",
        "BQML_EXPORT_LOCATION = f\"gs://{BUCKET_NAME}/artifacts/bqml/\"\n",
        "GCS_BATCH_PREDICTION_OUTPUT_PREFIX = f\"gs://{BUCKET_NAME}/predictions/\"\n",
        "\n",
        "image_prefix = REGION.split(\"-\")[0]\n",
        "BQML_SERVING_CONTAINER_IMAGE_URI = (\n",
        "    f\"{image_prefix}-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-6:latest\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QfxrJGn9Tty"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# If on Google Cloud Notebooks, then don't execute this code\n",
        "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "333e4035"
      },
      "outputs": [],
      "source": [
        "if os.getenv(\"IS_TESTING\"):\n",
        "    !gcloud --quiet components install beta\n",
        "    !gcloud --quiet components update\n",
        "!gcloud config set project $PROJECT_ID\n",
        "!gcloud config set ai/region $REGION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c211d559"
      },
      "source": [
        "## The Abalone Dataset\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/rafacarv-public-bucket-do-not-delete/abalone/dataset.png\" />\n",
        "\n",
        "<p>Dataset Credits</p>\n",
        "<p>Dua, D. and Graff, C. (2019). UCI Machine Learning Repository <a href=\"http://archive.ics.uci.edu/ml\">http://archive.ics.uci.edu/ml</a>. Irvine, CA: University of California, School of Information and Computer Science.</p>\n",
        "\n",
        "<p><a href=\"https://archive.ics.uci.edu/ml/datasets/abalone\">Direct link<a/></p>\n",
        "    \n",
        "    \n",
        "#### Attribute Information:\n",
        "\n",
        "<p>Given is the attribute name, attribute type, the measurement unit and a brief description. The number of rings is the value to predict: either as a continuous value or as a classification problem.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eca2303d"
      },
      "source": [
        "<body>\n",
        "\t<table>\n",
        "\t\t<tr>\n",
        "\t\t\t<th>Name</th>\n",
        "\t\t\t<th>Data Type</th>\n",
        "\t\t\t<th>Measurement Unit</th>\n",
        "\t\t\t<th>Description</th>\n",
        "\t\t</tr>\n",
        "\t\t<tr>\n",
        "\t\t\t<td>Sex</td>\n",
        "            <td>nominal</td>\n",
        "            <td>--</td>\n",
        "            <td>M, F, and I (infant)</td>\n",
        "\t\t</tr>\n",
        "\t\t<tr>\n",
        "\t\t\t<td>Length</td>\n",
        "            <td>continuous</td>\n",
        "            <td>mm</td>\n",
        "            <td>Longest shell measurement</td>\n",
        "\t\t</tr>\n",
        "\t\t<tr>\n",
        "\t\t\t<td>Diameter</td>\n",
        "            <td>continuous</td>\n",
        "            <td>mm</td>\n",
        "            <td>perpendicular to length</td>\n",
        "\t\t</tr>\n",
        "\t\t<tr>\n",
        "\t\t\t<td>Height</td>\n",
        "            <td>continuous</td>\n",
        "            <td>mm</td>\n",
        "            <td>with meat in shell</td>\n",
        "\t\t</tr>\n",
        "\t\t<tr>\n",
        "\t\t\t<td>Whole weight</td>\n",
        "            <td>continuous</td>\n",
        "            <td>grams</td>\n",
        "            <td>whole abalone</td>\n",
        "\t\t</tr>\n",
        "\t\t<tr>\n",
        "\t\t\t<td>Shucked weight</td>\n",
        "            <td>continuous</td>\n",
        "            <td>grams</td>\n",
        "            <td>weight of meat</td>\n",
        "\t\t</tr>\n",
        "\t\t<tr>\n",
        "\t\t\t<td>Viscera weight</td>\n",
        "            <td>continuous</td>\n",
        "            <td>grams</td>\n",
        "            <td>gut weight (after bleeding)</td>\n",
        "\t\t</tr>\n",
        "\t\t<tr>\n",
        "\t\t\t<td>Shell weight</td>\n",
        "            <td>continuous</td>\n",
        "            <td>grams</td>\n",
        "            <td>after being dried</td>\n",
        "\t\t</tr>\n",
        "        <tr>\n",
        "\t\t\t<td>Rings</td>\n",
        "            <td>integer</td>\n",
        "\t\t\t<td>--</td>\n",
        "            <td>+1.5 gives the age in years</td>\n",
        "\t\t</tr>\n",
        "\t</table>\n",
        "</body>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3-Bqs7vFU7Y"
      },
      "source": [
        "### Downloading the data\n",
        "\n",
        "If the bucket does not exist, the script below will create it.\n",
        "\n",
        "After creating the bucket, the cell below will download the dataset into a CSV file and save it in GCS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6bd858d"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -b gs://{BUCKET_NAME} || gsutil mb -l {REGION} gs://{BUCKET_NAME}\n",
        "\n",
        "! gsutil cp gs://cloud-samples-data/vertex-ai/community-content/datasets/abalone/abalone.data {RAW_INPUT_DATA}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owlPQF1KF8QO"
      },
      "source": [
        "## Pipeline Components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79eaa73a"
      },
      "source": [
        "### Import to BQ\n",
        "\n",
        "This component takes the csv file and imports it to a table in BigQuery. If the dataset does not exist, it will be created. If a table with the same name already exists, it will be deleted and recreated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e44af8ac"
      },
      "outputs": [],
      "source": [
        "@component(base_image=\"python:3.9\", packages_to_install=[\"google-cloud-bigquery\"])\n",
        "def import_data_to_bigquery(\n",
        "    project: str,\n",
        "    bq_location: str,\n",
        "    bq_dataset: str,\n",
        "    gcs_data_uri: str,\n",
        "    raw_dataset: Output[Artifact],\n",
        "    table_name_prefix: str = \"abalone\",\n",
        "):\n",
        "    from google.cloud import bigquery\n",
        "\n",
        "    # Construct a BigQuery client object.\n",
        "    client = bigquery.Client(project=project, location=bq_location)\n",
        "\n",
        "    def load_dataset(gcs_uri, table_id):\n",
        "        job_config = bigquery.LoadJobConfig(\n",
        "            schema=[\n",
        "                bigquery.SchemaField(\"Sex\", \"STRING\"),\n",
        "                bigquery.SchemaField(\"Length\", \"NUMERIC\"),\n",
        "                bigquery.SchemaField(\"Diameter\", \"NUMERIC\"),\n",
        "                bigquery.SchemaField(\"Height\", \"NUMERIC\"),\n",
        "                bigquery.SchemaField(\"Whole_weight\", \"NUMERIC\"),\n",
        "                bigquery.SchemaField(\"Shucked_weight\", \"NUMERIC\"),\n",
        "                bigquery.SchemaField(\"Viscera_weight\", \"NUMERIC\"),\n",
        "                bigquery.SchemaField(\"Shell_weight\", \"NUMERIC\"),\n",
        "                bigquery.SchemaField(\"Rings\", \"NUMERIC\"),\n",
        "            ],\n",
        "            skip_leading_rows=1,\n",
        "            # The source format defaults to CSV, so the line below is optional.\n",
        "            source_format=bigquery.SourceFormat.CSV,\n",
        "        )\n",
        "        print(f\"Loading {gcs_uri} into {table_id}\")\n",
        "        load_job = client.load_table_from_uri(\n",
        "            gcs_uri, table_id, job_config=job_config\n",
        "        )  # Make an API request.\n",
        "\n",
        "        load_job.result()  # Waits for the job to complete.\n",
        "        destination_table = client.get_table(table_id)  # Make an API request.\n",
        "        print(\"Loaded {} rows.\".format(destination_table.num_rows))\n",
        "\n",
        "    def create_dataset_if_not_exist(bq_dataset_id, bq_location):\n",
        "        print(\n",
        "            \"Checking for existence of bq dataset. If it does not exist, it creates one\"\n",
        "        )\n",
        "        dataset = bigquery.Dataset(bq_dataset_id)\n",
        "        dataset.location = bq_location\n",
        "        dataset = client.create_dataset(dataset, exists_ok=True, timeout=300)\n",
        "        print(f\"Created dataset {dataset.full_dataset_id} @ {dataset.location}\")\n",
        "\n",
        "    bq_dataset_id = f\"{project}.{bq_dataset}\"\n",
        "    create_dataset_if_not_exist(bq_dataset_id, bq_location)\n",
        "\n",
        "    raw_table_name = f\"{table_name_prefix}_raw\"\n",
        "    table_id = f\"{project}.{bq_dataset}.{raw_table_name}\"\n",
        "    print(\"Deleting any tables that might have the same name on the dataset\")\n",
        "    client.delete_table(table_id, not_found_ok=True)\n",
        "    print(\"will load data to table\")\n",
        "    load_dataset(gcs_data_uri, table_id)\n",
        "\n",
        "    raw_dataset_uri = f\"bq://{table_id}\"\n",
        "    raw_dataset.uri = raw_dataset_uri"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "637de8be"
      },
      "source": [
        "### Split Datasets\n",
        "\n",
        "Splits the dataset in 3 slices:\n",
        "- TRAIN\n",
        "- EVALUTE\n",
        "- TEST\n",
        "\n",
        "\n",
        "AutoML and BigQuery ML use different nomenclatures for data splits, so 2 BQ tables will be created.\n",
        "\n",
        "#### BQML\n",
        "- How BQML splits the data: [link](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-hyperparameter-tuning#data_split)\n",
        "\n",
        "#### AutoML\n",
        "How AutoML splits the data: [link](https://cloud.google.com/vertex-ai/docs/general/ml-use?hl=da&skip_cache=false)\n",
        "\n",
        "<ul>\n",
        "    <li>Model trials\n",
        "<p>The training set is used to train models with different preprocessing, architecture, and hyperparameter option combinations. These models are evaluated on the validation set for quality, which guides the exploration of additional option combinations. The best parameters and architectures determined in the parallel tuning phase are used to train two ensemble models as described below.</p></li>\n",
        "\n",
        "<li>Model evaluation\n",
        "<p>\n",
        "Vertex AI trains an evaluation model, using the training and validation sets as training data. Vertex AI generates the final model evaluation metrics on this model, using the test set. This is the first time in the process that the test set is used. This approach ensures that the final evaluation metrics are an unbiased reflection of how well the final trained model will perform in production.</p></li>\n",
        "\n",
        "<li>Serving model\n",
        "<p>A model is trained with the training, validation, and test sets, to maximize the amount of training data. This model is the one that you use to request predictions.</p></li>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cf0a61c"
      },
      "outputs": [],
      "source": [
        "@component(\n",
        "    base_image=\"python:3.9\",\n",
        "    packages_to_install=[\n",
        "        \"google-cloud-bigquery\",\n",
        "        \"pandas\",\n",
        "        \"pyarrow\",\n",
        "        \"fsspec\",\n",
        "        \"gcsfs\",\n",
        "    ],\n",
        ")  # pandas, pyarrow and fsspec required to export bq data to csv\n",
        "def split_datasets(\n",
        "    raw_dataset: Input[Artifact],\n",
        "    bqml_dataset: Output[Dataset],\n",
        "    test_dataset_folder: str,\n",
        "    bq_location: str,\n",
        ") -> NamedTuple(\n",
        "    \"bqml_split\",\n",
        "    [\n",
        "        (\"dataset_uri\", str),\n",
        "        (\"test_features_jsonl_filenames\", list),\n",
        "        (\"test_labels_csv_filename\", str),\n",
        "        (\"train_dataset_uri\", str),\n",
        "        (\"test_dataset_uri\", str),\n",
        "        (\"validate_dataset_uri\", str),\n",
        "    ],\n",
        "):\n",
        "\n",
        "    from collections import namedtuple\n",
        "\n",
        "    import gcsfs\n",
        "    from google.cloud import bigquery\n",
        "\n",
        "    raw_dataset_uri = raw_dataset.uri\n",
        "    table_name = raw_dataset_uri.split(\"bq://\")[-1]\n",
        "    print(table_name)\n",
        "    raw_dataset_uri = table_name.split(\".\")\n",
        "    print(raw_dataset_uri)\n",
        "    project = raw_dataset_uri[0]\n",
        "    bq_dataset = raw_dataset_uri[1]\n",
        "    bq_raw_table = raw_dataset_uri[2]\n",
        "\n",
        "    client = bigquery.Client(project=project, location=bq_location)\n",
        "\n",
        "    def split_dataset(table_name_dataset):\n",
        "        training_dataset_table_name = f\"{project}.{bq_dataset}.{table_name_dataset}\"\n",
        "        split_query = f\"\"\"\n",
        "        CREATE OR REPLACE TABLE\n",
        "            `{training_dataset_table_name}`\n",
        "           AS\n",
        "        SELECT\n",
        "          Sex,\n",
        "          Length,\n",
        "          Diameter,\n",
        "          Height,\n",
        "          Whole_weight,\n",
        "          Shucked_weight,\n",
        "          Viscera_weight,\n",
        "          Shell_weight,\n",
        "          Rings,\n",
        "            CASE(ABS(MOD(FARM_FINGERPRINT(TO_JSON_STRING(f)), 10)))\n",
        "              WHEN 9 THEN 'TEST'\n",
        "              WHEN 8 THEN 'VALIDATE'\n",
        "              ELSE 'TRAIN' END AS split_col\n",
        "        FROM\n",
        "          `{project}.{bq_dataset}.abalone_raw` f\n",
        "        \"\"\"\n",
        "        dataset_uri = f\"{project}.{bq_dataset}.{bq_raw_table}\"\n",
        "        print(\"Splitting the dataset\")\n",
        "        query_job = client.query(split_query)  # Make an API request.\n",
        "        query_job.result()\n",
        "        print(dataset_uri)\n",
        "        print(split_query.replace(\"\\n\", \" \"))\n",
        "        return training_dataset_table_name\n",
        "\n",
        "    def create_separate_tables(splits, training_dataset_table_name):\n",
        "        output = {}\n",
        "        for s in splits:\n",
        "            destination_table_name = f\"abalone_{s}\"\n",
        "            query = f\"\"\"\n",
        "             CREATE OR REPLACE TABLE `{project}.{bq_dataset}.{destination_table_name}` AS SELECT\n",
        "          Sex,\n",
        "          Length,\n",
        "          Diameter,\n",
        "          Height,\n",
        "          Whole_weight,\n",
        "          Shucked_weight,\n",
        "          Viscera_weight,\n",
        "          Shell_weight,\n",
        "          Rings \n",
        "          FROM `{training_dataset_table_name}`  f\n",
        "          WHERE \n",
        "          f.split_col = '{s}'\n",
        "          \"\"\"\n",
        "            print(f\"Creating table for {s} --> {destination_table_name}\")\n",
        "            print(query.replace(\"\\n\", \" \"))\n",
        "            output[s] = destination_table_name\n",
        "            query_job = client.query(query)  # Make an API request.\n",
        "            query_job.result()\n",
        "\n",
        "        print(output)\n",
        "        return output\n",
        "\n",
        "    def create_bqml_dataset(training_dataset_table_name):\n",
        "        bqml_dataset_table_name = \"dataset_bqml\"\n",
        "\n",
        "        query = f\"\"\"\n",
        "        CREATE OR REPLACE TABLE\n",
        "          `{project}.{bq_dataset}.{bqml_dataset_table_name}`  AS\n",
        "        SELECT\n",
        "          Sex,\n",
        "          Length,\n",
        "          Diameter,\n",
        "          Height,\n",
        "          Whole_weight,\n",
        "          Shucked_weight,\n",
        "          Viscera_weight,\n",
        "          Shell_weight,\n",
        "          Rings,\n",
        "          CASE(split_col)\n",
        "            WHEN 'VALIDATE' THEN 'EVAL'\n",
        "            WHEN 'TRAIN' THEN 'TRAIN'\n",
        "            WHEN 'TEST' THEN 'TEST'\n",
        "        END\n",
        "          AS split_col\n",
        "        FROM\n",
        "          `{project}.{bq_dataset}.{training_dataset_table_name}`  f\n",
        "        WHERE\n",
        "          split_col IN ('VALIDATE',\n",
        "            'TRAIN')\n",
        "        \"\"\"\n",
        "\n",
        "        # print(query)\n",
        "        query_job = client.query(query)  # Make an API request.\n",
        "        query_job.result()\n",
        "        return bqml_dataset_table_name\n",
        "\n",
        "    def export_test_features_to_gcs(bq_test_table_name, gcs_export_path_prefix):\n",
        "        query_string = f\"\"\"\n",
        "        SELECT\n",
        "          Sex,\n",
        "          Length,\n",
        "          Diameter,\n",
        "          Height,\n",
        "          Whole_weight,\n",
        "          Shucked_weight,\n",
        "          Viscera_weight,\n",
        "          Shell_weight,\n",
        "          Rings\n",
        "        FROM `{project}.{bq_dataset}.{bq_test_table_name}`  f\n",
        "        \"\"\"\n",
        "        print(f\"Exporting test dataset {project}.{bq_dataset}.{bq_test_table_name}\")\n",
        "        print(query_string.replace(\"\\n\", \" \"))\n",
        "        dataframe = (\n",
        "            client.query(query_string)\n",
        "            .result()\n",
        "            .to_dataframe(\n",
        "                # Optionally, explicitly request to use the BigQuery Storage API. As of\n",
        "                # google-cloud-bigquery version 1.26.0 and above, the BigQuery Storage\n",
        "                # API is used by default.\n",
        "                create_bqstorage_client=True,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        test_labels_csv_filename = f\"{gcs_export_path_prefix}/test_labels.csv\"\n",
        "        labels = dataframe[\"Rings\"]\n",
        "        print(f\"Exporting test labels into {test_labels_csv_filename}\")\n",
        "        labels.to_csv(test_labels_csv_filename, index=False, header=True)\n",
        "\n",
        "        test_features_jsonl_filename = f\"{gcs_export_path_prefix}/test_features.jsonl\"\n",
        "        features = dataframe.drop(columns=[\"Rings\"])\n",
        "        jsonl = features.to_json(orient=\"records\", lines=True)\n",
        "        gcs = gcsfs.GCSFileSystem()\n",
        "        with gcs.open(test_features_jsonl_filename, \"w\") as text_file:\n",
        "            text_file.write(jsonl)\n",
        "\n",
        "        print(f\"Exporting test labels into {test_features_jsonl_filename}\")\n",
        "        return test_features_jsonl_filename, test_labels_csv_filename\n",
        "\n",
        "    table_name_dataset = \"dataset\"\n",
        "\n",
        "    dataset_uri = split_dataset(table_name_dataset)\n",
        "    splits = [\"TRAIN\", \"VALIDATE\", \"TEST\"]\n",
        "    table_names_dict = create_separate_tables(splits, dataset_uri)\n",
        "    bqml_dataset_table_name = create_bqml_dataset(table_name_dataset)\n",
        "    test_table_name = table_names_dict[\"TEST\"]\n",
        "    (\n",
        "        test_features_jsonl_filename,\n",
        "        test_labels_csv_filename,\n",
        "    ) = export_test_features_to_gcs(test_table_name, test_dataset_folder)\n",
        "    dataset_uri = \"bq://\" + dataset_uri\n",
        "    train_dataset_uri = f\"bq://{project}.{bq_dataset}.{table_names_dict['TRAIN']}\"\n",
        "    test_dataset_uri = f\"bq://{project}.{bq_dataset}.{table_names_dict['TEST']}\"\n",
        "    validate_dataset_uri = f\"bq://{project}.{bq_dataset}.{table_names_dict['VALIDATE']}\"\n",
        "    bqml_dataset_uri = f\"bq://{project}.{bq_dataset}.{bqml_dataset_table_name}\"\n",
        "\n",
        "    print(f\"dataset: {dataset_uri}\")\n",
        "    print(f\"training: {train_dataset_uri}\")\n",
        "    print(f\"test: {test_dataset_uri}\")\n",
        "    print(f\"validation: {validate_dataset_uri}\")\n",
        "\n",
        "    # dataset.uri = dataset_uri\n",
        "    # train_dataset.uri = train_dataset_uri\n",
        "    # test_dataset.uri = test_dataset_uri\n",
        "    # validate_dataset.uri = validate_dataset_uri\n",
        "    bqml_dataset.uri = bqml_dataset_uri\n",
        "\n",
        "    result_tuple = namedtuple(\n",
        "        \"bqml_split\",\n",
        "        [\n",
        "            \"dataset_uri\",\n",
        "            \"test_features_jsonl_filenames\",\n",
        "            \"test_labels_csv_filename\",\n",
        "            \"train_dataset_uri\",\n",
        "            \"test_dataset_uri\",\n",
        "            \"validate_dataset_uri\",\n",
        "        ],\n",
        "    )\n",
        "    return result_tuple(\n",
        "        dataset_uri=str(dataset_uri),\n",
        "        test_features_jsonl_filenames=[test_features_jsonl_filename],\n",
        "        test_labels_csv_filename=test_labels_csv_filename,\n",
        "        train_dataset_uri=train_dataset_uri,\n",
        "        test_dataset_uri=test_dataset_uri,\n",
        "        validate_dataset_uri=validate_dataset_uri,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e5d1785"
      },
      "source": [
        "### Train BQML Model\n",
        "\n",
        "On this demo, we will use a simple linear regression model on BQML. However, you can be creative with other model architectures, such as Deep Neural Networks, XGboost, Logistic Regression, etc.\n",
        "\n",
        "For a full list of models supported by BQML, look here: [End-to-end user journey for each model](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-e2e-journey)\n",
        "\n",
        "BQML also supports **Hyperparameter Tuning**. On this demo, we are doing 4 different trials. You can expand this following the documentation: [BigQuery ML Hyperparameter Tuning Overview](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-hp-tuning-overview)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "802ee37d"
      },
      "outputs": [],
      "source": [
        "@component(base_image=\"python:3.9\", packages_to_install=[\"google-cloud-bigquery\"])\n",
        "def train_bqml_model(\n",
        "    dataset: Input[Dataset],\n",
        "    bqml_model: Output[Artifact],\n",
        "    bq_location: str,\n",
        "    model_name: str = \"linear_regression_model\",\n",
        "    num_trials: int = 4,\n",
        ") -> NamedTuple(\"bqml_training\", [(\"query\", str)]):\n",
        "    from collections import namedtuple\n",
        "\n",
        "    from google.cloud import bigquery\n",
        "\n",
        "    dataset_uri = dataset.uri\n",
        "    table_name = dataset_uri.split(\"bq://\")[-1]\n",
        "    print(table_name)\n",
        "    uri_parts = table_name.split(\".\")\n",
        "    print(uri_parts)\n",
        "    project = uri_parts[0]\n",
        "    bq_dataset = uri_parts[1]\n",
        "\n",
        "    client = bigquery.Client(project=project, location=bq_location)\n",
        "\n",
        "    model_table_name = f\"{project}.{bq_dataset}.{model_name}\"\n",
        "\n",
        "    model_options = \"\"\"OPTIONS\n",
        "      ( MODEL_TYPE='LINEAR_REG',\n",
        "        LS_INIT_LEARN_RATE=0.15,\n",
        "        L1_REG=1,\n",
        "        MAX_ITERATIONS=5,\n",
        "        DATA_SPLIT_COL='split_col',\n",
        "        DATA_SPLIT_METHOD='CUSTOM',\n",
        "        input_label_cols=['Rings']\n",
        "        \n",
        "    \"\"\"\n",
        "    if num_trials > 0:\n",
        "        model_options += f\"\"\", \n",
        "        \n",
        "        NUM_TRIALS={num_trials},\n",
        "        HPARAM_TUNING_OBJECTIVES=['mean_squared_error']\"\"\"\n",
        "    model_options += \")\" \"\"\n",
        "\n",
        "    query = f\"\"\"\n",
        "    CREATE OR REPLACE MODEL\n",
        "      `{model_table_name}`\n",
        "      {model_options}\n",
        "     AS\n",
        "    SELECT\n",
        "      Sex,\n",
        "      Length,\n",
        "      Diameter,\n",
        "      Height,\n",
        "      Whole_weight,\n",
        "      Shucked_weight,\n",
        "      Viscera_weight,\n",
        "      Shell_weight,\n",
        "      Rings,\n",
        "      split_col\n",
        "    FROM\n",
        "      `{table_name}`;\n",
        "    \"\"\"\n",
        "\n",
        "    print(query.replace(\"\\n\", \" \"))\n",
        "    query_job = client.query(query)  # Make an API request.\n",
        "    print(query_job.job_id)\n",
        "    query_job.result()\n",
        "    bqml_model.uri = f\"bq://{model_table_name}\"\n",
        "\n",
        "    result_tuple = namedtuple(\"bqml_training\", [\"query\"])\n",
        "\n",
        "    return result_tuple(query=str(query))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf89b4b9"
      },
      "source": [
        "### Export BQML Model\n",
        "\n",
        "Once the BQML model is trained, you will need to export it to a GCS bucket, so it can later be imported to Vertex AI.\n",
        "\n",
        "For more information on exporting your model, please follow the [documentation].(https://cloud.google.com/bigquery-ml/docs/exporting-models)\n",
        "\n",
        "The doc above also talks about limitations on the model export statement. \n",
        "\n",
        "As of September 2021, an important one to be aware is the following:\n",
        "\n",
        "- The model **doesn't contain** the information about the **feature engineering** performed in the `SELECT` statement during training, so you would need to manually convert the input data before feeding into the exported model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0f2927e"
      },
      "outputs": [],
      "source": [
        "@component(base_image=\"python:3.9\", packages_to_install=[\"google-cloud-bigquery\"])\n",
        "def export_bqml_to_tf(\n",
        "    project: str,\n",
        "    export_location: str,\n",
        "    bqml_model: Input[Model],\n",
        "    tf_model: Output[Artifact],\n",
        "    bq_location: str,\n",
        "):\n",
        "    from google.cloud import bigquery\n",
        "\n",
        "    bqml_table_name = bqml_model.uri.split(\"/\")[-1]\n",
        "    query = f\"\"\"\n",
        "     EXPORT MODEL `{bqml_table_name}`\n",
        "    OPTIONS(URI = '{export_location}')\n",
        "\n",
        "    \"\"\"\n",
        "    client = bigquery.Client(project=project, location=bq_location)\n",
        "    query_job = client.query(query)\n",
        "    query_job.result()\n",
        "\n",
        "    tf_model.uri = export_location"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bd78384"
      },
      "source": [
        "### Model Evaluation\n",
        "\n",
        "After your model has been trained, it is now time to run some predictions on data that it never saw before. In order to do that, we will make a batch prediction request to the recently trained model. In the previous data split, we reserved some instances on the **TEST** dataset to be used for this\n",
        "\n",
        "Once the batch prediction job is finalized, we will compare the predictions to the actual results, that were saved in the file that we are calling ground truth.\n",
        "\n",
        "In this demo, since this is a regression model, we are going to use RMSE to compare the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "253fcd22"
      },
      "outputs": [],
      "source": [
        "@component(\n",
        "    base_image=\"python:3.9\",\n",
        "    packages_to_install=[\n",
        "        \"google-cloud-aiplatform==1.3.0\",\n",
        "        \"numpy==1.21.1\",\n",
        "        \"pandas==1.3.0\",\n",
        "        \"scikit-learn==0.24.2\",\n",
        "        \"pyarrow==5.0.0\",\n",
        "        \"fsspec==2021.9.0\",\n",
        "        \"gcsfs==2021.9.0\",\n",
        "    ],\n",
        ")\n",
        "def evaluate_batch_predictions(\n",
        "    batch_prediction_job: Input[Artifact],\n",
        "    gcs_ground_truth: str,\n",
        "    model_framework: str,\n",
        "    model_type: str,\n",
        "    metrics: Output[Metrics],\n",
        "    reference_metric_name: str = \"rmse\",\n",
        ") -> NamedTuple(\"ModelEvaluationOutput\", [(\"metric\", float)]):\n",
        "    import json\n",
        "    from collections import namedtuple\n",
        "\n",
        "    import gcsfs\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    from google.cloud import aiplatform\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "\n",
        "    def treat_job_uri(uri):\n",
        "        return uri[uri.find(\"projects/\") :]\n",
        "\n",
        "    def get_job_output_dir(batch_prediction_uri):\n",
        "        bpj = aiplatform.BatchPredictionJob(batch_prediction_uri)\n",
        "        output = bpj.output_info.gcs_output_directory\n",
        "        return output\n",
        "\n",
        "    def create_dicts_from_predictions(local_files):\n",
        "        instances_output = []\n",
        "        predictions_output = []\n",
        "        for x in local_files:\n",
        "            with gcs.open(x) as f:\n",
        "                for line in f:\n",
        "                    line_json = json.loads(line)\n",
        "                    # print(json.dumps(line_json, indent=2))\n",
        "                    instance = line_json[\"instance\"]\n",
        "                    instances_output.append(instance)\n",
        "                    pred = line_json[\"prediction\"]\n",
        "                    if type(pred) is dict and \"value\" in pred.keys():\n",
        "                        # AutoML predictions\n",
        "                        prediction = pred[\"value\"]\n",
        "                    elif type(pred) is list:\n",
        "                        # BQML Predictions return different format\n",
        "                        prediction = pred[0]\n",
        "\n",
        "                    predictions_output.append(prediction)\n",
        "        return instances_output, predictions_output\n",
        "\n",
        "    def evaluate(predictions, test_results):\n",
        "        return dict(rmse=np.sqrt(mean_squared_error(test_results, predictions)))\n",
        "\n",
        "    def get_prediction_file_names(gcs, gcs_dir):\n",
        "        reg_expression = f\"{gcs_dir}/prediction.results*\"\n",
        "        file_names = [f\"gs://{x}\" for x in gcs.glob(reg_expression)]\n",
        "        return file_names\n",
        "\n",
        "    bpj_uri = batch_prediction_job.uri\n",
        "    print(f\"Batch Prediction Job URI: {bpj_uri}\")\n",
        "\n",
        "    treated_uri = treat_job_uri(bpj_uri)\n",
        "    gcs_dir = get_job_output_dir(treated_uri)\n",
        "    print(f\"Results saved to {gcs_dir}\")\n",
        "    gcs = gcsfs.GCSFileSystem()\n",
        "    prediction_files = get_prediction_file_names(gcs, gcs_dir)\n",
        "    print(f\"Predictions available on following files: {prediction_files}\")\n",
        "\n",
        "    instances, predictions = create_dicts_from_predictions(prediction_files)\n",
        "    print(f\"{len(predictions)} predictions found\")\n",
        "\n",
        "    labels = pd.read_csv(gcs_ground_truth).to_numpy()\n",
        "    print(f\"{len(labels)} ground truth labels found\")\n",
        "\n",
        "    evaluation_metric = evaluate(predictions, labels)\n",
        "    print(evaluation_metric)\n",
        "\n",
        "    print(\"Logging metrics to output artifact\")\n",
        "    metrics.metadata[\"model_type\"] = model_type\n",
        "    metrics.metadata[\"framework\"] = model_framework\n",
        "    metrics.metadata[\"SAMPLE_KEY\"] = \"You can add other metrics here\"\n",
        "\n",
        "    for k, v in evaluation_metric.items():\n",
        "        print(f\"{k} -> {v}\")\n",
        "        metrics.log_metric(k, v)\n",
        "    ModelEvaluationOutput = namedtuple(\"ModelEvaluationOutput\", [\"metric\"])\n",
        "\n",
        "    return ModelEvaluationOutput(metric=float(evaluation_metric[reference_metric_name]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7421c559"
      },
      "source": [
        "### Model Selection\n",
        "\n",
        "Now that we have evaluated the models independently, we are going to move forward with only one of them. This election will be done based on the model evaluation metrics gathered in the previous step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20d363d9"
      },
      "outputs": [],
      "source": [
        "@component(base_image=\"python:3.9\")\n",
        "def select_best_model(\n",
        "    model_one: Input[Model],\n",
        "    metrics_one: float,\n",
        "    model_two: Input[Model],\n",
        "    metrics_two: float,\n",
        "    thresholds_dict_str: str,\n",
        "    best_model: Output[Model],\n",
        "    reference_metric_name: str = \"rmse\",\n",
        ") -> NamedTuple(\n",
        "    \"Outputs\", [(\"deploy_decision\", str), (\"metric\", float), (\"metric_name\", str)]\n",
        "):\n",
        "    import json\n",
        "    from collections import namedtuple\n",
        "\n",
        "    message = None\n",
        "    best_metric = float(\"inf\")\n",
        "    if metrics_one <= metrics_two:\n",
        "        best_model.uri = model_one.uri\n",
        "        best_metric = metrics_one\n",
        "        message = \"Model one is the best\"\n",
        "    else:\n",
        "        best_model.uri = model_two.uri\n",
        "        best_metric = metrics_two\n",
        "        message = \"Model two is the best\"\n",
        "    thresholds_dict = json.loads(thresholds_dict_str)\n",
        "    deploy = False\n",
        "    if best_metric < thresholds_dict[reference_metric_name]:\n",
        "        deploy = True\n",
        "    if deploy:\n",
        "        deploy_decision = \"true\"\n",
        "    else:\n",
        "        deploy_decision = \"false\"\n",
        "\n",
        "    print(f\"Which model is best? {message}\")\n",
        "    print(f\"What metric is being used? {reference_metric_name}\")\n",
        "    print(f\"What is the best metric? {best_metric}\")\n",
        "    print(f\"What is the threshold to deploy? {thresholds_dict_str}\")\n",
        "    print(f\"Deploy decision: {deploy_decision}\")\n",
        "\n",
        "    Outputs = namedtuple(\"Outputs\", [\"deploy_decision\", \"metric\", \"metric_name\"])\n",
        "\n",
        "    return Outputs(\n",
        "        deploy_decision=deploy_decision,\n",
        "        metric=best_metric,\n",
        "        metric_name=reference_metric_name,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f573556"
      },
      "source": [
        "### Validate Infrastructure\n",
        "\n",
        "Now that the best model has been deployed, we will validate the endpoint by making a simple prediction to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa1bab55"
      },
      "outputs": [],
      "source": [
        "@component(base_image=\"python:3.9\", packages_to_install=[\"google-cloud-aiplatform\"])\n",
        "def validate_infra(\n",
        "    endpoint: Input[Artifact],\n",
        ") -> NamedTuple(\n",
        "    \"validate_infrastructure_output\", [(\"instance\", str), (\"prediction\", float)]\n",
        "):\n",
        "    import json\n",
        "    from collections import namedtuple\n",
        "\n",
        "    from google.cloud import aiplatform\n",
        "    from google.protobuf import json_format\n",
        "    from google.protobuf.struct_pb2 import Value\n",
        "\n",
        "    def treat_uri(uri):\n",
        "        return uri[uri.find(\"projects/\") :]\n",
        "\n",
        "    def request_prediction(endp, instance):\n",
        "        instance = json_format.ParseDict(instance, Value())\n",
        "        instances = [instance]\n",
        "        parameters_dict = {}\n",
        "        parameters = json_format.ParseDict(parameters_dict, Value())\n",
        "        response = endp.predict(instances=instances, parameters=parameters)\n",
        "        print(\"deployed_model_id:\", response.deployed_model_id)\n",
        "        print(\"predictions: \", response.predictions)\n",
        "        # The predictions are a google.protobuf.Value representation of the model's predictions.\n",
        "        predictions = response.predictions\n",
        "\n",
        "        for pred in predictions:\n",
        "            if type(pred) is dict and \"value\" in pred.keys():\n",
        "                # AutoML predictions\n",
        "                prediction = pred[\"value\"]\n",
        "            elif type(pred) is list:\n",
        "                # BQML Predictions return different format\n",
        "                prediction = pred[0]\n",
        "            return prediction\n",
        "\n",
        "    endpoint_uri = endpoint.uri\n",
        "    treated_uri = treat_uri(endpoint_uri)\n",
        "\n",
        "    instance = {\n",
        "        \"Sex\": \"M\",\n",
        "        \"Length\": 0.33,\n",
        "        \"Diameter\": 0.255,\n",
        "        \"Height\": 0.08,\n",
        "        \"Whole_weight\": 0.205,\n",
        "        \"Shucked_weight\": 0.0895,\n",
        "        \"Viscera_weight\": 0.0395,\n",
        "        \"Shell_weight\": 0.055,\n",
        "    }\n",
        "    instance_json = json.dumps(instance)\n",
        "    print(\"Will use the following instance: \" + instance_json)\n",
        "\n",
        "    endpoint = aiplatform.Endpoint(treated_uri)\n",
        "    prediction = request_prediction(endpoint, instance)\n",
        "    result_tuple = namedtuple(\n",
        "        \"validate_infrastructure_output\", [\"instance\", \"prediction\"]\n",
        "    )\n",
        "\n",
        "    return result_tuple(instance=str(instance_json), prediction=float(prediction))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpB_bdDbGGOp"
      },
      "source": [
        "## The Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5PsR31ysGuj"
      },
      "outputs": [],
      "source": [
        "DISPLAY_NAME = \"rapid-prototyping-bqml-vs-automl\"\n",
        "\n",
        "pipeline_params = {\n",
        "    \"project\": PROJECT_ID,\n",
        "    \"region\": REGION,\n",
        "    \"gcs_input_file_uri\": RAW_INPUT_DATA,\n",
        "    \"bq_dataset\": BQ_DATASET,\n",
        "    \"bq_location\": BQ_LOCATION,\n",
        "    \"bqml_model_export_location\": BQML_EXPORT_LOCATION,\n",
        "    \"bqml_serving_container_image_uri\": BQML_SERVING_CONTAINER_IMAGE_URI,\n",
        "    \"test_dataset_folder\": TEST_DATASET_CSV_LOCATION,\n",
        "    \"gcs_batch_prediction_output_prefix\": GCS_BATCH_PREDICTION_OUTPUT_PREFIX,\n",
        "    \"thresholds_dict_str\": '{\"rmse\": 20.0}',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e0d61b6"
      },
      "outputs": [],
      "source": [
        "@dsl.pipeline(name=DISPLAY_NAME, description=\"Rapid Prototyping\")\n",
        "def train_pipeline(\n",
        "    project: str,\n",
        "    gcs_input_file_uri: str,\n",
        "    region: str,\n",
        "    bq_dataset: str,\n",
        "    bq_location: str,\n",
        "    bqml_model_export_location: str,\n",
        "    bqml_serving_container_image_uri: str,\n",
        "    test_dataset_folder: str,\n",
        "    gcs_batch_prediction_output_prefix: str,\n",
        "    thresholds_dict_str: str,\n",
        "):\n",
        "    import_data_to_bigquery_op = import_data_to_bigquery(\n",
        "        project, bq_location, bq_dataset, gcs_input_file_uri\n",
        "    )\n",
        "\n",
        "    job_display_name = f\"{DISPLAY_NAME}_job\"\n",
        "\n",
        "    raw_dataset = import_data_to_bigquery_op.outputs[\"raw_dataset\"]\n",
        "\n",
        "    split_datasets_op = split_datasets(\n",
        "        raw_dataset, test_dataset_folder=test_dataset_folder, bq_location=bq_location\n",
        "    )\n",
        "\n",
        "    bqml_dataset = split_datasets_op.outputs[\"bqml_dataset\"]\n",
        "    train_bqml_model_op = train_bqml_model(bqml_dataset, bq_location=bq_location)\n",
        "    bqml_trained_model = train_bqml_model_op.outputs[\"bqml_model\"]\n",
        "    export_bqml_to_tf_op = export_bqml_to_tf(\n",
        "        export_location=bqml_model_export_location,\n",
        "        project=project,\n",
        "        bqml_model=bqml_trained_model,\n",
        "        bq_location=bq_location,\n",
        "    )\n",
        "\n",
        "    batch_prediction_input = split_datasets_op.outputs[\"test_features_jsonl_filenames\"]\n",
        "    ground_truth = split_datasets_op.outputs[\"test_labels_csv_filename\"]\n",
        "\n",
        "    bqml_model_upload_op = vertex_pipeline_components.ModelUploadOp(\n",
        "        project=project,\n",
        "        location=region,\n",
        "        display_name=DISPLAY_NAME + \"_bqml\",\n",
        "        artifact_uri=bqml_model_export_location,\n",
        "        serving_container_image_uri=bqml_serving_container_image_uri,\n",
        "    )\n",
        "    bqml_model_upload_op.after(export_bqml_to_tf_op)\n",
        "    bqml_model = bqml_model_upload_op.outputs[\"model\"]\n",
        "\n",
        "    bqml_model_batch_prediction_task = vertex_pipeline_components.ModelBatchPredictOp(\n",
        "        project=project,\n",
        "        location=region,\n",
        "        model=bqml_model,\n",
        "        job_display_name=job_display_name + \"_bqml\",\n",
        "        gcs_source=batch_prediction_input,\n",
        "        gcs_destination_prefix=gcs_batch_prediction_output_prefix,\n",
        "        predictions_format=\"jsonl\",\n",
        "        instances_format=\"jsonl\",\n",
        "        machine_type=\"n1-standard-2\",\n",
        "    )\n",
        "\n",
        "    dataset_create_op = vertex_pipeline_components.TabularDatasetCreateOp(\n",
        "        project=project,\n",
        "        location=region,\n",
        "        display_name=DISPLAY_NAME,\n",
        "        bq_source=split_datasets_op.outputs[\"dataset_uri\"],\n",
        "    )\n",
        "\n",
        "    automl_training_op = vertex_pipeline_components.AutoMLTabularTrainingJobRunOp(\n",
        "        project=project,\n",
        "        location=region,\n",
        "        display_name=f\"{DISPLAY_NAME}_automl\",\n",
        "        optimization_prediction_type=\"regression\",\n",
        "        optimization_objective=\"minimize-rmse\",\n",
        "        predefined_split_column_name=\"split_col\",\n",
        "        dataset=dataset_create_op.outputs[\"dataset\"],\n",
        "        target_column=\"Rings\",\n",
        "        column_transformations=[\n",
        "            {\"categorical\": {\"column_name\": \"Sex\"}},\n",
        "            {\"numeric\": {\"column_name\": \"Length\"}},\n",
        "            {\"numeric\": {\"column_name\": \"Diameter\"}},\n",
        "            {\"numeric\": {\"column_name\": \"Height\"}},\n",
        "            {\"numeric\": {\"column_name\": \"Whole_weight\"}},\n",
        "            {\"numeric\": {\"column_name\": \"Shucked_weight\"}},\n",
        "            {\"numeric\": {\"column_name\": \"Viscera_weight\"}},\n",
        "            {\"numeric\": {\"column_name\": \"Shell_weight\"}},\n",
        "            {\"numeric\": {\"column_name\": \"Rings\"}},\n",
        "        ],\n",
        "    )\n",
        "    automl_model = automl_training_op.outputs[\"model\"]\n",
        "    automl_model_batch_prediction_task = vertex_pipeline_components.ModelBatchPredictOp(\n",
        "        project=project,\n",
        "        location=region,\n",
        "        model=automl_model,\n",
        "        job_display_name=job_display_name + \"_automl\",\n",
        "        gcs_source=batch_prediction_input,\n",
        "        gcs_destination_prefix=gcs_batch_prediction_output_prefix,\n",
        "        predictions_format=\"jsonl\",\n",
        "        instances_format=\"jsonl\",\n",
        "        machine_type=\"n1-standard-2\",\n",
        "    )\n",
        "\n",
        "    automl_model_evaluation_task = evaluate_batch_predictions(\n",
        "        batch_prediction_job=automl_model_batch_prediction_task.outputs[\n",
        "            \"batchpredictionjob\"\n",
        "        ],\n",
        "        gcs_ground_truth=ground_truth,\n",
        "        model_framework=\"AutoML\",\n",
        "        model_type=\"Regression\",\n",
        "    )\n",
        "\n",
        "    bqml_model_evaluation_task = evaluate_batch_predictions(\n",
        "        batch_prediction_job=bqml_model_batch_prediction_task.outputs[\n",
        "            \"batchpredictionjob\"\n",
        "        ],\n",
        "        gcs_ground_truth=ground_truth,\n",
        "        model_framework=\"BQML\",\n",
        "        model_type=\"Regression\",\n",
        "    )\n",
        "\n",
        "    automl_model_metric = automl_model_evaluation_task.outputs[\"metric\"]\n",
        "    bqml_model_metric = bqml_model_evaluation_task.outputs[\"metric\"]\n",
        "\n",
        "    best_model_task = select_best_model(\n",
        "        model_one=automl_model,\n",
        "        metrics_one=automl_model_metric,  # ,\n",
        "        model_two=bqml_model,\n",
        "        metrics_two=bqml_model_metric,  # automl_model_evaluation_task.outputs['metric'],\n",
        "        thresholds_dict_str=thresholds_dict_str,\n",
        "    )\n",
        "\n",
        "    with dsl.Condition(\n",
        "        best_model_task.outputs[\"deploy_decision\"] == \"true\",\n",
        "        name=\"deploy_decision\",\n",
        "    ):\n",
        "        endpoint_create_op = vertex_pipeline_components.EndpointCreateOp(\n",
        "            project=project,\n",
        "            location=region,\n",
        "            display_name=f\"{DISPLAY_NAME}_endpoint\",\n",
        "        )\n",
        "\n",
        "        endpoint_create_op.after(best_model_task)\n",
        "\n",
        "        model_deploy_op = vertex_pipeline_components.ModelDeployOp(  # noqa: F841\n",
        "            project=project,\n",
        "            location=region,\n",
        "            endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
        "            model=best_model_task.outputs[\"best_model\"],\n",
        "            deployed_model_display_name=DISPLAY_NAME + \"_best\",\n",
        "            machine_type=\"n1-standard-2\",\n",
        "            # traffic_percentage=100\n",
        "        ).set_caching_options(False)\n",
        "\n",
        "        validate_infra_task = validate_infra(\n",
        "            endpoint=model_deploy_op.outputs[\"endpoint\"]\n",
        "        ).set_caching_options(False)\n",
        "\n",
        "        validate_infra_task.after(model_deploy_op)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxfy-pXXGS3R"
      },
      "source": [
        "### Running the Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLeS1xRpGYPx"
      },
      "outputs": [],
      "source": [
        "compiler.Compiler().compile(\n",
        "    pipeline_func=train_pipeline,\n",
        "    package_path=PIPELINE_JSON_PKG_PATH,\n",
        ")\n",
        "\n",
        "vertex.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "pipeline_job = vertex.PipelineJob(\n",
        "    display_name=DISPLAY_NAME,\n",
        "    template_path=PIPELINE_JSON_PKG_PATH,\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        "    parameter_values=pipeline_params,\n",
        "    enable_caching=True,\n",
        ")\n",
        "\n",
        "response = pipeline_job.run()\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGKH0lKwz7Ci"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuMukfDQz51r"
      },
      "outputs": [],
      "source": [
        "vertex.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "delete = False  # set to True if you want to delete resources\n",
        "\n",
        "\n",
        "delete_vertex_dataset = True and delete\n",
        "delete_pipeline = True and delete\n",
        "delete_model = True and delete\n",
        "delete_endpoint = True and delete\n",
        "delete_batchjob = True and delete\n",
        "delete_bucket = True and delete\n",
        "delete_bq_dataset = True and delete\n",
        "\n",
        "try:\n",
        "    if delete_endpoint and \"DISPLAY_NAME\" in globals():\n",
        "        print(\"Will delete endpoint\")\n",
        "        endpoints = vertex.Endpoint.list(\n",
        "            filter=f\"display_name={DISPLAY_NAME}_endpoint\", order_by=\"create_time\"\n",
        "        )\n",
        "        endpoint = endpoints[0]\n",
        "        endpoint.undeploy_all()\n",
        "        vertex.Endpoint.delete(endpoint.resource_name)\n",
        "        print(\"Deleted endpoint:\", endpoint)\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "if delete_model and \"DISPLAY_NAME\" in globals():\n",
        "    print(\"Will delete models\")\n",
        "    suffix_list = [\"bqml\", \"automl\", \"best\"]\n",
        "    for suffix in suffix_list:\n",
        "        try:\n",
        "            model_display_name = f\"{DISPLAY_NAME}_{suffix}\"\n",
        "            print(\"Will delete model with name \" + model_display_name)\n",
        "            models = vertex.Model.list(\n",
        "                filter=f\"display_name={model_display_name}\", order_by=\"create_time\"\n",
        "            )\n",
        "\n",
        "            model = models[0]\n",
        "            vertex.Model.delete(model)\n",
        "            print(\"Deleted model:\", model)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "\n",
        "if delete_vertex_dataset and \"DISPLAY_NAME\" in globals():\n",
        "    print(\"Will delete Vertex dataset\")\n",
        "    try:\n",
        "        datasets = vertex.TabularDataset.list(\n",
        "            filter=f\"display_name={DISPLAY_NAME}\", order_by=\"create_time\"\n",
        "        )\n",
        "\n",
        "        dataset = datasets[0]\n",
        "        vertex.TabularDataset.delete(dataset)\n",
        "        print(\"Deleted Vertex dataset:\", dataset)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "\n",
        "try:\n",
        "    if delete_pipeline and \"DISPLAY_NAME\" in globals():\n",
        "        pipelines = vertex.PipelineJob.list(\n",
        "            filter=f\"pipeline_name={DISPLAY_NAME}\", order_by=\"create_time\"\n",
        "        )\n",
        "        pipeline = pipelines[0]\n",
        "        vertex.PipelineJob.delete(pipeline)\n",
        "        print(\"Deleted pipeline:\", pipeline)\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "if delete_bq_dataset and \"DISPLAY_NAME\" in globals():\n",
        "    from google.cloud import bigquery\n",
        "\n",
        "    try:\n",
        "        # Construct a BigQuery client object.\n",
        "\n",
        "        bq_client = bigquery.Client(project=PROJECT_ID, location=BQ_LOCATION)\n",
        "\n",
        "        # TODO(developer): Set model_id to the ID of the model to fetch.\n",
        "        dataset_id = f\"{PROJECT_ID}.{BQ_DATASET}\"\n",
        "\n",
        "        print(f\"Will delete BQ dataset '{dataset_id}' from location {BQ_LOCATION}.\")\n",
        "        # Use the delete_contents parameter to delete a dataset and its contents.\n",
        "        # Use the not_found_ok parameter to not receive an error if the dataset has already been deleted.\n",
        "        bq_client.delete_dataset(\n",
        "            dataset_id, delete_contents=True, not_found_ok=True\n",
        "        )  # Make an API request.\n",
        "\n",
        "        print(f\"Deleted BQ dataset '{dataset_id}' from location {BQ_LOCATION}.\")\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "\n",
        "if delete_bucket and \"BUCKET_NAME\" in globals():\n",
        "    ! gsutil rm -r gs://$BUCKET_NAME"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "rapid_prototyping_bqml_automl.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40399883"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/structured_data/rapid_prototyping_bqml_automl.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/structured_data/rapid_prototyping_bqml_automl.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okkqa_U8AcsN"
   },
   "source": [
    "# BQML and AutoML - Experimenting with Vertex AI\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/rafacarv-public-bucket-do-not-delete/abalone/automl_and_bqml.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9681af0"
   },
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98513c63"
   },
   "source": [
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    "\n",
    "1. You'll use the *gcloud* command throughout this notebook. In the following cell, enter your project name and run the cell to authenticate yourself with the Google Cloud and initialize your *gcloud* configuration settings.\n",
    "\n",
    "**For this lab, we're going to use region us-central1 for all our resources (BigQuery training data, Cloud Storage bucket, model and endpoint locations, etc.). Those resources can be deployed in other regions, as long as they're consistently co-located, but we're going to use one fixed region to keep things as simple and error free as possible.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "739011eb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7c2da21e",
    "outputId": "9ffe7a3f-95b3-4574-acea-c7709d57e49e"
   },
   "outputs": [],
   "source": [
    "# Install Python package dependencies.\n",
    "print(\"Installing libraries\")\n",
    "! pip3 install {USER_FLAG} --quiet google-cloud-pipeline-components==1.0.0 kfp\n",
    "! pip3 install {USER_FLAG} --quiet --upgrade google-cloud-aiplatform google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9a2bb523c478"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7aBmVRZGr1d"
   },
   "source": [
    "### Required imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3444fe2c"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "from typing import NamedTuple\n",
    "\n",
    "from google.cloud import aiplatform as vertex\n",
    "from google_cloud_pipeline_components import \\\n",
    "    aiplatform as vertex_pipeline_components\n",
    "\n",
    "from google_cloud_pipeline_components.experimental import bigquery as bq_components\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Input, Metrics, Output, component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "050f045b34b8",
    "outputId": "a73ec671-1bf6-444b-91a0-83dcd429aa38"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63_AplznG3J7"
   },
   "source": [
    "### Determine some project and pipeline variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dy-vIuq2yWjw"
   },
   "source": [
    "Instructions:\n",
    "- Make sure the GCS bucket and the BigQuery Dataset do not exist. This script may **delete** any existing content.\n",
    "- Your bucket must be on the same region as your Vertex AI resources.\n",
    "- BQ region can be US or EU;\n",
    "- Make sure your preferred Vertex AI region is supported [[link]](https://cloud.google.com/vertex-ai/docs/general/locations#americas_1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ef138d54"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
    "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "PIPELINE_JSON_PKG_PATH = \"rapid_prototyping.json\"\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET_NAME}/pipeline_root\"\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "DATA_FOLDER = f\"{BUCKET_NAME}/data\"\n",
    "\n",
    "RAW_INPUT_DATA = f\"gs://{DATA_FOLDER}/abalone.csv\"\n",
    "BQ_DATASET = \"j90wipxexhrgq3cquanc5\"  # @param {type:\"string\"}\n",
    "BQ_LOCATION = \"US\"  # @param {type:\"string\"}\n",
    "BQ_LOCATION = BQ_LOCATION.upper()\n",
    "BQML_EXPORT_LOCATION = f\"gs://{BUCKET_NAME}/artifacts/bqml\"\n",
    "\n",
    "DISPLAY_NAME = \"rapid-prototyping\"\n",
    "ENDPOINT_DISPLAY_NAME = f'{DISPLAY_NAME}_endpoint'\n",
    "\n",
    "image_prefix = REGION.split(\"-\")[0]\n",
    "BQML_SERVING_CONTAINER_IMAGE_URI = (\n",
    "    f\"{image_prefix}-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-6:latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90FInWYfG0A9"
   },
   "source": [
    "### Authenticate with GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9QfxrJGn9Tty"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# If on Google Cloud Notebooks, then don't execute this code\n",
    "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "333e4035",
    "outputId": "521c3dd1-21b2-496e-ba1a-257d9ecdfad2"
   },
   "outputs": [],
   "source": [
    "if os.getenv(\"IS_TESTING\"):\n",
    "    !gcloud --quiet components install beta\n",
    "    !gcloud --quiet components update\n",
    "!gcloud config set project $PROJECT_ID\n",
    "!gcloud config set ai/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c211d559"
   },
   "source": [
    "## The Abalone Dataset\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/rafacarv-public-bucket-do-not-delete/abalone/dataset.png\" />\n",
    "\n",
    "<p>Dataset Credits</p>\n",
    "<p>Dua, D. and Graff, C. (2019). UCI Machine Learning Repository <a href=\"http://archive.ics.uci.edu/ml\">http://archive.ics.uci.edu/ml</a>. Irvine, CA: University of California, School of Information and Computer Science.</p>\n",
    "\n",
    "<p><a href=\"https://archive.ics.uci.edu/ml/datasets/abalone\">Direct link</a></p>\n",
    "    \n",
    "    \n",
    "#### Attribute Information:\n",
    "\n",
    "<p>Given is the attribute name, attribute type, the measurement unit and a brief description. The number of rings is the value to predict: either as a continuous value or as a classification problem.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eca2303d"
   },
   "source": [
    "<body>\n",
    "\t<table>\n",
    "\t\t<tr>\n",
    "\t\t\t<th>Name</th>\n",
    "\t\t\t<th>Data Type</th>\n",
    "\t\t\t<th>Measurement Unit</th>\n",
    "\t\t\t<th>Description</th>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Sex</td>\n",
    "            <td>nominal</td>\n",
    "            <td>--</td>\n",
    "            <td>M, F, and I (infant)</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Length</td>\n",
    "            <td>continuous</td>\n",
    "            <td>mm</td>\n",
    "            <td>Longest shell measurement</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Diameter</td>\n",
    "            <td>continuous</td>\n",
    "            <td>mm</td>\n",
    "            <td>perpendicular to length</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Height</td>\n",
    "            <td>continuous</td>\n",
    "            <td>mm</td>\n",
    "            <td>with meat in shell</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Whole weight</td>\n",
    "            <td>continuous</td>\n",
    "            <td>grams</td>\n",
    "            <td>whole abalone</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Shucked weight</td>\n",
    "            <td>continuous</td>\n",
    "            <td>grams</td>\n",
    "            <td>weight of meat</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Viscera weight</td>\n",
    "            <td>continuous</td>\n",
    "            <td>grams</td>\n",
    "            <td>gut weight (after bleeding)</td>\n",
    "\t\t</tr>\n",
    "\t\t<tr>\n",
    "\t\t\t<td>Shell weight</td>\n",
    "            <td>continuous</td>\n",
    "            <td>grams</td>\n",
    "            <td>after being dried</td>\n",
    "\t\t</tr>\n",
    "        <tr>\n",
    "\t\t\t<td>Rings</td>\n",
    "            <td>integer</td>\n",
    "\t\t\t<td>--</td>\n",
    "            <td>+1.5 gives the age in years</td>\n",
    "\t\t</tr>\n",
    "\t</table>\n",
    "</body>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3-Bqs7vFU7Y"
   },
   "source": [
    "### Downloading the data\n",
    "\n",
    "If the bucket does not exist, the script below will create it.\n",
    "\n",
    "After creating the bucket, the cell below will download the dataset into a CSV file and save it in GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d6bd858d",
    "outputId": "28a38d16-dc00-45ed-bb14-913fc6cb9bc3"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -b gs://{BUCKET_NAME} || gsutil mb -l {REGION} gs://{BUCKET_NAME}\n",
    "\n",
    "! gsutil cp gs://cloud-samples-data/vertex-ai/community-content/datasets/abalone/abalone.data {RAW_INPUT_DATA}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owlPQF1KF8QO"
   },
   "source": [
    "## Pipeline Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79eaa73a"
   },
   "source": [
    "### Import to BQ\n",
    "\n",
    "This component takes the csv file and imports it to a table in BigQuery. If the dataset does not exist, it will be created. If a table with the same name already exists, it will be deleted and recreated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e44af8ac"
   },
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.9\", packages_to_install=[\"google-cloud-bigquery\"])\n",
    "def import_data_to_bigquery(\n",
    "    project: str,\n",
    "    bq_location: str,\n",
    "    bq_dataset: str,\n",
    "    gcs_data_uri: str,\n",
    "    raw_dataset: Output[Artifact],\n",
    "    table_name_prefix: str = \"abalone\",\n",
    "):\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    # Construct a BigQuery client object.\n",
    "    client = bigquery.Client(project=project, location=bq_location)\n",
    "\n",
    "    def load_dataset(gcs_uri, table_id):\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            schema=[\n",
    "                bigquery.SchemaField(\"Sex\", \"STRING\"),\n",
    "                bigquery.SchemaField(\"Length\", \"NUMERIC\"),\n",
    "                bigquery.SchemaField(\"Diameter\", \"NUMERIC\"),\n",
    "                bigquery.SchemaField(\"Height\", \"NUMERIC\"),\n",
    "                bigquery.SchemaField(\"Whole_weight\", \"NUMERIC\"),\n",
    "                bigquery.SchemaField(\"Shucked_weight\", \"NUMERIC\"),\n",
    "                bigquery.SchemaField(\"Viscera_weight\", \"NUMERIC\"),\n",
    "                bigquery.SchemaField(\"Shell_weight\", \"NUMERIC\"),\n",
    "                bigquery.SchemaField(\"Rings\", \"NUMERIC\"),\n",
    "            ],\n",
    "            skip_leading_rows=1,\n",
    "            # The source format defaults to CSV, so the line below is optional.\n",
    "            source_format=bigquery.SourceFormat.CSV,\n",
    "        )\n",
    "        print(f\"Loading {gcs_uri} into {table_id}\")\n",
    "        load_job = client.load_table_from_uri(\n",
    "            gcs_uri, table_id, job_config=job_config\n",
    "        )  # Make an API request.\n",
    "\n",
    "        load_job.result()  # Waits for the job to complete.\n",
    "        destination_table = client.get_table(table_id)  # Make an API request.\n",
    "        print(\"Loaded {} rows.\".format(destination_table.num_rows))\n",
    "\n",
    "    def create_dataset_if_not_exist(bq_dataset_id, bq_location):\n",
    "        print(\n",
    "            \"Checking for existence of bq dataset. If it does not exist, it creates one\"\n",
    "        )\n",
    "        dataset = bigquery.Dataset(bq_dataset_id)\n",
    "        dataset.location = bq_location\n",
    "        dataset = client.create_dataset(dataset, exists_ok=True, timeout=300)\n",
    "        print(f\"Created dataset {dataset.full_dataset_id} @ {dataset.location}\")\n",
    "\n",
    "    bq_dataset_id = f\"{project}.{bq_dataset}\"\n",
    "    create_dataset_if_not_exist(bq_dataset_id, bq_location)\n",
    "\n",
    "    raw_table_name = f\"{table_name_prefix}_raw\"\n",
    "    table_id = f\"{project}.{bq_dataset}.{raw_table_name}\"\n",
    "    print(\"Deleting any tables that might have the same name on the dataset\")\n",
    "    client.delete_table(table_id, not_found_ok=True)\n",
    "    print(\"will load data to table\")\n",
    "    load_dataset(gcs_data_uri, table_id)\n",
    "\n",
    "    raw_dataset_uri = f\"bq://{table_id}\"\n",
    "    raw_dataset.uri = raw_dataset_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "637de8be"
   },
   "source": [
    "## Split Datasets\n",
    "\n",
    "Splits the dataset in 3 slices:\n",
    "- TRAIN\n",
    "- EVALUATE\n",
    "- TEST\n",
    "\n",
    "\n",
    "AutoML and BigQuery ML use different nomenclatures for data splits:\n",
    "\n",
    "#### BQML\n",
    "How BQML splits the data: [link](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-hyperparameter-tuning#data_split)\n",
    "\n",
    "#### AutoML\n",
    "How AutoML splits the data: [link](https://cloud.google.com/vertex-ai/docs/general/ml-use?hl=da&skip_cache=false)\n",
    "\n",
    "<ul>\n",
    "    <li>Model trials\n",
    "<p>The training set is used to train models with different preprocessing, architecture, and hyperparameter option combinations. These models are evaluated on the validation set for quality, which guides the exploration of additional option combinations. The best parameters and architectures determined in the parallel tuning phase are used to train two ensemble models as described below.</p></li>\n",
    "\n",
    "<li>Model evaluation\n",
    "<p>\n",
    "Vertex AI trains an evaluation model, using the training and validation sets as training data. Vertex AI generates the final model evaluation metrics on this model, using the test set. This is the first time in the process that the test set is used. This approach ensures that the final evaluation metrics are an unbiased reflection of how well the final trained model will perform in production.</p></li>\n",
    "\n",
    "<li>Serving model\n",
    "<p>A model is trained with the training, validation, and test sets, to maximize the amount of training data. This model is the one that you use to request predictions.</p></li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8cf0a61c"
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-bigquery\"\n",
    "    ],\n",
    ")  # pandas, pyarrow and fsspec required to export bq data to csv\n",
    "def split_datasets(\n",
    "    raw_dataset: Input[Artifact],\n",
    "    bq_location: str,\n",
    ") -> NamedTuple(\n",
    "    \"bqml_split\",\n",
    "    [\n",
    "        (\"dataset_uri\", str),\n",
    "        (\"dataset_bq_uri\", str),\n",
    "        (\"test_dataset_uri\", str),\n",
    "    ],\n",
    "):\n",
    "\n",
    "    from collections import namedtuple\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    raw_dataset_uri = raw_dataset.uri\n",
    "    table_name = raw_dataset_uri.split(\"bq://\")[-1]\n",
    "    print(table_name)\n",
    "    raw_dataset_uri = table_name.split(\".\")\n",
    "    print(raw_dataset_uri)\n",
    "    project = raw_dataset_uri[0]\n",
    "    bq_dataset = raw_dataset_uri[1]\n",
    "    bq_raw_table = raw_dataset_uri[2]\n",
    "\n",
    "    client = bigquery.Client(project=project, location=bq_location)\n",
    "\n",
    "    def split_dataset(table_name_dataset):\n",
    "        training_dataset_table_name = f\"{project}.{bq_dataset}.{table_name_dataset}\"\n",
    "        split_query = f\"\"\"\n",
    "        CREATE OR REPLACE TABLE\n",
    "            `{training_dataset_table_name}`\n",
    "           AS\n",
    "        SELECT\n",
    "          Sex,\n",
    "          Length,\n",
    "          Diameter,\n",
    "          Height,\n",
    "          Whole_weight,\n",
    "          Shucked_weight,\n",
    "          Viscera_weight,\n",
    "          Shell_weight,\n",
    "          Rings,\n",
    "            CASE(ABS(MOD(FARM_FINGERPRINT(TO_JSON_STRING(f)), 10)))\n",
    "              WHEN 9 THEN 'TEST'\n",
    "              WHEN 8 THEN 'VALIDATE'\n",
    "              ELSE 'TRAIN' END AS split_col\n",
    "        FROM\n",
    "          `{project}.{bq_dataset}.abalone_raw` f\n",
    "        \"\"\"\n",
    "        dataset_uri = f\"{project}.{bq_dataset}.{bq_raw_table}\"\n",
    "        print(\"Splitting the dataset\")\n",
    "        query_job = client.query(split_query)  # Make an API request.\n",
    "        query_job.result()\n",
    "        print(dataset_uri)\n",
    "        print(split_query.replace(\"\\n\", \" \"))\n",
    "        return training_dataset_table_name\n",
    "\n",
    "    def create_test_view(training_dataset_table_name, test_view_name='dataset_test'):\n",
    "        view_uri = f'{project}.{bq_dataset}.{test_view_name}'\n",
    "        query = f\"\"\"\n",
    "             CREATE OR REPLACE VIEW `{view_uri}` AS SELECT\n",
    "          Sex,\n",
    "          Length,\n",
    "          Diameter,\n",
    "          Height,\n",
    "          Whole_weight,\n",
    "          Shucked_weight,\n",
    "          Viscera_weight,\n",
    "          Shell_weight,\n",
    "          Rings \n",
    "          FROM `{training_dataset_table_name}`  f\n",
    "          WHERE \n",
    "          f.split_col = 'TEST'\n",
    "          \"\"\"\n",
    "        print(f\"Creating view for --> {test_view_name}\")\n",
    "        print(query.replace(\"\\n\", \" \"))\n",
    "        query_job = client.query(query)  # Make an API request.\n",
    "        query_job.result()\n",
    "        return view_uri\n",
    "\n",
    "   \n",
    "    table_name_dataset = \"dataset\"\n",
    "    \n",
    "    dataset_uri = split_dataset(table_name_dataset)\n",
    "    test_dataset_uri = create_test_view(dataset_uri)\n",
    "    dataset_bq_uri = \"bq://\" + dataset_uri\n",
    "    \n",
    "    print(f\"dataset: {dataset_uri}\")\n",
    "    \n",
    "    result_tuple = namedtuple(\n",
    "        \"bqml_split\",\n",
    "        [\n",
    "            \"dataset_uri\",\n",
    "            \"dataset_bq_uri\",\n",
    "            \"test_dataset_uri\"\n",
    "        ],\n",
    "    )\n",
    "    return result_tuple(\n",
    "        dataset_uri=str(dataset_uri),\n",
    "        dataset_bq_uri=str(dataset_bq_uri),\n",
    "        test_dataset_uri=str(test_dataset_uri)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5e5d1785"
   },
   "source": [
    "### Train BQML Model\n",
    "\n",
    "For this demo, we will use a simple linear regression model on BQML. However, you can be creative with other model architectures, such as Deep Neural Networks, XGboost, Logistic Regression, etc.\n",
    "\n",
    "For a full list of models supported by BQML, look here: [End-to-end user journey for each model](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-e2e-journey).\n",
    "\n",
    "As pointed out before, BQML and AutoML use different split terminologies, so we do an adaptation of the <i>split_col</i> column directly on the SELECT portion of the CREATE model query:\n",
    "\n",
    "> When the value of DATA_SPLIT_METHOD is 'CUSTOM', the corresponding column should be of type BOOL. The rows with TRUE or NULL values are used as evaluation data. Rows with FALSE values are used as training data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "802ee37d"
   },
   "outputs": [],
   "source": [
    "def _query_create_model(project_id:str, bq_dataset: str, training_data_uri: str, model_name: str = \"linear_regression_model_prototyping\"):\n",
    "    model_uri = f\"{project_id}.{bq_dataset}.{model_name}\"\n",
    "    \n",
    "    \n",
    "    model_options = \"\"\"OPTIONS\n",
    "      ( MODEL_TYPE='LINEAR_REG',\n",
    "        input_label_cols=['Rings'],\n",
    "         DATA_SPLIT_METHOD='CUSTOM',\n",
    "        DATA_SPLIT_COL='split_col'\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "    query = f\"\"\"\n",
    "    CREATE OR REPLACE MODEL\n",
    "      `{model_uri}`\n",
    "      {model_options}\n",
    "     AS\n",
    "    SELECT\n",
    "      Sex,\n",
    "      Length,\n",
    "      Diameter,\n",
    "      Height,\n",
    "      Whole_weight,\n",
    "      Shucked_weight,\n",
    "      Viscera_weight,\n",
    "      Shell_weight,\n",
    "      Rings,\n",
    "      CASE(split_col)\n",
    "        WHEN 'TEST' THEN TRUE\n",
    "      ELSE\n",
    "      FALSE\n",
    "    END\n",
    "      AS split_col\n",
    "    FROM\n",
    "      `{training_data_uri}`;\n",
    "    \"\"\"\n",
    "\n",
    "    print(query.replace(\"\\n\", \" \"))\n",
    "\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret BQML Model Evaluation\n",
    "\n",
    "When you do Hyperparameter tuning on the model creation query, the output of the pre-built component [BigqueryEvaluateModelJobOp](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.0/google_cloud_pipeline_components.experimental.bigquery.html#google_cloud_pipeline_components.experimental.bigquery.BigqueryEvaluateModelJobOp) will be a table with the metrics obtained by BQML when training the model. In your BigQuery console, they look like the image below. We need to access them programmatically so we can compare them to the AutoML model. \n",
    "\n",
    "<img src=\"https://storage.googleapis.com/rafacarv-public-bucket-do-not-delete/abalone/bqml-evaluate.png?\">\n",
    "\n",
    "The cell below shows you an example of how this can be done. BQML does not give you a root mean squared error to the list of metrics, so we're manually adding it to the metrics dictionary. For more information aboyt the output, please check [BQML's documentation](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-evaluate#mlevaluate_output). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.9\")\n",
    "def interpret_bqml_evaluation_metrics(bqml_evaluation_metrics: Input[Artifact], metrics: Output[Metrics]) -> dict:\n",
    "    import math\n",
    "\n",
    "    metadata = bqml_evaluation_metrics.metadata\n",
    "    for r in metadata['rows']:\n",
    "        \n",
    "        rows = r['f']\n",
    "        schema = metadata['schema']['fields']\n",
    "        \n",
    "        output = {}\n",
    "        for metric, value in zip(schema, rows):\n",
    "            metric_name = metric['name']\n",
    "            val = float(value['v'])\n",
    "            output[metric_name] = val\n",
    "            metrics.log_metric(metric_name, val)\n",
    "            if metric_name == 'mean_squared_error':\n",
    "                rmse = math.sqrt(val)\n",
    "                metrics.log_metric(\"root_mean_squared_error\", rmse)\n",
    "    \n",
    "    metrics.log_metric(\"framework\", \"BQML\")\n",
    "    \n",
    "    print(output)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret AutoML Model Evaluation\n",
    "\n",
    "Similar to BQML, AutoML also generates metrics during its model creation. These can be accessed in the UI, as seen below:\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/rafacarv-public-bucket-do-not-delete/abalone/automl-evaluate.png\" />\n",
    "\n",
    "Since we don't have a pre-built-component to access these metrics programmatically, we can use the Vertex AI GAPIC (Google API Compiler), which auto-generates low-level gRPC interfaces to the service.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0f2927e"
   },
   "outputs": [],
   "source": [
    "# Inspired by Andrew Ferlitsch's work on https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/get_started_with_automl_pipeline_components.ipynb\n",
    "\n",
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-aiplatform\",\n",
    "    ],\n",
    ")\n",
    "def interpret_automl_evaluation_metrics(region:str, model: Input[Artifact], metrics: Output[Metrics]):\n",
    "    ''''\n",
    "    For a list of available regression metrics, go here: gs://google-cloud-aiplatform/schema/modelevaluation/regression_metrics_1.0.0.yaml.\n",
    "\n",
    "    More information on available metrics for different types of models: https://cloud.google.com/vertex-ai/docs/predictions/online-predictions-automl\n",
    "    '''\n",
    "\n",
    "    import google.cloud.aiplatform.gapic as gapic\n",
    "\n",
    "    # Get a reference to the Model Service client\n",
    "    client_options = {\"api_endpoint\": f\"{region}-aiplatform.googleapis.com\"}\n",
    "\n",
    "    model_service_client = gapic.ModelServiceClient(client_options=client_options)\n",
    "    \n",
    "    model_resource_name = model.metadata[\"resourceName\"]\n",
    "\n",
    "    model_evaluations = model_service_client.list_model_evaluations(parent=model_resource_name)\n",
    "    model_evaluation = list(model_evaluations)[0]\n",
    "\n",
    "    available_metrics = ['meanAbsoluteError', 'meanAbsolutePercentageError', 'rSquared', 'rootMeanSquaredError', 'rootMeanSquaredLogError']\n",
    "    output = dict()\n",
    "    for x in available_metrics:\n",
    "        val = model_evaluation.metrics.get(x)\n",
    "        output[x] = val\n",
    "        metrics.log_metric(str(x), float(val))\n",
    "    \n",
    "    metrics.log_metric(\"framework\", \"AutoML\")\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7421c559"
   },
   "source": [
    "### Model Selection\n",
    "\n",
    "Now that we have evaluated the models independently, we are going to move forward with only one of them. This election will be done based on the model evaluation metrics gathered in the previous steps.\n",
    "\n",
    "Bear in mind that BQML and AutoML use different evaluation metric names, hence we had to do a mapping of these different nomenclatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20d363d9"
   },
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.9\")\n",
    "def select_best_model(\n",
    "    metrics_bqml: Input[Metrics],\n",
    "    metrics_automl: Input[Metrics],\n",
    "    thresholds_dict_str: str,\n",
    "    best_metrics: Output[Metrics],\n",
    "    reference_metric_name: str = \"rmse\"\n",
    ") -> NamedTuple(\n",
    "    \"Outputs\", [(\"deploy_decision\", str), ('best_model', str), (\"metric\", float), (\"metric_name\", str)]\n",
    "):\n",
    "    import json\n",
    "    from collections import namedtuple\n",
    "\n",
    "    best_metric = float(\"inf\")\n",
    "    best_model = None\n",
    "    \n",
    "    # BQML and AutoML use different metric names. \n",
    "    metric_possible_names = []\n",
    "\n",
    "    if reference_metric_name == 'mae':\n",
    "        metric_possible_names = ['meanAbsoluteError', 'mean_absolute_error'] \n",
    "    elif reference_metric_name == 'rmse':\n",
    "        metric_possible_names = ['rootMeanSquaredError', 'root_mean_squared_error'] \n",
    "    \n",
    "    metric_bqml = float(\"inf\")\n",
    "    metric_automl = float(\"inf\")\n",
    "    print(metrics_bqml.metadata)\n",
    "    print(metrics_automl.metadata)\n",
    "    for x in metric_possible_names:\n",
    "        \n",
    "        try:\n",
    "            metric_bqml = metrics_bqml.metadata[x]\n",
    "            print(f'Metric bqml: {metric_bqml}')\n",
    "        except:\n",
    "            print(f'{x} does not exist int the BQML dictionary')\n",
    "        \n",
    "        try:\n",
    "            metric_automl = metrics_automl.metadata[x]\n",
    "            print(f'Metric automl: {metric_automl}')\n",
    "        except:\n",
    "            print(f'{x} does not exist on the AutoML dictionary')\n",
    "\n",
    "    # Change condition if higher is better.\n",
    "    print(f'Comparing BQML ({metric_bqml}) vs AutoML ({metric_automl})')\n",
    "    if metric_bqml <= metric_automl:\n",
    "        best_model = 'bqml'\n",
    "        best_metric = metric_bqml\n",
    "        best_metrics.metadata = metrics_bqml.metadata\n",
    "    else:\n",
    "        best_model = 'automl'\n",
    "        best_metric = metric_automl\n",
    "        best_metrics.metadata = metrics_automl.metadata\n",
    "        \n",
    "    thresholds_dict = json.loads(thresholds_dict_str)\n",
    "    deploy = False\n",
    "    \n",
    "    # Change condition if higher is better.\n",
    "    if best_metric < thresholds_dict[reference_metric_name]:\n",
    "        deploy = True\n",
    "    \n",
    "    if deploy:\n",
    "        deploy_decision = \"true\"\n",
    "    else:\n",
    "        deploy_decision = \"false\"\n",
    "\n",
    "    print(f\"Which model is best? {best_model}\")\n",
    "    print(f\"What metric is being used? {reference_metric_name}\")\n",
    "    print(f\"What is the best metric? {best_metric}\")\n",
    "    print(f\"What is the threshold to deploy? {thresholds_dict_str}\")\n",
    "    print(f\"Deploy decision: {deploy_decision}\")\n",
    "\n",
    "    Outputs = namedtuple(\"Outputs\", [\"deploy_decision\", \"best_model\", \"metric\", \"metric_name\"])\n",
    "\n",
    "    return Outputs(\n",
    "        deploy_decision=deploy_decision,\n",
    "        best_model=best_model,\n",
    "        metric=best_metric,\n",
    "        metric_name=reference_metric_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0f573556"
   },
   "source": [
    "### Validate Infrastructure\n",
    "\n",
    "Once the best model has been deployed, we will validate the endpoint by making a simple prediction to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aa1bab55"
   },
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.9\", packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def validate_infrastructure(\n",
    "    endpoint: Input[Artifact],\n",
    ") -> NamedTuple(\n",
    "    \"validate_infrastructure_output\", [(\"instance\", str), (\"prediction\", float)]\n",
    "):\n",
    "    import json\n",
    "    from collections import namedtuple\n",
    "\n",
    "    from google.cloud import aiplatform\n",
    "    from google.protobuf import json_format\n",
    "    from google.protobuf.struct_pb2 import Value\n",
    "\n",
    "    def treat_uri(uri):\n",
    "        return uri[uri.find(\"projects/\") :]\n",
    "\n",
    "    def request_prediction(endp, instance):\n",
    "        instance = json_format.ParseDict(instance, Value())\n",
    "        instances = [instance]\n",
    "        parameters_dict = {}\n",
    "        parameters = json_format.ParseDict(parameters_dict, Value())\n",
    "        response = endp.predict(instances=instances, parameters=parameters)\n",
    "        print(\"deployed_model_id:\", response.deployed_model_id)\n",
    "        print(\"predictions: \", response.predictions)\n",
    "        # The predictions are a google.protobuf.Value representation of the model's predictions.\n",
    "        predictions = response.predictions\n",
    "\n",
    "        for pred in predictions:\n",
    "            if type(pred) is dict and \"value\" in pred.keys():\n",
    "                # AutoML predictions\n",
    "                prediction = pred[\"value\"]\n",
    "            elif type(pred) is list:\n",
    "                # BQML Predictions return different format\n",
    "                prediction = pred[0]\n",
    "            return prediction\n",
    "\n",
    "    endpoint_uri = endpoint.uri\n",
    "    treated_uri = treat_uri(endpoint_uri)\n",
    "\n",
    "    instance = {\n",
    "        \"Sex\": \"M\",\n",
    "        \"Length\": 0.33,\n",
    "        \"Diameter\": 0.255,\n",
    "        \"Height\": 0.08,\n",
    "        \"Whole_weight\": 0.205,\n",
    "        \"Shucked_weight\": 0.0895,\n",
    "        \"Viscera_weight\": 0.0395,\n",
    "        \"Shell_weight\": 0.055,\n",
    "    }\n",
    "    instance_json = json.dumps(instance)\n",
    "    print(\"Will use the following instance: \" + instance_json)\n",
    "\n",
    "    endpoint = aiplatform.Endpoint(treated_uri)\n",
    "    prediction = request_prediction(endpoint, instance)\n",
    "    result_tuple = namedtuple(\n",
    "        \"validate_infrastructure_output\", [\"instance\", \"prediction\"]\n",
    "    )\n",
    "\n",
    "    return result_tuple(instance=str(instance_json), prediction=float(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpB_bdDbGGOp"
   },
   "source": [
    "## The Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O5PsR31ysGuj"
   },
   "outputs": [],
   "source": [
    "\n",
    "pipeline_params = {\n",
    "    \"project\": PROJECT_ID,\n",
    "    \"region\": REGION,\n",
    "    \"gcs_input_file_uri\": RAW_INPUT_DATA,\n",
    "    \"bq_dataset\": BQ_DATASET,\n",
    "    \"bq_location\": BQ_LOCATION,\n",
    "    \"bqml_model_export_location\": BQML_EXPORT_LOCATION,\n",
    "    \"bqml_serving_container_image_uri\": BQML_SERVING_CONTAINER_IMAGE_URI,\n",
    "    \"endpoint_display_name\": ENDPOINT_DISPLAY_NAME,\n",
    "    \"thresholds_dict_str\": '{\"rmse\": 2.5}',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=DISPLAY_NAME, description=\"Rapid Prototyping\")\n",
    "def train_pipeline(\n",
    "    project: str,\n",
    "    gcs_input_file_uri: str,\n",
    "    region: str,\n",
    "    bq_dataset: str,\n",
    "    bq_location: str,\n",
    "    bqml_model_export_location: str,\n",
    "    bqml_serving_container_image_uri: str,\n",
    "    endpoint_display_name: str,\n",
    "    thresholds_dict_str: str,\n",
    "):\n",
    "    # Imports data to BigQuery using a custom component.\n",
    "    import_data_to_bigquery_op = import_data_to_bigquery(\n",
    "        project, bq_location, bq_dataset, gcs_input_file_uri\n",
    "    )\n",
    "    raw_dataset = import_data_to_bigquery_op.outputs[\"raw_dataset\"]\n",
    "\n",
    "    # Splits the BQ dataset using a custom component.\n",
    "    split_datasets_op = split_datasets(\n",
    "        raw_dataset, bq_location=bq_location\n",
    "    )\n",
    "\n",
    "    # Generates the query to create a BQML using a static function.\n",
    "    create_model_query = _query_create_model(project, bq_dataset, split_datasets_op.outputs['dataset_uri'])\n",
    "    \n",
    "    # Builds BQML model using pre-built-component.\n",
    "    bqml_create_op = bq_components.BigqueryCreateModelJobOp(\n",
    "        project=project,\n",
    "        location=bq_location,\n",
    "        query=create_model_query\n",
    "    )\n",
    "    bqml_model = bqml_create_op.outputs['model']\n",
    "    \n",
    "    # Gathers BQML evaluation metrics using a pre-built-component.\n",
    "    bqml_evaluate_op = bq_components.BigqueryEvaluateModelJobOp(project=project,location=bq_location,model=bqml_model)\n",
    "    bqml_eval_metrics_raw = bqml_evaluate_op.outputs['evaluation_metrics']\n",
    "\n",
    "    # Analyzes evaluation BQML metrics using a custom component.\n",
    "    interpret_bqml_evaluation_metrics_op = interpret_bqml_evaluation_metrics(bqml_evaluation_metrics=bqml_eval_metrics_raw)\n",
    "    bqml_eval_metrics = interpret_bqml_evaluation_metrics_op.outputs['metrics']\n",
    "    \n",
    "    # Exports the BQML model to a GCS bucket using a pre-built-component.\n",
    "    bqml_export_op = bq_components.BigqueryExportModelJobOp(project=project,\n",
    "        location=bq_location,\n",
    "        model = bqml_model,\n",
    "        model_destination_path=bqml_model_export_location\n",
    "        ).after(bqml_evaluate_op)\n",
    "    bqml_exported_gcs_path = bqml_export_op.outputs['exported_model_path']\n",
    "\n",
    "    # Uploads the recently exported the BQML model from GCS into Vertex AI using a pre-built-component.\n",
    "    bqml_model_upload_op = vertex_pipeline_components.ModelUploadOp(\n",
    "        project=project,\n",
    "        location=region,\n",
    "        display_name=DISPLAY_NAME + \"_bqml\",\n",
    "        artifact_uri=bqml_exported_gcs_path,\n",
    "        serving_container_image_uri=bqml_serving_container_image_uri,\n",
    "    )\n",
    "    bqml_vertex_model = bqml_model_upload_op.outputs['model']\n",
    "\n",
    "    # Creates a Vertex AI Tabular dataset using a pre-built-component.\n",
    "    dataset_create_op = vertex_pipeline_components.TabularDatasetCreateOp(\n",
    "        project=project,\n",
    "        location=region,\n",
    "        display_name=DISPLAY_NAME,\n",
    "        bq_source=split_datasets_op.outputs[\"dataset_bq_uri\"],\n",
    "    )\n",
    "\n",
    "    # Trains an AutoML Tables model using a pre-built-component.\n",
    "    automl_training_op = vertex_pipeline_components.AutoMLTabularTrainingJobRunOp(\n",
    "        project=project,\n",
    "        location=region,\n",
    "        display_name=f\"{DISPLAY_NAME}_automl\",\n",
    "        optimization_prediction_type=\"regression\",\n",
    "        optimization_objective=\"minimize-rmse\",\n",
    "        predefined_split_column_name=\"split_col\",\n",
    "        dataset=dataset_create_op.outputs[\"dataset\"],\n",
    "        target_column=\"Rings\",\n",
    "        column_transformations=[\n",
    "            {\"categorical\": {\"column_name\": \"Sex\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Length\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Diameter\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Height\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Whole_weight\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Shucked_weight\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Viscera_weight\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Shell_weight\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Rings\"}},\n",
    "        ],\n",
    "    )\n",
    "    automl_model = automl_training_op.outputs[\"model\"]\n",
    "\n",
    "    # Analyzes evaluation AutoML metrics using a custom component.\n",
    "    automl_eval_op = interpret_automl_evaluation_metrics(region=region, model=automl_model)\n",
    "    automl_eval_metrics = automl_eval_op.outputs['metrics']\n",
    "    \n",
    "    # 1) Decides which model is best (AutoML vs BQML);\n",
    "    # 2) Determines if the best model meets the deployment condition.\n",
    "    best_model_task = select_best_model(\n",
    "        metrics_bqml=bqml_eval_metrics,  \n",
    "        metrics_automl=automl_eval_metrics,\n",
    "        thresholds_dict_str=thresholds_dict_str,\n",
    "    )\n",
    "\n",
    "    # If the deploy condition is True, then deploy the best model.\n",
    "    with dsl.Condition(\n",
    "        best_model_task.outputs[\"deploy_decision\"] == \"true\",\n",
    "        name=\"deploy_decision\",\n",
    "    ):\n",
    "        # Creates a Vertex AI endpoint using a pre-built-component.\n",
    "        endpoint_create_op = vertex_pipeline_components.EndpointCreateOp(\n",
    "            project=project,\n",
    "            location=region,\n",
    "            display_name=endpoint_display_name,\n",
    "        )\n",
    "        #endpoint_create_op.after(best_model_task)\n",
    "\n",
    "        # In case the BQML model is the best...\n",
    "        with dsl.Condition(\n",
    "            best_model_task.outputs['best_model'] == 'bqml',\n",
    "            name=\"deploy_bqml\",\n",
    "        ):\n",
    "            # Deploys the BQML model (now on Vertex AI) to the recently created endpoint using a pre-built component.\n",
    "            model_deploy_bqml_op = vertex_pipeline_components.ModelDeployOp(  # noqa: F841\n",
    "                endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "                model=bqml_vertex_model,\n",
    "                deployed_model_display_name=DISPLAY_NAME + \"_best_bqml\",\n",
    "                dedicated_resources_machine_type=\"n1-standard-2\",\n",
    "                dedicated_resources_min_replica_count=2,\n",
    "                dedicated_resources_max_replica_count=2,\n",
    "                traffic_split={'0': 100} #newly deployed model gets 100% of the traffic\n",
    "            ).set_caching_options(False)\n",
    "            \n",
    "            # Sends an online prediction request to the recently deployed model using a custom component.\n",
    "            validate_infrastructure(\n",
    "                endpoint=endpoint_create_op.outputs[\"endpoint\"]\n",
    "                ).set_caching_options(False).after(model_deploy_bqml_op)\n",
    "        \n",
    "        # In case the AutoML model is the best...\n",
    "        with dsl.Condition(\n",
    "            best_model_task.outputs['best_model'] == 'automl',\n",
    "            name=\"deploy_automl\",\n",
    "        ):\n",
    "            # Deploys the AutoML model to the recently created endpoint using a pre-built component.\n",
    "            model_deploy_automl_op = vertex_pipeline_components.ModelDeployOp(  # noqa: F841\n",
    "                endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "                model=automl_model,\n",
    "                deployed_model_display_name=DISPLAY_NAME + \"_best_automl\",\n",
    "                dedicated_resources_machine_type=\"n1-standard-2\",\n",
    "                dedicated_resources_min_replica_count=2,\n",
    "                dedicated_resources_max_replica_count=2,\n",
    "                traffic_split={'0': 100} #newly deployed model gets 100% of the traffic\n",
    "            ).set_caching_options(False)\n",
    "            \n",
    "            # Sends an online prediction request to the recently deployed model using a custom component.\n",
    "            validate_infrastructure(\n",
    "                endpoint=endpoint_create_op.outputs[\"endpoint\"]\n",
    "                ).set_caching_options(False).after(model_deploy_automl_op)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxfy-pXXGS3R"
   },
   "source": [
    "### Running the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "PLeS1xRpGYPx",
    "outputId": "c0fa0e39-09d6-4714-8dea-032a426ca018"
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=train_pipeline,\n",
    "    package_path=PIPELINE_JSON_PKG_PATH,\n",
    ")\n",
    "\n",
    "\n",
    "vertex.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "pipeline_job = vertex.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=PIPELINE_JSON_PKG_PATH,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values=pipeline_params,\n",
    "    enable_caching=True,\n",
    ")\n",
    "\n",
    "response = pipeline_job.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGKH0lKwz7Ci"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "LuMukfDQz51r"
   },
   "outputs": [],
   "source": [
    "vertex.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "delete = False  # set to True if you want to delete resources\n",
    "\n",
    "\n",
    "delete_vertex_dataset = True and delete\n",
    "delete_pipeline = True and delete\n",
    "delete_model = True and delete\n",
    "delete_endpoint = True and delete\n",
    "delete_batchjob = True and delete\n",
    "delete_bucket = True and delete\n",
    "delete_bq_dataset = True and delete\n",
    "\n",
    "try:\n",
    "    if delete_endpoint and \"DISPLAY_NAME\" in globals():\n",
    "        print(\"Will delete endpoint\")\n",
    "        endpoints = vertex.Endpoint.list(\n",
    "            filter=f\"display_name={DISPLAY_NAME}_endpoint\", order_by=\"create_time\"\n",
    "        )\n",
    "        endpoint = endpoints[0]\n",
    "        endpoint.undeploy_all()\n",
    "        vertex.Endpoint.delete(endpoint.resource_name)\n",
    "        print(\"Deleted endpoint:\", endpoint)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "if delete_model and \"DISPLAY_NAME\" in globals():\n",
    "    print(\"Will delete models\")\n",
    "    suffix_list = [\"bqml\", \"automl\", \"best\"]\n",
    "    for suffix in suffix_list:\n",
    "        try:\n",
    "            model_display_name = f\"{DISPLAY_NAME}_{suffix}\"\n",
    "            print(\"Will delete model with name \" + model_display_name)\n",
    "            models = vertex.Model.list(\n",
    "                filter=f\"display_name={model_display_name}\", order_by=\"create_time\"\n",
    "            )\n",
    "\n",
    "            model = models[0]\n",
    "            vertex.Model.delete(model)\n",
    "            print(\"Deleted model:\", model)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "if delete_vertex_dataset and \"DISPLAY_NAME\" in globals():\n",
    "    print(\"Will delete Vertex dataset\")\n",
    "    try:\n",
    "        datasets = vertex.TabularDataset.list(\n",
    "            filter=f\"display_name={DISPLAY_NAME}\", order_by=\"create_time\"\n",
    "        )\n",
    "\n",
    "        dataset = datasets[0]\n",
    "        vertex.TabularDataset.delete(dataset)\n",
    "        print(\"Deleted Vertex dataset:\", dataset)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "try:\n",
    "    if delete_pipeline and \"DISPLAY_NAME\" in globals():\n",
    "        pipelines = vertex.PipelineJob.list(\n",
    "            filter=f\"pipeline_name={DISPLAY_NAME}\", order_by=\"create_time\"\n",
    "        )\n",
    "        pipeline = pipelines[0]\n",
    "        vertex.PipelineJob.delete(pipeline)\n",
    "        print(\"Deleted pipeline:\", pipeline)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "if delete_bq_dataset and \"DISPLAY_NAME\" in globals():\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    try:\n",
    "        # Construct a BigQuery client object.\n",
    "\n",
    "        bq_client = bigquery.Client(project=PROJECT_ID, location=BQ_LOCATION)\n",
    "\n",
    "        # TODO(developer): Set model_id to the ID of the model to fetch.\n",
    "        dataset_id = f\"{PROJECT_ID}.{BQ_DATASET}\"\n",
    "\n",
    "        print(f\"Will delete BQ dataset '{dataset_id}' from location {BQ_LOCATION}.\")\n",
    "        # Use the delete_contents parameter to delete a dataset and its contents.\n",
    "        # Use the not_found_ok parameter to not receive an error if the dataset has already been deleted.\n",
    "        bq_client.delete_dataset(\n",
    "            dataset_id, delete_contents=True, not_found_ok=True\n",
    "        )  # Make an API request.\n",
    "\n",
    "        print(f\"Deleted BQ dataset '{dataset_id}' from location {BQ_LOCATION}.\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "if delete_bucket and \"BUCKET_NAME\" in globals():\n",
    "    ! gsutil rm -r gs://$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "def deploy_model_with_dedicated_resources_sample(\n",
    "    project,\n",
    "    location,\n",
    "    model_name: str,\n",
    "    machine_type: str,\n",
    "    endpoint_id= None,\n",
    "    deployed_model_display_name = None,\n",
    "    traffic_percentage = 0,\n",
    "    traffic_split = None,\n",
    "    min_replica_count: int = 1,\n",
    "    max_replica_count: int = 1,\n",
    "    accelerator_type= None,\n",
    "    accelerator_count= None,\n",
    "    explanation_metadata= None,\n",
    "    explanation_parameters = None,\n",
    "    metadata = (),\n",
    "    sync = True,\n",
    "):\n",
    "    \"\"\"\n",
    "        model_name: A fully-qualified model resource name or model ID.\n",
    "              Example: \"projects/123/locations/us-central1/models/456\" or\n",
    "              \"456\" when project and location are initialized or passed.\n",
    "    \"\"\"\n",
    "\n",
    "    aiplatform.init(project=project, location=location)\n",
    "\n",
    "    model = aiplatform.Model(model_name=model_name)\n",
    "\n",
    "\n",
    "    endpoint = aiplatform.Endpoint(endpoint_name=endpoint_id)\n",
    "    # The explanation_metadata and explanation_parameters should only be\n",
    "    # provided for a custom trained model and not an AutoML model.\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        deployed_model_display_name=deployed_model_display_name,\n",
    "        traffic_percentage=traffic_percentage,\n",
    "        traffic_split=traffic_split,\n",
    "        machine_type=machine_type,\n",
    "        min_replica_count=min_replica_count,\n",
    "        max_replica_count=max_replica_count,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        explanation_metadata=explanation_metadata,\n",
    "        explanation_parameters=explanation_parameters,\n",
    "        metadata=metadata,\n",
    "        sync=sync,\n",
    "    )\n",
    "\n",
    "    model.wait()\n",
    "\n",
    "    print(model.display_name)\n",
    "    print(model.resource_name)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "deploy_model_with_dedicated_resources_sample(project=PROJECT_ID, location=REGION, model_name=\"7538713514915921920\", deployed_model_display_name=DISPLAY_NAME + \"_best_bqml\",\n",
    "                endpoint_id='4250597603772727296',\n",
    "                machine_type=\"n1-standard-2\",\n",
    "                min_replica_count=1,\n",
    "                max_replica_count=2,\n",
    "                traffic_split={'0': 100})\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "nov1st rapid_prototyping_bqml_automl.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
